
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10318v1")'>Anytime Neural Architecture Search on Tabular Data</div>
<div id='2403.10318v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T14:09:46Z</div><div>Authors: Naili Xing, Shaofeng Cai, Zhaojing Luo, BengChin Ooi, Jian Pei</div><div style='padding-top: 10px; width: 80ex'>The increasing demand for tabular data analysis calls for transitioning from
manual architecture design to Neural Architecture Search (NAS). This transition
demands an efficient and responsive anytime NAS approach that is capable of
returning current optimal architectures within any given time budget while
progressively enhancing architecture quality with increased budget allocation.
However, the area of research on Anytime NAS for tabular data remains
unexplored. To this end, we introduce ATLAS, the first anytime NAS approach
tailored for tabular data. ATLAS introduces a novel two-phase
filtering-and-refinement optimization scheme with joint optimization, combining
the strengths of both paradigms of training-free and training-based
architecture evaluation. Specifically, in the filtering phase, ATLAS employs a
new zero-cost proxy specifically designed for tabular data to efficiently
estimate the performance of candidate architectures, thereby obtaining a set of
promising architectures. Subsequently, in the refinement phase, ATLAS leverages
a fixed-budget search algorithm to schedule the training of the promising
candidates, so as to accurately identify the optimal architecture. To jointly
optimize the two phases for anytime NAS, we also devise a budget-aware
coordinator that delivers high NAS performance within constraints. Experimental
evaluations demonstrate that our ATLAS can obtain a good-performing
architecture within any predefined time budget and return better architectures
as and when a new time budget is made available. Overall, it reduces the search
time on tabular data by up to 82.75x compared to existing NAS approaches.</div><div><a href='http://arxiv.org/abs/2403.10318v1'>2403.10318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08139v1")'>Transferring Core Knowledge via Learngenes</div>
<div id='2401.08139v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T06:18:11Z</div><div>Authors: Fu Feng, Jing Wang, Xin Geng</div><div style='padding-top: 10px; width: 80ex'>The pre-training paradigm fine-tunes the models trained on large-scale
datasets to downstream tasks with enhanced performance. It transfers all
knowledge to downstream tasks without discriminating which part is necessary or
unnecessary, which may lead to negative transfer. In comparison, knowledge
transfer in nature is much more efficient. When passing genetic information to
descendants, ancestors encode only the essential knowledge into genes, which
act as the medium. Inspired by that, we adopt a recent concept called
``learngene'' and refine its structures by mimicking the structures of natural
genes. We propose the Genetic Transfer Learning (GTL) -- a framework to copy
the evolutionary process of organisms into neural networks. GTL trains a
population of networks, selects superior learngenes by tournaments, performs
learngene mutations, and passes the learngenes to next generations. Finally, we
successfully extract the learngenes of VGG11 and ResNet12. We show that the
learngenes bring the descendant networks instincts and strong learning ability:
with 20% parameters, the learngenes bring 12% and 16% improvements of accuracy
on CIFAR-FS and miniImageNet. Besides, the learngenes have the scalability and
adaptability on the downstream structure of networks and datasets. Overall, we
offer a novel insight that transferring core knowledge via learngenes may be
sufficient and efficient for neural networks.</div><div><a href='http://arxiv.org/abs/2401.08139v1'>2401.08139v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03480v1")'>Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A
  Survey and Vision</div>
<div id='2402.03480v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T19:48:31Z</div><div>Authors: Nathaniel Hudson, J. Gregory Pauloski, Matt Baughman, Alok Kamatar, Mansi Sakarvadia, Logan Ward, Ryan Chard, Andr√© Bauer, Maksim Levental, Wenyi Wang, Will Engler, Owen Price Skelly, Ben Blaiszik, Rick Stevens, Kyle Chard, Ian Foster</div><div style='padding-top: 10px; width: 80ex'>Deep learning methods are transforming research, enabling new techniques, and
ultimately leading to new discoveries. As the demand for more capable AI models
continues to grow, we are now entering an era of Trillion Parameter Models
(TPM), or models with more than a trillion parameters -- such as Huawei's
PanGu-$\Sigma$. We describe a vision for the ecosystem of TPM users and
providers that caters to the specific needs of the scientific community. We
then outline the significant technical challenges and open problems in system
design for serving TPMs to enable scientific research and discovery.
Specifically, we describe the requirements of a comprehensive software stack
and interfaces to support the diverse and flexible requirements of researchers.</div><div><a href='http://arxiv.org/abs/2402.03480v1'>2402.03480v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16848v1")'>InterroGate: Learning to Share, Specialize, and Prune Representations
  for Multi-task Learning</div>
<div id='2402.16848v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:59:52Z</div><div>Authors: Babak Ehteshami Bejnordi, Gaurav Kumar, Amelie Royer, Christos Louizos, Tijmen Blankevoort, Mohsen Ghafoorian</div><div style='padding-top: 10px; width: 80ex'>Jointly learning multiple tasks with a unified model can improve accuracy and
data efficiency, but it faces the challenge of task interference, where
optimizing one task objective may inadvertently compromise the performance of
another. A solution to mitigate this issue is to allocate task-specific
parameters, free from interference, on top of shared features. However,
manually designing such architectures is cumbersome, as practitioners need to
balance between the overall performance across all tasks and the higher
computational cost induced by the newly added parameters. In this work, we
propose \textit{InterroGate}, a novel multi-task learning (MTL) architecture
designed to mitigate task interference while optimizing inference computational
efficiency. We employ a learnable gating mechanism to automatically balance the
shared and task-specific representations while preserving the performance of
all tasks. Crucially, the patterns of parameter sharing and specialization
dynamically learned during training, become fixed at inference, resulting in a
static, optimized MTL architecture. Through extensive empirical evaluations, we
demonstrate SoTA results on three MTL benchmarks using convolutional as well as
transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context.</div><div><a href='http://arxiv.org/abs/2402.16848v1'>2402.16848v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11124v1")'>EMA-Net: Efficient Multitask Affinity Learning for Dense Scene
  Predictions</div>
<div id='2401.11124v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T05:31:47Z</div><div>Authors: Dimitrios Sinodinos, Narges Armanfard</div><div style='padding-top: 10px; width: 80ex'>Multitask learning (MTL) has gained prominence for its ability to jointly
predict multiple tasks, achieving better per-task performance while using fewer
per-task model parameters than single-task learning. More recently,
decoder-focused architectures have considerably improved multitask performance
by refining task predictions using the features of other related tasks.
However, most of these refinement methods fail to simultaneously capture local
and global task-specific representations, as well as cross-task patterns in a
parameter-efficient manner. In this paper, we introduce the Efficient Multitask
Affinity Learning Network (EMA-Net), which is a lightweight framework that
enhances the task refinement capabilities of multitask networks. EMA-Net
adeptly captures local, global, and cross-task interactions using our novel
Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in
its ability to manipulate task affinity matrices in a manner that is optimally
suited to apply parameter-efficient grouped convolutions without worrying about
information loss. Our results show that we achieve state-of-the-art MTL
performance for CNN-based decoder-focused models while using substantially
fewer model parameters. Our code is publicly available at
https://github.com/Armanfard-Lab/EMA-Net.</div><div><a href='http://arxiv.org/abs/2401.11124v1'>2401.11124v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02431v1")'>Learning Mutual Excitation for Hand-to-Hand and Human-to-Human
  Interaction Recognition</div>
<div id='2402.02431v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:00:00Z</div><div>Authors: Mengyuan Liu, Chen Chen, Songtao Wu, Fanyang Meng, Hong Liu</div><div style='padding-top: 10px; width: 80ex'>Recognizing interactive actions, including hand-to-hand interaction and
human-to-human interaction, has attracted increasing attention for various
applications in the field of video analysis and human-robot interaction.
Considering the success of graph convolution in modeling topology-aware
features from skeleton data, recent methods commonly operate graph convolution
on separate entities and use late fusion for interactive action recognition,
which can barely model the mutual semantic relationships between pairwise
entities. To this end, we propose a mutual excitation graph convolutional
network (me-GCN) by stacking mutual excitation graph convolution (me-GC)
layers. Specifically, me-GC uses a mutual topology excitation module to firstly
extract adjacency matrices from individual entities and then adaptively model
the mutual constraints between them. Moreover, me-GC extends the above idea and
further uses a mutual feature excitation module to extract and merge deep
features from pairwise entities. Compared with graph convolution, our proposed
me-GC gradually learns mutual information in each layer and each stage of graph
convolution operations. Extensive experiments on a challenging hand-to-hand
interaction dataset, i.e., the Assembely101 dataset, and two large-scale
human-to-human interaction datasets, i.e., NTU60-Interaction and
NTU120-Interaction consistently verify the superiority of our proposed method,
which outperforms the state-of-the-art GCN-based and Transformer-based methods.</div><div><a href='http://arxiv.org/abs/2402.02431v1'>2402.02431v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01342v1")'>Training-time Neuron Alignment through Permutation Subspace for
  Improving Linear Mode Connectivity and Model Fusion</div>
<div id='2402.01342v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:57:50Z</div><div>Authors: Zexi Li, Zhiqi Li, Jie Lin, Tao Shen, Tao Lin, Chao Wu</div><div style='padding-top: 10px; width: 80ex'>In deep learning, stochastic gradient descent often yields functionally
similar yet widely scattered solutions in the weight space even under the same
initialization, causing barriers in the Linear Mode Connectivity (LMC)
landscape. Overcoming these barriers is crucial for understanding deep learning
dynamics and enhancing model-fusion algorithms. Previous studies highlight the
role of permutation symmetry in reducing post-training barriers through network
permutation. However, these post-hoc methods, demanding extra computations, are
less effective for larger, complex models (e.g., ViT, LLM) due to numerous
permutation matrices. Thus, in this paper, we study training-time neuron
alignment. Our hypothesis suggests that training-time permutation subspace can
reduce LMC barriers for free. We find that pruning at initialization supports
this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm
using a partial gradient mask during training. TNA-PFN is theoretically and
empirically validated for reducing LMC barriers. It excels in wide model fusion
applications, especially in federated learning, two algorithms based on TNA-FPN
that are proposed to show its prospects even under heterogeneous datasets.
Moreover, TNA-PFN can enhance the generalization of model soup for vision
transformers and ColD fusion for pretrained language models.</div><div><a href='http://arxiv.org/abs/2402.01342v1'>2402.01342v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09613v1")'>Reawakening knowledge: Anticipatory recovery from catastrophic
  interference via structured training</div>
<div id='2403.09613v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:51:54Z</div><div>Authors: Yanlai Yang, Matt Jones, Michael C. Mozer, Mengye Ren</div><div style='padding-top: 10px; width: 80ex'>We explore the training dynamics of neural networks in a structured non-IID
setting where documents are presented cyclically in a fixed, repeated sequence.
Typically, networks suffer from catastrophic interference when training on a
sequence of documents; however, we discover a curious and remarkable property
of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory
behavior, recovering from the forgetting on documents before encountering them
again. The behavior emerges and becomes more robust as the architecture scales
up its number of parameters. Through comprehensive experiments and
visualizations, we uncover new insights into training over-parameterized
networks in structured environments.</div><div><a href='http://arxiv.org/abs/2403.09613v1'>2403.09613v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14746v1")'>Scaling Efficient LLMs</div>
<div id='2402.14746v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:06:19Z</div><div>Authors: B. N. Kausik</div><div style='padding-top: 10px; width: 80ex'>Trained LLMs are typically sparse in that most of the parameters are zero,
raising questions on efficiency. In response, we inquire into efficient LLMs,
i.e. those with the fewest parameters that achieve the desired accuracy on a
training corpus. Specifically, we compare theoretical and empirical estimates
for training loss at current scale to obtain upper and lower bounds on the
number of unique sequences in a natural training corpus as a function of its
size. Our result implies (1) to double the number of skills represented in a
training corpus, the corpus must scale roughly between three and five fold (2)
for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural
training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of
an LLM is smaller than the number of unique sequences in the training corpus,
scaling up can uncover emergent skills.</div><div><a href='http://arxiv.org/abs/2402.14746v1'>2402.14746v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17812v1")'>DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping
  Backward Propagation</div>
<div id='2402.17812v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T14:51:11Z</div><div>Authors: Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon, Dongsuk Jeon, Dongsoo Lee</div><div style='padding-top: 10px; width: 80ex'>Training deep neural networks typically involves substantial computational
costs during both forward and backward propagation. The conventional layer
dropping techniques drop certain layers during training for reducing the
computations burden. However, dropping layers during forward propagation
adversely affects the training process by degrading accuracy. In this paper, we
propose Dropping Backward Propagation (DropBP), a novel approach designed to
reduce computational costs while maintaining accuracy. DropBP randomly drops
layers during the backward propagation, which does not deviate forward
propagation. Moreover, DropBP calculates the sensitivity of each layer to
assign appropriate drop rate, thereby stabilizing the training process. DropBP
is designed to enhance the efficiency of the training process with
backpropagation, thereby enabling the acceleration of both full fine-tuning and
parameter-efficient fine-tuning using backpropagation. Specifically, utilizing
DropBP in QLoRA reduces training time by 44%, increases the convergence speed
to the identical loss level by 1.5$\times$, and enables training with a
6.2$\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in
LLaMA2-70B. The code is available at https://github.com/WooSunghyeon/dropbp.</div><div><a href='http://arxiv.org/abs/2402.17812v1'>2402.17812v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17227v1")'>Efficient Backpropagation with Variance-Controlled Adaptive Sampling</div>
<div id='2402.17227v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T05:40:36Z</div><div>Authors: Ziteng Wang, Jianfei Chen, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>Sampling-based algorithms, which eliminate ''unimportant'' computations
during forward and/or back propagation (BP), offer potential solutions to
accelerate neural network training. However, since sampling introduces
approximations to training, such algorithms may not consistently maintain
accuracy across various tasks. In this work, we introduce a variance-controlled
adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an
unbiased stochastic gradient with fine-grained layerwise importance sampling in
data dimension for activation gradient calculation and leverage score sampling
in token dimension for weight gradient calculation. To preserve accuracy, we
control the additional variance by learning the sample ratio jointly with model
parameters during training. We assessed VCAS on multiple fine-tuning and
pre-training tasks in both vision and natural language domains. On all the
tasks, VCAS can preserve the original training loss trajectory and validation
accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction
of the whole training process. The implementation is available at
https://github.com/thu-ml/VCAS .</div><div><a href='http://arxiv.org/abs/2402.17227v1'>2402.17227v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08893v2")'>MADA: Meta-Adaptive Optimizers through hyper-gradient Descent</div>
<div id='2401.08893v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T00:16:46Z</div><div>Authors: Kaan Ozkara, Can Karakus, Parameswaran Raman, Mingyi Hong, Shoham Sabach, Branislav Kveton, Volkan Cevher</div><div style='padding-top: 10px; width: 80ex'>Since Adam was introduced, several novel adaptive optimizers for deep
learning have been proposed. These optimizers typically excel in some tasks but
may not outperform Adam uniformly across all tasks. In this work, we introduce
Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can
generalize several known optimizers and dynamically learn the most suitable one
during training. The key idea in MADA is to parameterize the space of
optimizers and search through it using hyper-gradient descent. We compare MADA
to other popular optimizers empirically on vision and language tasks to train
CNN, ResNet and GPT-2 models. Results suggest that MADA is robust against
sub-optimally tuned hyper-parameters, and consistently outperforms Adam and
other popular optimizers. We find that MADA gives $3\times$ the validation
performance gain over Adam that other popular optimizers do on GPT-2 training.
We also propose AVGrad, a modification of AMSGrad that replaces the maximum
operator with averaging, that is suitable for hyper-gradient optimization
framework. Finally, we provide a convergence analysis to show that
interpolation of optimizers can improve their error bounds (up to constants),
hinting at an advantage for meta-optimizers.</div><div><a href='http://arxiv.org/abs/2401.08893v2'>2401.08893v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09240v1")'>Switch EMA: A Free Lunch for Better Flatness and Sharpness</div>
<div id='2402.09240v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:28:42Z</div><div>Authors: Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Exponential Moving Average (EMA) is a widely used weight averaging (WA)
regularization to learn flat optima for better generalizations without extra
cost in deep neural network (DNN) optimization. Despite achieving better
flatness, existing WA methods might fall into worse final performances or
require extra test-time computations. This work unveils the full potential of
EMA with a single line of modification, i.e., switching the EMA parameters to
the original model after each epoch, dubbed as Switch EMA (SEMA). From both
theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to
reach generalization optima that better trade-off between flatness and
sharpness. To verify the effectiveness of SEMA, we conduct comparison
experiments with discriminative, generative, and regression tasks on vision and
language datasets, including image classification, self-supervised learning,
object detection and segmentation, image generation, video prediction,
attribute regression, and language modeling. Comprehensive results with popular
optimizers and networks show that SEMA is a free lunch for DNN training by
improving performances and boosting convergence speeds.</div><div><a href='http://arxiv.org/abs/2402.09240v1'>2402.09240v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10962v1")'>One Step Learning, One Step Review</div>
<div id='2401.10962v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T11:45:31Z</div><div>Authors: Xiaolong Huang, Qiankun Li, Xueran Li, Xuesong Gao</div><div style='padding-top: 10px; width: 80ex'>Visual fine-tuning has garnered significant attention with the rise of
pre-trained vision models. The current prevailing method, full fine-tuning,
suffers from the issue of knowledge forgetting as it focuses solely on fitting
the downstream training set. In this paper, we propose a novel weight
rollback-based fine-tuning method called OLOR (One step Learning, One step
Review). OLOR combines fine-tuning with optimizers, incorporating a weight
rollback term into the weight update term at each step. This ensures
consistency in the weight range of upstream and downstream models, effectively
mitigating knowledge forgetting and enhancing fine-tuning performance. In
addition, a layer-wise penalty is presented to employ penalty decay and the
diversified decay rate to adjust the weight rollback levels of layers for
adapting varying downstream tasks. Through extensive experiments on various
tasks such as image classification, object detection, semantic segmentation,
and instance segmentation, we demonstrate the general applicability and
state-of-the-art performance of our proposed OLOR. Code is available at
https://github.com/rainbow-xiao/OLOR-AAAI-2024.</div><div><a href='http://arxiv.org/abs/2401.10962v1'>2401.10962v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02242v2")'>Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey</div>
<div id='2402.02242v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T19:12:20Z</div><div>Authors: Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing Li, Yuntao Du</div><div style='padding-top: 10px; width: 80ex'>Large-scale pre-trained vision models (PVMs) have shown great potential for
adaptability across various downstream vision tasks. However, with
state-of-the-art PVMs growing to billions or even trillions of parameters, the
standard full fine-tuning paradigm is becoming unsustainable due to high
computational and storage demands. In response, researchers are exploring
parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance
of full fine-tuning with minimal parameter modifications. This survey provides
a comprehensive overview and future directions for visual PEFT, offering a
systematic review of the latest advancements. First, we provide a formal
definition of PEFT and discuss model pre-training methods. We then categorize
existing methods into three categories: addition-based, partial-based, and
unified-based. Finally, we introduce the commonly used datasets and
applications and suggest potential future research challenges. A comprehensive
collection of resources is available at
https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning.</div><div><a href='http://arxiv.org/abs/2402.02242v2'>2402.02242v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06382v1")'>Pre-Trained Model Recommendation for Downstream Fine-tuning</div>
<div id='2403.06382v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T02:24:32Z</div><div>Authors: Jiameng Bai, Sai Wu, Jie Song, Junbo Zhao, Gang Chen</div><div style='padding-top: 10px; width: 80ex'>As a fundamental problem in transfer learning, model selection aims to rank
off-the-shelf pre-trained models and select the most suitable one for the new
target task. Existing model selection techniques are often constrained in their
scope and tend to overlook the nuanced relationships between models and tasks.
In this paper, we present a pragmatic framework \textbf{Fennec}, delving into a
diverse, large-scale model repository while meticulously considering the
intricate connections between tasks and models. The key insight is to map all
models and historical tasks into a transfer-related subspace, where the
distance between model vectors and task vectors represents the magnitude of
transferability. A large vision model, as a proxy, infers a new task's
representation in the transfer space, thereby circumventing the computational
burden of extensive forward passes. We also investigate the impact of the
inherent inductive bias of models on transfer results and propose a novel
method called \textbf{archi2vec} to encode the intricate structures of models.
The transfer score is computed through straightforward vector arithmetic with a
time complexity of $\mathcal{O}(1)$. Finally, we make a substantial
contribution to the field by releasing a comprehensive benchmark. We validate
the effectiveness of our framework through rigorous testing on two benchmarks.
The benchmark and the code will be publicly available in the near future.</div><div><a href='http://arxiv.org/abs/2403.06382v1'>2403.06382v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16828v1")'>Training Neural Networks from Scratch with Parallel Low-Rank Adapters</div>
<div id='2402.16828v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:55:13Z</div><div>Authors: Minyoung Huh, Brian Cheung, Jeremy Bernstein, Phillip Isola, Pulkit Agrawal</div><div style='padding-top: 10px; width: 80ex'>The scalability of deep learning models is fundamentally limited by computing
resources, memory, and communication. Although methods like low-rank adaptation
(LoRA) have reduced the cost of model finetuning, its application in model
pre-training remains largely unexplored. This paper explores extending LoRA to
model pre-training, identifying the inherent constraints and limitations of
standard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel
bi-level optimization algorithm designed to enable parallel training of
multiple low-rank heads across computing nodes, thereby reducing the need for
frequent synchronization. Our approach includes extensive experimentation on
vision transformers using various vision datasets, demonstrating that LTE is
competitive with standard pre-training.</div><div><a href='http://arxiv.org/abs/2402.16828v1'>2402.16828v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12418v1")'>Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural
  Architectures</div>
<div id='2402.12418v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:52:45Z</div><div>Authors: Akash Guna R. T, Arnav Chavan, Deepak Gupta</div><div style='padding-top: 10px; width: 80ex'>Conventional scaling of neural networks typically involves designing a base
network and growing different dimensions like width, depth, etc. of the same by
some predefined scaling factors. We introduce an automated scaling approach
leveraging second-order loss landscape information. Our method is flexible
towards skip connections a mainstay in modern vision transformers. Our
training-aware method jointly scales and trains transformers without additional
training iterations. Motivated by the hypothesis that not all neurons need
uniform depth complexity, our approach embraces depth heterogeneity. Extensive
evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10%
parameter efficiency improvement over conventional scaling. Scaled networks
demonstrate superior performance upon training small scale datasets from
scratch. We introduce the first intact scaling mechanism for vision
transformers, a step towards efficient model scaling.</div><div><a href='http://arxiv.org/abs/2402.12418v1'>2402.12418v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03752v1")'>Pre-training of Lightweight Vision Transformers on Small Datasets with
  Minimally Scaled Images</div>
<div id='2402.03752v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:41:24Z</div><div>Authors: Jen Hong Tan</div><div style='padding-top: 10px; width: 80ex'>Can a lightweight Vision Transformer (ViT) match or exceed the performance of
Convolutional Neural Networks (CNNs) like ResNet on small datasets with small
image resolutions? This report demonstrates that a pure ViT can indeed achieve
superior performance through pre-training, using a masked auto-encoder
technique with minimal image scaling. Our experiments on the CIFAR-10 and
CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters
and a multiply-accumulate (MAC) count below 0.27G, qualifying them as
'lightweight' models. Unlike previous approaches, our method attains
state-of-the-art performance among similar lightweight transformer-based
architectures without significantly scaling up images from CIFAR-10 and
CIFAR-100. This achievement underscores the efficiency of our model, not only
in handling small datasets but also in effectively processing images close to
their original scale.</div><div><a href='http://arxiv.org/abs/2402.03752v1'>2402.03752v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13196v1")'>ADAPT to Robustify Prompt Tuning Vision Transformers</div>
<div id='2403.13196v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T23:13:40Z</div><div>Authors: Masih Eskandar, Tooba Imtiaz, Zifeng Wang, Jennifer Dy</div><div style='padding-top: 10px; width: 80ex'>The performance of deep models, including Vision Transformers, is known to be
vulnerable to adversarial attacks. Many existing defenses against these
attacks, such as adversarial training, rely on full-model fine-tuning to induce
robustness in the models. These defenses require storing a copy of the entire
model, that can have billions of parameters, for each task. At the same time,
parameter-efficient prompt tuning is used to adapt large transformer-based
models to downstream tasks without the need to save large copies. In this
paper, we examine parameter-efficient prompt tuning of Vision Transformers for
downstream tasks under the lens of robustness. We show that previous
adversarial defense methods, when applied to the prompt tuning paradigm, suffer
from gradient obfuscation and are vulnerable to adaptive attacks. We introduce
ADAPT, a novel framework for performing adaptive adversarial training in the
prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40%
w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1%
of the number of parameters.</div><div><a href='http://arxiv.org/abs/2403.13196v1'>2403.13196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11953v1")'>Stealing the Invisible: Unveiling Pre-Trained CNN Models through
  Adversarial Examples and Timing Side-Channels</div>
<div id='2402.11953v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:47:20Z</div><div>Authors: Shubhi Shukla, Manaar Alam, Pabitra Mitra, Debdeep Mukhopadhyay</div><div style='padding-top: 10px; width: 80ex'>Machine learning, with its myriad applications, has become an integral
component of numerous technological systems. A common practice in this domain
is the use of transfer learning, where a pre-trained model's architecture,
readily available to the public, is fine-tuned to suit specific tasks. As
Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained
models in their backends, it's crucial to safeguard these architectures and
understand their vulnerabilities. In this work, we present an approach based on
the observation that the classification patterns of adversarial images can be
used as a means to steal the models. Furthermore, the adversarial image
classifications in conjunction with timing side channels can lead to a model
stealing method. Our approach, designed for typical user-level access in remote
MLaaS environments exploits varying misclassifications of adversarial images
across different models to fingerprint several renowned Convolutional Neural
Network (CNN) and Vision Transformer (ViT) architectures. We utilize the
profiling of remote model inference times to reduce the necessary adversarial
images, subsequently decreasing the number of queries required. We have
presented our results over 27 pre-trained models of different CNN and ViT
architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8%
while keeping the query budget under 20.</div><div><a href='http://arxiv.org/abs/2402.11953v1'>2402.11953v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16766v1")'>Detection and Recovery Against Deep Neural Network Fault Injection
  Attacks Based on Contrastive Learning</div>
<div id='2401.16766v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:06:57Z</div><div>Authors: Chenan Wang, Pu Zhao, Siyue Wang, Xue Lin</div><div style='padding-top: 10px; width: 80ex'>Deep Neural Network (DNN) models when implemented on executing devices as the
inference engines are susceptible to Fault Injection Attacks (FIAs) that
manipulate model parameters to disrupt inference execution with disastrous
performance. This work introduces Contrastive Learning (CL) of visual
representations i.e., a self-supervised learning approach into the deep
learning training and inference pipeline to implement DNN inference engines
with self-resilience under FIAs. Our proposed CL based FIA Detection and
Recovery (CFDR) framework features (i) real-time detection with only a single
batch of testing data and (ii) fast recovery effective even with only a small
amount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on
multiple types of FIAs, our CFDR shows promising detection and recovery
effectiveness.</div><div><a href='http://arxiv.org/abs/2401.16766v1'>2401.16766v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08996v1")'>MicroNAS: Zero-Shot Neural Architecture Search for MCUs</div>
<div id='2401.08996v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T06:17:42Z</div><div>Authors: Ye Qiao, Haocheng Xu, Yifan Zhang, Sitao Huang</div><div style='padding-top: 10px; width: 80ex'>Neural Architecture Search (NAS) effectively discovers new Convolutional
Neural Network (CNN) architectures, particularly for accuracy optimization.
However, prior approaches often require resource-intensive training on super
networks or extensive architecture evaluations, limiting practical
applications. To address these challenges, we propose MicroNAS, a
hardware-aware zero-shot NAS framework designed for microcontroller units
(MCUs) in edge computing. MicroNAS considers target hardware optimality during
the search, utilizing specialized performance indicators to identify optimal
neural architectures without high computational costs. Compared to previous
works, MicroNAS achieves up to 1104x improvement in search efficiency and
discovers models with over 3.23x faster MCU inference while maintaining similar
accuracy</div><div><a href='http://arxiv.org/abs/2401.08996v1'>2401.08996v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13525v1")'>MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via
  Automating Deep Neural Network Porting for Mobile Deployment</div>
<div id='2402.13525v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T04:43:12Z</div><div>Authors: Hongtao Huang, Xiaojun Chang, Wen Hu, Lina Yao</div><div style='padding-top: 10px; width: 80ex'>Recent years have seen the explosion of edge intelligence with powerful Deep
Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud
servers and subsequently porting them to mobile devices after being
lightweight. Conventional approaches manually specialized DNNs for various edge
platforms and retrain them with real-world data. However, as the number of
platforms increases, these approaches become labour-intensive and
computationally prohibitive. Additionally, real-world data tends to be
sparse-label, further increasing the difficulty of lightweight models. In this
paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices.
Specifically, we simultaneously optimise a large network family using both
labelled and unlabelled data and then automatically search for tailored
networks for different hardware platforms. MatchNAS acts as an intermediary
that bridges the gap between cloud-based DNNs and edge-based DNNs.</div><div><a href='http://arxiv.org/abs/2402.13525v1'>2402.13525v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10541v1")'>I-SplitEE: Image classification in Split Computing DNNs with Early Exits</div>
<div id='2401.10541v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T07:44:32Z</div><div>Authors: Divya Jyoti Bajpai, Aastha Jaiswal, Manjesh Kumar Hanawal</div><div style='padding-top: 10px; width: 80ex'>The recent advances in Deep Neural Networks (DNNs) stem from their
exceptional performance across various domains. However, their inherent large
size hinders deploying these networks on resource-constrained devices like
edge, mobile, and IoT platforms. Strategies have emerged, from partial cloud
computation offloading (split computing) to integrating early exits within DNN
layers. Our work presents an innovative unified approach merging early exits
and split computing. We determine the 'splitting layer', the optimal depth in
the DNN for edge device computations, and whether to infer on edge device or be
offloaded to the cloud for inference considering accuracy, computational
efficiency, and communication costs. Also, Image classification faces diverse
environmental distortions, influenced by factors like time of day, lighting,
and weather. To adapt to these distortions, we introduce I-SplitEE, an online
unsupervised algorithm ideal for scenarios lacking ground truths and with
sequential data. Experimental validation using Caltech-256 and Cifar-10
datasets subjected to varied distortions showcases I-SplitEE's ability to
reduce costs by a minimum of 55% with marginal performance degradation of at
most 5%.</div><div><a href='http://arxiv.org/abs/2401.10541v1'>2401.10541v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12562v1")'>Equity through Access: A Case for Small-scale Deep Learning</div>
<div id='2403.12562v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T09:17:18Z</div><div>Authors: Raghavendra Selvan, Bob Pepin, Christian Igel, Gabrielle Samuel, Erik B Dam</div><div style='padding-top: 10px; width: 80ex'>The recent advances in deep learning (DL) have been accelerated by access to
large-scale data and compute. These large-scale resources have been used to
train progressively larger models which are resource intensive in terms of
compute, data, energy, and carbon emissions. These costs are becoming a new
type of entry barrier to researchers and practitioners with limited access to
resources at such scale, particularly in the Global South. In this work, we
take a comprehensive look at the landscape of existing DL models for vision
tasks and demonstrate their usefulness in settings where resources are limited.
To account for the resource consumption of DL models, we introduce a novel
measure to estimate the performance per resource unit, which we call the PePR
score. Using a diverse family of 131 unique DL architectures (spanning 1M to
130M trainable parameters) and three medical image datasets, we capture trends
about the performance-resource trade-offs. In applications like medical image
analysis, we argue that small-scale, specialized models are better than
striving for large-scale models. Furthermore, we show that using pretrained
models can significantly reduce the computational resources and data required.
We hope this work will encourage the community to focus on improving AI equity
by developing methods and models with smaller resource footprints.</div><div><a href='http://arxiv.org/abs/2403.12562v1'>2403.12562v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06797v1")'>Leveraging Internal Representations of Model for Magnetic Image
  Classification</div>
<div id='2403.06797v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:15:50Z</div><div>Authors: Adarsh N L, Arun P V, Alok Porwal, Malcolm Aranha</div><div style='padding-top: 10px; width: 80ex'>Data generated by edge devices has the potential to train intelligent
autonomous systems across various domains. Despite the emergence of diverse
machine learning approaches addressing privacy concerns and utilizing
distributed data, security issues persist due to the sensitive storage of data
shards in disparate locations. This paper introduces a potentially
groundbreaking paradigm for machine learning model training, specifically
designed for scenarios with only a single magnetic image and its corresponding
label image available. We harness the capabilities of Deep Learning to generate
concise yet informative samples, aiming to overcome data scarcity. Through the
utilization of deep learning's internal representations, our objective is to
efficiently address data scarcity issues and produce meaningful results. This
methodology presents a promising avenue for training machine learning models
with minimal data.</div><div><a href='http://arxiv.org/abs/2403.06797v1'>2403.06797v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15150v1")'>An In-Depth Analysis of Data Reduction Methods for Sustainable Deep
  Learning</div>
<div id='2403.15150v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T12:06:40Z</div><div>Authors: V√≠ctor Toscano-Dur√°n, Javier Perera-Lago, Eduardo Paluzo-Hidalgo, Roc√≠o Gonzalez-Diaz, Miguel √Ångel Gutierrez-Naranjo, Matteo Rucco</div><div style='padding-top: 10px; width: 80ex'>In recent years, Deep Learning has gained popularity for its ability to solve
complex classification tasks, increasingly delivering better results thanks to
the development of more accurate models, the availability of huge volumes of
data and the improved computational capabilities of modern computers. However,
these improvements in performance also bring efficiency problems, related to
the storage of datasets and models, and to the waste of energy and time
involved in both the training and inference processes. In this context, data
reduction can help reduce energy consumption when training a deep learning
model. In this paper, we present up to eight different methods to reduce the
size of a tabular training dataset, and we develop a Python package to apply
them. We also introduce a representativeness metric based on topology to
measure how similar are the reduced datasets and the full training dataset.
Additionally, we develop a methodology to apply these data reduction methods to
image datasets for object detection tasks. Finally, we experimentally compare
how these data reduction methods affect the representativeness of the reduced
dataset, the energy consumption and the predictive performance of the model.</div><div><a href='http://arxiv.org/abs/2403.15150v1'>2403.15150v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07036v1")'>A Converting Autoencoder Toward Low-latency and Energy-efficient DNN
  Inference at the Edge</div>
<div id='2403.07036v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T08:13:42Z</div><div>Authors: Hasanul Mahmud, Peng Kang, Kevin Desai, Palden Lama, Sushil Prasad</div><div style='padding-top: 10px; width: 80ex'>Reducing inference time and energy usage while maintaining prediction
accuracy has become a significant concern for deep neural networks (DNN)
inference on resource-constrained edge devices. To address this problem, we
propose a novel approach based on "converting" autoencoder and lightweight
DNNs. This improves upon recent work such as early-exiting framework and DNN
partitioning. Early-exiting frameworks spend different amounts of computation
power for different input data depending upon their complexity. However, they
can be inefficient in real-world scenarios that deal with many hard image
samples. On the other hand, DNN partitioning algorithms that utilize the
computation power of both the cloud and edge devices can be affected by network
delays and intermittent connections between the cloud and the edge. We present
CBNet, a low-latency and energy-efficient DNN inference framework tailored for
edge devices. It utilizes a "converting" autoencoder to efficiently transform
hard images into easy ones, which are subsequently processed by a lightweight
DNN for inference. To the best of our knowledge, such autoencoder has not been
proposed earlier. Our experimental results using three popular
image-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and
an instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x
speedup in inference latency and 79% reduction in energy usage compared to
competing techniques while maintaining similar or higher accuracy.</div><div><a href='http://arxiv.org/abs/2403.07036v1'>2403.07036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12350v1")'>Scaling Up Quantization-Aware Neural Architecture Search for Efficient
  Deep Learning on the Edge</div>
<div id='2401.12350v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T20:32:31Z</div><div>Authors: Yao Lu, Hiram Rayo Torres Rodriguez, Sebastian Vogel, Nick van de Waterlaat, Pavol Jancura</div><div style='padding-top: 10px; width: 80ex'>Neural Architecture Search (NAS) has become the de-facto approach for
designing accurate and efficient networks for edge devices. Since models are
typically quantized for edge deployment, recent work has investigated
quantization-aware NAS (QA-NAS) to search for highly accurate and efficient
quantized models. However, existing QA-NAS approaches, particularly few-bit
mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently,
QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this
work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale
tasks by leveraging the block-wise formulation introduced by block-wise NAS. We
demonstrate strong results for the semantic segmentation task on the Cityscapes
dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than
DeepLabV3 (INT8) without compromising task performance.</div><div><a href='http://arxiv.org/abs/2401.12350v1'>2401.12350v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11925v1")'>Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching</div>
<div id='2402.11925v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:12:47Z</div><div>Authors: Sujin Kook, Won-Yong Shin, Seong-Lyun Kim, Seung-Woo Ko</div><div style='padding-top: 10px; width: 80ex'>The vision of pervasive artificial intelligence (AI) services can be realized
by training an AI model on time using real-time data collected by internet of
things (IoT) devices. To this end, IoT devices require offloading their data to
an edge server in proximity. However, transmitting high-dimensional and
voluminous data from energy-constrained IoT devices poses a significant
challenge. To address this limitation, we propose a novel offloading
architecture, called joint data deepening-and-prefetching (JD2P), which is
feature-by-feature offloading comprising two key techniques. The first one is
data deepening, where each data sample's features are sequentially offloaded in
the order of importance determined by the data embedding technique such as
principle component analysis (PCA). Offloading is terminated once the already
transmitted features are sufficient for accurate data classification, resulting
in a reduction in the amount of transmitted data. The criteria to offload data
are derived for binary and multi-class classifiers, which are designed based on
support vector machine (SVM) and deep neural network (DNN), respectively. The
second one is data prefetching, where some features potentially required in the
future are offloaded in advance, thus achieving high efficiency via precise
prediction and parameter optimization. We evaluate the effectiveness of JD2P
through experiments using the MNIST dataset, and the results demonstrate its
significant reduction in expected energy consumption compared to several
benchmarks without degrading learning accuracy.</div><div><a href='http://arxiv.org/abs/2402.11925v1'>2402.11925v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10476v1")'>Approximate Nullspace Augmented Finetuning for Robust Vision
  Transformers</div>
<div id='2403.10476v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:07:39Z</div><div>Authors: Haoyang Liu, Aditya Singh, Yijiang Li, Haohan Wang</div><div style='padding-top: 10px; width: 80ex'>Enhancing the robustness of deep learning models, particularly in the realm
of vision transformers (ViTs), is crucial for their real-world deployment. In
this work, we provide a finetuning approach to enhance the robustness of vision
transformers inspired by the concept of nullspace from linear algebra. Our
investigation centers on whether a vision transformer can exhibit resilience to
input variations akin to the nullspace property in linear mappings, implying
that perturbations sampled from this nullspace do not influence the model's
output when added to the input. Firstly, we show that for many pretrained ViTs,
a non-trivial nullspace exists due to the presence of the patch embedding
layer. Secondly, as nullspace is a concept associated with linear algebra, we
demonstrate that it is possible to synthesize approximate nullspace elements
for the non-linear blocks of ViTs employing an optimisation strategy. Finally,
we propose a fine-tuning strategy for ViTs wherein we augment the training data
with synthesized approximate nullspace noise. After finetuning, we find that
the model demonstrates robustness to adversarial and natural image perbutations
alike.</div><div><a href='http://arxiv.org/abs/2403.10476v1'>2403.10476v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09612v1")'>Compute-first optical detection for noise-resilient visual perception</div>
<div id='2403.09612v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:51:38Z</div><div>Authors: Jungmin Kim, Nanfang Yu, Zongfu Yu</div><div style='padding-top: 10px; width: 80ex'>In the context of visual perception, the optical signal from a scene is
transferred into the electronic domain by detectors in the form of image data,
which are then processed for the extraction of visual information. In noisy and
weak-signal environments such as thermal imaging for night vision applications,
however, the performance of neural computing tasks faces a significant
bottleneck due to the inherent degradation of data quality upon noisy
detection. Here, we propose a concept of optical signal processing before
detection to address this issue. We demonstrate that spatially redistributing
optical signals through a properly designed linear transformer can enhance the
detection noise resilience of visual perception tasks, as benchmarked with the
MNIST classification. Our idea is supported by a quantitative analysis
detailing the relationship between signal concentration and noise robustness,
as well as its practical implementation in an incoherent imaging system. This
compute-first detection scheme can pave the way for advancing infrared machine
vision technologies widely used for industrial and defense applications.</div><div><a href='http://arxiv.org/abs/2403.09612v1'>2403.09612v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16908v2")'>Lightweight, error-tolerant edge detection using memristor-enabled
  stochastic logics</div>
<div id='2402.16908v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T06:23:02Z</div><div>Authors: Lekai Song, Pengyu Liu, Jingfang Pei, Yang Liu, Songwei Liu, Shengbo Wang, Leonard W. T. Ng, Tawfique Hasan, Kong-Pang Pun, Shuo Gao, Guohua Hu</div><div style='padding-top: 10px; width: 80ex'>The demand for efficient edge vision has spurred the interest in developing
stochastic computing approaches for performing image processing tasks.
Memristors with inherent stochasticity readily introduce probability into the
computations and thus enable stochastic image processing computations. Here, we
present a stochastic computing approach for edge detection, a fundamental image
processing technique, facilitated with memristor-enabled stochastic logics.
Specifically, we integrate the memristors with logic circuits and harness the
stochasticity from the memristors to realize compact stochastic logics for
stochastic number encoding and processing. The stochastic numbers, exhibiting
well-regulated probabilities and correlations, can be processed to perform
logic operations with statistical probabilities. This can facilitate
lightweight stochastic edge detection for edge visual scenarios characterized
with high-level noise errors. As a practical demonstration, we implement a
hardware stochastic Roberts cross operator using the stochastic logics, and
prove its exceptional edge detection performance, remarkably, with 95% less
computational cost while withstanding 50% bit-flip errors. The results
underscore the great potential of our stochastic edge detection approach in
developing lightweight, error-tolerant edge vision hardware and systems for
autonomous driving, virtual/augmented reality, medical imaging diagnosis,
industrial automation, and beyond.</div><div><a href='http://arxiv.org/abs/2402.16908v2'>2402.16908v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03317v1")'>SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular
  Value Penalization</div>
<div id='2402.03317v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T14:27:24Z</div><div>Authors: Xixu Hu, Runkai Zheng, Jindong Wang, Cheuk Hang Leung, Qi Wu, Xing Xie</div><div style='padding-top: 10px; width: 80ex'>Vision Transformers (ViTs) have gained prominence as a preferred choice for a
wide range of computer vision tasks due to their exceptional performance.
However, their widespread adoption has raised concerns about security in the
face of malicious attacks. Most existing methods rely on empirical adjustments
during the training process, lacking a clear theoretical foundation. In this
study, we address this gap by introducing SpecFormer, specifically designed to
enhance ViTs' resilience against adversarial attacks, with support from
carefully derived theoretical guarantees. We establish local Lipschitz bounds
for the self-attention layer and introduce a novel approach, Maximum Singular
Value Penalization (MSVP), to attain precise control over these bounds. We
seamlessly integrate MSVP into ViTs' attention layers, using the power
iteration method for enhanced computational efficiency. The modified model,
SpecFormer, effectively reduces the spectral norms of attention weight
matrices, thereby enhancing network local Lipschitzness. This, in turn, leads
to improved training efficiency and robustness. Extensive experiments on CIFAR
and ImageNet datasets confirm SpecFormer's superior performance in defending
against adversarial attacks.</div><div><a href='http://arxiv.org/abs/2402.03317v1'>2402.03317v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11391v1")'>Investigating the Benefits of Projection Head for Representation
  Learning</div>
<div id='2403.11391v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T00:48:58Z</div><div>Authors: Yihao Xue, Eric Gan, Jiayi Ni, Siddharth Joshi, Baharan Mirzasoleiman</div><div style='padding-top: 10px; width: 80ex'>An effective technique for obtaining high-quality representations is adding a
projection head on top of the encoder during training, then discarding it and
using the pre-projection representations. Despite its proven practical
effectiveness, the reason behind the success of this technique is poorly
understood. The pre-projection representations are not directly optimized by
the loss function, raising the question: what makes them better? In this work,
we provide a rigorous theoretical answer to this question. We start by
examining linear models trained with self-supervised contrastive loss. We
reveal that the implicit bias of training algorithms leads to layer-wise
progressive feature weighting, where features become increasingly unequal as we
go deeper into the layers. Consequently, lower layers tend to have more
normalized and less specialized representations. We theoretically characterize
scenarios where such representations are more beneficial, highlighting the
intricate interplay between data augmentation and input features. Additionally,
we demonstrate that introducing non-linearity into the network allows lower
layers to learn features that are completely absent in higher layers. Finally,
we show how this mechanism improves the robustness in supervised contrastive
learning and supervised learning. We empirically validate our results through
various experiments on CIFAR-10/100, UrbanCars and shifted versions of
ImageNet. We also introduce a potential alternative to projection head, which
offers a more interpretable and controllable design.</div><div><a href='http://arxiv.org/abs/2403.11391v1'>2403.11391v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15194v1")'>Your Image is My Video: Reshaping the Receptive Field via Image-To-Video
  Differentiable AutoAugmentation and Fusion</div>
<div id='2403.15194v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T13:27:57Z</div><div>Authors: Sofia Casarin, Cynthia I. Ugwu, Sergio Escalera, Oswald Lanz</div><div style='padding-top: 10px; width: 80ex'>The landscape of deep learning research is moving towards innovative
strategies to harness the true potential of data. Traditionally, emphasis has
been on scaling model architectures, resulting in large and complex neural
networks, which can be difficult to train with limited computational resources.
However, independently of the model size, data quality (i.e. amount and
variability) is still a major factor that affects model generalization. In this
work, we propose a novel technique to exploit available data through the use of
automatic data augmentation for the tasks of image classification and semantic
segmentation. We introduce the first Differentiable Augmentation Search method
(DAS) to generate variations of images that can be processed as videos.
Compared to previous approaches, DAS is extremely fast and flexible, allowing
the search on very large search spaces in less than a GPU day. Our intuition is
that the increased receptive field in the temporal dimension provided by DAS
could lead to benefits also to the spatial receptive field. More specifically,
we leverage DAS to guide the reshaping of the spatial receptive field by
selecting task-dependant transformations. As a result, compared to standard
augmentation alternatives, we improve in terms of accuracy on ImageNet,
Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when
plugging-in our DAS over different light-weight video backbones.</div><div><a href='http://arxiv.org/abs/2403.15194v1'>2403.15194v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15490v2")'>A Comprehensive Survey of Convolutions in Deep Learning: Applications,
  Challenges, and Future Trends</div>
<div id='2402.15490v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:28:57Z</div><div>Authors: Abolfazl Younesi, Mohsen Ansari, MohammadAmin Fazli, Alireza Ejlali, Muhammad Shafique, J√∂rg Henkel</div><div style='padding-top: 10px; width: 80ex'>In today's digital age, Convolutional Neural Networks (CNNs), a subset of
Deep Learning (DL), are widely used for various computer vision tasks such as
image classification, object detection, and image segmentation. There are
numerous types of CNNs designed to meet specific needs and requirements,
including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention,
depthwise convolutions, and NAS, among others. Each type of CNN has its unique
structure and characteristics, making it suitable for specific tasks. It's
crucial to gain a thorough understanding and perform a comparative analysis of
these different CNN types to understand their strengths and weaknesses.
Furthermore, studying the performance, limitations, and practical applications
of each type of CNN can aid in the development of new and improved
architectures in the future. We also dive into the platforms and frameworks
that researchers utilize for their research or development from various
perspectives. Additionally, we explore the main research fields of CNN like 6D
vision, generative models, and meta-learning. This survey paper provides a
comprehensive examination and comparison of various CNN architectures,
highlighting their architectural differences and emphasizing their respective
advantages, disadvantages, applications, challenges, and future trends.</div><div><a href='http://arxiv.org/abs/2402.15490v2'>2402.15490v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02941v1")'>Exploring the Synergies of Hybrid CNNs and ViTs Architectures for
  Computer Vision: A survey</div>
<div id='2402.02941v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:08:17Z</div><div>Authors: Haruna Yunusa, Shiyin Qin, Abdulrahman Hamman Adama Chukkol, Abdulganiyu Abdu Yusuf, Isah Bello, Adamu Lawan</div><div style='padding-top: 10px; width: 80ex'>The hybrid of Convolutional Neural Network (CNN) and Vision Transformers
(ViT) architectures has emerged as a groundbreaking approach, pushing the
boundaries of computer vision (CV). This comprehensive review provides a
thorough examination of the literature on state-of-the-art hybrid CNN-ViT
architectures, exploring the synergies between these two approaches. The main
content of this survey includes: (1) a background on the vanilla CNN and ViT,
(2) systematic review of various taxonomic hybrid designs to explore the
synergy achieved through merging CNNs and ViTs models, (3) comparative analysis
and application task-specific synergy between different hybrid architectures,
(4) challenges and future directions for hybrid models, (5) lastly, the survey
concludes with a summary of key findings and recommendations. Through this
exploration of hybrid CV architectures, the survey aims to serve as a guiding
resource, fostering a deeper understanding of the intricate dynamics between
CNNs and ViTs and their collective impact on shaping the future of CV
architectures.</div><div><a href='http://arxiv.org/abs/2402.02941v1'>2402.02941v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17231v1")'>ReAlnet: Achieving More Human Brain-Like Vision via Human Neural
  Representational Alignment</div>
<div id='2401.17231v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T18:18:41Z</div><div>Authors: Zitong Lu, Yile Wang, Julie D. Golomb</div><div style='padding-top: 10px; width: 80ex'>Despite the remarkable strides made in artificial intelligence, current
object recognition models still lag behind in emulating the mechanism of visual
information processing in human brains. Recent studies have highlighted the
potential of using neural data to mimic brain processing; however, these often
reply on invasive neural recordings from non-human subjects, leaving a critical
gap in our understanding of human visual perception and the development of more
human brain-like vision models. Addressing this gap, we present, for the first
time, "Re(presentational)Al(ignment)net", a vision model aligned with human
brain activity based on non-invasive EEG recordings, demonstrating a
significantly higher similarity to human brain representations. Our innovative
image-to-brain multi-layer encoding alignment framework not only optimizes
multiple layers of the model, marking a substantial leap in neural alignment,
but also enables the model to efficiently learn and mimic human brain's visual
representational patterns across object categories and different neural data
modalities. Furthermore, we discover that alignment with human brain
representations improves the model's adversarial robustness. Our findings
suggest that ReAlnet sets a new precedent in the field, bridging the gap
between artificial and human vision, and paving the way for more brain-like
artificial intelligence systems.</div><div><a href='http://arxiv.org/abs/2401.17231v1'>2401.17231v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02020v1")'>Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN
  Ticket</div>
<div id='2401.02020v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T01:33:33Z</div><div>Authors: Zhaokun Zhou, Kaiwei Che, Wei Fang, Keyu Tian, Yuesheng Zhu, Shuicheng Yan, Yonghong Tian, Li Yuan</div><div style='padding-top: 10px; width: 80ex'>Spiking Neural Networks (SNNs), known for their biologically plausible
architecture, face the challenge of limited performance. The self-attention
mechanism, which is the cornerstone of the high-performance Transformer and
also a biologically inspired structure, is absent in existing SNNs. To this
end, we explore the potential of leveraging both self-attention capability and
biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)
and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for
softmax and captures the sparse visual feature employing spike-based Query,
Key, and Value. This sparse computation without multiplication makes SSA
efficient and energy-saving. Further, we develop a Spiking Convolutional Stem
(SCS) with supplementary convolutional layers to enhance the architecture of
Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer
V2. To train larger and deeper Spikformer V2, we introduce a pioneering
exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we
pre-train Spikformer V2 with masking and reconstruction style inspired by the
mainstream self-supervised Transformer, and then finetune the Spikformer V2 on
the image classification on ImageNet. Extensive experiments show that
Spikformer V2 outperforms other previous surrogate training and ANN2SNN
methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time
steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of
81.10% with just 1 time step. To the best of our knowledge, this is the first
time that the SNN achieves 80+% accuracy on ImageNet. The code will be
available at Spikformer V2.</div><div><a href='http://arxiv.org/abs/2401.02020v1'>2401.02020v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14302v1")'>SpikingResformer: Bridging ResNet and Vision Transformer in Spiking
  Neural Networks</div>
<div id='2403.14302v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T11:16:42Z</div><div>Authors: Xinyu Shi, Zecheng Hao, Zhaofei Yu</div><div style='padding-top: 10px; width: 80ex'>The remarkable success of Vision Transformers in Artificial Neural Networks
(ANNs) has led to a growing interest in incorporating the self-attention
mechanism and transformer-based architecture into Spiking Neural Networks
(SNNs). While existing methods propose spiking self-attention mechanisms that
are compatible with SNNs, they lack reasonable scaling methods, and the overall
architectures proposed by these methods suffer from a bottleneck in effectively
extracting local features. To address these challenges, we propose a novel
spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a
reasonable scaling method. Based on DSSA, we propose a novel spiking Vision
Transformer architecture called SpikingResformer, which combines the
ResNet-based multi-stage architecture with our proposed DSSA to improve both
performance and energy efficiency while reducing parameters. Experimental
results show that SpikingResformer achieves higher accuracy with fewer
parameters and lower energy consumption than other spiking Vision Transformer
counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on
ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN
field.</div><div><a href='http://arxiv.org/abs/2403.14302v1'>2403.14302v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10843v1")'>Training a General Spiking Neural Network with Improved Efficiency and
  Minimum Latency</div>
<div id='2401.10843v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T09:54:44Z</div><div>Authors: Yunpeng Yao, Man Wu, Zheng Chen, Renyuan Zhang</div><div style='padding-top: 10px; width: 80ex'>Spiking Neural Networks (SNNs) that operate in an event-driven manner and
employ binary spike representation have recently emerged as promising
candidates for energy-efficient computing. However, a cost bottleneck arises in
obtaining high-performance SNNs: training a SNN model requires a large number
of time steps in addition to the usual learning iterations, hence this limits
their energy efficiency. This paper proposes a general training framework that
enhances feature learning and activation efficiency within a limited time step,
providing a new solution for more energy-efficient SNNs. Our framework allows
SNN neurons to learn robust spike feature from different receptive fields and
update neuron states by utilizing both current stimuli and recurrence
information transmitted from other neurons. This setting continuously
complements information within a single time step. Additionally, we propose a
projection function to merge these two stimuli to smoothly optimize neuron
weights (spike firing threshold and activation). We evaluate the proposal for
both convolution and recurrent models. Our experimental results indicate
state-of-the-art visual classification tasks, including CIFAR10, CIFAR100, and
TinyImageNet.Our framework achieves 72.41% and 72.31% top-1 accuracy with only
1 time step on CIFAR100 for CNNs and RNNs, respectively. Our method reduces 10x
and 3x joule energy than a standard ANN and SNN, respectively, on CIFAR10,
without additional time steps.</div><div><a href='http://arxiv.org/abs/2401.10843v1'>2401.10843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09109v1")'>Stochastic Spiking Attention: Accelerating Attention with Stochastic
  Computing in Spiking Networks</div>
<div id='2402.09109v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T11:47:19Z</div><div>Authors: Zihang Song, Prabodh Katti, Osvaldo Simeone, Bipin Rajendran</div><div style='padding-top: 10px; width: 80ex'>Spiking Neural Networks (SNNs) have been recently integrated into Transformer
architectures due to their potential to reduce computational demands and to
improve power efficiency. Yet, the implementation of the attention mechanism
using spiking signals on general-purpose computing platforms remains
inefficient. In this paper, we propose a novel framework leveraging stochastic
computing (SC) to effectively execute the dot-product attention for SNN-based
Transformers. We demonstrate that our approach can achieve high classification
accuracy ($83.53\%$) on CIFAR-10 within 10 time steps, which is comparable to
the performance of a baseline artificial neural network implementation
($83.66\%$). We estimate that the proposed SC approach can lead to over
$6.3\times$ reduction in computing energy and $1.7\times$ reduction in memory
access costs for a digital CMOS-based ASIC design. We experimentally validate
our stochastic attention block design through an FPGA implementation, which is
shown to achieve $48\times$ lower latency as compared to a GPU implementation,
while consuming $15\times$ less power.</div><div><a href='http://arxiv.org/abs/2402.09109v1'>2402.09109v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18994v1")'>Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural
  Networks</div>
<div id='2402.18994v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T09:46:44Z</div><div>Authors: Kade M. Heckel, Thomas Nowotny</div><div style='padding-top: 10px; width: 80ex'>As the role of artificial intelligence becomes increasingly pivotal in modern
society, the efficient training and deployment of deep neural networks have
emerged as critical areas of focus. Recent advancements in attention-based
large neural architectures have spurred the development of AI accelerators,
facilitating the training of extensive, multi-billion parameter models. Despite
their effectiveness, these powerful networks often incur high execution costs
in production environments. Neuromorphic computing, inspired by biological
neural processes, offers a promising alternative. By utilizing
temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance
energy efficiency through a reduced and low-power hardware footprint. However,
the training of SNNs can be challenging due to their recurrent nature which
cannot as easily leverage the massive parallelism of modern AI accelerators. To
facilitate the investigation of SNN architectures and dynamics researchers have
sought to bridge Python-based deep learning frameworks such as PyTorch or
TensorFlow with custom-implemented compute kernels. This paper introduces Spyx,
a new and lightweight SNN simulation and optimization library designed in JAX.
By pre-staging data in the expansive vRAM of contemporary accelerators and
employing extensive JIT compilation, Spyx allows for SNN optimization to be
executed as a unified, low-level program on NVIDIA GPUs or Google TPUs. This
approach achieves optimal hardware utilization, surpassing the performance of
many existing SNN training frameworks while maintaining considerable
flexibility.</div><div><a href='http://arxiv.org/abs/2402.18994v1'>2402.18994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11322v1")'>SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for
  Spiking Neural Network Systems</div>
<div id='2402.11322v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T16:33:54Z</div><div>Authors: Rachmad Vidya Wicaksana Putra, Muhammad Shafique</div><div style='padding-top: 10px; width: 80ex'>Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra
low-power/energy computation for solving machine learning tasks. Currently,
most of the SNN architectures are derived from Artificial Neural Networks whose
neurons' architectures and operations are different from SNNs, or developed
without considering memory budgets from the underlying processing hardware.
These limitations hinder the SNNs from reaching their full potential in
accuracy and efficiency. Towards this, we propose SpikeNAS, a novel
memory-aware neural architecture search (NAS) framework for SNNs that can
quickly find an appropriate SNN architecture with high accuracy under the given
memory budgets. To do this, our SpikeNAS employs several key steps: analyzing
the impacts of network operations on the accuracy, enhancing the network
architecture to improve the learning quality, and developing a fast
memory-aware search algorithm. The experimental results show that our SpikeNAS
improves the searching time and maintains high accuracy as compared to
state-of-the-art while meeting the given memory budgets (e.g., 4.4x faster
search with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000
Ada GPU machine), thereby quickly providing the appropriate SNN architecture
for memory-constrained SNN-based systems.</div><div><a href='http://arxiv.org/abs/2402.11322v1'>2402.11322v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04491v1")'>SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and
  Asynchronous Machine Learning</div>
<div id='2401.04491v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T11:07:48Z</div><div>Authors: Hector A. Gonzalez, Jiaxin Huang, Florian Kelber, Khaleelulla Khan Nazeer, Tim Langer, Chen Liu, Matthias Lohrmann, Amirhossein Rostami, Mark Sch√∂ne, Bernhard Vogginger, Timo C. Wunderlich, Yexin Yan, Mahmoud Akl, Christian Mayr</div><div style='padding-top: 10px; width: 80ex'>The joint progress of artificial neural networks (ANNs) and domain specific
hardware accelerators such as GPUs and TPUs took over many domains of machine
learning research. This development is accompanied by a rapid growth of the
required computational demands for larger models and more data. Concurrently,
emerging properties of foundation models such as in-context learning drive new
opportunities for machine learning applications. However, the computational
cost of such applications is a limiting factor of the technology in data
centers, and more importantly in mobile devices and edge systems. To mediate
the energy footprint and non-trivial latency of contemporary systems,
neuromorphic computing systems deeply integrate computational principles of
neurobiological systems by leveraging low-power analog and digital
technologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable
machine learning. The event-based and asynchronous design of SpiNNaker2 allows
the composition of large-scale systems involving thousands of chips. This work
features the operating principles of SpiNNaker2 systems, outlining the
prototype of novel machine learning applications. These applications range from
ANNs over bio-inspired spiking neural networks to generalized event-based
neural networks. With the successful development and deployment of SpiNNaker2,
we aim to facilitate the advancement of event-based and asynchronous algorithms
for future generations of machine learning systems.</div><div><a href='http://arxiv.org/abs/2401.04491v1'>2401.04491v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14878v1")'>Energy-efficiency Limits on Training AI Systems using Learning-in-Memory</div>
<div id='2402.14878v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T21:02:11Z</div><div>Authors: Zihao Chen, Johannes Leugering, Gert Cauwenberghs, Shantanu Chakrabartty</div><div style='padding-top: 10px; width: 80ex'>Learning-in-memory (LIM) is a recently proposed paradigm to overcome
fundamental memory bottlenecks in training machine learning systems. While
compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e.
energy dissipated due to repeated memory read access) they are agnostic to the
energy dissipated due to repeated memory writes at the precision required for
training (the update-wall), and they don't account for the energy dissipated
when transferring information between short-term and long-term memories (the
consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can
be overcome if the energy barrier of physical memories is adaptively modulated
such that the dynamics of memory updates and consolidation match the Lyapunov
dynamics of gradient-descent training of an AI model. In this paper, we derive
new theoretical lower bounds on energy dissipation when training AI systems
using different LIM approaches. The analysis presented here is model-agnostic
and highlights the trade-off between energy efficiency and the speed of
training. The resulting non-equilibrium energy-efficiency bounds have a similar
flavor as that of Landauer's energy-dissipation bounds. We also extend these
limits by taking into account the number of floating-point operations (FLOPs)
used for training, the size of the AI model, and the precision of the training
parameters. Our projections suggest that the energy-dissipation lower-bound to
train a brain scale AI system (comprising of $10^{15}$ parameters) using LIM is
$10^8 \sim 10^9$ Joules, which is on the same magnitude the Landauer's
adiabatic lower-bound and $6$ to $7$ orders of magnitude lower than the
projections obtained using state-of-the-art AI accelerator hardware
lower-bounds.</div><div><a href='http://arxiv.org/abs/2402.14878v1'>2402.14878v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12630v1")'>Full-Stack Optimization for CAM-Only DNN Inference</div>
<div id='2401.12630v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:27:38Z</div><div>Authors: Jo√£o Paulo C. de Lima, Asif Ali Khan, Luigi Carro, Jeronimo Castrillon</div><div style='padding-top: 10px; width: 80ex'>The accuracy of neural networks has greatly improved across various domains
over the past years. Their ever-increasing complexity, however, leads to
prohibitively high energy demands and latency in von Neumann systems. Several
computing-in-memory (CIM) systems have recently been proposed to overcome this,
but trade-offs involving accuracy, hardware reliability, and scalability for
large models remain a challenge. Additionally, for some CIM designs, the
activation movement still requires considerable time and energy. This paper
explores the combination of algorithmic optimizations for ternary weight neural
networks and associative processors (APs) implemented using racetrack memory
(RTM). We propose a novel compilation flow to optimize convolutions on APs by
reducing their arithmetic intensity. By leveraging the benefits of RTM-based
APs, this approach substantially reduces data transfers within the memory while
addressing accuracy, energy efficiency, and reliability concerns. Concretely,
our solution improves the energy efficiency of ResNet-18 inference on ImageNet
by 7.5x compared to crossbar in-memory accelerators while retaining software
accuracy.</div><div><a href='http://arxiv.org/abs/2401.12630v1'>2401.12630v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00849v1")'>NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable
  Functions</div>
<div id='2403.00849v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T16:10:21Z</div><div>Authors: Marta Andronic, George A. Constantinides</div><div style='padding-top: 10px; width: 80ex'>Field-Programmable Gate Array (FPGA) accelerators have proven successful in
handling latency- and resource-critical deep neural network (DNN) inference
tasks. Among the most computationally intensive operations in a neural network
(NN) is the dot product between the feature and weight vectors. Thus, some
previous FPGA acceleration works have proposed mapping neurons with quantized
inputs and outputs directly to lookup tables (LUTs) for hardware
implementation. In these works, the boundaries of the neurons coincide with the
boundaries of the LUTs. We propose relaxing these boundaries and mapping entire
sub-networks to a single LUT. As the sub-networks are absorbed within the LUT,
the NN topology and precision within a partition do not affect the size of the
lookup tables generated. Therefore, we utilize fully connected layers with
floating-point precision inside each partition, which benefit from being
universal function approximators, with rigid sparsity and quantization enforced
only between partitions, where the NN topology becomes exposed to the circuit
topology. Although cheap to implement, this approach can lead to very deep NNs,
and so to tackle challenges like vanishing gradients, we also introduce skip
connections inside the partitions. The resulting methodology can be seen as
training DNNs with a specific sparsity pattern that allows them to be mapped to
much shallower circuit-level networks, thereby significantly improving latency.
We validate our proposed method on a known latency-critical task, jet
substructure tagging, and on the classical computer vision task, the digit
classification using MNIST. Our approach allows for greater function
expressivity within the LUTs compared to existing work, leading to lower
latency NNs for the same accuracy.</div><div><a href='http://arxiv.org/abs/2403.00849v1'>2403.00849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14110v1")'>Towards Cheaper Inference in Deep Networks with Lower Bit-Width
  Accumulators</div>
<div id='2401.14110v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:46:01Z</div><div>Authors: Yaniv Blumenfeld, Itay Hubara, Daniel Soudry</div><div style='padding-top: 10px; width: 80ex'>The majority of the research on the quantization of Deep Neural Networks
(DNNs) is focused on reducing the precision of tensors visible by high-level
frameworks (e.g., weights, activations, and gradients). However, current
hardware still relies on high-accuracy core operations. Most significant is the
operation of accumulating products. This high-precision accumulation operation
is gradually becoming the main computational bottleneck. This is because, so
far, the usage of low-precision accumulators led to a significant degradation
in performance. In this work, we present a simple method to train and fine-tune
high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits
accumulators, with no significant degradation in accuracy. Lastly, we show that
as we decrease the accumulation precision further, using fine-grained gradient
approximations can improve the DNN accuracy.</div><div><a href='http://arxiv.org/abs/2401.14110v1'>2401.14110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05465v1")'>Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit
  Encodings for Efficient DNN Inference</div>
<div id='2403.05465v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T17:28:49Z</div><div>Authors: Akshat Ramachandran, Zishen Wan, Geonhwa Jeong, John Gustafson, Tushar Krishna</div><div style='padding-top: 10px; width: 80ex'>Traditional Deep Neural Network (DNN) quantization methods using integer,
fixed-point, or floating-point data types struggle to capture diverse DNN
parameter distributions at low precision, and often require large silicon
overhead and intensive quantization-aware training. In this study, we introduce
Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by
posits that dynamically adapts to DNN weight/activation distributions by
parameterizing LP bit fields. We also develop a novel genetic-algorithm based
framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters
while reducing representational divergence between quantized and full-precision
models through a novel global-local contrastive objective. Additionally, we
design a unified mixed-precision LP accelerator (LPA) architecture comprising
of processing elements (PEs) incorporating LP in the computational datapath.
Our algorithm-hardware co-design demonstrates on average &lt;1% drop in top-1
accuracy across various CNN and ViT models. It also achieves ~ 2x improvements
in performance per unit area and 2.2x gains in energy efficiency compared to
state-of-the-art quantization accelerators using different data types.</div><div><a href='http://arxiv.org/abs/2403.05465v1'>2403.05465v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17544v1")'>Trainable Fixed-Point Quantization for Deep Learning Acceleration on
  FPGAs</div>
<div id='2401.17544v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T02:18:27Z</div><div>Authors: Dingyi Dai, Yichi Zhang, Jiahao Zhang, Zhanqiu Hu, Yaohui Cai, Qi Sun, Zhiru Zhang</div><div style='padding-top: 10px; width: 80ex'>Quantization is a crucial technique for deploying deep learning models on
resource-constrained devices, such as embedded FPGAs. Prior efforts mostly
focus on quantizing matrix multiplications, leaving other layers like BatchNorm
or shortcuts in floating-point form, even though fixed-point arithmetic is more
efficient on FPGAs. A common practice is to fine-tune a pre-trained model to
fixed-point for FPGA deployment, but potentially degrading accuracy.
  This work presents QFX, a novel trainable fixed-point quantization approach
that automatically learns the binary-point position during model training.
Additionally, we introduce a multiplier-free quantization strategy within QFX
to minimize DSP usage. QFX is implemented as a PyTorch-based library that
efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a
differentiable manner during backpropagation. With minimal effort, models
trained with QFX can readily be deployed through HLS, producing the same
numerical results as their software counterparts. Our evaluation shows that
compared to post-training quantization, QFX can quantize models trained with
element-wise layers quantized to fewer bits and achieve higher accuracy on both
CIFAR-10 and ImageNet datasets. We further demonstrate the efficacy of
multiplier-free quantization using a state-of-the-art binarized neural network
accelerator designed for an embedded FPGA (AMD Xilinx Ultra96 v2). We plan to
release QFX in open-source format.</div><div><a href='http://arxiv.org/abs/2401.17544v1'>2401.17544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12422v1")'>Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data
  Flow and Per-Block Quantization</div>
<div id='2403.12422v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T04:09:11Z</div><div>Authors: Haocheng Xi, Yuxiang Chen, Kang Zhao, Kaijun Zheng, Jianfei Chen, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>Pretraining transformers are generally time-consuming. Fully quantized
training (FQT) is a promising approach to speed up pretraining. However, most
FQT methods adopt a quantize-compute-dequantize procedure, which often leads to
suboptimal speedup and significant performance degradation when used in
transformers due to the high memory access overheads and low-precision
computations. In this work, we propose Jetfire, an efficient and accurate INT8
training method specific to transformers. Our method features an INT8 data flow
to optimize memory access and a per-block quantization method to maintain the
accuracy of pretrained transformers. Extensive experiments demonstrate that our
INT8 FQT method achieves comparable accuracy to the FP16 training baseline and
outperforms the existing INT8 training works for transformers. Moreover, for a
standard transformer block, our method offers an end-to-end training speedup of
1.42x and a 1.49x memory reduction compared to the FP16 baseline.</div><div><a href='http://arxiv.org/abs/2403.12422v1'>2403.12422v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12263v2")'>Towards a tailored mixed-precision sub-8-bit quantization scheme for
  Gated Recurrent Units using Genetic Algorithms</div>
<div id='2402.12263v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T16:24:20Z</div><div>Authors: Riccardo Miccini, Alessandro Cerioli, Cl√©ment Laroche, Tobias Piechowiak, Jens Spars√∏, Luca Pezzarossa</div><div style='padding-top: 10px; width: 80ex'>Despite the recent advances in model compression techniques for deep neural
networks, deploying such models on ultra-low-power embedded devices still
proves challenging. In particular, quantization schemes for Gated Recurrent
Units (GRU) are difficult to tune due to their dependence on an internal state,
preventing them from fully benefiting from sub-8bit quantization. In this work,
we propose a modular integer quantization scheme for GRUs where the bit width
of each operator can be selected independently. We then employ Genetic
Algorithms (GA) to explore the vast search space of possible bit widths,
simultaneously optimising for model size and accuracy. We evaluate our methods
on four different sequential tasks and demonstrate that mixed-precision
solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In
our results, we achieve a model size reduction between 25% and 55% while
maintaining an accuracy comparable with the 8-bit homogeneous equivalent.</div><div><a href='http://arxiv.org/abs/2402.12263v2'>2402.12263v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04012v1")'>Quantized Approximately Orthogonal Recurrent Neural Networks</div>
<div id='2402.04012v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T09:59:57Z</div><div>Authors: Armand Foucault, Franck Mamalet, Fran√ßois Malgouyres</div><div style='padding-top: 10px; width: 80ex'>Orthogonal recurrent neural networks (ORNNs) are an appealing option for
learning tasks involving time series with long-term dependencies, thanks to
their simplicity and computational stability. However, these networks often
require a substantial number of parameters to perform well, which can be
prohibitive in power-constrained environments, such as compact devices. One
approach to address this issue is neural network quantization. The construction
of such networks remains an open problem, acknowledged for its inherent
instability.In this paper, we explore the quantization of the recurrent and
input weight matrices in ORNNs, leading to Quantized approximately Orthogonal
RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and
three quantization-aware training (QAT) algorithms that incorporate orthogonal
constraints and quantized weights. Empirical results demonstrate the advantages
of employing QAT over PTQ. The most efficient model achieves results similar to
state-of-the-art full-precision ORNN and LSTM on a variety of standard
benchmarks, even with 3-bits quantization.</div><div><a href='http://arxiv.org/abs/2402.04012v1'>2402.04012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17269v1")'>Effect of Weight Quantization on Learning Models by Typical Case
  Analysis</div>
<div id='2401.17269v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T18:58:46Z</div><div>Authors: Shuhei Kashiwamura, Ayaka Sakata, Masaaki Imaizumi</div><div style='padding-top: 10px; width: 80ex'>This paper examines the quantization methods used in large-scale data
analysis models and their hyperparameter choices. The recent surge in data
analysis scale has significantly increased computational resource requirements.
To address this, quantizing model weights has become a prevalent practice in
data analysis applications such as deep learning. Quantization is particularly
vital for deploying large models on devices with limited computational
resources. However, the selection of quantization hyperparameters, like the
number of bits and value range for weight quantization, remains an
underexplored area. In this study, we employ the typical case analysis from
statistical physics, specifically the replica method, to explore the impact of
hyperparameters on the quantization of simple learning models. Our analysis
yields three key findings: (i) an unstable hyperparameter phase, known as
replica symmetry breaking, occurs with a small number of bits and a large
quantization width; (ii) there is an optimal quantization width that minimizes
error; and (iii) quantization delays the onset of overparameterization, helping
to mitigate overfitting as indicated by the double descent phenomenon. We also
discover that non-uniform quantization can enhance stability. Additionally, we
develop an approximate message-passing algorithm to validate our theoretical
results.</div><div><a href='http://arxiv.org/abs/2401.17269v1'>2401.17269v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11998v1")'>Learning Useful Representations of Recurrent Neural Network Weight
  Matrices</div>
<div id='2403.11998v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:32:23Z</div><div>Authors: Vincent Herrmann, Francesco Faccio, J√ºrgen Schmidhuber</div><div style='padding-top: 10px; width: 80ex'>Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential
computers. The program of an RNN is its weight matrix. How to learn useful
representations of RNN weights that facilitate RNN analysis as well as
downstream tasks? While the mechanistic approach directly looks at some RNN's
weights to predict its behavior, the functionalist approach analyzes its
overall functionality -- specifically, its input-output mapping. We consider
several mechanistic approaches for RNN weights and adapt the permutation
equivariant Deep Weight Space layer for RNNs. Our two novel functionalist
approaches extract information from RNN weights by 'interrogating' the RNN
through probing inputs. We develop a theoretical framework that demonstrates
conditions under which the functionalist approach can generate rich
representations that help determine RNN behavior. We create and release the
first two 'model zoo' datasets for RNN weight representation learning. One
consists of generative models of a class of formal languages, and the other one
of classifiers of sequentially processed MNIST digits. With the help of an
emulation-based self-supervised learning technique we compare and evaluate the
different RNN weight encoding techniques on multiple downstream applications.
On the most challenging one, namely predicting which exact task the RNN was
trained on, functionalist approaches show clear superiority.</div><div><a href='http://arxiv.org/abs/2403.11998v1'>2403.11998v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02930v1")'>Embedding Hardware Approximations in Discrete Genetic-based Training for
  Printed MLPs</div>
<div id='2402.02930v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:52:23Z</div><div>Authors: Florentia Afentaki, Michael Hefenbrock, Georgios Zervakis, Mehdi B. Tahoori</div><div style='padding-top: 10px; width: 80ex'>Printed Electronics (PE) stands out as a promisingtechnology for widespread
computing due to its distinct attributes, such as low costs and flexible
manufacturing. Unlike traditional silicon-based technologies, PE enables
stretchable, conformal,and non-toxic hardware. However, PE are constrained by
larger feature sizes, making it challenging to implement complex circuits such
as machine learning (ML) classifiers. Approximate computing has been proven to
reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs).
In this paper, we maximize the benefits of approximate computing by integrating
hardware approximation into the MLP training process. Due to the discrete
nature of hardware approximation, we propose and implement a genetic-based,
approximate, hardware-aware training approach specifically designed for printed
MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction
compared to the baseline while outperforming state of-the-art approximate and
stochastic printed MLPs.</div><div><a href='http://arxiv.org/abs/2402.02930v1'>2402.02930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07953v1")'>Abstracting Sparse DNN Acceleration via Structured Sparse Tensor
  Decomposition</div>
<div id='2403.07953v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T06:25:47Z</div><div>Authors: Geonhwa Jeong, Po-An Tsai, Abhimanyu R. Bambhaniya, Stephen W. Keckler, Tushar Krishna</div><div style='padding-top: 10px; width: 80ex'>Exploiting sparsity in deep neural networks (DNNs) has been a promising area
to meet the growing computation need of modern DNNs. However, in practice,
sparse DNN acceleration still faces a key challenge. To minimize the overhead
of sparse acceleration, hardware designers have proposed structured sparse
hardware support recently, which provides limited flexibility and requires
extra model fine-tuning. Moreover, any sparse model fine-tuned for certain
structured sparse hardware cannot be accelerated by other structured hardware.
To bridge the gap between sparse DNN models and hardware, this paper proposes
tensor approximation via structured decomposition (TASD), which leverages the
distributive property in linear algebra to turn any sparse tensor into a series
of structured sparse tensors. Next, we develop a software framework, TASDER, to
accelerate DNNs by searching layer-wise, high-quality structured decomposition
for both weight and activation tensors so that they can be accelerated by any
systems with structured sparse hardware support. Evaluation results show that,
by exploiting prior structured sparse hardware baselines, our method can
accelerate off-the-shelf dense and sparse DNNs without fine-tuning and improves
energy-delay-product by up to 83% and 74% on average.</div><div><a href='http://arxiv.org/abs/2403.07953v1'>2403.07953v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07393v1")'>TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge
  AI with Compact Slow-Light Electro-Optic Modulator</div>
<div id='2402.07393v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T03:40:32Z</div><div>Authors: Meng Zhang, Dennis Yin, Nicholas Gangi, Amir Begoviƒá, Alexander Chen, Zhaoran Rena Huang, Jiaqi Gu</div><div style='padding-top: 10px; width: 80ex'>Electronic-photonic computing systems offer immense potential in
energy-efficient artificial intelligence (AI) acceleration tasks due to the
superior computing speed and efficiency of optics, especially for real-time,
low-energy deep neural network (DNN) inference tasks on resource-restricted
edge platforms. However, current optical neural accelerators based on
foundry-available devices and conventional system architecture still encounter
a performance gap compared to highly customized electronic counterparts. To
bridge the performance gap due to lack of domain specialization, we present a
time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with
cross-layer device/circuit/architecture customization. At the device level, we
present foundry-compatible, customized photonic devices, including a slow-light
electro-optic modulator with experimental demonstration, optical splitters, and
phase shifters that significantly reduce the footprint and power in input
encoding and dot-product calculation. At the circuit level, partial products
are hierarchically accumulated via parallel photocurrent aggregation,
lightweight capacitive temporal integration, and sequential digital summation,
considerably relieving the analog-to-digital conversion bottleneck. We also
employ a multi-tile, multi-core architecture to maximize hardware sharing for
higher efficiency. Across diverse edge AI workloads, TeMPO delivers
digital-comparable task accuracy with superior quantization/noise tolerance. We
achieve a 368.6 TOPS peak performance, 22.3 TOPS/W energy efficiency, and 1.2
TOPS/mm$^2$ compute density, pushing the Pareto frontier in edge AI hardware.
This work signifies the power of cross-layer co-design and domain-specific
customization, paving the way for future electronic-photonic accelerators with
even greater performance and efficiency.</div><div><a href='http://arxiv.org/abs/2402.07393v1'>2402.07393v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03149v3")'>A Comparative Analysis of Microrings Based Incoherent Photonic GEMM
  Accelerators</div>
<div id='2402.03149v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:16:17Z</div><div>Authors: Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, Oluwaseun Adewunmi Alo, Ishan Thakkar</div><div style='padding-top: 10px; width: 80ex'>Several microring resonator (MRR) based analog photonic architectures have
been proposed to accelerate general matrix-matrix multiplications (GEMMs) in
deep neural networks with exceptional throughput and energy efficiency. To
implement GEMM functions, these MRR-based architectures, in general, manipulate
optical signals in five different ways: (i) Splitting (copying) of multiple
optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing)
of multiple optical signals to achieve a certain fan-in, (iii) Modulation of
optical signals to imprint input values onto analog signal amplitude, (iv)
Weighting of modulated optical signals to achieve analog input-weight
multiplication, (v) Summation of optical signals. The MRR-based GEMM
accelerators undertake the first four ways of signal manipulation in an
arbitrary order ignoring the possible impact of the order of these
manipulations on their performance. In this paper, we conduct a detailed
analysis of accelerator organizations with three different orders of these
manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2)
Aggregation-Splitting-Modulation-Weighting (ASMW), and (3)
Splitting-Modulation-Weighting-Aggregation (SMWA). We show that these
organizations affect the crosstalk noise and optical signal losses in different
magnitudes, which renders these organizations with different levels of
processing parallelism at the circuit level, and different magnitudes of
throughput and energy-area efficiency at the system level. Our evaluation
results for four CNN models show that SMWA organization achieves up to
4.4$\times$, 5$\times$, and 5.2$\times$ better throughput, energy efficiency,
and area-energy efficiency, respectively, compared to ASMW and MASW
organizations on average.</div><div><a href='http://arxiv.org/abs/2402.03149v3'>2402.03149v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02688v1")'>DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal
  Variations Toward Self-Corrected Photonic Tensor Accelerators</div>
<div id='2403.02688v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:17:13Z</div><div>Authors: Haotian Lu, Sanmitra Banerjee, Jiaqi Gu</div><div style='padding-top: 10px; width: 80ex'>Photonic computing has emerged as a promising solution for accelerating
computation-intensive artificial intelligence (AI) workloads, offering
unparalleled speed and energy efficiency, especially in resource-limited,
latency-sensitive edge computing environments. However, the deployment of
analog photonic tensor accelerators encounters reliability challenges due to
hardware noises and environmental variations. While off-chip noise-aware
training and on-chip training have been proposed to enhance the variation
tolerance of optical neural accelerators with moderate, static noises, we
observe a notable performance degradation over time due to temporally drifting
variations, which requires a real-time, in-situ calibration mechanism. To
tackle this challenging reliability issues, for the first time, we propose a
lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing
adaptive, in-situ accuracy recovery against temporally drifting noises. The
DOCTOR framework intelligently monitors the chip status using adaptive probing
and performs fast in-situ training-free calibration to restore accuracy when
necessary. Recognizing nonuniform spatial variation distributions across
devices and tensor cores, we also propose a variation-aware architectural
remapping strategy to avoid executing critical tasks on noisy devices.
Extensive experiments show that our proposed framework can guarantee sustained
performance under drifting variations with 34% higher accuracy and 2-3
orders-of-magnitude lower overhead compared to state-of-the-art on-chip
training methods.</div><div><a href='http://arxiv.org/abs/2403.02688v1'>2403.02688v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05121v1")'>Photonics for Sustainable Computing</div>
<div id='2401.05121v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T12:37:23Z</div><div>Authors: Farbin Fayza, Satyavolu Papa Rao, Darius Bunandar, Udit Gupta, Ajay Joshi</div><div style='padding-top: 10px; width: 80ex'>Photonic integrated circuits are finding use in a variety of applications
including optical transceivers, LIDAR, bio-sensing, photonic quantum computing,
and Machine Learning (ML). In particular, with the exponentially increasing
sizes of ML models, photonics-based accelerators are getting special attention
as a sustainable solution because they can perform ML inferences with multiple
orders of magnitude higher energy efficiency than CMOS-based accelerators.
However, recent studies have shown that hardware manufacturing and
infrastructure contribute significantly to the carbon footprint of computing
devices, even surpassing the emissions generated during their use. For example,
the manufacturing process accounts for 74% of the total carbon emissions from
Apple in 2019. This prompts us to ask -- if we consider both the embodied
(manufacturing) and operational carbon cost of photonics, is it indeed a viable
avenue for a sustainable future? So, in this paper, we build a carbon footprint
model for photonic chips and investigate the sustainability of photonics-based
accelerators by conducting a case study on ADEPT, a photonics-based accelerator
for deep neural network inference. Our analysis shows that photonics can reduce
both operational and embodied carbon footprints with its high energy efficiency
and at least 4$\times$ less fabrication carbon cost per unit area than 28 nm
CMOS.</div><div><a href='http://arxiv.org/abs/2401.05121v1'>2401.05121v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09780v1")'>TinyCL: An Efficient Hardware Architecture for Continual Learning on
  Autonomous Systems</div>
<div id='2402.09780v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T08:09:17Z</div><div>Authors: Eugenio Ressa, Alberto Marchisio, Maurizio Martina, Guido Masera, Muhammad Shafique</div><div style='padding-top: 10px; width: 80ex'>The Continuous Learning (CL) paradigm consists of continuously evolving the
parameters of the Deep Neural Network (DNN) model to progressively learn to
perform new tasks without reducing the performance on previous tasks, i.e.,
avoiding the so-called catastrophic forgetting. However, the DNN parameter
update in CL-based autonomous systems is extremely resource-hungry. The
existing DNN accelerators cannot be directly employed in CL because they only
support the execution of the forward propagation. Only a few prior
architectures execute the backpropagation and weight update, but they lack the
control and management for CL. Towards this, we design a hardware architecture,
TinyCL, to perform CL on resource-constrained autonomous systems. It consists
of a processing unit that executes both forward and backward propagation, and a
control unit that manages memory-based CL workload. To minimize the memory
accesses, the sliding window of the convolutional layer moves in a snake-like
fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at
runtime to execute different operations. As per our knowledge, our proposed
TinyCL represents the first hardware accelerator that executes CL on autonomous
systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS
technology node with the conventional ASIC design flow. It executes 1 epoch of
training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while
1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,
thus achieving a 58 x speedup, consuming 86 mW in a 4.74 mm2 die.</div><div><a href='http://arxiv.org/abs/2402.09780v1'>2402.09780v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14353v1")'>DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video
  Analytics</div>
<div id='2403.14353v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T12:28:44Z</div><div>Authors: Yoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung Kim, Seongryong Oh, Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, Jongse Park</div><div style='padding-top: 10px; width: 80ex'>Deep neural network (DNN) video analytics is crucial for autonomous systems
such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security
robots. However, real-world deployment faces challenges due to their limited
computational resources and battery power. To tackle these challenges,
continuous learning exploits a lightweight "student" model at deployment
(inference), leverages a larger "teacher" model for labeling sampled data
(labeling), and continuously retrains the student model to adapt to changing
scenarios (retraining). This paper highlights the limitations in
state-of-the-art continuous learning systems: (1) they focus on computations
for retraining, while overlooking the compute needs for inference and labeling,
(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous
systems, and (3) they are located on a remote centralized server, intended for
multi-tenant scenarios, again unsuitable for autonomous systems due to privacy,
network availability, and latency concerns. We propose a hardware-algorithm
co-designed solution for continuous learning, DaCapo, that enables autonomous
systems to perform concurrent executions of inference, labeling, and training
in a performant and energy-efficient manner. DaCapo comprises (1) a
spatially-partitionable and precision-flexible accelerator enabling parallel
execution of kernels on sub-accelerators at their respective precisions, and
(2) a spatiotemporal resource allocation algorithm that strategically navigates
the resource-accuracy tradeoff space, facilitating optimal decisions for
resource allocation to achieve maximal accuracy. Our evaluation shows that
DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based
continuous learning systems, Ekya and EOMU, respectively, while consuming 254x
less power.</div><div><a href='http://arxiv.org/abs/2403.14353v1'>2403.14353v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.07671v2")'>CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory
  Architectures</div>
<div id='2401.07671v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T13:35:21Z</div><div>Authors: Rebecca Pelke, Jose Cubero-Cascante, Nils Bosbach, Felix Staudigl, Rainer Leupers, Jan Moritz Joseph</div><div style='padding-top: 10px; width: 80ex'>The demand for efficient machine learning (ML) accelerators is growing
rapidly, driving the development of novel computing concepts such as resistive
random access memory (RRAM)-based tiled computing-in-memory (CIM)
architectures. CIM allows to compute within the memory unit, resulting in
faster data processing and reduced power consumption. Efficient compiler
algorithms are essential to exploit the potential of tiled CIM architectures.
While conventional ML compilers focus on code generation for CPUs, GPUs, and
other von Neumann architectures, adaptations are needed to cover CIM
architectures. Cross-layer scheduling is a promising approach, as it enhances
the utilization of CIM cores, thereby accelerating computations. Although
similar concepts are implicitly used in previous work, there is a lack of clear
and quantifiable algorithmic definitions for cross-layer scheduling for tiled
CIM architectures. To close this gap, we present CLSA-CIM, a cross-layer
scheduling algorithm for tiled CIM architectures. We integrate CLSA-CIM with
existing weight-mapping strategies and compare performance against
state-of-the-art (SOTA) scheduling algorithms. CLSA-CIM improves the
utilization by up to 17.9 x , resulting in an overall speedup increase of up to
29.2 x compared to SOTA.</div><div><a href='http://arxiv.org/abs/2401.07671v2'>2401.07671v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04189v1")'>Silicon Photonic 2.5D Interposer Networks for Overcoming Communication
  Bottlenecks in Scale-out Machine Learning Hardware Accelerators</div>
<div id='2403.04189v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T03:38:35Z</div><div>Authors: Febin Sunny, Ebadollah Taheri, Mahdi Nikdast, Sudeep Pasricha</div><div style='padding-top: 10px; width: 80ex'>Modern machine learning (ML) applications are becoming increasingly complex
and monolithic (single chip) accelerator architectures cannot keep up with
their energy efficiency and throughput demands. Even though modern digital
electronic accelerators are gradually adopting 2.5D architectures with multiple
smaller chiplets to improve scalability, they face fundamental limitations due
to a reliance on slow metallic interconnects. This paper outlines how optical
communication and computation can be leveraged in 2.5D platforms to realize
energy-efficient and high throughput 2.5D ML accelerator architectures.</div><div><a href='http://arxiv.org/abs/2403.04189v1'>2403.04189v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08980v1")'>Architectural Implications of Neural Network Inference for High
  Data-Rate, Low-Latency Scientific Applications</div>
<div id='2403.08980v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-13T22:10:42Z</div><div>Authors: Olivia Weng, Alexander Redding, Nhan Tran, Javier Mauricio Duarte, Ryan Kastner</div><div style='padding-top: 10px; width: 80ex'>With more scientific fields relying on neural networks (NNs) to process data
incoming at extreme throughputs and latencies, it is crucial to develop NNs
with all their parameters stored on-chip. In many of these applications, there
is not enough time to go off-chip and retrieve weights. Even more so, off-chip
memory such as DRAM does not have the bandwidth required to process these NNs
as fast as the data is being produced (e.g., every 25 ns). As such, these
extreme latency and bandwidth requirements have architectural implications for
the hardware intended to run these NNs: 1) all NN parameters must fit on-chip,
and 2) codesigning custom/reconfigurable logic is often required to meet these
latency and bandwidth constraints. In our work, we show that many scientific NN
applications must run fully on chip, in the extreme case requiring a custom
chip to meet such stringent constraints.</div><div><a href='http://arxiv.org/abs/2403.08980v1'>2403.08980v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02361v1")'>Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness</div>
<div id='2402.02361v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T06:11:12Z</div><div>Authors: Liang Qiao, Jun Shi, Xiaoyu Hao, Xi Fang, Minfan Zhao, Ziqi Zhu, Junshi Chen, Hong An, Bing Li, Honghui Yuan, Xinyang Wang</div><div style='padding-top: 10px; width: 80ex'>Tensor program optimization on Deep Learning Accelerators (DLAs) is critical
for efficient model deployment. Although search-based Deep Learning Compilers
(DLCs) have achieved significant performance gains compared to manual methods,
they still suffer from the persistent challenges of low search efficiency and
poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$,
following hardware/software co-design principles to hierarchically boost tensor
program optimization. Pruner comprises two primary components: a Parameterized
Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model
($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic
performance analysis tool, guiding the pruning of the search space, while the
latter enables the performance prediction of tensor programs according to the
critical data-flow patterns. Furthermore, to ensure effective cross-platform
adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy
using a Siamese network, which establishes a bidirectional feedback mechanism
to improve the robustness of the pre-trained cost model. The extensive
experimental results demonstrate the effectiveness and advancement of the
proposed Pruner in various tensor program tuning tasks across both online and
offline scenarios, with low resource overhead. The code is available at
https://github.com/qiaolian9/Pruner.</div><div><a href='http://arxiv.org/abs/2402.02361v1'>2402.02361v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.04882v1")'>LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre
  Memory Units</div>
<div id='2402.04882v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T01:10:18Z</div><div>Authors: Zeyu Liu, Gourav Datta, Anni Li, Peter Anthony Beerel</div><div style='padding-top: 10px; width: 80ex'>Transformer models have demonstrated high accuracy in numerous applications
but have high complexity and lack sequential processing capability making them
ill-suited for many streaming applications at the edge where devices are
heavily resource-constrained. Thus motivated, many researchers have proposed
reformulating the transformer models as RNN modules which modify the
self-attention computation with explicit states. However, these approaches
often incur significant performance degradation. The ultimate goal is to
develop a model that has the following properties: parallel training, streaming
and low-cost inference, and SOTA performance. In this paper, we propose a new
direction to achieve this goal. We show how architectural modifications to a
recurrent model can help push its performance toward Transformer models while
retaining its sequential processing capability. Specifically, inspired by the
recent success of Legendre Memory Units (LMU) in sequence learning tasks, we
propose LMUFormer, which augments the LMU with convolutional patch embedding
and convolutional channel mixer. Moreover, we present a spiking version of this
architecture, which introduces the benefit of states within the patch embedding
and channel mixer modules while simultaneously reducing the computing
complexity. We evaluated our architectures on multiple sequence datasets. In
comparison to SOTA transformer-based models within the ANN domain on the SCv2
dataset, our LMUFormer demonstrates comparable performance while necessitating
a remarkable 53 times reduction in parameters and a substantial 65 times
decrement in FLOPs. Additionally, owing to our model's proficiency in real-time
data processing, we can achieve a 32.03% reduction in sequence length, all
while incurring an inconsequential decline in performance. Our code is publicly
available at https://github.com/zeyuliu1037/LMUFormer.git.</div><div><a href='http://arxiv.org/abs/2402.04882v1'>2402.04882v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01912v1")'>Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object
  Recognition with Spiking Neural Network</div>
<div id='2401.01912v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T02:05:05Z</div><div>Authors: Yongqi Ding, Lin Zuo, Mengmeng Jing, Pei He, Yongjun Xiao</div><div style='padding-top: 10px; width: 80ex'>Neuromorphic object recognition with spiking neural networks (SNNs) is the
cornerstone of low-power neuromorphic computing. However, existing SNNs suffer
from significant latency, utilizing 10 to 40 timesteps or more, to recognize
neuromorphic objects. At low latencies, the performance of existing SNNs is
drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to
achieve low-latency neuromorphic object recognition without reducing
performance. Concretely, we alleviate the temporal redundancy in SNNs by
dividing SNNs into multiple stages with progressively shrinking timesteps,
which significantly reduces the inference latency. During timestep shrinkage,
the temporal transformer smoothly transforms the temporal scale and preserves
the information maximally. Moreover, we add multiple early classifiers to the
SNN during training to mitigate the mismatch between the surrogate gradient and
the true gradient, as well as the gradient vanishing/exploding, thus
eliminating the performance degradation at low latency. Extensive experiments
on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have
revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%.
With only 5 average timesteps and without any data augmentation, SSNN is able
to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a
heterogeneous temporal scale SNN and provides valuable insights into the
development of high-performance, low-latency SNNs.</div><div><a href='http://arxiv.org/abs/2401.01912v1'>2401.01912v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08792v1")'>Realtime Facial Expression Recognition: Neuromorphic Hardware vs. Edge
  AI Accelerators</div>
<div id='2403.08792v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T16:12:20Z</div><div>Authors: Heath Smith, James Seekings, Mohammadreza Mohammadi, Ramtin Zand</div><div style='padding-top: 10px; width: 80ex'>The paper focuses on real-time facial expression recognition (FER) systems as
an important component in various real-world applications such as social
robotics. We investigate two hardware options for the deployment of FER machine
learning (ML) models at the edge: neuromorphic hardware versus edge AI
accelerators. Our study includes exhaustive experiments providing comparative
analyses between the Intel Loihi neuromorphic processor and four distinct edge
platforms: Raspberry Pi-4, Intel Neural Compute Stick (NSC), Jetson Nano, and
Coral TPU. The results obtained show that Loihi can achieve approximately two
orders of magnitude reduction in power dissipation and one order of magnitude
energy savings compared to Coral TPU which happens to be the least
power-intensive and energy-consuming edge AI accelerator. These reductions in
power and energy are achieved while the neuromorphic solution maintains a
comparable level of accuracy with the edge accelerators, all within the
real-time latency requirements.</div><div><a href='http://arxiv.org/abs/2403.08792v1'>2403.08792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07314v1")'>Customizable Avatars with Dynamic Facial Action Coded Expressions
  (CADyFACE) for Improved User Engagement</div>
<div id='2403.07314v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T05:00:38Z</div><div>Authors: Megan A. Witherow, Crystal Butler, Winston J. Shields, Furkan Ilgin, Norou Diawara, Janice Keener, John W. Harrington, Khan M. Iftekharuddin</div><div style='padding-top: 10px; width: 80ex'>Customizable 3D avatar-based facial expression stimuli may improve user
engagement in behavioral biomarker discovery and therapeutic intervention for
autism, Alzheimer's disease, facial palsy, and more. However, there is a lack
of customizable avatar-based stimuli with Facial Action Coding System (FACS)
action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled,
customizable avatar-based expression stimuli for maintaining subjects'
engagement, (2) learning-based measurements that quantify subjects' facial
responses to such stimuli, and (3) validation of constructs represented by
stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial
Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS
expert. To measure subjects' AUs in response to CADyFACE, we propose a novel
Beta-guided Correlation and Multi-task Expression learning neural network
(BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss
encourages feature correlation with AUs while discouraging correlation with
subject identities for improved generalization. We train BeCoME-Net for
unilateral and bilateral AU detection and compare with state-of-the-art
approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty
healthy adult volunteers complete expression recognition and mimicry tasks in
an online feasibility study while webcam-based eye-tracking and video are
collected. We test validity of multiple constructs, including face preference
during recognition and AUs during mimicry.</div><div><a href='http://arxiv.org/abs/2403.07314v1'>2403.07314v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06315v1")'>A Study on Domain Generalization for Failure Detection through Human
  Reactions in HRI</div>
<div id='2403.06315v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T21:30:22Z</div><div>Authors: Maria Teresa Parreira, Sukruth Gowdru Lingaraju, Adolfo Ramirez-Aristizabal, Manaswi Saha, Michael Kuniavsky, Wendy Ju</div><div style='padding-top: 10px; width: 80ex'>Machine learning models are commonly tested in-distribution (same dataset);
performance almost always drops in out-of-distribution settings. For HRI
research, the goal is often to develop generalized models. This makes domain
generalization - retaining performance in different settings - a critical
issue. In this study, we present a concise analysis of domain generalization in
failure detection models trained on human facial expressions. Using two
distinct datasets of humans reacting to videos where error occurs, one from a
controlled lab setting and another collected online, we trained deep learning
models on each dataset. When testing these models on the alternate dataset, we
observed a significant performance drop. We reflect on the causes for the
observed model behavior and leave recommendations. This work emphasizes the
need for HRI research focusing on improving model robustness and real-life
applicability.</div><div><a href='http://arxiv.org/abs/2403.06315v1'>2403.06315v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09982v1")'>Data Augmentation and Transfer Learning Approaches Applied to Facial
  Expressions Recognition</div>
<div id='2402.09982v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:46:03Z</div><div>Authors: Enrico Randellini, Leonardo Rigutini, Claudio Sacca'</div><div style='padding-top: 10px; width: 80ex'>The face expression is the first thing we pay attention to when we want to
understand a person's state of mind. Thus, the ability to recognize facial
expressions in an automatic way is a very interesting research field. In this
paper, because the small size of available training datasets, we propose a
novel data augmentation technique that improves the performances in the
recognition task. We apply geometrical transformations and build from scratch
GAN models able to generate new synthetic images for each emotion type. Thus,
on the augmented datasets we fine tune pretrained convolutional neural networks
with different architectures. To measure the generalization ability of the
models, we apply extra-database protocol approach, namely we train models on
the augmented versions of training dataset and test them on two different
databases. The combination of these techniques allows to reach average accuracy
values of the order of 85\% for the InceptionResNetV2 model.</div><div><a href='http://arxiv.org/abs/2402.09982v1'>2402.09982v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10078v1")'>EventF2S: Asynchronous and Sparse Spiking AER Framework using
  Neuromorphic-Friendly Algorithm</div>
<div id='2402.10078v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T19:42:05Z</div><div>Authors: Lakshmi Annamalai, Chetan Singh Thakur</div><div style='padding-top: 10px; width: 80ex'>Bio-inspired Address Event Representation (AER) sensors have attracted
significant popularity owing to their low power consumption, high sparsity, and
high temporal resolution. Spiking Neural Network (SNN) has become the inherent
choice for AER data processing. However, the integration of the AER-SNN
paradigm has not adequately explored asynchronous processing, neuromorphic
compatibility, and sparse spiking, which are the key requirements of
resource-constrained applications. To address this gap, we introduce a
brain-inspired AER-SNN object recognition solution, which includes a data
encoder integrated with a First-To-Spike recognition network. Being fascinated
by the functionality of neurons in the visual cortex, we designed the solution
to be asynchronous and compatible with neuromorphic hardware. Furthermore, we
have adapted the principle of denoising and First-To-Spike coding to achieve
optimal spike signaling, significantly reducing computation costs. Experimental
evaluation has demonstrated that the proposed method incurs significantly less
computation cost to achieve state-of-the-art competitive accuracy. Overall, the
proposed solution offers an asynchronous and cost-effective AER recognition
system that harnesses the full potential of AER sensors.</div><div><a href='http://arxiv.org/abs/2402.10078v1'>2402.10078v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01571v1")'>Spiking Music: Audio Compression with Event Based Auto-encoders</div>
<div id='2402.01571v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T17:07:39Z</div><div>Authors: Martim Lisboa, Guillaume Bellec</div><div style='padding-top: 10px; width: 80ex'>Neurons in the brain communicate information via punctual events called
spikes. The timing of spikes is thought to carry rich information, but it is
not clear how to leverage this in digital systems. We demonstrate that
event-based encoding is efficient for audio compression. To build this
event-based representation we use a deep binary auto-encoder, and under high
sparsity pressure, the model enters a regime where the binary event matrix is
stored more efficiently with sparse matrix storage algorithms. We test this on
the large MAESTRO dataset of piano recordings against vector quantized
auto-encoders. Not only does our "Spiking Music compression" algorithm achieve
a competitive compression/reconstruction trade-off, but selectivity and
synchrony between encoded events and piano key strikes emerge without
supervision in the sparse regime.</div><div><a href='http://arxiv.org/abs/2402.01571v1'>2402.01571v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01287v1")'>Spiking CenterNet: A Distillation-boosted Spiking Neural Network for
  Object Detection</div>
<div id='2402.01287v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:23:03Z</div><div>Authors: Lennard Bodden, Franziska Schwaiger, Duc Bach Ha, Lars Kreuzberg, Sven Behnke</div><div style='padding-top: 10px; width: 80ex'>In the era of AI at the edge, self-driving cars, and climate change, the need
for energy-efficient, small, embedded AI is growing. Spiking Neural Networks
(SNNs) are a promising approach to address this challenge, with their
event-driven information flow and sparse activations. We propose Spiking
CenterNet for object detection on event data. It combines an SNN CenterNet
adaptation with an efficient M2U-Net-based decoder. Our model significantly
outperforms comparable previous work on Prophesee's challenging GEN1 Automotive
Detection Dataset while using less than half the energy. Distilling the
knowledge of a non-spiking teacher into our SNN further increases performance.
To the best of our knowledge, our work is the first approach that takes
advantage of knowledge distillation in the field of spiking object detection.</div><div><a href='http://arxiv.org/abs/2402.01287v1'>2402.01287v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15044v1")'>Fiducial Focus Augmentation for Facial Landmark Detection</div>
<div id='2402.15044v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T01:34:00Z</div><div>Authors: Purbayan Kar, Vishal Chudasama, Naoyuki Onoe, Pankaj Wasnik, Vineeth Balasubramanian</div><div style='padding-top: 10px; width: 80ex'>Deep learning methods have led to significant improvements in the performance
on the facial landmark detection (FLD) task. However, detecting landmarks in
challenging settings, such as head pose changes, exaggerated expressions, or
uneven illumination, continue to remain a challenge due to high variability and
insufficient samples. This inadequacy can be attributed to the model's
inability to effectively acquire appropriate facial structure information from
the input images. To address this, we propose a novel image augmentation
technique specifically designed for the FLD task to enhance the model's
understanding of facial structures. To effectively utilize the newly proposed
augmentation technique, we employ a Siamese architecture-based training
mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to
achieve collective learning of high-level feature representations from two
different views of the input images. Furthermore, we employ a Transformer +
CNN-based network with a custom hourglass module as the robust backbone for the
Siamese framework. Extensive experiments show that our approach outperforms
multiple state-of-the-art approaches across various benchmark datasets.</div><div><a href='http://arxiv.org/abs/2402.15044v1'>2402.15044v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14307v1")'>An FPGA-Based Accelerator Enabling Efficient Support for CNNs with
  Arbitrary Kernel Sizes</div>
<div id='2402.14307v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T05:52:55Z</div><div>Authors: Miaoxin Wang, Xiao Wu, Jun Lin, Zhongfeng Wang</div><div style='padding-top: 10px; width: 80ex'>Convolutional neural networks (CNNs) with large kernels, drawing inspiration
from the key operations of vision transformers (ViTs), have demonstrated
impressive performance in various vision-based applications. To address the
issue of computational efficiency degradation in existing designs for
supporting large-kernel convolutions, an FPGA-based inference accelerator is
proposed for the efficient deployment of CNNs with arbitrary kernel sizes.
Firstly, a Z-flow method is presented to optimize the computing data flow by
maximizing data reuse opportunity. Besides, the proposed design, incorporating
the kernel-segmentation (Kseg) scheme, enables extended support for
large-kernel convolutions, significantly reducing the storage requirements for
overlapped data. Moreover, based on the analysis of typical block structures in
emerging CNNs, vertical-fused (VF) and horizontal-fused (HF) methods are
developed to optimize CNN deployments from both computation and transmission
perspectives. The proposed hardware accelerator, evaluated on Intel Arria 10
FPGA, achieves up to 3.91 times better DSP efficiency than prior art on the
same network. Particularly, it demonstrates efficient support for large-kernel
CNNs, achieving throughputs of 169.68 GOPS and 244.55 GOPS for RepLKNet-31 and
PyConvResNet-50, respectively, both of which are implemented on hardware for
the first time.</div><div><a href='http://arxiv.org/abs/2402.14307v1'>2402.14307v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12981v1")'>Beyond Inference: Performance Analysis of DNN Server Overheads for
  Computer Vision</div>
<div id='2403.12981v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T02:35:08Z</div><div>Authors: Ahmed F. AbouElhamayed, Susanne Balle, Deshanand Singh, Mohamed S. Abdelfattah</div><div style='padding-top: 10px; width: 80ex'>Deep neural network (DNN) inference has become an important part of many
data-center workloads. This has prompted focused efforts to design ever-faster
deep learning accelerators such as GPUs and TPUs. However, an end-to-end
DNN-based vision application contains more than just DNN inference, including
input decompression, resizing, sampling, normalization, and data transfer. In
this paper, we perform a thorough evaluation of computer vision inference
requests performed on a throughput-optimized serving system. We quantify the
performance impact of server overheads such as data movement, preprocessing,
and message brokers between two DNNs producing outputs at different rates. Our
empirical analysis encompasses many computer vision tasks including image
classification, segmentation, detection, depth-estimation, and more complex
processing pipelines with multiple DNNs. Our results consistently demonstrate
that end-to-end application performance can easily be dominated by data
processing and data movement functions (up to 56% of end-to-end latency in a
medium-sized image, and $\sim$ 80% impact on system throughput in a large
image), even though these functions have been conventionally overlooked in deep
learning system design. Our work identifies important performance bottlenecks
in different application scenarios, achieves 2.25$\times$ better throughput
compared to prior work, and paves the way for more holistic deep learning
system design.</div><div><a href='http://arxiv.org/abs/2403.12981v1'>2403.12981v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13640v2")'>Green AI: A Preliminary Empirical Study on Energy Consumption in DL
  Models Across Different Runtime Infrastructures</div>
<div id='2402.13640v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:18:44Z</div><div>Authors: Negar Alizadeh, Fernando Castor</div><div style='padding-top: 10px; width: 80ex'>Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime
infrastructures responsible for executing trained models on target hardware,
managing memory, data transfers, and multi-accelerator execution, if
applicable. Additionally, it is a common practice to deploy pre-trained models
on environments distinct from their native development settings. This led to
the introduction of interchange formats such as ONNX, which includes its
runtime infrastructure, and ONNX Runtime, which work as standard formats that
can be used across diverse DL frameworks and languages. Even though these
runtime infrastructures have a great impact on inference performance, no
previous paper has investigated their energy efficiency. In this study, we
monitor the energy consumption and inference time in the runtime
infrastructures of three well-known DL frameworks as well as ONNX, using three
various DL models. To have nuance in our investigation, we also examine the
impact of using different execution providers. We find out that the performance
and energy efficiency of DL are difficult to predict. One framework, MXNet,
outperforms both PyTorch and TensorFlow for the computer vision models using
batch size 1, due to efficient GPU usage and thus low CPU usage. However, batch
size 64 makes PyTorch and MXNet practically indistinguishable, while TensorFlow
is outperformed consistently. For BERT, PyTorch exhibits the best performance.
Converting the models to ONNX yields significant performance improvements in
the majority of cases. Finally, in our preliminary investigation of execution
providers, we observe that TensorRT always outperforms CUDA.</div><div><a href='http://arxiv.org/abs/2402.13640v2'>2402.13640v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07415v1")'>Context-aware Multi-Model Object Detection for Diversely Heterogeneous
  Compute Systems</div>
<div id='2402.07415v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T05:38:11Z</div><div>Authors: Justin Davis, Mehmet E. Belviranli</div><div style='padding-top: 10px; width: 80ex'>In recent years, deep neural networks (DNNs) have gained widespread adoption
for continuous mobile object detection (OD) tasks, particularly in autonomous
systems. However, a prevalent issue in their deployment is the
one-size-fits-all approach, where a single DNN is used, resulting in
inefficient utilization of computational resources. This inefficiency is
particularly detrimental in energy-constrained systems, as it degrades overall
system efficiency. We identify that, the contextual information embedded in the
input data stream (e.g. the frames in the camera feed that the OD models are
run on) could be exploited to allow a more efficient multi-model-based OD
process. In this paper, we propose SHIFT which continuously selects from a
variety of DNN-based OD models depending on the dynamically changing contextual
information and computational constraints. During this selection, SHIFT
uniquely considers multi-accelerator execution to better optimize the
energy-efficiency while satisfying the latency constraints. Our proposed
methodology results in improvements of up to 7.5x in energy usage and 2.8x in
latency compared to state-of-the-art GPU-based single model OD approaches.</div><div><a href='http://arxiv.org/abs/2402.07415v1'>2402.07415v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01226v1")'>HW-SW Optimization of DNNs for Privacy-preserving People Counting on
  Low-resolution Infrared Arrays</div>
<div id='2402.01226v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T08:45:38Z</div><div>Authors: Matteo Risso, Chen Xie, Francesco Daghero, Alessio Burrello, Seyedmorteza Mollaei, Marco Castellano, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari</div><div style='padding-top: 10px; width: 80ex'>Low-resolution infrared (IR) array sensors enable people counting
applications such as monitoring the occupancy of spaces and people flows while
preserving privacy and minimizing energy consumption. Deep Neural Networks
(DNNs) have been shown to be well-suited to process these sensor data in an
accurate and efficient manner. Nevertheless, the space of DNNs' architectures
is huge and its manual exploration is burdensome and often leads to sub-optimal
solutions. To overcome this problem, in this work, we propose a highly
automated full-stack optimization flow for DNNs that goes from neural
architecture search, mixed-precision quantization, and post-processing, down to
the realization of a new smart sensor prototype, including a Microcontroller
with a customized instruction set. Integrating these cross-layer optimizations,
we obtain a large set of Pareto-optimal solutions in the 3D-space of energy,
memory, and accuracy. Deploying such solutions on our hardware platform, we
improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x
code size reduction, and 15.38x energy reduction at iso-accuracy.</div><div><a href='http://arxiv.org/abs/2402.01226v1'>2402.01226v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07957v1")'>Efficient Post-Training Augmentation for Adaptive Inference in
  Heterogeneous and Distributed IoT Environments</div>
<div id='2403.07957v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T08:27:53Z</div><div>Authors: Max Sponner, Lorenzo Servadei, Bernd Waschneck, Robert Wille, Akash Kumar</div><div style='padding-top: 10px; width: 80ex'>Early Exit Neural Networks (EENNs) present a solution to enhance the
efficiency of neural network deployments. However, creating EENNs is
challenging and requires specialized domain knowledge, due to the large amount
of additional design choices. To address this issue, we propose an automated
augmentation flow that focuses on converting an existing model into an EENN. It
performs all required design decisions for the deployment to heterogeneous or
distributed hardware targets: Our framework constructs the EENN architecture,
maps its subgraphs to the hardware targets, and configures its decision
mechanism. To the best of our knowledge, it is the first framework that is able
to perform all of these steps.
  We evaluated our approach on a collection of Internet-of-Things and standard
image classification use cases. For a speech command detection task, our
solution was able to reduce the mean operations per inference by 59.67%. For an
ECG classification task, it was able to terminate all samples early, reducing
the mean inference energy by 74.9% and computations by 78.3%. On CIFAR-10, our
solution was able to achieve up to a 58.75% reduction in computations.
  The search on a ResNet-152 base model for CIFAR-10 took less than nine hours
on a laptop CPU. Our proposed approach enables the creation of EENN optimized
for IoT environments and can reduce the inference cost of Deep Learning
applications on embedded and fog platforms, while also significantly reducing
the search cost - making it more accessible for scientists and engineers in
industry and research. The low search cost improves the accessibility of EENNs,
with the potential to improve the efficiency of neural networks in a wide range
of practical applications.</div><div><a href='http://arxiv.org/abs/2403.07957v1'>2403.07957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16757v1")'>SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond
  the Memory Budget</div>
<div id='2401.16757v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T05:29:49Z</div><div>Authors: Kun Wang, Jiani Cao, Zimu Zhou, Zhenjiang Li</div><div style='padding-top: 10px; width: 80ex'>Executing deep neural networks (DNNs) on edge artificial intelligence (AI)
devices enables various autonomous mobile computing applications. However, the
memory budget of edge AI devices restricts the number and complexity of DNNs
allowed in such applications. Existing solutions, such as model compression or
cloud offloading, reduce the memory footprint of DNN inference at the cost of
decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN
into blocks and swap them in and out in order, such that large DNNs can execute
within a small memory budget. Nevertheless, naive swapping on edge AI devices
induces significant delays due to the redundant memory operations in the DNN
development ecosystem for edge AI devices. To this end, we develop SwapNet, an
efficient DNN block swapping middleware for edge AI devices. We systematically
eliminate the unnecessary memory operations during block swapping while
retaining compatible with the deep learning frameworks, GPU backends, and
hardware architectures of edge AI devices. We further showcase the utility of
SwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference
tasks in three applications demonstrate that SwapNet achieves almost the same
latency as the case with sufficient memory even when DNNs demand 2.32x to 5.81x
memory beyond the available budget. The design of SwapNet also provides novel
and feasible insights for deploying large language models (LLMs) on edge AI
devices in the future.</div><div><a href='http://arxiv.org/abs/2401.16757v1'>2401.16757v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14290v1")'>Exploring Green AI for Audio Deepfake Detection</div>
<div id='2403.14290v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T10:54:21Z</div><div>Authors: Subhajit Saha, Md Sahidullah, Swagatam Das</div><div style='padding-top: 10px; width: 80ex'>The state-of-the-art audio deepfake detectors leveraging deep neural networks
exhibit impressive recognition performance. Nonetheless, this advantage is
accompanied by a significant carbon footprint. This is mainly due to the use of
high-performance computing with accelerators and high training time. Studies
show that average deep NLP model produces around 626k lbs of
CO\textsubscript{2} which is equivalent to five times of average US car
emission at its lifetime. This is certainly a massive threat to the
environment. To tackle this challenge, this study presents a novel framework
for audio deepfake detection that can be seamlessly trained using standard CPU
resources. Our proposed framework utilizes off-the-shelve self-supervised
learning (SSL) based models which are pre-trained and available in public
repositories. In contrast to existing methods that fine-tune SSL models and
employ additional deep neural networks for downstream tasks, we exploit
classical machine learning algorithms such as logistic regression and shallow
neural networks using the SSL embeddings extracted using the pre-trained model.
Our approach shows competitive results compared to the commonly used
high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA
dataset, we achieve a 0.90\% equal error rate (EER) with less than 1k trainable
model parameters. To encourage further research in this direction and support
reproducible results, the Python code will be made publicly accessible
following acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-</div><div><a href='http://arxiv.org/abs/2403.14290v1'>2403.14290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17249v1")'>Deep Learning-Based Speech and Vision Synthesis to Improve Phishing
  Attack Detection through a Multi-layer Adaptive Framework</div>
<div id='2402.17249v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T06:47:52Z</div><div>Authors: Tosin Ige, Christopher Kiekintveld, Aritran Piplai</div><div style='padding-top: 10px; width: 80ex'>The ever-evolving ways attacker continues to im prove their phishing
techniques to bypass existing state-of-the-art phishing detection methods pose
a mountain of challenges to researchers in both industry and academia research
due to the inability of current approaches to detect complex phishing attack.
Thus, current anti-phishing methods remain vulnerable to complex phishing
because of the increasingly sophistication tactics adopted by attacker coupled
with the rate at which new tactics are being developed to evade detection. In
this research, we proposed an adaptable framework that combines Deep learning
and Randon Forest to read images, synthesize speech from deep-fake videos, and
natural language processing at various predictions layered to significantly
increase the performance of machine learning models for phishing attack
detection.</div><div><a href='http://arxiv.org/abs/2402.17249v1'>2402.17249v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11778v1")'>Towards the Development of a Real-Time Deepfake Audio Detection System
  in Communication Platforms</div>
<div id='2403.11778v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:35:10Z</div><div>Authors: Jonat John Mathew, Rakin Ahsan, Sae Furukawa, Jagdish Gautham Krishna Kumar, Huzaifa Pallan, Agamjeet Singh Padda, Sara Adamski, Madhu Reddiboina, Arjun Pankajakshan</div><div style='padding-top: 10px; width: 80ex'>Deepfake audio poses a rising threat in communication platforms,
necessitating real-time detection for audio stream integrity. Unlike
traditional non-real-time approaches, this study assesses the viability of
employing static deepfake audio detection models in real-time communication
platforms. An executable software is developed for cross-platform
compatibility, enabling real-time execution. Two deepfake audio detection
models based on Resnet and LCNN architectures are implemented using the
ASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof
2019 challenge baselines. The study proposes strategies and frameworks for
enhancing these models, paving the way for real-time deepfake audio detection
in communication platforms. This work contributes to the advancement of audio
stream security, ensuring robust detection capabilities in dynamic, real-time
communication scenarios.</div><div><a href='http://arxiv.org/abs/2403.11778v1'>2403.11778v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07545v1")'>TransAxx: Efficient Transformers with Approximate Computing</div>
<div id='2402.07545v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T10:16:05Z</div><div>Authors: Dimitrios Danopoulos, Georgios Zervakis, Dimitrios Soudris, J√∂rg Henkel</div><div style='padding-top: 10px; width: 80ex'>Vision Transformer (ViT) models which were recently introduced by the
transformer architecture have shown to be very competitive and often become a
popular alternative to Convolutional Neural Networks (CNNs). However, the high
computational requirements of these models limit their practical applicability
especially on low-power devices. Current state-of-the-art employs approximate
multipliers to address the highly increased compute demands of DNN accelerators
but no prior research has explored their use on ViT models. In this work we
propose TransAxx, a framework based on the popular PyTorch library that enables
fast inherent support for approximate arithmetic to seamlessly evaluate the
impact of approximate computing on DNNs such as ViT models. Using TransAxx we
analyze the sensitivity of transformer models on the ImageNet dataset to
approximate multiplications and perform approximate-aware finetuning to regain
accuracy. Furthermore, we propose a methodology to generate approximate
accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS)
algorithm to efficiently search the space of possible configurations using a
hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy
of our methodology in achieving significant trade-offs between accuracy and
power, resulting in substantial gains without compromising on performance.</div><div><a href='http://arxiv.org/abs/2402.07545v1'>2402.07545v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02721v1")'>A Cost-Efficient FPGA Implementation of Tiny Transformer Model using
  Neural ODE</div>
<div id='2401.02721v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T09:32:39Z</div><div>Authors: Ikumi Okubo, Keisuke Sugiura, Hiroki Matsutani</div><div style='padding-top: 10px; width: 80ex'>Transformer is an emerging neural network model with attention mechanism. It
has been adopted to various tasks and achieved a favorable accuracy compared to
CNNs and RNNs. While the attention mechanism is recognized as a general-purpose
component, many of the Transformer models require a significant number of
parameters compared to the CNN-based ones. To mitigate the computational
complexity, recently, a hybrid approach has been proposed, which uses ResNet as
a backbone architecture and replaces a part of its convolution layers with an
MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly
reduce the parameter size of such models by using Neural ODE (Ordinary
Differential Equation) as a backbone architecture instead of ResNet. The
proposed hybrid model reduces the parameter size by 94.6% compared to the
CNN-based ones without degrading the accuracy. We then deploy the proposed
model on a modest-sized FPGA device for edge computing. To further reduce FPGA
resource utilization, we quantize the model following QAT (Quantization Aware
Training) scheme instead of PTQ (Post Training Quantization) to suppress the
accuracy loss. As a result, an extremely lightweight Transformer-based model
can be implemented on resource-limited FPGAs. The weights of the feature
extraction network are stored on-chip to minimize the memory transfer overhead,
allowing faster inference. By eliminating the overhead of memory transfers,
inference can be executed seamlessly, leading to accelerated inference. The
proposed FPGA implementation achieves 12.8x speedup and 9.21x energy efficiency
compared to ARM Cortex-A53 CPU.</div><div><a href='http://arxiv.org/abs/2401.02721v1'>2401.02721v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07542v1")'>A Survey of Vision Transformers in Autonomous Driving: Current Trends
  and Future Directions</div>
<div id='2403.07542v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:29:40Z</div><div>Authors: Quoc-Vinh Lai-Dang</div><div style='padding-top: 10px; width: 80ex'>This survey explores the adaptation of visual transformer models in
Autonomous Driving, a transition inspired by their success in Natural Language
Processing. Surpassing traditional Recurrent Neural Networks in tasks like
sequential image processing and outperforming Convolutional Neural Networks in
global context capture, as evidenced in complex scene recognition, Transformers
are gaining traction in computer vision. These capabilities are crucial in
Autonomous Driving for real-time, dynamic visual scene processing. Our survey
provides a comprehensive overview of Vision Transformer applications in
Autonomous Driving, focusing on foundational concepts such as self-attention,
multi-head attention, and encoder-decoder architecture. We cover applications
in object detection, segmentation, pedestrian detection, lane detection, and
more, comparing their architectural merits and limitations. The survey
concludes with future research directions, highlighting the growing role of
Vision Transformers in Autonomous Driving.</div><div><a href='http://arxiv.org/abs/2403.07542v1'>2403.07542v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11362v1")'>Exploiting T-norms for Deep Learning in Autonomous Driving</div>
<div id='2402.11362v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T18:51:21Z</div><div>Authors: Mihaela CƒÉtƒÉlina Stoian, Eleonora Giunchiglia, Thomas Lukasiewicz</div><div style='padding-top: 10px; width: 80ex'>Deep learning has been at the core of the autonomous driving field
development, due to the neural networks' success in finding patterns in raw
data and turning them into accurate predictions. Moreover, recent
neuro-symbolic works have shown that incorporating the available background
knowledge about the problem at hand in the loss function via t-norms can
further improve the deep learning models' performance. However, t-norm-based
losses may have very high memory requirements and, thus, they may be impossible
to apply in complex application domains like autonomous driving. In this paper,
we show how it is possible to define memory-efficient t-norm-based losses,
allowing for exploiting t-norms for the task of event detection in autonomous
driving. We conduct an extensive experimental analysis on the ROAD-R dataset
and show (i) that our proposal can be implemented and run on GPUs with less
than 25 GiB of available memory, while standard t-norm-based losses are
estimated to require more than 100 GiB, far exceeding the amount of memory
normally available, (ii) that t-norm-based losses improve performance,
especially when limited labelled data are available, and (iii) that
t-norm-based losses can further improve performance when exploited on both
labelled and unlabelled data.</div><div><a href='http://arxiv.org/abs/2402.11362v1'>2402.11362v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15584v1")'>State Space Models for Event Cameras</div>
<div id='2402.15584v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T19:51:55Z</div><div>Authors: Nikola Zubiƒá, Mathias Gehrig, Davide Scaramuzza</div><div style='padding-top: 10px; width: 80ex'>Today, state-of-the-art deep neural networks that process event-camera data
first convert a temporal window of events into dense, grid-like input
representations. As such, they exhibit poor generalizability when deployed at
higher inference frequencies (i.e., smaller temporal windows) than the ones
they were trained on. We address this challenge by introducing state-space
models (SSMs) with learnable timescale parameters to event-based vision. This
design adapts to varying frequencies without the need to retrain the network at
different frequencies. Additionally, we investigate two strategies to
counteract aliasing effects when deploying the model at higher frequencies. We
comprehensively evaluate our approach against existing methods based on RNN and
Transformer architectures across various benchmarks, including Gen1 and 1 Mpx
event camera datasets. Our results demonstrate that SSM-based models train 33%
faster and also exhibit minimal performance degradation when tested at higher
frequencies than the training input. Traditional RNN and Transformer models
exhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.31
mAP, highlighting the effectiveness of SSMs in event-based vision tasks.</div><div><a href='http://arxiv.org/abs/2402.15584v1'>2402.15584v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01393v2")'>ALERT-Transformer: Bridging Asynchronous and Synchronous Machine
  Learning for Real-Time Event-based Spatio-Temporal Data</div>
<div id='2402.01393v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:17:19Z</div><div>Authors: Carmen Martin-Turrero, Maxence Bouvier, Manuel Breitenstein, Pietro Zanuttigh, Vincent Parret</div><div style='padding-top: 10px; width: 80ex'>We seek to enable classic processing of continuous ultra-sparse
spatiotemporal data generated by event-based sensors with dense machine
learning models. We propose a novel hybrid pipeline composed of asynchronous
sensing and synchronous processing that combines several ideas: (1) an
embedding based on PointNet models -- the ALERT module -- that can continuously
integrate new and dismiss old events thanks to a leakage mechanism, (2) a
flexible readout of the embedded data that allows to feed any downstream model
with always up-to-date features at any sampling rate, (3) exploiting the input
sparsity in a patch-based approach inspired by Vision Transformer to optimize
the efficiency of the method. These embeddings are then processed by a
transformer model trained for object and gesture recognition. Using this
approach, we achieve performances at the state-of-the-art with a lower latency
than competitors. We also demonstrate that our asynchronous model can operate
at any desired sampling rate.</div><div><a href='http://arxiv.org/abs/2402.01393v2'>2402.01393v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02430v2")'>Exploiting Low-level Representations for Ultra-Fast Road Segmentation</div>
<div id='2402.02430v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:59:18Z</div><div>Authors: Huan Zhou, Feng Xue, Yucong Li, Shi Gong, Yiqun Li, Yu Zhou</div><div style='padding-top: 10px; width: 80ex'>Achieving real-time and accuracy on embedded platforms has always been the
pursuit of road segmentation methods. To this end, they have proposed many
lightweight networks. However, they ignore the fact that roads are "stuff"
(background or environmental elements) rather than "things" (specific
identifiable objects), which inspires us to explore the feasibility of
representing roads with low-level instead of high-level features. Surprisingly,
we find that the primary stage of mainstream network models is sufficient to
represent most pixels of the road for segmentation. Motivated by this, we
propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg).
Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail
branch is firstly designed to extract low-level feature representation for the
road by the first stage of ResNet-18. To suppress texture-less regions mistaken
as the road in the low-level feature, the context semantic branch is then
designed to extract the context feature in a fast manner. To this end, in the
second branch, we asymmetrically downsample the input image and design an
aggregation module to achieve comparable receptive fields to the third stage of
ResNet-18 but with less time consumption. Finally, to segment the road from the
low-level feature, a selective fusion module is proposed to calculate
pixel-wise attention between the low-level representation and context feature,
and suppress the non-road low-level response by this attention. On KITTI-Road,
LFD-RoadSeg achieves a maximum F1-measure (MaxF) of 95.21% and an average
precision of 93.71%, while reaching 238 FPS on a single TITAN Xp and 54 FPS on
a Jetson TX2, all with a compact model size of just 936k parameters. The source
code is available at https://github.com/zhouhuan-hust/LFD-RoadSeg.</div><div><a href='http://arxiv.org/abs/2402.02430v2'>2402.02430v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02286v1")'>Multi-Level Feature Aggregation and Recursive Alignment Network for
  Real-Time Semantic Segmentation</div>
<div id='2402.02286v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T22:51:17Z</div><div>Authors: Yanhua Zhang, Ke Zhang, Jingyu Wang, Yulin Wu, Wuwei Wang</div><div style='padding-top: 10px; width: 80ex'>Real-time semantic segmentation is a crucial research for real-world
applications. However, many methods lay particular emphasis on reducing the
computational complexity and model size, while largely sacrificing the
accuracy. In some scenarios, such as autonomous navigation and driver
assistance system, accuracy and speed are equally important. To tackle this
problem, we propose a novel Multi-level Feature Aggregation and Recursive
Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at
real-time inference speed. We employ ResNet-18 as the backbone to ensure
efficiency, and propose three core components to compensate for the reduced
model capacity due to the shallow backbone. Specifically, we first design
Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical
features in the encoder to each scale to benefit subsequent spatial alignment
and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by
combining the flow-based alignment module with recursive upsampling
architecture for accurate and efficient spatial alignment between multi-scale
score maps. Finally, the Adaptive Scores Fusion Module (ASFM) is proposed to
adaptively fuse multi-scale scores so that the final prediction can favor
objects of multiple scales. Comprehensive experiments on three benchmark
datasets including Cityscapes, CamVid and PASCAL-Context show the effectiveness
and efficiency of our method. In particular, we achieve a better balance
between speed and accuracy than state-of-the-art real-time methods on
Cityscapes and CamVid datasets. Code is available at:
https://github.com/Yanhua-Zhang/MFARANet.</div><div><a href='http://arxiv.org/abs/2402.02286v1'>2402.02286v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12997v1")'>A Multi-Task Oriented Semantic Communication Framework for Autonomous
  Vehicles</div>
<div id='2403.12997v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:04:24Z</div><div>Authors: Eslam Eldeeb, Mohammad Shehab, Hirley Alves</div><div style='padding-top: 10px; width: 80ex'>Task-oriented semantic communication is an emerging technology that transmits
only the relevant semantics of a message instead of the whole message to
achieve a specific task. It reduces latency, compresses the data, and is more
robust in low SNR scenarios. This work presents a multi-task-oriented semantic
communication framework for connected and autonomous vehicles (CAVs). We
propose a convolutional autoencoder (CAE) that performs the semantic encoding
of the road traffic signs. These encoded images are then transmitted from one
CAV to another CAV through satellite in challenging weather conditions where
visibility is impaired. In addition, we propose task-oriented semantic decoders
for image reconstruction and classification tasks. Simulation results show that
the proposed framework outperforms the conventional schemes, such as QAM-16,
regarding the reconstructed image's similarity and the classification's
accuracy. In addition, it can save up to 89 % of the bandwidth by sending fewer
bits.</div><div><a href='http://arxiv.org/abs/2403.12997v1'>2403.12997v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08251v1")'>Object Detection in Thermal Images Using Deep Learning for Unmanned
  Aerial Vehicles</div>
<div id='2402.08251v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T06:40:55Z</div><div>Authors: Minh Dang Tu, Kieu Trang Le, Manh Duong Phung</div><div style='padding-top: 10px; width: 80ex'>This work presents a neural network model capable of recognizing small and
tiny objects in thermal images collected by unmanned aerial vehicles. Our model
consists of three parts, the backbone, the neck, and the prediction head. The
backbone is developed based on the structure of YOLOv5 combined with the use of
a transformer encoder at the end. The neck includes a BI-FPN block combined
with the use of a sliding window and a transformer to increase the information
fed into the prediction head. The prediction head carries out the detection by
evaluating feature maps with the Sigmoid function. The use of transformers with
attention and sliding windows increases recognition accuracy while keeping the
model at a reasonable number of parameters and computation requirements for
embedded systems. Experiments conducted on public dataset VEDAI and our
collected datasets show that our model has a higher accuracy than
state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5,
SMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that
our model achieves a real-time computation speed with a stability rate of over
90%.</div><div><a href='http://arxiv.org/abs/2402.08251v1'>2402.08251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15273v1")'>Optimized Deployment of Deep Neural Networks for Visual Pose Estimation
  on Nano-drones</div>
<div id='2402.15273v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T11:35:57Z</div><div>Authors: Matteo Risso, Francesco Daghero, Beatrice Alessandra Motetti, Daniele Jahier Pagliari, Enrico Macii, Massimo Poncino, Alessio Burrello</div><div style='padding-top: 10px; width: 80ex'>Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining
popularity due to their small size, enabling new tasks such as indoor
navigation or people monitoring. Nonetheless, their size and simple electronics
pose severe challenges in implementing advanced onboard intelligence. This work
proposes a new automatic optimization pipeline for visual pose estimation tasks
using Deep Neural Networks (DNNs). The pipeline leverages two different Neural
Architecture Search (NAS) algorithms to pursue a vast complexity-driven
exploration in the DNNs' architectural space. The obtained networks are then
deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low
power System-on-Chip leveraging a set of novel software kernels for the
efficient fused execution of critical DNN layer sequences. Our results improve
the state-of-the-art reducing inference latency by up to 3.22x at iso-error.</div><div><a href='http://arxiv.org/abs/2402.15273v1'>2402.15273v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15236v2")'>Adaptive Deep Learning for Efficient Visual Pose Estimation aboard
  Ultra-low-power Nano-drones</div>
<div id='2401.15236v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T23:04:26Z</div><div>Authors: Beatrice Alessandra Motetti, Luca Crupi, Mustafa Omer Mohammed Elamin Elshaigi, Matteo Risso, Daniele Jahier Pagliari, Daniele Palossi, Alessio Burrello</div><div style='padding-top: 10px; width: 80ex'>Sub-10cm diameter nano-drones are gaining momentum thanks to their
applicability in scenarios prevented to bigger flying drones, such as in narrow
environments and close to humans. However, their tiny form factor also brings
their major drawback: ultra-constrained memory and processors for the onboard
execution of their perception pipelines. Therefore, lightweight deep
learning-based approaches are becoming increasingly popular, stressing how
computational efficiency and energy-saving are paramount as they can make the
difference between a fully working closed-loop system and a failing one. In
this work, to maximize the exploitation of the ultra-limited resources aboard
nano-drones, we present a novel adaptive deep learning-based mechanism for the
efficient execution of a vision-based human pose estimation task. We leverage
two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different
regression performance vs. computational costs trade-offs. By combining these
CNNs with three novel adaptation strategies based on the output's temporal
consistency and on auxiliary tasks to swap the CNN being executed proactively,
we present six different systems. On a real-world dataset and the actual
nano-drone hardware, our best-performing system, compared to executing only the
bigger and most accurate SoA model, shows 28% latency reduction while keeping
the same mean absolute error (MAE), 3% MAE reduction while being iso-latency,
and the absolute peak performance, i.e., 6% better than SoA model.</div><div><a href='http://arxiv.org/abs/2401.15236v2'>2401.15236v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10213v1")'>Improving automatic detection of driver fatigue and distraction using
  machine learning</div>
<div id='2401.10213v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T06:33:46Z</div><div>Authors: Dongjiang Wu</div><div style='padding-top: 10px; width: 80ex'>Changes and advances in information technology have played an important role
in the development of intelligent vehicle systems in recent years. Driver
fatigue and distracted driving are important factors in traffic accidents.
Thus, onboard monitoring of driving behavior has become a crucial component of
advanced driver assistance systems for intelligent vehicles. In this article,
we present techniques for simultaneously detecting fatigue and distracted
driving behaviors using vision-based and machine learning-based approaches. In
driving fatigue detection, we use facial alignment networks to identify facial
feature points in the images, and calculate the distance of the facial feature
points to detect the opening and closing of the eyes and mouth. Furthermore, we
use a convolutional neural network (CNN) based on the MobileNet architecture to
identify various distracted driving behaviors. Experiments are performed on a
PC based setup with a webcam and results are demonstrated using public datasets
as well as custom datasets created for training and testing. Compared to
previous approaches, we build our own datasets and provide better results in
terms of accuracy and computation time.</div><div><a href='http://arxiv.org/abs/2401.10213v1'>2401.10213v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09415v1")'>User Identification via Free Roaming Eye Tracking Data</div>
<div id='2403.09415v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T14:04:37Z</div><div>Authors: Rishabh Vallabh Varsha Haria, Amin El Abed, Sebastian Maneth</div><div style='padding-top: 10px; width: 80ex'>We present a new dataset of "free roaming" (FR) and "targeted roaming" (TR):
a pool of 41 participants is asked to walk around a university campus (FR) or
is asked to find a particular room within a library (TR). Eye movements are
recorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz). On
this dataset we investigate the accuracy of user identification using a
previously known machine learning pipeline where a Radial Basis Function
Network (RBFN) is used as classifier. Our highest accuracies are 87.3% for FR
and 89.4% for TR. This should be compared to 95.3% which is the (corresponding)
highest accuracy we are aware of (achieved in a laboratory setting using the
"RAN" stimulus of the BioEye 2015 competition dataset). To the best of our
knowledge, our results are the first that study user identification in a non
laboratory setting; such settings are often more feasible than laboratory
settings and may include further advantages. The minimum duration of each
recording is 263s for FR and 154s for TR. Our best accuracies are obtained when
restricting to 120s and 140s for FR and TR respectively, always cut from the
end of the trajectories (both for the training and testing sessions). If we cut
the same length from the beginning, then accuracies are 12.2% lower for FR and
around 6.4% lower for TR. On the full trajectories accuracies are lower by 5%
and 52% for FR and TR. We also investigate the impact of including higher order
velocity derivatives (such as acceleration, jerk, or jounce).</div><div><a href='http://arxiv.org/abs/2403.09415v1'>2403.09415v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00564v1")'>A Single Graph Convolution Is All You Need: Efficient Grayscale Image
  Classification</div>
<div id='2402.00564v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T12:50:48Z</div><div>Authors: Jacob Fein-Ashley, Tian Ye, Sachini Wickramasinghe, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna</div><div style='padding-top: 10px; width: 80ex'>Image classifiers often rely on convolutional neural networks (CNN) for their
tasks, which are inherently more heavyweight than multilayer perceptrons
(MLPs), which can be problematic in real-time applications. Additionally, many
image classification models work on both RGB and grayscale datasets.
Classifiers that operate solely on grayscale images are much less common.
Grayscale image classification has diverse applications, including but not
limited to medical image classification and synthetic aperture radar (SAR)
automatic target recognition (ATR). Thus, we present a novel grayscale (single
channel) image classification approach using a vectorized view of images. We
exploit the lightweightness of MLPs by viewing images as a vector and reducing
our problem setting to the grayscale image classification setting. We find that
using a single graph convolutional layer batch-wise increases accuracy and
reduces variance in the performance of our model. Moreover, we develop a
customized accelerator on FPGA for the proposed model with several
optimizations to improve its performance. Our experimental results on benchmark
grayscale image datasets demonstrate the effectiveness of the proposed model,
achieving vastly lower latency (up to 16$\times$ less) and competitive or
leading performance compared to other state-of-the-art image classification
models on various domain-specific grayscale image classification datasets.</div><div><a href='http://arxiv.org/abs/2402.00564v1'>2402.00564v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13214v1")'>AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical
  Attention Network</div>
<div id='2401.13214v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T03:56:33Z</div><div>Authors: Xiaolin Ma, Junkai Cheng, Aihua Li, Yuhua Zhang, Zhilong Lin</div><div style='padding-top: 10px; width: 80ex'>Recently, methods based on deep learning have been successfully applied to
ship detection for synthetic aperture radar (SAR) images. Despite the
development of numerous ship detection methodologies, detecting small and
coastal ships remains a significant challenge due to the limited features and
clutter in coastal environments. For that, a novel adaptive multi-hierarchical
attention module (AMAM) is proposed to learn multi-scale features and
adaptively aggregate salient features from various feature layers, even in
complex environments. Specifically, we first fuse information from adjacent
feature layers to enhance the detection of smaller targets, thereby achieving
multi-scale feature enhancement. Then, to filter out the adverse effects of
complex backgrounds, we dissect the previously fused multi-level features on
the channel, individually excavate the salient regions, and adaptively
amalgamate features originating from different channels. Thirdly, we present a
novel adaptive multi-hierarchical attention network (AMANet) by embedding the
AMAM between the backbone network and the feature pyramid network (FPN).
Besides, the AMAM can be readily inserted between different frameworks to
improve object detection. Lastly, extensive experiments on two large-scale SAR
ship detection datasets demonstrate that our AMANet method is superior to
state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2401.13214v1'>2401.13214v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02687v1")'>PAHD: Perception-Action based Human Decision Making using Explainable
  Graph Neural Networks on SAR Images</div>
<div id='2401.02687v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T07:37:51Z</div><div>Authors: Sasindu Wijeratne, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna, Carl Busart</div><div style='padding-top: 10px; width: 80ex'>Synthetic Aperture Radar (SAR) images are commonly utilized in military
applications for automatic target recognition (ATR). Machine learning (ML)
methods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks
(GNN), are frequently used to identify ground-based objects, including battle
tanks, personnel carriers, and missile launchers. Determining the vehicle
class, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is
crucial, as it can help determine whether the target object is an ally or an
enemy. While the ML algorithm provides feedback on the recognized target, the
final decision is left to the commanding officers. Therefore, providing
detailed information alongside the identified target can significantly impact
their actions. This detailed information includes the SAR image features that
contributed to the classification, the classification confidence, and the
probability of the identified object being classified as a different object
type or class. We propose a GNN-based ATR framework that provides the final
classified class and outputs the detailed information mentioned above. This is
the first study to provide a detailed analysis of the classification class,
making final decisions more straightforward. Moreover, our GNN framework
achieves an overall accuracy of 99.2\% when evaluated on the MSTAR dataset,
improving over previous state-of-the-art GNN methods.</div><div><a href='http://arxiv.org/abs/2401.02687v1'>2401.02687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08645v1")'>Peeking Behind the Curtains of Residual Learning</div>
<div id='2402.08645v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:24:10Z</div><div>Authors: Tunhou Zhang, Feng Yan, Hai Li, Yiran Chen</div><div style='padding-top: 10px; width: 80ex'>The utilization of residual learning has become widespread in deep and
scalable neural nets. However, the fundamental principles that contribute to
the success of residual learning remain elusive, thus hindering effective
training of plain nets with depth scalability. In this paper, we peek behind
the curtains of residual learning by uncovering the "dissipating inputs"
phenomenon that leads to convergence failure in plain neural nets: the input is
gradually compromised through plain layers due to non-linearities, resulting in
challenges of learning feature representations. We theoretically demonstrate
how plain neural nets degenerate the input to random noise and emphasize the
significance of a residual connection that maintains a better lower bound of
surviving neurons as a solution. With our theoretical discoveries, we propose
"The Plain Neural Net Hypothesis" (PNNH) that identifies the internal path
across non-linear layers as the most critical part in residual learning, and
establishes a paradigm to support the training of deep plain neural nets devoid
of residual connections. We thoroughly evaluate PNNH-enabled CNN architectures
and Transformers on popular vision benchmarks, showing on-par accuracy, up to
0.3% higher training throughput, and 2x better parameter efficiency compared to
ResNets and vision Transformers.</div><div><a href='http://arxiv.org/abs/2402.08645v1'>2402.08645v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14139v2")'>NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning</div>
<div id='2402.14139v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T21:33:07Z</div><div>Authors: Dhananjay Saikumar, Blesson Varghese</div><div style='padding-top: 10px; width: 80ex'>Efficient on-device Convolutional Neural Network (CNN) training in
resource-constrained mobile and edge environments is an open challenge.
Backpropagation is the standard approach adopted, but it is GPU memory
intensive due to its strong inter-layer dependencies that demand intermediate
activations across the entire CNN model to be retained in GPU memory. This
necessitates smaller batch sizes to make training possible within the available
GPU memory budget, but in turn, results in substantially high and impractical
training time. We introduce NeuroFlux, a novel CNN training system tailored for
memory-constrained scenarios. We develop two novel opportunities: firstly,
adaptive auxiliary networks that employ a variable number of filters to reduce
GPU memory usage, and secondly, block-specific adaptive batch sizes, which not
only cater to the GPU memory constraints but also accelerate the training
process. NeuroFlux segments a CNN into blocks based on GPU memory usage and
further attaches an auxiliary network to each layer in these blocks. This
disrupts the typical layer dependencies under a new training paradigm -
$\textit{`adaptive local learning'}$. Moreover, NeuroFlux adeptly caches
intermediate activations, eliminating redundant forward passes over previously
trained blocks, further accelerating the training process. The results are
twofold when compared to Backpropagation: on various hardware platforms,
NeuroFlux demonstrates training speed-ups of 2.3$\times$ to 6.1$\times$ under
stringent GPU memory budgets, and NeuroFlux generates streamlined models that
have 10.9$\times$ to 29.4$\times$ fewer parameters.</div><div><a href='http://arxiv.org/abs/2402.14139v2'>2402.14139v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11204v1")'>Partitioned Neural Network Training via Synthetic Intermediate Labels</div>
<div id='2403.11204v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T13:06:29Z</div><div>Authors: Cevat Volkan Karadaƒü, Nezih Topaloƒülu</div><div style='padding-top: 10px; width: 80ex'>The proliferation of extensive neural network architectures, particularly
deep learning models, presents a challenge in terms of resource-intensive
training. GPU memory constraints have become a notable bottleneck in training
such sizable models. Existing strategies, including data parallelism, model
parallelism, pipeline parallelism, and fully sharded data parallelism, offer
partial solutions. Model parallelism, in particular, enables the distribution
of the entire model across multiple GPUs, yet the ensuing data communication
between these partitions slows down training. Additionally, the substantial
memory overhead required to store auxiliary parameters on each GPU compounds
computational demands. Instead of using the entire model for training, this
study advocates partitioning the model across GPUs and generating synthetic
intermediate labels to train individual segments. These labels, produced
through a random process, mitigate memory overhead and computational load. This
approach results in a more efficient training process that minimizes data
communication while maintaining model accuracy. To validate this method, a
6-layer fully connected neural network is partitioned into two parts and its
performance is assessed on the extended MNIST dataset. Experimental results
indicate that the proposed approach achieves similar testing accuracies to
conventional training methods, while significantly reducing memory and
computational requirements. This work contributes to mitigating the
resource-intensive nature of training large neural networks, paving the way for
more efficient deep learning model development.</div><div><a href='http://arxiv.org/abs/2403.11204v1'>2403.11204v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13441v1")'>PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory
  Access Prediction Models</div>
<div id='2402.13441v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T00:24:34Z</div><div>Authors: Neelesh Gupta, Pengmiao Zhang, Rajgopal Kannan, Viktor Prasanna</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks (DNNs) have proven to be effective models for accurate
Memory Access Prediction (MAP), a critical task in mitigating memory latency
through data prefetching. However, existing DNN-based MAP models suffer from
the challenges such as significant physical storage space and poor inference
latency, primarily due to their large number of parameters. These limitations
render them impractical for deployment in real-world scenarios. In this paper,
we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to
compress MAP models while maintaining the prediction performance. The PaCKD
approach encompasses three steps: clustering memory access sequences into
distinct partitions involving similar patterns, training large pattern-specific
teacher models for memory access prediction for each partition, and training a
single lightweight student model by distilling the knowledge from the trained
pattern-specific teachers. We evaluate our approach on LSTM, MLP-Mixer, and
ResNet models, as they exhibit diverse structures and are widely used for image
classification tasks in order to test their effectiveness in four widely used
graph applications. Compared to the teacher models with 5.406M parameters and
an F1-score of 0.4626, our student models achieve a 552$\times$ model size
compression while maintaining an F1-score of 0.4538 (with a 1.92% performance
drop). Our approach yields an 8.70% higher result compared to student models
trained with standard knowledge distillation and an 8.88% higher result
compared to student models trained without any form of knowledge distillation.</div><div><a href='http://arxiv.org/abs/2402.13441v1'>2402.13441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17318v1")'>Scaling Supervised Local Learning with Augmented Auxiliary Networks</div>
<div id='2402.17318v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T08:50:45Z</div><div>Authors: Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks are typically trained using global error signals that
backpropagate (BP) end-to-end, which is not only biologically implausible but
also suffers from the update locking problem and requires huge memory
consumption. Local learning, which updates each layer independently with a
gradient-isolated auxiliary network, offers a promising alternative to address
the above problems. However, existing local learning methods are confronted
with a large accuracy gap with the BP counterpart, particularly for large-scale
networks. This is due to the weak coupling between local layers and their
subsequent network layers, as there is no gradient communication across layers.
To tackle this issue, we put forward an augmented local learning method, dubbed
AugLocal. AugLocal constructs each hidden layer's auxiliary network by
uniformly selecting a small subset of layers from its subsequent network layers
to enhance their synergy. We also propose to linearly reduce the depth of
auxiliary networks as the hidden layer goes deeper, ensuring sufficient network
capacity while reducing the computational cost of auxiliary networks. Our
extensive experiments on four image classification datasets (i.e., CIFAR-10,
SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up
to tens of local layers with a comparable accuracy to BP-trained networks while
reducing GPU memory usage by around 40%. The proposed AugLocal method,
therefore, opens up a myriad of opportunities for training high-performance
deep neural networks on resource-constrained platforms.Code is available at
https://github.com/ChenxiangMA/AugLocal.</div><div><a href='http://arxiv.org/abs/2402.17318v1'>2402.17318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12320v1")'>Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for
  Boosting Neural Network Training</div>
<div id='2403.12320v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:23:50Z</div><div>Authors: Zeliang Zhang, Jinyang Jiang, Zhuo Liu, Susan Liang, Yijie Peng, Chenliang Xu</div><div style='padding-top: 10px; width: 80ex'>Efficient and biologically plausible alternatives to backpropagation in
neural network training remain a challenge due to issues such as high
computational complexity and additional assumptions about neural networks,
which limit scalability to deeper networks. The likelihood ratio method offers
a promising gradient estimation strategy but is constrained by significant
memory consumption, especially when deploying multiple copies of data to reduce
estimation variance. In this paper, we introduce an approximation technique for
the likelihood ratio (LR) method to alleviate computational and memory demands
in gradient estimation. By exploiting the natural parallelism during the
backward pass using LR, we further provide a high-performance training
strategy, which pipelines both the forward and backward pass, to make it more
suitable for the computation on specialized hardware. Extensive experiments
demonstrate the effectiveness of the approximation technique in neural network
training. This work underscores the potential of the likelihood ratio method in
achieving high-performance neural network training, suggesting avenues for
further exploration.</div><div><a href='http://arxiv.org/abs/2403.12320v1'>2403.12320v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03293v1")'>Flora: Low-Rank Adapters Are Secretly Gradient Compressors</div>
<div id='2402.03293v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:50:39Z</div><div>Authors: Yongchang Hao, Yanshuai Cao, Lili Mou</div><div style='padding-top: 10px; width: 80ex'>Despite large neural networks demonstrating remarkable abilities to complete
different tasks, they require excessive memory usage to store the optimization
states for training. To alleviate this, the low-rank adaptation (LoRA) is
proposed to reduce the optimization states by training fewer parameters.
However, LoRA restricts overall weight update matrices to be low-rank, limiting
the model performance. In this work, we investigate the dynamics of LoRA and
identify that it can be approximated by a random projection. Based on this
observation, we propose Flora, which is able to achieve high-rank updates by
resampling the projection matrices while enjoying the sublinear space
complexity of optimization states. We conduct experiments across different
tasks and model architectures to verify the effectiveness of our approach.</div><div><a href='http://arxiv.org/abs/2402.03293v1'>2402.03293v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14958v1")'>Adapprox: Adaptive Approximation in Adam Optimization via Randomized
  Low-Rank Matrices</div>
<div id='2403.14958v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T05:23:31Z</div><div>Authors: Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger K√∂lker, Zhefeng Wang, Xiaoming Yuan</div><div style='padding-top: 10px; width: 80ex'>As deep learning models exponentially increase in size, optimizers such as
Adam encounter significant memory consumption challenges due to the storage of
first and second moment data. Current memory-efficient methods like Adafactor
and CAME often compromise accuracy with their matrix factorization techniques.
Addressing this, we introduce Adapprox, a novel approach that employs
randomized low-rank matrix approximation for a more effective and accurate
approximation of Adam's second moment. Adapprox features an adaptive rank
selection mechanism, finely balancing accuracy and memory efficiency, and
includes an optional cosine similarity guidance strategy to enhance stability
and expedite convergence. In GPT-2 training and downstream tasks, Adapprox
surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings
for the 117M and 345M models, respectively, with the first moment enabled, and
further increases these savings without the first moment. Besides, it enhances
convergence speed and improves downstream task performance relative to its
counterparts.</div><div><a href='http://arxiv.org/abs/2403.14958v1'>2403.14958v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10569v1")'>Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs
  in Resource-Constrained Edge Environment</div>
<div id='2403.10569v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T19:40:58Z</div><div>Authors: Atah Nuh Mih, Alireza Rahimi, Asfia Kawnine, Francis Palma, Monica Wachowicz, Rickey Dubay, Hung Cao</div><div style='padding-top: 10px; width: 80ex'>This paper proposes an optimization of an existing Deep Neural Network (DNN)
that improves its hardware utilization and facilitates on-device training for
resource-constrained edge environments. We implement efficient parameter
reduction strategies on Xception that shrink the model size without sacrificing
accuracy, thus decreasing memory utilization during training. We evaluate our
model in two experiments: Caltech-101 image classification and PCB defect
detection and compare its performance against the original Xception and
lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the
Caltech-101 image classification show that our model has a better test accuracy
(76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than
Xception (874.6MB), and has faster training and inference times. The
lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy
and MobileNetV2 having a 58.11% test accuracy. Both lightweight models have
better memory usage than our model and Xception. On the PCB defect detection,
our model has the best test accuracy (90.30%), compared to Xception (88.10%),
EfficientNetV2B1 (55.25%), and MobileNetV2 (50.50%). MobileNetV2 has the least
average memory usage (849.4MB), followed by our model (865.8MB), then
EfficientNetV2B1 (874.8MB), and Xception has the highest (893.6MB). We further
experiment with pre-trained weights and observe that memory usage decreases
thereby showing the benefits of transfer learning. A Pareto analysis of the
models' performance shows that our optimized model architecture satisfies
accuracy and low memory utilization objectives.</div><div><a href='http://arxiv.org/abs/2403.10569v1'>2403.10569v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10522v1")'>FARe: Fault-Aware GNN Training on ReRAM-based PIM Accelerators</div>
<div id='2401.10522v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T06:56:09Z</div><div>Authors: Pratyush Dhingra, Chukwufumnanya Ogbogu, Biresh Kumar Joardar, Janardhan Rao Doppa, Ananth Kalyanaraman, Partha Pratim Pande</div><div style='padding-top: 10px; width: 80ex'>Resistive random-access memory (ReRAM)-based processing-in-memory (PIM)
architecture is an attractive solution for training Graph Neural Networks
(GNNs) on edge platforms. However, the immature fabrication process and limited
write endurance of ReRAMs make them prone to hardware faults, thereby limiting
their widespread adoption for GNN training. Further, the existing
fault-tolerant solutions prove inadequate for effectively training GNNs in the
presence of faults. In this paper, we propose a fault-aware framework referred
to as FARe that mitigates the effect of faults during GNN training. FARe
outperforms existing approaches in terms of both accuracy and timing overhead.
Experimental results demonstrate that FARe framework can restore GNN test
accuracy by 47.6% on faulty ReRAM hardware with a ~1% timing overhead compared
to the fault-free counterpart.</div><div><a href='http://arxiv.org/abs/2401.10522v1'>2401.10522v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02936v1")'>AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN
  Accelerators</div>
<div id='2403.02936v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:03:31Z</div><div>Authors: Mahdi Taheri, Natalia Cherezova, Samira Nazari, Ahsan Rafiq, Ali Azarpeyvand, Tara Ghasempouri, Masoud Daneshtalab, Jaan Raik, Maksim Jenihhin</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose an architecture of a novel adaptive fault-tolerant
approximate multiplier tailored for ASIC-based DNN accelerators.</div><div><a href='http://arxiv.org/abs/2403.02936v1'>2403.02936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02088v1")'>Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe</div>
<div id='2401.02088v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T06:23:22Z</div><div>Authors: Mincong Huang, Chao Wang, Chi Ma, Yineng Zhang, Peng Zhang, Lei Yu</div><div style='padding-top: 10px; width: 80ex'>Pipeline parallelism is an essential technique in the training of large-scale
Transformer models. However, it suffers from imbalanced memory consumption,
leading to insufficient memory utilization. The BPipe technique was proposed to
address this issue and has proven effective in the GPT-3 model. Nevertheless,
our experiments have not yielded similar benefits for LLaMA training.
Additionally, BPipe only yields negligible benefits for GPT-3 training when
applying flash attention. We analyze the underlying causes of the divergent
performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel
method to estimate the performance of BPipe.</div><div><a href='http://arxiv.org/abs/2401.02088v1'>2401.02088v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05820v1")'>Implications of Noise in Resistive Memory on Deep Neural Networks for
  Image Classification</div>
<div id='2401.05820v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T10:36:45Z</div><div>Authors: Yannick Emonds, Kai Xi, Holger Fr√∂ning</div><div style='padding-top: 10px; width: 80ex'>Resistive memory is a promising alternative to SRAM, but is also an
inherently unstable device that requires substantial effort to ensure correct
read and write operations. To avoid the associated costs in terms of area, time
and energy, the present work is concerned with exploring how much noise in
memory operations can be tolerated by image classification tasks based on
neural networks. We introduce a special noisy operator that mimics the noise in
an exemplary resistive memory unit, explore the resilience of convolutional
neural networks on the CIFAR-10 classification task, and discuss a couple of
countermeasures to improve this resilience.</div><div><a href='http://arxiv.org/abs/2401.05820v1'>2401.05820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06925v1")'>Simplicity Bias of Transformers to Learn Low Sensitivity Functions</div>
<div id='2403.06925v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:12:09Z</div><div>Authors: Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan</div><div style='padding-top: 10px; width: 80ex'>Transformers achieve state-of-the-art accuracy and robustness across many
tasks, but an understanding of the inductive biases that they have and how
those biases are different from other neural network architectures remains
elusive. Various neural network architectures such as fully connected networks
have been found to have a simplicity bias towards simple functions of the data;
one version of this simplicity bias is a spectral bias to learn simple
functions in the Fourier space. In this work, we identify the notion of
sensitivity of the model to random changes in the input as a notion of
simplicity bias which provides a unified metric to explain the simplicity and
spectral bias of transformers across different data modalities. We show that
transformers have lower sensitivity than alternative architectures, such as
LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that
low-sensitivity bias correlates with improved robustness; furthermore, it can
also be used as an efficient intervention to further improve the robustness of
transformers.</div><div><a href='http://arxiv.org/abs/2403.06925v1'>2403.06925v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01524v1")'>HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation</div>
<div id='2402.01524v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T16:10:29Z</div><div>Authors: Pawe≈Ç Batorski, Dawid Malarz, Marcin Przewiƒô≈∫likowski, Marcin Mazur, S≈Çawomir Tadeja, Przemys≈Çaw Spurek</div><div style='padding-top: 10px; width: 80ex'>Neural radiance fields (NeRFs) are a widely accepted standard for
synthesizing new 3D object views from a small number of base images. However,
NeRFs have limited generalization properties, which means that we need to use
significant computational resources to train individual architectures for each
item we want to represent. To address this issue, we propose a few-shot
learning approach based on the hypernetwork paradigm that does not require
gradient optimization during inference. The hypernetwork gathers information
from the training data and generates an update for universal weights. As a
result, we have developed an efficient method for generating a high-quality 3D
object representation from a small number of images in a single step. This has
been confirmed by direct comparison with the state-of-the-art solutions and a
comprehensive ablation study.</div><div><a href='http://arxiv.org/abs/2402.01524v1'>2402.01524v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01058v1")'>Neural Field Classifiers via Target Encoding and Classification Loss</div>
<div id='2403.01058v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T01:20:59Z</div><div>Authors: Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun</div><div style='padding-top: 10px; width: 80ex'>Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.</div><div><a href='http://arxiv.org/abs/2403.01058v1'>2403.01058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15351v1")'>AutoMMLab: Automatically Generating Deployable Models from Language
  Instructions for Computer Vision Tasks</div>
<div id='2402.15351v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:38:19Z</div><div>Authors: Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, Wentao Liu</div><div style='padding-top: 10px; width: 80ex'>Automated machine learning (AutoML) is a collection of techniques designed to
automate the machine learning development process. While traditional AutoML
approaches have been successfully applied in several critical steps of model
development (e.g. hyperparameter optimization), there lacks a AutoML system
that automates the entire end-to-end model production workflow. To fill this
blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that
follows user's language instructions to automate the whole model production
workflow for computer vision tasks. The proposed AutoMMLab system effectively
employs LLMs as the bridge to connect AutoML and OpenMMLab community,
empowering non-expert individuals to easily build task-specific models via a
user-friendly language interface. Specifically, we propose RU-LLaMA to
understand users' request and schedule the whole pipeline, and propose a novel
LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for
the optimal hyperparameters. Experiments show that our AutoMMLab system is
versatile and covers a wide range of mainstream tasks, including
classification, detection, segmentation and keypoint estimation. We further
develop a new benchmark, called LAMP, for studying key components in the
end-to-end prompt-based model training pipeline. Code, model, and data will be
released.</div><div><a href='http://arxiv.org/abs/2402.15351v1'>2402.15351v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02887v1")'>Time-, Memory- and Parameter-Efficient Visual Adaptation</div>
<div id='2402.02887v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:55:47Z</div><div>Authors: Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab</div><div style='padding-top: 10px; width: 80ex'>As foundation models become more popular, there is a growing need to
efficiently finetune them for downstream tasks. Although numerous adaptation
methods have been proposed, they are designed to be efficient only in terms of
how many parameters are trained. They, however, typically still require
backpropagating gradients throughout the model, meaning that their
training-time and -memory cost does not reduce as significantly. We propose an
adaptation method which does not backpropagate gradients through the backbone.
We achieve this by designing a lightweight network in parallel that operates on
features from the frozen, pretrained backbone. As a result, our method is
efficient not only in terms of parameters, but also in training-time and memory
usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on
the popular VTAB benchmark, and we further show how we outperform prior works
with respect to training-time and -memory usage too. We further demonstrate the
training efficiency and scalability of our method by adapting a vision
transformer backbone of 4 billion parameters for the computationally demanding
task of video classification, without any intricate model parallelism. Here, we
outperform a prior adaptor-based method which could only scale to a 1 billion
parameter backbone, or fully-finetuning a smaller backbone, with the same GPU
and less training time.</div><div><a href='http://arxiv.org/abs/2402.02887v1'>2402.02887v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09417v2")'>Vision Mamba: Efficient Visual Representation Learning with
  Bidirectional State Space Model</div>
<div id='2401.09417v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T18:56:18Z</div><div>Authors: Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang</div><div style='padding-top: 10px; width: 80ex'>Recently the state space models (SSMs) with efficient hardware-aware designs,
i.e., the Mamba deep learning model, have shown great potential for long
sequence modeling. Meanwhile building efficient and generic vision backbones
purely upon SSMs is an appealing direction. However, representing visual data
is challenging for SSMs due to the position-sensitivity of visual data and the
requirement of global context for visual understanding. In this paper, we show
that the reliance on self-attention for visual representation learning is not
necessary and propose a new generic vision backbone with bidirectional Mamba
blocks (Vim), which marks the image sequences with position embeddings and
compresses the visual representation with bidirectional state space models. On
ImageNet classification, COCO object detection, and ADE20k semantic
segmentation tasks, Vim achieves higher performance compared to
well-established vision transformers like DeiT, while also demonstrating
significantly improved computation &amp; memory efficiency. For example, Vim is
2.8$\times$ faster than DeiT and saves 86.8% GPU memory when performing batch
inference to extract features on images with a resolution of 1248$\times$1248.
The results demonstrate that Vim is capable of overcoming the computation &amp;
memory constraints on performing Transformer-style understanding for
high-resolution images and it has great potential to be the next-generation
backbone for vision foundation models. Code is available at
https://github.com/hustvl/Vim.</div><div><a href='http://arxiv.org/abs/2401.09417v2'>2401.09417v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11110v1")'>VONet: Unsupervised Video Object Learning With Parallel U-Net Attention
  and Object-wise Sequential VAE</div>
<div id='2401.11110v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T04:13:54Z</div><div>Authors: Haonan Yu, Wei Xu</div><div style='padding-top: 10px; width: 80ex'>Unsupervised video object learning seeks to decompose video scenes into
structural object representations without any supervision from depth, optical
flow, or segmentation. We present VONet, an innovative approach that is
inspired by MONet. While utilizing a U-Net architecture, VONet employs an
efficient and effective parallel attention inference process, generating
attention masks for all slots simultaneously. Additionally, to enhance the
temporal consistency of each mask across consecutive video frames, VONet
develops an object-wise sequential VAE framework. The integration of these
innovative encoder-side techniques, in conjunction with an expressive
transformer-based decoder, establishes VONet as the leading unsupervised method
for object learning across five MOVI datasets, encompassing videos of diverse
complexities. Code is available at https://github.com/hnyu/vonet.</div><div><a href='http://arxiv.org/abs/2401.11110v1'>2401.11110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17077v1")'>Parallelized Spatiotemporal Binding</div>
<div id='2402.17077v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T23:16:34Z</div><div>Authors: Gautam Singh, Yue Wang, Jiawei Yang, Boris Ivanovic, Sungjin Ahn, Marco Pavone, Tong Che</div><div style='padding-top: 10px; width: 80ex'>While modern best practices advocate for scalable architectures that support
long-range interactions, object-centric models are yet to fully embrace these
architectures. In particular, existing object-centric models for handling
sequential inputs, due to their reliance on RNN-based implementation, show poor
stability and capacity and are slow to train on long sequences. We introduce
Parallelizable Spatiotemporal Binder or PSB, the first
temporally-parallelizable slot learning architecture for sequential inputs.
Unlike conventional RNN-based approaches, PSB produces object-centric
representations, known as slots, for all time-steps in parallel. This is
achieved by refining the initial slots across all time-steps through a fixed
number of layers equipped with causal attention. By capitalizing on the
parallelism induced by our architecture, the proposed model exhibits a
significant boost in efficiency. In experiments, we test PSB extensively as an
encoder within an auto-encoding framework paired with a wide variety of decoder
options. Compared to the state-of-the-art, our architecture demonstrates stable
training on longer sequences, achieves parallelization that results in a 60%
increase in training speed, and yields performance that is on par with or
better on unsupervised 2D and 3D object-centric scene decomposition and
understanding.</div><div><a href='http://arxiv.org/abs/2402.17077v1'>2402.17077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09506v1")'>Don't Judge by the Look: A Motion Coherent Augmentation for Video
  Recognition</div>
<div id='2403.09506v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:53:04Z</div><div>Authors: Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu</div><div style='padding-top: 10px; width: 80ex'>Current training pipelines in object recognition neglect Hue Jittering when
doing data augmentation as it not only brings appearance changes that are
detrimental to classification, but also the implementation is inefficient in
practice. In this study, we investigate the effect of hue variance in the
context of video recognition and find this variance to be beneficial since
static appearances are less important in videos that contain motion
information. Based on this observation, we propose a data augmentation method
for video recognition, named Motion Coherent Augmentation (MCA), that
introduces appearance variation in videos and implicitly encourages the model
to prioritize motion patterns, rather than static appearances. Concretely, we
propose an operation SwapMix to efficiently modify the appearance of video
samples, and introduce Variation Alignment (VA) to resolve the distribution
shift caused by SwapMix, enforcing the model to learn appearance invariant
representations. Comprehensive empirical evaluation across various
architectures and different datasets solidly validates the effectiveness and
generalization ability of MCA, and the application of VA in other augmentation
methods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.</div><div><a href='http://arxiv.org/abs/2403.09506v1'>2403.09506v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03241v1")'>FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action
  Recognition</div>
<div id='2402.03241v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:56:41Z</div><div>Authors: Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce FROSTER, an effective framework for
open-vocabulary action recognition. The CLIP model has achieved remarkable
success in a range of image-based tasks, benefiting from its strong
generalization capability stemming from pretaining on massive image-text pairs.
However, applying CLIP directly to the open-vocabulary action recognition task
is challenging due to the absence of temporal information in CLIP's
pretraining. Further, fine-tuning CLIP on action recognition datasets may lead
to overfitting and hinder its generalizability, resulting in unsatisfactory
results when dealing with unseen actions.
  To address these issues, FROSTER employs a residual feature distillation
approach to ensure that CLIP retains its generalization capability while
effectively adapting to the action recognition task. Specifically, the residual
feature distillation treats the frozen CLIP model as a teacher to maintain the
generalizability exhibited by the original CLIP and supervises the feature
learning for the extraction of video-specific features to bridge the gap
between images and videos. Meanwhile, it uses a residual sub-network for
feature distillation to reach a balance between the two distinct objectives of
learning generalizable and video-specific features.
  We extensively evaluate FROSTER on open-vocabulary action recognition
benchmarks under both base-to-novel and cross-dataset settings. FROSTER
consistently achieves state-of-the-art performance on all datasets across the
board. Project page: https://visual-ai.github.io/froster.</div><div><a href='http://arxiv.org/abs/2402.03241v1'>2402.03241v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04661v2")'>Dynamic Cross Attention for Audio-Visual Person Verification</div>
<div id='2403.04661v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T17:07:51Z</div><div>Authors: R. Gnana Praveen, Jahangir Alam</div><div style='padding-top: 10px; width: 80ex'>Although person or identity verification has been predominantly explored
using individual modalities such as face and voice, audio-visual fusion has
recently shown immense potential to outperform unimodal approaches. Audio and
visual modalities are often expected to pose strong complementary
relationships, which plays a crucial role in effective audio-visual fusion.
However, they may not always strongly complement each other, they may also
exhibit weak complementary relationships, resulting in poor audio-visual
feature representations. In this paper, we propose a Dynamic Cross-Attention
(DCA) model that can dynamically select the cross-attended or unattended
features on the fly based on the strong or weak complementary relationships,
respectively, across audio and visual modalities. In particular, a conditional
gating layer is designed to evaluate the contribution of the cross-attention
mechanism and choose cross-attended features only when they exhibit strong
complementary relationships, otherwise unattended features. Extensive
experiments are conducted on the Voxceleb1 dataset to demonstrate the
robustness of the proposed model. Results indicate that the proposed model
consistently improves the performance on multiple variants of cross-attention
while outperforming the state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.04661v2'>2403.04661v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03179v2")'>Cool-chic video: Learned video coding with 800 parameters</div>
<div id='2402.03179v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:45:38Z</div><div>Authors: Thomas Leguay, Th√©o Ladune, Pierrick Philippe, Olivier D√©forges</div><div style='padding-top: 10px; width: 80ex'>We propose a lightweight learned video codec with 900 multiplications per
decoded pixel and 800 parameters overall. To the best of our knowledge, this is
one of the neural video codecs with the lowest decoding complexity. It is built
upon the overfitted image codec Cool-chic and supplements it with an inter
coding module to leverage the video's temporal redundancies. The proposed model
is able to compress videos using both low-delay and random access
configurations and achieves rate-distortion close to AVC while out-performing
other overfitted codecs such as FFNeRV. The system is made open-source:
orange-opensource.github.io/Cool-Chic.</div><div><a href='http://arxiv.org/abs/2402.03179v2'>2402.03179v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03195v2")'>Efficient Bitrate Ladder Construction using Transfer Learning and
  Spatio-Temporal Features</div>
<div id='2401.03195v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T11:37:20Z</div><div>Authors: Ali Falahati, Mohammad Karim Safavi, Ardavan Elahi, Farhad Pakdaman, Moncef Gabbouj</div><div style='padding-top: 10px; width: 80ex'>Providing high-quality video with efficient bitrate is a main challenge in
video industry. The traditional one-size-fits-all scheme for bitrate ladders is
inefficient and reaching the best content-aware decision computationally
impractical due to extensive encodings required. To mitigate this, we propose a
bitrate and complexity efficient bitrate ladder prediction method using
transfer learning and spatio-temporal features. We propose: (1) using feature
maps from well-known pre-trained DNNs to predict rate-quality behavior with
limited training data; and (2) improving highest quality rung efficiency by
predicting minimum bitrate for top quality and using it for the top rung. The
method tested on 102 video scenes demonstrates 94.1% reduction in complexity
versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning
was thoroughly studied through four networks and ablation studies.</div><div><a href='http://arxiv.org/abs/2401.03195v2'>2401.03195v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07200v1")'>Exploring Compressed Image Representation as a Perceptual Proxy: A Study</div>
<div id='2401.07200v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T04:37:17Z</div><div>Authors: Chen-Hsiu Huang, Ja-Ling Wu</div><div style='padding-top: 10px; width: 80ex'>We propose an end-to-end learned image compression codec wherein the analysis
transform is jointly trained with an object classification task. This study
affirms that the compressed latent representation can predict human perceptual
distance judgments with an accuracy comparable to a custom-tailored DNN-based
quality metric. We further investigate various neural encoders and demonstrate
the effectiveness of employing the analysis transform as a perceptual loss
network for image tasks beyond quality judgments. Our experiments show that the
off-the-shelf neural encoder proves proficient in perceptual modeling without
needing an additional VGG network. We expect this research to serve as a
valuable reference developing of a semantic-aware and coding-efficient neural
encoder.</div><div><a href='http://arxiv.org/abs/2401.07200v1'>2401.07200v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17789v1")'>Robustly overfitting latents for flexible neural image compression</div>
<div id='2401.17789v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T12:32:17Z</div><div>Authors: Yura Perugachi-Diaz, Arwin Gansekoele, Sandjai Bhulai</div><div style='padding-top: 10px; width: 80ex'>Neural image compression has made a great deal of progress. State-of-the-art
models are based on variational autoencoders and are outperforming classical
models. Neural compression models learn to encode an image into a quantized
latent representation that can be efficiently sent to the decoder, which
decodes the quantized latent into a reconstructed image. While these models
have proven successful in practice, they lead to sub-optimal results due to
imperfect optimization and limitations in the encoder and decoder capacity.
Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the
latents of pre-trained neural image compression models. We extend this idea by
introducing SGA+, which contains three different methods that build upon SGA.
Further, we give a detailed analysis of our proposed methods, show how they
improve performance, and show that they are less sensitive to hyperparameter
choices. Besides, we show how each method can be extended to three- instead of
two-class rounding. Finally, we show how refinement of the latents with our
best-performing method improves the compression performance on the Tecnick
dataset and how it can be deployed to partly move along the rate-distortion
curve.</div><div><a href='http://arxiv.org/abs/2401.17789v1'>2401.17789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07320v1")'>Approaching Rate-Distortion Limits in Neural Compression with Lattice
  Transform Coding</div>
<div id='2403.07320v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T05:09:25Z</div><div>Authors: Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti</div><div style='padding-top: 10px; width: 80ex'>Neural compression has brought tremendous progress in designing lossy
compressors with good rate-distortion (RD) performance at low complexity. Thus
far, neural compression design involves transforming the source to a latent
vector, which is then rounded to integers and entropy coded. While this
approach has been shown to be optimal in a one-shot sense on certain sources,
we show that it is highly sub-optimal on i.i.d. sequences, and in fact always
recovers scalar quantization of the original source sequence. We demonstrate
that the sub-optimality is due to the choice of quantization scheme in the
latent space, and not the transform design. By employing lattice quantization
instead of scalar quantization in the latent space, we demonstrate that Lattice
Transform Coding (LTC) is able to recover optimal vector quantization at
various dimensions and approach the asymptotically-achievable rate-distortion
function at reasonable complexity. On general vector sources, LTC improves upon
standard neural compressors in one-shot coding performance. LTC also enables
neural compressors that perform block coding on i.i.d. vector sources, which
yields coding gain over optimal one-shot coding.</div><div><a href='http://arxiv.org/abs/2403.07320v1'>2403.07320v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12207v1")'>Rate-Distortion-Perception Tradeoff Based on the
  Conditional-Distribution Perception Measure</div>
<div id='2401.12207v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:49:56Z</div><div>Authors: Sadaf Salehkalaibar, Jun Chen, Ashish Khisti, Wei Yu</div><div style='padding-top: 10px; width: 80ex'>We study the rate-distortion-perception (RDP) tradeoff for a memoryless
source model in the asymptotic limit of large block-lengths. Our perception
measure is based on a divergence between the distributions of the source and
reconstruction sequences conditioned on the encoder output, which was first
proposed in [1], [2]. We consider the case when there is no shared randomness
between the encoder and the decoder. For the case of discrete memoryless
sources we derive a single-letter characterization of the RDP function, thus
settling a problem that remains open for the marginal metric introduced in Blau
and Michaeli [3] (with no shared randomness). Our achievability scheme is based
on lossy source coding with a posterior reference map proposed in [4]. For the
case of continuous valued sources under squared error distortion measure and
squared quadratic Wasserstein perception measure we also derive a single-letter
characterization and show that a noise-adding mechanism at the decoder suffices
to achieve the optimal representation. For the case of zero perception loss, we
show that our characterization interestingly coincides with the results for the
marginal metric derived in [5], [6] and again demonstrate that zero perception
loss can be achieved with a $3$-dB penalty in the minimum distortion. Finally
we specialize our results to the case of Gaussian sources. We derive the RDP
function for vector Gaussian sources and propose a waterfilling type solution.
We also partially characterize the RDP function for a mixture of vector
Gaussians.</div><div><a href='http://arxiv.org/abs/2401.12207v1'>2401.12207v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14849v1")'>Output-Constrained Lossy Source Coding With Application to
  Rate-Distortion-Perception Theory</div>
<div id='2403.14849v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T21:51:36Z</div><div>Authors: Li Xie, Liangyan Li, Jun Chen, Zhongshan Zhang</div><div style='padding-top: 10px; width: 80ex'>The distortion-rate function of output-constrained lossy source coding with
limited common randomness is analyzed for the special case of squared error
distortion measure. An explicit expression is obtained when both source and
reconstruction distributions are Gaussian. This further leads to a partial
characterization of the information-theoretic limit of quadratic Gaussian
rate-distortion-perception coding with the perception measure given by
Kullback-Leibler divergence or squared quadratic Wasserstein distance.</div><div><a href='http://arxiv.org/abs/2403.14849v1'>2403.14849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01176v1")'>Fundamental Limitation of Semantic Communications: Neural Estimation for
  Rate-Distortion</div>
<div id='2401.01176v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T12:10:16Z</div><div>Authors: Dongxu Li, Jianhao Huang, Chuan Huang, Xiaoqi Qin, Han Zhang, Ping Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper studies the fundamental limit of semantic communications over the
discrete memoryless channel. We consider the scenario to send a semantic source
consisting of an observation state and its corresponding semantic state, both
of which are recovered at the receiver. To derive the performance limitation,
we adopt the semantic rate-distortion function (SRDF) to study the relationship
among the minimum compression rate, observation distortion, semantic
distortion, and channel capacity. For the case with unknown semantic source
distribution, while only a set of the source samples is available, we propose a
neural-network-based method by leveraging the generative networks to learn the
semantic source distribution. Furthermore, for a special case where the
semantic state is a deterministic function of the observation, we design a
cascade neural network to estimate the SRDF. For the case with perfectly known
semantic source distribution, we propose a general Blahut-Arimoto algorithm to
effectively compute the SRDF. Finally, experimental results validate our
proposed algorithms for the scenarios with ideal Gaussian semantic source and
some practical datasets.</div><div><a href='http://arxiv.org/abs/2401.01176v1'>2401.01176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05783v1")'>Large Generative Model Assisted 3D Semantic Communication</div>
<div id='2403.05783v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T03:33:07Z</div><div>Authors: Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You</div><div style='padding-top: 10px; width: 80ex'>Semantic Communication (SC) is a novel paradigm for data transmission in 6G.
However, there are several challenges posed when performing SC in 3D scenarios:
1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain
channel estimation. To address these issues, we propose a Generative AI Model
assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor
(3DSE), which employs generative AI models, including Segment Anything Model
(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D
scenario based on user requirements. The extracted 3D semantics are represented
as multi-perspective images of the goal-oriented 3D object. Then, we present an
Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective
images, in which we use a semantic encoder with two output heads to perform
semantic encoding and mask redundant semantics in the latent semantic space,
respectively. Next, we design a conditional Generative adversarial network and
Diffusion model aided-Channel Estimation (GDCE) to estimate and refine the
Channel State Information (CSI) of physical channels. Finally, simulation
results demonstrate the advantages of the proposed GAM-3DSC system in
effectively transmitting the goal-oriented 3D scenario.</div><div><a href='http://arxiv.org/abs/2403.05783v1'>2403.05783v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06803v1")'>Generative AI Meets Semantic Communication: Evolution and Revolution of
  Communication Tasks</div>
<div id='2401.06803v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T09:56:36Z</div><div>Authors: Eleonora Grassucci, Jihong Park, Sergio Barbarossa, Seong-Lyun Kim, Jinho Choi, Danilo Comminiello</div><div style='padding-top: 10px; width: 80ex'>While deep generative models are showing exciting abilities in computer
vision and natural language processing, their adoption in communication
frameworks is still far underestimated. These methods are demonstrated to
evolve solutions to classic communication problems such as denoising,
restoration, or compression. Nevertheless, generative models can unveil their
real potential in semantic communication frameworks, in which the receiver is
not asked to recover the sequence of bits used to encode the transmitted
(semantic) message, but only to regenerate content that is semantically
consistent with the transmitted message. Disclosing generative models
capabilities in semantic communication paves the way for a paradigm shift with
respect to conventional communication systems, which has great potential to
reduce the amount of data traffic and offers a revolutionary versatility to
novel tasks and applications that were not even conceivable a few years ago. In
this paper, we present a unified perspective of deep generative models in
semantic communication and we unveil their revolutionary role in future
communication frameworks, enabling emerging applications and tasks. Finally, we
analyze the challenges and opportunities to face to develop generative models
specifically tailored for communication systems.</div><div><a href='http://arxiv.org/abs/2401.06803v1'>2401.06803v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03545v1")'>Diffusion-based Generative Prior for Low-Complexity MIMO Channel
  Estimation</div>
<div id='2403.03545v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:47:31Z</div><div>Authors: Benedikt Fesl, Michael Baur, Florian Strasser, Michael Joham, Wolfgang Utschick</div><div style='padding-top: 10px; width: 80ex'>This work proposes a novel channel estimator based on diffusion models (DMs),
one of the currently top-rated generative models. Contrary to related works
utilizing generative priors, a lightweight convolutional neural network (CNN)
with positional embedding of the signal-to-noise ratio (SNR) information is
designed by learning the channel distribution in the sparse angular domain.
Combined with an estimation strategy that avoids stochastic resampling and
truncates reverse diffusion steps that account for lower SNR than the given
pilot observation, the resulting DM estimator has both low complexity and
memory overhead. Numerical results exhibit better performance than
state-of-the-art channel estimators utilizing generative priors.</div><div><a href='http://arxiv.org/abs/2403.03545v1'>2403.03545v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00299v1")'>Universal Auto-encoder Framework for MIMO CSI Feedback</div>
<div id='2403.00299v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T05:57:08Z</div><div>Authors: Jinhyun So, Hyukjoon Kwon</div><div style='padding-top: 10px; width: 80ex'>Existing auto-encoder (AE)-based channel state information (CSI) frameworks
have focused on a specific configuration of user equipment (UE) and base
station (BS), and thus the input and output sizes of the AE are fixed. However,
in the real-world scenario, the input and output sizes may vary depending on
the number of antennas of the BS and UE and the allocated resource block in the
frequency dimension. A naive approach to support the different input and output
sizes is to use multiple AE models, which is impractical for the UE due to the
limited HW resources. In this paper, we propose a universal AE framework that
can support different input sizes and multiple compression ratios. The proposed
AE framework significantly reduces the HW complexity while providing comparable
performance in terms of compression ratio-distortion trade-off compared to the
naive and state-of-the-art approaches.</div><div><a href='http://arxiv.org/abs/2403.00299v1'>2403.00299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10814v1")'>Associative Memories in the Feature Space</div>
<div id='2402.10814v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:37:48Z</div><div>Authors: Tommaso Salvatori, Beren Millidge, Yuhang Song, Rafal Bogacz, Thomas Lukasiewicz</div><div style='padding-top: 10px; width: 80ex'>An autoassociative memory model is a function that, given a set of data
points, takes as input an arbitrary vector and outputs the most similar data
point from the memorized set. However, popular memory models fail to retrieve
images even when the corruption is mild and easy to detect for a human
evaluator. This is because similarities are evaluated in the raw pixel space,
which does not contain any semantic information about the images. This problem
can be easily solved by computing \emph{similarities} in an embedding space
instead of the pixel space. We show that an effective way of computing such
embeddings is via a network pretrained with a contrastive loss. As the
dimension of embedding spaces is often significantly smaller than the pixel
space, we also have a faster computation of similarity scores. We test this
method on complex datasets such as CIFAR10 and STL10. An additional drawback of
current models is the need of storing the whole dataset in the pixel space,
which is often extremely large. We relax this condition and propose a class of
memory models that only stores low-dimensional semantic embeddings, and uses
them to retrieve similar, but not identical, memories. We demonstrate a proof
of concept of this method on a simple task on the MNIST dataset.</div><div><a href='http://arxiv.org/abs/2402.10814v1'>2402.10814v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11961v1")'>Enhanced Event-Based Video Reconstruction with Motion Compensation</div>
<div id='2403.11961v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:58:23Z</div><div>Authors: Siying Liu, Pier Luigi Dragotti</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks for event-based video reconstruction often suffer from a
lack of interpretability and have high memory demands. A lightweight network
called CISTA-LSTC has recently been introduced showing that high-quality
reconstruction can be achieved through the systematic design of its
architecture. However, its modelling assumption that input signals and output
reconstructed frame share the same sparse representation neglects the
displacement caused by motion. To address this, we propose warping the input
intensity frames and sparse codes to enhance reconstruction quality. A
CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC
for motion compensation. The system relies solely on events, in which predicted
flow aids in reconstruction and then reconstructed frames are used to
facilitate flow estimation. We also introduce an iterative training framework
for this combined system. Results demonstrate that our approach achieves
state-of-the-art reconstruction accuracy and simultaneously provides reliable
dense flow estimation. Furthermore, our model exhibits flexibility in that it
can integrate different flow networks, suggesting its potential for further
performance enhancement.</div><div><a href='http://arxiv.org/abs/2403.11961v1'>2403.11961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17487v1")'>Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model</div>
<div id='2402.17487v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T13:12:18Z</div><div>Authors: Panqi Jia, A. Burakhan Koyuncu, Jue Mao, Ze Cui, Yi Ma, Tiansheng Guo, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Jing Wang, Elena Alshina, Andre Kaup</div><div style='padding-top: 10px; width: 80ex'>The research on neural network (NN) based image compression has shown
superior performance compared to classical compression frameworks. Unlike the
hand-engineered transforms in the classical frameworks, NN-based models learn
the non-linear transforms providing more compact bit representations, and
achieve faster coding speed on parallel devices over their classical
counterparts. Those properties evoked the attention of both scientific and
industrial communities, resulting in the standardization activity JPEG-AI. The
verification model for the standardization process of JPEG-AI is already in
development and has surpassed the advanced VVC intra codec. To generate
reconstructed images with the desired bits per pixel and assess the BD-rate
performance of both the JPEG-AI verification model and VVC intra, bit rate
matching is employed. However, the current state of the JPEG-AI verification
model experiences significant slowdowns during bit rate matching, resulting in
suboptimal performance due to an unsuitable model. The proposed methodology
offers a gradual algorithmic optimization for matching bit rates, resulting in
a fourfold acceleration and over 1% improvement in BD-rate at the base
operation point. At the high operation point, the acceleration increases up to
sixfold.</div><div><a href='http://arxiv.org/abs/2402.17487v1'>2402.17487v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17470v1")'>Bit Distribution Study and Implementation of Spatial Quality Map in the
  JPEG-AI Standardization</div>
<div id='2402.17470v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T12:52:44Z</div><div>Authors: Panqi Jia, Jue Mao, Esin Koyuncu, A. Burakhan Koyuncu, Timofey Solovyev, Alexander Karabutov, Yin Zhao, Elena Alshina, Andre Kaup</div><div style='padding-top: 10px; width: 80ex'>Currently, there is a high demand for neural network-based image compression
codecs. These codecs employ non-linear transforms to create compact bit
representations and facilitate faster coding speeds on devices compared to the
hand-crafted transforms used in classical frameworks. The scientific and
industrial communities are highly interested in these properties, leading to
the standardization effort of JPEG-AI. The JPEG-AI verification model has been
released and is currently under development for standardization. Utilizing
neural networks, it can outperform the classic codec VVC intra by over 10%
BD-rate operating at base operation point. Researchers attribute this success
to the flexible bit distribution in the spatial domain, in contrast to VVC
intra's anchor that is generated with a constant quality point. However, our
study reveals that VVC intra displays a more adaptable bit distribution
structure through the implementation of various block sizes. As a result of our
observations, we have proposed a spatial bit allocation method to optimize the
JPEG-AI verification model's bit distribution and enhance the visual quality.
Furthermore, by applying the VVC bit distribution strategy, the objective
performance of JPEG-AI verification mode can be further improved, resulting in
a maximum gain of 0.45 dB in PSNR-Y.</div><div><a href='http://arxiv.org/abs/2402.17470v1'>2402.17470v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.15360v1")'>SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate
  Time series</div>
<div id='2403.15360v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:22:56Z</div><div>Authors: Badri N. Patro, Vijay S. Agneeswaran</div><div style='padding-top: 10px; width: 80ex'>Transformers have widely adopted attention networks for sequence mixing and
MLPs for channel mixing, playing a pivotal role in achieving breakthroughs
across domains. However, recent literature highlights issues with attention
networks, including low inductive bias and quadratic complexity concerning
input sequence length. State Space Models (SSMs) like S4 and others (Hippo,
Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address
the above issues to help handle longer sequence lengths. Mamba, while being the
state-of-the-art SSM, has a stability issue when scaled to large networks for
computer vision datasets. We propose SiMBA, a new architecture that introduces
Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations
and uses the Mamba block for sequence modeling. Extensive performance studies
across image and time-series benchmarks demonstrate that SiMBA outperforms
existing SSMs, bridging the performance gap with state-of-the-art transformers.
Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet
and transfer learning benchmarks such as Stanford Car and Flower as well as
task learning benchmarks as well as seven time series benchmark datasets. The
project page is available on this website
~\url{https://github.com/badripatro/Simba}.</div><div><a href='http://arxiv.org/abs/2403.15360v1'>2403.15360v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15949v1")'>TFDMNet: A Novel Network Structure Combines the Time Domain and
  Frequency Domain Features</div>
<div id='2401.15949v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:18:21Z</div><div>Authors: Hengyue Pan, Yixin Chen, Zhiliang Tian, Peng Qiao, Linbo Qiao, Dongsheng Li</div><div style='padding-top: 10px; width: 80ex'>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, it also has
high computation complexity and hard to be parallelized. This paper proposes a
novel Element-wise Multiplication Layer (EML) to replace convolution layers,
which can be trained in the frequency domain. Theoretical analyses show that
EMLs lower the computation complexity and easier to be parallelized. Moreover,
we introduce a Weight Fixation mechanism to alleviate the problem of
over-fitting, and analyze the working behavior of Batch Normalization and
Dropout in the frequency domain. To get the balance between the computation
complexity and memory usage, we propose a new network structure, namely
Time-Frequency Domain Mixture Network (TFDMNet), which combines the advantages
of both convolution layers and EMLs. Experimental results imply that TFDMNet
achieves good performance on MNIST, CIFAR-10 and ImageNet databases with less
number of operations comparing with corresponding CNNs.</div><div><a href='http://arxiv.org/abs/2401.15949v1'>2401.15949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01056v1")'>Enhancing Automatic Modulation Recognition through Robust Global Feature
  Extraction</div>
<div id='2401.01056v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T06:31:24Z</div><div>Authors: Yunpeng Qu, Zhilin Lu, Rui Zeng, Jintao Wang, Jian Wang</div><div style='padding-top: 10px; width: 80ex'>Automatic Modulation Recognition (AMR) plays a crucial role in wireless
communication systems. Deep learning AMR strategies have achieved tremendous
success in recent years. Modulated signals exhibit long temporal dependencies,
and extracting global features is crucial in identifying modulation schemes.
Traditionally, human experts analyze patterns in constellation diagrams to
classify modulation schemes. Classical convolutional-based networks, due to
their limited receptive fields, excel at extracting local features but struggle
to capture global relationships. To address this limitation, we introduce a
novel hybrid deep framework named TLDNN, which incorporates the architectures
of the transformer and long short-term memory (LSTM). We utilize the
self-attention mechanism of the transformer to model the global correlations in
signal sequences while employing LSTM to enhance the capture of temporal
dependencies. To mitigate the impact like RF fingerprint features and channel
characteristics on model generalization, we propose data augmentation
strategies known as segment substitution (SS) to enhance the model's robustness
to modulation-related features. Experimental results on widely-used datasets
demonstrate that our method achieves state-of-the-art performance and exhibits
significant advantages in terms of complexity. Our proposed framework serves as
a foundational backbone that can be extended to different datasets. We have
verified the effectiveness of our augmentation approach in enhancing the
generalization of the models, particularly in few-shot scenarios. Code is
available at \url{https://github.com/AMR-Master/TLDNN}.</div><div><a href='http://arxiv.org/abs/2401.01056v1'>2401.01056v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.15245v1")'>Reasoning-Enhanced Object-Centric Learning for Videos</div>
<div id='2403.15245v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:41:55Z</div><div>Authors: Jian Li, Pu Ren, Yang Liu, Hao Sun</div><div style='padding-top: 10px; width: 80ex'>Object-centric learning aims to break down complex visual scenes into more
manageable object representations, enhancing the understanding and reasoning
abilities of machine learning systems toward the physical world. Recently,
slot-based video models have demonstrated remarkable proficiency in segmenting
and tracking objects, but they overlook the importance of the effective
reasoning module. In the real world, reasoning and predictive abilities play a
crucial role in human perception and object tracking; in particular, these
abilities are closely related to human intuitive physics. Inspired by this, we
designed a novel reasoning module called the Slot-based Time-Space Transformer
with Memory buffer (STATM) to enhance the model's perception ability in complex
scenes. The memory buffer primarily serves as storage for slot information from
upstream modules, the Slot-based Time-Space Transformer makes predictions
through slot-based spatiotemporal attention computations and fusion. Our
experiment results on various datasets show that STATM can significantly
enhance object-centric learning capabilities of slot-based video models.</div><div><a href='http://arxiv.org/abs/2403.15245v1'>2403.15245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16569v2")'>Pretrained Visual Uncertainties</div>
<div id='2402.16569v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T13:47:32Z</div><div>Authors: Michael Kirchhof, Mark Collier, Seong Joon Oh, Enkelejda Kasneci</div><div style='padding-top: 10px; width: 80ex'>Accurate uncertainty estimation is vital to trustworthy machine learning, yet
uncertainties typically have to be learned for each task anew. This work
introduces the first pretrained uncertainty modules for vision models. Similar
to standard pretraining this enables the zero-shot transfer of uncertainties
learned on a large pretraining dataset to specialized downstream datasets. We
enable our large-scale pretraining on ImageNet-21k by solving a gradient
conflict in previous uncertainty modules and accelerating the training by up to
180x. We find that the pretrained uncertainties generalize to unseen datasets.
In scrutinizing the learned uncertainties, we find that they capture aleatoric
uncertainty, disentangled from epistemic components. We demonstrate that this
enables safe retrieval and uncertainty-aware dataset visualization. To
encourage applications to further problems and domains, we release all
pretrained checkpoints and code under https://github.com/mkirchhof/url .</div><div><a href='http://arxiv.org/abs/2402.16569v2'>2402.16569v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16304v1")'>Regressing Transformers for Data-efficient Visual Place Recognition</div>
<div id='2401.16304v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:04:32Z</div><div>Authors: Mar√≠a Leyva-Vallina, Nicola Strisciuglio, Nicolai Petkov</div><div style='padding-top: 10px; width: 80ex'>Visual place recognition is a critical task in computer vision, especially
for localization and navigation systems. Existing methods often rely on
contrastive learning: image descriptors are trained to have small distance for
similar images and larger distance for dissimilar ones in a latent space.
However, this approach struggles to ensure accurate distance-based image
similarity representation, particularly when training with binary pairwise
labels, and complex re-ranking strategies are required. This work introduces a
fresh perspective by framing place recognition as a regression problem, using
camera field-of-view overlap as similarity ground truth for learning. By
optimizing image descriptors to align directly with graded similarity labels,
this approach enhances ranking capabilities without expensive re-ranking,
offering data-efficient training and strong generalization across several
benchmark datasets.</div><div><a href='http://arxiv.org/abs/2401.16304v1'>2401.16304v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01470v2")'>TPC-ViT: Token Propagation Controller for Efficient Vision Transformer</div>
<div id='2401.01470v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T00:10:33Z</div><div>Authors: Wentao Zhu</div><div style='padding-top: 10px; width: 80ex'>Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.</div><div><a href='http://arxiv.org/abs/2401.01470v2'>2401.01470v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06869v1")'>Learning with Noisy Foundation Models</div>
<div id='2403.06869v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:22:41Z</div><div>Authors: Hao Chen, Jindong Wang, Zihan Wang, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj</div><div style='padding-top: 10px; width: 80ex'>Foundation models are usually pre-trained on large-scale datasets and then
adapted to downstream tasks through tuning. However, the large-scale
pre-training datasets, often inaccessible or too expensive to handle, can
contain label noise that may adversely affect the generalization of the model
and pose unexpected risks. This paper stands out as the first work to
comprehensively understand and analyze the nature of noise in pre-training
datasets and then effectively mitigate its impacts on downstream tasks.
Specifically, through extensive experiments of fully-supervised and image-text
contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M
datasets, we demonstrate that, while slight noise in pre-training can benefit
in-domain (ID) performance, where the training and testing data share a similar
distribution, it always deteriorates out-of-domain (OOD) performance, where
training and testing distributions are significantly different. These
observations are agnostic to scales of pre-training datasets, pre-training
noise types, model architectures, pre-training objectives, downstream tuning
methods, and downstream applications. We empirically ascertain that the reason
behind this is that the pre-training noise shapes the feature space
differently. We then propose a tuning method (NMTune) to affine the feature
space to mitigate the malignant effect of noise and improve generalization,
which is applicable in both parameter-efficient and black-box tuning manners.
We additionally conduct extensive experiments on popular vision and language
models, including APIs, which are supervised and self-supervised pre-trained on
realistic noisy data for evaluation. Our analysis and results demonstrate the
importance of this novel and fundamental research direction, which we term as
Noisy Model Learning.</div><div><a href='http://arxiv.org/abs/2403.06869v1'>2403.06869v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07410v1")'>A Closer Look at the Robustness of Contrastive Language-Image
  Pre-Training (CLIP)</div>
<div id='2402.07410v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T05:05:55Z</div><div>Authors: Weijie Tu, Weijian Deng, Tom Gedeon</div><div style='padding-top: 10px; width: 80ex'>Contrastive Language-Image Pre-training (CLIP) models have demonstrated
remarkable generalization capabilities across multiple challenging distribution
shifts. However, there is still much to be explored in terms of their
robustness to the variations of specific visual factors. In real-world
applications, reliable and safe systems must consider other safety objectives
beyond classification accuracy, such as predictive uncertainty. Yet, the
effectiveness of CLIP models on such safety-related features is less-explored.
Driven by the above, this work comprehensively investigates the safety
objectives of CLIP models, specifically focusing on three key properties:
resilience to visual factor variations, calibrated uncertainty estimations, and
the ability to detect anomalous inputs. To this end, we study 83 CLIP models
and 127 ImageNet classifiers. They are diverse in architecture, (pre)training
distribution and training strategies. We consider 10 visual factors (e.g.,
shape and pattern), 5 types of out-of-distribution data, and 8 natural and
challenging test conditions with different shift types, such as texture, style,
and perturbation shifts. Our study has unveiled several previously unknown
insights into CLIP models. For instance, they are not consistently more
calibrated than other ImageNet models, which contradicts existing findings.
Additionally, our analysis underscores the significance of training source
design by showcasing its profound influence on the three safety-related
properties. We believe our comprehensive study can shed light on and help guide
the development of more robust and reliable CLIP models.</div><div><a href='http://arxiv.org/abs/2402.07410v1'>2402.07410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03757v1")'>The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs</div>
<div id='2402.03757v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:48:46Z</div><div>Authors: Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have recently experienced remarkable progress,
where the advent of multi-modal large language models (MLLMs) has endowed LLMs
with visual capabilities, leading to impressive performances in various
multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail
spectacularly when presented with certain image and text inputs. In this paper,
we identify a typical class of inputs that baffles MLLMs, which consist of
images that are highly relevant but inconsistent with answers, causing MLLMs to
suffer from hallucination. To quantify the effect, we propose CorrelationQA,
the first benchmark that assesses the hallucination level given spurious
images. This benchmark contains 7,308 text-image pairs across 13 categories.
Based on the proposed CorrelationQA, we conduct a thorough analysis on 9
mainstream MLLMs, illustrating that they universally suffer from this
instinctive bias to varying degrees. We hope that our curated benchmark and
evaluation results aid in better assessments of the MLLMs' robustness in the
presence of misleading images. The resource is available in
https://github.com/MasaiahHan/CorrelationQA.</div><div><a href='http://arxiv.org/abs/2402.03757v1'>2402.03757v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14683v1")'>Visual Hallucinations of Multi-modal Large Language Models</div>
<div id='2402.14683v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:40:33Z</div><div>Authors: Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong</div><div style='padding-top: 10px; width: 80ex'>Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines
incorrect details about an image in visual question answering. Existing studies
find VH instances only in existing image datasets, which results in biased
understanding of MLLMs' performance under VH due to limited diversity of such
VH instances. In this work, we propose a tool called VHTest to generate a
diverse set of VH instances. Specifically, VHTest finds some initial VH
instances in existing image datasets (e.g., COCO), generates a text description
for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to
generate VH images based on the text descriptions. We collect a benchmark
dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that
existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a
large fraction of the instances in our benchmark. Moreover, we find that
fine-tuning an MLLM using our benchmark dataset reduces its likelihood to
hallucinate without sacrificing its performance on other benchmarks. Our
benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.</div><div><a href='http://arxiv.org/abs/2402.14683v1'>2402.14683v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04547v1")'>CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?</div>
<div id='2403.04547v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:43:17Z</div><div>Authors: Ibrahim Alabdulmohsin, Xiao Wang, Andreas Steiner, Priya Goyal, Alexander D'Amour, Xiaohua Zhai</div><div style='padding-top: 10px; width: 80ex'>We study the effectiveness of data-balancing for mitigating biases in
contrastive language-image pretraining (CLIP), identifying areas of strength
and limitation. First, we reaffirm prior conclusions that CLIP models can
inadvertently absorb societal stereotypes. To counter this, we present a novel
algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both
representation and association biases (i.e. in first- and second-order
statistics) in multimodal data. We use M4 to conduct an in-depth analysis
taking into account various factors, such as the model, representation, and
data size. Our study also explores the dynamic nature of how CLIP learns and
unlearns biases. In particular, we find that fine-tuning is effective in
countering representation biases, though its impact diminishes for association
biases. Also, data balancing has a mixed impact on quality: it tends to improve
classification but can hurt retrieval. Interestingly, data and architectural
improvements seem to mitigate the negative impact of data balancing on
performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves
COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and
ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with
recommendations for improving the efficacy of data balancing in multimodal
systems.</div><div><a href='http://arxiv.org/abs/2403.04547v1'>2403.04547v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02055v1")'>Variance Alignment Score: A Simple But Tough-to-Beat Data Selection
  Method for Multimodal Contrastive Learning</div>
<div id='2402.02055v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T06:29:04Z</div><div>Authors: Yiping Wang, Yifang Chen, Wendan Yan, Kevin Jamieson, Simon Shaolei Du</div><div style='padding-top: 10px; width: 80ex'>In recent years, data selection has emerged as a core issue for large-scale
visual-language model pretraining, especially on noisy web-curated datasets.
One widely adopted strategy assigns quality scores such as CLIP similarity for
each sample and retains the data pairs with the highest scores. However, these
approaches are agnostic of data distribution and always fail to select the most
informative samples. To solve this problem, we propose a simple yet
theoretically principled metric named Variance Alignment Score (VAS), which has
the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here,
$\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim
to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the
tensor product of single or multi-modal representations for the $i$-th sample.
We further design a new data selection method that maximizes the total VAS. We
provide theoretical analysis in a simplified setting to demonstrate the
theoretical advantage of VAS over random or other existing data selection.
Experimentally, applying VAS and CLIP scores together can outperform baselines
by a margin of $1.3\%$ average on 38 evaluation sets for noisy dataset DataComp
and $2.5\%$ on VTAB for high-quality dataset CC12M. Additionally, our ablation
study also shows visual features are better than text for calculating VAS, and
the related classical experimental design methods may fail under this context.</div><div><a href='http://arxiv.org/abs/2402.02055v1'>2402.02055v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09611v3")'>MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</div>
<div id='2403.09611v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:51:32Z</div><div>Authors: Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu H√®, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang</div><div style='padding-top: 10px; width: 80ex'>In this work, we discuss building performant Multimodal Large Language Models
(MLLMs). In particular, we study the importance of various architecture
components and data choices. Through careful and comprehensive ablations of the
image encoder, the vision language connector, and various pre-training data
choices, we identified several crucial design lessons. For example, we
demonstrate that for large-scale multimodal pre-training using a careful mix of
image-caption, interleaved image-text, and text-only data is crucial for
achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,
compared to other published pre-training results. Further, we show that the
image encoder together with image resolution and the image token count has
substantial impact, while the vision-language connector design is of
comparatively negligible importance. By scaling up the presented recipe, we
build MM1, a family of multimodal models up to 30B parameters, including both
dense models and mixture-of-experts (MoE) variants, that are SOTA in
pre-training metrics and achieve competitive performance after supervised
fine-tuning on a range of established multimodal benchmarks. Thanks to
large-scale pre-training, MM1 enjoys appealing properties such as enhanced
in-context learning, and multi-image reasoning, enabling few-shot
chain-of-thought prompting.</div><div><a href='http://arxiv.org/abs/2403.09611v3'>2403.09611v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13164v1")'>VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal
  In-Context Learning</div>
<div id='2403.13164v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T21:31:56Z</div><div>Authors: Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) famously exhibit emergent in-context learning
(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples
provided as a prompt, without updating the model's weights. Built on top of
LLMs, vision large language models (VLLMs) have advanced significantly in areas
such as recognition, reasoning, and grounding. However, investigations into
\emph{multimodal ICL} have predominantly focused on few-shot visual question
answering (VQA), and image captioning, which we will show neither exploit the
strengths of ICL, nor test its limitations. The broader capabilities and
limitations of multimodal ICL remain under-explored. In this study, we
introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context
learning, encompassing a broad spectrum of tasks that involve both images and
text as inputs and outputs, and different types of challenges, from {perception
to reasoning and long context length}. We evaluate the abilities of
state-of-the-art VLLMs against this benchmark suite, revealing their diverse
strengths and weaknesses, and showing that even the most advanced models, such
as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,
and the associated strengths and limitations of existing models, we hope that
our dataset will inspire future work on enhancing the in-context learning
capabilities of VLLMs, as well as inspire new applications that leverage VLLM
ICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.</div><div><a href='http://arxiv.org/abs/2403.13164v1'>2403.13164v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01293v1")'>Can MLLMs Perform Text-to-Image In-Context Learning?</div>
<div id='2402.01293v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:30:05Z</div><div>Authors: Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, Kangwook Lee</div><div style='padding-top: 10px; width: 80ex'>The evolution from Large Language Models (LLMs) to Multimodal Large Language
Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to
its multimodal counterpart. Existing such studies have primarily concentrated
on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique
characteristics and potential applications, remains underexplored. To address
this gap, we formally define the task of T2I-ICL and present CoBSAT, the first
T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to
benchmark six state-of-the-art MLLMs, we uncover considerable difficulties
MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the
inherent complexity of multimodality and image generation. To overcome these
challenges, we explore strategies like fine-tuning and Chain-of-Thought
prompting, demonstrating notable improvements. Our code and dataset are
available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.</div><div><a href='http://arxiv.org/abs/2402.01293v1'>2402.01293v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11411v1")'>Aligning Modalities in Vision Large Language Models via Preference
  Fine-tuning</div>
<div id='2402.11411v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T00:56:16Z</div><div>Authors: Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao</div><div style='padding-top: 10px; width: 80ex'>Instruction-following Vision Large Language Models (VLLMs) have achieved
significant progress recently on a variety of tasks. These approaches merge
strong pre-trained vision models and large language models (LLMs). Since these
components are trained separately, the learned representations need to be
aligned with joint training on additional image-language pairs. This procedure
is not perfect and can cause the model to hallucinate - provide answers that do
not accurately reflect the image, even when the core LLM is highly factual and
the vision backbone has sufficiently complete representations. In this work, we
frame the hallucination problem as an alignment issue, tackle it with
preference tuning. Specifically, we propose POVID to generate feedback data
with AI models. We use ground-truth instructions as the preferred response and
a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to
inject plausible hallucinations into the correct answer. Second, we distort the
image to trigger the inherent hallucination behavior of the VLLM. This is an
automated approach, which does not rely on human data generation or require a
perfect expert, which makes it easily scalable. Finally, both of these
generation strategies are integrated into an RLHF pipeline via Direct
Preference Optimization. In experiments across broad benchmarks, we show that
we can not only reduce hallucinations, but improve model performance across
standard benchmarks, outperforming prior approaches. Our data and code are
available at https://github.com/YiyangZhou/POVID.</div><div><a href='http://arxiv.org/abs/2402.11411v1'>2402.11411v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00376v1")'>Invariant Test-Time Adaptation for Vision-Language Model Generalization</div>
<div id='2403.00376v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T09:01:53Z</div><div>Authors: Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu</div><div style='padding-top: 10px; width: 80ex'>Vision-language foundation models have exhibited remarkable success across a
multitude of downstream tasks due to their scalability on extensive image-text
paired datasets. However, these models display significant limitations when
applied to long-tail tasks, such as fine-grained image classification, as a
result of "decision shortcuts" that hinders their generalization capabilities.
In this work, we find that the CLIP model possesses a rich set of features,
encompassing both \textit{desired invariant causal features} and
\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP
on downstream tasks originates from its inability to effectively utilize
pre-trained features in accordance with specific task requirements. To address
this challenge, this paper introduces a test-time prompt tuning paradigm that
optimizes a learnable prompt, thereby compelling the model to exploit genuine
causal invariant features while disregarding decision shortcuts during the
inference phase. The proposed method effectively alleviates excessive
dependence on potentially misleading, task-irrelevant contextual information,
while concurrently emphasizing critical, task-related visual cues. We conduct
comparative analysis of the proposed method against various approaches which
validates its effectiveness.</div><div><a href='http://arxiv.org/abs/2403.00376v1'>2403.00376v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06799v1")'>Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt
  Learning with Data-Dependent Prior</div>
<div id='2401.06799v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T10:15:59Z</div><div>Authors: Youngjae Cho, HeeSun Bae, Seungjae Shin, Yeo Dong Youn, Weonyoung Joo, Il-Chul Moon</div><div style='padding-top: 10px; width: 80ex'>Recent Vision-Language Pretrained (VLP) models have become the backbone for
many downstream tasks, but they are utilized as frozen model without learning.
Prompt learning is a method to improve the pre-trained VLP model by adding a
learnable context vector to the inputs of the text encoder. In a few-shot
learning scenario of the downstream task, MLE training can lead the context
vector to over-fit dominant image features in the training data. This
overfitting can potentially harm the generalization ability, especially in the
presence of a distribution shift between the training and test dataset. This
paper presents a Bayesian-based framework of prompt learning, which could
alleviate the overfitting issues on few-shot learning application and increase
the adaptability of prompts on unseen instances. Specifically, modeling
data-dependent prior enhances the adaptability of text features for both seen
and unseen image features without the trade-off of performance between them.
Based on the Bayesian framework, we utilize the Wasserstein Gradient Flow in
the estimation of our target posterior distribution, which enables our prompt
to be flexible in capturing the complex modes of image features. We demonstrate
the effectiveness of our method on benchmark datasets for several experiments
by showing statistically significant improvements on performance compared to
existing methods. The code is available at https://github.com/youngjae-cho/APP.</div><div><a href='http://arxiv.org/abs/2401.06799v1'>2401.06799v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14119v1")'>C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via
  Text Feature Dispersion</div>
<div id='2403.14119v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T04:08:29Z</div><div>Authors: Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo</div><div style='padding-top: 10px; width: 80ex'>In deep learning, test-time adaptation has gained attention as a method for
model fine-tuning without the need for labeled data. A prime exemplification is
the recently proposed test-time prompt tuning for large-scale vision-language
models such as CLIP. Unfortunately, these prompts have been mainly developed to
improve accuracy, overlooking the importance of calibration-a crucial aspect
for quantifying prediction uncertainty. However, traditional calibration
methods rely on substantial amounts of labeled data, making them impractical
for test-time scenarios. To this end, this paper explores calibration during
test-time prompt tuning by leveraging the inherent properties of CLIP. Through
a series of observations, we find that the prompt choice significantly affects
the calibration in CLIP, where the prompts leading to higher text feature
dispersion result in better-calibrated predictions. Introducing the Average
Text Feature Dispersion (ATFD), we establish its relationship with calibration
error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),
for optimizing prompts during test-time with enhanced calibration. Through
extensive experiments on different CLIP architectures and datasets, we show
that C-TPT can effectively improve the calibration of test-time prompt tuning
without needing labeled data.</div><div><a href='http://arxiv.org/abs/2403.14119v1'>2403.14119v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02382v1")'>Revisiting the Power of Prompt for Visual Tuning</div>
<div id='2402.02382v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T07:49:02Z</div><div>Authors: Yuzhu Wang, Lechao Cheng, Chaowei Fang, Dingwen Zhang, Manni Duan, Meng Wang</div><div style='padding-top: 10px; width: 80ex'>Visual prompt tuning (VPT) is a promising solution incorporating learnable
prompt tokens to customize pre-trained models for downstream tasks. However,
VPT and its variants often encounter challenges like prompt initialization,
prompt length, and subpar performance in self-supervised pretraining, hindering
successful contextual adaptation. This study commences by exploring the
correlation evolvement between prompts and patch tokens during proficient
training. Inspired by the observation that the prompt tokens tend to share high
mutual information with patch tokens, we propose initializing prompts with
downstream token prototypes. The strategic initialization, a stand-in for the
previous initialization, substantially improves performance in fine-tuning. To
refine further, we optimize token construction with a streamlined pipeline that
maintains excellent performance with almost no increase in computational
expenses compared to VPT. Exhaustive experiments show our proposed approach
outperforms existing methods by a remarkable margin. For instance, it surpasses
full fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable
parameters on the FGVC and VTAB-1K benchmarks. Notably, our method
significantly advances the adaptation for self-supervised pretraining,
achieving impressive task performance gains of at least 10% to 30%. Besides,
the experimental results demonstrate the proposed SPT is robust to prompt
lengths and scales well with model capacity and training data size. We finally
provide an insightful exploration into the amount of target data facilitating
the adaptation of pre-trained models to downstream tasks.</div><div><a href='http://arxiv.org/abs/2402.02382v1'>2402.02382v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11537v1")'>Semantic Prompting with Image-Token for Continual Learning</div>
<div id='2403.11537v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T07:43:14Z</div><div>Authors: Jisu Han, Jaemin Na, Wonjun Hwang</div><div style='padding-top: 10px; width: 80ex'>Continual learning aims to refine model parameters for new tasks while
retaining knowledge from previous tasks. Recently, prompt-based learning has
emerged to leverage pre-trained models to be prompted to learn subsequent tasks
without the reliance on the rehearsal buffer. Although this approach has
demonstrated outstanding results, existing methods depend on preceding
task-selection process to choose appropriate prompts. However, imperfectness in
task-selection may lead to negative impacts on the performance particularly in
the scenarios where the number of tasks is large or task distributions are
imbalanced. To address this issue, we introduce I-Prompt, a task-agnostic
approach focuses on the visual semantic information of image tokens to
eliminate task prediction. Our method consists of semantic prompt matching,
which determines prompts based on similarities between tokens, and image
token-level prompting, which applies prompts directly to image tokens in the
intermediate layers. Consequently, our method achieves competitive performance
on four benchmarks while significantly reducing training time compared to
state-of-the-art methods. Moreover, we demonstrate the superiority of our
method across various scenarios through extensive experiments.</div><div><a href='http://arxiv.org/abs/2403.11537v1'>2403.11537v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06870v2")'>Semantic Residual Prompts for Continual Learning</div>
<div id='2403.06870v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:23:38Z</div><div>Authors: Martin Menabue, Emanuele Frascaroli, Matteo Boschini, Enver Sangineto, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara</div><div style='padding-top: 10px; width: 80ex'>Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained
model and focus training on a few parameter vectors termed prompts. Most of
these methods organize these vectors in a pool of key-value pairs, and use the
input image as query to retrieve the prompts (values). However, as keys are
learned while tasks progress, the prompting selection strategy is itself
subject to catastrophic forgetting, an issue often overlooked by existing
approaches. For instance, prompts introduced to accommodate new tasks might end
up interfering with previously learned prompts. To make the selection strategy
more stable, we ask a foundational model (CLIP) to select our prompt within a
two-level adaptation mechanism. Specifically, the first level leverages
standard textual prompts for the CLIP textual encoder, leading to stable class
prototypes. The second level, instead, uses these prototypes along with the
query image as keys to index a second pool. The retrieved prompts serve to
adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a
novel residual mechanism to transfer CLIP semantics to the ViT layers. Through
extensive analysis on established CL benchmarks, we show that our method
significantly outperforms both state-of-the-art CL approaches and the zero-shot
CLIP test. Notably, our findings hold true even for datasets with a substantial
domain gap w.r.t. the pre-training knowledge of the backbone model, as
showcased by experiments on satellite imagery and medical datasets.</div><div><a href='http://arxiv.org/abs/2403.06870v2'>2403.06870v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08714v1")'>PRDP: Proximal Reward Difference Prediction for Large-Scale Reward
  Finetuning of Diffusion Models</div>
<div id='2402.08714v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:58:16Z</div><div>Authors: Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou</div><div style='padding-top: 10px; width: 80ex'>Reward finetuning has emerged as a promising approach to aligning foundation
models with downstream objectives. Remarkable success has been achieved in the
language domain by using reinforcement learning (RL) to maximize rewards that
reflect human preference. However, in the vision domain, existing RL-based
reward finetuning methods are limited by their instability in large-scale
training, rendering them incapable of generalizing to complex, unseen prompts.
In this paper, we propose Proximal Reward Difference Prediction (PRDP),
enabling stable black-box reward finetuning for diffusion models for the first
time on large-scale prompt datasets with over 100K prompts. Our key innovation
is the Reward Difference Prediction (RDP) objective that has the same optimal
solution as the RL objective while enjoying better training stability.
Specifically, the RDP objective is a supervised regression objective that tasks
the diffusion model with predicting the reward difference of generated image
pairs from their denoising trajectories. We theoretically prove that the
diffusion model that obtains perfect reward difference prediction is exactly
the maximizer of the RL objective. We further develop an online algorithm with
proximal updates to stably optimize the RDP objective. In experiments, we
demonstrate that PRDP can match the reward maximization ability of
well-established RL-based methods in small-scale training. Furthermore, through
large-scale training on text prompts from the Human Preference Dataset v2 and
the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a
diverse set of complex, unseen prompts whereas RL-based methods completely
fail.</div><div><a href='http://arxiv.org/abs/2402.08714v1'>2402.08714v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00041v1")'>Global and Local Prompts Cooperation via Optimal Transport for Federated
  Learning</div>
<div id='2403.00041v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T11:43:04Z</div><div>Authors: Hongxia Li, Wei Huang, Jingya Wang, Ye Shi</div><div style='padding-top: 10px; width: 80ex'>Prompt learning in pretrained visual-language models has shown remarkable
flexibility across various downstream tasks. Leveraging its inherent
lightweight nature, recent research attempted to integrate the powerful
pretrained models into federated learning frameworks to simultaneously reduce
communication costs and promote local training on insufficient data. Despite
these efforts, current federated prompt learning methods lack specialized
designs to systematically address severe data heterogeneities, e.g., data
distribution with both label and feature shifts involved. To address this
challenge, we present Federated Prompts Cooperation via Optimal Transport
(FedOTP), which introduces efficient collaborative prompt learning strategies
to capture diverse category traits on a per-client basis. Specifically, for
each client, we learn a global prompt to extract consensus knowledge among
clients, and a local prompt to capture client-specific category
characteristics. Unbalanced Optimal Transport is then employed to align local
visual features with these prompts, striking a balance between global consensus
and local personalization. Extensive experiments on datasets with various types
of heterogeneities have demonstrated that our FedOTP outperforms the
state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.00041v1'>2403.00041v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10568v1")'>MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of
  Prompt Experts</div>
<div id='2403.10568v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:47:10Z</div><div>Authors: Ruixiang Jiang, Lingbo Liu, Changwen Chen</div><div style='padding-top: 10px; width: 80ex'>Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal
foundation models for multimodal tasks. However, its limited adaptivity and
expressiveness lead to suboptimal performance when compared with other tuning
methods. In this paper, we address this issue by disentangling the vanilla
prompts to adaptively capture dataset-level and instance-level features.
Building upon this disentanglement, we introduce the mixture of prompt experts
(MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing
priors to route the most effective prompt on a per-instance basis. Compared to
vanilla prompting, our MoPE-based conditional prompting exhibits greater
expressiveness for multimodal fusion, scaling better with the training data and
the overall number of trainable parameters. We also study a regularization term
for expert routing, leading to emergent expert specialization, where different
experts focus on different concepts, enabling interpretable soft prompting.
Extensive experiments across three multimodal datasets demonstrate that our
method achieves state-of-the-art results, matching or even surpassing the
performance of fine-tuning, while requiring only 0.8% of the trainable
parameters. Code will be released: https://github.com/songrise/MoPE.</div><div><a href='http://arxiv.org/abs/2403.10568v1'>2403.10568v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02695v1")'>Controllable Prompt Tuning For Balancing Group Distributional Robustness</div>
<div id='2403.02695v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:23:55Z</div><div>Authors: Hoang Phan, Andrew Gordon Wilson, Qi Lei</div><div style='padding-top: 10px; width: 80ex'>Models trained on data composed of different groups or domains can suffer
from severe performance degradation under distribution shifts. While recent
methods have largely focused on optimizing the worst-group objective, this
often comes at the expense of good performance on other groups. To address this
problem, we introduce an optimization scheme to achieve good performance across
groups and find a good solution for all without severely sacrificing
performance on any of them. However, directly applying such optimization
involves updating the parameters of the entire network, making it both
computationally expensive and challenging. Thus, we introduce Controllable
Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques.
On spurious correlation benchmarks, our procedures achieve state-of-the-art
results across both transformer and non-transformer architectures, as well as
unimodal and multimodal data, while requiring only 0.4% tunable parameters.</div><div><a href='http://arxiv.org/abs/2403.02695v1'>2403.02695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14774v1")'>Few-Shot Adversarial Prompt Learning on Vision-Language Models</div>
<div id='2403.14774v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T18:28:43Z</div><div>Authors: Yiwei Zhou, Xiaobo Xia, Zhiwei Lin, Bo Han, Tongliang Liu</div><div style='padding-top: 10px; width: 80ex'>The vulnerability of deep neural networks to imperceptible adversarial
perturbations has attracted widespread attention. Inspired by the success of
vision-language foundation models, previous efforts achieved zero-shot
adversarial robustness by aligning adversarial visual features with text
supervision. However, in practice, they are still unsatisfactory due to several
issues, including heavy adaptation cost, suboptimal text supervision, and
uncontrolled natural generalization capacity. In this paper, to address these
issues, we propose a few-shot adversarial prompt framework where adapting input
sequences with limited data makes significant adversarial robustness
improvement. Specifically, we achieve this by providing adversarially
correlated text supervision that is end-to-end learned from adversarial
examples. We also propose a novel training objective that enhances the
consistency of multi-modal features while encourages differentiated uni-modal
features between natural and adversarial examples. The proposed framework gives
access to learn adversarial text supervision, which provides superior
cross-modal adversarial alignment and matches state-of-the-art zero-shot
adversarial robustness with only 1% training data.</div><div><a href='http://arxiv.org/abs/2403.14774v1'>2403.14774v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11299v1")'>SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant</div>
<div id='2403.11299v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T18:42:38Z</div><div>Authors: Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, Zhiqiang Tao</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in the vision-language model have shown notable
generalization in vision-language tasks after visual instruction tuning.
However, bridging the gap between the pre-trained vision encoder and the large
language models becomes the whole network's bottleneck. To improve
cross-modality alignment, existing works usually consider more visual
instruction data covering a broader range of vision tasks to fine-tune the
model for question-answering, which are costly to obtain. However, the image
contains rich contextual information that has been largely under-explored. This
paper first attempts to harness this overlooked context within visual
instruction data, training the model to self-supervised `learning' how to ask
high-quality questions. In this way, we introduce a novel framework named
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA
exhibits proficiency in generating flexible and meaningful image-related
questions while analyzing the visual clue and prior language knowledge,
signifying an advanced level of generalized visual understanding. Moreover,
fine-tuning SQ-LLaVA on higher-quality instruction data shows a consistent
performance improvement compared with traditional visual-instruction tuning
methods. This improvement highlights the efficacy of self-questioning
techniques in achieving a deeper and more nuanced comprehension of visual
content across various contexts.</div><div><a href='http://arxiv.org/abs/2403.11299v1'>2403.11299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03253v2")'>VLLaVO: Mitigating Visual Gap through LLMs</div>
<div id='2401.03253v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T16:33:39Z</div><div>Authors: Shuhao Chen, Yulong Zhang, Weisen Jiang, Jiangang Lu, Yu Zhang</div><div style='padding-top: 10px; width: 80ex'>Recent advances achieved by deep learning models rely on the independent and
identically distributed assumption, hindering their applications in real-world
scenarios with domain shifts. To tackle this issue, cross-domain learning aims
at extracting domain-invariant knowledge to reduce the domain shift between
training and testing data. However, in visual cross-domain learning,
traditional methods concentrate solely on the image modality, disregarding the
potential benefits of incorporating the text modality. In this work, we propose
VLLaVO, combining Vision language models and Large Language models as Visual
cross-dOmain learners. VLLaVO uses vision-language models to convert images
into detailed textual descriptions. A large language model is then finetuned on
textual descriptions of the source/target domain generated by a designed
instruction template. Extensive experimental results under domain
generalization and unsupervised domain adaptation settings demonstrate the
effectiveness of the proposed method.</div><div><a href='http://arxiv.org/abs/2401.03253v2'>2401.03253v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14252v1")'>LayoutLLM: Large Language Model Instruction Tuning for Visually Rich
  Document Understanding</div>
<div id='2403.14252v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T09:25:24Z</div><div>Authors: Masato Fujitake</div><div style='padding-top: 10px; width: 80ex'>This paper proposes LayoutLLM, a more flexible document analysis method for
understanding imaged documents. Visually Rich Document Understanding tasks,
such as document image classification and information extraction, have gained
significant attention due to their importance. Existing methods have been
developed to enhance document comprehension by incorporating pre-training
awareness of images, text, and layout structure. However, these methods require
fine-tuning for each task and dataset, and the models are expensive to train
and operate. To overcome this limitation, we propose a new LayoutLLM that
integrates these with large-scale language models (LLMs). By leveraging the
strengths of existing research in document image understanding and LLMs'
superior language understanding capabilities, the proposed model, fine-tuned
with multimodal instruction datasets, performs an understanding of document
images in a single model. Our experiments demonstrate improvement over the
baseline model in various document analysis tasks.</div><div><a href='http://arxiv.org/abs/2403.14252v1'>2403.14252v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12728v2")'>Modality-Aware Integration with Large Language Models for
  Knowledge-based Visual Question Answering</div>
<div id='2402.12728v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T05:32:24Z</div><div>Authors: Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang</div><div style='padding-top: 10px; width: 80ex'>Knowledge-based visual question answering (KVQA) has been extensively studied
to answer visual questions with external knowledge, e.g., knowledge graphs
(KGs). While several attempts have been proposed to leverage large language
models (LLMs) as an implicit knowledge source, it remains challenging since
LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,
images, KGs and LLMs, cannot be readily aligned for complex scenarios. To
tackle these, we present a novel modality-aware integration with LLMs for KVQA
(MAIL). It carefully leverages multimodal knowledge for both image
understanding and knowledge reasoning. Specifically, (i) we propose a two-stage
prompting strategy with LLMs to densely embody the image into a scene graph
with detailed visual features; (ii) We construct a coupled concept graph by
linking the mentioned entities with external facts. (iii) A tailored
pseudo-siamese graph medium fusion is designed for sufficient multimodal
fusion. We utilize the shared mentioned entities in two graphs as mediums to
bridge a tight inter-modal exchange, while maximally preserving insightful
intra-modal learning by constraining the fusion within mediums. Extensive
experiments on two benchmark datasets show the superiority of MAIL with 24x
less resources.</div><div><a href='http://arxiv.org/abs/2402.12728v2'>2402.12728v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15933v1")'>Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion
  Approach for 3D VQA</div>
<div id='2402.15933v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T23:31:34Z</div><div>Authors: Wentao Mo, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated
data and limited visual content diversity hampers the generalization to novel
scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and
SQA dataset). Current approaches resort supplement 3D reasoning with 2D
information. However, these methods face challenges: either they use top-down
2D views that introduce overly complex and sometimes question-irrelevant visual
clues, or they rely on globally aggregated scene/image-level representations
from 2D VLMs, losing the fine-grained vision-language correlations. To overcome
these limitations, our approach utilizes question-conditional 2D view selection
procedure, pinpointing semantically relevant 2D inputs for crucial visual
clues. We then integrate this 2D knowledge into the 3D-VQA system via a
two-branch Transformer structure. This structure, featuring a Twin-Transformer
design, compactly combines 2D and 3D modalities and captures fine-grained
correlations between modalities, allowing them mutually augmenting each other.
Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh
perspective on multi-modal transformer-based architectures for 3D-VQA.
Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets
and significantly outperforms existing solutions. Code is available at
$\href{https://github.com/matthewdm0816/BridgeQA}{\text{this URL}}$.</div><div><a href='http://arxiv.org/abs/2402.15933v1'>2402.15933v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14783v1")'>Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot
  Visual Question Answering</div>
<div id='2403.14783v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T18:57:25Z</div><div>Authors: Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Dan Roth, Camillo J. Taylor</div><div style='padding-top: 10px; width: 80ex'>This work explores the zero-shot capabilities of foundation models in Visual
Question Answering (VQA) tasks. We propose an adaptive multi-agent system,
named Multi-Agent VQA, to overcome the limitations of foundation models in
object detection and counting by using specialized agents as tools. Unlike
existing approaches, our study focuses on the system's performance without
fine-tuning it on specific VQA datasets, making it more practical and robust in
the open world. We present preliminary experimental results under zero-shot
scenarios and highlight some failure cases, offering new directions for future
research.</div><div><a href='http://arxiv.org/abs/2403.14783v1'>2403.14783v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08268v2")'>World Model on Million-Length Video And Language With Blockwise
  RingAttention</div>
<div id='2402.08268v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T07:47:36Z</div><div>Authors: Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel</div><div style='padding-top: 10px; width: 80ex'>Current language models fall short in understanding aspects of the world not
easily described in words, and struggle with complex, long-form tasks. Video
sequences offer valuable temporal information absent in language and static
images, making them attractive for joint modeling with language. Such models
could develop a understanding of both human textual knowledge and the physical
world, enabling broader AI capabilities for assisting humans. However, learning
from millions of tokens of video and language sequences poses challenges due to
memory constraints, computational complexity, and limited datasets. To address
these challenges, we curate a large dataset of diverse videos and books,
utilize the Blockwise RingAttention technique to scalably train on long
sequences, and gradually increase context size from 4K to 1M tokens. This paper
makes the following contributions: (a) Largest context size neural network: We
train one of the largest context size transformers on long video and language
sequences, setting new benchmarks in difficult retrieval tasks and long video
understanding. (b) Solutions for overcoming vision-language training
challenges, including using masked sequence packing for mixing different
sequence lengths, loss weighting to balance language and vision, and
model-generated QA dataset for long sequence chat. (c) A highly-optimized
implementation with RingAttention, Blockwise Transformers, masked sequence
packing, and other key features for training on millions-length multimodal
sequences. (d) Fully open-sourced a family of 7B parameter models capable of
processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM,
LWM-Chat) of over 1M tokens. This work paves the way for training on massive
datasets of long video and language to develop understanding of both human
knowledge and the multimodal world, and broader capabilities.</div><div><a href='http://arxiv.org/abs/2402.08268v2'>2402.08268v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14870v1")'>VidLA: Video-Language Alignment at Scale</div>
<div id='2403.14870v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T22:36:24Z</div><div>Authors: Mamshad Nayeem Rizve, Fan Fei, Jayakrishnan Unnikrishnan, Son Tran, Benjamin Z. Yao, Belinda Zeng, Mubarak Shah, Trishul Chilimbi</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose VidLA, an approach for video-language alignment at
scale. There are two major limitations of previous video-language alignment
approaches. First, they do not capture both short-range and long-range temporal
dependencies and typically employ complex hierarchical deep network
architectures that are hard to integrate with existing pretrained image-text
foundation models. To effectively address this limitation, we instead keep the
network architecture simple and use a set of data tokens that operate at
different temporal resolutions in a hierarchical manner, accounting for the
temporally hierarchical nature of videos. By employing a simple two-tower
architecture, we are able to initialize our video-language model with
pretrained image-text foundation models, thereby boosting the final
performance. Second, existing video-language alignment works struggle due to
the lack of semantically aligned large-scale training data. To overcome it, we
leverage recent LLMs to curate the largest video-language dataset to date with
better visual grounding. Furthermore, unlike existing video-text datasets which
only contain short clips, our dataset is enriched with video clips of varying
durations to aid our temporally hierarchical data tokens in extracting better
representations at varying temporal scales. Overall, empirical results show
that our proposed approach surpasses state-of-the-art methods on multiple
retrieval benchmarks, especially on longer videos, and performs competitively
on classification benchmarks.</div><div><a href='http://arxiv.org/abs/2403.14870v1'>2403.14870v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02872v1")'>How do Large Language Models Learn In-Context? Query and Key Matrices of
  In-Context Heads are Two Towers for Metric Learning</div>
<div id='2402.02872v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:39:32Z</div><div>Authors: Zeping Yu, Sophia Ananiadou</div><div style='padding-top: 10px; width: 80ex'>We explore the mechanism of in-context learning and propose a hypothesis
using locate-and-project method. In shallow layers, the features of
demonstrations are merged into their corresponding labels, and the features of
the input text are aggregated into the last token. In deep layers, in-context
heads make great contributions. In each in-context head, the value-output
matrix extracts the labels' features. Query and key matrices compute the
attention weights between the input text and each demonstration. The larger the
attention weight is, the more label information is transferred into the last
token for predicting the next word. Query and key matrices can be regarded as
two towers for learning the similarity metric between the input text and each
demonstration. Based on this hypothesis, we explain why imbalanced labels and
demonstration order affect predictions. We conduct experiments on GPT2 large,
Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study
provides a new method and a reasonable hypothesis for understanding the
mechanism of in-context learning. Our code will be released on github.</div><div><a href='http://arxiv.org/abs/2402.02872v1'>2402.02872v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08577v1")'>MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in
  3D World</div>
<div id='2401.08577v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:59:45Z</div><div>Authors: Yining Hong, Zishuo Zheng, Peihao Chen, Yian Wang, Junyan Li, Chuang Gan</div><div style='padding-top: 10px; width: 80ex'>Human beings possess the capability to multiply a melange of multisensory
cues while actively exploring and interacting with the 3D world. Current
multi-modal large language models, however, passively absorb sensory data as
inputs, lacking the capacity to actively interact with the objects in the 3D
environment and dynamically collect their multisensory information. To usher in
the study of this area, we propose MultiPLY, a multisensory embodied large
language model that could incorporate multisensory interactive data, including
visual, audio, tactile, and thermal information into large language models,
thereby establishing the correlation among words, actions, and percepts. To
this end, we first collect Multisensory Universe, a large-scale multisensory
interaction dataset comprising 500k data by deploying an LLM-powered embodied
agent to engage with the 3D environment. To perform instruction tuning with
pre-trained LLM on such generated data, we first encode the 3D scene as
abstracted object-centric representations and then introduce action tokens
denoting that the embodied agent takes certain actions within the environment,
as well as state tokens that represent the multisensory state observations of
the agent at each time step. In the inference time, MultiPLY could generate
action tokens, instructing the agent to take the action in the environment and
obtain the next multisensory state observation. The observation is then
appended back to the LLM via state tokens to generate subsequent text or action
tokens. We demonstrate that MultiPLY outperforms baselines by a large margin
through a diverse set of embodied tasks involving object retrieval, tool use,
multisensory captioning, and task decomposition.</div><div><a href='http://arxiv.org/abs/2401.08577v1'>2401.08577v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17128v3")'>OSCaR: Object State Captioning and State Change Representation</div>
<div id='2402.17128v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T01:48:19Z</div><div>Authors: Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, Chenliang Xu</div><div style='padding-top: 10px; width: 80ex'>The capability of intelligent models to extrapolate and comprehend changes in
object states is a crucial yet demanding aspect of AI research, particularly
through the lens of human interaction in real-world settings. This task
involves describing complex visual environments, identifying active objects,
and interpreting their changes as conveyed through language. Traditional
methods, which isolate object captioning and state change detection, offer a
limited view of dynamic environments. Moreover, relying on a small set of
symbolic words to represent changes has restricted the expressiveness of the
language. To address these challenges, in this paper, we introduce the Object
State Captioning and State Change Representation (OSCaR) dataset and benchmark.
OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique
objects from various egocentric video collections. It sets a new testbed for
evaluating multimodal large language models (MLLMs). Our experiments
demonstrate that while MLLMs show some skill, they lack a full understanding of
object state changes. The benchmark includes a fine-tuned model that, despite
initial capabilities, requires significant improvements in accuracy and
generalization ability for effective understanding of these changes. Our code
and dataset are available at https://github.com/nguyennm1024/OSCaR.</div><div><a href='http://arxiv.org/abs/2402.17128v3'>2402.17128v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12972v1")'>On the Efficacy of Text-Based Input Modalities for Action Anticipation</div>
<div id='2401.12972v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T18:58:35Z</div><div>Authors: Apoorva Beedu, Karan Samel, Irfan Essa</div><div style='padding-top: 10px; width: 80ex'>Although the task of anticipating future actions is highly uncertain,
information from additional modalities help to narrow down plausible action
choices. Each modality provides different environmental context for the model
to learn from. While previous multi-modal methods leverage information from
modalities such as video and audio, we primarily explore how text inputs for
actions and objects can also enable more accurate action anticipation.
Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an
attention-based video transformer architecture that jointly learns from
multi-modal features and text captions. We train our model in two-stages, where
the model first learns to predict actions in the video clip by aligning with
captions, and during the second stage, we fine-tune the model to predict future
actions. Compared to existing methods, MAT has the advantage of learning
additional environmental context from two kinds of text inputs: action
descriptions during the pre-training stage, and the text inputs for detected
objects and actions during modality feature fusion. Through extensive
experiments, we evaluate the effectiveness of the pre-training stage, and show
that our model outperforms previous methods on all datasets. In addition, we
examine the impact of object and action information obtained via text and
perform extensive ablations. We evaluate the performance on on three datasets:
EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text
descriptions do indeed aid in more effective action anticipation.</div><div><a href='http://arxiv.org/abs/2401.12972v1'>2401.12972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13805v1")'>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</div>
<div id='2403.13805v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:59:55Z</div><div>Authors: Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang</div><div style='padding-top: 10px; width: 80ex'>CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from
noise image-text pairs to excel at recognizing a wide array of candidates, yet
its focus on broad associations hinders the precision in distinguishing subtle
differences among fine-grained items. Conversely, Multimodal Large Language
Models (MLLMs) excel at classifying fine-grained categories, thanks to their
substantial knowledge from pre-training on web-level corpora. However, the
performance of MLLMs declines with an increase in category numbers, primarily
due to growing complexity and constraints of limited context window size. To
synergize the strengths of both approaches and enhance the few-shot/zero-shot
recognition abilities for datasets characterized by extensive and fine-grained
vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented
method for MLLMs. We initially establish a multi-modal retriever based on CLIP
to create and store explicit memory for different categories beyond the
immediate context window. During inference, RAR retrieves the top-k similar
results from the memory and uses MLLMs to rank and make the final predictions.
Our proposed approach not only addresses the inherent limitations in
fine-grained recognition but also preserves the model's comprehensive knowledge
base, significantly boosting accuracy across a range of vision-language
recognition tasks. Notably, our approach demonstrates a significant improvement
in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot
image recognition datasets, and the 2 object detection datasets under the
zero-shot recognition setting.</div><div><a href='http://arxiv.org/abs/2403.13805v1'>2403.13805v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12425v2")'>The Neglected Tails of Vision-Language Models</div>
<div id='2401.12425v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T01:25:00Z</div><div>Authors: Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, Shu Kong</div><div style='padding-top: 10px; width: 80ex'>Vision-language models (VLMs) excel in zero-shot recognition but their
performance varies greatly across different visual concepts. For example,
although CLIP achieves impressive accuracy on ImageNet (60-80%), its
performance drops below 10% for more than ten concepts like night snake,
presumably due to their limited presence in the pretraining data. However,
measuring the frequency of concepts in VLMs' large-scale datasets is
challenging. We address this by using large language models (LLMs) to count the
number of pretraining texts that contain synonyms of these concepts. Our
analysis confirms that popular datasets, such as LAION, exhibit a long-tailed
concept distribution, yielding biased performance in VLMs. We also find that
downstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and
text-to-image models (e.g., Stable Diffusion), often fail to recognize or
generate images of rare concepts identified by our method. To mitigate the
imbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented
Learning (REAL). First, instead of prompting VLMs using the original class
names, REAL uses their most frequent synonyms found in pretraining texts. This
simple change already outperforms costly human-engineered and LLM-enriched
prompts over nine benchmark datasets. Second, REAL trains a linear classifier
on a small yet balanced set of pretraining data retrieved using concept
synonyms. REAL surpasses the previous zero-shot SOTA, using 400x less storage
and 10,000x less training time!</div><div><a href='http://arxiv.org/abs/2401.12425v2'>2401.12425v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08473v1")'>Intriguing Differences Between Zero-Shot and Systematic Evaluations of
  Vision-Language Transformer Models</div>
<div id='2402.08473v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T14:07:49Z</div><div>Authors: Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu, Lingjiong Zhu</div><div style='padding-top: 10px; width: 80ex'>Transformer-based models have dominated natural language processing and other
areas in the last few years due to their superior (zero-shot) performance on
benchmark datasets. However, these models are poorly understood due to their
complexity and size. While probing-based methods are widely used to understand
specific properties, the structures of the representation space are not
systematically characterized; consequently, it is unclear how such models
generalize and overgeneralize to new inputs beyond datasets. In this paper,
based on a new gradient descent optimization method, we are able to explore the
embedding space of a commonly used vision-language model. Using the Imagenette
dataset, we show that while the model achieves over 99\% zero-shot
classification performance, it fails systematic evaluations completely. Using a
linear approximation, we provide a framework to explain the striking
differences. We have also obtained similar results using a different model to
support that our results are applicable to other transformer models with
continuous inputs. We also propose a robust way to detect the modified images.</div><div><a href='http://arxiv.org/abs/2402.08473v1'>2402.08473v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12336v1")'>Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings
  for Robust Large Vision-Language Models</div>
<div id='2402.12336v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:09:48Z</div><div>Authors: Christian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein</div><div style='padding-top: 10px; width: 80ex'>Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are
increasingly used for various real-world tasks. Prior work has shown that these
models are highly vulnerable to adversarial attacks on the vision modality.
These attacks can be leveraged to spread fake information or defraud users, and
thus pose a significant risk, which makes the robustness of large multi-modal
foundation models a pressing problem. The CLIP model, or one of its variants,
is used as a frozen vision encoder in many vision-language models (VLMs), e.g.
LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning
scheme to obtain a robust CLIP vision encoder, which yields robustness on all
vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In
particular, we show that stealth-attacks on users of VLMs by a malicious third
party providing manipulated images are no longer possible once one replaces the
original CLIP model with our robust one. No retraining or fine-tuning of the
VLM is required. The code and robust models are available at
https://github.com/chs20/RobustVLM</div><div><a href='http://arxiv.org/abs/2402.12336v1'>2402.12336v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12983v1")'>OSSCAR: One-Shot Structured Pruning in Vision and Language Models with
  Combinatorial Optimization</div>
<div id='2403.12983v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T19:38:10Z</div><div>Authors: Xiang Meng, Shibal Ibrahim, Kayhan Behdin, Hussein Hazimeh, Natalia Ponomareva, Rahul Mazumder</div><div style='padding-top: 10px; width: 80ex'>Structured pruning is a promising approach for reducing the inference costs
of large vision and language models. By removing carefully chosen structures,
e.g., neurons or attention heads, the improvements from this approach can be
realized on standard deep learning hardware. In this work, we focus on
structured pruning in the one-shot (post-training) setting, which does not
require model retraining after pruning. We propose a novel combinatorial
optimization framework for this problem, based on a layer-wise reconstruction
objective and a careful reformulation that allows for scalable optimization.
Moreover, we design a new local combinatorial optimization algorithm, which
exploits low-rank updates for efficient local search. Our framework is time and
memory-efficient and considerably improves upon state-of-the-art one-shot
methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g.,
OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to
$125\times$ lower test perplexity on WikiText with $2\times$ inference time
speedup in comparison to the state-of-the-art ZipLM approach. Our framework is
also $6\times$ -- $8\times$ faster. Notably, our work considers models with
tens of billions of parameters, which is up to $100\times$ larger than what has
been previously considered in the structured pruning literature.</div><div><a href='http://arxiv.org/abs/2403.12983v1'>2403.12983v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00626v2")'>Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks</div>
<div id='2402.00626v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T14:41:20Z</div><div>Authors: Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A. Plummer</div><div style='padding-top: 10px; width: 80ex'>Typographic Attacks, which involve pasting misleading text onto an image,
were noted to harm the performance of Vision-Language Models like CLIP.
However, the susceptibility of recent Large Vision-Language Models to these
attacks remains understudied. Furthermore, prior work's Typographic attacks
against CLIP randomly sample a misleading class from a predefined set of
categories. However, this simple strategy misses more effective attacks that
exploit LVLM(s) stronger language skills. To address these issues, we first
introduce a benchmark for testing Typographic attacks against LVLM(s).
Moreover, we introduce two novel and more effective \textit{Self-Generated}
attacks which prompt the LVLM to generate an attack against itself: 1) Class
Based Attack where the LVLM (e.g. LLaVA) is asked which deceiving class is most
similar to the target class and 2) Descriptive Attacks where a more advanced
LVLM (e.g. GPT4-V) is asked to recommend a Typographic attack that includes
both a deceiving class and description. Using our benchmark, we uncover that
Self-Generated attacks pose a significant threat, reducing LVLM(s)
classification performance by up to 33\%. We also uncover that attacks
generated by one model (e.g. GPT-4V or LLaVA) are effective against the model
itself and other models like InstructBLIP and MiniGPT4. Code:
\url{https://github.com/mqraitem/Self-Gen-Typo-Attack}</div><div><a href='http://arxiv.org/abs/2402.00626v2'>2402.00626v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06659v1")'>Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language
  Models</div>
<div id='2402.06659v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:55:53Z</div><div>Authors: Yuancheng Xu, Jiarui Yao, Manli Shu, Yanchao Sun, Zichu Wu, Ning Yu, Tom Goldstein, Furong Huang</div><div style='padding-top: 10px; width: 80ex'>Vision-Language Models (VLMs) excel in generating textual responses from
visual inputs, yet their versatility raises significant security concerns. This
study takes the first step in exposing VLMs' susceptibility to data poisoning
attacks that can manipulate responses to innocuous, everyday prompts. We
introduce Shadowcast, a stealthy data poisoning attack method where poison
samples are visually indistinguishable from benign images with matching texts.
Shadowcast demonstrates effectiveness in two attack types. The first is Label
Attack, tricking VLMs into misidentifying class labels, such as confusing
Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages
VLMs' text generation capabilities to craft narratives, such as portraying junk
food as health food, through persuasive and seemingly rational descriptions. We
show that Shadowcast are highly effective in achieving attacker's intentions
using as few as 50 poison samples. Moreover, these poison samples remain
effective across various prompts and are transferable across different VLM
architectures in the black-box setting. This work reveals how poisoned VLMs can
generate convincing yet deceptive misinformation and underscores the importance
of data quality for responsible deployments of VLMs. Our code is available at:
https://github.com/umd-huang-lab/VLM-Poisoning.</div><div><a href='http://arxiv.org/abs/2402.06659v1'>2402.06659v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09671v1")'>Exploiting Alpha Transparency In Language And Vision-Based AI Systems</div>
<div id='2402.09671v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T02:38:23Z</div><div>Authors: David Noever, Forrest McKee</div><div style='padding-top: 10px; width: 80ex'>This investigation reveals a novel exploit derived from PNG image file
formats, specifically their alpha transparency layer, and its potential to fool
multiple AI vision systems. Our method uses this alpha layer as a clandestine
channel invisible to human observers but fully actionable by AI image
processors. The scope tested for the vulnerability spans representative vision
systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook,
highlighting the attack's potential breadth. This vulnerability challenges the
security protocols of existing and fielded vision systems, from medical imaging
to autonomous driving technologies. Our experiments demonstrate that the
affected systems, which rely on convolutional neural networks or the latest
multimodal language models, cannot quickly mitigate these vulnerabilities
through simple patches or updates. Instead, they require retraining and
architectural changes, indicating a persistent hole in multimodal technologies
without some future adversarial hardening against such vision-language
exploits.</div><div><a href='http://arxiv.org/abs/2402.09671v1'>2402.09671v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15817v1")'>Transparency Attacks: How Imperceptible Image Layers Can Fool AI
  Perception</div>
<div id='2401.15817v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T00:52:01Z</div><div>Authors: Forrest McKee, David Noever</div><div style='padding-top: 10px; width: 80ex'>This paper investigates a novel algorithmic vulnerability when imperceptible
image layers confound multiple vision models into arbitrary label assignments
and captions. We explore image preprocessing methods to introduce stealth
transparency, which triggers AI misinterpretation of what the human eye
perceives. The research compiles a broad attack surface to investigate the
consequences ranging from traditional watermarking, steganography, and
background-foreground miscues. We demonstrate dataset poisoning using the
attack to mislabel a collection of grayscale landscapes and logos using either
a single attack layer or randomly selected poisoning classes. For example, a
military tank to the human eye is a mislabeled bridge to object classifiers
based on convolutional networks (YOLO, etc.) and vision transformers (ViT,
GPT-Vision, etc.). A notable attack limitation stems from its dependency on the
background (hidden) layer in grayscale as a rough match to the transparent
foreground image that the human eye perceives. This dependency limits the
practical success rate without manual tuning and exposes the hidden layers when
placed on the opposite display theme (e.g., light background, light transparent
foreground visible, works best against a light theme image viewer or browser).
The stealth transparency confounds established vision systems, including
evading facial recognition and surveillance, digital watermarking, content
filtering, dataset curating, automotive and drone autonomy, forensic evidence
tampering, and retail product misclassifying. This method stands in contrast to
traditional adversarial attacks that typically focus on modifying pixel values
in ways that are either slightly perceptible or entirely imperceptible for both
humans and machines.</div><div><a href='http://arxiv.org/abs/2401.15817v1'>2401.15817v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15430v1")'>Hierarchical Invariance for Robust and Interpretable Vision Tasks at
  Larger Scales</div>
<div id='2402.15430v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:50:07Z</div><div>Authors: Shuren Qi, Yushu Zhang, Chao Wang, Zhihua Xia, Jian Weng, Xiaochun Cao</div><div style='padding-top: 10px; width: 80ex'>Developing robust and interpretable vision systems is a crucial step towards
trustworthy artificial intelligence. In this regard, a promising paradigm
considers embedding task-required invariant structures, e.g., geometric
invariance, in the fundamental image representation. However, such invariant
representations typically exhibit limited discriminability, limiting their
applications in larger-scale trustworthy vision tasks. For this open problem,
we conduct a systematic investigation of hierarchical invariance, exploring
this topic from theoretical, practical, and application perspectives. At the
theoretical level, we show how to construct over-complete invariants with a
Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a
fully interpretable manner. The general blueprint, specific definitions,
invariant properties, and numerical implementations are provided. At the
practical level, we discuss how to customize this theoretical framework into a
given task. With the over-completeness, discriminative features w.r.t. the task
can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We
demonstrate the above arguments with accuracy, invariance, and efficiency
results on texture, digit, and parasite classification experiments.
Furthermore, at the application level, our representations are explored in
real-world forensics tasks on adversarial perturbations and Artificial
Intelligence Generated Content (AIGC). Such applications reveal that the
proposed strategy not only realizes the theoretically promised invariance, but
also exhibits competitive discriminability even in the era of deep learning.
For robust and interpretable vision tasks at larger scales, hierarchical
invariant representation can be considered as an effective alternative to
traditional CNN and invariants.</div><div><a href='http://arxiv.org/abs/2402.15430v1'>2402.15430v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03095v1")'>Transcending Adversarial Perturbations: Manifold-Aided Adversarial
  Examples with Legitimate Semantics</div>
<div id='2402.03095v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:25:40Z</div><div>Authors: Shuai Li, Xiaoyu Jiang, Xiaoguang Ma</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks were significantly vulnerable to adversarial examples
manipulated by malicious tiny perturbations. Although most conventional
adversarial attacks ensured the visual imperceptibility between adversarial
examples and corresponding raw images by minimizing their geometric distance,
these constraints on geometric distance led to limited attack transferability,
inferior visual quality, and human-imperceptible interpretability. In this
paper, we proposed a supervised semantic-transformation generative model to
generate adversarial examples with real and legitimate semantics, wherein an
unrestricted adversarial manifold containing continuous semantic variations was
constructed for the first time to realize a legitimate transition from
non-adversarial examples to adversarial ones. Comprehensive experiments on
MNIST and industrial defect datasets showed that our adversarial examples not
only exhibited better visual quality but also achieved superior attack
transferability and more effective explanations for model vulnerabilities,
indicating their great potential as generic adversarial examples. The code and
pre-trained models were available at https://github.com/shuaili1027/MAELS.git.</div><div><a href='http://arxiv.org/abs/2402.03095v1'>2402.03095v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12075v1")'>Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse
  Harms in Text-to-Image Generation</div>
<div id='2403.12075v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T22:21:12Z</div><div>Authors: Jessica Quaye, Alicia Parrish, Oana Inel, Charvi Rastogi, Hannah Rose Kirk, Minsuk Kahng, Erin van Liemt, Max Bartolo, Jess Tsang, Justin White, Nathan Clement, Rafael Mosquera, Juan Ciro, Vijay Janapa Reddi, Lora Aroyo</div><div style='padding-top: 10px; width: 80ex'>With the rise of text-to-image (T2I) generative AI models reaching wide
audiences, it is critical to evaluate model robustness against non-obvious
attacks to mitigate the generation of offensive images. By focusing on
``implicitly adversarial'' prompts (those that trigger T2I models to generate
unsafe images for non-obvious reasons), we isolate a set of difficult safety
issues that human creativity is well-suited to uncover. To this end, we built
the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing
a diverse set of implicitly adversarial prompts. We have assembled a suite of
state-of-the-art T2I models, employed a simple user interface to identify and
annotate harms, and engaged diverse populations to capture long-tail safety
issues that may be overlooked in standard testing. The challenge is run in
consecutive rounds to enable a sustained discovery and analysis of safety
pitfalls in T2I models.
  In this paper, we present an in-depth account of our methodology, a
systematic study of novel attack strategies and discussion of safety failures
revealed by challenge participants. We also release a companion visualization
tool for easy exploration and derivation of insights from the dataset. The
first challenge round resulted in over 10k prompt-image pairs with machine
annotations for safety. A subset of 1.5k samples contains rich human
annotations of harm types and attack styles. We find that 14% of images that
humans consider harmful are mislabeled as ``safe'' by machines. We have
identified new attack strategies that highlight the complexity of ensuring T2I
model robustness. Our findings emphasize the necessity of continual auditing
and adaptation as new vulnerabilities emerge. We are confident that this work
will enable proactive, iterative safety assessments and promote responsible
development of T2I models.</div><div><a href='http://arxiv.org/abs/2403.12075v1'>2403.12075v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01787v1")'>Harm Amplification in Text-to-Image Models</div>
<div id='2402.01787v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T23:12:57Z</div><div>Authors: Susan Hao, Renee Shelby, Yuchi Liu, Hansa Srinivasan, Mukul Bhutani, Burcu Karagol Ayan, Shivani Poddar, Sarah Laszlo</div><div style='padding-top: 10px; width: 80ex'>Text-to-image (T2I) models have emerged as a significant advancement in
generative AI; however, there exist safety concerns regarding their potential
to produce harmful image outputs even when users input seemingly safe prompts.
This phenomenon, where T2I models generate harmful representations that were
not explicit in the input, poses a potentially greater risk than adversarial
prompts, leaving users unintentionally exposed to harms. Our paper addresses
this issue by first introducing a formal definition for this phenomenon, termed
harm amplification. We further contribute to the field by developing
methodologies to quantify harm amplification in which we consider the harm of
the model output in the context of user input. We then empirically examine how
to apply these different methodologies to simulate real-world deployment
scenarios including a quantification of disparate impacts across genders
resulting from harm amplification. Together, our work aims to offer researchers
tools to comprehensively address safety challenges in T2I systems and
contribute to the responsible deployment of generative AI models.</div><div><a href='http://arxiv.org/abs/2402.01787v1'>2402.01787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16092v2")'>Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and
  Prompt Engineering May Not Help You</div>
<div id='2401.16092v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T12:02:28Z</div><div>Authors: Felix Friedrich, Katharina H√§mmerl, Patrick Schramowski, Jindrich Libovicky, Kristian Kersting, Alexander Fraser</div><div style='padding-top: 10px; width: 80ex'>Text-to-image generation models have recently achieved astonishing results in
image quality, flexibility, and text alignment and are consequently employed in
a fast-growing number of applications. Through improvements in multilingual
abilities, a larger community now has access to this kind of technology. Yet,
as we will show, multilingual models suffer similarly from (gender) biases as
monolingual models. Furthermore, the natural expectation is that these models
will provide similar results across languages, but this is not the case and
there are important differences between languages. Thus, we propose a novel
benchmark MAGBIG intending to foster research in multilingual models without
gender bias. We investigate whether multilingual T2I models magnify gender bias
with MAGBIG. To this end, we use multilingual prompts requesting portrait
images of persons of a certain occupation or trait (using adjectives). Our
results show not only that models deviate from the normative assumption that
each gender should be equally likely to be generated, but that there are also
big differences across languages. Furthermore, we investigate prompt
engineering strategies, i.e. the use of indirect, neutral formulations, as a
possible remedy for these biases. Unfortunately, they help only to a limited
extent and result in worse text-to-image alignment. Consequently, this work
calls for more research into diverse representations across languages in image
generators.</div><div><a href='http://arxiv.org/abs/2401.16092v2'>2401.16092v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02580v1")'>What do we learn from inverting CLIP models?</div>
<div id='2403.02580v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T01:32:29Z</div><div>Authors: Hamid Kazemi, Atoosa Chegini, Jonas Geiping, Soheil Feizi, Tom Goldstein</div><div style='padding-top: 10px; width: 80ex'>We employ an inversion-based approach to examine CLIP models. Our examination
reveals that inverting CLIP models results in the generation of images that
exhibit semantic alignment with the specified target prompts. We leverage these
inverted images to gain insights into various aspects of CLIP models, such as
their ability to blend concepts and inclusion of gender biases. We notably
observe instances of NSFW (Not Safe For Work) images during model inversion.
This phenomenon occurs even for semantically innocuous prompts, like "a
beautiful landscape," as well as for prompts involving the names of
celebrities.</div><div><a href='http://arxiv.org/abs/2403.02580v1'>2403.02580v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13641v2")'>How Good is ChatGPT at Face Biometrics? A First Look into Recognition,
  Soft Biometrics, and Explainability</div>
<div id='2401.13641v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T18:10:39Z</div><div>Authors: Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) such as GPT developed by OpenAI, have already
shown astonishing results, introducing quick changes in our society. This has
been intensified by the release of ChatGPT which allows anyone to interact in a
simple conversational way with LLMs, without any experience in the field
needed. As a result, ChatGPT has been rapidly applied to many different tasks
such as code- and song-writer, education, virtual assistants, etc., showing
impressive results for tasks for which it was not trained (zero-shot learning).
  The present study aims to explore the ability of ChatGPT, based on the recent
GPT-4 multimodal LLM, for the task of face biometrics. In particular, we
analyze the ability of ChatGPT to perform tasks such as face verification,
soft-biometrics estimation, and explainability of the results. ChatGPT could be
very valuable to further increase the explainability and transparency of
automatic decisions in human scenarios. Experiments are carried out in order to
evaluate the performance and robustness of ChatGPT, using popular public
benchmarks and comparing the results with state-of-the-art methods in the
field. The results achieved in this study show the potential of LLMs such as
ChatGPT for face biometrics, especially to enhance explainability. For
reproducibility reasons, we release all the code in GitHub.</div><div><a href='http://arxiv.org/abs/2401.13641v2'>2401.13641v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10499v1")'>Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A
  Pilot Study</div>
<div id='2403.10499v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:33:49Z</div><div>Authors: Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song</div><div style='padding-top: 10px; width: 80ex'>Pre-training image representations from the raw text about images enables
zero-shot vision transfer to downstream tasks. Through pre-training on millions
of samples collected from the internet, multimodal foundation models, such as
CLIP, produce state-of-the-art zero-shot results that often reach
competitiveness with fully supervised methods without the need for
task-specific training. Besides the encouraging performance on classification
accuracy, it is reported that these models close the robustness gap by matching
the performance of supervised models trained on ImageNet under natural
distribution shift. Because robustness is critical to real-world applications,
especially safety-critical ones, in this paper, we present a comprehensive
evaluation based on a large-scale robustness benchmark covering 7 natural, 3
synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a
pilot study. We show that CLIP leads to a significant robustness drop compared
to supervised ImageNet models on our benchmark, especially under synthetic
distribution shift and adversarial attacks. Furthermore, data overlap analysis
suggests that the observed robustness under natural distribution shifts could
be attributed, at least in part, to data overlap. In summary, our evaluation
shows a comprehensive evaluation of robustness is necessary; and there is a
significant need to improve the robustness of zero-shot multimodal models.</div><div><a href='http://arxiv.org/abs/2403.10499v1'>2403.10499v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13854v1")'>Embedding Attack Project (Work Report)</div>
<div id='2401.13854v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T23:35:29Z</div><div>Authors: Jiameng Pu, Zafar Takhirov</div><div style='padding-top: 10px; width: 80ex'>This report summarizes all the MIA experiments (Membership Inference Attacks)
of the Embedding Attack Project, including threat models, experimental setup,
experimental results, findings and discussion. Current results cover the
evaluation of two main MIA strategies (loss-based and embedding-based MIAs) on
6 AI models ranging from Computer Vision to Language Modelling. There are two
ongoing experiments on MIA defense and neighborhood-comparison embedding
attacks. These are ongoing projects.
  The current work on MIA and PIA can be summarized into six conclusions: (1)
Amount of overfitting is directly proportional to model's vulnerability; (2)
early embedding layers in the model are less susceptible to privacy leaks; (3)
Deeper model layers contain more membership information; (4) Models are more
vulnerable to MIA if both embeddings and corresponding training labels are
compromised; (5) it is possible to use pseudo-labels to increase the MIA
success; and (6) although MIA and PIA success rates are proportional, reducing
the MIA does not necessarily reduce the PIA.</div><div><a href='http://arxiv.org/abs/2401.13854v1'>2401.13854v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12952v1")'>Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization
  with Vision-Language Models</div>
<div id='2403.12952v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:54:34Z</div><div>Authors: Elaine Sui, Xiaohan Wang, Serena Yeung-Levy</div><div style='padding-top: 10px; width: 80ex'>Advancements in vision-language models (VLMs) have propelled the field of
computer vision, particularly in the zero-shot learning setting. Despite their
promise, the effectiveness of these models often diminishes due to domain
shifts in test environments. To address this, we introduce the Test-Time
Prototype Shifting (TPS) framework, a pioneering approach designed to adapt
VLMs to test datasets using unlabeled test inputs. Our method is based on the
notion of modulating per-class prototypes in the shared embedding space. By
pre-computing and caching prototypes generated with the pre-trained text
encoder, TPS not only facilitates optimization-free prototype reuse for
subsequent predictions but also enables seamless integration with current
advancements in prompt engineering. At test-time, TPS dynamically learns shift
vectors for each prototype based solely on the given test sample, effectively
bridging the domain gap and enhancing classification accuracy. A notable aspect
of our framework is its significantly reduced memory and computational demands
when compared to conventional text-prompt tuning methods. Extensive evaluations
across 15 datasets involving natural distribution shifts and cross-dataset
generalization demonstrate TPS's superior performance, achieving
state-of-the-art results while reducing resource requirements.</div><div><a href='http://arxiv.org/abs/2403.12952v1'>2403.12952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07270v1")'>Open-ended VQA benchmarking of Vision-Language models by exploiting
  Classification datasets and their semantic hierarchy</div>
<div id='2402.07270v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T18:26:18Z</div><div>Authors: Simon Ging, Mar√≠a A. Bravo, Thomas Brox</div><div style='padding-top: 10px; width: 80ex'>The evaluation of text-generative vision-language models is a challenging yet
crucial endeavor. By addressing the limitations of existing Visual Question
Answering (VQA) benchmarks and proposing innovative evaluation methodologies,
our research seeks to advance our understanding of these models' capabilities.
We propose a novel VQA benchmark based on well-known visual classification
datasets which allows a granular evaluation of text-generative vision-language
models and their comparison with discriminative vision-language models. To
improve the assessment of coarse answers on fine-grained classification tasks,
we suggest using the semantic hierarchy of the label space to ask automatically
generated follow-up questions about the ground-truth category. Finally, we
compare traditional NLP and LLM-based metrics for the problem of evaluating
model predictions given ground-truth answers. We perform a human evaluation
study upon which we base our decision on the final metric. We apply our
benchmark to a suite of vision-language models and show a detailed comparison
of their abilities on object, action, and attribute classification. Our
contributions aim to lay the foundation for more precise and meaningful
assessments, facilitating targeted progress in the exciting field of
vision-language modeling.</div><div><a href='http://arxiv.org/abs/2402.07270v1'>2402.07270v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02325v1")'>Contrastive Region Guidance: Improving Grounding in Vision-Language
  Models without Training</div>
<div id='2403.02325v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T18:55:30Z</div><div>Authors: David Wan, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</div><div style='padding-top: 10px; width: 80ex'>Highlighting particularly relevant regions of an image can improve the
performance of vision-language models (VLMs) on various vision-language (VL)
tasks by guiding the model to attend more closely to these regions of interest.
For example, VLMs can be given a "visual prompt", where visual markers such as
bounding boxes delineate key image regions. However, current VLMs that can
incorporate visual guidance are either proprietary and expensive or require
costly training on curated data that includes visual prompts. We introduce
Contrastive Region Guidance (CRG), a training-free guidance method that enables
open-source VLMs to respond to visual prompts. CRG contrasts model outputs
produced with and without visual prompts, factoring out biases revealed by the
model when answering without the information required to produce a correct
answer (i.e., the model's prior). CRG achieves substantial improvements in a
wide variety of VL tasks: When region annotations are provided, CRG increases
absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse
region-based tasks such as recognition, math, and object relationship
reasoning. We also show CRG's applicability to spatial reasoning, with 10%
improvement on What'sUp, as well as to compositional generalization --
improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe
-- and to image-text alignment for generated images, where we improve by up to
8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG
allows us to re-rank proposed regions in referring expression comprehension and
phrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an
average gain of 3.2% in accuracy. Our analysis explores alternative masking
strategies for CRG, quantifies CRG's probability shift, and evaluates the role
of region guidance strength, empirically validating CRG's design choices.</div><div><a href='http://arxiv.org/abs/2403.02325v1'>2403.02325v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13804v1")'>Learning from Models and Data for Visual Grounding</div>
<div id='2403.13804v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:59:43Z</div><div>Authors: Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg, Vicente Ordonez</div><div style='padding-top: 10px; width: 80ex'>We introduce SynGround, a novel framework that combines data-driven learning
and knowledge transfer from various large-scale pretrained models to enhance
the visual grounding capabilities of a pretrained vision-and-language model.
The knowledge transfer from the models initiates the generation of image
descriptions through an image description generator. These descriptions serve
dual purposes: they act as prompts for synthesizing images through a
text-to-image generator, and as queries for synthesizing text, from which
phrases are extracted using a large language model. Finally, we leverage an
open-vocabulary object detector to generate synthetic bounding boxes for the
synthetic images and texts. We finetune a pretrained vision-and-language model
on this dataset by optimizing a mask-attention consistency objective that
aligns region annotations with gradient-based model explanations. The resulting
model improves the grounding capabilities of an off-the-shelf
vision-and-language model. Particularly, SynGround improves the pointing game
accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on
RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to
63.67%.</div><div><a href='http://arxiv.org/abs/2403.13804v1'>2403.13804v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08680v1")'>Mitigating Object Hallucination in Large Vision-Language Models via
  Classifier-Free Guidance</div>
<div id='2402.08680v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:59:05Z</div><div>Authors: Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>The advancement of Large Vision-Language Models (LVLMs) has increasingly
highlighted the critical issue of their tendency to hallucinate non-existing
objects in the images. To address this issue, previous works focused on using
specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the
outputs of LVLMs. However, these approaches require either expensive
training/fine-tuning or API access to advanced LLMs to correct the model's
output post-generation. In this paper, we tackle this challenge by introducing
a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE
(MARINE), which is both training-free and API-free, and can effectively and
efficiently reduce object hallucinations during the generation process.
Specifically, MARINE enriches the visual context of LVLMs by integrating
existing open-source vision models, and employs classifier-free guidance to
incorporate the additional object grounding features to improve the precision
of LVLMs' generations. Through comprehensive evaluations across $6$ popular
LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of
MARINE, which even outperforms existing fine-tuning-based methods. Remarkably,
it not only reduces hallucinations but also improves the detailedness of LVLMs'
generations, as assessed by GPT-4V.</div><div><a href='http://arxiv.org/abs/2402.08680v1'>2402.08680v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07865v1")'>Prismatic VLMs: Investigating the Design Space of Visually-Conditioned
  Language Models</div>
<div id='2402.07865v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:21:14Z</div><div>Authors: Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, Dorsa Sadigh</div><div style='padding-top: 10px; width: 80ex'>Visually-conditioned language models (VLMs) have seen growing adoption in
applications such as visual dialogue, scene understanding, and robotic task
planning; adoption that has fueled a wealth of new models such as LLaVa,
InstructBLIP, and PaLI-3. Despite the volume of new releases, key design
decisions around image preprocessing, architecture, and optimization are
under-explored, making it challenging to understand what factors account for
model performance $-$ a challenge further complicated by the lack of objective,
consistent evaluations. To address these gaps, we first compile a suite of
standardized evaluations spanning visual question answering, object
localization from language, and targeted challenge sets that probe properties
such as hallucination; evaluations that provide calibrated, fine-grained
insight into a VLM's capabilities. Second, we rigorously investigate VLMs along
key design axes, including pretrained visual representations and quantifying
the tradeoffs of using base vs. instruct-tuned language models, amongst others.
We couple our analysis with three resource contributions: (1) a unified
framework for evaluating VLMs, (2) optimized, flexible code for VLM training,
and (3) checkpoints for all models, including a family of VLMs at the 7-13B
scale that strictly outperform InstructBLIP and LLaVa v1.5, the
state-of-the-art in open-source VLMs.</div><div><a href='http://arxiv.org/abs/2402.07865v1'>2402.07865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07872v1")'>PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs</div>
<div id='2402.07872v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:33:47Z</div><div>Authors: Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, Brian Ichter</div><div style='padding-top: 10px; width: 80ex'>Vision language models (VLMs) have shown impressive capabilities across a
variety of tasks, from logical reasoning to visual understanding. This opens
the door to richer interaction with the world, for example robotic control.
However, VLMs produce only textual outputs, while robotic control and other
spatial tasks require outputting continuous coordinates, actions, or
trajectories. How can we enable VLMs to handle such settings without
fine-tuning on task-specific data?
  In this paper, we propose a novel visual prompting approach for VLMs that we
call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as
iterative visual question answering. In each iteration, the image is annotated
with a visual representation of proposals that the VLM can refer to (e.g.,
candidate robot actions, localizations, or trajectories). The VLM then selects
the best ones for the task. These proposals are iteratively refined, allowing
the VLM to eventually zero in on the best available answer. We investigate
PIVOT on real-world robotic navigation, real-world manipulation from images,
instruction following in simulation, and additional spatial inference tasks
such as localization. We find, perhaps surprisingly, that our approach enables
zero-shot control of robotic systems without any robot training data,
navigation in a variety of environments, and other capabilities. Although
current performance is far from perfect, our work highlights potentials and
limitations of this new regime and shows a promising approach for
Internet-Scale VLMs in robotic and spatial reasoning domains. Website:
pivot-prompt.github.io and HuggingFace:
https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.</div><div><a href='http://arxiv.org/abs/2402.07872v1'>2402.07872v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12168v1")'>SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning
  Capabilities</div>
<div id='2401.12168v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:01:01Z</div><div>Authors: Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia</div><div style='padding-top: 10px; width: 80ex'>Understanding and reasoning about spatial relationships is a fundamental
capability for Visual Question Answering (VQA) and robotics. While Vision
Language Models (VLM) have demonstrated remarkable performance in certain VQA
benchmarks, they still lack capabilities in 3D spatial reasoning, such as
recognizing quantitative relationships of physical objects like distances or
size differences. We hypothesize that VLMs' limited spatial reasoning
capability is due to the lack of 3D spatial knowledge in training data and aim
to solve this problem by training VLMs with Internet-scale spatial reasoning
data. To this end, we present a system to facilitate this approach. We first
develop an automatic 3D spatial VQA data generation framework that scales up to
2 billion VQA examples on 10 million real-world images. We then investigate
various factors in the training recipe, including data quality, training
pipeline, and VLM architecture. Our work features the first internet-scale 3D
spatial reasoning dataset in metric space. By training a VLM on such data, we
significantly enhance its ability on both qualitative and quantitative spatial
VQA. Finally, we demonstrate that this VLM unlocks novel downstream
applications in chain-of-thought spatial reasoning and robotics due to its
quantitative estimation capability. Project website:
https://spatial-vlm.github.io/</div><div><a href='http://arxiv.org/abs/2401.12168v1'>2401.12168v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02416v1")'>ODIN: A Single Model for 2D and 3D Perception</div>
<div id='2401.02416v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T18:59:25Z</div><div>Authors: Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki</div><div style='padding-top: 10px; width: 80ex'>State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.</div><div><a href='http://arxiv.org/abs/2401.02416v1'>2401.02416v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09477v1")'>VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance
  Fields</div>
<div id='2403.09477v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:19:19Z</div><div>Authors: Nicolaj Schmid, Cornelius von Einem, Cesar Cadena, Roland Siegwart, Lorenz Hruby, Florian Tschopp</div><div style='padding-top: 10px; width: 80ex'>Autonomous mobile robots are an increasingly integral part of modern factory
and warehouse operations. Obstacle detection, avoidance and path planning are
critical safety-relevant tasks, which are often solved using expensive LiDAR
sensors and depth cameras. We propose to use cost-effective low-resolution
ranging sensors, such as ultrasonic and infrared time-of-flight sensors by
developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance
Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution
Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from
ultrasonic and infrared sensors and utilizes them to update the occupancy grid
used for ray marching. Experimental evaluation in 2D demonstrates that
VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds
regarding coverage. Notably, in small environments, its accuracy aligns with
that of LiDAR measurements, while in larger ones, it is bounded by the utilized
ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic
and infrared sensors is highly effective when dealing with sparse data and low
view variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the
mapping capabilities and increases the training speed by 46% compared to
Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for
cost-effective local mapping in mobile robotics, with potential applications in
safety and navigation tasks. The code can be found at
https://github.com/ethz-asl/virus nerf.</div><div><a href='http://arxiv.org/abs/2403.09477v1'>2403.09477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00658v1")'>Point Cloud in the Air</div>
<div id='2401.00658v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T04:11:55Z</div><div>Authors: Yulin Shao, Chenghong Bian, Li Yang, Qianqian Yang, Zhaoyang Zhang, Deniz Gunduz</div><div style='padding-top: 10px; width: 80ex'>Acquisition and processing of point clouds (PCs) is a crucial enabler for
many emerging applications reliant on 3D spatial data, such as robot
navigation, autonomous vehicles, and augmented reality. In most scenarios, PCs
acquired by remote sensors must be transmitted to an edge server for fusion,
segmentation, or inference. Wireless transmission of PCs not only puts on
increased burden on the already congested wireless spectrum, but also confronts
a unique set of challenges arising from the irregular and unstructured nature
of PCs. In this paper, we meticulously delineate these challenges and offer a
comprehensive examination of existing solutions while candidly acknowledging
their inherent limitations. In response to these intricacies, we proffer four
pragmatic solution frameworks, spanning advanced techniques, hybrid schemes,
and distributed data aggregation approaches. In doing so, our goal is to chart
a path toward efficient, reliable, and low-latency wireless PC transmission.</div><div><a href='http://arxiv.org/abs/2401.00658v1'>2401.00658v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12202v2")'>OK-Robot: What Really Matters in Integrating Open-Knowledge Models for
  Robotics</div>
<div id='2401.12202v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:42:20Z</div><div>Authors: Peiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto</div><div style='padding-top: 10px; width: 80ex'>Remarkable progress has been made in recent years in the fields of vision,
language, and robotics. We now have vision models capable of recognizing
objects based on language queries, navigation systems that can effectively
control mobile systems, and grasping models that can handle a wide range of
objects. Despite these advancements, general-purpose applications of robotics
still lag behind, even though they rely on these fundamental capabilities of
recognition, navigation, and grasping. In this paper, we adopt a systems-first
approach to develop a new Open Knowledge-based robotics framework called
OK-Robot. By combining Vision-Language Models (VLMs) for object detection,
navigation primitives for movement, and grasping primitives for object
manipulation, OK-Robot offers a integrated solution for pick-and-drop
operations without requiring any training. To evaluate its performance, we run
OK-Robot in 10 real-world home environments. The results demonstrate that
OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks,
representing a new state-of-the-art in Open Vocabulary Mobile Manipulation
(OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered
environments, OK-Robot's performance increases to 82%. However, the most
important insight gained from OK-Robot is the critical role of nuanced details
when combining Open Knowledge systems like VLMs with robotic modules. Videos of
our experiments and code are available on our website:
https://ok-robot.github.io</div><div><a href='http://arxiv.org/abs/2401.12202v2'>2401.12202v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03388v1")'>LLMs for Robotic Object Disambiguation</div>
<div id='2401.03388v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T04:46:23Z</div><div>Authors: Connie Jiang, Yiqing Xu, David Hsu</div><div style='padding-top: 10px; width: 80ex'>The advantages of pre-trained large language models (LLMs) are apparent in a
variety of language processing tasks. But can a language model's knowledge be
further harnessed to effectively disambiguate objects and navigate
decision-making challenges within the realm of robotics? Our study reveals the
LLM's aptitude for solving complex decision making challenges that are often
previously modeled by Partially Observable Markov Decision Processes (POMDPs).
A pivotal focus of our research is the object disambiguation capability of
LLMs. We detail the integration of an LLM into a tabletop environment
disambiguation task, a decision making problem where the robot's task is to
discern and retrieve a user's desired object from an arbitrarily large and
complex cluster of objects. Despite multiple query attempts with zero-shot
prompt engineering (details can be found in the Appendix), the LLM struggled to
inquire about features not explicitly provided in the scene description. In
response, we have developed a few-shot prompt engineering system to improve the
LLM's ability to pose disambiguating queries. The result is a model capable of
both using given features when they are available and inferring new relevant
features when necessary, to successfully generate and navigate down a precise
decision tree to the correct object--even when faced with identical options.</div><div><a href='http://arxiv.org/abs/2401.03388v1'>2401.03388v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08570v1")'>Online Foundation Model Selection in Robotics</div>
<div id='2402.08570v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:14:32Z</div><div>Authors: Po-han Li, Oyku Selin Toprak, Aditya Narayanan, Ufuk Topcu, Sandeep Chinchali</div><div style='padding-top: 10px; width: 80ex'>Foundation models have recently expanded into robotics after excelling in
computer vision and natural language processing. The models are accessible in
two ways: open-source or paid, closed-source options. Users with access to both
face a problem when deciding between effective yet costly closed-source models
and free but less powerful open-source alternatives. We call it the model
selection problem. Existing supervised-learning methods are impractical due to
the high cost of collecting extensive training data from closed-source models.
Hence, we focus on the online learning setting where algorithms learn while
collecting data, eliminating the need for large pre-collected datasets. We thus
formulate a user-centric online model selection problem and propose a novel
solution that combines an open-source encoder to output context and an online
learning algorithm that processes this context. The encoder distills vast data
distributions into low-dimensional features, i.e., the context, without
additional training. The online learning algorithm aims to maximize a composite
reward that includes model performance, execution time, and costs based on the
context extracted from the data. It results in an improved trade-off between
selecting open-source and closed-source models compared to non-contextual
methods, as validated by our theoretical analysis. Experiments across
language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open
X-Embodiment demonstrate real-world applications of the solution. The results
show that the solution significantly improves the task success rate by up to
14%.</div><div><a href='http://arxiv.org/abs/2402.08570v1'>2402.08570v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12999v1")'>Prompt Selection and Augmentation for Few Examples Code Generation in
  Large Language Model and its Application in Robotics Control</div>
<div id='2403.12999v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T04:13:29Z</div><div>Authors: On Tai Wu, Frodo Kin Sun Chan, Zunhao Zhang, Yan Nei Law, Benny Drescher, Edmond Shiao Bun Lai</div><div style='padding-top: 10px; width: 80ex'>Few-shot prompting and step-by-step reasoning have enhanced the capabilities
of Large Language Models (LLMs) in tackling complex tasks including code
generation. In this paper, we introduce a prompt selection and augmentation
algorithm aimed at improving mathematical reasoning and robot arm operations.
Our approach incorporates a multi-stage example augmentation scheme combined
with an example selection scheme. This algorithm improves LLM performance by
selecting a set of examples that increase diversity, minimize redundancy, and
increase relevance to the question. When combined with the Program-of-Thought
prompting, our algorithm demonstrates an improvement in performance on the
GSM8K and SVAMP benchmarks, with increases of 0.3% and 1.1% respectively.
Furthermore, in simulated tabletop environments, our algorithm surpasses the
Code-as-Policies approach by achieving a 3.4% increase in successful task
completions and a decrease of over 70% in the number of examples used. Its
ability to discard examples that contribute little to solving the problem
reduces the inferencing time of an LLM-powered robotics system. This algorithm
also offers important benefits for industrial process automation by
streamlining the development and deployment process, reducing manual
programming effort, and enhancing code reusability.</div><div><a href='http://arxiv.org/abs/2403.12999v1'>2403.12999v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01974v1")'>Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as
  Programmers</div>
<div id='2401.01974v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T20:48:47Z</div><div>Authors: Aleksandar Staniƒá, Sergi Caelles, Michael Tschannen</div><div style='padding-top: 10px; width: 80ex'>Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.</div><div><a href='http://arxiv.org/abs/2401.01974v1'>2401.01974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12201v1")'>Compositional learning of functions in humans and machines</div>
<div id='2403.12201v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T19:22:53Z</div><div>Authors: Yanli Zhou, Brenden M. Lake, Adina Williams</div><div style='padding-top: 10px; width: 80ex'>The ability to learn and compose functions is foundational to efficient
learning and reasoning in humans, enabling flexible generalizations such as
creating new dishes from known cooking processes. Beyond sequential chaining of
functions, existing linguistics literature indicates that humans can grasp more
complex compositions with interacting functions, where output production
depends on context changes induced by different function orderings. Extending
the investigation into the visual domain, we developed a function learning
paradigm to explore the capacity of humans and neural network models in
learning and reasoning with compositional functions under varied interaction
conditions. Following brief training on individual functions, human
participants were assessed on composing two learned functions, in ways covering
four main interaction types, including instances in which the application of
the first function creates or removes the context for applying the second
function. Our findings indicate that humans can make zero-shot generalizations
on novel visual function compositions across interaction conditions,
demonstrating sensitivity to contextual changes. A comparison with a neural
network model on the same task reveals that, through the meta-learning for
compositionality (MLC) approach, a standard sequence-to-sequence Transformer
can mimic human generalization patterns in composing functions.</div><div><a href='http://arxiv.org/abs/2403.12201v1'>2403.12201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10529v2")'>Mementos: A Comprehensive Benchmark for Multimodal Large Language Model
  Reasoning over Image Sequences</div>
<div id='2401.10529v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T07:10:13Z</div><div>Authors: Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, Furong Huang</div><div style='padding-top: 10px; width: 80ex'>Multimodal Large Language Models (MLLMs) have demonstrated proficiency in
handling a variety of visual-language tasks. However, current MLLM benchmarks
are predominantly designed to evaluate reasoning based on static information
about a single image, and the ability of modern MLLMs to extrapolate from image
sequences, which is essential for understanding our ever-changing world, has
been less investigated. To address this challenge, this paper introduces
Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning
abilities. Mementos features 4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning
performance. Through a careful evaluation of nine recent MLLMs on Mementos,
including GPT-4V and Gemini, we find that they struggle to accurately describe
dynamic information about given image sequences, often leading to
hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting
MLLMs' sequential image reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the
compounding impact of behavioral hallucinations. Our dataset is available at
https://github.com/umd-huang-lab/Mementos.</div><div><a href='http://arxiv.org/abs/2401.10529v2'>2401.10529v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12026v1")'>FlexCap: Generating Rich, Localized, and Flexible Captions in Images</div>
<div id='2403.12026v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:57:02Z</div><div>Authors: Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman, Yusuf Aytar</div><div style='padding-top: 10px; width: 80ex'>We introduce a versatile $\textit{flexible-captioning}$ vision-language model
(VLM) capable of generating region-specific descriptions of varying lengths.
The model, FlexCap, is trained to produce length-conditioned captions for input
bounding boxes, and this allows control over the information density of its
output, with descriptions ranging from concise object labels to detailed
captions. To achieve this we create large-scale training datasets of image
region descriptions of varying length, starting from captioned images. This
flexible-captioning capability has several valuable applications.
  First, FlexCap demonstrates superior performance in dense captioning tasks on
the Visual Genome dataset. Second, a visual question answering (VQA) system can
be built by employing FlexCap to generate localized descriptions as inputs to a
large language model. The resulting system achieves state-of-the-art zero-shot
performance on a number of VQA datasets. We also demonstrate a
$\textit{localize-then-describe}$ approach with FlexCap can be better at
open-ended object detection than a $\textit{describe-then-localize}$ approach
with other VLMs. We highlight a novel characteristic of FlexCap, which is its
ability to extract diverse visual information through prefix conditioning.
Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks
such as image labeling, object attribute recognition, and visual dialog.
Project webpage: https://flex-cap.github.io .</div><div><a href='http://arxiv.org/abs/2403.12026v1'>2403.12026v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02996v2")'>Text-Guided Image Clustering</div>
<div id='2402.02996v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:34:21Z</div><div>Authors: Andreas Stephan, Lukas Miklautz, Kevin Sidak, Jan Philip Wahle, Bela Gipp, Claudia Plant, Benjamin Roth</div><div style='padding-top: 10px; width: 80ex'>Image clustering divides a collection of images into meaningful groups,
typically interpreted post-hoc via human-given annotations. Those are usually
in the form of text, begging the question of using text as an abstraction for
image clustering. Current image clustering methods, however, neglect the use of
generated textual descriptions. We, therefore, propose Text-Guided Image
Clustering, i.e., generating text using image captioning and visual
question-answering (VQA) models and subsequently clustering the generated text.
Further, we introduce a novel approach to inject task- or domain knowledge for
clustering by prompting VQA models. Across eight diverse image clustering
datasets, our results show that the obtained text representations often
outperform image features. Additionally, we propose a counting-based cluster
explainability method. Our evaluations show that the derived keyword-based
explanations describe clusters better than the respective cluster accuracy
suggests. Overall, this research challenges traditional approaches and paves
the way for a paradigm shift in image clustering, using generated text.</div><div><a href='http://arxiv.org/abs/2402.02996v2'>2402.02996v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09340v2")'>SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene
  Understanding</div>
<div id='2401.09340v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T17:04:35Z</div><div>Authors: Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang</div><div style='padding-top: 10px; width: 80ex'>3D vision-language grounding, which focuses on aligning language with the 3D
physical environment, stands as a cornerstone in the development of embodied
agents. In comparison to recent advancements in the 2D domain, grounding
language in 3D scenes faces several significant challenges: (i) the inherent
complexity of 3D scenes due to the diverse object configurations, their rich
attributes, and intricate relationships; (ii) the scarcity of paired 3D
vision-language data to support grounded learning; and (iii) the absence of a
unified learning framework to distill knowledge from grounded 3D data. In this
work, we aim to address these three major challenges in 3D vision-language by
examining the potential of systematically upscaling 3D vision-language learning
in indoor environments. We introduce the first million-scale 3D vision-language
dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising
2.5M vision-language pairs derived from both human annotations and our scalable
scene-graph-based generation approach. We demonstrate that this scaling allows
for a unified pre-training framework, Grounded Pre-training for Scenes (GPS),
for 3D vision-language learning. Through extensive experiments, we showcase the
effectiveness of GPS by achieving state-of-the-art performance on all existing
3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is
unveiled through zero-shot transfer experiments in the challenging 3D
vision-language tasks. Project website: https://scene-verse.github.io.</div><div><a href='http://arxiv.org/abs/2401.09340v2'>2401.09340v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15321v2")'>OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene
  Understanding</div>
<div id='2402.15321v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:39:59Z</div><div>Authors: Francis Engelmann, Ayca Takmaz, Jonas Schult, Elisabetta Fedele, Johanna Wald, Songyou Peng, Xi Wang, Or Litany, Siyu Tang, Federico Tombari, Marc Pollefeys, Leonidas Guibas, Hongbo Tian, Chunjie Wang, Xiaosheng Yan, Bingwen Wang, Xuanyang Zhang, Xiao Liu, Phuc Nguyen, Khoi Nguyen, Anh Tran, Cuong Pham, Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby</div><div style='padding-top: 10px; width: 80ex'>This report provides an overview of the challenge hosted at the OpenSUN3D
Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with
ICCV 2023. The goal of this workshop series is to provide a platform for
exploration and discussion of open-vocabulary 3D scene understanding tasks,
including but not limited to segmentation, detection and mapping. We provide an
overview of the challenge hosted at the workshop, present the challenge
dataset, the evaluation methodology, and brief descriptions of the winning
methods. For additional details, please see
https://opensun3d.github.io/index_iccv23.html.</div><div><a href='http://arxiv.org/abs/2402.15321v2'>2402.15321v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01105v1")'>A Survey for Foundation Models in Autonomous Driving</div>
<div id='2402.01105v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:44:59Z</div><div>Authors: Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, Yiqing Shen</div><div style='padding-top: 10px; width: 80ex'>The advent of foundation models has revolutionized the fields of natural
language processing and computer vision, paving the way for their application
in autonomous driving (AD). This survey presents a comprehensive review of more
than 40 research papers, demonstrating the role of foundation models in
enhancing AD. Large language models contribute to planning and simulation in
AD, particularly through their proficiency in reasoning, code generation and
translation. In parallel, vision foundation models are increasingly adapted for
critical tasks such as 3D object detection and tracking, as well as creating
realistic driving scenarios for simulation and testing. Multi-modal foundation
models, integrating diverse inputs, exhibit exceptional visual understanding
and spatial reasoning, crucial for end-to-end AD. This survey not only provides
a structured taxonomy, categorizing foundation models based on their modalities
and functionalities within the AD domain but also delves into the methods
employed in current research. It identifies the gaps between existing
foundation models and cutting-edge AD approaches, thereby charting future
research directions and proposing a roadmap for bridging these gaps.</div><div><a href='http://arxiv.org/abs/2402.01105v1'>2402.01105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02968v1")'>Delving into Multi-modal Multi-task Foundation Models for Road Scene
  Understanding: From Learning Paradigm Perspectives</div>
<div id='2402.02968v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:47:09Z</div><div>Authors: Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, Ling Shao, Yi Yang, Bojun Gao, Qun Li, Guobin Wu</div><div style='padding-top: 10px; width: 80ex'>Foundation models have indeed made a profound impact on various fields,
emerging as pivotal components that significantly shape the capabilities of
intelligent systems. In the context of intelligent vehicles, leveraging the
power of foundation models has proven to be transformative, offering notable
advancements in visual understanding. Equipped with multi-modal and multi-task
learning capabilities, multi-modal multi-task visual understanding foundation
models (MM-VUFMs) effectively process and fuse data from diverse modalities and
simultaneously handle various driving-related tasks with powerful adaptability,
contributing to a more holistic understanding of the surrounding scene. In this
survey, we present a systematic analysis of MM-VUFMs specifically designed for
road scenes. Our objective is not only to provide a comprehensive overview of
common practices, referring to task-specific models, unified multi-modal
models, unified multi-task models, and foundation model prompting techniques,
but also to highlight their advanced capabilities in diverse learning
paradigms. These paradigms include open-world understanding, efficient transfer
for road scenes, continual learning, interactive and generative capability.
Moreover, we provide insights into key challenges and future trends, such as
closed-loop driving systems, interpretability, embodied driving agents, and
world models. To facilitate researchers in staying abreast of the latest
developments in MM-VUFMs for road scenes, we have established a continuously
updated repository at https://github.com/rolsheng/MM-VUFM4DS</div><div><a href='http://arxiv.org/abs/2402.02968v1'>2402.02968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15583v1")'>Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation
  Learning of Vision-based Autonomous Driving</div>
<div id='2402.15583v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T19:43:01Z</div><div>Authors: Yichen Xie, Hongge Chen, Gregory P. Meyer, Yong Jae Lee, Eric M. Wolff, Masayoshi Tomizuka, Wei Zhan, Yuning Chai, Xin Huang</div><div style='padding-top: 10px; width: 80ex'>Due to the lack of depth cues in images, multi-frame inputs are important for
the success of vision-based perception, prediction, and planning in autonomous
driving. Observations from different angles enable the recovery of 3D object
states from 2D image inputs if we can identify the same instance in different
input frames. However, the dynamic nature of autonomous driving scenes leads to
significant changes in the appearance and shape of each instance captured by
the camera at different time steps. To this end, we propose a novel contrastive
learning algorithm, Cohere3D, to learn coherent instance representations in a
long-term input sequence robust to the change in distance and perspective. The
learned representation aids in instance-level correspondence across multiple
input frames in downstream tasks. In the pretraining stage, the raw point
clouds from LiDAR sensors are utilized to construct the long-term temporal
correspondence for each instance, which serves as guidance for the extraction
of instance-level representation from the vision-based bird's eye-view (BEV)
feature map. Cohere3D encourages a consistent representation for the same
instance at different frames but distinguishes between representations of
different instances. We evaluate our algorithm by finetuning the pretrained
model on various downstream perception, prediction, and planning tasks. Results
show a notable improvement in both data efficiency and task performance.</div><div><a href='http://arxiv.org/abs/2402.15583v1'>2402.15583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11876v1")'>Deep Bayesian Future Fusion for Self-Supervised, High-Resolution,
  Off-Road Mapping</div>
<div id='2403.11876v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:28:35Z</div><div>Authors: Shubhra Aich, Wenshan Wang, Parv Maheshwari, Matthew Sivaprakasam, Samuel Triest, Cherie Ho, Jason M. Gregory, John G. Rogers III, Sebastian Scherer</div><div style='padding-top: 10px; width: 80ex'>The limited sensing resolution of resource-constrained off-road vehicles
poses significant challenges towards reliable off-road autonomy. To overcome
this limitation, we propose a general framework based on fusing the future
information (i.e. future fusion) for self-supervision. Recent approaches
exploit this future information alongside the hand-crafted heuristics to
directly supervise the targeted downstream tasks (e.g. traversability
estimation). However, in this paper, we opt for a more general line of
development - time-efficient completion of the highest resolution (i.e. 2cm per
pixel) BEV map in a self-supervised manner via future fusion, which can be used
for any downstream tasks for better longer range prediction. To this end,
first, we create a high-resolution future-fusion dataset containing pairs of
(RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to
accommodate the noise and sparsity of the sensory information, especially in
the distal regions, we design an efficient realization of the Bayes filter onto
the vanilla convolutional network via the recurrent mechanism. Equipped with
the ideas from SOTA generative models, our Bayesian structure effectively
predicts high-quality BEV maps in the distal regions. Extensive evaluation on
both the quality of completion and downstream task on our future-fusion dataset
demonstrates the potential of our approach.</div><div><a href='http://arxiv.org/abs/2403.11876v1'>2403.11876v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17484v1")'>Pixel to Elevation: Learning to Predict Elevation Maps at Long Range
  using Images for Autonomous Offroad Navigation</div>
<div id='2401.17484v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T22:37:24Z</div><div>Authors: Chanyoung Chung, Georgios Georgakis, Patrick Spieler, Curtis Padgett, Shehryar Khattak</div><div style='padding-top: 10px; width: 80ex'>Understanding terrain topology at long-range is crucial for the success of
off-road robotic missions, especially when navigating at high-speeds. LiDAR
sensors, which are currently heavily relied upon for geometric mapping, provide
sparse measurements when mapping at greater distances. To address this
challenge, we present a novel learning-based approach capable of predicting
terrain elevation maps at long-range using only onboard egocentric images in
real-time. Our proposed method is comprised of three main elements. First, a
transformer-based encoder is introduced that learns cross-view associations
between the egocentric views and prior bird-eye-view elevation map predictions.
Second, an orientation-aware positional encoding is proposed to incorporate the
3D vehicle pose information over complex unstructured terrain with multi-view
visual image features. Lastly, a history-augmented learn-able map embedding is
proposed to achieve better temporal consistency between elevation map
predictions to facilitate the downstream navigational tasks. We experimentally
validate the applicability of our proposed approach for autonomous offroad
robotic navigation in complex and unstructured terrain using real-world offroad
driving data. Furthermore, the method is qualitatively and quantitatively
compared against the current state-of-the-art methods. Extensive field
experiments demonstrate that our method surpasses baseline models in accurately
predicting terrain elevation while effectively capturing the overall terrain
topology at long-ranges. Finally, ablation studies are conducted to highlight
and understand the effect of key components of the proposed approach and
validate their suitability to improve offroad robotic navigation capabilities.</div><div><a href='http://arxiv.org/abs/2401.17484v1'>2401.17484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07320v1")'>Towards Explainable, Safe Autonomous Driving with Language Embeddings
  for Novelty Identification and Active Learning: Framework and Experimental
  Analysis with Real-World Data Sets</div>
<div id='2402.07320v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T22:53:21Z</div><div>Authors: Ross Greer, Mohan Trivedi</div><div style='padding-top: 10px; width: 80ex'>This research explores the integration of language embeddings for active
learning in autonomous driving datasets, with a focus on novelty detection.
Novelty arises from unexpected scenarios that autonomous vehicles struggle to
navigate, necessitating higher-level reasoning abilities. Our proposed method
employs language-based representations to identify novel scenes, emphasizing
the dual purpose of safety takeover responses and active learning. The research
presents a clustering experiment using Contrastive Language-Image Pretrained
(CLIP) embeddings to organize datasets and detect novelties. We find that the
proposed algorithm effectively isolates novel scenes from a collection of
subsets derived from two real-world driving datasets, one vehicle-mounted and
one infrastructure-mounted. From the generated clusters, we further present
methods for generating textual explanations of elements which differentiate
scenes classified as novel from other scenes in the data pool, presenting
qualitative examples from the clustered results. Our results demonstrate the
effectiveness of language-driven embeddings in identifying novel elements and
generating explanations of data, and we further discuss potential applications
in safe takeovers, data curation, and multi-task active learning.</div><div><a href='http://arxiv.org/abs/2402.07320v1'>2402.07320v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02622v1")'>World Models for Autonomous Driving: An Initial Survey</div>
<div id='2403.02622v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T03:23:55Z</div><div>Authors: Yanchen Guan, Haicheng Liao, Zhenning Li, Guohui Zhang, Chengzhong Xu</div><div style='padding-top: 10px; width: 80ex'>In the rapidly evolving landscape of autonomous driving, the capability to
accurately predict future events and assess their implications is paramount for
both safety and efficiency, critically aiding the decision-making process.
World models have emerged as a transformative approach, enabling autonomous
driving systems to synthesize and interpret vast amounts of sensor data,
thereby predicting potential future scenarios and compensating for information
gaps. This paper provides an initial review of the current state and
prospective advancements of world models in autonomous driving, spanning their
theoretical underpinnings, practical applications, and the ongoing research
efforts aimed at overcoming existing limitations. Highlighting the significant
role of world models in advancing autonomous driving technologies, this survey
aspires to serve as a foundational reference for the research community,
facilitating swift access to and comprehension of this burgeoning field, and
inspiring continued innovation and exploration.</div><div><a href='http://arxiv.org/abs/2403.02622v1'>2403.02622v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09571v1")'>Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis</div>
<div id='2403.09571v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:00:29Z</div><div>Authors: Fabio Maresca, Filippo Grazioli, Antonio Albanese, Vincenzo Sciancalepore, Gianpiero Negri, Xavier Costa-Perez</div><div style='padding-top: 10px; width: 80ex'>The tremendous hype around autonomous driving is eagerly calling for emerging
and novel technologies to support advanced mobility use cases. As car
manufactures keep developing SAE level 3+ systems to improve the safety and
comfort of passengers, traffic authorities need to establish new procedures to
manage the transition from human-driven to fully-autonomous vehicles while
providing a feedback-loop mechanism to fine-tune envisioned autonomous systems.
Thus, a way to automatically profile autonomous vehicles and differentiate
those from human-driven ones is a must. In this paper, we present a
fully-fledged framework that monitors active vehicles using camera images and
state information in order to determine whether vehicles are autonomous,
without requiring any active notification from the vehicles themselves.
Essentially, it builds on the cooperation among vehicles, which share their
data acquired on the road feeding a machine learning model to identify
autonomous cars. We extensively tested our solution and created the NexusStreet
dataset, by means of the CARLA simulator, employing an autonomous driving
control agent and a steering wheel maneuvered by licensed drivers. Experiments
show it is possible to discriminate the two behaviors by analyzing video clips
with an accuracy of 80%, which improves up to 93% when the target state
information is available. Lastly, we deliberately degraded the state to observe
how the framework performs under non-ideal data collection conditions.</div><div><a href='http://arxiv.org/abs/2403.09571v1'>2403.09571v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17690v2")'>Autonomous Vehicles: Evolution of Artificial Intelligence and Learning
  Algorithms</div>
<div id='2402.17690v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:07:18Z</div><div>Authors: Divya Garikapati, Sneha Sudhir Shetiya</div><div style='padding-top: 10px; width: 80ex'>The advent of autonomous vehicles has heralded a transformative era in
transportation, reshaping the landscape of mobility through cutting-edge
technologies. Central to this evolution is the integration of Artificial
Intelligence (AI) and learning algorithms, propelling vehicles into realms of
unprecedented autonomy. This paper provides a comprehensive exploration of the
evolutionary trajectory of AI within autonomous vehicles, tracing the journey
from foundational principles to the most recent advancements. Commencing with a
current landscape overview, the paper delves into the fundamental role of AI in
shaping the autonomous decision-making capabilities of vehicles. It elucidates
the steps involved in the AI-powered development life cycle in vehicles,
addressing ethical considerations and bias in AI-driven software development
for autonomous vehicles. The study presents statistical insights into the usage
and types of AI/learning algorithms over the years, showcasing the evolving
research landscape within the automotive industry. Furthermore, the paper
highlights the pivotal role of parameters in refining algorithms for both
trucks and cars, facilitating vehicles to adapt, learn, and improve performance
over time. It concludes by outlining different levels of autonomy, elucidating
the nuanced usage of AI and learning algorithms, and automating key tasks at
each level. Additionally, the document discusses the variation in software
package sizes across different autonomy levels</div><div><a href='http://arxiv.org/abs/2402.17690v2'>2402.17690v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14641v1")'>Testing autonomous vehicles and AI: perspectives and challenges from
  cybersecurity, transparency, robustness and fairness</div>
<div id='2403.14641v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T08:29:42Z</div><div>Authors: David Fern√°ndez Llorca, Ronan Hamon, Henrik Junklewitz, Kathrin Grosse, Lars Kunze, Patrick Seiniger, Robert Swaim, Nick Reed, Alexandre Alahi, Emilia G√≥mez, Ignacio S√°nchez, Akos Kriston</div><div style='padding-top: 10px; width: 80ex'>This study explores the complexities of integrating Artificial Intelligence
(AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI
components and the impact on testing procedures, focusing on some of the
essential requirements for trustworthy AI. Topics addressed include the role of
AI at various operational layers of AVs, the implications of the EU's AI Act on
AVs, and the need for new testing methodologies for Advanced Driver Assistance
Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a
detailed analysis on the importance of cybersecurity audits, the need for
explainability in AI decision-making processes and protocols for assessing the
robustness and ethical behaviour of predictive systems in AVs. The paper
identifies significant challenges and suggests future directions for research
and development of AI in AV technology, highlighting the need for
multidisciplinary expertise.</div><div><a href='http://arxiv.org/abs/2403.14641v1'>2403.14641v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09509v1")'>On STPA for Distributed Development of Safe Autonomous Driving: An
  Interview Study</div>
<div id='2403.09509v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:56:02Z</div><div>Authors: Ali Nouri, Christian Berger, Fredrik T√∂rner</div><div style='padding-top: 10px; width: 80ex'>Safety analysis is used to identify hazards and build knowledge during the
design phase of safety-relevant functions. This is especially true for complex
AI-enabled and software intensive systems such as Autonomous Drive (AD).
System-Theoretic Process Analysis (STPA) is a novel method applied in
safety-related fields like defense and aerospace, which is also becoming
popular in the automotive industry. However, STPA assumes prerequisites that
are not fully valid in the automotive system engineering with distributed
system development and multi-abstraction design levels. This would inhibit
software developers from using STPA to analyze their software as part of a
bigger system, resulting in a lack of traceability. This can be seen as a
maintainability challenge in continuous development and deployment (DevOps). In
this paper, STPA's different guidelines for the automotive industry, e.g.
J31887/ISO21448/STPA handbook, are firstly compared to assess their
applicability to the distributed development of complex AI-enabled systems like
AD. Further, an approach to overcome the challenges of using STPA in a
multi-level design context is proposed. By conducting an interview study with
automotive industry experts for the development of AD, the challenges are
validated and the effectiveness of the proposed approach is evaluated.</div><div><a href='http://arxiv.org/abs/2403.09509v1'>2403.09509v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10559v1")'>Generative Models and Connected and Automated Vehicles: A Survey in
  Exploring the Intersection of Transportation and AI</div>
<div id='2403.10559v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T06:51:26Z</div><div>Authors: Dong Shu, Zhouyao Zhu</div><div style='padding-top: 10px; width: 80ex'>This report investigates the history and impact of Generative Models and
Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing
progress in technology and transportation. By focusing on the application of
generative models within the context of CAVs, the study aims to unravel how
this integration could enhance predictive modeling, simulation accuracy, and
decision-making processes in autonomous vehicles. This thesis discusses the
benefits and challenges of integrating generative models and CAV technology in
transportation. It aims to highlight the progress made, the remaining
obstacles, and the potential for advancements in safety and innovation.</div><div><a href='http://arxiv.org/abs/2403.10559v1'>2403.10559v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14874v1")'>WeatherProof: Leveraging Language Guidance for Semantic Segmentation in
  Adverse Weather</div>
<div id='2403.14874v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T22:46:27Z</div><div>Authors: Blake Gella, Howard Zhang, Rishi Upadhyay, Tiffany Chang, Nathan Wei, Matthew Waliman, Yunhao Bao, Celso de Melo, Alex Wong, Achuta Kadambi</div><div style='padding-top: 10px; width: 80ex'>We propose a method to infer semantic segmentation maps from images captured
under adverse weather conditions. We begin by examining existing models on
images degraded by weather conditions such as rain, fog, or snow, and found
that they exhibit a large performance drop as compared to those captured under
clear weather. To control for changes in scene structures, we propose
WeatherProof, the first semantic segmentation dataset with accurate clear and
adverse weather image pairs that share an underlying scene. Through this
dataset, we analyze the error modes in existing models and found that they were
sensitive to the highly complex combination of different weather effects
induced on the image during capture. To improve robustness, we propose a way to
use language as guidance by identifying contributions of adverse weather
conditions and injecting that as "side information". Models trained using our
language guidance exhibit performance gains by up to 10.2% in mIoU on
WeatherProof, up to 8.44% in mIoU on the widely used ACDC dataset compared to
standard training techniques, and up to 6.21% in mIoU on the ACDC dataset as
compared to previous SOTA methods.</div><div><a href='http://arxiv.org/abs/2403.14874v1'>2403.14874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.15048v1")'>Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning</div>
<div id='2403.15048v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T09:13:09Z</div><div>Authors: Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo</div><div style='padding-top: 10px; width: 80ex'>Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.</div><div><a href='http://arxiv.org/abs/2403.15048v1'>2403.15048v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15969v1")'>Routers in Vision Mixture of Experts: An Empirical Study</div>
<div id='2401.15969v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:58:07Z</div><div>Authors: Tianlin Liu, Mathieu Blondel, Carlos Riquelme, Joan Puigcerver</div><div style='padding-top: 10px; width: 80ex'>Mixture-of-Experts (MoE) models are a promising way to scale up model
capacity without significantly increasing computational cost. A key component
of MoEs is the router, which decides which subset of parameters (experts)
process which feature embeddings (tokens). In this paper, we present a
comprehensive study of routers in MoEs for computer vision tasks. We introduce
a unified MoE formulation that subsumes different MoEs with two parametric
routing tensors. This formulation covers both sparse MoE, which uses a binary
or hard assignment between experts and tokens, and soft MoE, which uses a soft
assignment between experts and weighted combinations of tokens. Routers for
sparse MoEs can be further grouped into two variants: Token Choice, which
matches experts to each token, and Expert Choice, which matches tokens to each
expert. We conduct head-to-head experiments with 6 different routers, including
existing routers from prior work and new ones we introduce. We show that (i)
many routers originally developed for language modeling can be adapted to
perform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routers
generally outperform Token Choice routers, and (iii) soft MoEs generally
outperform sparse MoEs with a fixed compute budget. These results provide new
insights regarding the crucial role of routers in vision MoE models.</div><div><a href='http://arxiv.org/abs/2401.15969v1'>2401.15969v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13254v2")'>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples</div>
<div id='2402.13254v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:59:55Z</div><div>Authors: Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee</div><div style='padding-top: 10px; width: 80ex'>We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two critical
under-explored problems: the neglect of the physically grounded reasoning
(counting and position understanding) and the potential of using highly capable
text and image generation models for semantic counterfactual fine-tuning. Our
work pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
grounded image generation model GLIGEN to generate fine-tuning data, resulting
in significant performance improvements: +33% and +37% for CLIP and LLaVA,
respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we
exploit the capabilities of high-performing text generation and image
generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.</div><div><a href='http://arxiv.org/abs/2402.13254v2'>2402.13254v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14123v1")'>DeiSAM: Segment Anything with Deictic Prompting</div>
<div id='2402.14123v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T20:43:49Z</div><div>Authors: Hikaru Shindo, Manuel Brack, Gopika Sudhakaran, Devendra Singh Dhami, Patrick Schramowski, Kristian Kersting</div><div style='padding-top: 10px; width: 80ex'>Large-scale, pre-trained neural networks have demonstrated strong
capabilities in various tasks, including zero-shot image segmentation. To
identify concrete objects in complex scenes, humans instinctively rely on
deictic descriptions in natural language, i.e., referring to something
depending on the context such as "The object that is on the desk and behind the
cup.". However, deep learning approaches cannot reliably interpret such deictic
representations due to their lack of reasoning capabilities in complex
scenarios. To remedy this issue, we propose DeiSAM -- a combination of large
pre-trained neural networks with differentiable logic reasoners -- for deictic
promptable segmentation. Given a complex, textual segmentation description,
DeiSAM leverages Large Language Models (LLMs) to generate first-order logic
rules and performs differentiable forward reasoning on generated scene graphs.
Subsequently, DeiSAM segments objects by matching them to the logically
inferred image regions. As part of our evaluation, we propose the Deictic
Visual Genome (DeiVG) dataset, containing paired visual input and complex,
deictic textual prompts. Our empirical results demonstrate that DeiSAM is a
substantial improvement over purely data-driven baselines for deictic
promptable segmentation.</div><div><a href='http://arxiv.org/abs/2402.14123v1'>2402.14123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04306v2")'>Effectiveness Assessment of Recent Large Vision-Language Models</div>
<div id='2403.04306v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T08:25:27Z</div><div>Authors: Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong, Deng-Ping Fan, Fahad Shahbaz Khan</div><div style='padding-top: 10px; width: 80ex'>The advent of large vision-language models (LVLMs) represents a noteworthy
advancement towards the pursuit of artificial general intelligence. However,
the extent of their efficacy across both specialized and general tasks warrants
further investigation. This article endeavors to evaluate the competency of
popular LVLMs in specialized and general tasks, respectively, aiming to offer a
comprehensive comprehension of these innovative methodologies. To gauge their
efficacy in specialized tasks, we tailor a comprehensive testbed comprising
three distinct scenarios: natural, healthcare, and industrial, encompassing six
challenging tasks. These tasks include salient, camouflaged, and transparent
object detection, as well as polyp and skin lesion detection, alongside
industrial anomaly detection. We examine the performance of three recent
open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of
visual recognition and localization. Moreover, we conduct empirical
investigations utilizing the aforementioned models alongside GPT-4V, assessing
their multi-modal understanding capacities in general tasks such as object
counting, absurd question answering, affordance reasoning, attribute
recognition, and spatial relation reasoning. Our investigations reveal that
these models demonstrate limited proficiency not only in specialized tasks but
also in general tasks. We delve deeper into this inadequacy and suggest several
potential factors, including limited cognition in specialized tasks, object
hallucination, text-to-image interference, and decreased robustness in complex
problems. We hope this study would provide valuable insights for the future
development of LVLMs, augmenting their power in coping with both general and
specialized applications.</div><div><a href='http://arxiv.org/abs/2403.04306v2'>2403.04306v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11622v1")'>Logical Closed Loop: Uncovering Object Hallucinations in Large
  Vision-Language Models</div>
<div id='2402.11622v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T15:28:39Z</div><div>Authors: Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan</div><div style='padding-top: 10px; width: 80ex'>Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.</div><div><a href='http://arxiv.org/abs/2402.11622v1'>2402.11622v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00776v2")'>Hybrid Quantum Vision Transformers for Event Classification in High
  Energy Physics</div>
<div id='2402.00776v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:05:37Z</div><div>Authors: Eyup B. Unlu, Mar√ßal Comajoan Cara, Gopal Ramesh Dahale, Zhongtian Dong, Roy T. Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom Magorsch, Konstantin T. Matchev, Katia Matcheva</div><div style='padding-top: 10px; width: 80ex'>Models based on vision transformer architectures are considered
state-of-the-art when it comes to image classification tasks. However, they
require extensive computational resources both for training and deployment. The
problem is exacerbated as the amount and complexity of the data increases.
Quantum-based vision transformer models could potentially alleviate this issue
by reducing the training and operating time while maintaining the same
predictive power. Although current quantum computers are not yet able to
perform high-dimensional tasks yet, they do offer one of the most efficient
solutions for the future. In this work, we construct several variations of a
quantum hybrid vision transformer for a classification problem in high energy
physics (distinguishing photons and electrons in the electromagnetic
calorimeter). We test them against classical vision transformer architectures.
Our findings indicate that the hybrid models can achieve comparable performance
to their classical analogues with a similar number of parameters.</div><div><a href='http://arxiv.org/abs/2402.00776v2'>2402.00776v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08169v2")'>Statistical Test for Attention Map in Vision Transformer</div>
<div id='2401.08169v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T07:18:47Z</div><div>Authors: Tomohiro Shiraishi, Daiki Miwa, Teruyuki Katsuoka, Vo Nguyen Le Duy, Kouichi Taji, Ichiro Takeuchi</div><div style='padding-top: 10px; width: 80ex'>The Vision Transformer (ViT) demonstrates exceptional performance in various
computer vision tasks. Attention is crucial for ViT to capture complex
wide-ranging relationships among image patches, allowing the model to weigh the
importance of image patches and aiding our understanding of the decision-making
process. However, when utilizing the attention of ViT as evidence in
high-stakes decision-making tasks such as medical diagnostics, a challenge
arises due to the potential of attention mechanisms erroneously focusing on
irrelevant regions. In this study, we propose a statistical test for ViT's
attentions, enabling us to use the attentions as reliable quantitative evidence
indicators for ViT's decision-making with a rigorously controlled error rate.
Using the framework called selective inference, we quantify the statistical
significance of attentions in the form of p-values, which enables the
theoretically grounded quantification of the false positive detection
probability of attentions. We demonstrate the validity and the effectiveness of
the proposed method through numerical experiments and applications to brain
image diagnoses.</div><div><a href='http://arxiv.org/abs/2401.08169v2'>2401.08169v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02626v2")'>Modeling Collaborator: Enabling Subjective Vision Classification With
  Minimal Human Effort via LLM Tool-Use</div>
<div id='2403.02626v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T03:34:11Z</div><div>Authors: Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou, Ranjay Krishna, Ariel Fuxman, Tom Duerig</div><div style='padding-top: 10px; width: 80ex'>From content moderation to wildlife conservation, the number of applications
that require models to recognize nuanced or subjective visual concepts is
growing. Traditionally, developing classifiers for such concepts requires
substantial manual effort measured in hours, days, or even months to identify
and annotate data needed for training. Even with recently proposed Agile
Modeling techniques, which enable rapid bootstrapping of image classifiers,
users are still required to spend 30 minutes or more of monotonous, repetitive
data labeling just to train a single classifier. Drawing on Fiske's Cognitive
Miser theory, we propose a new framework that alleviates manual effort by
replacing human labeling with natural language interactions, reducing the total
effort required to define a concept by an order of magnitude: from labeling
2,000 images to only 100 plus some natural language interactions. Our framework
leverages recent advances in foundation models, both large language models and
vision-language models, to carve out the concept space through conversation and
by automatically labeling training data points. Most importantly, our framework
eliminates the need for crowd-sourced annotations. Moreover, our framework
ultimately produces lightweight classification models that are deployable in
cost-sensitive scenarios. Across 15 subjective concepts and across 2 public
image classification datasets, our trained models outperform traditional Agile
Modeling as well as state-of-the-art zero-shot classification models like
ALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.</div><div><a href='http://arxiv.org/abs/2403.02626v2'>2403.02626v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11755v2")'>Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs</div>
<div id='2403.11755v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:03:24Z</div><div>Authors: M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski, Hilde Kuhene, Horst Possegger</div><div style='padding-top: 10px; width: 80ex'>Prompt ensembling of Large Language Model (LLM) generated category-specific
prompts has emerged as an effective method to enhance zero-shot recognition
ability of Vision-Language Models (VLMs). To obtain these category-specific
prompts, the present methods rely on hand-crafting the prompts to the LLMs for
generating VLM prompts for the downstream tasks. However, this requires
manually composing these task-specific prompts and still, they might not cover
the diverse set of visual concepts and task-specific styles associated with the
categories of interest. To effectively take humans out of the loop and
completely automate the prompt generation process for zero-shot recognition, we
propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only
minimal information about the target task, in the form of its short natural
language description, and a list of associated class labels, MPVR automatically
produces a diverse set of category-specific prompts resulting in a strong
zero-shot classifier. MPVR generalizes effectively across various popular
zero-shot image recognition benchmarks belonging to widely different domains
when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot
recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on
average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively</div><div><a href='http://arxiv.org/abs/2403.11755v2'>2403.11755v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02103v1")'>D√©j√† Vu Memorization in Vision-Language Models</div>
<div id='2402.02103v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T09:55:35Z</div><div>Authors: Bargav Jayaraman, Chuan Guo, Kamalika Chaudhuri</div><div style='padding-top: 10px; width: 80ex'>Vision-Language Models (VLMs) have emerged as the state-of-the-art
representation learning solution, with myriads of downstream applications such
as image classification, retrieval and generation. A natural question is
whether these models memorize their training data, which also has implications
for generalization. We propose a new method for measuring memorization in VLMs,
which we call d\'ej\`a vu memorization. For VLMs trained on image-caption
pairs, we show that the model indeed retains information about individual
objects in the training images beyond what can be inferred from correlations or
the image caption. We evaluate d\'ej\`a vu memorization at both sample and
population level, and show that it is significant for OpenCLIP trained on as
many as 50M image-caption pairs. Finally, we show that text randomization
considerably mitigates memorization while only moderately impacting the model's
downstream task performance.</div><div><a href='http://arxiv.org/abs/2402.02103v1'>2402.02103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09193v1")'>Are Vision Language Models Texture or Shape Biased and Can We Steer
  Them?</div>
<div id='2403.09193v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T09:07:14Z</div><div>Authors: Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Bianca Lamm, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper</div><div style='padding-top: 10px; width: 80ex'>Vision language models (VLMs) have drastically changed the computer vision
model landscape in only a few years, opening an exciting array of new
applications from zero-shot image classification, over to image captioning, and
visual question answering. Unlike pure vision models, they offer an intuitive
way to access visual content through language prompting. The wide applicability
of such models encourages us to ask whether they also align with human vision -
specifically, how far they adopt human-induced visual biases through multimodal
fusion, or whether they simply inherit biases from pure vision models. One
important visual bias is the texture vs. shape bias, or the dominance of local
over global information. In this paper, we study this bias in a wide range of
popular VLMs. Interestingly, we find that VLMs are often more shape-biased than
their vision encoders, indicating that visual biases are modulated to some
extent through text in multimodal models. If text does indeed influence visual
biases, this suggests that we may be able to steer visual biases not just
through visual input but also through language: a hypothesis that we confirm
through extensive experiments. For instance, we are able to steer shape bias
from as low as 49% to as high as 72% through prompting alone. For now, the
strong human bias towards shape (96%) remains out of reach for all tested VLMs.</div><div><a href='http://arxiv.org/abs/2403.09193v1'>2403.09193v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01345v4")'>Skip \n: A Simple Method to Reduce Hallucination in Large
  Vision-Language Models</div>
<div id='2402.01345v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:02:46Z</div><div>Authors: Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike Zheng Shou</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in large vision-language models (LVLMs) have demonstrated
impressive capability in visual information understanding with human language.
Despite these advances, LVLMs still face challenges with multimodal
hallucination, such as generating text descriptions of objects that are not
present in the visual information. However, the underlying fundamental reasons
of multimodal hallucinations remain poorly explored. In this paper, we propose
a new perspective, suggesting that the inherent biases in LVLMs might be a key
factor in hallucinations. Specifically, we systematically identify a semantic
shift bias related to paragraph breaks (\n\n), where the content before and
after '\n\n' in the training data frequently exhibit significant semantic
changes. This pattern leads the model to infer that the contents following
'\n\n' should be obviously different from the preceding contents with less
hallucinatory descriptions, thereby increasing the probability of hallucinatory
descriptions subsequent to the '\n\n'. We have validated this hypothesis on
multiple publicly available LVLMs. Besides, we find that deliberately inserting
'\n\n' at the generated description can induce more hallucinations. A simple
method is proposed to effectively mitigate the hallucination of LVLMs by
skipping the output of '\n'.</div><div><a href='http://arxiv.org/abs/2402.01345v4'>2402.01345v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00253v1")'>A Survey on Hallucination in Large Vision-Language Models</div>
<div id='2402.00253v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T00:33:21Z</div><div>Authors: Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, Wei Peng</div><div style='padding-top: 10px; width: 80ex'>Recent development of Large Vision-Language Models (LVLMs) has attracted
growing attention within the AI landscape for its practical implementation
potential. However, ``hallucination'', or more specifically, the misalignment
between factual visual content and corresponding textual generation, poses a
significant challenge of utilizing LVLMs. In this comprehensive survey, we
dissect LVLM-related hallucinations in an attempt to establish an overview and
facilitate future mitigation. Our scrutiny starts with a clarification of the
concept of hallucinations in LVLMs, presenting a variety of hallucination
symptoms and highlighting the unique challenges inherent in LVLM
hallucinations. Subsequently, we outline the benchmarks and methodologies
tailored specifically for evaluating hallucinations unique to LVLMs.
Additionally, we delve into an investigation of the root causes of these
hallucinations, encompassing insights from the training data and model
components. We also critically review existing methods for mitigating
hallucinations. The open questions and future directions pertaining to
hallucinations within LVLMs are discussed to conclude this survey.</div><div><a href='http://arxiv.org/abs/2402.00253v1'>2402.00253v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15300v1")'>Seeing is Believing: Mitigating Hallucination in Large Vision-Language
  Models via CLIP-Guided Decoding</div>
<div id='2402.15300v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:57:16Z</div><div>Authors: Ailin Deng, Zhirui Chen, Bryan Hooi</div><div style='padding-top: 10px; width: 80ex'>Large Vision-Language Models (LVLMs) are susceptible to object
hallucinations, an issue in which their generated text contains non-existent
objects, greatly limiting their reliability and practicality. Current
approaches often rely on the model's token likelihoods or other internal
information, instruction tuning on additional datasets, or incorporating
complex external tools. We first perform empirical analysis on sentence-level
LVLM hallucination, finding that CLIP similarity to the image acts as a
stronger and more robust indicator of hallucination compared to token
likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)
approach, a straightforward but effective training-free approach to reduce
object hallucination at decoding time. CGD uses CLIP to guide the model's
decoding process by enhancing visual grounding of generated text with the
image. Experiments demonstrate that CGD effectively mitigates object
hallucination across multiple LVLM families while preserving the utility of
text generation.</div><div><a href='http://arxiv.org/abs/2402.15300v1'>2402.15300v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00425v1")'>HALC: Object Hallucination Reduction via Adaptive Focal-Contrast
  Decoding</div>
<div id='2403.00425v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T10:21:52Z</div><div>Authors: Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, Jiawei Zhou</div><div style='padding-top: 10px; width: 80ex'>While large vision-language models (LVLMs) have demonstrated impressive
capabilities in interpreting multi-modal contexts, they invariably suffer from
object hallucinations (OH). We introduce HALC, a novel decoding algorithm
designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal
visual information in vision-language tasks and operates on both local and
global contexts simultaneously. Specifically, HALC integrates a robust
auto-focal grounding mechanism (locally) to correct hallucinated tokens on the
fly, and a specialized beam search algorithm (globally) to significantly reduce
OH while preserving text generation quality. Additionally, HALC can be
integrated into any LVLMs as a plug-and-play module without extra training.
Extensive experimental studies demonstrate the effectiveness of HALC in
reducing OH, outperforming state-of-the-arts across four benchmarks.</div><div><a href='http://arxiv.org/abs/2403.00425v1'>2403.00425v1</a></div>
</div></div>
    <div><a href="arxiv_14.html">Prev (14)</a></div>
    <div><a href="arxiv_16.html">Next (16)</a></div>
    