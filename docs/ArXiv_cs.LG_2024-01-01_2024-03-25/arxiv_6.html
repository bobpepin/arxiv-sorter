
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13144v1")'>Neural Network Diffusion</div>
<div id='2402.13144v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T16:59:03Z</div><div>Authors: Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have achieved remarkable success in image and video
generation. In this work, we demonstrate that diffusion models can also
\textit{generate high-performing neural network parameters}. Our approach is
simple, utilizing an autoencoder and a standard latent diffusion model. The
autoencoder extracts latent representations of a subset of the trained network
parameters. A diffusion model is then trained to synthesize these latent
parameter representations from random noise. It then generates new
representations that are passed through the autoencoder's decoder, whose
outputs are ready to use as new subsets of network parameters. Across various
architectures and datasets, our diffusion process consistently generates models
of comparable or improved performance over trained networks, with minimal
additional cost. Notably, we empirically find that the generated models perform
differently with the trained networks. Our results encourage more exploration
on the versatile use of diffusion models.</div><div><a href='http://arxiv.org/abs/2402.13144v1'>2402.13144v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14148v1")'>Efficient Video Diffusion Models via Content-Frame Motion-Latent
  Decomposition</div>
<div id='2403.14148v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T05:48:48Z</div><div>Authors: Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar</div><div style='padding-top: 10px; width: 80ex'>Video diffusion models have recently made great progress in generation
quality, but are still limited by the high memory and computational
requirements. This is because current video diffusion models often attempt to
process high-dimensional videos directly. To tackle this issue, we propose
content-motion latent diffusion model (CMD), a novel efficient extension of
pretrained image diffusion models for video generation. Specifically, we
propose an autoencoder that succinctly encodes a video as a combination of a
content frame (like an image) and a low-dimensional motion latent
representation. The former represents the common content, and the latter
represents the underlying motion in the video, respectively. We generate the
content frame by fine-tuning a pretrained image diffusion model, and we
generate the motion latent representation by training a new lightweight
diffusion model. A key innovation here is the design of a compact latent space
that can directly utilizes a pretrained image diffusion model, which has not
been done in previous latent video diffusion models. This leads to considerably
better quality generation and reduced computational costs. For instance, CMD
can sample a video 7.7$\times$ faster than prior approaches by generating a
video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD
achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous
state-of-the-art of 292.4.</div><div><a href='http://arxiv.org/abs/2403.14148v1'>2403.14148v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14773v1")'>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation
  from Text</div>
<div id='2403.14773v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T18:27:29Z</div><div>Authors: Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi</div><div style='padding-top: 10px; width: 80ex'>Text-to-video diffusion models enable the generation of high-quality videos
that follow text instructions, making it easy to create diverse and individual
content. However, existing approaches mostly focus on high-quality short video
generation (typically 16 or 24 frames), ending up with hard-cuts when naively
extended to the case of long video synthesis. To overcome these limitations, we
introduce StreamingT2V, an autoregressive approach for long video generation of
80, 240, 600, 1200 or more frames with smooth transitions. The key components
are:(i) a short-term memory block called conditional attention module (CAM),
which conditions the current generation on the features extracted from the
previous chunk via an attentional mechanism, leading to consistent chunk
transitions, (ii) a long-term memory block called appearance preservation
module, which extracts high-level scene and object features from the first
video chunk to prevent the model from forgetting the initial scene, and (iii) a
randomized blending approach that enables to apply a video enhancer
autoregressively for infinitely long videos without inconsistencies between
chunks. Experiments show that StreamingT2V generates high motion amount. In
contrast, all competing image-to-video methods are prone to video stagnation
when applied naively in an autoregressive manner. Thus, we propose with
StreamingT2V a high-quality seamless text-to-long video generator that
outperforms competitors with consistency and motion. Our code will be available
at: https://github.com/Picsart-AI-Research/StreamingT2V</div><div><a href='http://arxiv.org/abs/2403.14773v1'>2403.14773v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13501v1")'>VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis</div>
<div id='2403.13501v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T10:58:58Z</div><div>Authors: Yumeng Li, William Beluch, Margret Keuper, Dan Zhang, Anna Khoreva</div><div style='padding-top: 10px; width: 80ex'>Despite tremendous progress in the field of text-to-video (T2V) synthesis,
open-sourced T2V diffusion models struggle to generate longer videos with
dynamically varying and evolving content. They tend to synthesize quasi-static
videos, ignoring the necessary visual change-over-time implied in the text
prompt. At the same time, scaling these models to enable longer, more dynamic
video synthesis often remains computationally intractable. To address this
challenge, we introduce the concept of Generative Temporal Nursing (GTN), where
we aim to alter the generative process on the fly during inference to improve
control over the temporal dynamics and enable generation of longer videos. We
propose a method for GTN, dubbed VSTAR, which consists of two key ingredients:
1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis
based on the original single prompt leveraging LLMs, which gives accurate
textual guidance to different visual states of longer videos, and 2) Temporal
Attention Regularization (TAR) - a regularization technique to refine the
temporal attention units of the pre-trained T2V diffusion models, which enables
control over the video dynamics. We experimentally showcase the superiority of
the proposed approach in generating longer, visually appealing videos over
existing open-sourced T2V models. We additionally analyze the temporal
attention maps realized with and without VSTAR, demonstrating the importance of
applying our method to mitigate neglect of the desired visual change over time.</div><div><a href='http://arxiv.org/abs/2403.13501v1'>2403.13501v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07938v1")'>Text-to-Audio Generation Synchronized with Videos</div>
<div id='2403.07938v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T22:27:38Z</div><div>Authors: Shentong Mo, Jing Shi, Yapeng Tian</div><div style='padding-top: 10px; width: 80ex'>In recent times, the focus on text-to-audio (TTA) generation has intensified,
as researchers strive to synthesize audio from textual descriptions. However,
most existing methods, though leveraging latent diffusion models to learn the
correlation between audio and text embeddings, fall short when it comes to
maintaining a seamless synchronization between the produced audio and its
video. This often results in discernible audio-visual mismatches. To bridge
this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation
that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself
with three novel metrics dedicated to evaluating visual alignment and temporal
consistency. To complement this, we also present a simple yet effective
video-aligned TTA generation model, namely T2AV. Moving beyond traditional
methods, T2AV refines the latent diffusion approach by integrating
visual-aligned text embeddings as its conditional foundation. It employs a
temporal multi-head attention transformer to extract and understand temporal
nuances from video data, a feat amplified by our Audio-Visual ControlNet that
adeptly merges temporal visual representations with text embeddings. Further
enhancing this integration, we weave in a contrastive learning objective,
designed to ensure that the visual-aligned text embeddings resonate closely
with the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench
demonstrate that our T2AV sets a new standard for video-aligned TTA generation
in ensuring visual alignment and temporal consistency.</div><div><a href='http://arxiv.org/abs/2403.07938v1'>2403.07938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14843v1")'>Text Diffusion with Reinforced Conditioning</div>
<div id='2402.14843v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:24:02Z</div><div>Authors: Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have demonstrated exceptional capability in generating
high-quality images, videos, and audio. Due to their adaptiveness in iterative
refinement, they provide a strong potential for achieving better
non-autoregressive sequence generation. However, existing text diffusion models
still fall short in their performance due to a challenge in handling the
discreteness of language. This paper thoroughly analyzes text diffusion models
and uncovers two significant limitations: degradation of self-conditioning
during training and misalignment between training and sampling. Motivated by
our findings, we propose a novel Text Diffusion model called TREC, which
mitigates the degradation with Reinforced Conditioning and the misalignment by
Time-Aware Variance Scaling. Our extensive experiments demonstrate the
competitiveness of TREC against autoregressive, non-autoregressive, and
diffusion baselines. Moreover, qualitative analysis shows its advanced ability
to fully utilize the diffusion process in refining samples.</div><div><a href='http://arxiv.org/abs/2402.14843v1'>2402.14843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00769v1")'>AnimateLCM: Accelerating the Animation of Personalized Diffusion Models
  and Adapters with Decoupled Consistency Learning</div>
<div id='2402.00769v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:58:11Z</div><div>Authors: Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li</div><div style='padding-top: 10px; width: 80ex'>Video diffusion models has been gaining increasing attention for its ability
to produce videos that are both coherent and of high fidelity. However, the
iterative denoising process makes it computationally intensive and
time-consuming, thus limiting its applications. Inspired by the Consistency
Model (CM) that distills pretrained image diffusion models to accelerate the
sampling with minimal steps and its successful extension Latent Consistency
Model (LCM) on conditional image generation, we propose AnimateLCM, allowing
for high-fidelity video generation within minimal steps. Instead of directly
conducting consistency learning on the raw video dataset, we propose a
decoupled consistency learning strategy that decouples the distillation of
image generation priors and motion generation priors, which improves the
training efficiency and enhance the generation visual quality. Additionally, to
enable the combination of plug-and-play adapters in stable diffusion community
to achieve various functions (e.g., ControlNet for controllable generation). we
propose an efficient strategy to adapt existing adapters to our distilled
text-conditioned video consistency model or train adapters from scratch without
harming the sampling speed. We validate the proposed strategy in
image-conditioned video generation and layout-conditioned video generation, all
achieving top-performing results. Experimental results validate the
effectiveness of our proposed method. Code and weights will be made public.
More details are available at https://github.com/G-U-N/AnimateLCM.</div><div><a href='http://arxiv.org/abs/2402.00769v1'>2402.00769v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16627v2")'>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual
  Generation and Editing</div>
<div id='2402.16627v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T15:01:16Z</div><div>Authors: Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui</div><div style='padding-top: 10px; width: 80ex'>Conditional diffusion models have exhibited superior performance in
high-fidelity text-guided visual generation and editing. Nevertheless,
prevailing text-guided visual diffusion models primarily focus on incorporating
text-visual relationships exclusively into the reverse process, often
disregarding their relevance in the forward process. This inconsistency between
forward and reverse processes may limit the precise conveyance of textual
semantics in visual synthesis results. To address this issue, we propose a
novel and general contextualized diffusion model (ContextDiff) by incorporating
the cross-modal context encompassing interactions and alignments between text
condition and visual sample into forward and reverse processes. We propagate
this context to all timesteps in the two processes to adapt their trajectories,
thereby facilitating cross-modal conditional modeling. We generalize our
contextualized diffusion to both DDPMs and DDIMs with theoretical derivations,
and demonstrate the effectiveness of our model in evaluations with two
challenging tasks: text-to-image generation, and text-to-video editing. In each
task, our ContextDiff achieves new state-of-the-art performance, significantly
enhancing the semantic alignment between text condition and generated samples,
as evidenced by quantitative and qualitative evaluations. Our code is available
at https://github.com/YangLing0818/ContextDiff</div><div><a href='http://arxiv.org/abs/2402.16627v2'>2402.16627v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03040v1")'>InteractiveVideo: User-Centric Controllable Video Generation with
  Synergistic Multimodal Instructions</div>
<div id='2402.03040v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:24:46Z</div><div>Authors: Yiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao, Xiangyu Yue</div><div style='padding-top: 10px; width: 80ex'>We introduce $\textit{InteractiveVideo}$, a user-centric framework for video
generation. Different from traditional generative approaches that operate based
on user-provided images or text, our framework is designed for dynamic
interaction, allowing users to instruct the generative model through various
intuitive mechanisms during the whole generation process, e.g. text and image
prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal
Instruction mechanism, designed to seamlessly integrate users' multimodal
instructions into generative models, thus facilitating a cooperative and
responsive interaction between user inputs and the generative process. This
approach enables iterative and fine-grained refinement of the generation result
through precise and effective user instructions. With
$\textit{InteractiveVideo}$, users are given the flexibility to meticulously
tailor key aspects of a video. They can paint the reference image, edit
semantics, and adjust video motions until their requirements are fully met.
Code, models, and demo are available at
https://github.com/invictus717/InteractiveVideo</div><div><a href='http://arxiv.org/abs/2402.03040v1'>2402.03040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08049v1")'>TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial
  Creation on Physical Tasks</div>
<div id='2403.08049v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T19:46:59Z</div><div>Authors: Yuexi Chen, Vlad I. Morariu, Anh Truong, Zhicheng Liu</div><div style='padding-top: 10px; width: 80ex'>Mixed-media tutorials, which integrate videos, images, text, and diagrams to
teach procedural skills, offer more browsable alternatives than timeline-based
videos. However, manually creating such tutorials is tedious, and existing
automated solutions are often restricted to a particular domain. While AI
models hold promise, it is unclear how to effectively harness their powers,
given the multi-modal data involved and the vast landscape of models. We
present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial
creation on physical tasks. First, we distill common tutorial components by
surveying existing work; then, we present an approach to identify, assemble,
and evaluate AI models for component extraction; finally, we propose guidelines
for designing user interfaces (UI) that support tutorial creation based on
AI-generated components. We show that TutoAI has achieved higher or similar
quality compared to a baseline model in preliminary user studies.</div><div><a href='http://arxiv.org/abs/2403.08049v1'>2403.08049v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02847v2")'>Generating Non-Stationary Textures using Self-Rectification</div>
<div id='2401.02847v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T15:07:05Z</div><div>Authors: Yang Zhou, Rongjun Xiao, Dani Lischinski, Daniel Cohen-Or, Hui Huang</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the challenge of example-based non-stationary texture
synthesis. We introduce a novel twostep approach wherein users first modify a
reference texture using standard image editing tools, yielding an initial rough
target for the synthesis. Subsequently, our proposed method, termed
"self-rectification", automatically refines this target into a coherent,
seamless texture, while faithfully preserving the distinct visual
characteristics of the reference exemplar. Our method leverages a pre-trained
diffusion network, and uses self-attention mechanisms, to gradually align the
synthesized texture with the reference, ensuring the retention of the
structures in the provided target. Through experimental validation, our
approach exhibits exceptional proficiency in handling non-stationary textures,
demonstrating significant advancements in texture synthesis when compared to
existing state-of-the-art techniques. Code is available at
https://github.com/xiaorongjun000/Self-Rectification</div><div><a href='http://arxiv.org/abs/2401.02847v2'>2401.02847v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04940v1")'>A spatiotemporal style transfer algorithm for dynamic visual stimulus
  generation</div>
<div id='2403.04940v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T23:07:46Z</div><div>Authors: Antonino Greco, Markus Siegel</div><div style='padding-top: 10px; width: 80ex'>Understanding how visual information is encoded in biological and artificial
systems often requires vision scientists to generate appropriate stimuli to
test specific hypotheses. Although deep neural network models have
revolutionized the field of image generation with methods such as image style
transfer, available methods for video generation are scarce. Here, we introduce
the Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus
generation framework that allows powerful manipulation and synthesis of video
stimuli for vision research. It is based on a two-stream deep neural network
model that factorizes spatial and temporal features to generate dynamic visual
stimuli whose model layer activations are matched to those of input videos. As
an example, we show that our algorithm enables the generation of model
metamers, dynamic stimuli whose layer activations within our two-stream model
are matched to those of natural videos. We show that these generated stimuli
match the low-level spatiotemporal features of their natural counterparts but
lack their high-level semantic features, making it a powerful paradigm to study
object recognition. Late layer activations in deep vision models exhibited a
lower similarity between natural and metameric stimuli compared to early
layers, confirming the lack of high-level information in the generated stimuli.
Finally, we use our generated stimuli to probe the representational
capabilities of predictive coding deep networks. These results showcase
potential applications of our algorithm as a versatile tool for dynamic
stimulus generation in vision science.</div><div><a href='http://arxiv.org/abs/2403.04940v1'>2403.04940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12961v1")'>TexTile: A Differentiable Metric for Texture Tileability</div>
<div id='2403.12961v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:59:09Z</div><div>Authors: Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, Jorge Lopez-Moreno</div><div style='padding-top: 10px; width: 80ex'>We introduce TexTile, a novel differentiable metric to quantify the degree
upon which a texture image can be concatenated with itself without introducing
repeating artifacts (i.e., the tileability). Existing methods for tileable
texture synthesis focus on general texture quality, but lack explicit analysis
of the intrinsic repeatability properties of a texture. In contrast, our
TexTile metric effectively evaluates the tileable properties of a texture,
opening the door to more informed synthesis and analysis of tileable textures.
Under the hood, TexTile is formulated as a binary classifier carefully built
from a large dataset of textures of different styles, semantics, regularities,
and human annotations.Key to our method is a set of architectural modifications
to baseline pre-train image classifiers to overcome their shortcomings at
measuring tileability, along with a custom data augmentation and training
regime aimed at increasing robustness and accuracy. We demonstrate that TexTile
can be plugged into different state-of-the-art texture synthesis methods,
including diffusion-based strategies, and generate tileable textures while
keeping or even improving the overall texture quality. Furthermore, we show
that TexTile can objectively evaluate any tileable texture synthesis method,
whereas the current mix of existing metrics produces uncorrelated scores which
heavily hinders progress in the field.</div><div><a href='http://arxiv.org/abs/2403.12961v1'>2403.12961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.09414v1")'>Vlogger: Make Your Dream A Vlog</div>
<div id='2401.09414v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T18:55:12Z</div><div>Authors: Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, Yali Wang</div><div style='padding-top: 10px; width: 80ex'>In this work, we present Vlogger, a generic AI system for generating a
minute-level video blog (i.e., vlog) of user descriptions. Different from short
videos with a few seconds, vlog often contains a complex storyline with
diversified scenes, which is challenging for most existing video generation
approaches. To break through this bottleneck, our Vlogger smartly leverages
Large Language Model (LLM) as Director and decomposes a long video generation
task of vlog into four key stages, where we invoke various foundation models to
play the critical roles of vlog professionals, including (1) Script, (2) Actor,
(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,
our Vlogger can generate vlogs through explainable cooperation of top-down
planning and bottom-up shooting. Moreover, we introduce a novel video diffusion
model, ShowMaker, which serves as a videographer in our Vlogger for generating
the video snippet of each shooting scene. By incorporating Script and Actor
attentively as textual and visual prompts, it can effectively enhance
spatial-temporal coherence in the snippet. Besides, we design a concise mixed
training paradigm for ShowMaker, boosting its capacity for both T2V generation
and prediction. Finally, the extensive experiments show that our method
achieves state-of-the-art performance on zero-shot T2V generation and
prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs
from open-world descriptions, without loss of video coherence on script and
actor. The code and model is all available at
https://github.com/zhuangshaobin/Vlogger.</div><div><a href='http://arxiv.org/abs/2401.09414v1'>2401.09414v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06035v1")'>RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane
  Networks</div>
<div id='2401.06035v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T16:48:44Z</div><div>Authors: Partha Ghosh, Soubhik Sanyal, Cordelia Schmid, Bernhard Schölkopf</div><div style='padding-top: 10px; width: 80ex'>We present a novel unconditional video generative model designed to address
long-term spatial and temporal dependencies. To capture these dependencies, our
approach incorporates a hybrid explicit-implicit tri-plane representation
inspired by 3D-aware generative frameworks developed for three-dimensional
object representation and employs a singular latent code to model an entire
video sequence. Individual video frames are then synthesized from an
intermediate tri-plane representation, which itself is derived from the primary
latent code. This novel strategy reduces computational complexity by a factor
of $2$ as measured in FLOPs. Consequently, our approach facilitates the
efficient and temporally coherent generation of videos. Moreover, our joint
frame modeling approach, in contrast to autoregressive methods, mitigates the
generation of visual artifacts. We further enhance the model's capabilities by
integrating an optical flow-based module within our Generative Adversarial
Network (GAN) based generator architecture, thereby compensating for the
constraints imposed by a smaller generator size. As a result, our model is
capable of synthesizing high-fidelity video clips at a resolution of
$256\times256$ pixels, with durations extending to more than $5$ seconds at a
frame rate of 30 fps. The efficacy and versatility of our approach are
empirically validated through qualitative and quantitative assessments across
three different datasets comprising both synthetic and real video clips.</div><div><a href='http://arxiv.org/abs/2401.06035v1'>2401.06035v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13126v1")'>VGMShield: Mitigating Misuse of Video Generative Models</div>
<div id='2402.13126v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T16:39:23Z</div><div>Authors: Yan Pang, Yang Zhang, Tianhao Wang</div><div style='padding-top: 10px; width: 80ex'>With the rapid advancement in video generation, people can conveniently
utilize video generation models to create videos tailored to their specific
desires. Nevertheless, there are also growing concerns about their potential
misuse in creating and disseminating false information.
  In this work, we introduce VGMShield: a set of three straightforward but
pioneering mitigations through the lifecycle of fake video generation. We start
from \textit{fake video detection} trying to understand whether there is
uniqueness in generated videos and whether we can differentiate them from real
videos; then, we investigate the \textit{tracing} problem, which maps a fake
video back to a model that generates it. Towards these, we propose to leverage
pre-trained models that focus on {\it spatial-temporal dynamics} as the
backbone to identify inconsistencies in videos. Through experiments on seven
state-of-the-art open-source models, we demonstrate that current models still
cannot perfectly handle spatial-temporal relationships, and thus, we can
accomplish detection and tracing with nearly perfect accuracy.
  Furthermore, anticipating future generative model improvements, we propose a
{\it prevention} method that adds invisible perturbations to images to make the
generated videos look unreal. Together with fake video detection and tracing,
our multi-faceted set of solutions can effectively mitigate misuse of video
generative models.</div><div><a href='http://arxiv.org/abs/2402.13126v1'>2402.13126v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15249v1")'>Spectral Motion Alignment for Video Motion Transfer using Diffusion
  Models</div>
<div id='2403.15249v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:47:18Z</div><div>Authors: Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye</div><div style='padding-top: 10px; width: 80ex'>The evolution of diffusion models has greatly impacted video generation and
understanding. Particularly, text-to-video diffusion models (VDMs) have
significantly facilitated the customization of input video with target
appearance, motion, etc. Despite these advances, challenges persist in
accurately distilling motion information from video frames. While existing
works leverage the consecutive frame residual as the target motion vector, they
inherently lack global motion context and are vulnerable to frame-wise
distortions. To address this, we present Spectral Motion Alignment (SMA), a
novel framework that refines and aligns motion vectors using Fourier and
wavelet transforms. SMA learns motion patterns by incorporating
frequency-domain regularization, facilitating the learning of whole-frame
global motion dynamics, and mitigating spatial artifacts. Extensive experiments
demonstrate SMA's efficacy in improving motion transfer while maintaining
computational efficiency and compatibility across various video customization
frameworks.</div><div><a href='http://arxiv.org/abs/2403.15249v1'>2403.15249v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03019v3")'>Taylor Videos for Action Recognition</div>
<div id='2402.03019v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:00:13Z</div><div>Authors: Lei Wang, Xiuyuan Yuan, Tom Gedeon, Liang Zheng</div><div style='padding-top: 10px; width: 80ex'>Effectively extracting motions from video is a critical and long-standing
problem for action recognition. This problem is very challenging because
motions (i) do not have an explicit form, (ii) have various concepts such as
displacement, velocity, and acceleration, and (iii) often contain noise caused
by unstable pixels. Addressing these challenges, we propose the Taylor video, a
new video format that highlights the dominate motions (e.g., a waving hand) in
each of its frames named the Taylor frame. Taylor video is named after Taylor
series, which approximates a function at a given point using important terms.
In the scenario of videos, we define an implicit motion-extraction function
which aims to extract motions from video temporal block. In this block, using
the frames, the difference frames, and higher-order difference frames, we
perform Taylor expansion to approximate this function at the starting frame. We
show the summation of the higher-order terms in the Taylor series gives us
dominant motion patterns, where static objects, small and unstable motions are
removed. Experimentally we show that Taylor videos are effective inputs to
popular architectures including 2D CNNs, 3D CNNs, and transformers. When used
individually, Taylor videos yield competitive action recognition accuracy
compared to RGB videos and optical flow. When fused with RGB or optical flow
videos, further accuracy improvement is achieved.</div><div><a href='http://arxiv.org/abs/2402.03019v3'>2402.03019v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13798v1")'>Hierarchical NeuroSymbolic Approach for Action Quality Assessment</div>
<div id='2403.13798v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:55:21Z</div><div>Authors: Lauren Okamoto, Paritosh Parmar</div><div style='padding-top: 10px; width: 80ex'>Action quality assessment (AQA) applies computer vision to quantitatively
assess the performance or execution of a human action. Current AQA approaches
are end-to-end neural models, which lack transparency and tend to be biased
because they are trained on subjective human judgements as ground-truth. To
address these issues, we introduce a neuro-symbolic paradigm for AQA, which
uses neural networks to abstract interpretable symbols from video data and
makes quality assessments by applying rules to those symbols. We take diving as
the case study. We found that domain experts prefer our system and find it more
informative than purely neural approaches to AQA in diving. Our system also
achieves state-of-the-art action recognition and temporal segmentation, and
automatically generates a detailed report that breaks the dive down into its
elements and provides objective scoring with visual evidence. As verified by a
group of domain experts, this report may be used to assist judges in scoring,
help train judges, and provide feedback to divers. We will open-source all of
our annotated training data and code for ease of reproducibility.</div><div><a href='http://arxiv.org/abs/2403.13798v1'>2403.13798v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07891v1")'>Digital Video Manipulation Detection Technique Based on Compression
  Algorithms</div>
<div id='2403.07891v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T16:05:27Z</div><div>Authors: Edgar Gonzalez Fernandez, Ana Lucila Sandoval Orozco, Luis Javier Garcia Villalba</div><div style='padding-top: 10px; width: 80ex'>Digital images and videos play a very important role in everyday life.
Nowadays, people have access the affordable mobile devices equipped with
advanced integrated cameras and powerful image processing applications.
Technological development facilitates not only the generation of multimedia
content, but also the intentional modification of it, either with recreational
or malicious purposes. This is where forensic techniques to detect manipulation
of images and videos become essential. This paper proposes a forensic technique
by analysing compression algorithms used by the H.264 coding. The presence of
recompression uses information of macroblocks, a characteristic of the
H.264-MPEG4 standard, and motion vectors. A Vector Support Machine is used to
create the model that allows to accurately detect if a video has been
recompressed.</div><div><a href='http://arxiv.org/abs/2403.07891v1'>2403.07891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12244v1")'>Large-scale Reinforcement Learning for Diffusion Models</div>
<div id='2401.12244v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T08:10:43Z</div><div>Authors: Yinan Zhang, Eric Tzeng, Yilun Du, Dmitry Kislyuk</div><div style='padding-top: 10px; width: 80ex'>Text-to-image diffusion models are a class of deep generative models that
have demonstrated an impressive capacity for high-quality image generation.
However, these models are susceptible to implicit biases that arise from
web-scale text-image training pairs and may inaccurately model aspects of
images we care about. This can result in suboptimal samples, model bias, and
images that do not align with human ethics and preferences. In this paper, we
present an effective scalable algorithm to improve diffusion models using
Reinforcement Learning (RL) across a diverse set of reward functions, such as
human preference, compositionality, and fairness over millions of images. We
illustrate how our approach substantially outperforms existing methods for
aligning diffusion models with human preferences. We further illustrate how
this substantially improves pretrained Stable Diffusion (SD) models, generating
samples that are preferred by humans 80.3% of the time over those from the base
SD model while simultaneously improving both the composition and diversity of
generated samples.</div><div><a href='http://arxiv.org/abs/2401.12244v1'>2401.12244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02182v1")'>Diffusion Cross-domain Recommendation</div>
<div id='2402.02182v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T15:14:51Z</div><div>Authors: Yuner Xuan</div><div style='padding-top: 10px; width: 80ex'>It is always a challenge for recommender systems to give high-quality
outcomes to cold-start users. One potential solution to alleviate the data
sparsity problem for cold-start users in the target domain is to add data from
the auxiliary domain. Finding a proper way to extract knowledge from an
auxiliary domain and transfer it into a target domain is one of the main
objectives for cross-domain recommendation (CDR) research. Among the existing
methods, mapping approach is a popular one to implement cross-domain
recommendation models (CDRs). For models of this type, a mapping module plays
the role of transforming data from one domain to another. It primarily
determines the performance of mapping approach CDRs. Recently, diffusion
probability models (DPMs) have achieved impressive success for image synthesis
related tasks. They involve recovering images from noise-added samples, which
can be viewed as a data transformation process with outstanding performance. To
further enhance the performance of CDRs, we first reveal the potential
connection between DPMs and mapping modules of CDRs, and then propose a novel
CDR model named Diffusion Cross-domain Recommendation (DiffCDR). More
specifically, we first adopt the theory of DPM and design a Diffusion Module
(DIM), which generates user's embedding in target domain. To reduce the
negative impact of randomness introduced in DIM and improve the stability, we
employ an Alignment Module to produce the aligned user embeddings. In addition,
we consider the label data of the target domain and form the task-oriented loss
function, which enables our DiffCDR to adapt to specific tasks. By conducting
extensive experiments on datasets collected from reality, we demonstrate the
effectiveness and adaptability of DiffCDR to outperform baseline models on
various CDR tasks in both cold-start and warm-start scenarios.</div><div><a href='http://arxiv.org/abs/2402.02182v1'>2402.02182v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01965v2")'>Analyzing Neural Network-Based Generative Diffusion Models through
  Convex Optimization</div>
<div id='2402.01965v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T00:20:25Z</div><div>Authors: Fangzhao Zhang, Mert Pilanci</div><div style='padding-top: 10px; width: 80ex'>Diffusion models are becoming widely used in state-of-the-art image, video
and audio generation. Score-based diffusion models stand out among these
methods, necessitating the estimation of score function of the input data
distribution. In this study, we present a theoretical framework to analyze
two-layer neural network-based diffusion models by reframing score matching and
denoising score matching as convex optimization. Though existing diffusion
theory is mainly asymptotic, we characterize the exact predicted score function
and establish the convergence result for neural network-based diffusion models
with finite data. This work contributes to understanding what neural
network-based diffusion model learns in non-asymptotic settings.</div><div><a href='http://arxiv.org/abs/2402.01965v2'>2402.01965v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01951v1")'>Can We Generate Realistic Hands Only Using Convolution?</div>
<div id='2401.01951v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T19:27:20Z</div><div>Authors: Mehran Hosseini, Peyman Hosseini</div><div style='padding-top: 10px; width: 80ex'>The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).</div><div><a href='http://arxiv.org/abs/2401.01951v1'>2401.01951v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16369v1")'>Generative AI in Vision: A Survey on Models, Metrics and Applications</div>
<div id='2402.16369v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:47:12Z</div><div>Authors: Gaurav Raut, Apoorv Singh</div><div style='padding-top: 10px; width: 80ex'>Generative AI models have revolutionized various fields by enabling the
creation of realistic and diverse data samples. Among these models, diffusion
models have emerged as a powerful approach for generating high-quality images,
text, and audio. This survey paper provides a comprehensive overview of
generative AI diffusion and legacy models, focusing on their underlying
techniques, applications across different domains, and their challenges. We
delve into the theoretical foundations of diffusion models, including concepts
such as denoising diffusion probabilistic models (DDPM) and score-based
generative modeling. Furthermore, we explore the diverse applications of these
models in text-to-image, image inpainting, and image super-resolution, along
with others, showcasing their potential in creative tasks and data
augmentation. By synthesizing existing research and highlighting critical
advancements in this field, this survey aims to provide researchers and
practitioners with a comprehensive understanding of generative AI diffusion and
legacy models and inspire future innovations in this exciting area of
artificial intelligence.</div><div><a href='http://arxiv.org/abs/2402.16369v1'>2402.16369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17177v2")'>Sora: A Review on Background, Technology, Limitations, and Opportunities
  of Large Vision Models</div>
<div id='2402.17177v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T03:30:58Z</div><div>Authors: Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, Lichao Sun</div><div style='padding-top: 10px; width: 80ex'>Sora is a text-to-video generative AI model, released by OpenAI in February
2024. The model is trained to generate videos of realistic or imaginative
scenes from text instructions and show potential in simulating the physical
world. Based on public technical reports and reverse engineering, this paper
presents a comprehensive review of the model's background, related
technologies, applications, remaining challenges, and future directions of
text-to-video AI models. We first trace Sora's development and investigate the
underlying technologies used to build this "world simulator". Then, we describe
in detail the applications and potential impact of Sora in multiple industries
ranging from film-making and education to marketing. We discuss the main
challenges and limitations that need to be addressed to widely deploy Sora,
such as ensuring safe and unbiased video generation. Lastly, we discuss the
future development of Sora and video generation models in general, and how
advancements in the field could enable new ways of human-AI interaction,
boosting productivity and creativity of video generation.</div><div><a href='http://arxiv.org/abs/2402.17177v2'>2402.17177v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04493v3")'>What makes an image realistic?</div>
<div id='2403.04493v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T13:49:43Z</div><div>Authors: Lucas Theis</div><div style='padding-top: 10px; width: 80ex'>The last decade has seen tremendous progress in our ability to generate
realistic-looking data, be it images, text, audio, or video. Here, we discuss
the closely related problem of quantifying realism, that is, designing
functions that can reliably tell realistic data from unrealistic data. This
problem turns out to be significantly harder to solve and remains poorly
understood, despite its prevalence in machine learning and recent breakthroughs
in generative AI. Drawing on insights from algorithmic information theory, we
discuss why this problem is challenging, why a good generative model alone is
insufficient to solve it, and what a good solution would look like. In
particular, we introduce the notion of a universal critic, which unlike
adversarial critics does not require adversarial training. While universal
critics are not immediately practical, they can serve both as a North Star for
guiding practical implementations and as a tool for analyzing existing attempts
to capture realism.</div><div><a href='http://arxiv.org/abs/2403.04493v3'>2403.04493v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15391v1")'>Genie: Generative Interactive Environments</div>
<div id='2402.15391v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:47:26Z</div><div>Authors: Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel</div><div style='padding-top: 10px; width: 80ex'>We introduce Genie, the first generative interactive environment trained in
an unsupervised manner from unlabelled Internet videos. The model can be
prompted to generate an endless variety of action-controllable virtual worlds
described through text, synthetic images, photographs, and even sketches. At
11B parameters, Genie can be considered a foundation world model. It is
comprised of a spatiotemporal video tokenizer, an autoregressive dynamics
model, and a simple and scalable latent action model. Genie enables users to
act in the generated environments on a frame-by-frame basis despite training
without any ground-truth action labels or other domain-specific requirements
typically found in the world model literature. Further the resulting learned
latent action space facilitates training agents to imitate behaviors from
unseen videos, opening the path for training generalist agents of the future.</div><div><a href='http://arxiv.org/abs/2402.15391v1'>2402.15391v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09786v3")'>Examining Pathological Bias in a Generative Adversarial Network
  Discriminator: A Case Study on a StyleGAN3 Model</div>
<div id='2402.09786v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T08:34:21Z</div><div>Authors: Alvin Grissom II, Ryan F. Lei, Matt Gusdorff, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter</div><div style='padding-top: 10px; width: 80ex'>Generative adversarial networks (GANs) generate photorealistic faces that are
often indistinguishable by humans from real faces. While biases in machine
learning models are often assumed to be due to biases in training data, we find
pathological internal color and luminance biases in the discriminator of a
pre-trained StyleGAN3-r model that are not explicable by the training data. We
also find that the discriminator systematically stratifies scores by both
image- and face-level qualities and that this disproportionately affects images
across gender, race, and other categories. We examine axes common in research
on stereotyping in social psychology.</div><div><a href='http://arxiv.org/abs/2402.09786v3'>2402.09786v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13555v1")'>Benchmarking the Fairness of Image Upsampling Methods</div>
<div id='2401.13555v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T16:13:26Z</div><div>Authors: Mike Laszkiewicz, Imant Daunhawer, Julia E. Vogt, Asja Fischer, Johannes Lederer</div><div style='padding-top: 10px; width: 80ex'>Recent years have witnessed a rapid development of deep generative models for
creating synthetic media, such as images and videos. While the practical
applications of these models in everyday tasks are enticing, it is crucial to
assess the inherent risks regarding their fairness. In this work, we introduce
a comprehensive framework for benchmarking the performance and fairness of
conditional generative models. We develop a set of
metrics$\unicode{x2013}$inspired by their supervised fairness
counterparts$\unicode{x2013}$to evaluate the models on their fairness and
diversity. Focusing on the specific application of image upsampling, we create
a benchmark covering a wide variety of modern upsampling methods. As part of
the benchmark, we introduce UnfairFace, a subset of FairFace that replicates
the racial distribution of common large-scale face datasets. Our empirical
study highlights the importance of using an unbiased training set and reveals
variations in how the algorithms respond to dataset imbalances. Alarmingly, we
find that none of the considered methods produces statistically fair and
diverse results.</div><div><a href='http://arxiv.org/abs/2401.13555v1'>2401.13555v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06606v1")'>Distributionally Generative Augmentation for Fair Facial Attribute
  Classification</div>
<div id='2403.06606v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:50:53Z</div><div>Authors: Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, Hanwang Zhang</div><div style='padding-top: 10px; width: 80ex'>Facial Attribute Classification (FAC) holds substantial promise in widespread
applications. However, FAC models trained by traditional methodologies can be
unfair by exhibiting accuracy inconsistencies across varied data
subpopulations. This unfairness is largely attributed to bias in data, where
some spurious attributes (e.g., Male) statistically correlate with the target
attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the
labels of spurious attributes, which may be unavailable in practice. This work
proposes a novel, generation-based two-stage framework to train a fair FAC
model on biased data without additional annotation. Initially, we identify the
potential spurious attributes based on generative models. Notably, it enhances
interpretability by explicitly showing the spurious attributes in image space.
Following this, for each image, we first edit the spurious attributes with a
random degree sampled from a uniform distribution, while keeping target
attribute unchanged. Then we train a fair FAC model by fostering model
invariance to these augmentation. Extensive experiments on three common
datasets demonstrate the effectiveness of our method in promoting fairness in
FAC without compromising accuracy. Codes are in
https://github.com/heqianpei/DiGA.</div><div><a href='http://arxiv.org/abs/2403.06606v1'>2403.06606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09164v2")'>Less is More: Fewer Interpretable Region via Submodular Subset Selection</div>
<div id='2402.09164v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:30:02Z</div><div>Authors: Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao</div><div style='padding-top: 10px; width: 80ex'>Image attribution algorithms aim to identify important regions that are
highly relevant to model decisions. Although existing attribution solutions can
effectively assign importance to target elements, they still face the following
challenges: 1) existing attribution methods generate inaccurate small regions
thus misleading the direction of correct attribution, and 2) the model cannot
produce good attribution results for samples with wrong predictions. To address
the above challenges, this paper re-models the above image attribution problem
as a submodular subset selection problem, aiming to enhance model
interpretability using fewer regions. To address the lack of attention to local
regions, we construct a novel submodular function to discover more accurate
small interpretation regions. To enhance the attribution effect for all
samples, we also impose four different constraints on the selection of
sub-regions, i.e., confidence, effectiveness, consistency, and collaboration
scores, to assess the importance of various subsets. Moreover, our theoretical
analysis substantiates that the proposed function is in fact submodular.
Extensive experiments show that the proposed method outperforms SOTA methods on
two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset
(CUB-200-2011). For correctly predicted samples, the proposed method improves
the Deletion and Insertion scores with an average of 4.9% and 2.5% gain
relative to HSIC-Attribution. For incorrectly predicted samples, our method
achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in
the average highest confidence and Insertion score respectively. The code is
released at https://github.com/RuoyuChen10/SMDL-Attribution.</div><div><a href='http://arxiv.org/abs/2402.09164v2'>2402.09164v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05174v1")'>VTruST: Controllable value function based subset selection for
  Data-Centric Trustworthy AI</div>
<div id='2403.05174v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T09:28:42Z</div><div>Authors: Soumi Das, Shubhadip Nag, Shreyyash Sharma, Suparna Bhattacharya, Sourangshu Bhattacharya</div><div style='padding-top: 10px; width: 80ex'>Trustworthy AI is crucial to the widespread adoption of AI in high-stakes
applications with fairness, robustness, and accuracy being some of the key
trustworthiness metrics. In this work, we propose a controllable framework for
data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the
trade-offs between the different trustworthiness metrics of the constructed
training datasets. A key challenge in implementing an efficient DCTAI framework
is to design an online value-function-based training data subset selection
algorithm. We pose the training data valuation and subset selection problem as
an online sparse approximation formulation. We propose a novel online version
of the Orthogonal Matching Pursuit (OMP) algorithm for solving this problem.
Experimental results show that VTruST outperforms the state-of-the-art
baselines on social, image, and scientific datasets. We also show that the data
values generated by VTruST can provide effective data-centric explanations for
different trustworthiness metrics.</div><div><a href='http://arxiv.org/abs/2403.05174v1'>2403.05174v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13826v1")'>Measuring Diversity in Co-creative Image Generation</div>
<div id='2403.13826v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T01:55:14Z</div><div>Authors: Francisco Ibarrola, Kazjon Grace</div><div style='padding-top: 10px; width: 80ex'>Quality and diversity have been proposed as reasonable heuristics for
assessing content generated by co-creative systems, but to date there has been
little agreement around what constitutes the latter or how to measure it.
Proposed approaches for assessing generative models in terms of diversity have
limitations in that they compare the model's outputs to a ground truth that in
the era of large pre-trained generative models might not be available, or
entail an impractical number of computations. We propose an alternative based
on entropy of neural network encodings for comparing diversity between sets of
images that does not require ground-truth knowledge and is easy to compute. We
also compare two pre-trained networks and show how the choice relates to the
notion of diversity that we want to evaluate. We conclude with a discussion of
the potential applications of these measures for ideation in interactive
systems, model evaluation, and more broadly within computational creativity.</div><div><a href='http://arxiv.org/abs/2403.13826v1'>2403.13826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02015v1")'>Improving Diffusion-Based Image Synthesis with Context Prediction</div>
<div id='2401.02015v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T01:10:56Z</div><div>Authors: Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, Bin Cui</div><div style='padding-top: 10px; width: 80ex'>Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.</div><div><a href='http://arxiv.org/abs/2401.02015v1'>2401.02015v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03214v2")'>Organic or Diffused: Can We Distinguish Human Art from AI-generated
  Images?</div>
<div id='2402.03214v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:25:04Z</div><div>Authors: Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao</div><div style='padding-top: 10px; width: 80ex'>The advent of generative AI images has completely disrupted the art world.
Distinguishing AI generated images from human art is a challenging problem
whose impact is growing over time. A failure to address this problem allows bad
actors to defraud individuals paying a premium for human art and companies
whose stated policies forbid AI imagery. It is also critical for content owners
to establish copyright, and for model trainers interested in curating training
data in order to avoid potential model collapse.
  There are several different approaches to distinguishing human art from AI
images, including classifiers trained by supervised learning, research tools
targeting diffusion models, and identification by professional artists using
their knowledge of artistic techniques. In this paper, we seek to understand
how well these approaches can perform against today's modern generative models
in both benign and adversarial settings. We curate real human art across 7
styles, generate matching images from 5 generative models, and apply 8
detectors (5 automated detectors and 3 different human groups including 180
crowdworkers, 4000+ professional artists, and 13 expert artists experienced at
detecting AI). Both Hive and expert artists do very well, but make mistakes in
different ways (Hive is weaker against adversarial perturbations while Expert
artists produce higher false positives). We believe these weaknesses will
remain as models continue to evolve, and use our data to demonstrate why a
combined team of human and automated detectors provides the best combination of
accuracy and robustness.</div><div><a href='http://arxiv.org/abs/2402.03214v2'>2402.03214v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06135v1")'>MACE: Mass Concept Erasure in Diffusion Models</div>
<div id='2403.06135v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T08:50:56Z</div><div>Authors: Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, Adams Wai-Kin Kong</div><div style='padding-top: 10px; width: 80ex'>The rapid expansion of large-scale text-to-image diffusion models has raised
growing concerns regarding their potential misuse in creating harmful or
misleading content. In this paper, we introduce MACE, a finetuning framework
for the task of mass concept erasure. This task aims to prevent models from
generating images that embody unwanted concepts when prompted. Existing concept
erasure methods are typically restricted to handling fewer than five concepts
simultaneously and struggle to find a balance between erasing concept synonyms
(generality) and maintaining unrelated concepts (specificity). In contrast,
MACE differs by successfully scaling the erasure scope up to 100 concepts and
by achieving an effective balance between generality and specificity. This is
achieved by leveraging closed-form cross-attention refinement along with LoRA
finetuning, collectively eliminating the information of undesirable concepts.
Furthermore, MACE integrates multiple LoRAs without mutual interference. We
conduct extensive evaluations of MACE against prior methods across four
different tasks: object erasure, celebrity erasure, explicit content erasure,
and artistic style erasure. Our results reveal that MACE surpasses prior
methods in all evaluated tasks. Code is available at
https://github.com/Shilin-LU/MACE.</div><div><a href='http://arxiv.org/abs/2403.06135v1'>2403.06135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12326v1")'>Removing Undesirable Concepts in Text-to-Image Generative Models with
  Learnable Prompts</div>
<div id='2403.12326v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:42:04Z</div><div>Authors: Anh Bui, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh Phung</div><div style='padding-top: 10px; width: 80ex'>Generative models have demonstrated remarkable potential in generating
visually impressive content from textual descriptions. However, training these
models on unfiltered internet data poses the risk of learning and subsequently
propagating undesirable concepts, such as copyrighted or unethical content. In
this paper, we propose a novel method to remove undesirable concepts from
text-to-image generative models by incorporating a learnable prompt into the
cross-attention module. This learnable prompt acts as additional memory to
transfer the knowledge of undesirable concepts into it and reduce the
dependency of these concepts on the model parameters and corresponding textual
inputs. Because of this knowledge transfer into the prompt, erasing these
undesirable concepts is more stable and has minimal negative impact on other
concepts. We demonstrate the effectiveness of our method on the Stable
Diffusion model, showcasing its superiority over state-of-the-art erasure
methods in terms of removing undesirable content while preserving other
unrelated elements.</div><div><a href='http://arxiv.org/abs/2403.12326v1'>2403.12326v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05947v1")'>Separable Multi-Concept Erasure from Diffusion Models</div>
<div id='2402.05947v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T11:10:57Z</div><div>Authors: Mengnan Zhao, Lihe Zhang, Tianhang Zheng, Yuqiu Kong, Baocai Yin</div><div style='padding-top: 10px; width: 80ex'>Large-scale diffusion models, known for their impressive image generation
capabilities, have raised concerns among researchers regarding social impacts,
such as the imitation of copyrighted artistic styles. In response, existing
approaches turn to machine unlearning techniques to eliminate unsafe concepts
from pre-trained models. However, these methods compromise the generative
performance and neglect the coupling among multi-concept erasures, as well as
the concept restoration problem. To address these issues, we propose a
Separable Multi-concept Eraser (SepME), which mainly includes two parts: the
generation of concept-irrelevant representations and the weight decoupling. The
former aims to avoid unlearning substantial information that is irrelevant to
forgotten concepts. The latter separates optimizable model weights, making each
weight increment correspond to a specific concept erasure without affecting
generative performance on other concepts. Specifically, the weight increment
for erasing a specified concept is formulated as a linear combination of
solutions calculated based on other known undesirable concepts. Extensive
experiments indicate the efficacy of our approach in eliminating concepts,
preserving model performance, and offering flexibility in the erasure or
recovery of various concepts.</div><div><a href='http://arxiv.org/abs/2402.05947v1'>2402.05947v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13807v1")'>Editing Massive Concepts in Text-to-Image Diffusion Models</div>
<div id='2403.13807v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:59:57Z</div><div>Authors: Tianwei Xiong, Yue Wu, Enze Xie, Yue Wu, Zhenguo Li, Xihui Liu</div><div style='padding-top: 10px; width: 80ex'>Text-to-image diffusion models suffer from the risk of generating outdated,
copyrighted, incorrect, and biased content. While previous methods have
mitigated the issues on a small scale, it is essential to handle them
simultaneously in larger-scale real-world scenarios. We propose a two-stage
method, Editing Massive Concepts In Diffusion Models (EMCID). The first stage
performs memory optimization for each individual concept with dual
self-distillation from text alignment loss and diffusion noise prediction loss.
The second stage conducts massive concept editing with multi-layer, closed form
model editing. We further propose a comprehensive benchmark, named ImageNet
Concept Editing Benchmark (ICEB), for evaluating massive concept editing for
T2I models with two subtasks, free-form prompts, massive concept categories,
and extensive evaluation metrics. Extensive experiments conducted on our
proposed benchmark and previous benchmarks demonstrate the superior scalability
of EMCID for editing up to 1,000 concepts, providing a practical approach for
fast adjustment and re-deployment of T2I diffusion models in real-world
applications.</div><div><a href='http://arxiv.org/abs/2403.13807v1'>2403.13807v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10615v1")'>LightIt: Illumination Modeling and Control for Diffusion Models</div>
<div id='2403.10615v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T18:26:33Z</div><div>Authors: Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, Yannick Hold-Geoffroy</div><div style='padding-top: 10px; width: 80ex'>We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.10615v1'>2403.10615v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16843v1")'>Multi-LoRA Composition for Image Generation</div>
<div id='2402.16843v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:59:18Z</div><div>Authors: Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, Weizhu Chen</div><div style='padding-top: 10px; width: 80ex'>Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models
for the accurate rendition of specific elements like distinct characters or
unique styles in generated images. Nonetheless, existing methods face
challenges in effectively composing multiple LoRAs, especially as the number of
LoRAs to be integrated grows, thus hindering the creation of complex imagery.
In this paper, we study multi-LoRA composition through a decoding-centric
perspective. We present two training-free methods: LoRA Switch, which
alternates between different LoRAs at each denoising step, and LoRA Composite,
which simultaneously incorporates all LoRAs to guide more cohesive image
synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new
comprehensive testbed as part of this research. It features a diverse range of
LoRA categories with 480 composition sets. Utilizing an evaluation framework
based on GPT-4V, our findings demonstrate a clear improvement in performance
with our methods over the prevalent baseline, particularly evident when
increasing the number of LoRAs in a composition.</div><div><a href='http://arxiv.org/abs/2402.16843v1'>2402.16843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13916v1")'>Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and
  Style Transfer Techniques</div>
<div id='2403.13916v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T18:36:30Z</div><div>Authors: W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis</div><div style='padding-top: 10px; width: 80ex'>We present novel approaches involving generative adversarial networks and
diffusion models in order to synthesize high quality, live and spoof
fingerprint images while preserving features such as uniqueness and diversity.
We generate live fingerprints from noise with a variety of methods, and we use
image translation techniques to translate live fingerprint images to spoof. To
generate different types of spoof images based on limited training data we
incorporate style transfer techniques through a cycle autoencoder equipped with
a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to
avoid mode collapse and instability. We find that when the spoof training data
includes distinct spoof characteristics, it leads to improved live-to-spoof
translation. We assess the diversity and realism of the generated live
fingerprint images mainly through the Fr\'echet Inception Distance (FID) and
the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of
15.78. The comparable WGAN-GP model achieved slightly higher FID while
performing better in the uniqueness assessment due to a slightly lower FAR when
matched against the training data, indicating better creativity. Moreover, we
give example images showing that a DDPM model clearly can generate realistic
fingerprint images.</div><div><a href='http://arxiv.org/abs/2403.13916v1'>2403.13916v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01329v1")'>Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow
  Models</div>
<div id='2403.01329v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T22:27:44Z</div><div>Authors: Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet, Albert Pumarola, Yaron Lipman</div><div style='padding-top: 10px; width: 80ex'>This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver
distillation approach to improve sample efficiency of Diffusion and Flow
models. BNS solvers are based on a family of non-stationary solvers that
provably subsumes existing numerical ODE solvers and consequently demonstrate
considerable improvement in sample approximation (PSNR) over these baselines.
Compared to model distillation, BNS solvers benefit from a tiny parameter space
($&lt;$200 parameters), fast optimization (two orders of magnitude faster),
maintain diversity of samples, and in contrast to previous solver distillation
approaches nearly close the gap from standard distillation methods such as
Progressive Distillation in the low-medium NFE regime. For example, BNS solver
achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We
experimented with BNS solvers for conditional image generation, text-to-image
generation, and text-2-audio generation showing significant improvement in
sample approximation (PSNR) in all.</div><div><a href='http://arxiv.org/abs/2403.01329v1'>2403.01329v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14017v1")'>D-Flow: Differentiating through Flows for Controlled Generation</div>
<div id='2402.14017v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:56:03Z</div><div>Authors: Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, Yaron Lipman</div><div style='padding-top: 10px; width: 80ex'>Taming the generation outcome of state of the art Diffusion and Flow-Matching
(FM) models without having to re-train a task-specific model unlocks a powerful
tool for solving inverse problems, conditional generation, and controlled
generation in general. In this work we introduce D-Flow, a simple framework for
controlling the generation process by differentiating through the flow,
optimizing for the source (noise) point. We motivate this framework by our key
observation stating that for Diffusion/FM models trained with Gaussian
probability paths, differentiating through the generation process projects
gradient on the data manifold, implicitly injecting the prior into the
optimization process. We validate our framework on linear and non-linear
controlled generation problems including: image and audio inverse problems and
conditional molecule generation reaching state of the art performance across
all.</div><div><a href='http://arxiv.org/abs/2402.14017v1'>2402.14017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12179v1")'>DITTO: Diffusion Inference-Time T-Optimization for Music Generation</div>
<div id='2401.12179v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:10:10Z</div><div>Authors: Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas J. Bryan</div><div style='padding-top: 10px; width: 80ex'>We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose
frame-work for controlling pre-trained text-to-music diffusion models at
inference-time via optimizing initial noise latents. Our method can be used to
optimize through any differentiable feature matching loss to achieve a target
(stylized) output and leverages gradient checkpointing for memory efficiency.
We demonstrate a surprisingly wide-range of applications for music generation
including inpainting, outpainting, and looping as well as intensity, melody,
and musical structure control - all without ever fine-tuning the underlying
model. When we compare our approach against related training, guidance, and
optimization-based methods, we find DITTO achieves state-of-the-art performance
on nearly all tasks, including outperforming comparable approaches on
controllability, audio quality, and computational efficiency, thus opening the
door for high-quality, flexible, training-free control of diffusion models.
Sound examples can be found at https://DITTO-Music.github.io/web/.</div><div><a href='http://arxiv.org/abs/2401.12179v1'>2401.12179v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14285v2")'>Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion</div>
<div id='2402.14285v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T04:55:58Z</div><div>Authors: Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, Yisong Yue</div><div style='padding-top: 10px; width: 80ex'>We study the problem of symbolic music generation (e.g., generating piano
rolls), with a technical focus on non-differentiable rule guidance. Musical
rules are often expressed in symbolic form on note characteristics, such as
note density or chord progression, many of which are non-differentiable which
pose a challenge when using them for guided diffusion. We propose Stochastic
Control Guidance (SCG), a novel guidance method that only requires forward
evaluation of rule functions that can work with pre-trained diffusion models in
a plug-and-play way, thus achieving training-free guidance for
non-differentiable rules for the first time. Additionally, we introduce a
latent diffusion architecture for symbolic music generation with high time
resolution, which can be composed with SCG in a plug-and-play fashion. Compared
to standard strong baselines in symbolic music generation, this framework
demonstrates marked advancements in music quality and rule-based
controllability, outperforming current state-of-the-art generators in a variety
of settings. For detailed demonstrations, code and model checkpoints, please
visit our project website: https://scg-rule-guided-music.github.io/.</div><div><a href='http://arxiv.org/abs/2402.14285v2'>2402.14285v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07995v1")'>Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic
  Music Generation</div>
<div id='2403.07995v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T18:03:08Z</div><div>Authors: Keshav Bhandari, Simon Colton</div><div style='padding-top: 10px; width: 80ex'>Modelling musical structure is vital yet challenging for artificial
intelligence systems that generate symbolic music compositions. This literature
review dissects the evolution of techniques for incorporating coherent
structure, from symbolic approaches to foundational and transformative deep
learning methods that harness the power of computation and data across a wide
variety of training paradigms. In the later stages, we review an emerging
technique which we refer to as "sub-task decomposition" that involves
decomposing music generation into separate high-level structural planning and
content creation stages. Such systems incorporate some form of musical
knowledge or neuro-symbolic methods by extracting melodic skeletons or
structural templates to guide the generation. Progress is evident in capturing
motifs and repetitions across all three eras reviewed, yet modelling the
nuanced development of themes across extended compositions in the style of
human composers remains difficult. We outline several key future directions to
realize the synergistic benefits of combining approaches from all eras
examined.</div><div><a href='http://arxiv.org/abs/2403.07995v1'>2403.07995v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16153v1")'>ChatMusician: Understanding and Generating Music Intrinsically with LLM</div>
<div id='2402.16153v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T17:19:41Z</div><div>Authors: Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu, Cong Liu, Ziya Zhou, Ziyang Ma, Liumeng Xue, Ziyu Wang, Qin Liu, Tianyu Zheng, Yizhi Li, Yinghao Ma, Yiming Liang, Xiaowei Chi, Ruibo Liu, Zili Wang, Pengfei Li, Jingcheng Wu, Chenghua Lin, Qifeng Liu, Tao Jiang, Wenhao Huang, Wenhu Chen, Emmanouil Benetos, Jie Fu, Gus Xia, Roger Dannenberg, Wei Xue, Shiyin Kang, Yike Guo</div><div style='padding-top: 10px; width: 80ex'>While Large Language Models (LLMs) demonstrate impressive capabilities in
text generation, we find that their ability has yet to be generalized to music,
humanity's creative language. We introduce ChatMusician, an open-source LLM
that integrates intrinsic musical abilities. It is based on continual
pre-training and finetuning LLaMA2 on a text-compatible music representation,
ABC notation, and the music is treated as a second language. ChatMusician can
understand and generate music with a pure text tokenizer without any external
multi-modal neural structures or tokenizers. Interestingly, endowing musical
abilities does not harm language abilities, even achieving a slightly higher
MMLU score. Our model is capable of composing well-structured, full-length
music, conditioned on texts, chords, melodies, motifs, musical forms, etc,
surpassing GPT-4 baseline. On our meticulously curated college-level music
understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and
GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs
can be an excellent compressor for music, but there remains significant
territory to be conquered. We release our 4B token music-language corpora
MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.</div><div><a href='http://arxiv.org/abs/2402.16153v1'>2402.16153v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11706v1")'>Generalized Multi-Source Inference for Text Conditioned Music Diffusion
  Models</div>
<div id='2403.11706v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T12:08:01Z</div><div>Authors: Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, Emanuele Rodolà</div><div style='padding-top: 10px; width: 80ex'>Multi-Source Diffusion Models (MSDM) allow for compositional musical
generation tasks: generating a set of coherent sources, creating
accompaniments, and performing source separation. Despite their versatility,
they require estimating the joint distribution over the sources, necessitating
pre-separated musical data, which is rarely available, and fixing the number
and type of sources at training time. This paper generalizes MSDM to arbitrary
time-domain diffusion models conditioned on text embeddings. These models do
not require separated data as they are trained on mixtures, can parameterize an
arbitrary number of sources, and allow for rich semantic control. We propose an
inference procedure enabling the coherent generation of sources and
accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform
source separation. We experiment with diffusion models trained on Slakh2100 and
MTG-Jamendo, showcasing competitive generation and separation results in a
relaxed data setting.</div><div><a href='http://arxiv.org/abs/2403.11706v1'>2403.11706v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15258v1")'>High Resolution Guitar Transcription via Domain Adaptation</div>
<div id='2402.15258v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T10:56:47Z</div><div>Authors: Xavier Riley, Drew Edwards, Simon Dixon</div><div style='padding-top: 10px; width: 80ex'>Automatic music transcription (AMT) has achieved high accuracy for piano due
to the availability of large, high-quality datasets such as MAESTRO and MAPS,
but comparable datasets are not yet available for other instruments. In recent
work, however, it has been demonstrated that aligning scores to transcription
model activations can produce high quality AMT training data for instruments
other than piano. Focusing on the guitar, we refine this approach to training
on score data using a dataset of commercially available score-audio pairs. We
propose the use of a high-resolution piano transcription model to train a new
guitar transcription model. The resulting model obtains state-of-the-art
transcription results on GuitarSet in a zero-shot context, improving on
previously published methods.</div><div><a href='http://arxiv.org/abs/2402.15258v1'>2402.15258v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01424v1")'>A Data-Driven Analysis of Robust Automatic Piano Transcription</div>
<div id='2402.01424v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:11:23Z</div><div>Authors: Drew Edwards, Simon Dixon, Emmanouil Benetos, Akira Maezawa, Yuta Kusaka</div><div style='padding-top: 10px; width: 80ex'>Algorithms for automatic piano transcription have improved dramatically in
recent years due to new datasets and modeling techniques. Recent developments
have focused primarily on adapting new neural network architectures, such as
the Transformer and Perceiver, in order to yield more accurate systems. In this
work, we study transcription systems from the perspective of their training
data. By measuring their performance on out-of-distribution annotated piano
data, we show how these models can severely overfit to acoustic properties of
the training data. We create a new set of audio for the MAESTRO dataset,
captured automatically in a professional studio recording environment via
Yamaha Disklavier playback. Using various data augmentation techniques when
training with the original and re-performed versions of the MAESTRO dataset, we
achieve state-of-the-art note-onset accuracy of 88.4 F1-score on the MAPS
dataset, without seeing any of its training data. We subsequently analyze these
data augmentation techniques in a series of ablation studies to better
understand their influence on the resulting models.</div><div><a href='http://arxiv.org/abs/2402.01424v1'>2402.01424v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15569v1")'>Toward Fully Self-Supervised Multi-Pitch Estimation</div>
<div id='2402.15569v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T19:12:41Z</div><div>Authors: Frank Cwitkowitz, Zhiyao Duan</div><div style='padding-top: 10px; width: 80ex'>Multi-pitch estimation is a decades-long research problem involving the
detection of pitch activity associated with concurrent musical events within
multi-instrument mixtures. Supervised learning techniques have demonstrated
solid performance on more narrow characterizations of the task, but suffer from
limitations concerning the shortage of large-scale and diverse polyphonic music
datasets with multi-pitch annotations. We present a suite of self-supervised
learning objectives for multi-pitch estimation, which encourage the
concentration of support around harmonics, invariance to timbral
transformations, and equivariance to geometric transformations. These
objectives are sufficient to train an entirely convolutional autoencoder to
produce multi-pitch salience-grams directly, without any fine-tuning. Despite
training exclusively on a collection of synthetic single-note audio samples,
our fully self-supervised framework generalizes to polyphonic music mixtures,
and achieves performance comparable to supervised models trained on
conventional multi-pitch datasets.</div><div><a href='http://arxiv.org/abs/2402.15569v1'>2402.15569v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08891v1")'>Tempo estimation as fully self-supervised binary classification</div>
<div id='2401.08891v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T00:15:16Z</div><div>Authors: Florian Henkel, Jaehun Kim, Matthew C. McCallum, Samuel E. Sandberg, Matthew E. P. Davies</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the problem of global tempo estimation in musical audio.
Given that annotating tempo is time-consuming and requires certain musical
expertise, few publicly available data sources exist to train machine learning
models for this task. Towards alleviating this issue, we propose a fully
self-supervised approach that does not rely on any human labeled data. Our
method builds on the fact that generic (music) audio embeddings already encode
a variety of properties, including information about tempo, making them easily
adaptable for downstream tasks. While recent work in self-supervised tempo
estimation aimed to learn a tempo specific representation that was subsequently
used to train a supervised classifier, we reformulate the task into the binary
classification problem of predicting whether a target track has the same or a
different tempo compared to a reference. While the former still requires
labeled training data for the final classification model, our approach uses
arbitrary unlabeled music data in combination with time-stretching for model
training as well as a small set of synthetically created reference samples for
predicting the final tempo. Evaluation of our approach in comparison with the
state-of-the-art reveals highly competitive performance when the constraint of
finding the precise tempo octave is relaxed.</div><div><a href='http://arxiv.org/abs/2401.08891v1'>2401.08891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08902v1")'>Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for
  Tempo Prediction and Search</div>
<div id='2401.08902v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T01:06:22Z</div><div>Authors: Matthew C. McCallum, Florian Henkel, Jaehun Kim, Samuel E. Sandberg, Matthew E. P. Davies</div><div style='padding-top: 10px; width: 80ex'>Audio embeddings enable large scale comparisons of the similarity of audio
files for applications such as search and recommendation. Due to the
subjectivity of audio similarity, it can be desirable to design systems that
answer not only whether audio is similar, but similar in what way (e.g., wrt.
tempo, mood or genre). Previous works have proposed disentangled embedding
spaces where subspaces representing specific, yet possibly correlated,
attributes can be weighted to emphasize those attributes in downstream tasks.
However, no research has been conducted into the independence of these
subspaces, nor their manipulation, in order to retrieve tracks that are similar
but different in a specific way. Here, we explore the manipulation of tempo in
embedding spaces as a case-study towards this goal. We propose tempo
translation functions that allow for efficient manipulation of tempo within a
pre-existing embedding space whilst maintaining other properties such as genre.
As this translation is specific to tempo it enables retrieval of tracks that
are similar but have specifically different tempi. We show that such a function
can be used as an efficient data augmentation strategy for both training of
downstream tempo predictors, and improved nearest neighbor retrieval of
properties largely independent of tempo.</div><div><a href='http://arxiv.org/abs/2401.08902v1'>2401.08902v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08889v1")'>On the Effect of Data-Augmentation on Local Embedding Properties in the
  Contrastive Learning of Music Audio Representations</div>
<div id='2401.08889v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T00:12:13Z</div><div>Authors: Matthew C. McCallum, Matthew E. P. Davies, Florian Henkel, Jaehun Kim, Samuel E. Sandberg</div><div style='padding-top: 10px; width: 80ex'>Audio embeddings are crucial tools in understanding large catalogs of music.
Typically embeddings are evaluated on the basis of the performance they provide
in a wide range of downstream tasks, however few studies have investigated the
local properties of the embedding spaces themselves which are important in
nearest neighbor algorithms, commonly used in music search and recommendation.
In this work we show that when learning audio representations on music datasets
via contrastive learning, musical properties that are typically homogeneous
within a track (e.g., key and tempo) are reflected in the locality of
neighborhoods in the resulting embedding space. By applying appropriate data
augmentation strategies, localisation of such properties can not only be
reduced but the localisation of other attributes is increased. For example,
locality of features such as pitch and tempo that are less relevant to
non-expert listeners, may be mitigated while improving the locality of more
salient features such as genre and mood, achieving state-of-the-art performance
in nearest neighbor retrieval accuracy. Similarly, we show that the optimal
selection of data augmentation strategies for contrastive learning of music
audio embeddings is dependent on the downstream task, highlighting this as an
important embedding design decision.</div><div><a href='http://arxiv.org/abs/2401.08889v1'>2401.08889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09598v1")'>Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds</div>
<div id='2403.09598v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:39:14Z</div><div>Authors: Ilyass Moummad, Nicolas Farrugia, Romain Serizel, Jeremy Froidevaux, Vincent Lostanlen</div><div style='padding-top: 10px; width: 80ex'>Multi-label imbalanced classification poses a significant challenge in
machine learning, particularly evident in bioacoustics where animal sounds
often co-occur, and certain sounds are much less frequent than others. This
paper focuses on the specific case of classifying anuran species sounds using
the dataset AnuraSet, that contains both class imbalance and multi-label
examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a
framework that leverages mixing regularization methods Mixup, Manifold Mixup,
and MultiMix. Experimental results show that these methods, individually, may
lead to suboptimal results; however, when applied randomly, with one selected
at each training iteration, they prove effective in addressing the mentioned
challenges, particularly for rare classes with few occurrences. Further
analysis reveals that Mix2 is also proficient in classifying sounds across
various levels of class co-occurrences.</div><div><a href='http://arxiv.org/abs/2403.09598v1'>2403.09598v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02566v1")'>Siamese Residual Neural Network for Musical Shape Evaluation in Piano
  Performance Assessment</div>
<div id='2401.02566v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T22:51:13Z</div><div>Authors: Xiaoquan Li, Stephan Weiss, Yijun Yan, Yinhe Li, Jinchang Ren, John Soraghan, Ming Gong</div><div style='padding-top: 10px; width: 80ex'>Understanding and identifying musical shape plays an important role in music
education and performance assessment. To simplify the otherwise time- and
cost-intensive musical shape evaluation, in this paper we explore how
artificial intelligence (AI) driven models can be applied. Considering musical
shape evaluation as a classification problem, a light-weight Siamese residual
neural network (S-ResNN) is proposed to automatically identify musical shapes.
To assess the proposed approach in the context of piano musical shape
evaluation, we have generated a new dataset, containing 4116 music pieces
derived by 147 piano preparatory exercises and performed in 28 categories of
musical shapes. The experimental results show that the S-ResNN significantly
outperforms a number of benchmark methods in terms of the precision, recall and
F1 score.</div><div><a href='http://arxiv.org/abs/2401.02566v1'>2401.02566v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10024v1")'>MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate
  Instrument Leakage</div>
<div id='2403.10024v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T05:13:38Z</div><div>Authors: Hao Hao Tan, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki Mitsufuji</div><div style='padding-top: 10px; width: 80ex'>This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA)
token-based multi-instrument automatic music transcription (AMT) model. Despite
SOTA performance, MT3 has the issue of instrument leakage, where transcriptions
are fragmented across different instruments. To mitigate this, we propose
MR-MT3, with enhancements including a memory retention mechanism, prior token
sampling, and token shuffling are proposed. These methods are evaluated on the
Slakh2100 dataset, demonstrating improved onset F1 scores and reduced
instrument leakage. In addition to the conventional multi-instrument
transcription F1 score, new metrics such as the instrument leakage ratio and
the instrument detection F1 score are introduced for a more comprehensive
assessment of transcription quality. The study also explores the issue of
domain overfitting by evaluating MT3 on single-instrument monophonic datasets
such as ComMU and NSynth. The findings, along with the source code, are shared
to facilitate future work aimed at refining token-based multi-instrument AMT
models.</div><div><a href='http://arxiv.org/abs/2403.10024v1'>2403.10024v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14589v1")'>Avoiding an AI-imposed Taylor's Version of all music history</div>
<div id='2402.14589v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:36:19Z</div><div>Authors: Nick Collins, Mick Grierson</div><div style='padding-top: 10px; width: 80ex'>As future musical AIs adhere closely to human music, they may form their own
attachments to particular human artists in their databases, and these biases
may in the worst case lead to potential existential threats to all musical
history. AI super fans may act to corrupt the historical record and extant
recordings in favour of their own preferences, and preservation of the
diversity of world music culture may become even more of a pressing issue than
the imposition of 12 tone equal temperament or other Western homogenisations.
We discuss the technical capability of AI cover software and produce Taylor's
Versions of famous tracks from Western pop history as provocative examples; the
quality of these productions does not affect the overall argument (which might
even see a future AI try to impose the sound of paperclips onto all existing
audio files, let alone Taylor Swift). We discuss some potential defenses
against the danger of future musical monopolies, whilst analysing the
feasibility of a maximal 'Taylor Swiftication' of the complete musical record.</div><div><a href='http://arxiv.org/abs/2402.14589v1'>2402.14589v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15294v1")'>A Survey of Music Generation in the Context of Interaction</div>
<div id='2402.15294v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:41:44Z</div><div>Authors: Ismael Agchar, Ilja Baumann, Franziska Braun, Paula Andrea Perez-Toro, Korbinian Riedhammer, Sebastian Trump, Martin Ullrich</div><div style='padding-top: 10px; width: 80ex'>In recent years, machine learning, and in particular generative adversarial
neural networks (GANs) and attention-based neural networks (transformers), have
been successfully used to compose and generate music, both melodies and
polyphonic pieces. Current research focuses foremost on style replication (eg.
generating a Bach-style chorale) or style transfer (eg. classical to jazz)
based on large amounts of recorded or transcribed music, which in turn also
allows for fairly straight-forward "performance" evaluation. However, most of
these models are not suitable for human-machine co-creation through live
interaction, neither is clear, how such models and resulting creations would be
evaluated. This article presents a thorough review of music representation,
feature analysis, heuristic algorithms, statistical and parametric modelling,
and human and automatic evaluation measures, along with a discussion of which
approaches and models seem most suitable for live interaction.</div><div><a href='http://arxiv.org/abs/2402.15294v1'>2402.15294v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17109v1")'>Evaluation in Neural Style Transfer: A Review</div>
<div id='2401.17109v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T15:45:30Z</div><div>Authors: Eleftherios Ioannou, Steve Maddock</div><div style='padding-top: 10px; width: 80ex'>The field of Neural Style Transfer (NST) has witnessed remarkable progress in
the past few years, with approaches being able to synthesize artistic and
photorealistic images and videos of exceptional quality. To evaluate such
results, a diverse landscape of evaluation methods and metrics is used,
including authors' opinions based on side-by-side comparisons, human evaluation
studies that quantify the subjective judgements of participants, and a
multitude of quantitative computational metrics which objectively assess the
different aspects of an algorithm's performance. However, there is no consensus
regarding the most suitable and effective evaluation procedure that can
guarantee the reliability of the results. In this review, we provide an
in-depth analysis of existing evaluation techniques, identify the
inconsistencies and limitations of current evaluation methods, and give
recommendations for standardized evaluation practices. We believe that the
development of a robust evaluation framework will not only enable more
meaningful and fairer comparisons among NST methods but will also enhance the
comprehension and interpretation of research findings in the field.</div><div><a href='http://arxiv.org/abs/2401.17109v1'>2401.17109v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01412v1")'>Bass Accompaniment Generation via Latent Diffusion</div>
<div id='2402.01412v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:44:47Z</div><div>Authors: Marco Pasini, Maarten Grachten, Stefan Lattner</div><div style='padding-top: 10px; width: 80ex'>The ability to automatically generate music that appropriately matches an
arbitrary input track is a challenging task. We present a novel controllable
system for generating single stems to accompany musical mixes of arbitrary
length. At the core of our method are audio autoencoders that efficiently
compress audio waveform samples into invertible latent representations, and a
conditional latent diffusion model that takes as input the latent encoding of a
mix and generates the latent encoding of a corresponding stem. To provide
control over the timbre of generated samples, we introduce a technique to
ground the latent space to a user-provided reference style during diffusion
sampling. For further improving audio quality, we adapt classifier-free
guidance to avoid distortions at high guidance strengths when generating an
unbounded latent space. We train our model on a dataset of pairs of mixes and
matching bass stems. Quantitative experiments demonstrate that, given an input
mix, the proposed system can generate basslines with user-specified timbres.
Our controllable conditional audio generation framework represents a
significant step forward in creating generative AI tools to assist musicians in
music production.</div><div><a href='http://arxiv.org/abs/2402.01412v1'>2402.01412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09294v1")'>T-FOLEY: A Controllable Waveform-Domain Diffusion Model for
  Temporal-Event-Guided Foley Sound Synthesis</div>
<div id='2401.09294v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T15:54:36Z</div><div>Authors: Yoonjin Chung, Junwon Lee, Juhan Nam</div><div style='padding-top: 10px; width: 80ex'>Foley sound, audio content inserted synchronously with videos, plays a
critical role in the user experience of multimedia content. Recently, there has
been active research in Foley sound synthesis, leveraging the advancements in
deep generative models. However, such works mainly focus on replicating a
single sound class or a textual sound description, neglecting temporal
information, which is crucial in the practical applications of Foley sound. We
present T-Foley, a Temporal-event-guided waveform generation model for Foley
sound synthesis. T-Foley generates high-quality audio using two conditions: the
sound class and temporal event feature. For temporal conditioning, we devise a
temporal event feature and a novel conditioning technique named Block-FiLM.
T-Foley achieves superior performance in both objective and subjective
evaluation metrics and generates Foley sound well-synchronized with the
temporal events. Additionally, we showcase T-Foley's practical applications,
particularly in scenarios involving vocal mimicry for temporal event control.
We show the demo on our companion website.</div><div><a href='http://arxiv.org/abs/2401.09294v1'>2401.09294v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09407v1")'>LM2D: Lyrics- and Music-Driven Dance Synthesis</div>
<div id='2403.09407v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T13:59:04Z</div><div>Authors: Wenjie Yin, Xuejiao Zhao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman</div><div style='padding-top: 10px; width: 80ex'>Dance typically involves professional choreography with complex movements
that follow a musical rhythm and can also be influenced by lyrical content. The
integration of lyrics in addition to the auditory dimension, enriches the
foundational tone and makes motion generation more amenable to its semantic
meanings. However, existing dance synthesis methods tend to model motions only
conditioned on audio signals. In this work, we make two contributions to bridge
this gap. First, we propose LM2D, a novel probabilistic architecture that
incorporates a multimodal diffusion model with consistency distillation,
designed to create dance conditioned on both music and lyrics in one diffusion
generation step. Second, we introduce the first 3D dance-motion dataset that
encompasses both music and lyrics, obtained with pose estimation technologies.
We evaluate our model against music-only baseline models with objective metrics
and human evaluations, including dancers and choreographers. The results
demonstrate LM2D is able to produce realistic and diverse dance matching both
lyrics and music. A video summary can be accessed at:
https://youtu.be/4XCgvYookvA.</div><div><a href='http://arxiv.org/abs/2403.09407v1'>2403.09407v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00892v1")'>EVA-GAN: Enhanced Various Audio Generation via Scalable Generative
  Adversarial Networks</div>
<div id='2402.00892v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T03:31:03Z</div><div>Authors: Shijia Liao, Shiyi Lan, Arun George Zachariah</div><div style='padding-top: 10px; width: 80ex'>The advent of Large Models marks a new era in machine learning, significantly
outperforming smaller models by leveraging vast datasets to capture and
synthesize complex patterns. Despite these advancements, the exploration into
scaling, especially in the audio generation domain, remains limited, with
previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and
suffering from both spectral discontinuities and blurriness in the
high-frequency domain, alongside a lack of robustness against out-of-domain
data. These limitations restrict the applicability of models to diverse use
cases, including music and singing generation. Our work introduces Enhanced
Various Audio Generation via Scalable Generative Adversarial Networks
(EVA-GAN), yields significant improvements over previous state-of-the-art in
spectral and high-frequency reconstruction and robustness in out-of-domain data
performance, enabling the generation of HiFi audios by employing an extensive
dataset of 36,000 hours of 44.1kHz audio, a context-aware module, a
Human-In-The-Loop artifact measurement toolkit, and expands the model to
approximately 200 million parameters. Demonstrations of our work are available
at https://double-blind-eva-gan.cc.</div><div><a href='http://arxiv.org/abs/2402.00892v1'>2402.00892v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01753v1")'>SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and
  Music Synthesis</div>
<div id='2402.01753v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T09:17:57Z</div><div>Authors: Teysir Baoueb, Haocheng Liu, Mathieu Fontaine, Jonathan Le Roux, Gael Richard</div><div style='padding-top: 10px; width: 80ex'>Generative adversarial network (GAN) models can synthesize highquality audio
signals while ensuring fast sample generation. However, they are difficult to
train and are prone to several issues including mode collapse and divergence.
In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN,
which was initially devised for speech synthesis from mel spectrogram. In our
model, the training stability is enhanced by means of a forward diffusion
process which consists in injecting noise from a Gaussian distribution to both
real and fake samples before inputting them to the discriminator. We further
improve the model by exploiting a spectrally-shaped noise distribution with the
aim to make the discriminator's task more challenging. We then show the merits
of our proposed model for speech and music synthesis on several datasets. Our
experiments confirm that our model compares favorably in audio quality and
efficiency compared to several baselines.</div><div><a href='http://arxiv.org/abs/2402.01753v1'>2402.01753v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00811v1")'>An Analysis of the Variance of Diffusion-based Speech Enhancement</div>
<div id='2402.00811v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:46:19Z</div><div>Authors: Bunlong Lay, Timo Gerkmann</div><div style='padding-top: 10px; width: 80ex'>Diffusion models proved to be powerful models for generative speech
enhancement. In recent SGMSE+ approaches, training involves a stochastic
differential equation for the diffusion process, adding both Gaussian and
environmental noise to the clean speech signal gradually. The speech
enhancement performance varies depending on the choice of the stochastic
differential equation that controls the evolution of the mean and the variance
along the diffusion processes when adding environmental and Gaussian noise. In
this work, we highlight that the scale of the variance is a dominant parameter
for speech enhancement performance and show that it controls the tradeoff
between noise attenuation and speech distortions. More concretely, we show that
a larger variance increases the noise attenuation and allows for reducing the
computational footprint, as fewer function evaluations for generating the
estimate are required.</div><div><a href='http://arxiv.org/abs/2402.00811v1'>2402.00811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13498v1")'>Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific
  Input Representation and Diffusion Outpainting</div>
<div id='2401.13498v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T14:44:01Z</div><div>Authors: Hounsu Kim, Soonbeom Choi, Juhan Nam</div><div style='padding-top: 10px; width: 80ex'>Synthesizing performing guitar sound is a highly challenging task due to the
polyphony and high variability in expression. Recently, deep generative models
have shown promising results in synthesizing expressive polyphonic instrument
sounds from music scores, often using a generic MIDI input. In this work, we
propose an expressive acoustic guitar sound synthesis model with a customized
input representation to the instrument, which we call guitarroll. We implement
the proposed approach using diffusion-based outpainting which can generate
audio with long-term consistency. To overcome the lack of MIDI/audio-paired
datasets, we used not only an existing guitar dataset but also collected data
from a high quality sample-based guitar synthesizer. Through quantitative and
qualitative evaluations, we show that our proposed model has higher audio
quality than the baseline model and generates more realistic timbre sounds than
the previous leading work.</div><div><a href='http://arxiv.org/abs/2401.13498v1'>2401.13498v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04558v1")'>HyperGANStrument: Instrument Sound Synthesis and Editing with
  Pitch-Invariant Hypernetworks</div>
<div id='2401.04558v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T13:54:32Z</div><div>Authors: Zhe Zhang, Taketo Akama</div><div style='padding-top: 10px; width: 80ex'>GANStrument, exploiting GANs with a pitch-invariant feature extractor and
instance conditioning technique, has shown remarkable capabilities in
synthesizing realistic instrument sounds. To further improve the reconstruction
ability and pitch accuracy to enhance the editability of user-provided sound,
we propose HyperGANStrument, which introduces a pitch-invariant hypernetwork to
modulate the weights of a pre-trained GANStrument generator, given a one-shot
sound as input. The hypernetwork modulation provides feedback for the generator
in the reconstruction of the input sound. In addition, we take advantage of an
adversarial fine-tuning scheme for the hypernetwork to improve the
reconstruction fidelity and generation diversity of the generator. Experimental
results show that the proposed model not only enhances the generation
capability of GANStrument but also significantly improves the editability of
synthesized sounds. Audio examples are available at the online demo page.</div><div><a href='http://arxiv.org/abs/2401.04558v1'>2401.04558v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04577v2")'>Masked Audio Generation using a Single Non-Autoregressive Transformer</div>
<div id='2401.04577v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T14:29:39Z</div><div>Authors: Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi</div><div style='padding-top: 10px; width: 80ex'>We introduce MAGNeT, a masked generative sequence modeling method that
operates directly over several streams of audio tokens. Unlike prior work,
MAGNeT is comprised of a single-stage, non-autoregressive transformer. During
training, we predict spans of masked tokens obtained from a masking scheduler,
while during inference we gradually construct the output sequence using several
decoding steps. To further enhance the quality of the generated audio, we
introduce a novel rescoring method in which, we leverage an external
pre-trained model to rescore and rank predictions from MAGNeT, which will be
then used for later decoding steps. Lastly, we explore a hybrid version of
MAGNeT, in which we fuse between autoregressive and non-autoregressive models
to generate the first few seconds in an autoregressive manner while the rest of
the sequence is being decoded in parallel. We demonstrate the efficiency of
MAGNeT for the task of text-to-music and text-to-audio generation and conduct
an extensive empirical evaluation, considering both objective metrics and human
studies. The proposed approach is comparable to the evaluated baselines, while
being significantly faster (x7 faster than the autoregressive baseline).
Through ablation studies and analysis, we shed light on the importance of each
of the components comprising MAGNeT, together with pointing to the trade-offs
between autoregressive and non-autoregressive modeling, considering latency,
throughput, and generation quality. Samples are available on our demo page
https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.</div><div><a href='http://arxiv.org/abs/2401.04577v2'>2401.04577v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01099v1")'>Efficient Parallel Audio Generation using Group Masked Language Modeling</div>
<div id='2401.01099v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T08:42:48Z</div><div>Authors: Myeonghun Jeong, Minchan Kim, Joun Yeop Lee, Nam Soo Kim</div><div style='padding-top: 10px; width: 80ex'>We present a fast and high-quality codec language model for parallel audio
generation. While SoundStorm, a state-of-the-art parallel audio generation
model, accelerates inference speed compared to autoregressive models, it still
suffers from slow inference due to iterative sampling. To resolve this problem,
we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel
Decoding~(G-IPD) for efficient parallel audio generation. Both the training and
sampling schemes enable the model to synthesize high-quality audio with a small
number of iterations by effectively modeling the group-wise conditional
dependencies. In addition, our model employs a cross-attention-based
architecture to capture the speaker style of the prompt voice and improves
computational efficiency. Experimental results demonstrate that our proposed
model outperforms the baselines in prompt-based audio generation.</div><div><a href='http://arxiv.org/abs/2401.01099v1'>2401.01099v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09821v1")'>Diffusion Models for Audio Restoration</div>
<div id='2402.09821v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T09:36:36Z</div><div>Authors: Jean-Marie Lemercier, Julius Richter, Simon Welker, Eloi Moliner, Vesa Välimäki, Timo Gerkmann</div><div style='padding-top: 10px; width: 80ex'>With the development of audio playback devices and fast data transmission,
the demand for high sound quality is rising, for both entertainment and
communications. In this quest for better sound quality, challenges emerge from
distortions and interferences originating at the recording side or caused by an
imperfect transmission pipeline. To address this problem, audio restoration
methods aim to recover clean sound signals from the corrupted input data. We
present here audio restoration algorithms based on diffusion models, with a
focus on speech enhancement and music restoration tasks. Traditional
approaches, often grounded in handcrafted rules and statistical heuristics,
have shaped our understanding of audio signals. In the past decades, there has
been a notable shift towards data-driven methods that exploit the modeling
capabilities of deep neural networks (DNNs). Deep generative models, and among
them diffusion models, have emerged as powerful techniques for learning complex
data distributions. However, relying solely on DNN-based learning approaches
carries the risk of reducing interpretability, particularly when employing
end-to-end models. Nonetheless, data-driven approaches allow more flexibility
in comparison to statistical model-based frameworks whose performance depends
on distributional and statistical assumptions that can be difficult to
guarantee. Here, we aim to show that diffusion models can combine the best of
both worlds and offer the opportunity to design audio restoration algorithms
with a good degree of interpretability and a remarkable performance in terms of
sound quality.</div><div><a href='http://arxiv.org/abs/2402.09821v1'>2402.09821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04800v1")'>(Un)paired signal-to-signal translation with 1D conditional GANs</div>
<div id='2403.04800v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:52:50Z</div><div>Authors: Eric Easthope</div><div style='padding-top: 10px; width: 80ex'>I show that a one-dimensional (1D) conditional generative adversarial network
(cGAN) with an adversarial training architecture is capable of unpaired
signal-to-signal ("sig2sig") translation. Using a simplified CycleGAN model
with 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe
two-dimensional (2D) image generation as 1D audio generation, I show that
recasting the 2D image-to-image translation task to a 1D signal-to-signal
translation task with deep convolutional GANs is possible without substantial
modification to the conventional U-Net model and adversarial architecture
developed as CycleGAN. With this I show for a small tunable dataset that noisy
test signals unseen by the 1D CycleGAN model and without paired training
transform from the source domain to signals similar to paired test signals in
the translated domain, especially in terms of frequency, and I quantify these
differences in terms of correlation and error.</div><div><a href='http://arxiv.org/abs/2403.04800v1'>2403.04800v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10009v3")'>Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion</div>
<div id='2402.10009v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T15:17:26Z</div><div>Authors: Hila Manor, Tomer Michaeli</div><div style='padding-top: 10px; width: 80ex'>Editing signals using large pre-trained models, in a zero-shot manner, has
recently seen rapid advancements in the image domain. However, this wave has
yet to reach the audio domain. In this paper, we explore two zero-shot editing
techniques for audio signals, which use DDPM inversion on pre-trained diffusion
models. The first, adopted from the image domain, allows text-based editing.
The second, is a novel approach for discovering semantically meaningful editing
directions without supervision. When applied to music signals, this method
exposes a range of musically interesting modifications, from controlling the
participation of specific instruments to improvisations on the melody. Samples
and code can be found on our examples page in
https://hilamanor.github.io/AudioEditing/ .</div><div><a href='http://arxiv.org/abs/2402.10009v3'>2402.10009v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12423v1")'>On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models</div>
<div id='2402.12423v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T16:22:21Z</div><div>Authors: Miri Varshavsky-Hassid, Roy Hirsch, Regev Cohen, Tomer Golany, Daniel Freedman, Ehud Rivlin</div><div style='padding-top: 10px; width: 80ex'>The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech
(TTS) domain is rising, providing great value in synthesizing high quality
speech. Although they exhibit impressive audio quality, the extent of their
semantic capabilities is unknown, and controlling their synthesized speech's
vocal properties remains a challenge. Inspired by recent advances in image
synthesis, we explore the latent space of frozen TTS models, which is composed
of the latent bottleneck activations of the DDM's denoiser. We identify that
this space contains rich semantic information, and outline several novel
methods for finding semantic directions within it, both supervised and
unsupervised. We then demonstrate how these enable off-the-shelf audio editing,
without any further training, architectural changes or data requirements. We
present evidence of the semantic and acoustic qualities of the edited audio,
and provide supplemental samples:
https://latent-analysis-grad-tts.github.io/speech-samples/.</div><div><a href='http://arxiv.org/abs/2402.12423v1'>2402.12423v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04804v1")'>AttentionStitch: How Attention Solves the Speech Editing Problem</div>
<div id='2403.04804v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T22:09:58Z</div><div>Authors: Antonios Alexos, Pierre Baldi</div><div style='padding-top: 10px; width: 80ex'>The generation of natural and high-quality speech from text is a challenging
problem in the field of natural language processing. In addition to speech
generation, speech editing is also a crucial task, which requires the seamless
and unnoticeable integration of edited speech into synthesized speech. We
propose a novel approach to speech editing by leveraging a pre-trained
text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double
attention block network on top of it to automatically merge the synthesized
mel-spectrogram with the mel-spectrogram of the edited text. We refer to this
model as AttentionStitch, as it harnesses attention to stitch audio samples
together. We evaluate the proposed AttentionStitch model against
state-of-the-art baselines on both single and multi-speaker datasets, namely
LJSpeech and VCTK. We demonstrate its superior performance through an objective
and a subjective evaluation test involving 15 human participants.
AttentionStitch is capable of producing high-quality speech, even for words not
seen during training, while operating automatically without the need for human
intervention. Moreover, AttentionStitch is fast during both training and
inference and is able to generate human-sounding edited speech.</div><div><a href='http://arxiv.org/abs/2403.04804v1'>2403.04804v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01831v2")'>Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and
  Dialogue Abilities</div>
<div id='2402.01831v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:58:34Z</div><div>Authors: Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, Bryan Catanzaro</div><div style='padding-top: 10px; width: 80ex'>Augmenting large language models (LLMs) to understand audio -- including
non-speech sounds and non-verbal speech -- is critically important for diverse
real-world applications of LLMs. In this paper, we propose Audio Flamingo, a
novel audio language model with 1) strong audio understanding abilities, 2) the
ability to quickly adapt to unseen tasks via in-context learning and retrieval,
and 3) strong multi-turn dialogue abilities. We introduce a series of training
techniques, architecture design, and data strategies to enhance our model with
these abilities. Extensive evaluations across various audio understanding tasks
confirm the efficacy of our method, setting new state-of-the-art benchmarks.
Our demo website is: \url{https://audioflamingo.github.io/}.</div><div><a href='http://arxiv.org/abs/2402.01831v2'>2402.01831v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07729v1")'>AIR-Bench: Benchmarking Large Audio-Language Models via Generative
  Comprehension</div>
<div id='2402.07729v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:41:22Z</div><div>Authors: Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, Jingren Zhou</div><div style='padding-top: 10px; width: 80ex'>Recently, instruction-following audio-language models have received broad
attention for human-audio interaction. However, the absence of benchmarks
capable of evaluating audio-centric interaction capabilities has impeded
advancements in this field. Previous models primarily focus on assessing
different fundamental tasks, such as Automatic Speech Recognition (ASR), and
lack an assessment of the open-ended generative capabilities centered around
audio. Thus, it is challenging to track the progression in the Large
Audio-Language Models (LALMs) domain and to provide guidance for future
improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio
\textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed
to evaluate the ability of LALMs to understand various types of audio signals
(including human speech, natural sounds, and music), and furthermore, to
interact with humans in the textual format. AIR-Bench encompasses two
dimensions: \textit{foundation} and \textit{chat} benchmarks. The former
consists of 19 tasks with approximately 19k single-choice questions, intending
to inspect the basic single-task ability of LALMs. The latter one contains 2k
instances of open-ended question-and-answer data, directly assessing the
comprehension of the model on complex audio and its capacity to follow
instructions. Both benchmarks require the model to generate hypotheses
directly. We design a unified framework that leverages advanced language
models, such as GPT-4, to evaluate the scores of generated hypotheses given the
meta-information of the audio. Experimental results demonstrate a high level of
consistency between GPT-4-based evaluation and human evaluation. By revealing
the limitations of existing LALMs through evaluation results, AIR-Bench can
provide insights into the direction of future research.</div><div><a href='http://arxiv.org/abs/2402.07729v1'>2402.07729v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12226v3")'>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</div>
<div id='2402.12226v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:33:10Z</div><div>Authors: Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu</div><div style='padding-top: 10px; width: 80ex'>We introduce AnyGPT, an any-to-any multimodal language model that utilizes
discrete representations for the unified processing of various modalities,
including speech, text, images, and music. AnyGPT can be trained stably without
any alterations to the current large language model (LLM) architecture or
training paradigms. Instead, it relies exclusively on data-level preprocessing,
facilitating the seamless integration of new modalities into LLMs, akin to the
incorporation of new languages. We build a multimodal text-centric dataset for
multimodal alignment pre-training. Utilizing generative models, we synthesize
the first large-scale any-to-any multimodal instruction dataset. It consists of
108k samples of multi-turn conversations that intricately interweave various
modalities, thus equipping the model to handle arbitrary combinations of
multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is
capable of facilitating any-to-any multimodal conversation while achieving
performance comparable to specialized models across all modalities, proving
that discrete representations can effectively and conveniently unify multiple
modalities within a language model. Demos are shown in
https://junzhan2000.github.io/AnyGPT.github.io/</div><div><a href='http://arxiv.org/abs/2402.12226v3'>2402.12226v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02417v1")'>Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic
  Speech Recognition</div>
<div id='2401.02417v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T18:59:31Z</div><div>Authors: David M. Chan, Shalini Ghosh, Hitesh Tulsiani, Ariya Rastrow, Björn Hoffmeister</div><div style='padding-top: 10px; width: 80ex'>While word error rates of automatic speech recognition (ASR) systems have
consistently fallen, natural language understanding (NLU) applications built on
top of ASR systems still attribute significant numbers of failures to
low-quality speech recognition results. Existing assistant systems collect
large numbers of these unsuccessful interactions, but these systems usually
fail to learn from these interactions, even in an offline fashion. In this
work, we introduce CLC: Contrastive Learning for Conversations, a family of
methods for contrastive fine-tuning of models in a self-supervised fashion,
making use of easily detectable artifacts in unsuccessful conversations with
assistants. We demonstrate that our CLC family of approaches can improve the
performance of ASR models on OD3, a new public large-scale semi-synthetic
meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains
transfer to real-world systems as well, where we show that CLC can help to
improve performance by up to 6.7% over baselines. We make OD3 publicly
available at https://github.com/amazon-science/amazon-od3 .</div><div><a href='http://arxiv.org/abs/2401.02417v1'>2401.02417v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14717v1")'>Turn-taking and Backchannel Prediction with Acoustic and Large Language
  Model Fusion</div>
<div id='2401.14717v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T08:59:07Z</div><div>Authors: Jinhan Wang, Long Chen, Aparna Khare, Anirudh Raju, Pranav Dheram, Di He, Minhua Wu, Andreas Stolcke, Venkatesh Ravichandran</div><div style='padding-top: 10px; width: 80ex'>We propose an approach for continuous prediction of turn-taking and
backchanneling locations in spoken dialogue by fusing a neural acoustic model
with a large language model (LLM). Experiments on the Switchboard human-human
conversation dataset demonstrate that our approach consistently outperforms the
baseline models with single modality. We also develop a novel multi-task
instruction fine-tuning strategy to further benefit from LLM-encoded knowledge
for understanding the tasks and conversational contexts, leading to additional
improvements. Our approach demonstrates the potential of combined LLMs and
acoustic models for a more natural and conversational interaction between
humans and speech-enabled AI agents.</div><div><a href='http://arxiv.org/abs/2401.14717v1'>2401.14717v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01274v3")'>On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio
  Classification</div>
<div id='2402.01274v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:00:51Z</div><div>Authors: Calum Heggan, Sam Budgett, Timothy Hospedales, Mehrdad Yaghoobi</div><div style='padding-top: 10px; width: 80ex'>In recent years, self-supervised learning has excelled for its capacity to
learn robust feature representations from unlabelled data. Networks pretrained
through self-supervision serve as effective feature extractors for downstream
tasks, including Few-Shot Learning. While the evaluation of unsupervised
approaches for few-shot learning is well-established in imagery, it is notably
absent in acoustics. This study addresses this gap by assessing large-scale
self-supervised models' performance in few-shot audio classification.
Additionally, we explore the relationship between a model's few-shot learning
capability and other downstream task benchmarks. Our findings reveal
state-of-the-art performance in some few-shot problems such as
SpeechCommandsv2, as well as strong correlations between speech-based few-shot
problems and various downstream audio tasks.</div><div><a href='http://arxiv.org/abs/2402.01274v3'>2402.01274v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03497v1")'>EAT: Self-Supervised Pre-Training with Efficient Audio Transformer</div>
<div id='2401.03497v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T14:31:27Z</div><div>Authors: Wenxi Chen, Yuzhe Liang, Ziyang Ma, Zhisheng Zheng, Xie Chen</div><div style='padding-top: 10px; width: 80ex'>Audio self-supervised learning (SSL) pre-training, which aims to learn good
representations from unlabeled audio, has made remarkable progress. However,
the extensive computational demands during pre-training pose a significant
barrier to the potential application and optimization of audio SSL models. In
this paper, inspired by the success of data2vec 2.0 in image modality and
Audio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to
further improve the effectiveness and efficiency in audio SSL. The proposed EAT
adopts the bootstrap self-supervised training paradigm to the audio domain. A
novel Utterance-Frame Objective (UFO) is designed to enhance the modeling
capability of acoustic events. Furthermore, we reveal that the masking strategy
is critical in audio SSL pre-training, and superior audio representations can
be obtained with large inverse block masks. Experiment results demonstrate that
EAT achieves state-of-the-art (SOTA) performance on a range of audio-related
tasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a
significant pre-training speedup up to ~15x compared to existing audio SSL
models.</div><div><a href='http://arxiv.org/abs/2401.03497v1'>2401.03497v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02889v1")'>Exploring Federated Self-Supervised Learning for General Purpose Audio
  Understanding</div>
<div id='2402.02889v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:57:48Z</div><div>Authors: Yasar Abbas Ur Rehman, Kin Wai Lau, Yuyang Xie, Lan Ma, Jiajun Shen</div><div style='padding-top: 10px; width: 80ex'>The integration of Federated Learning (FL) and Self-supervised Learning (SSL)
offers a unique and synergetic combination to exploit the audio data for
general-purpose audio understanding, without compromising user data privacy.
However, rare efforts have been made to investigate the SSL models in the FL
regime for general-purpose audio understanding, especially when the training
data is generated by large-scale heterogeneous audio sources. In this paper, we
evaluate the performance of feature-matching and predictive audio-SSL
techniques when integrated into large-scale FL settings simulated with
non-independently identically distributed (non-iid) data. We propose a novel
Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning
intermediate feature representations from large-scale decentralized
heterogeneous clients, holding unlabelled audio data. Our study has found that
audio F-SSL approaches perform on par with the centralized audio-SSL approaches
on the audio-retrieval task. Extensive experiments demonstrate the
effectiveness and significance of FASSL as it assists in obtaining the optimal
global model for state-of-the-art FL aggregation methods.</div><div><a href='http://arxiv.org/abs/2402.02889v1'>2402.02889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08415v1")'>From Coarse to Fine: Efficient Training for Audio Spectrogram
  Transformers</div>
<div id='2401.08415v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:59:37Z</div><div>Authors: Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, Arda Senocak</div><div style='padding-top: 10px; width: 80ex'>Transformers have become central to recent advances in audio classification.
However, training an audio spectrogram transformer, e.g. AST, from scratch can
be resource and time-intensive. Furthermore, the complexity of transformers
heavily depends on the input audio spectrogram size. In this work, we aim to
optimize AST training by linking to the resolution in the time-axis. We
introduce multi-phase training of audio spectrogram transformers by connecting
the seminal idea of coarse-to-fine with transformer models. To achieve this, we
propose a set of methods for temporal compression. By employing one of these
methods, the transformer model learns from lower-resolution (coarse) data in
the initial phases, and then is fine-tuned with high-resolution data later in a
curriculum learning strategy. Experimental results demonstrate that the
proposed training mechanism for AST leads to improved (or on-par) performance
with faster convergence, i.e. requiring fewer computational resources and less
time. This approach is also generalizable to other AST-based methods regardless
of their learning paradigms.</div><div><a href='http://arxiv.org/abs/2401.08415v1'>2401.08415v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08268v2")'>An Explainable Proxy Model for Multiabel Audio Segmentation</div>
<div id='2401.08268v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T10:41:33Z</div><div>Authors: Théo Mariotte, Antonio Almudévar, Marie Tahon, Alfonso Ortega</div><div style='padding-top: 10px; width: 80ex'>Audio signal segmentation is a key task for automatic audio indexing. It
consists of detecting the boundaries of class-homogeneous segments in the
signal. In many applications, explainable AI is a vital process for
transparency of decision-making with machine learning. In this paper, we
propose an explainable multilabel segmentation model that solves speech
activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD)
simultaneously. This proxy uses the non-negative matrix factorization (NMF) to
map the embedding used for the segmentation to the frequency domain.
Experiments conducted on two datasets show similar performances as the
pre-trained black box model while showing strong explainability features.
Specifically, the frequency bins used for the decision can be easily identified
at both the segment level (local explanations) and global level (class
prototypes).</div><div><a href='http://arxiv.org/abs/2401.08268v2'>2401.08268v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03145v1")'>Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for
  Audio-Visual Source Localization</div>
<div id='2403.03145v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T17:35:46Z</div><div>Authors: Yuxin Guo, Shijie Ma, Hu Su, Zhiqing Wang, Yuhao Zhao, Wei Zou, Siyang Sun, Yun Zheng</div><div style='padding-top: 10px; width: 80ex'>Audio-Visual Source Localization (AVSL) aims to locate sounding objects
within video frames given the paired audio clips. Existing methods
predominantly rely on self-supervised contrastive learning of audio-visual
correspondence. Without any bounding-box annotations, they struggle to achieve
precise localization, especially for small objects, and suffer from blurry
boundaries and false positives. Moreover, the naive semi-supervised method is
poor in fully leveraging the information of abundant unlabeled data. In this
paper, we propose a novel semi-supervised learning framework for AVSL, namely
Dual Mean-Teacher (DMT), comprising two teacher-student structures to
circumvent the confirmation bias issue. Specifically, two teachers, pre-trained
on limited labeled data, are employed to filter out noisy samples via the
consensus between their predictions, and then generate high-quality
pseudo-labels by intersecting their confidence maps. The sufficient utilization
of both labeled and unlabeled data and the proposed unbiased framework enable
DMT to outperform current state-of-the-art methods by a large margin, with CIoU
of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%,
9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods
respectively, given only 3% positional-annotations. We also extend our
framework to some existing AVSL methods and consistently boost their
performance.</div><div><a href='http://arxiv.org/abs/2403.03145v1'>2403.03145v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09579v1")'>uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with
  Unsupervised Audio Mixtures</div>
<div id='2403.09579v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:13:37Z</div><div>Authors: Afrina Tabassum, Dung Tran, Trung Dang, Ismini Lourentzou, Kazuhito Koishida</div><div style='padding-top: 10px; width: 80ex'>Masked Autoencoders (MAEs) learn rich low-level representations from
unlabeled data but require substantial labeled data to effectively adapt to
downstream tasks. Conversely, Instance Discrimination (ID) emphasizes
high-level semantics, offering a potential solution to alleviate annotation
requirements in MAEs. Although combining these two approaches can address
downstream tasks with limited labeled data, naively integrating ID into MAEs
leads to extended training times and high computational costs. To address this
challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that
leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE
aligns the representations of pretrained MAEs, thereby facilitating effective
adaptation to task-specific semantics. To optimize the model with small amounts
of unlabeled data, we propose an audio mixing technique that manipulates audio
samples in both input and virtual label spaces. Experiments in low/few-shot
settings demonstrate that \modelname achieves 4-6% accuracy improvements over
various benchmarks when tuned with limited unlabeled data, such as
AudioSet-20K. Code is available at https://github.com/PLAN-Lab/uamix-MAE</div><div><a href='http://arxiv.org/abs/2403.09579v1'>2403.09579v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07383v2")'>Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like</div>
<div id='2402.07383v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T02:58:10Z</div><div>Authors: Naoyuki Kanda, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker, Hemin Yang, Zirun Zhu, Min Tang, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Yufei Xia, Jinzhu Li, Yanqing Liu, Sheng Zhao, Michael Zeng</div><div style='padding-top: 10px; width: 80ex'>Laughter is one of the most expressive and natural aspects of human speech,
conveying emotions, social cues, and humor. However, most text-to-speech (TTS)
systems lack the ability to produce realistic and appropriate laughter sounds,
limiting their applications and user experience. While there have been prior
works to generate natural laughter, they fell short in terms of controlling the
timing and variety of the laughter to be generated. In this work, we propose
ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker
based on a short audio prompt with precise control of laughter timing and
expression. Specifically, ELaTE works on the audio prompt to mimic the voice
characteristic, the text prompt to indicate the contents of the generated
speech, and the input to control the laughter expression, which can be either
the start and end times of laughter, or the additional audio prompt that
contains laughter to be mimicked. We develop our model based on the foundation
of conditional flow-matching-based zero-shot TTS, and fine-tune it with
frame-level representation from a laughter detector as additional conditioning.
With a simple scheme to mix small-scale laughter-conditioned data with
large-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS
model can be readily fine-tuned to generate natural laughter with precise
controllability, without losing any quality of the pre-trained zero-shot TTS
model. Through objective and subjective evaluations, we show that ELaTE can
generate laughing speech with significantly higher quality and controllability
compared to conventional models. See https://aka.ms/elate/ for demo samples.</div><div><a href='http://arxiv.org/abs/2402.07383v2'>2402.07383v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04511v1")'>Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement</div>
<div id='2401.04511v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T12:10:04Z</div><div>Authors: Soumya Dutta, Sriram Ganapathy</div><div style='padding-top: 10px; width: 80ex'>The problem of audio-to-audio (A2A) style transfer involves replacing the
style features of the source audio with those from the target audio while
preserving the content related attributes of the source audio. In this paper,
we propose an efficient approach, termed as Zero-shot Emotion Style Transfer
(ZEST), that allows the transfer of emotional content present in the given
source audio with the one embedded in the target audio while retaining the
speaker and speech content from the source. The proposed system builds upon
decomposing speech into semantic tokens, speaker representations and emotion
embeddings. Using these factors, we propose a framework to reconstruct the
pitch contour of the given speech signal and train a decoder that reconstructs
the speech signal. The model is trained using a self-supervision based
reconstruction loss. During conversion, the emotion embedding is alone derived
from the target audio, while rest of the factors are derived from the source
audio. In our experiments, we show that, even without using parallel training
data or labels from the source or target audio, we illustrate zero shot emotion
transfer capabilities of the proposed ZEST model using objective and subjective
quality evaluations.</div><div><a href='http://arxiv.org/abs/2401.04511v1'>2401.04511v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10488v1")'>Joint Multimodal Transformer for Dimensional Emotional Recognition in
  the Wild</div>
<div id='2403.10488v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:23:38Z</div><div>Authors: Paul Waligora, Osama Zeeshan, Haseeb Aslam, Soufiane Belharbi, Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger</div><div style='padding-top: 10px; width: 80ex'>Audiovisual emotion recognition (ER) in videos has immense potential over
unimodal performance. It effectively leverages the inter- and intra-modal
dependencies between visual and auditory modalities. This work proposes a novel
audio-visual emotion recognition system utilizing a joint multimodal
transformer architecture with key-based cross-attention. This framework aims to
exploit the complementary nature of audio and visual cues (facial expressions
and vocal patterns) in videos, leading to superior performance compared to
solely relying on a single modality. The proposed model leverages separate
backbones for capturing intra-modal temporal dependencies within each modality
(audio and visual). Subsequently, a joint multimodal transformer architecture
integrates the individual modality embeddings, enabling the model to
effectively capture inter-modal (between audio and visual) and intra-modal
(within each modality) relationships. Extensive evaluations on the challenging
Affwild2 dataset demonstrate that the proposed model significantly outperforms
baseline and state-of-the-art methods in ER tasks.</div><div><a href='http://arxiv.org/abs/2403.10488v1'>2403.10488v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12609v1")'>SUN Team's Contribution to ABAW 2024 Competition: Audio-visual
  Valence-Arousal Estimation and Expression Recognition</div>
<div id='2403.12609v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T10:24:15Z</div><div>Authors: Denis Dresvyanskiy, Maxim Markitantov, Jiawei Yu, Peitong Li, Heysem Kaya, Alexey Karpov</div><div style='padding-top: 10px; width: 80ex'>As emotions play a central role in human communication, automatic emotion
recognition has attracted increasing attention in the last two decades. While
multimodal systems enjoy high performances on lab-controlled data, they are
still far from providing ecological validity on non-lab-controlled, namely
'in-the-wild' data. This work investigates audiovisual deep learning approaches
for emotion recognition in-the-wild problem. We particularly explore the
effectiveness of architectures based on fine-tuned Convolutional Neural
Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio
modality, respectively. We compare alternative temporal modeling and fusion
strategies using the embeddings from these multi-stage trained
modality-specific Deep Neural Networks (DNN). We report results on the AffWild2
dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge
protocol.</div><div><a href='http://arxiv.org/abs/2403.12609v1'>2403.12609v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11757v2")'>Efficient Feature Extraction and Late Fusion Strategy for Audiovisual
  Emotional Mimicry Intensity Estimation</div>
<div id='2403.11757v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:11:10Z</div><div>Authors: Jun Yu, Wangyuan Zhu, Jichao Zhu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present the solution to the Emotional Mimicry Intensity
(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis
in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to
evaluate the emotional intensity of seed videos by assessing them from a set of
predefined emotion categories (i.e., "Admiration", "Amusement",
"Determination", "Empathic Pain", "Excitement" and "Joy"). To tackle this
challenge, we extracted rich dual-channel visual features based on ResNet18 and
AUs for the video modality and effective single-channel features based on
Wav2Vec2.0 for the audio modality. This allowed us to obtain comprehensive
emotional features for the audiovisual modality. Additionally, leveraging a
late fusion strategy, we averaged the predictions of the visual and acoustic
models, resulting in a more accurate estimation of audiovisual emotional
mimicry intensity. Experimental results validate the effectiveness of our
approach, with the average Pearson's correlation Coefficient($\rho$) across the
6 emotion dimensionson the validation set achieving 0.3288.</div><div><a href='http://arxiv.org/abs/2403.11757v2'>2403.11757v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12687v1")'>Audio-Visual Compound Expression Recognition Method based on Late
  Modality Fusion and Rule-based Decision</div>
<div id='2403.12687v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T12:45:52Z</div><div>Authors: Elena Ryumina, Maxim Markitantov, Dmitry Ryumin, Heysem Kaya, Alexey Karpov</div><div style='padding-top: 10px; width: 80ex'>This paper presents the results of the SUN team for the Compound Expressions
Recognition Challenge of the 6th ABAW Competition. We propose a novel
audio-visual method for compound expression recognition. Our method relies on
emotion recognition models that fuse modalities at the emotion probability
level, while decisions regarding the prediction of compound expressions are
based on predefined rules. Notably, our method does not use any training data
specific to the target task. The method is evaluated in multi-corpus training
and cross-corpus validation setups. Our findings from the challenge demonstrate
that the proposed method can potentially form a basis for development of
intelligent tools for annotating audio-visual data in the context of human's
basic and compound emotions. The source code is publicly available.</div><div><a href='http://arxiv.org/abs/2403.12687v1'>2403.12687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14083v1")'>emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network
  Architectures for Superior Speech Emotion Recognition</div>
<div id='2403.14083v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T02:26:30Z</div><div>Authors: Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Berrak Sisman, Bjorn W. Schuller, Carlos Busso</div><div style='padding-top: 10px; width: 80ex'>Speech Emotion Recognition (SER) is crucial for enabling computers to
understand the emotions conveyed in human communication. With recent
advancements in Deep Learning (DL), the performance of SER models has
significantly improved. However, designing an optimal DL architecture requires
specialised knowledge and experimental assessments. Fortunately, Neural
Architecture Search (NAS) provides a potential solution for automatically
determining the best DL model. The Differentiable Architecture Search (DARTS)
is a particularly efficient method for discovering optimal models. This study
presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network
(SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature
supports the selection of CNN and LSTM coupling to improve performance.
  While DARTS has previously been used to choose CNN and LSTM operations
independently, our technique adds a novel mechanism for selecting CNN and SeqNN
operations in conjunction using DARTS. Unlike earlier work, we do not impose
limits on the layer order of the CNN. Instead, we let DARTS choose the best
layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms
conventionally designed CNN-LSTM models and surpasses the best-reported SER
results achieved through DARTS on CNN-LSTM by evaluating our approach on the
IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.</div><div><a href='http://arxiv.org/abs/2403.14083v1'>2403.14083v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11747v1")'>Parameter Efficient Finetuning for Speech Emotion Recognition and Domain
  Adaptation</div>
<div id='2402.11747v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T00:21:07Z</div><div>Authors: Nineli Lashkarashvili, Wen Wu, Guangzhi Sun, Philip C. Woodland</div><div style='padding-top: 10px; width: 80ex'>Foundation models have shown superior performance for speech emotion
recognition (SER). However, given the limited data in emotion corpora,
finetuning all parameters of large pre-trained models for SER can be both
resource-intensive and susceptible to overfitting. This paper investigates
parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are
systematically studied for both classification of discrete emotion categories
and prediction of dimensional emotional attributes. The results demonstrate
that the combination of PEFT methods surpasses full finetuning with a
significant reduction in the number of trainable parameters. Furthermore, a
two-stage adaptation strategy is proposed to adapt models trained on acted
emotion data, which is more readily available, to make the model more adept at
capturing natural emotional expressions. Both intra- and cross-corpus
experiments validate the efficacy of the proposed approach in enhancing the
performance on both the source and target domains.</div><div><a href='http://arxiv.org/abs/2402.11747v1'>2402.11747v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00887v1")'>SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in
  Speech</div>
<div id='2403.00887v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T11:28:37Z</div><div>Authors: Aron R, Indra Sigicharla, Chirag Periwal, Mohanaprasad K, Nithya Darisini P S, Sourabh Tiwari, Shivani Arora</div><div style='padding-top: 10px; width: 80ex'>The interpretation of human voices holds importance across various
applications. This study ventures into predicting age, gender, and emotion from
vocal cues, a field with vast applications. Voice analysis tech advancements
span domains, from improving customer interactions to enhancing healthcare and
retail experiences. Discerning emotions aids mental health, while age and
gender detection are vital in various contexts. Exploring deep learning models
for these predictions involves comparing single, multi-output, and sequential
models highlighted in this paper. Sourcing suitable data posed challenges,
resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work
showed promise in individual predictions, but limited research considered all
three variables simultaneously. This paper identifies flaws in an individual
model approach and advocates for our novel multi-output learning architecture
Speech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments
suggest that Multi-output models perform comparably to individual models,
efficiently capturing the intricate relationships between variables and speech
inputs, all while achieving improved runtime.</div><div><a href='http://arxiv.org/abs/2403.00887v1'>2403.00887v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02184v1")'>Sentiment analysis in non-fixed length audios using a Fully
  Convolutional Neural Network</div>
<div id='2402.02184v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T15:26:28Z</div><div>Authors: María Teresa García-Ordás, Héctor Alaiz-Moretón, José Alberto Benítez-Andrades, Isaías García-Rodríguez, Oscar García-Olalla, Carmen Benavides</div><div style='padding-top: 10px; width: 80ex'>In this work, a sentiment analysis method that is capable of accepting audio
of any length, without being fixed a priori, is proposed. Mel spectrogram and
Mel Frequency Cepstral Coefficients are used as audio description methods and a
Fully Convolutional Neural Network architecture is proposed as a classifier.
The results have been validated using three well known datasets: EMODB,
RAVDESS, and TESS. The results obtained were promising, outperforming the
state-of-the-art methods. Also, thanks to the fact that the proposed method
admits audios of any size, it allows a sentiment analysis to be made in near
real time, which is very interesting for a wide range of fields such as call
centers, medical consultations, or financial brokers.</div><div><a href='http://arxiv.org/abs/2402.02184v1'>2402.02184v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14050v1")'>Extracting Emotion Phrases from Tweets using BART</div>
<div id='2403.14050v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T00:20:16Z</div><div>Authors: Mahdi Rezapour</div><div style='padding-top: 10px; width: 80ex'>Sentiment analysis is a natural language processing task that aims to
identify and extract the emotional aspects of a text. However, many existing
sentiment analysis methods primarily classify the overall polarity of a text,
overlooking the specific phrases that convey sentiment. In this paper, we
applied an approach to sentiment analysis based on a question-answering
framework. Our approach leverages the power of Bidirectional Autoregressive
Transformer (BART), a pre-trained sequence-to-sequence model, to extract a
phrase from a given text that amplifies a given sentiment polarity. We create a
natural language question that identifies the specific emotion to extract and
then guide BART to pay attention to the relevant emotional cues in the text. We
use a classifier within BART to predict the start and end positions of the
answer span within the text, which helps to identify the precise boundaries of
the extracted emotion phrase. Our approach offers several advantages over most
sentiment analysis studies, including capturing the complete context and
meaning of the text and extracting precise token spans that highlight the
intended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.</div><div><a href='http://arxiv.org/abs/2403.14050v1'>2403.14050v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10535v1")'>The "Colonial Impulse" of Natural Language Processing: An Audit of
  Bengali Sentiment Analysis Tools and Their Identity-based Biases</div>
<div id='2401.10535v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T07:21:45Z</div><div>Authors: Dipto Das, Shion Guha, Jed Brubaker, Bryan Semaan</div><div style='padding-top: 10px; width: 80ex'>While colonization has sociohistorically impacted people's identities across
various dimensions, those colonial values and biases continue to be perpetuated
by sociotechnical systems. One category of sociotechnical systems--sentiment
analysis tools--can also perpetuate colonial values and bias, yet less
attention has been paid to how such tools may be complicit in perpetuating
coloniality, although they are often used to guide various practices (e.g.,
content moderation). In this paper, we explore potential bias in sentiment
analysis tools in the context of Bengali communities that have experienced and
continue to experience the impacts of colonialism. Drawing on identity
categories most impacted by colonialism amongst local Bengali communities, we
focused our analytic attention on gender, religion, and nationality. We
conducted an algorithmic audit of all sentiment analysis tools for Bengali,
available on the Python package index (PyPI) and GitHub. Despite similar
semantic content and structure, our analyses showed that in addition to
inconsistencies in output from different tools, Bengali sentiment analysis
tools exhibit bias between different identity categories and respond
differently to different ways of identity expression. Connecting our findings
with colonially shaped sociocultural structures of Bengali communities, we
discuss the implications of downstream bias of sentiment analysis tools.</div><div><a href='http://arxiv.org/abs/2401.10535v1'>2401.10535v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12382v1")'>Longitudinal Sentiment Classification of Reddit Posts</div>
<div id='2401.12382v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T22:16:55Z</div><div>Authors: Fabian Nwaoha, Ziyad Gaffar, Ho Joon Chun, Marina Sokolova</div><div style='padding-top: 10px; width: 80ex'>We report results of a longitudinal sentiment classification of Reddit posts
written by students of four major Canadian universities. We work with the texts
of the posts, concentrating on the years 2020-2023. By finely tuning a
sentiment threshold to a range of [-0.075,0.075], we successfully built
classifiers proficient in categorizing post sentiments into positive and
negative categories. Noticeably, our sentiment classification results are
consistent across the four university data sets.</div><div><a href='http://arxiv.org/abs/2401.12382v1'>2401.12382v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09897v1")'>COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web
  Application for Classifying COVID-19 Discussions</div>
<div id='2402.09897v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T11:45:34Z</div><div>Authors: Mahathir Mohammad Bishal, Md. Rakibul Hassan Chowdory, Anik Das, Muhammad Ashad Kabir</div><div style='padding-top: 10px; width: 80ex'>The COVID-19 pandemic has had adverse effects on both physical and mental
health. During this pandemic, numerous studies have focused on gaining insights
into health-related perspectives from social media. In this study, our primary
objective is to develop a machine learning-based web application for
automatically classifying COVID-19-related discussions on social media. To
achieve this, we label COVID-19-related Twitter data, provide benchmark
classification results, and develop a web application. We collected data using
the Twitter API and labeled a total of 6,667 tweets into five different
classes: health risks, prevention, symptoms, transmission, and treatment. We
extracted features using various feature extraction methods and applied them to
seven different traditional machine learning algorithms, including Decision
Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest
Neighbour, Logistic Regression, and Linear SVC. Additionally, we used four deep
learning algorithms: LSTM, CNN, RNN, and BERT, for classification. Overall, we
achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning.
The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing
other traditional machine learning approaches. Our study not only contributes
to the field of health-related data analysis but also provides a valuable
resource in the form of a web-based tool for efficient data classification,
which can aid in addressing public health challenges and increasing awareness
during pandemics. We made the dataset and application publicly available, which
can be downloaded from this link
https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website.</div><div><a href='http://arxiv.org/abs/2402.09897v1'>2402.09897v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03581v1")'>Enhancing ASD detection accuracy: a combined approach of machine
  learning and deep learning models with natural language processing</div>
<div id='2403.03581v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T09:57:42Z</div><div>Authors: Sergio Rubio-Martín, María Teresa García-Ordás, Martín Bayón-Gutiérrez, Natalia Prieto-Fernández, José Alberto Benítez-Andrades</div><div style='padding-top: 10px; width: 80ex'>Purpose: Our study explored the use of artificial intelligence (AI) to
diagnose autism spectrum disorder (ASD). It focused on machine learning (ML)
and deep learning (DL) to detect ASD from text inputs on social media,
addressing challenges in traditional ASD diagnosis.
  Methods: We used natural language processing (NLP), ML, and DL models
(including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to
analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A
subset of 90,000 tweets was used for model training and testing.
  Results: Our AI models showed high accuracy, with an 88% success rate in
identifying texts from individuals with ASD.
  Conclusion: The study demonstrates AI's potential in improving ASD diagnosis,
especially in children, highlighting the importance of early detection.</div><div><a href='http://arxiv.org/abs/2403.03581v1'>2403.03581v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12538v1")'>A Machine Learning Ensemble Model for the Detection of Cyberbullying</div>
<div id='2402.12538v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:55:12Z</div><div>Authors: Abulkarim Faraj Alqahtani, Mohammad Ilyas</div><div style='padding-top: 10px; width: 80ex'>The pervasive use of social media platforms, such as Facebook, Instagram, and
X, has significantly amplified our electronic interconnectedness. Moreover,
these platforms are now easily accessible from any location at any given time.
However, the increased popularity of social media has also led to
cyberbullying.It is imperative to address the need for finding, monitoring, and
mitigating cyberbullying posts on social media platforms. Motivated by this
necessity, we present this paper to contribute to developing an automated
system for detecting binary labels of aggressive tweets.Our study has
demonstrated remarkable performance compared to previous experiments on the
same dataset. We employed the stacking ensemble machine learning method,
utilizing four various feature extraction techniques to optimize performance
within the stacking ensemble learning framework. Combining five machine
learning algorithms,Decision Trees, Random Forest, Linear Support Vector
Classification, Logistic Regression, and K-Nearest Neighbors into an ensemble
method, we achieved superior results compared to traditional machine learning
classifier models. The stacking classifier achieved a high accuracy rate of
94.00%, outperforming traditional machine learning models and surpassing the
results of prior experiments that utilized the same dataset. The outcomes of
our experiments showcased an accuracy rate of 0.94% in detection tweets as
aggressive or non-aggressive.</div><div><a href='http://arxiv.org/abs/2402.12538v1'>2402.12538v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17705v1")'>Predicting suicidal behavior among Indian adults using childhood trauma,
  mental health questionnaires and machine learning cascade ensembles</div>
<div id='2401.17705v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T09:49:46Z</div><div>Authors: Akash K Rao, Gunjan Y Trivedi, Riri G Trivedi, Anshika Bajpai, Gajraj Singh Chauhan, Vishnu K Menon, Kathirvel Soundappan, Hemalatha Ramani, Neha Pandya, Varun Dutt</div><div style='padding-top: 10px; width: 80ex'>Among young adults, suicide is India's leading cause of death, accounting for
an alarming national suicide rate of around 16%. In recent years, machine
learning algorithms have emerged to predict suicidal behavior using various
behavioral traits. But to date, the efficacy of machine learning algorithms in
predicting suicidal behavior in the Indian context has not been explored in
literature. In this study, different machine learning algorithms and ensembles
were developed to predict suicide behavior based on childhood trauma, different
mental health parameters, and other behavioral factors. The dataset was
acquired from 391 individuals from a wellness center in India. Information
regarding their childhood trauma, psychological wellness, and other mental
health issues was acquired through standardized questionnaires. Results
revealed that cascade ensemble learning methods using a support vector machine,
decision trees, and random forest were able to classify suicidal behavior with
an accuracy of 95.04% using data from childhood trauma and mental health
questionnaires. The study highlights the potential of using these machine
learning ensembles to identify individuals with suicidal tendencies so that
targeted interinterventions could be provided efficiently.</div><div><a href='http://arxiv.org/abs/2401.17705v1'>2401.17705v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00821v1")'>Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer
  Medication Effects Using Natural Language Processing</div>
<div id='2403.00821v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T16:17:19Z</div><div>Authors: Seibi Kobara, Alireza Rafiei, Masoud Nateghi, Selen Bozkurt, Rishikesan Kamaleswaran, Abeed Sarker</div><div style='padding-top: 10px; width: 80ex'>Breast cancer is a significant public health concern and is the leading cause
of cancer-related deaths among women. Despite advances in breast cancer
treatments, medication non-adherence remains a major problem. As electronic
health records do not typically capture patient-reported outcomes that may
reveal information about medication-related experiences, social media presents
an attractive resource for enhancing our understanding of the patients'
treatment experiences. In this paper, we developed natural language processing
(NLP) based methodologies to study information posted by an automatically
curated breast cancer cohort from social media. We employed a transformer-based
classifier to identify breast cancer patients/survivors on X (Twitter) based on
their self-reported information, and we collected longitudinal data from their
profiles. We then designed a multi-layer rule-based model to develop a breast
cancer therapy-associated side effect lexicon and detect patterns of medication
usage and associated side effects among breast cancer patients. 1,454,637 posts
were available from 583,962 unique users, of which 62,042 were detected as
breast cancer members using our transformer-based model. 198 cohort members
mentioned breast cancer medications with tamoxifen as the most common. Our side
effect lexicon identified well-known side effects of hormone and chemotherapy.
Furthermore, it discovered a subject feeling towards cancer and medications,
which may suggest a pre-clinical phase of side effects or emotional distress.
This analysis highlighted not only the utility of NLP techniques in
unstructured social media data to identify self-reported breast cancer posts,
medication usage patterns, and treatment side effects but also the richness of
social data on such clinical questions.</div><div><a href='http://arxiv.org/abs/2403.00821v1'>2403.00821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13452v1")'>LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data</div>
<div id='2402.13452v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T01:11:28Z</div><div>Authors: Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu</div><div style='padding-top: 10px; width: 80ex'>Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.</div><div><a href='http://arxiv.org/abs/2402.13452v1'>2402.13452v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01716v1")'>Bloom-epistemic and sentiment analysis hierarchical classification in
  course discussion forums</div>
<div id='2402.01716v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T08:20:13Z</div><div>Authors: H. Toba, Y. T. Hernita, M. Ayub, M. C. Wijanto</div><div style='padding-top: 10px; width: 80ex'>Online discussion forums are widely used for active textual interaction
between lecturers and students, and to see how the students have progressed in
a learning process. The objective of this study is to compare appropriate
machine-learning models to assess sentiments and Bloom\'s epistemic taxonomy
based on textual comments in educational discussion forums. Our proposed method
is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis
(BE-Sent). The research methodology consists of three main steps. The first
step is the data collection from the internal discussion forum and YouTube
comments of a Web Programming channel. The next step is text preprocessing to
annotate the text and clear unimportant words. Furthermore, with the text
dataset that has been successfully cleaned, sentiment analysis and epistemic
categorization will be done in each sentence of the text. Sentiment analysis is
divided into three categories: positive, negative, and neutral. Bloom\'s
epistemic is divided into six categories: remembering, understanding, applying,
analyzing, evaluating, and creating. This research has succeeded in producing a
course learning subsystem that assesses opinions based on text reviews of
discussion forums according to the category of sentiment and epistemic
analysis.</div><div><a href='http://arxiv.org/abs/2402.01716v1'>2402.01716v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06060v1")'>Ensemble Language Models for Multilingual Sentiment Analysis</div>
<div id='2403.06060v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T01:39:10Z</div><div>Authors: Md Arid Hasan</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of social media enables us to analyze user opinions. In
recent times, sentiment analysis has shown a prominent research gap in
understanding human sentiment based on the content shared on social media.
Although sentiment analysis for commonly spoken languages has advanced
significantly, low-resource languages like Arabic continue to get little
research due to resource limitations. In this study, we explore sentiment
analysis on tweet texts from SemEval-17 and the Arabic Sentiment Tweet dataset.
Moreover, We investigated four pretrained language models and proposed two
ensemble language models. Our findings include monolingual models exhibiting
superior performance and ensemble models outperforming the baseline while the
majority voting ensemble outperforms the English language.</div><div><a href='http://arxiv.org/abs/2403.06060v1'>2403.06060v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15289v1")'>Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis
  with Diffusion Models</div>
<div id='2402.15289v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:35:43Z</div><div>Authors: Shunyu Liu, Jie Zhou, Qunxi Zhu, Qin Chen, Qingchun Bai, Jun Xiao, Liang He</div><div style='padding-top: 10px; width: 80ex'>Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting
the sentiment polarity associated with identified aspects within text. However,
a notable challenge in ABSA lies in precisely determining the aspects'
boundaries (start and end indices), especially for long ones, due to users'
colloquial expressions. We propose DiffusionABSA, a novel diffusion model
tailored for ABSA, which extracts the aspects progressively step by step.
Particularly, DiffusionABSA gradually adds noise to the aspect terms in the
training process, subsequently learning a denoising process that progressively
restores these terms in a reverse manner. To estimate the boundaries, we design
a denoising neural network enhanced by a syntax-aware temporal attention
mechanism to chronologically capture the interplay between aspects and
surrounding text. Empirical evaluations conducted on eight benchmark datasets
underscore the compelling advantages offered by DiffusionABSA when compared
against robust baseline models. Our code is publicly available at
https://github.com/Qlb6x/DiffusionABSA.</div><div><a href='http://arxiv.org/abs/2402.15289v1'>2402.15289v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06210v1")'>Learning Unsupervised Semantic Document Representation for Fine-grained
  Aspect-based Sentiment Analysis</div>
<div id='2401.06210v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:59:52Z</div><div>Authors: Hao-Ming Fu, Pu-Jen Cheng</div><div style='padding-top: 10px; width: 80ex'>Document representation is the core of many NLP tasks on machine
understanding. A general representation learned in an unsupervised manner
reserves generality and can be used for various applications. In practice,
sentiment analysis (SA) has been a challenging task that is regarded to be
deeply semantic-related and is often used to assess general representations.
Existing methods on unsupervised document representation learning can be
separated into two families: sequential ones, which explicitly take the
ordering of words into consideration, and non-sequential ones, which do not
explicitly do so. However, both of them suffer from their own weaknesses. In
this paper, we propose a model that overcomes difficulties encountered by both
families of methods. Experiments show that our model outperforms
state-of-the-art methods on popular SA datasets and a fine-grained aspect-based
SA by a large margin.</div><div><a href='http://arxiv.org/abs/2401.06210v1'>2401.06210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15370v1")'>Dual Encoder: Exploiting the Potential of Syntactic and Semantic for
  Aspect Sentiment Triplet Extraction</div>
<div id='2402.15370v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:07:13Z</div><div>Authors: Xiaowei Zhao, Yong Zhou, Xiujuan Xu</div><div style='padding-top: 10px; width: 80ex'>Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained
sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to
model the syntax-semantic relationships inherent in triplet elements. However,
they have yet to fully tap into the vast potential of syntactic and semantic
information within the ASTE task. In this work, we propose a \emph{Dual
Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S),
which maximizes the syntactic and semantic relationships among words.
Specifically, our model utilizes a dual-channel encoder with a BERT channel to
capture semantic information, and an enhanced LSTM channel for comprehensive
syntactic information capture. Subsequently, we introduce the heterogeneous
feature interaction module to capture intricate interactions between dependency
syntax and attention semantics, and to dynamically select vital nodes. We
leverage the synergy of these modules to harness the significant potential of
syntactic and semantic information in ASTE tasks. Testing on public benchmarks,
our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its
effectiveness.</div><div><a href='http://arxiv.org/abs/2402.15370v1'>2402.15370v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12133v1")'>VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear
  Responses in VR Stand-up Interactive Games</div>
<div id='2401.12133v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T17:15:02Z</div><div>Authors: He Zhang, Xinyang Li, Yuanxi Sun, Xinyi Fu, Christine Qiu, John M. Carroll</div><div style='padding-top: 10px; width: 80ex'>Understanding and recognizing emotions are important and challenging issues
in the metaverse era. Understanding, identifying, and predicting fear, which is
one of the fundamental human emotions, in virtual reality (VR) environments
plays an essential role in immersive game development, scene development, and
next-generation virtual human-computer interaction applications. In this
article, we used VR horror games as a medium to analyze fear emotions by
collecting multi-modal data (posture, audio, and physiological signals) from 23
players. We used an LSTM-based model to predict fear with accuracies of 65.31%
and 90.47% under 6-level classification (no fear and five different levels of
fear) and 2-level classification (no fear and fear), respectively. We
constructed a multi-modal natural behavior dataset of immersive human fear
responses (VRMN-bD) and compared it with existing relevant advanced datasets.
The results show that our dataset has fewer limitations in terms of collection
method, data scale and audience scope. We are unique and advanced in targeting
multi-modal datasets of fear and behavior in VR stand-up interactive
environments. Moreover, we discussed the implications of this work for
communities and applications. The dataset and pre-trained model are available
at https://github.com/KindOPSTAR/VRMN-bD.</div><div><a href='http://arxiv.org/abs/2401.12133v1'>2401.12133v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10921v1")'>AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in
  Conversation via Joint Embedding Learning</div>
<div id='2402.10921v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T19:57:26Z</div><div>Authors: Naresh Kumar Devulapally, Sidharth Anand, Sreyasee Das Bhattacharjee, Junsong Yuan</div><div style='padding-top: 10px; width: 80ex'>Human emotion can be presented in different modes i.e., audio, video, and
text. However, the contribution of each mode in exhibiting each emotion is not
uniform. Furthermore, the availability of complete mode-specific details may
not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE,
a model for Adaptive Missing-Modality Emotion Recognition in Conversation via
Joint Embedding Learning model that is grounded on two-fold contributions:
First, a query adaptive fusion that can automatically learn the relative
importance of its mode-specific representations in a query-specific manner. By
this the model aims to prioritize the mode-invariant spatial query details of
the emotion patterns, while also retaining its mode-exclusive aspects within
the learned multimodal query descriptor. Second the multimodal joint embedding
learning module that explicitly addresses various missing modality scenarios in
test-time. By this, the model learns to emphasize on the correlated patterns
across modalities, which may help align the cross-attended mode-specific
descriptors pairwise within a joint-embedding space and thereby compensate for
missing modalities during inference. By leveraging the spatio-temporal details
at the dialogue level, the proposed AM^2-EmoJE not only demonstrates superior
performance compared to the best-performing state-of-the-art multimodal
methods, by effectively leveraging body language in place of face expression,
it also exhibits an enhanced privacy feature. By reporting around 2-5%
improvement in the weighted-F1 score, the proposed multimodal joint embedding
module facilitates an impressive performance gain in a variety of
missing-modality query scenarios during test time.</div><div><a href='http://arxiv.org/abs/2402.10921v1'>2402.10921v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15164v1")'>AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in
  Group Conversations</div>
<div id='2401.15164v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T19:17:05Z</div><div>Authors: Naresh Kumar Devulapally, Sidharth Anand, Sreyasee Das Bhattacharjee, Junsong Yuan, Yu-Ping Chang</div><div style='padding-top: 10px; width: 80ex'>Analyzing individual emotions during group conversation is crucial in
developing intelligent agents capable of natural human-machine interaction.
While reliable emotion recognition techniques depend on different modalities
(text, audio, video), the inherent heterogeneity between these modalities and
the dynamic cross-modal interactions influenced by an individual's unique
behavioral patterns make the task of emotion recognition very challenging. This
difficulty is compounded in group settings, where the emotion and its temporal
evolution are not only influenced by the individual but also by external
contexts like audience reaction and context of the ongoing conversation. To
meet this challenge, we propose a Multimodal Attention Network that captures
cross-modal interactions at various levels of spatial abstraction by jointly
learning its interactive bunch of mode-specific Peripheral and Central
networks. The proposed MAN injects cross-modal attention via its Peripheral
key-value pairs within each layer of a mode-specific Central query network. The
resulting cross-attended mode-specific descriptors are then combined using an
Adaptive Fusion technique that enables the model to integrate the
discriminative and complementary mode-specific data patterns within an
instance-specific multimodal descriptor. Given a dialogue represented by a
sequence of utterances, the proposed AMuSE model condenses both spatial and
temporal features into two dense descriptors: speaker-level and
utterance-level. This helps not only in delivering better classification
performance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy)
in large-scale public datasets but also helps the users in understanding the
reasoning behind each emotion prediction made by the model via its Multimodal
Explainability Visualization module.</div><div><a href='http://arxiv.org/abs/2401.15164v1'>2401.15164v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12987v1")'>TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition
  in Conversation</div>
<div id='2401.12987v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T07:18:41Z</div><div>Authors: Taeyang Yun, Hyunkuk Lim, Jeonghwan Lee, Min Song</div><div style='padding-top: 10px; width: 80ex'>Emotion Recognition in Conversation (ERC) plays a crucial role in enabling
dialogue systems to effectively respond to user requests. The emotions in a
conversation can be identified by the representations from various modalities,
such as audio, visual, and text. However, due to the weak contribution of
non-verbal modalities to recognize emotions, multimodal ERC has always been
considered a challenging task. In this paper, we propose Teacher-leading
Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal
knowledge distillation to transfer information from a language model acting as
the teacher to the non-verbal students, thereby optimizing the efficacy of the
weak modalities. We then combine multimodal features using a shifting fusion
approach in which student networks support the teacher. TelME achieves
state-of-the-art performance in MELD, a multi-speaker conversation dataset for
ERC. Finally, we demonstrate the effectiveness of our components through
additional experiments.</div><div><a href='http://arxiv.org/abs/2401.12987v1'>2401.12987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03000v1")'>Bridging Modalities: Knowledge Distillation and Masked Training for
  Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion
  Recognition</div>
<div id='2401.03000v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T22:42:14Z</div><div>Authors: Muhammad Muaz, Nathan Paull, Jahnavi Malagavalli</div><div style='padding-top: 10px; width: 80ex'>This paper presents an innovative approach to address the challenges of
translating multi-modal emotion recognition models to a more practical and
resource-efficient uni-modal counterpart, specifically focusing on speech-only
emotion recognition. Recognizing emotions from speech signals is a critical
task with applications in human-computer interaction, affective computing, and
mental health assessment. However, existing state-of-the-art models often rely
on multi-modal inputs, incorporating information from multiple sources such as
facial expressions and gestures, which may not be readily available or feasible
in real-world scenarios. To tackle this issue, we propose a novel framework
that leverages knowledge distillation and masked training techniques.</div><div><a href='http://arxiv.org/abs/2401.03000v1'>2401.03000v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11017v1")'>Revealing Emotional Clusters in Speaker Embeddings: A Contrastive
  Learning Strategy for Speech Emotion Recognition</div>
<div id='2401.11017v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T20:31:53Z</div><div>Authors: Ismail Rasim Ulgen, Zongyang Du, Carlos Busso, Berrak Sisman</div><div style='padding-top: 10px; width: 80ex'>Speaker embeddings carry valuable emotion-related information, which makes
them a promising resource for enhancing speech emotion recognition (SER),
especially with limited labeled data. Traditionally, it has been assumed that
emotion information is indirectly embedded within speaker embeddings, leading
to their under-utilization. Our study reveals a direct and useful link between
emotion and state-of-the-art speaker embeddings in the form of intra-speaker
clusters. By conducting a thorough clustering analysis, we demonstrate that
emotion information can be readily extracted from speaker embeddings. In order
to leverage this information, we introduce a novel contrastive pretraining
approach applied to emotion-unlabeled data for speech emotion recognition. The
proposed approach involves the sampling of positive and the negative examples
based on the intra-speaker clusters of speaker embeddings. The proposed
strategy, which leverages extensive emotion-unlabeled data, leads to a
significant improvement in SER performance, whether employed as a standalone
pretraining task or integrated into a multi-task pretraining setting.</div><div><a href='http://arxiv.org/abs/2401.11017v1'>2401.11017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03100v1")'>NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models</div>
<div id='2403.03100v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T16:35:25Z</div><div>Authors: Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao</div><div style='padding-top: 10px; width: 80ex'>While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model the intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility. Furthermore, we achieve better performance by scaling to 1B
parameters and 200K hours of training data.</div><div><a href='http://arxiv.org/abs/2403.03100v1'>2403.03100v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00529v1")'>VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech
  Synthesis</div>
<div id='2403.00529v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:39:56Z</div><div>Authors: Weiwei Lin, Chenhang He, Man-Wai Mak, Jiachen Lian, Kong Aik Lee</div><div style='padding-top: 10px; width: 80ex'>Achieving nuanced and accurate emulation of human voice has been a
longstanding goal in artificial intelligence. Although significant progress has
been made in recent years, the mainstream of speech synthesis models still
relies on supervised speaker modeling and explicit reference utterances.
However, there are many aspects of human voice, such as emotion, intonation,
and speaking style, for which it is hard to obtain accurate labels. In this
paper, we propose VoxGenesis, a novel unsupervised speech synthesis framework
that can discover a latent speaker manifold and meaningful voice editing
directions without supervision. VoxGenesis is conceptually simple. Instead of
mapping speech features to waveforms deterministically, VoxGenesis transforms a
Gaussian distribution into speech distributions conditioned and aligned by
semantic tokens. This forces the model to learn a speaker distribution
disentangled from the semantic content. During the inference, sampling from the
Gaussian distribution enables the creation of novel speakers with distinct
characteristics. More importantly, the exploration of latent space uncovers
human-interpretable directions associated with specific speaker characteristics
such as gender attributes, pitch, tone, and emotion, allowing for voice editing
by manipulating the latent codes along these identified directions. We conduct
extensive experiments to evaluate the proposed VoxGenesis using both subjective
and objective metrics, finding that it produces significantly more diverse and
realistic speakers with distinct characteristics than the previous approaches.
We also show that latent space manipulation produces consistent and
human-identifiable effects that are not detrimental to the speech quality,
which was not possible with previous approaches. Audio samples of VoxGenesis
can be found at: \url{https://bit.ly/VoxGenesis}.</div><div><a href='http://arxiv.org/abs/2403.00529v1'>2403.00529v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03407v1")'>Enhancing the Stability of LLM-based Speech Generation Systems through
  Self-Supervised Representations</div>
<div id='2402.03407v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:08:19Z</div><div>Authors: Álvaro Martín-Cortinas, Daniel Sáez-Trigueros, Iván Vallés-Pérez, Biel Tura-Vecino, Piotr Biliński, Mateusz Lajszczak, Grzegorz Beringer, Roberto Barra-Chicote, Jaime Lorenzo-Trueba</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) are one of the most promising technologies for
the next era of speech generation systems, due to their scalability and
in-context learning capabilities. Nevertheless, they suffer from multiple
stability issues at inference time, such as hallucinations, content skipping or
speech repetitions. In this work, we introduce a new self-supervised Voice
Conversion (VC) architecture which can be used to learn to encode transitory
features, such as content, separately from stationary ones, such as speaker ID
or recording conditions, creating speaker-disentangled representations. Using
speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the
LLM to generate the content and the style of the speech only from the text,
similarly to humans, while the speaker identity is provided by the decoder of
the VC model. Results show that LLMs trained over speaker-disentangled
self-supervised representations provide an improvement of 4.7pp in speaker
similarity over SOTA entangled representations, and a word error rate (WER)
5.4pp lower. Furthermore, they achieve higher naturalness than human recordings
of the LibriTTS test-other dataset. Finally, we show that using explicit
reference embedding negatively impacts intelligibility (stability), with WER
increasing by 14pp compared to the model that only uses text to infer the
style.</div><div><a href='http://arxiv.org/abs/2402.03407v1'>2402.03407v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08093v2")'>BASE TTS: Lessons from building a billion-parameter Text-to-Speech model
  on 100K hours of data</div>
<div id='2402.08093v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:21:30Z</div><div>Authors: Mateusz Łajszczak, Guillermo Cámbara, Yang Li, Fatih Beyhan, Arent van Korlaar, Fan Yang, Arnaud Joly, Álvaro Martín-Cortinas, Ammar Abbas, Adam Michalski, Alexis Moinet, Sri Karlapati, Ewa Muszyńska, Haohan Guo, Bartosz Putrycz, Soledad López Gambino, Kayeon Yoo, Elena Sokolova, Thomas Drugman</div><div style='padding-top: 10px; width: 80ex'>We introduce a text-to-speech (TTS) model called BASE TTS, which stands for
$\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with
$\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date,
trained on 100K hours of public domain speech data, achieving a new
state-of-the-art in speech naturalness. It deploys a 1-billion-parameter
autoregressive Transformer that converts raw texts into discrete codes
("speechcodes") followed by a convolution-based decoder which converts these
speechcodes into waveforms in an incremental, streamable manner. Further, our
speechcodes are built using a novel speech tokenization technique that features
speaker ID disentanglement and compression with byte-pair encoding. Echoing the
widely-reported "emergent abilities" of large language models when trained on
increasing volume of data, we show that BASE TTS variants built with 10K+ hours
and 500M+ parameters begin to demonstrate natural prosody on textually complex
sentences. We design and share a specialized dataset to measure these emergent
abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE
TTS by evaluating against baselines that include publicly available large-scale
text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated
by the model can be heard at https://amazon-ltts-paper.com/.</div><div><a href='http://arxiv.org/abs/2402.08093v2'>2402.08093v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01498v1")'>Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic
  Token Prediction</div>
<div id='2401.01498v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T02:03:36Z</div><div>Authors: Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Semin Kim, Joun Yeop Lee, Nam Soo Kim</div><div style='padding-top: 10px; width: 80ex'>We propose a novel text-to-speech (TTS) framework centered around a neural
transducer. Our approach divides the whole TTS pipeline into semantic-level
sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling
stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.
For a robust and efficient alignment modeling, we employ a neural transducer
named token transducer for the semantic token prediction, benefiting from its
hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)
speech generator efficiently synthesizes waveforms from these semantic tokens.
Additionally, a reference speech controls temporal dynamics and acoustic
conditions at each stage. This decoupled framework reduces the training
complexity of TTS while allowing each stage to focus on semantic and acoustic
modeling. Our experimental results on zero-shot adaptive TTS demonstrate that
our model surpasses the baseline in terms of speech quality and speaker
similarity, both objectively and subjectively. We also delve into the inference
speed and prosody control capabilities of our approach, highlighting the
potential of neural transducers in TTS frameworks.</div><div><a href='http://arxiv.org/abs/2401.01498v1'>2401.01498v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06183v1")'>End to end Hindi to English speech conversion using Bark, mBART and a
  finetuned XLSR Wav2Vec2</div>
<div id='2401.06183v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T04:26:21Z</div><div>Authors: Aniket Tathe, Anand Kamble, Suyash Kumbharkar, Atharva Bhandare, Anirban C. Mitra</div><div style='padding-top: 10px; width: 80ex'>Speech has long been a barrier to effective communication and connection,
persisting as a challenge in our increasingly interconnected world. This
research paper introduces a transformative solution to this persistent obstacle
an end-to-end speech conversion framework tailored for Hindi-to-English
translation, culminating in the synthesis of English audio. By integrating
cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech
recognition (ASR), mBART for neural machine translation (NMT), and a
Text-to-Speech (TTS) synthesis component, this framework offers a unified and
seamless approach to cross-lingual communication. We delve into the intricate
details of each component, elucidating their individual contributions and
exploring the synergies that enable a fluid transition from spoken Hindi to
synthesized English audio.</div><div><a href='http://arxiv.org/abs/2401.06183v1'>2401.06183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16380v1")'>An Automated End-to-End Open-Source Software for High-Quality
  Text-to-Speech Dataset Generation</div>
<div id='2402.16380v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:58:33Z</div><div>Authors: Ahmet Gunduz, Kamer Ali Yuksel, Kareem Darwish, Golara Javadi, Fabio Minazzi, Nicola Sobieski, Sebastien Bratieres</div><div style='padding-top: 10px; width: 80ex'>Data availability is crucial for advancing artificial intelligence
applications, including voice-based technologies. As content creation,
particularly in social media, experiences increasing demand, translation and
text-to-speech (TTS) technologies have become essential tools. Notably, the
performance of these TTS technologies is highly dependent on the quality of the
training data, emphasizing the mutual dependence of data availability and
technological progress. This paper introduces an end-to-end tool to generate
high-quality datasets for text-to-speech (TTS) models to address this critical
need for high-quality data. The contributions of this work are manifold and
include: the integration of language-specific phoneme distribution into sample
selection, automation of the recording process, automated and human-in-the-loop
quality assurance of recordings, and processing of recordings to meet specified
formats. The proposed application aims to streamline the dataset creation
process for TTS models through these features, thereby facilitating
advancements in voice-based technologies.</div><div><a href='http://arxiv.org/abs/2402.16380v1'>2402.16380v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14438v1")'>A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models</div>
<div id='2403.14438v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:44:03Z</div><div>Authors: Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi</div><div style='padding-top: 10px; width: 80ex'>Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.</div><div><a href='http://arxiv.org/abs/2403.14438v1'>2403.14438v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07575v1")'>Cascaded Cross-Modal Transformer for Audio-Textual Classification</div>
<div id='2401.07575v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T10:18:08Z</div><div>Authors: Nicolae-Catalin Ristea, Andrei Anghel, Radu Tudor Ionescu</div><div style='padding-top: 10px; width: 80ex'>Speech classification tasks often require powerful language understanding
models to grasp useful features, which becomes problematic when limited
training data is available. To attain superior classification performance, we
propose to harness the inherent value of multimodal representations by
transcribing speech using automatic speech recognition (ASR) models and
translating the transcripts into different languages via pretrained translation
models. We thus obtain an audio-textual (multimodal) representation for each
data sample. Subsequently, we combine language-specific Bidirectional Encoder
Representations from Transformers (BERT) with Wav2Vec2.0 audio features via a
novel cascaded cross-modal transformer (CCMT). Our model is based on two
cascaded transformer blocks. The first one combines text-specific features from
distinct languages, while the second one combines acoustic features with
multilingual features previously learned by the first transformer block. We
employed our system in the Requests Sub-Challenge of the ACM Multimedia 2023
Computational Paralinguistics Challenge. CCMT was declared the winning
solution, obtaining an unweighted average recall (UAR) of 65.41% and 85.87% for
complaint and request detection, respectively. Moreover, we applied our
framework on the Speech Commands v2 and HarperValleyBank dialog data sets,
surpassing previous studies reporting results on these benchmarks. Our code is
freely available for download at: https://github.com/ristea/ccmt.</div><div><a href='http://arxiv.org/abs/2401.07575v1'>2401.07575v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06832v1")'>XLS-R Deep Learning Model for Multilingual ASR on Low- Resource
  Languages: Indonesian, Javanese, and Sundanese</div>
<div id='2401.06832v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T13:44:48Z</div><div>Authors: Panji Arisaputra, Alif Tri Handoyo, Amalia Zahra</div><div style='padding-top: 10px; width: 80ex'>This research paper focuses on the development and evaluation of Automatic
Speech Recognition (ASR) technology using the XLS-R 300m model. The study aims
to improve ASR performance in converting spoken language into written text,
specifically for Indonesian, Javanese, and Sundanese languages. The paper
discusses the testing procedures, datasets used, and methodology employed in
training and evaluating the ASR systems. The results show that the XLS-R 300m
model achieves competitive Word Error Rate (WER) measurements, with a slight
compromise in performance for Javanese and Sundanese languages. The integration
of a 5-gram KenLM language model significantly reduces WER and enhances ASR
accuracy. The research contributes to the advancement of ASR technology by
addressing linguistic diversity and improving performance across various
languages. The findings provide insights into optimizing ASR accuracy and
applicability for diverse linguistic contexts.</div><div><a href='http://arxiv.org/abs/2401.06832v1'>2401.06832v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03506v4")'>DiarizationLM: Speaker Diarization Post-Processing with Large Language
  Models</div>
<div id='2401.03506v4' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T14:54:57Z</div><div>Authors: Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, Hank Liao</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce DiarizationLM, a framework to leverage large
language models (LLM) to post-process the outputs from a speaker diarization
system. Various goals can be achieved with the proposed framework, such as
improving the readability of the diarized transcript, or reducing the word
diarization error rate (WDER). In this framework, the outputs of the automatic
speech recognition (ASR) and speaker diarization systems are represented as a
compact textual format, which is included in the prompt to an optionally
finetuned LLM. The outputs of the LLM can be used as the refined diarization
results with the desired enhancement. As a post-processing step, this framework
can be easily applied to any off-the-shelf ASR and speaker diarization systems
without retraining existing components. Our experiments show that a finetuned
PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone
conversation dataset, and rel. 44.9% on the Callhome English dataset.</div><div><a href='http://arxiv.org/abs/2401.03506v4'>2401.03506v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10961v1")'>Energy-Based Models with Applications to Speech and Language Processing</div>
<div id='2403.10961v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T16:16:31Z</div><div>Authors: Zhijian Ou</div><div style='padding-top: 10px; width: 80ex'>Energy-Based Models (EBMs) are an important class of probabilistic models,
also known as random fields and undirected graphical models. EBMs are
un-normalized and thus radically different from other popular self-normalized
probabilistic models such as hidden Markov models (HMMs), autoregressive
models, generative adversarial nets (GANs) and variational auto-encoders
(VAEs). Over the past years, EBMs have attracted increasing interest not only
from the core machine learning community, but also from application domains
such as speech, vision, natural language processing (NLP) and so on, due to
significant theoretical and algorithmic progress. The sequential nature of
speech and language also presents special challenges and needs a different
treatment from processing fix-dimensional data (e.g., images). Therefore, the
purpose of this monograph is to present a systematic introduction to
energy-based models, including both algorithmic progress and applications in
speech and language processing. First, the basics of EBMs are introduced,
including classic models, recent models parameterized by neural networks,
sampling methods, and various learning methods from the classic learning
algorithms to the most advanced ones. Then, the application of EBMs in three
different scenarios is presented, i.e., for modeling marginal, conditional and
joint distributions, respectively. 1) EBMs for sequential data with
applications in language modeling, where the main focus is on the marginal
distribution of a sequence itself; 2) EBMs for modeling conditional
distributions of target sequences given observation sequences, with
applications in speech recognition, sequence labeling and text generation; 3)
EBMs for modeling joint distributions of both sequences of observations and
targets, and their applications in semi-supervised learning and calibrated
natural language understanding.</div><div><a href='http://arxiv.org/abs/2403.10961v1'>2403.10961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00067v1")'>Online speaker diarization of meetings guided by speech separation</div>
<div id='2402.00067v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T09:09:22Z</div><div>Authors: Elio Gruttadauria, Mathieu Fontaine, Slim Essid</div><div style='padding-top: 10px; width: 80ex'>Overlapped speech is notoriously problematic for speaker diarization systems.
Consequently, the use of speech separation has recently been proposed to
improve their performance. Although promising, speech separation models
struggle with realistic data because they are trained on simulated mixtures
with a fixed number of speakers. In this work, we introduce a new speech
separation-guided diarization scheme suitable for the online speaker
diarization of long meeting recordings with a variable number of speakers, as
present in the AMI corpus. We envisage ConvTasNet and DPRNN as alternatives for
the separation networks, with two or three output sources. To obtain the
speaker diarization result, voice activity detection is applied on each
estimated source. The final model is fine-tuned end-to-end, after first
adapting the separation to real data using AMI. The system operates on short
segments, and inference is performed by stitching the local predictions using
speaker embeddings and incremental clustering. The results show that our system
improves the state-of-the-art on the AMI headset mix, using no oracle
information and under full evaluation (no collar and including overlapped
speech). Finally, we show the strength of our system particularly on overlapped
speech sections.</div><div><a href='http://arxiv.org/abs/2402.00067v1'>2402.00067v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01413v1")'>Objective and subjective evaluation of speech enhancement methods in the
  UDASE task of the 7th CHiME challenge</div>
<div id='2402.01413v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:45:42Z</div><div>Authors: Simon Leglaive, Matthieu Fraticelli, Hend ElGhazaly, Léonie Borne, Mostafa Sadeghi, Scott Wisdom, Manuel Pariente, John R. Hershey, Daniel Pressnitzer, Jon P. Barker</div><div style='padding-top: 10px; width: 80ex'>Supervised models for speech enhancement are trained using artificially
generated mixtures of clean speech and noise signals. However, the synthetic
training conditions may not accurately reflect real-world conditions
encountered during testing. This discrepancy can result in poor performance
when the test domain significantly differs from the synthetic training domain.
To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to
leverage real-world noisy speech recordings from the test domain for
unsupervised domain adaptation of speech enhancement models. Specifically, this
test domain corresponds to the CHiME-5 dataset, characterized by real
multi-speaker and conversational speech recordings made in noisy and
reverberant domestic environments, for which ground-truth clean speech signals
are not available. In this paper, we present the objective and subjective
evaluations of the systems that were submitted to the CHiME-7 UDASE task, and
we provide an analysis of the results. This analysis reveals a limited
correlation between subjective ratings and several supervised nonintrusive
performance metrics recently proposed for speech enhancement. Conversely, the
results suggest that more traditional intrusive objective metrics can be used
for in-domain performance evaluation using the reverberant LibriCHiME-5 dataset
developed for the challenge. The subjective evaluation indicates that all
systems successfully reduced the background noise, but always at the expense of
increased distortion. Out of the four speech enhancement methods evaluated
subjectively, only one demonstrated an improvement in overall quality compared
to the unprocessed noisy speech, highlighting the difficulty of the task. The
tools and audio material created for the CHiME-7 UDASE task are shared with the
community.</div><div><a href='http://arxiv.org/abs/2402.01413v1'>2402.01413v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06913v1")'>Microphone Conversion: Mitigating Device Variability in Sound Event
  Classification</div>
<div id='2401.06913v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T21:59:01Z</div><div>Authors: Myeonghoon Ryu, Hongseok Oh, Suji Lee, Han Park</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce a new augmentation technique to enhance the
resilience of sound event classification (SEC) systems against device
variability through the use of CycleGAN. We also present a unique dataset to
evaluate this method. As SEC systems become increasingly common, it is crucial
that they work well with audio from diverse recording devices. Our method
addresses limited device diversity in training data by enabling unpaired
training to transform input spectrograms as if they are recorded on a different
device. Our experiments show that our approach outperforms existing methods in
generalization by 5.2% - 11.5% in weighted f1 score. Additionally, it surpasses
the current methods in adaptability across diverse recording devices by
achieving a 6.5% - 12.8% improvement in weighted f1 score.</div><div><a href='http://arxiv.org/abs/2401.06913v1'>2401.06913v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18007v2")'>Mixer is more than just a model</div>
<div id='2402.18007v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T02:45:58Z</div><div>Authors: Qingfeng Ji, Yuxin Wang, Letong Sun</div><div style='padding-top: 10px; width: 80ex'>Recently, MLP structures have regained popularity, with MLP-Mixer standing
out as a prominent example. In the field of computer vision, MLP-Mixer is noted
for its ability to extract data information from both channel and token
perspectives, effectively acting as a fusion of channel and token information.
Indeed, Mixer represents a paradigm for information extraction that amalgamates
channel and token information. The essence of Mixer lies in its ability to
blend information from diverse perspectives, epitomizing the true concept of
"mixing" in the realm of neural network architectures. Beyond channel and token
considerations, it is possible to create more tailored mixers from various
perspectives to better suit specific task requirements. This study focuses on
the domain of audio recognition, introducing a novel model named Audio
Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates
insights from both time and frequency domains. Experimental results demonstrate
that ASM-RH is particularly well-suited for audio data and yields promising
outcomes across multiple classification tasks. The models and optimal weights
files will be published.</div><div><a href='http://arxiv.org/abs/2402.18007v2'>2402.18007v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16423v1")'>Synchformer: Efficient Synchronization from Sparse Cues</div>
<div id='2401.16423v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:59:55Z</div><div>Authors: Vladimir Iashin, Weidi Xie, Esa Rahtu, Andrew Zisserman</div><div style='padding-top: 10px; width: 80ex'>Our objective is audio-visual synchronization with a focus on 'in-the-wild'
videos, such as those on YouTube, where synchronization cues can be sparse. Our
contributions include a novel audio-visual synchronization model, and training
that decouples feature extraction from synchronization modelling through
multi-modal segment-level contrastive pre-training. This approach achieves
state-of-the-art performance in both dense and sparse settings. We also extend
synchronization model training to AudioSet a million-scale 'in-the-wild'
dataset, investigate evidence attribution techniques for interpretability, and
explore a new capability for synchronization models: audio-visual
synchronizability.</div><div><a href='http://arxiv.org/abs/2401.16423v1'>2401.16423v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09245v1")'>Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality</div>
<div id='2402.09245v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:34:28Z</div><div>Authors: Christian Marinoni, Riccardo Fosco Gramaccioni, Changan Chen, Aurelio Uncini, Danilo Comminiello</div><div style='padding-top: 10px; width: 80ex'>The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP
2023 is to promote and support collaborative research on machine learning for
3D audio signal processing, with a specific emphasis on 3D speech enhancement
and 3D Sound Event Localization and Detection in Extended Reality applications.
As part of our latest competition, we provide a brand-new dataset, which
maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets,
but with first-order Ambisonics recordings from multiple reverberant simulated
environments. Moreover, we start exploring an audio-visual scenario by
providing images of these environments, as perceived by the different
microphone positions and orientations. We also propose updated baseline models
for both tasks that can now support audio-image couples as input and a
supporting API to replicate our results. Finally, we present the results of the
participants. Further details about the challenge are available at
https://www.l3das.com/icassp2023.</div><div><a href='http://arxiv.org/abs/2402.09245v1'>2402.09245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11827v1")'>Sound Event Detection and Localization with Distance Estimation</div>
<div id='2403.11827v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:34:16Z</div><div>Authors: Daniel Aleksander Krause, Archontis Politis, Annamaria Mesaros</div><div style='padding-top: 10px; width: 80ex'>Sound Event Detection and Localization (SELD) is a combined task of
identifying sound events and their corresponding direction-of-arrival (DOA).
While this task has numerous applications and has been extensively researched
in recent years, it fails to provide full information about the sound source
position. In this paper, we overcome this problem by extending the task to
Sound Event Detection, Localization with Distance Estimation (3D SELD). We
study two ways of integrating distance estimation within the SELD core - a
multi-task approach, in which the problem is tackled by a separate model
output, and a single-task approach obtained by extending the multi-ACCDOA
method to include distance information. We investigate both methods for the
Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial
Soundscapes 2023. Moreover, our study involves experiments on the loss function
related to the distance estimation part. Our results show that it is possible
to perform 3D SELD without any degradation of performance in sound event
detection and DOA estimation.</div><div><a href='http://arxiv.org/abs/2403.11827v1'>2403.11827v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12238v1")'>Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound
  Event Localization and Detection in Realistic Rooms</div>
<div id='2401.12238v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T19:01:13Z</div><div>Authors: Iran R. Roman, Christopher Ick, Sivan Ding, Adrian S. Roman, Brian McFee, Juan P. Bello</div><div style='padding-top: 10px; width: 80ex'>Sound event localization and detection (SELD) is an important task in machine
listening. Major advancements rely on simulated data with sound events in
specific rooms and strong spatio-temporal labels. SELD data is simulated by
convolving spatialy-localized room impulse responses (RIRs) with sound
waveforms to place sound events in a soundscape. However, RIRs require manual
collection in specific rooms. We present SpatialScaper, a library for SELD data
simulation and augmentation. Compared to existing tools, SpatialScaper emulates
virtual rooms via parameters such as size and wall absorption. This allows for
parameterized placement (including movement) of foreground and background sound
sources. SpatialScaper also includes data augmentation pipelines that can be
applied to existing SELD data. As a case study, we use SpatialScaper to add
rooms to the DCASE SELD data. Training a model with our data led to progressive
performance improves as a direct function of acoustic diversity. These results
show that SpatialScaper is valuable to train robust SELD models.</div><div><a href='http://arxiv.org/abs/2401.12238v1'>2401.12238v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13957v1")'>Advancing Audio Fingerprinting Accuracy Addressing Background Noise and
  Distortion Challenges</div>
<div id='2402.13957v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T17:37:30Z</div><div>Authors: Navin Kamuni, Sathishkumar Chintala, Naveen Kunchakuri, Jyothi Swaroop Arlagadda Narasimharaju, Venkat Kumar</div><div style='padding-top: 10px; width: 80ex'>Audio fingerprinting, exemplified by pioneers like Shazam, has transformed
digital audio recognition. However, existing systems struggle with accuracy in
challenging conditions, limiting broad applicability. This research proposes an
AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built
on the Dejavu Project's foundations, the study emphasizes real-world scenario
simulations with diverse background noises and distortions. Signal processing,
central to Dejavu's model, includes the Fast Fourier Transform, spectrograms,
and peak extraction. The "constellation" concept and fingerprint hashing enable
unique song identification. Performance evaluation attests to 100% accuracy
within a 5-second audio input, with a system showcasing predictable matching
speed for efficiency. Storage analysis highlights the critical space-speed
trade-off for practical implementation. This research advances audio
fingerprinting's adaptability, addressing challenges in varied environments and
applications.</div><div><a href='http://arxiv.org/abs/2402.13957v1'>2402.13957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01369v1")'>A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech
  Enhancement</div>
<div id='2403.01369v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T02:05:17Z</div><div>Authors: Ravi Shankar, Ke Tan, Buye Xu, Anurag Kumar</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learned models have been found to be very effective for
certain speech tasks such as automatic speech recognition, speaker
identification, keyword spotting and others. While the features are undeniably
useful in speech recognition and associated tasks, their utility in speech
enhancement systems is yet to be firmly established, and perhaps not properly
understood. In this paper, we investigate the uses of SSL representations for
single-channel speech enhancement in challenging conditions and find that they
add very little value for the enhancement task. Our constraints are designed
around on-device real-time speech enhancement -- model is causal, the compute
footprint is small. Additionally, we focus on low SNR conditions where such
models struggle to provide good enhancement. In order to systematically examine
how SSL representations impact performance of such enhancement models, we
propose a variety of techniques to utilize these embeddings which include
different forms of knowledge-distillation and pre-training.</div><div><a href='http://arxiv.org/abs/2403.01369v1'>2403.01369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07085v1")'>Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and
  Phoneme Duration for Multi-Speaker Speech Synthesis</div>
<div id='2402.07085v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T02:26:43Z</div><div>Authors: Kenichi Fujita, Atsushi Ando, Yusuke Ijima</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a speech rhythm-based method for speaker embeddings to
model phoneme duration using a few utterances by the target speaker. Speech
rhythm is one of the essential factors among speaker characteristics, along
with acoustic features such as F0, for reproducing individual utterances in
speech synthesis. A novel feature of the proposed method is the rhythm-based
embeddings extracted from phonemes and their durations, which are known to be
related to speaking rhythm. They are extracted with a speaker identification
model similar to the conventional spectral feature-based one. We conducted
three experiments, speaker embeddings generation, speech synthesis with
generated embeddings, and embedding space analysis, to evaluate the
performance. The proposed method demonstrated a moderate speaker identification
performance (15.2% EER), even with only phonemes and their duration
information. The objective and subjective evaluation results demonstrated that
the proposed method can synthesize speech with speech rhythm closer to the
target speaker than the conventional method. We also visualized the embeddings
to evaluate the relationship between the distance of the embeddings and the
perceptual similarity. The visualization of the embedding space and the
relation analysis between the closeness indicated that the distribution of
embeddings reflects the subjective and objective similarity.</div><div><a href='http://arxiv.org/abs/2402.07085v1'>2402.07085v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17775v1")'>Wavelet Scattering Transform for Bioacustics: Application to Watkins
  Marine Mammal Sound Database</div>
<div id='2402.17775v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T11:36:23Z</div><div>Authors: Davide Carbone, Alessandro Licciardi</div><div style='padding-top: 10px; width: 80ex'>Marine mammal communication is a complex field, hindered by the diversity of
vocalizations and environmental factors. The Watkins Marine Mammal Sound
Database (WMMD) is an extensive labeled dataset used in machine learning
applications. However, the methods for data preparation, preprocessing, and
classification found in the literature are quite disparate. This study first
focuses on a brief review of the state-of-the-art benchmarks on the dataset,
with an emphasis on clarifying data preparation and preprocessing methods.
Subsequently, we propose the application of the Wavelet Scattering Transform
(WST) in place of standard methods based on the Short-Time Fourier Transform
(STFT). The study also tackles a classification task using an ad-hoc deep
architecture with residual layers. We outperform the existing classification
architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram
preprocessing, effectively reducing by half the number of misclassified
samples, and reaching a top accuracy of $96\%$.</div><div><a href='http://arxiv.org/abs/2402.17775v1'>2402.17775v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02522v1")'>HeAR -- Health Acoustic Representations</div>
<div id='2403.02522v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T22:26:25Z</div><div>Authors: Sebastien Baur, Zaid Nabulsi, Wei-Hung Weng, Jake Garrison, Louis Blankemeier, Sam Fishman, Christina Chen, Sujay Kakarmath, Minyoi Maimbolwa, Nsala Sanjase, Brian Shuma, Yossi Matias, Greg S. Corrado, Shwetak Patel, Shravya Shetty, Shruthi Prabhakara, Monde Muyoyeta, Diego Ardila</div><div style='padding-top: 10px; width: 80ex'>Health acoustic sounds such as coughs and breaths are known to contain useful
health signals with significant potential for monitoring health and disease,
yet are underexplored in the medical machine learning community. The existing
deep learning systems for health acoustics are often narrowly trained and
evaluated on a single task, which is limited by data and may hinder
generalization to other tasks. To mitigate these gaps, we develop HeAR, a
scalable self-supervised learning-based deep learning system using masked
autoencoders trained on a large dataset of 313 million two-second long audio
clips. Through linear probes, we establish HeAR as a state-of-the-art health
audio embedding model on a benchmark of 33 health acoustic tasks across 6
datasets. By introducing this work, we hope to enable and accelerate further
health acoustics research.</div><div><a href='http://arxiv.org/abs/2403.02522v1'>2403.02522v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01145v3")'>HAAQI-Net: A non-intrusive neural music quality assessment model for
  hearing aids</div>
<div id='2401.01145v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T10:55:01Z</div><div>Authors: Dyah A. M. G. Wisnu, Epri W. Pratiwi, Stefano Rini, Ryandhimas E. Zezario, Hsin-Min Wang, Yu Tsao</div><div style='padding-top: 10px; width: 80ex'>This paper introduces HAAQI-Net, a non-intrusive deep learning model for
music quality assessment tailored to hearing aid users. In contrast to
traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net
utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It
takes an assessed music sample and a hearing loss pattern as input, generating
a predicted HAAQI score. The model employs the pre-trained Bidirectional
Encoder representation from Audio Transformers (BEATs) for acoustic feature
extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a
Longitudinal Concordance Correlation (LCC) of 0.9368, Spearman's Rank
Correlation Coefficient (SRCC) of 0.9486, and Mean Squared Error (MSE) of
0.0064. Notably, this high performance comes with a substantial reduction in
inference time: from 62.52 seconds (by HAAQI) to 2.54 seconds (by HAAQI-Net),
serving as an efficient music quality assessment model for hearing aid users.</div><div><a href='http://arxiv.org/abs/2401.01145v3'>2401.01145v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14982v2")'>Human Brain Exhibits Distinct Patterns When Listening to Fake Versus
  Real Audio: Preliminary Evidence</div>
<div id='2402.14982v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T21:44:58Z</div><div>Authors: Mahsa Salehi, Kalin Stefanov, Ehsan Shareghi</div><div style='padding-top: 10px; width: 80ex'>In this paper we study the variations in human brain activity when listening
to real and fake audio. Our preliminary results suggest that the
representations learned by a state-of-the-art deepfake audio detection
algorithm, do not exhibit clear distinct patterns between real and fake audio.
In contrast, human brain activity, as measured by EEG, displays distinct
patterns when individuals are exposed to fake versus real audio. This
preliminary evidence enables future research directions in areas such as
deepfake audio detection.</div><div><a href='http://arxiv.org/abs/2402.14982v2'>2402.14982v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16321v1")'>Self-Supervised Speech Quality Estimation and Enhancement Using Only
  Clean Speech</div>
<div id='2402.16321v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T06:01:38Z</div><div>Authors: Szu-Wei Fu, Kuo-Hsuan Hung, Yu Tsao, Yu-Chiang Frank Wang</div><div style='padding-top: 10px; width: 80ex'>Speech quality estimation has recently undergone a paradigm shift from
human-hearing expert designs to machine-learning models. However, current
models rely mainly on supervised learning, which is time-consuming and
expensive for label collection. To solve this problem, we propose VQScore, a
self-supervised metric for evaluating speech based on the quantization error of
a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE
relies on clean speech; hence, large quantization errors can be expected when
the speech is distorted. To further improve correlation with real quality
scores, domain knowledge of speech processing is incorporated into the model
design. We found that the vector quantization mechanism could also be used for
self-supervised speech enhancement (SE) model training. To improve the
robustness of the encoder for SE, a novel self-distillation mechanism combined
with adversarial training is introduced. In summary, the proposed speech
quality estimation method and enhancement models require only clean speech for
training without any label requirements. Experimental results show that the
proposed VQScore and enhancement model are competitive with supervised
baselines. The code will be released after publication.</div><div><a href='http://arxiv.org/abs/2402.16321v1'>2402.16321v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14205v1")'>Compression Robust Synthetic Speech Detection Using Patched Spectrogram
  Transformer</div>
<div id='2402.14205v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T01:18:55Z</div><div>Authors: Amit Kumar Singh Yadav, Ziyue Xiang, Kratika Bhagtani, Paolo Bestagini, Stefano Tubaro, Edward J. Delp</div><div style='padding-top: 10px; width: 80ex'>Many deep learning synthetic speech generation tools are readily available.
The use of synthetic speech has caused financial fraud, impersonation of
people, and misinformation to spread. For this reason forensic methods that can
detect synthetic speech have been proposed. Existing methods often overfit on
one dataset and their performance reduces substantially in practical scenarios
such as detecting synthetic speech shared on social platforms. In this paper we
propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a
synthetic speech detector that converts a time domain speech signal to a
mel-spectrogram and processes it in patches using a transformer neural network.
We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our
experiments show that PS3DT performs well on ASVspoof2019 dataset compared to
other approaches using spectrogram for synthetic speech detection. We also
investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT
generalizes well than several existing methods on detecting synthetic speech
from an out-of-distribution dataset. We also evaluate robustness of PS3DT to
detect telephone quality synthetic speech and synthetic speech shared on social
platforms (compressed speech). PS3DT is robust to compression and can detect
telephone quality synthetic speech better than several existing methods.</div><div><a href='http://arxiv.org/abs/2402.14205v1'>2402.14205v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07937v1")'>Speech Robust Bench: A Robustness Benchmark For Speech Recognition</div>
<div id='2403.07937v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T08:10:29Z</div><div>Authors: Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila, Nicolas Kourtellis</div><div style='padding-top: 10px; width: 80ex'>As Automatic Speech Recognition (ASR) models become ever more pervasive, it
is important to ensure that they make reliable predictions under corruptions
present in the physical and digital world. We propose Speech Robust Bench
(SRB), a comprehensive benchmark for evaluating the robustness of ASR models to
diverse corruptions. SRB is composed of 69 input perturbations which are
intended to simulate various corruptions that ASR models may encounter in the
physical and digital world. We use SRB to evaluate the robustness of several
state-of-the-art ASR models and observe that model size and certain modeling
choices such as discrete representations, and self-training appear to be
conducive to robustness. We extend this analysis to measure the robustness of
ASR models on data from various demographic subgroups, namely English and
Spanish speakers, and males and females, and observed noticeable disparities in
the model's robustness across subgroups. We believe that SRB will facilitate
future research towards robust ASR models, by making it easier to conduct
comprehensive and comparable robustness evaluations.</div><div><a href='http://arxiv.org/abs/2403.07937v1'>2403.07937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13723v1")'>The Effect of Batch Size on Contrastive Self-Supervised Speech
  Representation Learning</div>
<div id='2402.13723v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T11:35:19Z</div><div>Authors: Nik Vaessen, David A. van Leeuwen</div><div style='padding-top: 10px; width: 80ex'>Foundation models in speech are often trained using many GPUs, which
implicitly leads to large effective batch sizes. In this paper we study the
effect of batch size on pre-training, both in terms of statistics that can be
monitored during training, and in the effect on the performance of a downstream
fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes
of speech we show that, for a fixed amount of iterations, larger batch sizes
result in better pre-trained models. However, there is lower limit for
stability, and an upper limit for effectiveness. We then show that the quality
of the pre-trained model depends mainly on the amount of speech data seen
during training, i.e., on the product of batch size and number of iterations.
All results are produced with an independent implementation of the wav2vec 2.0
architecture, which to a large extent reproduces the results of the original
work (arXiv:2006.11477). Our extensions can help researchers choose effective
operating conditions when studying self-supervised learning in speech, and
hints towards benchmarking self-supervision with a fixed amount of seen data.
Code and model checkpoints are available at
https://github.com/nikvaessen/w2v2-batch-size.</div><div><a href='http://arxiv.org/abs/2402.13723v1'>2402.13723v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06980v1")'>Joint Unsupervised and Supervised Training for Automatic Speech
  Recognition via Bilevel Optimization</div>
<div id='2401.06980v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T05:01:47Z</div><div>Authors: A F M Saif, Xiaodong Cui, Han Shen, Songtao Lu, Brian Kingsbury, Tianyi Chen</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present a novel bilevel optimization-based training
approach to training acoustic models for automatic speech recognition (ASR)
tasks that we term {bi-level joint unsupervised and supervised training
(BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an
unsupervised loss and a supervised loss respectively, leveraging recent
advances in penalty-based bilevel optimization to solve this challenging ASR
problem with affordable complexity and rigorous convergence guarantees.} To
evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2
datasets have been conducted. BL-JUST achieves superior performance over the
commonly used pre-training followed by fine-tuning strategy.</div><div><a href='http://arxiv.org/abs/2401.06980v1'>2401.06980v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00293v1")'>Efficient Adapter Tuning of Pre-trained Speech Models for Automatic
  Speaker Verification</div>
<div id='2403.00293v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T05:32:14Z</div><div>Authors: Mufan Sang, John H. L. Hansen</div><div style='padding-top: 10px; width: 80ex'>With excellent generalization ability, self-supervised speech models have
shown impressive performance on various downstream speech tasks in the
pre-training and fine-tuning paradigm. However, as the growing size of
pre-trained models, fine-tuning becomes practically unfeasible due to heavy
computation and storage overhead, as well as the risk of overfitting. Adapters
are lightweight modules inserted into pre-trained models to facilitate
parameter-efficient adaptation. In this paper, we propose an effective adapter
framework designed for adapting self-supervised speech models to the speaker
verification task. With a parallel adapter design, our proposed framework
inserts two types of adapters into the pre-trained model, allowing the
adaptation of latent features within intermediate Transformer layers and output
embeddings from all Transformer layers. We conduct comprehensive experiments to
validate the efficiency and effectiveness of the proposed framework.
Experimental results on the VoxCeleb1 dataset demonstrate that the proposed
adapters surpass fine-tuning and other parameter-efficient transfer learning
methods, achieving superior performance while updating only 5% of the
parameters.</div><div><a href='http://arxiv.org/abs/2403.00293v1'>2403.00293v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12220v1")'>Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic
  Forgetting</div>
<div id='2402.12220v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:26:19Z</div><div>Authors: Haolin Chen, Philip N. Garner</div><div style='padding-top: 10px; width: 80ex'>Although motivated by the adaptation of text-to-speech synthesis models, we
argue that more generic parameter-efficient fine-tuning (PEFT) is an
appropriate framework to do such adaptation. However, catastrophic forgetting
remains an issue with PEFT, damaging the pre-trained model's inherent
capabilities. We demonstrate that existing Bayesian learning techniques can be
applied to PEFT to prevent catastrophic forgetting as long as the parameter
shift of the fine-tuned layers can be calculated differentiably. In a
principled series of experiments on language modeling and speech synthesis
tasks, we utilize established Laplace approximations, including diagonal and
Kronecker factored approaches, to regularize PEFT with the low-rank adaptation
(LoRA) and compare their performance in pre-training knowledge preservation.
Our results demonstrate that catastrophic forgetting can be overcome by our
methods without degrading the fine-tuning performance, and using the Kronecker
factored approximations produces a better preservation of the pre-training
knowledge than the diagonal ones.</div><div><a href='http://arxiv.org/abs/2402.12220v1'>2402.12220v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01692v1")'>Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by
  Self-Supervised Representation Mixing and Embedding Initialization</div>
<div id='2402.01692v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T21:55:34Z</div><div>Authors: Wei-Ping Huang, Sung-Feng Huang, Hung-yi Lee</div><div style='padding-top: 10px; width: 80ex'>This paper presents an effective transfer learning framework for language
adaptation in text-to-speech systems, with a focus on achieving language
adaptation using minimal labeled and unlabeled data. While many works focus on
reducing the usage of labeled data, very few consider minimizing the usage of
unlabeled data. By utilizing self-supervised features in the pretraining stage,
replacing the noisy portion of pseudo labels with these features during
fine-tuning, and incorporating an embedding initialization trick, our method
leverages more information from unlabeled data compared to conventional
approaches. Experimental results show that our framework is able to synthesize
intelligible speech in unseen languages with only 4 utterances of labeled data
and 15 minutes of unlabeled data. Our methodology continues to surpass
conventional techniques, even when a greater volume of data is accessible.
These findings highlight the potential of our data-efficient language
adaptation framework.</div><div><a href='http://arxiv.org/abs/2402.01692v1'>2402.01692v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01796v1")'>Exploring transfer learning for pathological speech feature prediction:
  Impact of layer selection</div>
<div id='2402.01796v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T05:09:42Z</div><div>Authors: Daniela A. Wiepert, Rene L. Utianski, Joseph R. Duffy, John L. Stricker, Leland R. Barnard, David T. Jones, Hugo Botha</div><div style='padding-top: 10px; width: 80ex'>There is interest in leveraging AI to conduct automatic, objective
assessments of clinical speech, in turn facilitating diagnosis and treatment of
speech disorders. We explore transfer learning, focusing on the impact of layer
selection, for the downstream task of predicting the presence of pathological
speech. We find that selecting an optimal layer offers large performance
improvements (12.4% average increase in balanced accuracy), though the best
layer varies by predicted feature and does not always generalize well to unseen
data. A learned weighted sum offers comparable performance to the average best
layer in-distribution and has better generalization for out-of-distribution
data.</div><div><a href='http://arxiv.org/abs/2402.01796v1'>2402.01796v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07802v1")'>Boosting keyword spotting through on-device learnable user speech
  characteristics</div>
<div id='2403.07802v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:41:31Z</div><div>Authors: Cristian Cioflan, Lukas Cavigelli, Luca Benini</div><div style='padding-top: 10px; width: 80ex'>Keyword spotting systems for always-on TinyML-constrained applications
require on-site tuning to boost the accuracy of offline trained classifiers
when deployed in unseen inference conditions. Adapting to the speech
peculiarities of target users requires many in-domain samples, often
unavailable in real-world scenarios. Furthermore, current on-device learning
techniques rely on computationally intensive and memory-hungry backbone update
schemes, unfit for always-on, battery-powered devices. In this work, we propose
a novel on-device learning architecture, composed of a pretrained backbone and
a user-aware embedding learning the user's speech characteristics. The
so-generated features are fused and used to classify the input utterance. For
domain shifts generated by unseen speakers, we measure error rate reductions of
up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google
Speech Commands dataset, through the inexpensive update of the user
projections. We moreover demonstrate the few-shot learning capabilities of our
proposed architecture in sample- and class-scarce learning conditions. With
23.7 kparameters and 1 MFLOP per epoch required for on-device training, our
system is feasible for TinyML applications aimed at battery-powered
microcontrollers.</div><div><a href='http://arxiv.org/abs/2403.07802v1'>2403.07802v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10549v1")'>On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge
  Embedded Systems</div>
<div id='2403.10549v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T19:54:35Z</div><div>Authors: Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel de Prado, Luca Benini</div><div style='padding-top: 10px; width: 80ex'>Keyword spotting accuracy degrades when neural networks are exposed to noisy
environments. On-site adaptation to previously unseen noise is crucial to
recovering accuracy loss, and on-device learning is required to ensure that the
adaptation process happens entirely on the edge device. In this work, we
propose a fully on-device domain adaptation system achieving up to 14% accuracy
gains over already-robust keyword spotting models. We enable on-device learning
with less than 10 kB of memory, using only 100 labeled utterances to recover 5%
accuracy after adapting to the complex speech noise. We demonstrate that domain
adaptation can be achieved on ultra-low-power microcontrollers with as little
as 806 mJ in only 14 s on always-on, battery-operated devices.</div><div><a href='http://arxiv.org/abs/2403.10549v1'>2403.10549v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14888v1")'>Efficient data selection employing Semantic Similarity-based Graph
  Structures for model training</div>
<div id='2402.14888v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:43:53Z</div><div>Authors: Roxana Petcu, Subhadeep Maji</div><div style='padding-top: 10px; width: 80ex'>Recent developments in natural language processing (NLP) have highlighted the
need for substantial amounts of data for models to capture textual information
accurately. This raises concerns regarding the computational resources and time
required for training such models. This paper introduces Semantics for data
SAliency in Model performance Estimation (SeSaME). It is an efficient data
sampling mechanism solely based on textual information without passing the data
through a compute-heavy model or other intensive pre-processing
transformations. The application of this approach is demonstrated in the use
case of low-resource automated speech recognition (ASR) models, which
excessively rely on text-to-speech (TTS) calls when using augmented data.
SeSaME learns to categorize new incoming data points into speech recognition
difficulty buckets by employing semantic similarity-based graph structures and
discrete ASR information from homophilous neighbourhoods through message
passing. The results indicate reliable projections of ASR performance, with a
93% accuracy increase when using the proposed method compared to random
predictions, bringing non-trivial information on the impact of textual
representations in speech models. Furthermore, a series of experiments show
both the benefits and challenges of using the ASR information on incoming data
to fine-tune the model. We report a 7% drop in validation loss compared to
random sampling, 7% WER drop with non-local aggregation when evaluating against
a highly difficult dataset, and 1.8% WER drop with local aggregation and high
semantic similarity between datasets.</div><div><a href='http://arxiv.org/abs/2402.14888v1'>2402.14888v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03251v1")'>TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in
  End-to-End ASR</div>
<div id='2401.03251v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T16:29:13Z</div><div>Authors: Nagarathna Ravi, Thishyan Raj T, Vipul Arora</div><div style='padding-top: 10px; width: 80ex'>Confidence estimation of predictions from an End-to-End (E2E) Automatic
Speech Recognition (ASR) model benefits ASR's downstream and upstream tasks.
Class-probability-based confidence scores do not accurately represent the
quality of overconfident ASR predictions. An ancillary Confidence Estimation
Model (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use
binary target scores for CEM training. However, the binary labels do not reveal
the granular information of predicted words, such as temporal alignment between
reference and hypothesis and whether the predicted word is entirely incorrect
or contains spelling errors. Addressing this issue, we propose a novel
Temporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address
the data imbalance of target scores while training CEM, we use shrinkage loss
to focus on hard-to-learn data points and minimise the impact of easily learned
data points. We conduct experiments with ASR models trained in three languages,
namely Hindi, Tamil, and Kannada, with varying training data sizes. Experiments
show that TeLeS generalises well across domains. To demonstrate the
applicability of the proposed method, we formulate a TeLeS-based Acquisition
(TeLeS-A) function for sampling uncertainty in active learning. We observe a
significant reduction in the Word Error Rate (WER) as compared to SOTA methods.</div><div><a href='http://arxiv.org/abs/2401.03251v1'>2401.03251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07506v1")'>SeMaScore : a new evaluation metric for automatic speech recognition
  tasks</div>
<div id='2401.07506v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T07:13:43Z</div><div>Authors: Zitha Sasindran, Harsha Yelchuri, T. V. Prabhakar</div><div style='padding-top: 10px; width: 80ex'>In this study, we present SeMaScore, generated using a segment-wise mapping
and scoring algorithm that serves as an evaluation metric for automatic speech
recognition tasks. SeMaScore leverages both the error rate and a more robust
similarity score. We show that our algorithm's score generation improves upon
the state-of-the-art BERTscore. Our experimental results show that SeMaScore
corresponds well with expert human assessments, signal-to-noise ratio levels,
and other natural language metrics. We outperform BERTscore by 41x in metric
computation speed. Overall, we demonstrate that SeMaScore serves as a more
dependable evaluation metric, particularly in real-world situations involving
atypical speech patterns.</div><div><a href='http://arxiv.org/abs/2401.07506v1'>2401.07506v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01778v1")'>Introduction to speech recognition</div>
<div id='2402.01778v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:54:15Z</div><div>Authors: Gabriel Dauphin</div><div style='padding-top: 10px; width: 80ex'>This document contains lectures and practical experimentations using Matlab
and implementing a system which is actually correctly classifying three words
(one, two and three) with the help of a very small database. To achieve this
performance, it uses speech modeling specificities, powerful computer
algorithms (dynamic time warping and Dijktra's algorithm) and machine learning
(nearest neighbor). This document introduces also some machine learning
evaluation metrics.</div><div><a href='http://arxiv.org/abs/2402.01778v1'>2402.01778v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15018v1")'>Enhancement of a Text-Independent Speaker Verification System by using
  Feature Combination and Parallel-Structure Classifiers</div>
<div id='2401.15018v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T17:19:59Z</div><div>Authors: Kerlos Atia Abdalmalak, Ascensión Gallardo-Antol'in</div><div style='padding-top: 10px; width: 80ex'>Speaker Verification (SV) systems involve mainly two individual stages:
feature extraction and classification. In this paper, we explore these two
modules with the aim of improving the performance of a speaker verification
system under noisy conditions. On the one hand, the choice of the most
appropriate acoustic features is a crucial factor for performing robust speaker
verification. The acoustic parameters used in the proposed system are: Mel
Frequency Cepstral Coefficients (MFCC), their first and second derivatives
(Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC),
Perceptual Linear Predictive (PLP), and Relative Spectral Transform -
Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison
of different combinations of the previous features is discussed. On the other
hand, the major weakness of a conventional Support Vector Machine (SVM)
classifier is the use of generic traditional kernel functions to compute the
distances among data points. However, the kernel function of an SVM has great
influence on its performance. In this work, we propose the combination of two
SVM-based classifiers with different kernel functions: Linear kernel and
Gaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR)
classifier. The combination is carried out by means of a parallel structure
approach, in which different voting rules to take the final decision are
considered. Results show that significant improvement in the performance of the
SV system is achieved by using the combined features with the combined
classifiers either with clean speech or in the presence of noise. Finally, to
enhance the system more in noisy environments, the inclusion of the multiband
noise removal technique as a preprocessing stage is proposed.</div><div><a href='http://arxiv.org/abs/2401.15018v1'>2401.15018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05717v2")'>Segment Boundary Detection via Class Entropy Measurements in
  Connectionist Phoneme Recognition</div>
<div id='2401.05717v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T07:47:10Z</div><div>Authors: Giampiero Salvi</div><div style='padding-top: 10px; width: 80ex'>This article investigates the possibility to use the class entropy of the
output of a connectionist phoneme recogniser to predict time boundaries between
phonetic classes. The rationale is that the value of the entropy should
increase in proximity of a transition between two segments that are well
modelled (known) by the recognition network since it is a measure of
uncertainty. The advantage of this measure is its simplicity as the posterior
probabilities of each class are available in connectionist phoneme recognition.
The entropy and a number of measures based on differentiation of the entropy
are used in isolation and in combination. The decision methods for predicting
the boundaries range from simple thresholds to neural network based procedure.
The different methods are compared with respect to their precision, measured in
terms of the ratio between the number C of predicted boundaries within 10 or 20
msec of the reference and the total number of predicted boundaries, and recall,
measured as the ratio between C and the total number of reference boundaries.</div><div><a href='http://arxiv.org/abs/2401.05717v2'>2401.05717v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06100v1")'>Automatic design optimization of preference-based subjective evaluation
  with online learning in crowdsourcing environment</div>
<div id='2403.06100v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T05:55:00Z</div><div>Authors: Yusuke Yasuda, Tomoki Toda</div><div style='padding-top: 10px; width: 80ex'>A preference-based subjective evaluation is a key method for evaluating
generative media reliably. However, its huge combinations of pairs prohibit it
from being applied to large-scale evaluation using crowdsourcing. To address
this issue, we propose an automatic optimization method for preference-based
subjective evaluation in terms of pair combination selections and allocation of
evaluation volumes with online learning in a crowdsourcing environment. We use
a preference-based online learning method based on a sorting algorithm to
identify the total order of evaluation targets with minimum sample volumes. Our
online learning algorithm supports parallel and asynchronous execution under
fixed-budget conditions required for crowdsourcing. Our experiment on
preference-based subjective evaluation of synthetic speech shows that our
method successfully optimizes the test by reducing pair combinations from 351
to 83 and allocating optimal evaluation volumes for each pair ranging from 30
to 663 without compromising evaluation accuracies and wasting budget
allocations.</div><div><a href='http://arxiv.org/abs/2403.06100v1'>2403.06100v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10937v1")'>Initial Decoding with Minimally Augmented Language Model for Improved
  Lattice Rescoring in Low Resource ASR</div>
<div id='2403.10937v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T14:34:31Z</div><div>Authors: Savitha Murthy, Dinkar Sitaram</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the problem of improving speech recognition accuracy
with lattice rescoring in low-resource languages where the baseline language
model is insufficient for generating inclusive lattices. We minimally augment
the baseline language model with word unigram counts that are present in a
larger text corpus of the target language but absent in the baseline. The
lattices generated after decoding with such an augmented baseline language
model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada)
relative word error reduction with our proposed method. This reduction in word
error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word
error reduction obtained by decoding with full Wikipedia text augmented
language mode while our approach consumes only 1/8th the memory. We demonstrate
that our method is comparable with various text selection-based language model
augmentation and also consistent for data sets of different sizes. Our approach
is applicable for training speech recognition systems under low resource
conditions where speech data and compute resources are insufficient, while
there is a large text corpus that is available in the target language. Our
research involves addressing the issue of out-of-vocabulary words of the
baseline in general and does not focus on resolving the absence of named
entities. Our proposed method is simple and yet computationally less expensive.</div><div><a href='http://arxiv.org/abs/2403.10937v1'>2403.10937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08992v1")'>Efficient Adapter Finetuning for Tail Languages in Streaming
  Multilingual ASR</div>
<div id='2401.08992v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T06:01:16Z</div><div>Authors: Junwen Bai, Bo Li, Qiujia Li, Tara N. Sainath, Trevor Strohman</div><div style='padding-top: 10px; width: 80ex'>The end-to-end ASR model is often desired in the streaming multilingual
scenario since it is easier to deploy and can benefit from pre-trained speech
models such as powerful foundation models. Meanwhile, the heterogeneous nature
and imbalanced data abundance of different languages may cause performance
degradation, leading to asynchronous peak performance for different languages
during training, especially on tail ones. Sometimes even the data itself may
become unavailable as a result of the enhanced privacy protection. Existing
work tend to significantly increase the model size or learn language-specific
decoders to accommodate each language separately. In this study, we explore
simple yet effective Language-Dependent Adapter (LDA) finetuning under a
cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for
tail languages in the streaming multilingual ASR. The adapter only accounts for
0.4% of the full model per language. It is plugged into the frozen foundation
model and is the only trainable module during the finetuning process with noisy
student training. The final model merges the adapter parameters from different
checkpoints for different languages. The model performance is validated on a
challenging multilingual dictation dataset, which includes 39 tail languages
across Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word error
rate reduction on average and up to 37.5% on a single locale. Furthermore, we
show that our parameter-efficient LDA can match the quality of the full model
finetuning, thus greatly alleviating the asynchronous peak performance issue.</div><div><a href='http://arxiv.org/abs/2401.08992v1'>2401.08992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04482v1")'>Continuously Learning New Words in Automatic Speech Recognition</div>
<div id='2401.04482v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T10:39:17Z</div><div>Authors: Christian Huber, Alexander Waibel</div><div style='padding-top: 10px; width: 80ex'>Despite recent advances, Automatic Speech Recognition (ASR) systems are still
far from perfect. Typical errors include acronyms, named entities and
domain-specific special words for which little or no data is available. To
address the problem of recognizing these words, we propose an self-supervised
continual learning approach. Given the audio of a lecture talk with
corresponding slides, we bias the model towards decoding new words from the
slides by using a memory-enhanced ASR model from previous work. Then, we
perform inference on the talk, collecting utterances that contain detected new
words into an adaptation dataset. Continual learning is then performed on this
set by adapting low-rank matrix weights added to each weight matrix of the
model. The whole procedure is iterated for many talks. We show that with this
approach, we obtain increasing performance on the new words when they occur
more frequently (more than 80% recall) while preserving the general performance
of the model.</div><div><a href='http://arxiv.org/abs/2401.04482v1'>2401.04482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08011v1")'>Gujarati-English Code-Switching Speech Recognition using ensemble
  prediction of spoken language</div>
<div id='2403.08011v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T18:21:20Z</div><div>Authors: Yash Sharma, Basil Abraham, Preethi Jyothi</div><div style='padding-top: 10px; width: 80ex'>An important and difficult task in code-switched speech recognition is to
recognize the language, as lots of words in two languages can sound similar,
especially in some accents. We focus on improving performance of end-to-end
Automatic Speech Recognition models by conditioning transformer layers on
language ID of words and character in the output in an per layer supervised
manner. To this end, we propose two methods of introducing language specific
parameters and explainability in the multi-head attention mechanism, and
implement a Temporal Loss that helps maintain continuity in input alignment.
Despite being unable to reduce WER significantly, our method shows promise in
predicting the correct language from just spoken data. We introduce
regularization in the language prediction by dropping LID in the sequence,
which helps align long repeated output sequences.</div><div><a href='http://arxiv.org/abs/2403.08011v1'>2403.08011v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07947v1")'>The evaluation of a code-switched Sepedi-English automatic speech
  recognition system</div>
<div id='2403.07947v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:11:28Z</div><div>Authors: Amanda Phaladi, Thipe Modipa</div><div style='padding-top: 10px; width: 80ex'>Speech technology is a field that encompasses various techniques and tools
used to enable machines to interact with speech, such as automatic speech
recognition (ASR), spoken dialog systems, and others, allowing a device to
capture spoken words through a microphone from a human speaker. End-to-end
approaches such as Connectionist Temporal Classification (CTC) and
attention-based methods are the most used for the development of ASR systems.
However, these techniques were commonly used for research and development for
many high-resourced languages with large amounts of speech data for training
and evaluation, leaving low-resource languages relatively underdeveloped. While
the CTC method has been successfully used for other languages, its
effectiveness for the Sepedi language remains uncertain. In this study, we
present the evaluation of the Sepedi-English code-switched automatic speech
recognition system. This end-to-end system was developed using the Sepedi
Prompted Code Switching corpus and the CTC approach. The performance of the
system was evaluated using both the NCHLT Sepedi test corpus and the Sepedi
Prompted Code Switching corpus. The model produced the lowest WER of 41.9%,
however, the model faced challenges in recognizing the Sepedi only text.</div><div><a href='http://arxiv.org/abs/2403.07947v1'>2403.07947v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16830v1")'>SKILL: Similarity-aware Knowledge distILLation for Speech
  Self-Supervised Learning</div>
<div id='2402.16830v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:56:42Z</div><div>Authors: Luca Zampierin, Ghouthi Boukli Hacene, Bac Nguyen, Mirco Ravanelli</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) has achieved remarkable success across various
speech-processing tasks. To enhance its efficiency, previous works often
leverage the use of compression techniques. A notable recent attempt is
DPHuBERT, which applies joint knowledge distillation (KD) and structured
pruning to learn a significantly smaller SSL model. In this paper, we
contribute to this research domain by introducing SKILL, a novel method that
conducts distillation across groups of layers instead of distilling individual
arbitrarily selected layers within the teacher network. The identification of
the layers to distill is achieved through a hierarchical clustering procedure
applied to layer similarity measures. Extensive experiments demonstrate that
our distilled version of WavLM Base+ not only outperforms DPHuBERT but also
achieves state-of-the-art results in the 30M parameters model class across
several SUPERB tasks.</div><div><a href='http://arxiv.org/abs/2402.16830v1'>2402.16830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01931v1")'>Digits micro-model for accurate and secure transactions</div>
<div id='2402.01931v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T22:01:27Z</div><div>Authors: Chirag Chhablani, Nikhita Sharma, Jordan Hosier, Vijay K. Gurbani</div><div style='padding-top: 10px; width: 80ex'>Automatic Speech Recognition (ASR) systems are used in the financial domain
to enhance the caller experience by enabling natural language understanding and
facilitating efficient and intuitive interactions. Increasing use of ASR
systems requires that such systems exhibit very low error rates. The
predominant ASR models to collect numeric data are large, general-purpose
commercial models -- Google Speech-to-text (STT), or Amazon Transcribe -- or
open source (OpenAI's Whisper). Such ASR models are trained on hundreds of
thousands of hours of audio data and require considerable resources to run.
Despite recent progress large speech recognition models, we highlight the
potential of smaller, specialized "micro" models. Such light models can be
trained perform well on number recognition specific tasks, competing with
general models like Whisper or Google STT while using less than 80 minutes of
training time and occupying at least an order of less memory resources. Also,
unlike larger speech recognition models, micro-models are trained on carefully
selected and curated datasets, which makes them highly accurate, agile, and
easy to retrain, while using low compute resources. We present our work on
creating micro models for multi-digit number recognition that handle diverse
speaking styles reflecting real-world pronunciation patterns. Our work
contributes to domain-specific ASR models, improving digit recognition
accuracy, and privacy of data. An added advantage, their low resource
consumption allows them to be hosted on-premise, keeping private data local
instead uploading to an external cloud. Our results indicate that our
micro-model makes less errors than the best-of-breed commercial or open-source
ASRs in recognizing digits (1.8% error rate of our best micro-model versus 5.8%
error rate of Whisper), and has a low memory footprint (0.66 GB VRAM for our
model versus 11 GB VRAM for Whisper).</div><div><a href='http://arxiv.org/abs/2402.01931v1'>2402.01931v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13076v1")'>Not All Weights Are Created Equal: Enhancing Energy Efficiency in
  On-Device Streaming Speech Recognition</div>
<div id='2402.13076v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:22:25Z</div><div>Authors: Yang Li, Yuan Shangguan, Yuhao Wang, Liangzhen Lai, Ernie Chang, Changsheng Zhao, Yangyang Shi, Vikas Chandra</div><div style='padding-top: 10px; width: 80ex'>Power consumption plays an important role in on-device streaming speech
recognition, as it has a direct impact on the user experience. This study
delves into how weight parameters in speech recognition models influence the
overall power consumption of these models. We discovered that the impact of
weight parameters on power consumption varies, influenced by factors including
how often they are invoked and their placement in memory. Armed with this
insight, we developed design guidelines aimed at optimizing on-device speech
recognition models. These guidelines focus on minimizing power use without
substantially affecting accuracy. Our method, which employs targeted
compression based on the varying sensitivities of weight parameters,
demonstrates superior performance compared to state-of-the-art compression
methods. It achieves a reduction in energy usage of up to 47% while maintaining
similar model accuracy and improving the real-time factor.</div><div><a href='http://arxiv.org/abs/2402.13076v1'>2402.13076v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14289v1")'>Speech foundation models on intelligibility prediction for
  hearing-impaired listeners</div>
<div id='2401.14289v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T18:26:52Z</div><div>Authors: Santiago Cuervo, Ricard Marxer</div><div style='padding-top: 10px; width: 80ex'>Speech foundation models (SFMs) have been benchmarked on many speech
processing tasks, often achieving state-of-the-art performance with minimal
adaptation. However, the SFM paradigm has been significantly less explored for
applications of interest to the speech perception community. In this paper we
present a systematic evaluation of 10 SFMs on one such application: Speech
intelligibility prediction. We focus on the non-intrusive setup of the Clarity
Prediction Challenge 2 (CPC2), where the task is to predict the percentage of
words correctly perceived by hearing-impaired listeners from speech-in-noise
recordings. We propose a simple method that learns a lightweight specialized
prediction head on top of frozen SFMs to approach the problem. Our results
reveal statistically significant differences in performance across SFMs. Our
method resulted in the winning submission in the CPC2, demonstrating its
promise for speech perception applications.</div><div><a href='http://arxiv.org/abs/2401.14289v1'>2401.14289v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12482v1")'>SECP: A Speech Enhancement-Based Curation Pipeline For Scalable
  Acquisition Of Clean Speech</div>
<div id='2402.12482v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:38:37Z</div><div>Authors: Adam Sabra, Cyprian Wronka, Michelle Mao, Samer Hijazi</div><div style='padding-top: 10px; width: 80ex'>As more speech technologies rely on a supervised deep learning approach with
clean speech as the ground truth, a methodology to onboard said speech at scale
is needed. However, this approach needs to minimize the dependency on human
listening and annotation, only requiring a human-in-the-loop when needed. In
this paper, we address this issue by outlining Speech Enhancement-based
Curation Pipeline (SECP) which serves as a framework to onboard clean speech.
This clean speech can then train a speech enhancement model, which can further
refine the original dataset and thus close the iterative loop. By running two
iterative rounds, we observe that enhanced output used as ground truth does not
degrade model performance according to $\Delta_{PESQ}$, a metric used in this
paper. We also show through comparative mean opinion score (CMOS) based
subjective tests that the highest and lowest bound of refined data is
perceptually better than the original data.</div><div><a href='http://arxiv.org/abs/2402.12482v1'>2402.12482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04245v1")'>A Study of Dropout-Induced Modality Bias on Robustness to Missing Video
  Frames for Audio-Visual Speech Recognition</div>
<div id='2403.04245v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T06:06:55Z</div><div>Authors: Yusheng Dai, Hang Chen, Jun Du, Ruoyu Wang, Shihao Chen, Jiefeng Ma, Haotian Wang, Chin-Hui Lee</div><div style='padding-top: 10px; width: 80ex'>Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to
be sensitive to missing video frames, performing even worse than
single-modality models. While applying the dropout technique to the video
modality enhances robustness to missing frames, it simultaneously results in a
performance loss when dealing with complete data input. In this paper, we
investigate this contrasting phenomenon from the perspective of modality bias
and reveal that an excessive modality bias on the audio caused by dropout is
the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH)
to systematically describe the relationship between modality bias and
robustness against missing modality in multimodal systems. Building on these
findings, we propose a novel Multimodal Distribution Approximation with
Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio
modality and to maintain performance and robustness simultaneously. Finally, to
address an entirely missing modality, we adopt adapters to dynamically switch
decision strategies. The effectiveness of our proposed approach is evaluated
and validated through a series of comprehensive experiments using the MISP2021
and MISP2022 datasets. Our code is available at
https://github.com/dalision/ModalBiasAVSR</div><div><a href='http://arxiv.org/abs/2403.04245v1'>2403.04245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12440v1")'>Post-Training Embedding Alignment for Decoupling Enrollment and Runtime
  Speaker Recognition Models</div>
<div id='2401.12440v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T02:19:31Z</div><div>Authors: Chenyang Gao, Brecht Desplanques, Chelsea J. -T. Ju, Aman Chadha, Andreas Stolcke</div><div style='padding-top: 10px; width: 80ex'>Automated speaker identification (SID) is a crucial step for the
personalization of a wide range of speech-enabled services. Typical SID systems
use a symmetric enrollment-verification framework with a single model to derive
embeddings both offline for voice profiles extracted from enrollment
utterances, and online from runtime utterances. Due to the distinct
circumstances of enrollment and runtime, such as different computation and
latency constraints, several applications would benefit from an asymmetric
enrollment-verification framework that uses different models for enrollment and
runtime embedding generation. To support this asymmetric SID where each of the
two models can be updated independently, we propose using a lightweight neural
network to map the embeddings from the two independent models to a shared
speaker embedding space. Our results show that this approach significantly
outperforms cosine scoring in a shared speaker logit space for models that were
trained with a contrastive loss on large datasets with many speaker identities.
This proposed Neural Embedding Speaker Space Alignment (NESSA) combined with an
asymmetric update of only one of the models delivers at least 60% of the
performance gain achieved by updating both models in the standard symmetric SID
approach.</div><div><a href='http://arxiv.org/abs/2401.12440v1'>2401.12440v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06404v1")'>Cosine Scoring with Uncertainty for Neural Speaker Embedding</div>
<div id='2403.06404v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T03:31:35Z</div><div>Authors: Qiongqiong Wang, Kong Aik Lee</div><div style='padding-top: 10px; width: 80ex'>Uncertainty modeling in speaker representation aims to learn the variability
present in speech utterances. While the conventional cosine-scoring is
computationally efficient and prevalent in speaker recognition, it lacks the
capability to handle uncertainty. To address this challenge, this paper
proposes an approach for estimating uncertainty at the speaker embedding
front-end and propagating it to the cosine scoring back-end. Experiments
conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the
proposed method in handling uncertainty arising from embedding estimation. It
achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF
compared to the conventional cosine similarity. It is also computationally
efficient in practice.</div><div><a href='http://arxiv.org/abs/2403.06404v1'>2403.06404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14286v1")'>Assessing the Robustness of Spectral Clustering for Deep Speaker
  Diarization</div>
<div id='2403.14286v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T10:49:54Z</div><div>Authors: Nikhil Raghav, Md Sahidullah</div><div style='padding-top: 10px; width: 80ex'>Clustering speaker embeddings is crucial in speaker diarization but hasn't
received as much focus as other components. Moreover, the robustness of speaker
diarization across various datasets hasn't been explored when the development
and evaluation data are from different domains. To bridge this gap, this study
thoroughly examines spectral clustering for both same-domain and cross-domain
speaker diarization. Our extensive experiments on two widely used corpora, AMI
and DIHARD, reveal the performance trend of speaker diarization in the presence
of domain mismatch. We observe that the performance difference between two
different domain conditions can be attributed to the role of spectral
clustering. In particular, keeping other modules unchanged, we show that
differences in optimal tuning parameters as well as speaker count estimation
originates due to the mismatch. This study opens several future directions for
speaker diarization research.</div><div><a href='http://arxiv.org/abs/2403.14286v1'>2403.14286v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01520v1")'>Low-Resource Cross-Domain Singing Voice Synthesis via Reduced
  Self-Supervised Speech Representations</div>
<div id='2402.01520v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T16:06:24Z</div><div>Authors: Panos Kakoulidis, Nikolaos Ellinas, Georgios Vamvoukakis, Myrsini Christidou, Alexandra Vioni, Georgia Maniati, Junkwang Oh, Gunu Jho, Inchul Hwang, Pirros Tsiakoulis, Aimilios Chalamandaris</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a singing voice synthesis model, Karaoker-SSL, that
is trained only on text and speech data as a typical multi-speaker acoustic
model. It is a low-resource pipeline that does not utilize any singing data
end-to-end, since its vocoder is also trained on speech data. Karaoker-SSL is
conditioned by self-supervised speech representations in an unsupervised
manner. We preprocess these representations by selecting only a subset of their
task-correlated dimensions. The conditioning module is indirectly guided to
capture style information during training by multi-tasking. This is achieved
with a Conformer-based module, which predicts the pitch from the acoustic
model's output. Thus, Karaoker-SSL allows singing voice synthesis without
reliance on hand-crafted and domain-specific features. There are also no
requirements for text alignments or lyrics timestamps. To refine the voice
quality, we employ a U-Net discriminator that is conditioned on the target
speaker and follows a Diffusion GAN training scheme.</div><div><a href='http://arxiv.org/abs/2402.01520v1'>2402.01520v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11780v1")'>Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural
  Language Prompt</div>
<div id='2403.11780v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:39:05Z</div><div>Authors: Yongqi Wang, Ruofan Hu, Rongjie Huang, Zhiqing Hong, Ruiqi Li, Wenrui Liu, Fuming You, Tao Jin, Zhou Zhao</div><div style='padding-top: 10px; width: 80ex'>Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio
quality and naturalness, yet they lack the capability to control the style
attributes of the synthesized singing explicitly. We propose Prompt-Singer, the
first SVS method that enables attribute controlling on singer gender, vocal
range and volume with natural language. We adopt a model architecture based on
a decoder-only transformer with a multi-scale hierarchy, and design a
range-melody decoupled pitch representation that enables text-conditioned vocal
range control while keeping melodic accuracy. Furthermore, we explore various
experiment settings, including different types of text representations, text
encoder fine-tuning, and introducing speech data to alleviate data scarcity,
aiming to facilitate further research. Experiments show that our model achieves
favorable controlling ability and audio quality. Audio samples are available at
http://prompt-singer.github.io .</div><div><a href='http://arxiv.org/abs/2403.11780v1'>2403.11780v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01792v1")'>CoMoSVC: Consistency Model-based Singing Voice Conversion</div>
<div id='2401.01792v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T15:47:17Z</div><div>Authors: Yiwen Lu, Zhen Ye, Wei Xue, Xu Tan, Qifeng Liu, Yike Guo</div><div style='padding-top: 10px; width: 80ex'>The diffusion-based Singing Voice Conversion (SVC) methods have achieved
remarkable performances, producing natural audios with high similarity to the
target timbre. However, the iterative sampling process results in slow
inference speed, and acceleration thus becomes crucial. In this paper, we
propose CoMoSVC, a consistency model-based SVC method, which aims to achieve
both high-quality generation and high-speed sampling. A diffusion-based teacher
model is first specially designed for SVC, and a student model is further
distilled under self-consistency properties to achieve one-step sampling.
Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a
significantly faster inference speed than the state-of-the-art (SOTA)
diffusion-based SVC system, it still achieves comparable or superior conversion
performance based on both subjective and objective metrics. Audio samples and
codes are available at https://comosvc.github.io/.</div><div><a href='http://arxiv.org/abs/2401.01792v1'>2401.01792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17133v1")'>A Proactive and Dual Prevention Mechanism against Illegal Song Covers
  empowered by Singing Voice Conversion</div>
<div id='2401.17133v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T16:07:44Z</div><div>Authors: Guangke Chen, Yedi Zhang, Fu Song, Ting Wang, Xiaoning Du, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>Singing voice conversion (SVC) automates song covers by converting one
singer's singing voice into another target singer's singing voice with the
original lyrics and melody. However, it raises serious concerns about copyright
and civil right infringements to multiple entities. This work proposes
SongBsAb, the first proactive approach to mitigate unauthorized SVC-based
illegal song covers. SongBsAb introduces human-imperceptible perturbations to
singing voices before releasing them, so that when they are used, the
generation process of SVC will be interfered, resulting in unexpected singing
voices. SongBsAb features a dual prevention effect by causing both (singer)
identity disruption and lyric disruption, namely, the SVC-covered singing voice
neither imitates the target singer nor preserves the original lyrics. To
improve the imperceptibility of perturbations, we refine a psychoacoustic
model-based loss with the backing track as an additional masker, a unique
accompanying element for singing voices compared to ordinary speech voices. To
enhance the transferability, we propose to utilize a frame-level interaction
reduction-based loss. We demonstrate the prevention effectiveness, utility, and
robustness of SongBsAb on three SVC models and two datasets using both
objective and human study-based subjective metrics. Our work fosters an
emerging research direction for mitigating illegal automated song covers.</div><div><a href='http://arxiv.org/abs/2401.17133v1'>2401.17133v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09298v1")'>More than words: Advancements and challenges in speech recognition for
  singing</div>
<div id='2403.09298v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T11:37:02Z</div><div>Authors: Anna Kruspe</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the challenges and advancements in speech recognition
for singing, a domain distinctly different from standard speech recognition.
Singing encompasses unique challenges, including extensive pitch variations,
diverse vocal styles, and background music interference. We explore key areas
such as phoneme recognition, language identification in songs, keyword
spotting, and full lyrics transcription. I will describe some of my own
experiences when performing research on these tasks just as they were starting
to gain traction, but will also show how recent developments in deep learning
and large-scale datasets have propelled progress in this field. My goal is to
illuminate the complexities of applying speech recognition to singing, evaluate
current capabilities, and outline future research directions.</div><div><a href='http://arxiv.org/abs/2403.09298v1'>2403.09298v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05064v1")'>Singer Identity Representation Learning using Self-Supervised Techniques</div>
<div id='2401.05064v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T10:41:38Z</div><div>Authors: Bernardo Torres, Stefan Lattner, Gaël Richard</div><div style='padding-top: 10px; width: 80ex'>Significant strides have been made in creating voice identity representations
using speech data. However, the same level of progress has not been achieved
for singing voices. To bridge this gap, we suggest a framework for training
singer identity encoders to extract representations suitable for various
singing-related tasks, such as singing voice similarity and synthesis. We
explore different self-supervised learning techniques on a large collection of
isolated vocal tracks and apply data augmentations during training to ensure
that the representations are invariant to pitch and content variations. We
evaluate the quality of the resulting representations on singer similarity and
identification tasks across multiple datasets, with a particular emphasis on
out-of-domain generalization. Our proposed framework produces high-quality
embeddings that outperform both speaker verification and wav2vec 2.0
pre-trained baselines on singing voice while operating at 44.1 kHz. We release
our code and trained models to facilitate further research on singing voice and
related areas.</div><div><a href='http://arxiv.org/abs/2401.05064v1'>2401.05064v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09200v1")'>A Real-Time Lyrics Alignment System Using Chroma And Phonetic Features
  For Classical Vocal Performance</div>
<div id='2401.09200v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T13:25:32Z</div><div>Authors: Jiyun Park, Sangeon Yong, Taegyun Kwon, Juhan Nam</div><div style='padding-top: 10px; width: 80ex'>The goal of real-time lyrics alignment is to take live singing audio as input
and to pinpoint the exact position within given lyrics on the fly. The task can
benefit real-world applications such as the automatic subtitling of live
concerts or operas. However, designing a real-time model poses a great
challenge due to the constraints of only using past input and operating within
a minimal latency. Furthermore, due to the lack of datasets for real-time
models for lyrics alignment, previous studies have mostly evaluated with
private in-house datasets, resulting in a lack of standard evaluation methods.
This paper presents a real-time lyrics alignment system for classical vocal
performances with two contributions. First, we improve the lyrics alignment
algorithm by finding an optimal combination of chromagram and phonetic
posteriorgram (PPG) that capture melodic and phonetics features of the singing
voice, respectively. Second, we recast the Schubert Winterreise Dataset (SWD)
which contains multiple performance renditions of the same pieces as an
evaluation set for the real-time lyrics alignment.</div><div><a href='http://arxiv.org/abs/2401.09200v1'>2401.09200v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12068v1")'>Resource-constrained stereo singing voice cancellation</div>
<div id='2401.12068v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:05:30Z</div><div>Authors: Clara Borrelli, James Rae, Dogac Basaran, Matt McVicar, Mehrez Souden, Matthias Mauch</div><div style='padding-top: 10px; width: 80ex'>We study the problem of stereo singing voice cancellation, a subtask of music
source separation, whose goal is to estimate an instrumental background from a
stereo mix. We explore how to achieve performance similar to large
state-of-the-art source separation networks starting from a small, efficient
model for real-time speech separation. Such a model is useful when memory and
compute are limited and singing voice processing has to run with limited
look-ahead. In practice, this is realised by adapting an existing mono model to
handle stereo input. Improvements in quality are obtained by tuning model
parameters and expanding the training set. Moreover, we highlight the benefits
a stereo model brings by introducing a new metric which detects attenuation
inconsistencies between channels. Our approach is evaluated using objective
offline metrics and a large-scale MUSHRA trial, confirming the effectiveness of
our techniques in stringent listening tests.</div><div><a href='http://arxiv.org/abs/2401.12068v1'>2401.12068v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17701v1")'>Real-time Low-latency Music Source Separation using Hybrid
  Spectrogram-TasNet</div>
<div id='2402.17701v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:26:33Z</div><div>Authors: Satvik Venkatesh, Arthur Benilov, Philip Coleman, Frederic Roskam</div><div style='padding-top: 10px; width: 80ex'>There have been significant advances in deep learning for music demixing in
recent years. However, there has been little attention given to how these
neural networks can be adapted for real-time low-latency applications, which
could be helpful for hearing aids, remixing audio streams and live shows. In
this paper, we investigate the various challenges involved in adapting current
demixing models in the literature for this use case. Subsequently, inspired by
the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain
Audio Separation Network HS-TasNet, which utilises the advantages of spectral
and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall
signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases
to 5.55 with additional training data. These results demonstrate the potential
of efficient demixing for real-time low-latency music applications.</div><div><a href='http://arxiv.org/abs/2402.17701v1'>2402.17701v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10460v1")'>Ultra-lightweight Neural Differential DSP Vocoder For High Quality
  Speech Synthesis</div>
<div id='2401.10460v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T02:51:00Z</div><div>Authors: Prabhav Agrawal, Thilo Koehler, Zhiping Xiu, Prashant Serai, Qing He</div><div style='padding-top: 10px; width: 80ex'>Neural vocoders model the raw audio waveform and synthesize high-quality
audio, but even the highly efficient ones, like MB-MelGAN and LPCNet, fail to
run real-time on a low-end device like a smartglass. A pure digital signal
processing (DSP) based vocoder can be implemented via lightweight fast Fourier
transforms (FFT), and therefore, is a magnitude faster than any neural vocoder.
A DSP vocoder often gets a lower audio quality due to consuming over-smoothed
acoustic model predictions of approximate representations for the vocal tract.
In this paper, we propose an ultra-lightweight differential DSP (DDSP) vocoder
that uses a jointly optimized acoustic model with a DSP vocoder, and learns
without an extracted spectral feature for the vocal tract. The model achieves
audio quality comparable to neural vocoders with a high average MOS of 4.36
while being efficient as a DSP vocoder. Our C++ implementation, without any
hardware-specific optimization, is at 15 MFLOPS, surpasses MB-MelGAN by 340
times in terms of FLOPS, and achieves a vocoder-only RTF of 0.003 and overall
RTF of 0.044 while running single-threaded on a 2GHz Intel Xeon CPU.</div><div><a href='http://arxiv.org/abs/2401.10460v1'>2401.10460v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03078v1")'>StreamVC: Real-Time Low-Latency Voice Conversion</div>
<div id='2401.03078v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T22:37:26Z</div><div>Authors: Yang Yang, Yury Kartynnik, Yunpeng Li, Jiuqiang Tang, Xing Li, George Sung, Matthias Grundmann</div><div style='padding-top: 10px; width: 80ex'>We present StreamVC, a streaming voice conversion solution that preserves the
content and prosody of any source speech while matching the voice timbre from
any target speech. Unlike previous approaches, StreamVC produces the resulting
waveform at low latency from the input signal even on a mobile platform, making
it applicable to real-time communication scenarios like calls and video
conferencing, and addressing use cases such as voice anonymization in these
scenarios. Our design leverages the architecture and training strategy of the
SoundStream neural audio codec for lightweight high-quality speech synthesis.
We demonstrate the feasibility of learning soft speech units causally, as well
as the effectiveness of supplying whitened fundamental frequency information to
improve pitch stability without leaking the source timbre information.</div><div><a href='http://arxiv.org/abs/2401.03078v1'>2401.03078v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06588v1")'>Dynamic Behaviour of Connectionist Speech Recognition with Strong
  Latency Constraints</div>
<div id='2401.06588v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T14:10:28Z</div><div>Authors: Giampiero Salvi</div><div style='padding-top: 10px; width: 80ex'>This paper describes the use of connectionist techniques in phonetic speech
recognition with strong latency constraints. The constraints are imposed by the
task of deriving the lip movements of a synthetic face in real time from the
speech signal, by feeding the phonetic string into an articulatory synthesiser.
Particular attention has been paid to analysing the interaction between the
time evolution model learnt by the multi-layer perceptrons and the transition
model imposed by the Viterbi decoder, in different latency conditions. Two
experiments were conducted in which the time dependencies in the language model
(LM) were controlled by a parameter. The results show a strong interaction
between the three factors involved, namely the neural network topology, the
length of time dependencies in the LM and the decoder latency.</div><div><a href='http://arxiv.org/abs/2401.06588v1'>2401.06588v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04965v1")'>ConvConcatNet: a deep convolutional neural network to reconstruct mel
  spectrogram from the EEG</div>
<div id='2401.04965v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T07:15:45Z</div><div>Authors: Xiran Xu, Bo Wang, Yujie Yan, Haolin Zhu, Zechen Zhang, Xihong Wu, Jing Chen</div><div style='padding-top: 10px; width: 80ex'>To investigate the processing of speech in the brain, simple linear models
are commonly used to establish a relationship between brain signals and speech
features. However, these linear models are ill-equipped to model a highly
dynamic and complex non-linear system like the brain. Although non-linear
methods with neural networks have been developed recently, reconstructing
unseen stimuli from unseen subjects' EEG is still a highly challenging task.
This work presents a novel method, ConvConcatNet, to reconstruct mel-specgrams
from EEG, in which the deep convolution neural network and extensive
concatenation operation were combined. With our ConvConcatNet model, the
Pearson correlation between the reconstructed and the target mel-spectrogram
can achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEG
Challenge. The codes and models to implement our work will be available on
Github: https://github.com/xuxiran/ConvConcatNet</div><div><a href='http://arxiv.org/abs/2401.04965v1'>2401.04965v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16996v1")'>Towards Decoding Brain Activity During Passive Listening of Speech</div>
<div id='2402.16996v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T20:04:01Z</div><div>Authors: Milán András Fodor, Tamás Gábor Csapó, Frigyes Viktor Arthur</div><div style='padding-top: 10px; width: 80ex'>The aim of the study is to investigate the complex mechanisms of speech
perception and ultimately decode the electrical changes in the brain accruing
while listening to speech. We attempt to decode heard speech from intracranial
electroencephalographic (iEEG) data using deep learning methods. The goal is to
aid the advancement of brain-computer interface (BCI) technology for speech
synthesis, and, hopefully, to provide an additional perspective on the
cognitive processes of speech perception. This approach diverges from the
conventional focus on speech production and instead chooses to investigate
neural representations of perceived speech. This angle opened up a complex
perspective, potentially allowing us to study more sophisticated neural
patterns. Leveraging the power of deep learning models, the research aimed to
establish a connection between these intricate neural activities and the
corresponding speech sounds. Despite the approach not having achieved a
breakthrough yet, the research sheds light on the potential of decoding neural
activity during speech perception. Our current efforts can serve as a
foundation, and we are optimistic about the potential of expanding and
improving upon this work to move closer towards more advanced BCIs, better
understanding of processes underlying perceived speech and its relation to
spoken speech.</div><div><a href='http://arxiv.org/abs/2402.16996v1'>2402.16996v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15733v1")'>ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for
  Arabic Characters</div>
<div id='2402.15733v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T06:05:15Z</div><div>Authors: Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim</div><div style='padding-top: 10px; width: 80ex'>Brain-Computer-Interface (BCI) has been a hot research topic in the last few
years that could help paralyzed people in their lives. Several researches were
done to classify electroencephalography (EEG) signals automatically into
English characters and words. Arabic language is one of the most used languages
around the world. However, to the best of our knowledge, there is no dataset
for Arabic characters EEG signals. In this paper, we have created an EEG
dataset for Arabic characters and named it ArEEG_Chars. Moreover, several
experiments were done on ArEEG_Chars using deep learning. Best results were
achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be
public for researchers.</div><div><a href='http://arxiv.org/abs/2402.15733v1'>2402.15733v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02865v1")'>On combining acoustic and modulation spectrograms in an attention
  LSTM-based system for speech intelligibility level classification</div>
<div id='2402.02865v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:26:28Z</div><div>Authors: Ascensión Gallardo-Antolín, Juan M. Montero</div><div style='padding-top: 10px; width: 80ex'>Speech intelligibility can be affected by multiple factors, such as noisy
environments, channel distortions or physiological issues. In this work, we
deal with the problem of automatic prediction of the speech intelligibility
level in this latter case. Starting from our previous work, a non-intrusive
system based on LSTM networks with attention mechanism designed for this task,
we present two main contributions. In the first one, it is proposed the use of
per-frame modulation spectrograms as input features, instead of compact
representations derived from them that discard important temporal information.
In the second one, two different strategies for the combination of per-frame
acoustic log-mel and modulation spectrograms into the LSTM framework are
explored: at decision level or late fusion and at utterance level or
Weighted-Pooling (WP) fusion. The proposed models are evaluated with the
UA-Speech database that contains dysarthric speech with different degrees of
severity. On the one hand, results show that attentional LSTM networks are able
to adequately modeling the modulation spectrograms sequences producing similar
classification rates as in the case of log-mel spectrograms. On the other hand,
both combination strategies, late and WP fusion, outperform the single-feature
systems, suggesting that per-frame log-mel and modulation spectrograms carry
complementary information for the task of speech intelligibility prediction,
than can be effectively exploited by the LSTM-based architectures, being the
system with the WP fusion strategy and Attention-Pooling the one that achieves
best results.</div><div><a href='http://arxiv.org/abs/2402.02865v1'>2402.02865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02850v1")'>An Attention Long Short-Term Memory based system for automatic
  classification of speech intelligibility</div>
<div id='2402.02850v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:03:28Z</div><div>Authors: Miguel Fernández-Díaz, Ascensión Gallardo-Antolín</div><div style='padding-top: 10px; width: 80ex'>Speech intelligibility can be degraded due to multiple factors, such as noisy
environments, technical difficulties or biological conditions. This work is
focused on the development of an automatic non-intrusive system for predicting
the speech intelligibility level in this latter case. The main contribution of
our research on this topic is the use of Long Short-Term Memory (LSTM) networks
with log-mel spectrograms as input features for this purpose. In addition, this
LSTM-based system is further enhanced by the incorporation of a simple
attention mechanism that is able to determine the more relevant frames to this
task. The proposed models are evaluated with the UA-Speech database that
contains dysarthric speech with different degrees of severity. Results show
that the attention LSTM architecture outperforms both, a reference Support
Vector Machine (SVM)-based system with hand-crafted features and a LSTM-based
system with Mean-Pooling.</div><div><a href='http://arxiv.org/abs/2402.02850v1'>2402.02850v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14692v1")'>PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a
  Diffusion Probabilistic Model</div>
<div id='2402.14692v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:47:15Z</div><div>Authors: Yukiya Hono, Kei Hashimoto, Yoshihiko Nankaku, Keiichi Tokuda</div><div style='padding-top: 10px; width: 80ex'>This paper presents a neural vocoder based on a denoising diffusion
probabilistic model (DDPM) incorporating explicit periodic signals as auxiliary
conditioning signals. Recently, DDPM-based neural vocoders have gained
prominence as non-autoregressive models that can generate high-quality
waveforms. The neural vocoders based on DDPM have the advantage of training
with a simple time-domain loss. In practical applications, such as singing
voice synthesis, there is a demand for neural vocoders to generate
high-fidelity speech waveforms with flexible pitch control. However,
conventional DDPM-based neural vocoders struggle to generate speech waveforms
under such conditions. Our proposed model aims to accurately capture the
periodic structure of speech waveforms by incorporating explicit periodic
signals. Experimental results show that our model improves sound quality and
provides better pitch control than conventional DDPM-based neural vocoders.</div><div><a href='http://arxiv.org/abs/2402.14692v1'>2402.14692v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02938v1")'>AIx Speed: Playback Speed Optimization Using Listening Comprehension of
  Speech Recognition Models</div>
<div id='2403.02938v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:08:52Z</div><div>Authors: Kazuki Kawamura, Jun Rekimoto</div><div style='padding-top: 10px; width: 80ex'>Since humans can listen to audio and watch videos at faster speeds than
actually observed, we often listen to or watch these pieces of content at
higher playback speeds to increase the time efficiency of content
comprehension. To further utilize this capability, systems that automatically
adjust the playback speed according to the user's condition and the type of
content to assist in more efficient comprehension of time-series content have
been developed. However, there is still room for these systems to further
extend human speed-listening ability by generating speech with playback speed
optimized for even finer time units and providing it to humans. In this study,
we determine whether humans can hear the optimized speech and propose a system
that automatically adjusts playback speed at units as small as phonemes while
ensuring speech intelligibility. The system uses the speech recognizer score as
a proxy for how well a human can hear a certain unit of speech and maximizes
the speech playback speed to the extent that a human can hear. This method can
be used to produce fast but intelligible speech. In the evaluation experiment,
we compared the speech played back at a constant fast speed and the flexibly
speed-up speech generated by the proposed method in a blind test and confirmed
that the proposed method produced speech that was easier to listen to.</div><div><a href='http://arxiv.org/abs/2403.02938v1'>2403.02938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08864v1")'>Binaural Angular Separation Network</div>
<div id='2401.08864v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T22:36:12Z</div><div>Authors: Yang Yang, George Sung, Shao-Fu Shih, Hakan Erdogan, Chehung Lee, Matthias Grundmann</div><div style='padding-top: 10px; width: 80ex'>We propose a neural network model that can separate target speech sources
from interfering sources at different angular regions using two microphones.
The model is trained with simulated room impulse responses (RIRs) using
omni-directional microphones without needing to collect real RIRs. By relying
on specific angular regions and multiple room simulations, the model utilizes
consistent time difference of arrival (TDOA) cues, or what we call delay
contrast, to separate target and interference sources while remaining robust in
various reverberation environments. We demonstrate the model is not only
generalizable to a commercially available device with a slightly different
microphone geometry, but also outperforms our previous work which uses one
additional microphone on the same device. The model runs in real-time on-device
and is suitable for low-latency streaming applications such as telephony and
video conferencing.</div><div><a href='http://arxiv.org/abs/2401.08864v1'>2401.08864v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04866v1")'>Room transfer function reconstruction using complex-valued neural
  networks and irregularly distributed microphones</div>
<div id='2402.04866v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T21:16:40Z</div><div>Authors: Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti</div><div style='padding-top: 10px; width: 80ex'>Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several important real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of room transfer functions measured at scattered points in the room. In
this study, we employ complex-valued neural networks to estimate room transfer
functions in the frequency range of the first room resonances, using a few
irregularly distributed microphones. To the best of our knowledge, this is the
first time complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex-valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
real-valued neural network method and a state-of-the-art kernel-based signal
processing approach for sound field reconstruction, showing that the proposed
technique exhibits relevant advantages in terms of phase accuracy and overall
quality of the reconstructed sound field.</div><div><a href='http://arxiv.org/abs/2402.04866v1'>2402.04866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05819v1")'>TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial
  Attention Decoding with a Short Decision Window</div>
<div id='2401.05819v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T10:36:27Z</div><div>Authors: Yuting Ding, Fei Chen</div><div style='padding-top: 10px; width: 80ex'>Auditory spatial attention detection (ASAD) is used to determine the
direction of a listener's attention to a speaker by analyzing her/his
electroencephalographic (EEG) signals. This study aimed to further improve the
performance of ASAD with a short decision window (i.e., &lt;1 s) rather than with
long decision windows in previous studies. An end-to-end temporal attention
network (i.e., TAnet) was introduced in this work. TAnet employs a multi-head
attention (MHA) mechanism, which can more effectively capture the interactions
among time steps in collected EEG signals and efficiently assign corresponding
weights to those EEG time steps. Experiments demonstrated that, compared with
the CNN-based method and recent ASAD methods, TAnet provided improved decoding
performance in the KUL dataset, with decoding accuracies of 92.4% (decision
window 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s)
with short decision windows (i.e., &lt;1 s). As a new ASAD model with a short
decision window, TAnet can potentially facilitate the design of EEG-controlled
intelligent hearing aids and sound recognition systems.</div><div><a href='http://arxiv.org/abs/2401.05819v1'>2401.05819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.07957v1")'>Machine Perceptual Quality: Evaluating the Impact of Severe Lossy
  Compression on Audio and Image Models</div>
<div id='2401.07957v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T20:47:24Z</div><div>Authors: Dan Jacobellis, Daniel Cummings, Neeraja J. Yadwadkar</div><div style='padding-top: 10px; width: 80ex'>In the field of neural data compression, the prevailing focus has been on
optimizing algorithms for either classical distortion metrics, such as PSNR or
SSIM, or human perceptual quality. With increasing amounts of data consumed by
machines rather than humans, a new paradigm of machine-oriented
compression$\unicode{x2013}$which prioritizes the retention of features salient
for machine perception over traditional human-centric
criteria$\unicode{x2013}$has emerged, creating several new challenges to the
development, evaluation, and deployment of systems utilizing lossy compression.
In particular, it is unclear how different approaches to lossy compression will
affect the performance of downstream machine perception tasks. To address this
under-explored area, we evaluate various perception
models$\unicode{x2013}$including image classification, image segmentation,
speech recognition, and music source separation$\unicode{x2013}$under severe
lossy compression. We utilize several popular codecs spanning conventional,
neural, and generative compression architectures. Our results indicate three
key findings: (1) using generative compression, it is feasible to leverage
highly compressed data while incurring a negligible impact on machine
perceptual quality; (2) machine perceptual quality correlates strongly with
deep similarity metrics, indicating a crucial role of these metrics in the
development of machine-oriented codecs; and (3) using lossy compressed
datasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive
scenarios where lossy compression increases machine perceptual quality rather
than degrading it. To encourage engagement on this growing area of research,
our code and experiments are available at:
https://github.com/danjacobellis/MPQ.</div><div><a href='http://arxiv.org/abs/2401.07957v1'>2401.07957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06203v2")'>Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source
  Separators</div>
<div id='2401.06203v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T16:04:53Z</div><div>Authors: Matthew Daly</div><div style='padding-top: 10px; width: 80ex'>This paper introduces our system submission for the Cadenza ICASSP 2024 Grand
Challenge, which presents the problem of remixing and enhancing music for
hearing aid users. Our system placed first in the challenge, achieving the best
average Hearing-Aid Audio Quality Index (HAAQI) score on the evaluation data
set. We describe the system, which uses an ensemble of deep learning music
source separators that are fine tuned on the challenge data. We demonstrate the
effectiveness of our system through the challenge results and analyze the
importance of different system aspects through ablation studies.</div><div><a href='http://arxiv.org/abs/2401.06203v2'>2401.06203v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05111v1")'>Noise-robust zero-shot text-to-speech synthesis conditioned on
  self-supervised speech-representation model with adapters</div>
<div id='2401.05111v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T12:21:21Z</div><div>Authors: Kenichi Fujita, Hiroshi Sato, Takanori Ashihara, Hiroki Kanagawa, Marc Delcroix, Takafumi Moriya, Yusuke Ijima</div><div style='padding-top: 10px; width: 80ex'>The zero-shot text-to-speech (TTS) method, based on speaker embeddings
extracted from reference speech using self-supervised learning (SSL) speech
representations, can reproduce speaker characteristics very accurately.
However, this approach suffers from degradation in speech synthesis quality
when the reference speech contains noise. In this paper, we propose a
noise-robust zero-shot TTS method. We incorporated adapters into the SSL model,
which we fine-tuned with the TTS model using noisy reference speech. In
addition, to further improve performance, we adopted a speech enhancement (SE)
front-end. With these improvements, our proposed SSL-based zero-shot TTS
achieved high-quality speech synthesis with noisy reference speech. Through the
objective and subjective evaluations, we confirmed that the proposed method is
highly robust to noise in reference speech, and effectively works in
combination with SE.</div><div><a href='http://arxiv.org/abs/2401.05111v1'>2401.05111v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08559v1")'>Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation</div>
<div id='2401.08559v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:39:15Z</div><div>Authors: Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black, Gül Varol, Xue Bin Peng, Davis Rempe</div><div style='padding-top: 10px; width: 80ex'>Recent advances in generative modeling have led to promising progress on
synthesizing 3D human motion from text, with methods that can generate
character animations from short prompts and specified durations. However, using
a single text prompt as input lacks the fine-grained control needed by
animators, such as composing multiple actions and defining precise durations
for parts of the motion. To address this, we introduce the new problem of
timeline control for text-driven motion synthesis, which provides an intuitive,
yet fine-grained, input interface for users. Instead of a single prompt, users
can specify a multi-track timeline of multiple prompts organized in temporal
intervals that may overlap. This enables specifying the exact timings of each
action and composing multiple actions in sequence or at overlapping intervals.
To generate composite animations from a multi-track timeline, we propose a new
test-time denoising method. This method can be integrated with any pre-trained
motion diffusion model to synthesize realistic motions that accurately reflect
the timeline. At every step of denoising, our method processes each timeline
interval (text prompt) individually, subsequently aggregating the predictions
with consideration for the specific body parts engaged in each action.
Experimental comparisons and ablations validate that our method produces
realistic motions that respect the semantics and timing of given text prompts.
Our code and models are publicly available at https://mathis.petrovich.fr/stmc.</div><div><a href='http://arxiv.org/abs/2401.08559v1'>2401.08559v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06807v1")'>Multistep Consistency Models</div>
<div id='2403.06807v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:26:34Z</div><div>Authors: Jonathan Heek, Emiel Hoogeboom, Tim Salimans</div><div style='padding-top: 10px; width: 80ex'>Diffusion models are relatively easy to train but require many steps to
generate samples. Consistency models are far more difficult to train, but
generate samples in a single step.
  In this paper we propose Multistep Consistency Models: A unification between
Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that
can interpolate between a consistency model and a diffusion model: a trade-off
between sampling speed and sampling quality. Specifically, a 1-step consistency
model is a conventional consistency model whereas we show that a $\infty$-step
consistency model is a diffusion model.
  Multistep Consistency Models work really well in practice. By increasing the
sample budget from a single step to 2-8 steps, we can train models more easily
that generate higher quality samples, while retaining much of the sampling
speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1
FID on Imagenet128 in 8 steps with consistency distillation. We also show that
our method scales to a text-to-image diffusion model, generating samples that
are very close to the quality of the original model.</div><div><a href='http://arxiv.org/abs/2403.06807v1'>2403.06807v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10348v1")'>Denoising Task Difficulty-based Curriculum for Training Diffusion Models</div>
<div id='2403.10348v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T14:34:34Z</div><div>Authors: Jin-Young Kim, Hyojun Go, Soonwoo Kwon, Hyun-Gyoon Kim</div><div style='padding-top: 10px; width: 80ex'>Diffusion-based generative models have emerged as powerful tools in the realm
of generative modeling. Despite extensive research on denoising across various
timesteps and noise levels, a conflict persists regarding the relative
difficulties of the denoising tasks. While various studies argue that lower
timesteps present more challenging tasks, others contend that higher timesteps
are more difficult. To address this conflict, our study undertakes a
comprehensive examination of task difficulties, focusing on convergence
behavior and changes in relative entropy between consecutive probability
distributions across timesteps. Our observational study reveals that denoising
at earlier timesteps poses challenges characterized by slower convergence and
higher relative entropy, indicating increased task difficulty at these lower
timesteps. Building on these observations, we introduce an easy-to-hard
learning scheme, drawing from curriculum learning, to enhance the training
process of diffusion models. By organizing timesteps or noise levels into
clusters and training models with descending orders of difficulty, we
facilitate an order-aware training regime, progressing from easier to harder
denoising tasks, thereby deviating from the conventional approach of training
diffusion models simultaneously across all timesteps. Our approach leads to
improved performance and faster convergence by leveraging the benefits of
curriculum learning, while maintaining orthogonality with existing improvements
in diffusion training techniques. We validate these advantages through
comprehensive experiments in image generation tasks, including unconditional,
class-conditional, and text-to-image generation.</div><div><a href='http://arxiv.org/abs/2403.10348v1'>2403.10348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15309v1")'>Controlled Training Data Generation with Diffusion Models</div>
<div id='2403.15309v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T15:59:24Z</div><div>Authors: Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir</div><div style='padding-top: 10px; width: 80ex'>In this work, we present a method to control a text-to-image generative model
to produce training data specifically "useful" for supervised learning. Unlike
previous works that employ an open-loop approach and pre-define prompts to
generate new data using either a language model or human expertise, we develop
an automated closed-loop system which involves two feedback mechanisms. The
first mechanism uses feedback from a given supervised model and finds
adversarial prompts that result in image generations that maximize the model
loss. While these adversarial prompts result in diverse data informed by the
model, they are not informed of the target distribution, which can be
inefficient. Therefore, we introduce the second feedback mechanism that guides
the generation process towards a certain target distribution. We call the
method combining these two mechanisms Guided Adversarial Prompts. We perform
our evaluations on different tasks, datasets and architectures, with different
types of distribution shifts (spuriously correlated data, unseen domains) and
demonstrate the efficiency of the proposed feedback mechanisms compared to
open-loop approaches.</div><div><a href='http://arxiv.org/abs/2403.15309v1'>2403.15309v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07356v1")'>Premonition: Using Generative Models to Preempt Future Data Changes in
  Continual Learning</div>
<div id='2403.07356v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T06:29:54Z</div><div>Authors: Mark D. McDonnell, Dong Gong, Ehsan Abbasnejad, Anton van den Hengel</div><div style='padding-top: 10px; width: 80ex'>Continual learning requires a model to adapt to ongoing changes in the data
distribution, and often to the set of tasks to be performed. It is rare,
however, that the data and task changes are completely unpredictable. Given a
description of an overarching goal or data theme, which we call a realm, humans
can often guess what concepts are associated with it. We show here that the
combination of a large language model and an image generation model can
similarly provide useful premonitions as to how a continual learning challenge
might develop over time. We use the large language model to generate text
descriptions of semantically related classes that might potentially appear in
the data stream in future. These descriptions are then rendered using Stable
Diffusion to generate new labelled image samples. The resulting synthetic
dataset is employed for supervised pre-training, but is discarded prior to
commencing continual learning, along with the pre-training classification head.
We find that the backbone of our pre-trained networks can learn representations
useful for the downstream continual learning problem, thus becoming a valuable
input to any existing continual learning method. Although there are
complexities arising from the domain gap between real and synthetic images, we
show that pre-training models in this manner improves multiple Class Incremenal
Learning (CIL) methods on fine-grained image classification benchmarks.
Supporting code can be found at https://github.com/cl-premonition/premonition.</div><div><a href='http://arxiv.org/abs/2403.07356v1'>2403.07356v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03938v1")'>GUIDE: Guidance-based Incremental Learning with Diffusion Models</div>
<div id='2403.03938v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T18:47:32Z</div><div>Authors: Bartosz Cywiński, Kamil Deja, Tomasz Trzciński, Bartłomiej Twardowski, Łukasz Kuciński</div><div style='padding-top: 10px; width: 80ex'>We introduce GUIDE, a novel continual learning approach that directs
diffusion models to rehearse samples at risk of being forgotten. Existing
generative strategies combat catastrophic forgetting by randomly sampling
rehearsal examples from a generative model. Such an approach contradicts
buffer-based approaches where sampling strategy plays an important role. We
propose to bridge this gap by integrating diffusion models with classifier
guidance techniques to produce rehearsal examples specifically targeting
information forgotten by a continuously trained model. This approach enables
the generation of samples from preceding task distributions, which are more
likely to be misclassified in the context of recently encountered classes. Our
experimental results show that GUIDE significantly reduces catastrophic
forgetting, outperforming conventional random sampling approaches and
surpassing recent state-of-the-art methods in continual learning with
generative replay.</div><div><a href='http://arxiv.org/abs/2403.03938v1'>2403.03938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04599v1")'>Contrastive Continual Learning with Importance Sampling and
  Prototype-Instance Relation Distillation</div>
<div id='2403.04599v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T15:47:52Z</div><div>Authors: Jiyong Li, Dilshod Azizov, Yang Li, Shangsong Liang</div><div style='padding-top: 10px; width: 80ex'>Recently, because of the high-quality representations of contrastive learning
methods, rehearsal-based contrastive continual learning has been proposed to
explore how to continually learn transferable representation embeddings to
avoid the catastrophic forgetting issue in traditional continual settings.
Based on this framework, we propose Contrastive Continual Learning via
Importance Sampling (CCLIS) to preserve knowledge by recovering previous data
distributions with a new strategy for Replay Buffer Selection (RBS), which
minimize estimated variance to save hard negative samples for representation
learning with high quality. Furthermore, we present the Prototype-instance
Relation Distillation (PRD) loss, a technique designed to maintain the
relationship between prototypes and sample representations using a
self-distillation process. Experiments on standard continual learning
benchmarks reveal that our method notably outperforms existing baselines in
terms of knowledge preservation and thereby effectively counteracts
catastrophic forgetting in online contexts. The code is available at
https://github.com/lijy373/CCLIS.</div><div><a href='http://arxiv.org/abs/2403.04599v1'>2403.04599v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04190v1")'>Generative AI for Synthetic Data Generation: Methods, Challenges and the
  Future</div>
<div id='2403.04190v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T03:38:44Z</div><div>Authors: Xu Guo, Yiqiang Chen</div><div style='padding-top: 10px; width: 80ex'>The recent surge in research focused on generating synthetic data from large
language models (LLMs), especially for scenarios with limited data
availability, marks a notable shift in Generative Artificial Intelligence (AI).
Their ability to perform comparably to real-world data positions this approach
as a compelling solution to low-resource challenges. This paper delves into
advanced technologies that leverage these gigantic LLMs for the generation of
task-specific training data. We outline methodologies, evaluation techniques,
and practical applications, discuss the current limitations, and suggest
potential pathways for future research.</div><div><a href='http://arxiv.org/abs/2403.04190v1'>2403.04190v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.11274v1")'>TC-DiffRecon: Texture coordination MRI reconstruction method based on
  diffusion model and modified MF-UNet method</div>
<div id='2402.11274v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T13:09:00Z</div><div>Authors: Chenyan Zhang, Yifei Chen, Zhenxiong Fan, Yiyu Huang, Wenchao Weng, Ruiquan Ge, Dong Zeng, Changmiao Wang</div><div style='padding-top: 10px; width: 80ex'>Recently, diffusion models have gained significant attention as a novel set
of deep learning-based generative methods. These models attempt to sample data
from a Gaussian distribution that adheres to a target distribution, and have
been successfully adapted to the reconstruction of MRI data. However, as an
unconditional generative model, the diffusion model typically disrupts image
coordination because of the consistent projection of data introduced by
conditional bootstrap. This often results in image fragmentation and
incoherence. Furthermore, the inherent limitations of the diffusion model often
lead to excessive smoothing of the generated images. In the same vein, some
deep learning-based models often suffer from poor generalization performance,
meaning their effectiveness is greatly affected by different acceleration
factors. To address these challenges, we propose a novel diffusion model-based
MRI reconstruction method, named TC-DiffRecon, which does not rely on a
specific acceleration factor for training. We also suggest the incorporation of
the MF-UNet module, designed to enhance the quality of MRI images generated by
the model while mitigating the over-smoothing issue to a certain extent. During
the image generation sampling process, we employ a novel TCKG module and a
Coarse-to-Fine sampling scheme. These additions aim to harmonize image texture,
expedite the sampling process, while achieving data consistency. Our source
code is available at https://github.com/JustlfC03/TC-DiffRecon.</div><div><a href='http://arxiv.org/abs/2402.11274v1'>2402.11274v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12034v1")'>VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion
  Models</div>
<div id='2403.12034v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:59:12Z</div><div>Authors: Junlin Han, Filippos Kokkinos, Philip Torr</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel paradigm for building scalable 3D generative
models utilizing pre-trained video diffusion models. The primary obstacle in
developing foundation 3D generative models is the limited availability of 3D
data. Unlike images, texts, or videos, 3D data are not readily accessible and
are difficult to acquire. This results in a significant disparity in scale
compared to the vast quantities of other types of data. To address this issue,
we propose using a video diffusion model, trained with extensive volumes of
text, images, and videos, as a knowledge source for 3D data. By unlocking its
multi-view generative capabilities through fine-tuning, we generate a
large-scale synthetic multi-view dataset to train a feed-forward 3D generative
model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view
data, can generate a 3D asset from a single image in seconds and achieves
superior performance when compared to current SOTA feed-forward 3D generative
models, with users preferring our results over 70% of the time.</div><div><a href='http://arxiv.org/abs/2403.12034v1'>2403.12034v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13802v1")'>ZigMa: Zigzag Mamba Diffusion Model</div>
<div id='2403.13802v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:59:14Z</div><div>Authors: Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, Bjorn Ommer</div><div style='padding-top: 10px; width: 80ex'>The diffusion model has long been plagued by scalability and quadratic
complexity issues, especially within transformer-based structures. In this
study, we aim to leverage the long sequence modeling capability of a
State-Space Model called Mamba to extend its applicability to visual data
generation. Firstly, we identify a critical oversight in most current
Mamba-based vision methods, namely the lack of consideration for spatial
continuity in the scan scheme of Mamba. Secondly, building upon this insight,
we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,
which outperforms Mamba-based baselines and demonstrates improved speed and
memory utilization compared to transformer-based baselines. Lastly, we
integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate
the scalability of the model on large-resolution visual datasets, such as
FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO
$256\times 256$. Code will be released at https://taohu.me/zigma/</div><div><a href='http://arxiv.org/abs/2403.13802v1'>2403.13802v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00570v1")'>Rethinking cluster-conditioned diffusion models</div>
<div id='2403.00570v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T14:47:46Z</div><div>Authors: Nikolas Adaloglou, Tim Kaiser, Felix Michels, Markus Kollmann</div><div style='padding-top: 10px; width: 80ex'>We present a comprehensive experimental study on image-level conditioning for
diffusion models using cluster assignments. We elucidate how individual
components regarding image clustering impact image synthesis across three
datasets. By combining recent advancements from image clustering and diffusion
models, we show that, given the optimal cluster granularity with respect to
image synthesis (visual groups), cluster-conditioning can achieve
state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively),
while attaining a strong training sample efficiency. Finally, we propose a
novel method to derive an upper cluster bound that reduces the search space of
the visual groups using solely feature-based clustering. Unlike existing
approaches, we find no significant connection between clustering and
cluster-conditional image generation. The code and cluster assignments will be
released.</div><div><a href='http://arxiv.org/abs/2403.00570v1'>2403.00570v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00025v1")'>On the Challenges and Opportunities in Generative AI</div>
<div id='2403.00025v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:19:33Z</div><div>Authors: Laura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina Däubener, Sophie Fellenz, Asja Fischer, Thomas Gärtner, Matthias Kirchler, Marius Kloft, Yingzhen Li, Christoph Lippert, Gerard de Melo, Eric Nalisnick, Björn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy Van den Broeck, Julia E Vogt, Yixin Wang, Florian Wenzel, Frank Wood, Stephan Mandt, Vincent Fortuin</div><div style='padding-top: 10px; width: 80ex'>The field of deep generative modeling has grown rapidly and consistently over
the years. With the availability of massive amounts of training data coupled
with advances in scalable unsupervised learning paradigms, recent large-scale
generative models show tremendous promise in synthesizing high-resolution
images and text, as well as structured data such as videos and molecules.
However, we argue that current large-scale generative AI models do not
sufficiently address several fundamental issues that hinder their widespread
adoption across domains. In this work, we aim to identify key unresolved
challenges in modern generative AI paradigms that should be tackled to further
enhance their capabilities, versatility, and reliability. By identifying these
challenges, we aim to provide researchers with valuable insights for exploring
fruitful research directions, thereby fostering the development of more robust
and accessible generative AI solutions.</div><div><a href='http://arxiv.org/abs/2403.00025v1'>2403.00025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02333v1")'>Copyright Protection in Generative AI: A Technical Perspective</div>
<div id='2402.02333v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T04:00:33Z</div><div>Authors: Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, Jiliang Tang</div><div style='padding-top: 10px; width: 80ex'>Generative AI has witnessed rapid advancement in recent years, expanding
their capabilities to create synthesized content such as text, images, audio,
and code. The high fidelity and authenticity of contents generated by these
Deep Generative Models (DGMs) have sparked significant copyright concerns.
There have been various legal debates on how to effectively safeguard
copyrights in DGMs. This work delves into this issue by providing a
comprehensive overview of copyright protection from a technical perspective. We
examine from two distinct viewpoints: the copyrights pertaining to the source
data held by the data owners and those of the generative models maintained by
the model builders. For data copyright, we delve into methods data owners can
protect their content and DGMs can be utilized without infringing upon these
rights. For model copyright, our discussion extends to strategies for
preventing model theft and identifying outputs generated by specific models.
Finally, we highlight the limitations of existing techniques and identify areas
that remain unexplored. Furthermore, we discuss prospective directions for the
future of copyright protection, underscoring its importance for the sustainable
and ethical development of Generative AI.</div><div><a href='http://arxiv.org/abs/2402.02333v1'>2402.02333v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00861v1")'>Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy,
  Survey and Insights</div>
<div id='2403.00861v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T21:03:46Z</div><div>Authors: Yuan Wang, Lokesh Kumar Sambasivan, Mingang Fu, Prakhar Mehrotra</div><div style='padding-top: 10px; width: 80ex'>Generative AI applications, such as ChatGPT or DALL-E, have shown the world
their impressive capabilities in generating human-like text or image. Diving
deeper, the science stakeholder for those AI applications are Deep Generative
Models, a.k.a DGMs, which are designed to learn the underlying distribution of
the data and generate new data points that are statistically similar to the
original dataset. One critical question is raised: how can we leverage DGMs
into morden retail supply chain realm? To address this question, this paper
expects to provide a comprehensive review of DGMs and discuss their existing
and potential usecases in retail supply chain, by (1) providing a taxonomy and
overview of state-of-the-art DGMs and their variants, (2) reviewing existing
DGM applications in retail supply chain from a end-to-end view of point, and
(3) discussing insights and potential directions on how DGMs can be further
utilized on solving retail supply chain problems.</div><div><a href='http://arxiv.org/abs/2403.00861v1'>2403.00861v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09500v1")'>A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal
  Morphology Generation</div>
<div id='2401.09500v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T09:03:14Z</div><div>Authors: Nianzu Yang, Kaipeng Zeng, Haotian Lu, Yexin Wu, Zexin Yuan, Shengdian Jiang, Jiaxiang Wu, Yimin Wang, Junchi Yan</div><div style='padding-top: 10px; width: 80ex'>Neuronal morphology is essential for studying brain functioning and
understanding neurodegenerative disorders. As the acquiring of real-world
morphology data is expensive, computational approaches especially
learning-based ones e.g. MorphVAE for morphology generation were recently
studied, which are often conducted in a way of randomly augmenting a given
authentic morphology to achieve plausibility. Under such a setting, this paper
proposes \textbf{MorphGrower} which aims to generate more plausible morphology
samples by mimicking the natural growth mechanism instead of a one-shot
treatment as done in MorphVAE. Specifically, MorphGrower generates morphologies
layer by layer synchronously and chooses a pair of sibling branches as the
basic generation block, and the generation of each layer is conditioned on the
morphological structure of previous layers and then generate morphologies via a
conditional variational autoencoder with spherical latent space. Extensive
experimental results on four real-world datasets demonstrate that MorphGrower
outperforms MorphVAE by a notable margin. Our code will be publicly available
to facilitate future research.</div><div><a href='http://arxiv.org/abs/2401.09500v1'>2401.09500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14404v1")'>Deconstructing Denoising Diffusion Models for Self-Supervised Learning</div>
<div id='2401.14404v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:59:57Z</div><div>Authors: Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He</div><div style='padding-top: 10px; width: 80ex'>In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.</div><div><a href='http://arxiv.org/abs/2401.14404v1'>2401.14404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11793v3")'>Generative Kaleidoscopic Networks</div>
<div id='2402.11793v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:48:40Z</div><div>Authors: Harsh Shrivastava</div><div style='padding-top: 10px; width: 80ex'>We discovered that the neural networks, especially the deep ReLU networks,
demonstrate an `over-generalization' phenomenon. That is, the output values for
the inputs that were not seen during training are mapped close to the output
range that were observed during the learning process. In other words, the
neural networks learn a many-to-one mapping and this effect is more prominent
as we increase the number of layers or the depth of the neural network. We
utilize this property of neural networks to design a dataset kaleidoscope,
termed as `Generative Kaleidoscopic Networks'. Briefly, if we learn a model to
map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$,
the proposed `Kaleidoscopic sampling' procedure starts with a random input
noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots
f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing
samples from the input distribution and the quality of samples recovered
improves as we increase the depth of the model. Scope: We observed this
phenomenon to various degrees for the other deep learning architectures like
CNNs, Transformers &amp; U-Nets and we are currently investigating them further.</div><div><a href='http://arxiv.org/abs/2402.11793v3'>2402.11793v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19009v1")'>Generating, Reconstructing, and Representing Discrete and Continuous
  Data: Generalized Diffusion with Learnable Encoding-Decoding</div>
<div id='2402.19009v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T10:08:57Z</div><div>Authors: Guangyi Liu, Yu Wang, Zeyu Feng, Qiyu Wu, Liping Tang, Yuan Gao, Zhen Li, Shuguang Cui, Julian McAuley, Eric P. Xing, Zichao Yang, Zhiting Hu</div><div style='padding-top: 10px; width: 80ex'>The vast applications of deep generative models are anchored in three core
capabilities -- generating new instances, reconstructing inputs, and learning
compact representations -- across various data types, such as discrete
text/protein sequences and continuous images. Existing model families, like
Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs),
autoregressive models, and diffusion models, generally excel in specific
capabilities and data types but fall short in others. We introduce generalized
diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates
the core capabilities for broad applicability and enhanced performance. DiLED
generalizes the Gaussian noising-denoising in standard diffusion by introducing
parameterized encoding-decoding. Crucially, DiLED is compatible with the
well-established diffusion model objective and training recipes, allowing
effective learning of the encoder-decoder parameters jointly with diffusion. By
choosing appropriate encoder/decoder (e.g., large language models), DiLED
naturally applies to different data types. Extensive experiments on text,
proteins, and images demonstrate DiLED's flexibility to handle diverse data and
tasks and its strong improvement over various existing models.</div><div><a href='http://arxiv.org/abs/2402.19009v1'>2402.19009v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03701v1")'>Improving and Unifying Discrete&amp;Continuous-time Discrete Denoising
  Diffusion</div>
<div id='2402.03701v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T04:42:36Z</div><div>Authors: Lingxiao Zhao, Xueying Ding, Lijun Yu, Leman Akoglu</div><div style='padding-top: 10px; width: 80ex'>Discrete diffusion models have seen a surge of attention with applications on
naturally discrete data such as language and graphs. Although discrete-time
discrete diffusion has been established for a while, only recently Campbell et
al. (2022) introduced the first framework for continuous-time discrete
diffusion. However, their training and sampling processes differ significantly
from the discrete-time version, necessitating nontrivial approximations for
tractability. In this paper, we first present a series of mathematical
simplifications of the variational lower bound that enable more accurate and
easy-to-optimize training for discrete diffusion. In addition, we derive a
simple formulation for backward denoising that enables exact and accelerated
sampling, and importantly, an elegant unification of discrete-time and
continuous-time discrete diffusion. Thanks to simpler analytical formulations,
both forward and now also backward probabilities can flexibly accommodate any
noise distribution, including different noise distributions for multi-element
objects. Experiments show that our proposed USD3 (for Unified Simplified
Discrete Denoising Diffusion) outperform all SOTA baselines on established
datasets. We open-source our unified code at
https://github.com/LingxiaoShawn/USD3.</div><div><a href='http://arxiv.org/abs/2402.03701v1'>2402.03701v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07211v2")'>Towards Fast Stochastic Sampling in Diffusion Generative Models</div>
<div id='2402.07211v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T14:04:13Z</div><div>Authors: Kushagra Pandey, Maja Rudolph, Stephan Mandt</div><div style='padding-top: 10px; width: 80ex'>Diffusion models suffer from slow sample generation at inference time.
Despite recent efforts, improving the sampling efficiency of stochastic
samplers for diffusion models remains a promising direction. We propose
Splitting Integrators for fast stochastic sampling in pre-trained diffusion
models in augmented spaces. Commonly used in molecular dynamics,
splitting-based integrators attempt to improve sampling efficiency by cleverly
alternating between numerical updates involving the data, auxiliary, or noise
variables. However, we show that a naive application of splitting integrators
is sub-optimal for fast sampling. Consequently, we propose several principled
modifications to naive splitting samplers for improving sampling efficiency and
denote the resulting samplers as Reduced Splitting Integrators. In the context
of Phase Space Langevin Diffusion (PSLD) [Pandey \&amp; Mandt, 2023] on CIFAR-10,
our stochastic sampler achieves an FID score of 2.36 in only 100 network
function evaluations (NFE) as compared to 2.63 for the best baselines.</div><div><a href='http://arxiv.org/abs/2402.07211v2'>2402.07211v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02346v1")'>Closed-Loop Unsupervised Representation Disentanglement with $β$-VAE
  Distillation and Diffusion Probabilistic Feedback</div>
<div id='2402.02346v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:03:22Z</div><div>Authors: Xin Jin, Bohan Li, BAAO Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li, Tao Yang, Wenjun Zeng</div><div style='padding-top: 10px; width: 80ex'>Representation disentanglement may help AI fundamentally understand the real
world and thus benefit both discrimination and generation tasks. It currently
has at least three unresolved core issues: (i) heavy reliance on label
annotation and synthetic data -- causing poor generalization on natural
scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to
adaptively achieve an optimal training trade-off; (iii) lacking reasonable
evaluation metric, especially for the real label-free data. To address these
challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised
representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}.
Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while
resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled
representations. The strong generation ability of diffusion model and the good
disentanglement ability of VAE model are complementary. To strengthen
disentangling, VAE-latent distillation and diffusion-wise feedback are
interconnected in a closed-loop system for a further mutual promotion. Then, a
self-supervised \textbf{Navigation} strategy is introduced to identify
interpretable semantic directions in the disentangled latent space. Finally, a
new metric based on content tracking is designed to evaluate the
disentanglement effect. Experiments demonstrate the superiority of CL-Dis on
applications like real image manipulation and visual analysis.</div><div><a href='http://arxiv.org/abs/2402.02346v1'>2402.02346v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12517v2")'>DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing
  High-Quality Implicit Neural Representations</div>
<div id='2401.12517v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T06:21:34Z</div><div>Authors: Dogyun Park, Sihyeon Kim, Sojin Lee, Hyunwoo J. Kim</div><div style='padding-top: 10px; width: 80ex'>Recent studies have introduced a new class of generative models for
synthesizing implicit neural representations (INRs) that capture arbitrary
continuous signals in various domains. These models opened the door for
domain-agnostic generative models, but they often fail to achieve high-quality
generation. We observed that the existing methods generate the weights of
neural networks to parameterize INRs and evaluate the network with fixed
positional embeddings (PEs). Arguably, this architecture limits the expressive
power of generative models and results in low-quality INR generation. To
address this limitation, we propose Domain-agnostic Latent Diffusion Model for
INRs (DDMI) that generates adaptive positional embeddings instead of neural
networks' weights. Specifically, we develop a Discrete-to-continuous space
Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and
the continuous signal functions in the shared latent space. Additionally, we
introduce a novel conditioning mechanism for evaluating INRs with the
hierarchically decomposed PEs to further enhance expressive power. Extensive
experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance
Fields, and videos, with seven benchmark datasets, demonstrate the versatility
of DDMI and its superior performance compared to the existing INR generative
models.</div><div><a href='http://arxiv.org/abs/2401.12517v2'>2401.12517v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09180v2")'>Unsupervised Multiple Domain Translation through Controlled
  Disentanglement in Variational Autoencoder</div>
<div id='2401.09180v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T12:43:28Z</div><div>Authors: Antonio Almudévar, Théo Mariotte, Alfonso Ortega, Marie Tahon</div><div style='padding-top: 10px; width: 80ex'>Unsupervised Multiple Domain Translation is the task of transforming data
from one domain to other domains without having paired data to train the
systems. Typically, methods based on Generative Adversarial Networks (GANs) are
used to address this task. However, our proposal exclusively relies on a
modified version of a Variational Autoencoder. This modification consists of
the use of two latent variables disentangled in a controlled way by design. One
of this latent variables is imposed to depend exclusively on the domain, while
the other one must depend on the rest of the variability factors of the data.
Additionally, the conditions imposed over the domain latent variable allow for
better control and understanding of the latent space. We empirically
demonstrate that our approach works on different vision datasets improving the
performance of other well known methods. Finally, we prove that, indeed, one of
the latent variables stores all the information related to the domain and the
other one hardly contains any domain information.</div><div><a href='http://arxiv.org/abs/2401.09180v2'>2401.09180v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08897v2")'>CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in
  Variational AutoEncoder</div>
<div id='2401.08897v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T00:46:24Z</div><div>Authors: Hee-Jun Jung, Jaehyoung Jeong, Kangil Kim</div><div style='padding-top: 10px; width: 80ex'>Symmetries of input and latent vectors have provided valuable insights for
disentanglement learning in VAEs.However, only a few works were proposed as an
unsupervised method, and even these works require known factor information in
training data. We propose a novel method, Composite Factor-Aligned Symmetry
Learning (CFASL), which is integrated into VAEs for learning symmetry-based
disentanglement in unsupervised learning without any knowledge of the dataset
factor information.CFASL incorporates three novel features for learning
symmetry-based disentanglement: 1) Injecting inductive bias to align latent
vector dimensions to factor-aligned symmetries within an explicit learnable
symmetry codebook 2) Learning a composite symmetry to express unknown factors
change between two random samples by learning factor-aligned symmetries within
the codebook 3) Inducing group equivariant encoder and decoder in training VAEs
with the two conditions. In addition, we propose an extended evaluation metric
for multi-factor changes in comparison to disentanglement evaluation in VAEs.
In quantitative and in-depth qualitative analysis, CFASL demonstrates a
significant improvement of disentanglement in single-factor change, and
multi-factor change conditions compared to state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2401.08897v2'>2401.08897v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12588v1")'>Interpreting Equivariant Representations</div>
<div id='2401.12588v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T09:43:30Z</div><div>Authors: Andreas Abildtrup Hansen, Anna Calissano, Aasa Feragen</div><div style='padding-top: 10px; width: 80ex'>Latent representations are used extensively for downstream tasks, such as
visualization, interpolation or feature extraction of deep learning models.
Invariant and equivariant neural networks are powerful and well-established
models for enforcing inductive biases. In this paper, we demonstrate that the
inductive bias imposed on the by an equivariant model must also be taken into
account when using latent representations. We show how not accounting for the
inductive biases leads to decreased performance on downstream tasks, and vice
versa, how accounting for inductive biases can be done effectively by using an
invariant projection of the latent representations. We propose principles for
how to choose such a projection, and show the impact of using these principles
in two common examples: First, we study a permutation equivariant variational
auto-encoder trained for molecule graph generation; here we show that invariant
projections can be designed that incur no loss of information in the resulting
invariant representation. Next, we study a rotation-equivariant representation
used for image classification. Here, we illustrate how random invariant
projections can be used to obtain an invariant representation with a high
degree of retained information. In both cases, the analysis of invariant latent
representations proves superior to their equivariant counterparts. Finally, we
illustrate that the phenomena documented here for equivariant neural networks
have counterparts in standard neural networks where invariance is encouraged
via augmentation. Thus, while these ambiguities may be known by experienced
developers of equivariant models, we make both the knowledge as well as
effective tools to handle the ambiguities available to the broader community.</div><div><a href='http://arxiv.org/abs/2401.12588v1'>2401.12588v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03115v1")'>Discovering interpretable models of scientific image data with deep
  learning</div>
<div id='2402.03115v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:45:55Z</div><div>Authors: Christopher J. Soelistyo, Alan R. Lowe</div><div style='padding-top: 10px; width: 80ex'>How can we find interpretable, domain-appropriate models of natural phenomena
given some complex, raw data such as images? Can we use such models to derive
scientific insight from the data? In this paper, we propose some methods for
achieving this. In particular, we implement disentangled representation
learning, sparse deep neural network training and symbolic regression, and
assess their usefulness in forming interpretable models of complex image data.
We demonstrate their relevance to the field of bioimaging using a well-studied
test problem of classifying cell states in microscopy data. We find that such
methods can produce highly parsimonious models that achieve $\sim98\%$ of the
accuracy of black-box benchmark models, with a tiny fraction of the complexity.
We explore the utility of such interpretable models in producing scientific
explanations of the underlying biological phenomenon.</div><div><a href='http://arxiv.org/abs/2402.03115v1'>2402.03115v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18732v1")'>GAIA: Categorical Foundations of Generative AI</div>
<div id='2402.18732v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T22:25:02Z</div><div>Authors: Sridhar Mahadevan</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose GAIA, a generative AI architecture based on
category theory. GAIA is based on a hierarchical model where modules are
organized as a simplicial complex. Each simplicial complex updates its internal
parameters biased on information it receives from its superior simplices and in
turn relays updates to its subordinate sub-simplices. Parameter updates are
formulated in terms of lifting diagrams over simplicial sets, where inner and
outer horn extensions correspond to different types of learning problems.
Backpropagation is modeled as an endofunctor over the category of parameters,
leading to a coalgebraic formulation of deep learning.</div><div><a href='http://arxiv.org/abs/2402.18732v1'>2402.18732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14623v1")'>Simplified Diffusion Schrödinger Bridge</div>
<div id='2403.14623v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:59:41Z</div><div>Authors: Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.</div><div><a href='http://arxiv.org/abs/2403.14623v1'>2403.14623v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05033v1")'>Quantifying Manifolds: Do the manifolds learned by Generative
  Adversarial Networks converge to the real data manifold</div>
<div id='2403.05033v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T04:23:50Z</div><div>Authors: Anupam Chaudhuri, Anj Simmons, Mohamed Abdelrazek</div><div style='padding-top: 10px; width: 80ex'>This paper presents our experiments to quantify the manifolds learned by ML
models (in our experiment, we use a GAN model) as they train. We compare the
manifolds learned at each epoch to the real manifolds representing the real
data. To quantify a manifold, we study the intrinsic dimensions and topological
features of the manifold learned by the ML model, how these metrics change as
we continue to train the model, and whether these metrics convergence over the
course of training to the metrics of the real data manifold.</div><div><a href='http://arxiv.org/abs/2403.05033v1'>2403.05033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04856v2")'>A Good Score Does not Lead to A Good Generative Model</div>
<div id='2401.04856v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T00:17:36Z</div><div>Authors: Sixu Li, Shi Chen, Qin Li</div><div style='padding-top: 10px; width: 80ex'>Score-based Generative Models (SGMs) is one leading method in generative
modeling, renowned for their ability to generate high-quality samples from
complex, high-dimensional data distributions. The method enjoys empirical
success and is supported by rigorous theoretical convergence properties. In
particular, it has been shown that SGMs can generate samples from a
distribution that is close to the ground-truth if the underlying score function
is learned well, suggesting the success of SGM as a generative model. We
provide a counter-example in this paper. Through the sample complexity
argument, we provide one specific setting where the score function is learned
well. Yet, SGMs in this setting can only output samples that are Gaussian
blurring of training data points, mimicking the effects of kernel density
estimation. The finding resonates a series of recent finding that reveal that
SGMs can demonstrate strong memorization effect and fail to generate.</div><div><a href='http://arxiv.org/abs/2401.04856v2'>2401.04856v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08082v3")'>Score-based generative models break the curse of dimensionality in
  learning a family of sub-Gaussian probability distributions</div>
<div id='2402.08082v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:02:23Z</div><div>Authors: Frank Cole, Yulong Lu</div><div style='padding-top: 10px; width: 80ex'>While score-based generative models (SGMs) have achieved remarkable success
in enormous image generation tasks, their mathematical foundations are still
limited. In this paper, we analyze the approximation and generalization of SGMs
in learning a family of sub-Gaussian probability distributions. We introduce a
notion of complexity for probability distributions in terms of their relative
density with respect to the standard Gaussian measure. We prove that if the
log-relative density can be locally approximated by a neural network whose
parameters can be suitably bounded, then the distribution generated by
empirical score matching approximates the target distribution in total
variation with a dimension-independent rate. We illustrate our theory through
examples, which include certain mixtures of Gaussians. An essential ingredient
of our proof is to derive a dimension-free deep neural network approximation
rate for the true score function associated with the forward process, which is
interesting in its own right.</div><div><a href='http://arxiv.org/abs/2402.08082v3'>2402.08082v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14098v1")'>Intriguing Properties of Modern GANs</div>
<div id='2402.14098v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:48:11Z</div><div>Authors: Roy Friedman, Yair Weiss</div><div style='padding-top: 10px; width: 80ex'>Modern GANs achieve remarkable performance in terms of generating realistic
and diverse samples. This has led many to believe that ``GANs capture the
training data manifold''. In this work we show that this interpretation is
wrong. We empirically show that the manifold learned by modern GANs does not
fit the training distribution: specifically the manifold does not pass through
the training examples and passes closer to out-of-distribution images than to
in-distribution images. We also investigate the distribution over images
implied by the prior over the latent codes and study whether modern GANs learn
a density that approximates the training distribution. Surprisingly, we find
that the learned density is very far from the data distribution and that GANs
tend to assign higher density to out-of-distribution images. Finally, we
demonstrate that the set of images used to train modern GANs are often not part
of the typical set described by the GANs' distribution.</div><div><a href='http://arxiv.org/abs/2402.14098v1'>2402.14098v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09849v1")'>Recommendations for Baselines and Benchmarking Approximate Gaussian
  Processes</div>
<div id='2402.09849v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T10:11:28Z</div><div>Authors: Sebastian W. Ober, Artem Artemev, Marcel Wagenländer, Rudolfs Grobins, Mark van der Wilk</div><div style='padding-top: 10px; width: 80ex'>Gaussian processes (GPs) are a mature and widely-used component of the ML
toolbox. One of their desirable qualities is automatic hyperparameter
selection, which allows for training without user intervention. However, in
many realistic settings, approximations are typically needed, which typically
do require tuning. We argue that this requirement for tuning complicates
evaluation, which has led to a lack of a clear recommendations on which method
should be used in which situation. To address this, we make recommendations for
comparing GP approximations based on a specification of what a user should
expect from a method. In addition, we develop a training procedure for the
variational method of Titsias [2009] that leaves no choices to the user, and
show that this is a strong baseline that meets our specification. We conclude
that benchmarking according to our suggestions gives a clearer view of the
current state of the field, and uncovers problems that are still open that
future papers should address.</div><div><a href='http://arxiv.org/abs/2402.09849v1'>2402.09849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17287v1")'>An Interpretable Evaluation of Entropy-based Novelty of Generative
  Models</div>
<div id='2402.17287v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T08:00:52Z</div><div>Authors: Jingwei Zhang, Cheuk Ting Li, Farzan Farnia</div><div style='padding-top: 10px; width: 80ex'>The massive developments of generative model frameworks and architectures
require principled methods for the evaluation of a model's novelty compared to
a reference dataset or baseline generative models. While the recent literature
has extensively studied the evaluation of the quality, diversity, and
generalizability of generative models, the assessment of a model's novelty
compared to a baseline model has not been adequately studied in the machine
learning community. In this work, we focus on the novelty assessment under
multi-modal generative models and attempt to answer the following question:
Given the samples of a generative model $\mathcal{G}$ and a reference dataset
$\mathcal{S}$, how can we discover and count the modes expressed by
$\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral
approach to the described task and propose the Kernel-based Entropic Novelty
(KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$
with respect to distribution $P_\mathcal{S}$. We analytically interpret the
behavior of the KEN score under mixture distributions with sub-Gaussian
components. Next, we develop a method based on Cholesky decomposition to
compute the KEN score from observed samples. We support the KEN-based
quantification of novelty by presenting several numerical results on synthetic
and real image distributions. Our numerical results indicate the success of the
proposed approach in detecting the novel modes and the comparison of
state-of-the-art generative models.</div><div><a href='http://arxiv.org/abs/2402.17287v1'>2402.17287v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13610v1")'>Data-driven Discovery with Large Generative Models</div>
<div id='2402.13610v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T08:26:43Z</div><div>Authors: Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, Peter Clark</div><div style='padding-top: 10px; width: 80ex'>With the accumulation of data at an unprecedented rate, its potential to fuel
scientific discovery is growing exponentially. This position paper urges the
Machine Learning (ML) community to exploit the capabilities of large generative
models (LGMs) to develop automated systems for end-to-end data-driven discovery
-- a paradigm encompassing the search and verification of hypotheses purely
from a set of provided datasets, without the need for additional data
collection or physical experiments. We first outline several desiderata for an
ideal data-driven discovery system. Then, through DATAVOYAGER, a
proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of
these desiderata -- a feat previously unattainable -- while also highlighting
important limitations in the current system that open up opportunities for
novel ML research. We contend that achieving accurate, reliable, and robust
end-to-end discovery systems solely through the current capabilities of LGMs is
challenging. We instead advocate for fail-proof tool integration, along with
active user moderation through feedback mechanisms, to foster data-driven
scientific discoveries with efficiency and reproducibility.</div><div><a href='http://arxiv.org/abs/2402.13610v1'>2402.13610v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12391v2")'>Toward a Team of AI-made Scientists for Scientific Discovery from Gene
  Expression Data</div>
<div id='2402.12391v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T06:30:12Z</div><div>Authors: Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang</div><div style='padding-top: 10px; width: 80ex'>Machine learning has emerged as a powerful tool for scientific discovery,
enabling researchers to extract meaningful insights from complex datasets. For
instance, it has facilitated the identification of disease-predictive genes
from gene expression data, significantly advancing healthcare. However, the
traditional process for analyzing such datasets demands substantial human
effort and expertise for the data selection, processing, and analysis. To
address this challenge, we introduce a novel framework, a Team of AI-made
Scientists (TAIS), designed to streamline the scientific discovery pipeline.
TAIS comprises simulated roles, including a project manager, data engineer, and
domain expert, each represented by a Large Language Model (LLM). These roles
collaborate to replicate the tasks typically performed by data scientists, with
a specific focus on identifying disease-predictive genes. Furthermore, we have
curated a benchmark dataset to assess TAIS's effectiveness in gene
identification, demonstrating our system's potential to significantly enhance
the efficiency and scope of scientific exploration. Our findings represent a
solid step towards automating scientific discovery through large language
models.</div><div><a href='http://arxiv.org/abs/2402.12391v2'>2402.12391v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02524v2")'>Comprehensive Exploration of Synthetic Data Generation: A Survey</div>
<div id='2401.02524v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T20:23:51Z</div><div>Authors: André Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, Ian Foster</div><div style='padding-top: 10px; width: 80ex'>Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.</div><div><a href='http://arxiv.org/abs/2401.02524v2'>2401.02524v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10424v1")'>Structured Evaluation of Synthetic Tabular Data</div>
<div id='2403.10424v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:58:37Z</div><div>Authors: Scott Cheng-Hsin Yang, Baxter Eaves, Michael Schmidt, Ken Swanson, Patrick Shafto</div><div style='padding-top: 10px; width: 80ex'>Tabular data is common yet typically incomplete, small in volume, and
access-restricted due to privacy concerns. Synthetic data generation offers
potential solutions. Many metrics exist for evaluating the quality of synthetic
tabular data; however, we lack an objective, coherent interpretation of the
many metrics. To address this issue, we propose an evaluation framework with a
single, mathematical objective that posits that the synthetic data should be
drawn from the same distribution as the observed data. Through various
structural decomposition of the objective, this framework allows us to reason
for the first time the completeness of any set of metrics, as well as unifies
existing metrics, including those that stem from fidelity considerations,
downstream application, and model-based approaches. Moreover, the framework
motivates model-free baselines and a new spectrum of metrics. We evaluate
structurally informed synthesizers and synthesizers powered by deep learning.
Using our structured framework, we show that synthetic data generators that
explicitly represent tabular structure outperform other methods, especially on
smaller datasets.</div><div><a href='http://arxiv.org/abs/2403.10424v1'>2403.10424v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07842v1")'>Quantifying and Mitigating Privacy Risks for Tabular Generative Models</div>
<div id='2403.07842v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:27:49Z</div><div>Authors: Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. Pérez, Marten van Dijk, Lydia Y. Chen</div><div style='padding-top: 10px; width: 80ex'>Synthetic data from generative models emerges as the privacy-preserving
data-sharing solution. Such a synthetic data set shall resemble the original
data without revealing identifiable private information. The backbone
technology of tabular synthesizers is rooted in image generative models,
ranging from Generative Adversarial Networks (GANs) to recent diffusion models.
Recent prior work sheds light on the utility-privacy tradeoff on tabular data,
revealing and quantifying privacy risks on synthetic data. We first conduct an
exhaustive empirical analysis, highlighting the utility-privacy tradeoff of
five state-of-the-art tabular synthesizers, against eight privacy attacks, with
a special focus on membership inference attacks. Motivated by the observation
of high data quality but also high privacy risk in tabular diffusion, we
propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which
is composed of an autoencoder network to encode the tabular data and a latent
diffusion model to synthesize the latent tables. Following the emerging f-DP
framework, we apply DP-SGD to train the auto-encoder in combination with batch
clipping and use the separation value as the privacy metric to better capture
the privacy gain from DP algorithms. Our empirical evaluation demonstrates that
DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee
while also significantly enhancing the utility of synthetic data. Specifically,
compared to other DP-protected tabular generative models, DP-TLDM improves the
synthetic quality by an average of 35% in data resemblance, 15% in the utility
for downstream tasks, and 50% in data discriminability, all while preserving a
comparable level of privacy risk.</div><div><a href='http://arxiv.org/abs/2403.07842v1'>2403.07842v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00974v1")'>Downstream Task-Oriented Generative Model Selections on Synthetic Data
  Training for Fraud Detection Models</div>
<div id='2401.00974v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T23:33:56Z</div><div>Authors: Yinan Cheng, Chi-Hua Wang, Vamsi K. Potluru, Tucker Balch, Guang Cheng</div><div style='padding-top: 10px; width: 80ex'>Devising procedures for downstream task-oriented generative model selections
is an unresolved problem of practical importance. Existing studies focused on
the utility of a single family of generative models. They provided limited
insights on how synthetic data practitioners select the best family generative
models for synthetic training tasks given a specific combination of machine
learning model class and performance metric. In this paper, we approach the
downstream task-oriented generative model selections problem in the case of
training fraud detection models and investigate the best practice given
different combinations of model interpretability and model performance
constraints. Our investigation supports that, while both Neural
Network(NN)-based and Bayesian Network(BN)-based generative models are both
good to complete synthetic training task under loose model interpretability
constrain, the BN-based generative models is better than NN-based when
synthetic training fraud detection model under strict model interpretability
constrain. Our results provides practical guidance for machine learning
practitioner who is interested in replacing their training dataset from real to
synthetic, and shed lights on more general downstream task-oriented generative
model selection problems.</div><div><a href='http://arxiv.org/abs/2401.00974v1'>2401.00974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02124v1")'>Grammar-based evolutionary approach for automated workflow composition
  with domain-specific operators and ensemble diversity</div>
<div id='2402.02124v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T11:29:14Z</div><div>Authors: Rafael Barbudo, Aurora Ramírez, José Raúl Romero</div><div style='padding-top: 10px; width: 80ex'>The process of extracting valuable and novel insights from raw data involves
a series of complex steps. In the realm of Automated Machine Learning (AutoML),
a significant research focus is on automating aspects of this process,
specifically tasks like selecting algorithms and optimising their
hyper-parameters. A particularly challenging task in AutoML is automatic
workflow composition (AWC). AWC aims to identify the most effective sequence of
data preprocessing and ML algorithms, coupled with their best hyper-parameters,
for a specific dataset. However, existing AWC methods are limited in how many
and in what ways they can combine algorithms within a workflow.
  Addressing this gap, this paper introduces EvoFlow, a grammar-based
evolutionary approach for AWC. EvoFlow enhances the flexibility in designing
workflow structures, empowering practitioners to select algorithms that best
fit their specific requirements. EvoFlow stands out by integrating two
innovative features. First, it employs a suite of genetic operators, designed
specifically for AWC, to optimise both the structure of workflows and their
hyper-parameters. Second, it implements a novel updating mechanism that
enriches the variety of predictions made by different workflows. Promoting this
diversity helps prevent the algorithm from overfitting. With this aim, EvoFlow
builds an ensemble whose workflows differ in their misclassified instances.
  To evaluate EvoFlow's effectiveness, we carried out empirical validation
using a set of classification benchmarks. We begin with an ablation study to
demonstrate the enhanced performance attributable to EvoFlow's unique
components. Then, we compare EvoFlow with other AWC approaches, encompassing
both evolutionary and non-evolutionary techniques. Our findings show that
EvoFlow's specialised genetic operators and updating mechanism substantially
outperform current leading methods[..]</div><div><a href='http://arxiv.org/abs/2402.02124v1'>2402.02124v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18505v1")'>Evolving machine learning workflows through interactive AutoML</div>
<div id='2402.18505v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:34:21Z</div><div>Authors: Rafael Barbudo, Aurora Ramírez, José Raúl Romero</div><div style='padding-top: 10px; width: 80ex'>Automatic workflow composition (AWC) is a relevant problem in automated
machine learning (AutoML) that allows finding suitable sequences of
preprocessing and prediction models together with their optimal
hyperparameters. This problem can be solved using evolutionary algorithms and,
in particular, grammar-guided genetic programming (G3P). Current G3P approaches
to AWC define a fixed grammar that formally specifies how workflow elements can
be combined and which algorithms can be included. In this paper we present
\ourmethod, an interactive G3P algorithm that allows users to dynamically
modify the grammar to prune the search space and focus on their regions of
interest. Our proposal is the first to combine the advantages of a G3P method
with ideas from interactive optimisation and human-guided machine learning, an
area little explored in the context of AutoML. To evaluate our approach, we
present an experimental study in which 20 participants interact with \ourmethod
to evolve workflows according to their preferences. Our results confirm that
the collaboration between \ourmethod and humans allows us to find
high-performance workflows in terms of accuracy that require less tuning time
than those found without human intervention.</div><div><a href='http://arxiv.org/abs/2402.18505v1'>2402.18505v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07041v1")'>Ant Colony Sampling with GFlowNets for Combinatorial Optimization</div>
<div id='2403.07041v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:26:06Z</div><div>Authors: Minsu Kim, Sanghyeok Choi, Jiwoo Son, Hyeonah Kim, Jinkyoo Park, Yoshua Bengio</div><div style='padding-top: 10px; width: 80ex'>This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel
neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS
integrates generative flow networks (GFlowNets) with the ant colony
optimization (ACO) methodology. GFlowNets, a generative model that learns a
constructive policy in combinatorial spaces, enhance ACO by providing an
informed prior distribution of decision variables conditioned on input graph
instances. Furthermore, we introduce a novel combination of training tricks,
including search-guided local exploration, energy normalization, and energy
shaping to improve GFACS. Our experimental results demonstrate that GFACS
outperforms baseline ACO algorithms in seven CO tasks and is competitive with
problem-specific heuristics for vehicle routing problems. The source code is
available at \url{https://github.com/ai4co/gfacs}.</div><div><a href='http://arxiv.org/abs/2403.07041v1'>2403.07041v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.09547v1")'>How do Machine Learning Projects use Continuous Integration Practices?
  An Empirical Study on GitHub Actions</div>
<div id='2403.09547v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T16:35:39Z</div><div>Authors: João Helis Bernardo, Daniel Alencar da Costa, Sérgio Queiroz de Medeiros, Uirá Kulesza</div><div style='padding-top: 10px; width: 80ex'>Continuous Integration (CI) is a well-established practice in traditional
software development, but its nuances in the domain of Machine Learning (ML)
projects remain relatively unexplored. Given the distinctive nature of ML
development, understanding how CI practices are adopted in this context is
crucial for tailoring effective approaches. In this study, we conduct a
comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92
non-ML projects). Our investigation comprises both quantitative and qualitative
dimensions, aiming to uncover differences in CI adoption between ML and non-ML
projects. Our findings indicate that ML projects often require longer build
durations, and medium-sized ML projects exhibit lower test coverage compared to
non-ML projects. Moreover, small and medium-sized ML projects show a higher
prevalence of increasing build duration trends compared to their non-ML
counterparts. Additionally, our qualitative analysis illuminates the
discussions around CI in both ML and non-ML projects, encompassing themes like
CI Build Execution and Status, CI Testing, and CI Infrastructure. These
insights shed light on the unique challenges faced by ML projects in adopting
CI practices effectively.</div><div><a href='http://arxiv.org/abs/2403.09547v1'>2403.09547v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09651v1")'>Practitioners' Challenges and Perceptions of CI Build Failure
  Predictions at Atlassian</div>
<div id='2402.09651v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T01:28:18Z</div><div>Authors: Yang Hong, Chakkrit Tantithamthavorn, Jirat Pasuksmit, Patanamon Thongtanunam, Arik Friedman, Xing Zhao, Anton Krasikov</div><div style='padding-top: 10px; width: 80ex'>Continuous Integration (CI) build failures could significantly impact the
software development process and teams, such as delaying the release of new
features and reducing developers' productivity. In this work, we report on an
empirical study that investigates CI build failures throughout product
development at Atlassian. Our quantitative analysis found that the repository
dimension is the key factor influencing CI build failures. In addition, our
qualitative survey revealed that Atlassian developers perceive CI build
failures as challenging issues in practice. Furthermore, we found that the CI
build prediction can not only provide proactive insight into CI build failures
but also facilitate the team's decision-making. Our study sheds light on the
challenges and expectations involved in integrating CI build prediction tools
into the Bitbucket environment, providing valuable insights for enhancing CI
processes.</div><div><a href='http://arxiv.org/abs/2402.09651v1'>2402.09651v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07797v1")'>Joint Selection: Adaptively Incorporating Public Information for Private
  Synthetic Data</div>
<div id='2403.07797v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:34:07Z</div><div>Authors: Miguel Fuentes, Brett Mullins, Ryan McKenna, Gerome Miklau, Daniel Sheldon</div><div style='padding-top: 10px; width: 80ex'>Mechanisms for generating differentially private synthetic data based on
marginals and graphical models have been successful in a wide range of
settings. However, one limitation of these methods is their inability to
incorporate public data. Initializing a data generating model by pre-training
on public data has shown to improve the quality of synthetic data, but this
technique is not applicable when model structure is not determined a priori. We
develop the mechanism jam-pgm, which expands the adaptive measurements
framework to jointly select between measuring public data and private data.
This technique allows for public data to be included in a graphical-model-based
mechanism. We show that jam-pgm is able to outperform both publicly assisted
and non publicly assisted synthetic data generation mechanisms even when the
public data distribution is biased.</div><div><a href='http://arxiv.org/abs/2403.07797v1'>2403.07797v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08012v1")'>Online Differentially Private Synthetic Data Generation</div>
<div id='2402.08012v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:21:14Z</div><div>Authors: Yiyun He, Roman Vershynin, Yizhe Zhu</div><div style='padding-top: 10px; width: 80ex'>We present a polynomial-time algorithm for online differentially private
synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and
an infinite time horizon, we develop an online algorithm that generates a
differentially private synthetic dataset at each time $t$. This algorithm
achieves a near-optimal accuracy bound of $O(t^{-1/d}\log(t))$ for $d\geq 2$
and $O(t^{-1}\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This
result generalizes the previous work on the continual release model for
counting queries to include Lipschitz queries. Compared to the offline case,
where the entire dataset is available at once, our approach requires only an
extra polylog factor in the accuracy bound.</div><div><a href='http://arxiv.org/abs/2402.08012v1'>2402.08012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.18803v1")'>To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair
  Training on Shared Models</div>
<div id='2402.18803v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T02:16:57Z</div><div>Authors: Cyrus Cousins, I. Elizabeth Kumar, Suresh Venkatasubramanian</div><div style='padding-top: 10px; width: 80ex'>In fair machine learning, one source of performance disparities between
groups is over-fitting to groups with relatively few training samples. We
derive group-specific bounds on the generalization error of welfare-centric
fair machine learning that benefit from the larger sample size of the majority
group. We do this by considering group-specific Rademacher averages over a
restricted hypothesis class, which contains the family of models likely to
perform well with respect to a fair learning objective (e.g., a power-mean).
Our simulations demonstrate these bounds improve over a naive method, as
expected by theory, with particularly significant improvement for smaller group
sizes.</div><div><a href='http://arxiv.org/abs/2402.18803v1'>2402.18803v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14929v1")'>Federated Fairness without Access to Sensitive Groups</div>
<div id='2402.14929v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T19:24:59Z</div><div>Authors: Afroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, Miguel Rodrigues</div><div style='padding-top: 10px; width: 80ex'>Current approaches to group fairness in federated learning assume the
existence of predefined and labeled sensitive groups during training. However,
due to factors ranging from emerging regulations to dynamics and
location-dependency of protected groups, this assumption may be unsuitable in
many real-world scenarios. In this work, we propose a new approach to guarantee
group fairness that does not rely on any predefined definition of sensitive
groups or additional labels. Our objective allows the federation to learn a
Pareto efficient global model ensuring worst-case group fairness and it
enables, via a single hyper-parameter, trade-offs between fairness and utility,
subject only to a group size constraint. This implies that any sufficiently
large subset of the population is guaranteed to receive at least a minimum
level of utility performance from the model. The proposed objective encompasses
existing approaches as special cases, such as empirical risk minimization and
subgroup robustness objectives from centralized machine learning. We provide an
algorithm to solve this problem in federation that enjoys convergence and
excess risk guarantees. Our empirical results indicate that the proposed
approach can effectively improve the worst-performing group that may be present
without unnecessarily hurting the average performance, exhibits superior or
comparable performance to relevant baselines, and achieves a large set of
solutions with different fairness-utility trade-offs.</div><div><a href='http://arxiv.org/abs/2402.14929v1'>2402.14929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07586v2")'>Unveiling Group-Specific Distributed Concept Drift: A Fairness
  Imperative in Federated Learning</div>
<div id='2402.07586v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:35:25Z</div><div>Authors: Teresa Salazar, João Gama, Helder Araújo, Pedro Henriques Abreu</div><div style='padding-top: 10px; width: 80ex'>In the evolving field of machine learning, ensuring fairness has become a
critical concern, prompting the development of algorithms designed to mitigate
discriminatory outcomes in decision-making processes. However, achieving
fairness in the presence of group-specific concept drift remains an unexplored
frontier, and our research represents pioneering efforts in this regard.
Group-specific concept drift refers to situations where one group experiences
concept drift over time while another does not, leading to a decrease in
fairness even if accuracy remains fairly stable. Within the framework of
federated learning, where clients collaboratively train models, its distributed
nature further amplifies these challenges since each client can experience
group-specific concept drift independently while still sharing the same
underlying concept, creating a complex and dynamic environment for maintaining
fairness. One of the significant contributions of our research is the
formalization and introduction of the problem of group-specific concept drift
and its distributed counterpart, shedding light on its critical importance in
the realm of fairness. In addition, leveraging insights from prior research, we
adapt an existing distributed concept drift adaptation algorithm to tackle
group-specific distributed concept drift which utilizes a multi-model approach,
a local group-specific drift detection mechanism, and continuous clustering of
models over time. The findings from our experiments highlight the importance of
addressing group-specific concept drift and its distributed counterpart to
advance fairness in machine learning.</div><div><a href='http://arxiv.org/abs/2402.07586v2'>2402.07586v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05146v2")'>Federated Unlearning: A Survey on Methods, Design Guidelines, and
  Evaluation Metrics</div>
<div id='2401.05146v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T13:26:19Z</div><div>Authors: Nicolò Romandini, Alessio Mora, Carlo Mazzocca, Rebecca Montanari, Paolo Bellavista</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) enables collaborative training of a Machine Learning
(ML) model across multiple parties, facilitating the preservation of users' and
institutions' privacy by keeping data stored locally. Instead of centralizing
raw data, FL exchanges locally refined model parameters to build a global model
incrementally. While FL is more compliant with emerging regulations such as the
European General Data Protection Regulation (GDPR), ensuring the right to be
forgotten in this context - allowing FL participants to remove their data
contributions from the learned model - remains unclear. In addition, it is
recognized that malicious clients may inject backdoors into the global model
through updates, e.g. to generate mispredictions on specially crafted data
examples. Consequently, there is the need for mechanisms that can guarantee
individuals the possibility to remove their data and erase malicious
contributions even after aggregation, without compromising the already acquired
"good" knowledge. This highlights the necessity for novel Federated Unlearning
(FU) algorithms, which can efficiently remove specific clients' contributions
without full model retraining. This survey provides background concepts,
empirical evidence, and practical guidelines to design/implement efficient FU
schemes. Our study includes a detailed analysis of the metrics for evaluating
unlearning in FL and presents an in-depth literature review categorizing
state-of-the-art FU contributions under a novel taxonomy. Finally, we outline
the most relevant and still open technical challenges, by identifying the most
promising research directions in the field.</div><div><a href='http://arxiv.org/abs/2401.05146v2'>2401.05146v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02437v1")'>SoK: Challenges and Opportunities in Federated Unlearning</div>
<div id='2403.02437v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:35:08Z</div><div>Authors: Hyejun Jeong, Shiqing Ma, Amir Houmansadr</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL), introduced in 2017, facilitates collaborative
learning between non-trusting parties with no need for the parties to
explicitly share their data among themselves. This allows training models on
user data while respecting privacy regulations such as GDPR and CPRA. However,
emerging privacy requirements may mandate model owners to be able to
\emph{forget} some learned data, e.g., when requested by data owners or law
enforcement. This has given birth to an active field of research called
\emph{machine unlearning}. In the context of FL, many techniques developed for
unlearning in centralized settings are not trivially applicable! This is due to
the unique differences between centralized and distributed learning, in
particular, interactivity, stochasticity, heterogeneity, and limited
accessibility in FL. In response, a recent line of work has focused on
developing unlearning mechanisms tailored to FL.
  This SoK paper aims to take a deep look at the \emph{federated unlearning}
literature, with the goal of identifying research trends and challenges in this
emerging field. By carefully categorizing papers published on FL unlearning
(since 2020), we aim to pinpoint the unique complexities of federated
unlearning, highlighting limitations on directly applying centralized
unlearning methods. We compare existing federated unlearning methods regarding
influence removal and performance recovery, compare their threat models and
assumptions, and discuss their implications and limitations. For instance, we
analyze the experimental setup of FL unlearning studies from various
perspectives, including data heterogeneity and its simulation, the datasets
used for demonstration, and evaluation metrics. Our work aims to offer insights
and suggestions for future research on federated unlearning.</div><div><a href='http://arxiv.org/abs/2403.02437v1'>2403.02437v1</a></div>
</div></div>
    <div><a href="arxiv_5.html">Prev (5)</a></div>
    <div><a href="arxiv_7.html">Next (7)</a></div>
    