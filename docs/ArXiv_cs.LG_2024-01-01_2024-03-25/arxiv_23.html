
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11837v2")'>Self-Guided Robust Graph Structure Refinement</div>
<div id='2402.11837v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T05:00:07Z</div><div>Authors: Yeonjun In, Kanghoon Yoon, Kibum Kim, Kijung Shin, Chanyoung Park</div><div style='padding-top: 10px; width: 80ex'>Recent studies have revealed that GNNs are vulnerable to adversarial attacks.
To defend against such attacks, robust graph structure refinement (GSR) methods
aim at minimizing the effect of adversarial edges based on node features, graph
structure, or external information. However, we have discovered that existing
GSR methods are limited by narrowassumptions, such as assuming clean node
features, moderate structural attacks, and the availability of external clean
graphs, resulting in the restricted applicability in real-world scenarios. In
this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a
clean sub-graph found within the given attacked graph itself. Furthermore, we
propose a novel graph augmentation and a group-training strategy to handle the
two technical challenges in the clean sub-graph extraction: 1) loss of
structural information, and 2) imbalanced node degree distribution. Extensive
experiments demonstrate the effectiveness of SG-GSR under various scenarios
including non-targeted attacks, targeted attacks, feature attacks, e-commerce
fraud, and noisy node labels. Our code is available at
https://github.com/yeonjun-in/torch-SG-GSR.</div><div><a href='http://arxiv.org/abs/2402.11837v2'>2402.11837v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15615v1")'>Addressing Noise and Efficiency Issues in Graph-Based Machine Learning
  Models From the Perspective of Adversarial Attack</div>
<div id='2401.15615v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T10:03:37Z</div><div>Authors: Yongyu Wang</div><div style='padding-top: 10px; width: 80ex'>Given that no existing graph construction method can generate a perfect graph
for a given dataset, graph-based algorithms are invariably affected by the
plethora of redundant and erroneous edges present within the constructed
graphs. In this paper, we propose treating these noisy edges as adversarial
attack and use a spectral adversarial robustness evaluation method to diminish
the impact of noisy edges on the performance of graph algorithms. Our method
identifies those points that are less vulnerable to noisy edges and leverages
only these robust points to perform graph-based algorithms. Our experiments
with spectral clustering, one of the most representative and widely utilized
graph algorithms, reveal that our methodology not only substantially elevates
the precision of the algorithm but also greatly accelerates its computational
efficiency by leveraging only a select number of robust data points.</div><div><a href='http://arxiv.org/abs/2401.15615v1'>2401.15615v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00014v1")'>GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional
  Encoding and Attentive Fusion</div>
<div id='2403.00014v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T09:35:54Z</div><div>Authors: Le Cheng, Peican Zhu, Keke Tang, Chao Gao, Zhen Wang</div><div style='padding-top: 10px; width: 80ex'>Source detection in graphs has demonstrated robust efficacy in the domain of
rumor source identification. Although recent solutions have enhanced
performance by leveraging deep neural networks, they often require complete
user data. In this paper, we address a more challenging task, rumor source
detection with incomplete user data, and propose a novel framework, i.e.,
Source Detection in Graphs with Incomplete Nodes via Positional Encoding and
Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach
utilizes a positional embedding module to distinguish nodes that are incomplete
and employs a self-attention mechanism to focus on nodes with greater
information transmission capacity. To mitigate the prediction bias caused by
the significant disparity between the numbers of source and non-source nodes,
we also introduce a class-balancing mechanism. Extensive experiments validate
the effectiveness of GIN-SD and its superiority to state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.00014v1'>2403.00014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08401v1")'>LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph
  Attention Network for Fake News Detection</div>
<div id='2402.08401v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T12:02:37Z</div><div>Authors: Batool Lakzaei, Mostafa Haghir Chehreghani, Alireza Bagheri</div><div style='padding-top: 10px; width: 80ex'>In the era of widespread social networks, the rapid dissemination of fake
news has emerged as a significant threat, inflicting detrimental consequences
across various dimensions of people's lives. Machine learning and deep learning
approaches have been extensively employed for identifying fake news. However, a
significant challenge in identifying fake news is the limited availability of
labeled news datasets. Therefore, the One-Class Learning (OCL) approach,
utilizing only a small set of labeled data from the interest class, can be a
suitable approach to address this challenge. On the other hand, representing
data as a graph enables access to diverse content and structural information,
and label propagation methods on graphs can be effective in predicting node
labels. In this paper, we adopt a graph-based model for data representation and
introduce a semi-supervised and one-class approach for fake news detection,
called LOSS-GAT. Initially, we employ a two-step label propagation algorithm,
utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize
news into two groups: interest (fake) and non-interest (real). Subsequently, we
enhance the graph structure using structural augmentation techniques.
Ultimately, we predict the final labels for all unlabeled data using a GNN that
induces randomness within the local neighborhood of nodes through the
aggregation function. We evaluate our proposed method on five common datasets
and compare the results against a set of baseline models, including both OCL
and binary labeled models. The results demonstrate that LOSS-GAT achieves a
notable improvement, surpassing 10%, with the advantage of utilizing only a
limited set of labeled fake news. Noteworthy, LOSS-GAT even outperforms binary
labeled models.</div><div><a href='http://arxiv.org/abs/2402.08401v1'>2402.08401v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08228v2")'>Investigating Out-of-Distribution Generalization of GNNs: An
  Architecture Perspective</div>
<div id='2402.08228v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T05:38:45Z</div><div>Authors: Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have exhibited remarkable performance under the
assumption that test data comes from the same distribution of training data.
However, in real-world scenarios, this assumption may not always be valid.
Consequently, there is a growing focus on exploring the Out-of-Distribution
(OOD) problem in the context of graphs. Most existing efforts have primarily
concentrated on improving graph OOD generalization from two
\textbf{model-agnostic} perspectives: data-driven methods and strategy-based
learning. However, there has been limited attention dedicated to investigating
the impact of well-known \textbf{GNN model architectures} on graph OOD
generalization, which is orthogonal to existing research. In this work, we
provide the first comprehensive investigation of OOD generalization on graphs
from an architecture perspective, by examining the common building blocks of
modern GNNs. Through extensive experiments, we reveal that both the graph
self-attention mechanism and the decoupled architecture contribute positively
to graph OOD generalization. In contrast, we observe that the linear
classification layer tends to compromise graph OOD generalization capability.
Furthermore, we provide in-depth theoretical insights and discussions to
underpin these discoveries. These insights have empowered us to develop a novel
GNN backbone model, DGAT, designed to harness the robust properties of both
graph self-attention mechanism and the decoupled architecture. Extensive
experimental results demonstrate the effectiveness of our model under graph
OOD, exhibiting substantial and consistent enhancements across various training
strategies.</div><div><a href='http://arxiv.org/abs/2402.08228v2'>2402.08228v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03599v1")'>Learning Invariant Representations of Graph Neural Networks via Cluster
  Generalization</div>
<div id='2403.03599v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T10:36:56Z</div><div>Authors: Donglin Xia, Xiao Wang, Nian Liu, Chuan Shi</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have become increasingly popular in modeling
graph-structured data due to their ability to learn node representations by
aggregating local structure information. However, it is widely acknowledged
that the test graph structure may differ from the training graph structure,
resulting in a structure shift. In this paper, we experimentally find that the
performance of GNNs drops significantly when the structure shift happens,
suggesting that the learned models may be biased towards specific structure
patterns. To address this challenge, we propose the Cluster Information
Transfer (CIT) mechanism (Code available at
https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant
representations for GNNs, thereby improving their generalization ability to
various and unknown test graphs with structure shift. The CIT mechanism
achieves this by combining different cluster information with the nodes while
preserving their cluster-independent information. By generating nodes across
different clusters, the mechanism significantly enhances the diversity of the
nodes and helps GNNs learn the invariant representations. We provide a
theoretical analysis of the CIT mechanism, showing that the impact of changing
clusters during structure shift can be mitigated after transfer. Additionally,
the proposed mechanism is a plug-in that can be easily used to improve existing
GNNs. We comprehensively evaluate our proposed method on three typical
structure shift scenarios, demonstrating its effectiveness in enhancing GNNs'
performance.</div><div><a href='http://arxiv.org/abs/2403.03599v1'>2403.03599v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04482v1")'>On the Topology Awareness and Generalization Performance of Graph Neural
  Networks</div>
<div id='2403.04482v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T13:33:30Z</div><div>Authors: Junwei Su, Chuan Wu</div><div style='padding-top: 10px; width: 80ex'>Many computer vision and machine learning problems are modelled as learning
tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant
tool for learning representations of graph-structured data. A key feature of
GNNs is their use of graph structures as input, enabling them to exploit the
graphs' inherent topological properties-known as the topology awareness of
GNNs. Despite the empirical successes of GNNs, the influence of topology
awareness on generalization performance remains unexplored, particularly for
node-level tasks that diverge from the assumption of data being independent and
identically distributed (I.I.D.). The precise definition and characterization
of the topology awareness of GNNs, especially concerning different topological
features, are still unclear. This paper introduces a comprehensive framework to
characterize the topology awareness of GNNs across any topological feature.
Using this framework, we investigate the effects of topology awareness on GNN
generalization performance. Contrary to the prevailing belief that enhancing
the topology awareness of GNNs is always advantageous, our analysis reveals a
critical insight: improving the topology awareness of GNNs may inadvertently
lead to unfair generalization across structural groups, which might not be
desired in some scenarios. Additionally, we conduct a case study using the
intrinsic graph metric, the shortest path distance, on various benchmark
datasets. The empirical results of this case study confirm our theoretical
insights. Moreover, we demonstrate the practical applicability of our framework
by using it to tackle the cold start problem in graph active learning.</div><div><a href='http://arxiv.org/abs/2403.04482v1'>2403.04482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07845v2")'>Unsupervised Optimisation of GNNs for Node Clustering</div>
<div id='2402.07845v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:53:43Z</div><div>Authors: William Leeney, Ryan McConville</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) can be trained to detect communities within a
graph by learning from the duality of feature and connectivity information.
Currently, the common approach for optimisation of GNNs is to use comparisons
to ground-truth for hyperparameter tuning and model selection. In this work, we
show that nodes can be clustered into communities with GNNs by solely
optimising for modularity, without any comparison to ground-truth. Although
modularity is a graph partitioning quality metric, we show that this can be
used to optimise GNNs that also encode features without a drop in performance.
We take it a step further and also study whether the unsupervised metric
performance can predict ground-truth performance. To investigate why modularity
can be used to optimise GNNs, we design synthetic experiments that show the
limitations of this approach. The synthetic graphs are created to highlight
current capabilities in distinct, random and zero information space partitions
in attributed graphs. We conclude that modularity can be used for
hyperparameter optimisation and model selection on real-world datasets as well
as being a suitable proxy for predicting ground-truth performance, however,
GNNs fail to balance the information duality when the spaces contain
conflicting signals.</div><div><a href='http://arxiv.org/abs/2402.07845v2'>2402.07845v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17906v1")'>Representation learning in multiplex graphs: Where and how to fuse
  information?</div>
<div id='2402.17906v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:47:06Z</div><div>Authors: Piotr Bielak, Tomasz Kajdanowicz</div><div style='padding-top: 10px; width: 80ex'>In recent years, unsupervised and self-supervised graph representation
learning has gained popularity in the research community. However, most
proposed methods are focused on homogeneous networks, whereas real-world graphs
often contain multiple node and edge types. Multiplex graphs, a special type of
heterogeneous graphs, possess richer information, provide better modeling
capabilities and integrate more detailed data from potentially different
sources. The diverse edge types in multiplex graphs provide more context and
insights into the underlying processes of representation learning. In this
paper, we tackle the problem of learning representations for nodes in multiplex
networks in an unsupervised or self-supervised manner. To that end, we explore
diverse information fusion schemes performed at different levels of the graph
processing pipeline. The detailed analysis and experimental evaluation of
various scenarios inspired us to propose improvements in how to construct GNN
architectures that deal with multiplex graphs.</div><div><a href='http://arxiv.org/abs/2402.17906v1'>2402.17906v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11074v1")'>On The Temporal Domain of Differential Equation Inspired Graph Neural
  Networks</div>
<div id='2401.11074v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T01:12:57Z</div><div>Authors: Moshe Eliasof, Eldad Haber, Eran Treister, Carola-Bibiane Schönlieb</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling
complex relationships in graph-structured data. A recent innovation in this
field is the family of Differential Equation-Inspired Graph Neural Networks
(DE-GNNs), which leverage principles from continuous dynamical systems to model
information flow on graphs with built-in properties such as feature smoothing
or preservation. However, existing DE-GNNs rely on first or second-order
temporal dependencies. In this paper, we propose a neural extension to those
pre-defined temporal dependencies. We show that our model, called TDE-GNN, can
capture a wide range of temporal dynamics that go beyond typical first or
second-order methods, and provide use cases where existing temporal models are
challenged. We demonstrate the benefit of learning the temporal dependencies
using our method rather than using pre-defined temporal dynamics on several
graph benchmarks.</div><div><a href='http://arxiv.org/abs/2401.11074v1'>2401.11074v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16078v1")'>Beyond Spatio-Temporal Representations: Evolving Fourier Transform for
  Temporal Graphs</div>
<div id='2402.16078v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T13:05:25Z</div><div>Authors: Anson Bastos, Kuldeep Singh, Abhishek Nadgeri, Manish Singh, Toyotaro Suzumura</div><div style='padding-top: 10px; width: 80ex'>We present the Evolving Graph Fourier Transform (EFT), the first invertible
spectral transform that captures evolving representations on temporal graphs.
We motivate our work by the inadequacy of existing methods for capturing the
evolving graph spectra, which are also computationally expensive due to the
temporal aspect along with the graph vertex domain. We view the problem as an
optimization over the Laplacian of the continuous time dynamic graph.
Additionally, we propose pseudo-spectrum relaxations that decompose the
transformation process, making it highly computationally efficient. The EFT
method adeptly captures the evolving graph's structural and positional
properties, making it effective for downstream tasks on evolving graphs. Hence,
as a reference implementation, we develop a simple neural model induced with
EFT for capturing evolving graph spectra. We empirically validate our
theoretical findings on a number of large-scale and standard temporal graph
benchmarks and demonstrate that our model achieves state-of-the-art
performance.</div><div><a href='http://arxiv.org/abs/2402.16078v1'>2402.16078v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05026v1")'>Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts</div>
<div id='2403.05026v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T04:07:23Z</div><div>Authors: Zeyang Zhang, Xin Wang, Ziwei Zhang, Zhou Qin, Weigao Wen, Hui Xue, Haoyang Li, Wenwu Zhu</div><div style='padding-top: 10px; width: 80ex'>Dynamic graph neural networks (DyGNNs) currently struggle with handling
distribution shifts that are inherent in dynamic graphs. Existing work on
DyGNNs with out-of-distribution settings only focuses on the time domain,
failing to handle cases involving distribution shifts in the spectral domain.
In this paper, we discover that there exist cases with distribution shifts
unobservable in the time domain while observable in the spectral domain, and
propose to study distribution shifts on dynamic graphs in the spectral domain
for the first time. However, this investigation poses two key challenges: i) it
is non-trivial to capture different graph patterns that are driven by various
frequency components entangled in the spectral domain; and ii) it remains
unclear how to handle distribution shifts with the discovered spectral
patterns. To address these challenges, we propose Spectral Invariant Learning
for Dynamic Graphs under Distribution Shifts (SILD), which can handle
distribution shifts on dynamic graphs by capturing and utilizing invariant and
variant spectral patterns. Specifically, we first design a DyGNN with Fourier
transform to obtain the ego-graph trajectory spectrums, allowing the mixed
dynamic graph patterns to be transformed into separate frequency components. We
then develop a disentangled spectrum mask to filter graph dynamics from various
frequency components and discover the invariant and variant spectral patterns.
Finally, we propose invariant spectral filtering, which encourages the model to
rely on invariant patterns for generalization under distribution shifts.
Experimental results on synthetic and real-world dynamic graph datasets
demonstrate the superiority of our method for both node classification and link
prediction tasks under distribution shifts.</div><div><a href='http://arxiv.org/abs/2403.05026v1'>2403.05026v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12843v1")'>An embedding-based distance for temporal graphs</div>
<div id='2401.12843v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T15:25:21Z</div><div>Authors: Lorenzo Dall'Amico, Alain Barrat, Ciro Cattuto</div><div style='padding-top: 10px; width: 80ex'>We define a distance between temporal graphs based on graph embeddings built
using time-respecting random walks. We study both the case of matched graphs,
when there exists a known relation between the nodes, and the unmatched case,
when such a relation is unavailable and the graphs may be of different sizes.
We illustrate the interest of our distance definition, using both real and
synthetic temporal network data, by showing its ability to discriminate between
graphs with different structural and temporal properties. Leveraging
state-of-the-art machine learning techniques, we propose an efficient
implementation of distance computation that is viable for large-scale temporal
graphs.</div><div><a href='http://arxiv.org/abs/2401.12843v1'>2401.12843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06559v1")'>A General Benchmark Framework is Dynamic Graph Neural Network Need</div>
<div id='2401.06559v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T13:12:07Z</div><div>Authors: Yusen Zhang</div><div style='padding-top: 10px; width: 80ex'>Dynamic graph learning is crucial for modeling real-world systems with
evolving relationships and temporal dynamics. However, the lack of a unified
benchmark framework in current research has led to inaccurate evaluations of
dynamic graph models. This paper highlights the significance of dynamic graph
learning and its applications in various domains. It emphasizes the need for a
standardized benchmark framework that captures temporal dynamics, evolving
graph structures, and downstream task requirements. Establishing a unified
benchmark will help researchers understand the strengths and limitations of
existing models, foster innovation, and advance dynamic graph learning. In
conclusion, this paper identifies the lack of a standardized benchmark
framework as a current limitation in dynamic graph learning research . Such a
framework will facilitate accurate model evaluation, drive advancements in
dynamic graph learning techniques, and enable the development of more effective
models for real-world applications.</div><div><a href='http://arxiv.org/abs/2401.06559v1'>2401.06559v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08147v1")'>Machine Learning on Dynamic Graphs: A Survey on Applications</div>
<div id='2401.08147v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T06:40:24Z</div><div>Authors: Sanaz Hasanzadeh Fard</div><div style='padding-top: 10px; width: 80ex'>Dynamic graph learning has gained significant attention as it offers a
powerful means to model intricate interactions among entities across various
real-world and scientific domains. Notably, graphs serve as effective
representations for diverse networks such as transportation, brain, social, and
internet networks. Furthermore, the rapid advancements in machine learning have
expanded the scope of dynamic graph applications beyond the aforementioned
domains. In this paper, we present a review of lesser-explored applications of
dynamic graph learning. This study revealed the potential of machine learning
on dynamic graphs in addressing challenges across diverse domains, including
those with limited levels of association with the field.</div><div><a href='http://arxiv.org/abs/2401.08147v1'>2401.08147v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16387v1")'>On the Generalization Capability of Temporal Graph Learning Algorithms:
  Theoretical Insights and a Simpler Method</div>
<div id='2402.16387v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T08:22:22Z</div><div>Authors: Weilin Cong, Jian Kang, Hanghang Tong, Mehrdad Mahdavi</div><div style='padding-top: 10px; width: 80ex'>Temporal Graph Learning (TGL) has become a prevalent technique across diverse
real-world applications, especially in domains where data can be represented as
a graph and evolves over time. Although TGL has recently seen notable progress
in algorithmic solutions, its theoretical foundations remain largely
unexplored. This paper aims at bridging this gap by investigating the
generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based,
and memory-based methods) under the finite-wide over-parameterized regime. We
establish the connection between the generalization error of TGL algorithms and
"the number of layers/steps" in the GNN-/RNN-based TGL methods and "the
feature-label alignment (FLA) score", where FLA can be used as a proxy for the
expressive power and explains the performance of memory-based methods. Guided
by our theoretical analysis, we propose Simplified-Temporal-Graph-Network,
which enjoys a small generalization error, improved overall performance, and
lower model complexity. Extensive experiments on real-world datasets
demonstrate the effectiveness of our method. Our theoretical findings and
proposed algorithm offer essential insights into TGL from a theoretical
standpoint, laying the groundwork for the designing practical TGL algorithms in
future studies.</div><div><a href='http://arxiv.org/abs/2402.16387v1'>2402.16387v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01964v1")'>No Need to Look Back: An Efficient and Scalable Approach for Temporal
  Network Representation Learning</div>
<div id='2402.01964v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T00:12:36Z</div><div>Authors: Yuhong Luo, Pan Li</div><div style='padding-top: 10px; width: 80ex'>Temporal graph representation learning (TGRL) is crucial for modeling
complex, dynamic systems in real-world networks. Traditional TGRL methods,
though effective, suffer from high computational demands and inference latency.
This is mainly induced by their inefficient sampling of temporal neighbors by
backtracking the interaction history of each node when making model inference.
This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB).
NLB employs a "forward recent sampling" strategy, which bypasses the need for
backtracking historical interactions. This strategy is implemented using a
GPU-executable size-constrained hash table for each node, recording
down-sampled recent interactions, which enables rapid response to queries with
minimal inference latency. The maintenance of this hash table is highly
efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing,
maximizing programmability, parallelism, and power efficiency. Empirical
evaluations demonstrate that NLB matches or surpasses state-of-the-art methods
in accuracy for link prediction and node classification across six real-world
datasets. Significantly, it is 1.32-4.40 $\times$ faster in training, 1.2-7.94
$\times$ more energy efficient, and 1.97-5.02 $\times$ more effective in
reducing inference latency compared to the most competitive baselines. The link
to the code: https://github.com/Graph-COM/NLB.</div><div><a href='http://arxiv.org/abs/2402.01964v1'>2402.01964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13157v1")'>Time-Aware Knowledge Representations of Dynamic Objects with
  Multidimensional Persistence</div>
<div id='2401.13157v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T00:33:53Z</div><div>Authors: Baris Coskunuzer, Ignacio Segovia-Dominguez, Yuzhou Chen, Yulia R. Gel</div><div style='padding-top: 10px; width: 80ex'>Learning time-evolving objects such as multivariate time series and dynamic
networks requires the development of novel knowledge representation mechanisms
and neural network architectures, which allow for capturing implicit
time-dependent information contained in the data. Such information is typically
not directly observed but plays a key role in the learning task performance. In
turn, lack of time dimension in knowledge encoding mechanisms for
time-dependent data leads to frequent model updates, poor learning performance,
and, as a result, subpar decision-making. Here we propose a new approach to a
time-aware knowledge representation mechanism that notably focuses on implicit
time-dependent topological information along multiple geometric dimensions. In
particular, we propose a new approach, named \textit{Temporal MultiPersistence}
(TMP), which produces multidimensional topological fingerprints of the data by
using the existing single parameter topological summaries. The main idea behind
TMP is to merge the two newest directions in topological representation
learning, that is, multi-persistence which simultaneously describes data shape
evolution along multiple key parameters, and zigzag persistence to enable us to
extract the most salient data shape information over time. We derive
theoretical guarantees of TMP vectorizations and show its utility, in
application to forecasting on benchmark traffic flow, Ethereum blockchain, and
electrocardiogram datasets, demonstrating the competitive performance,
especially, in scenarios of limited data records. In addition, our TMP method
improves the computational efficiency of the state-of-the-art multipersistence
summaries up to 59.5 times.</div><div><a href='http://arxiv.org/abs/2401.13157v1'>2401.13157v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01242v1")'>Encoding Binary Events from Continuous Time Series in Rooted Trees using
  Contrastive Learning</div>
<div id='2401.01242v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T15:18:23Z</div><div>Authors: Tobias Engelhardt Rasmussen, Siv Sørensen</div><div style='padding-top: 10px; width: 80ex'>Broadband infrastructure owners do not always know how their customers are
connected in the local networks, which are structured as rooted trees. A recent
study is able to infer the topology of a local network using discrete time
series data from the leaves of the tree (customers). In this study we propose a
contrastive approach for learning a binary event encoder from continuous time
series data. As a preliminary result, we show that our approach has some
potential in learning a valuable encoder.</div><div><a href='http://arxiv.org/abs/2401.01242v1'>2401.01242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17838v1")'>A Cross-View Hierarchical Graph Learning Hypernetwork for Skill
  Demand-Supply Joint Prediction</div>
<div id='2401.17838v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T13:56:08Z</div><div>Authors: Wenshuo Chao, Zhaopeng Qiu, Likang Wu, Zhuoning Guo, Zhi Zheng, Hengshu Zhu, Hao Liu</div><div style='padding-top: 10px; width: 80ex'>The rapidly changing landscape of technology and industries leads to dynamic
skill requirements, making it crucial for employees and employers to anticipate
such shifts to maintain a competitive edge in the labor market. Existing
efforts in this area either rely on domain-expert knowledge or regarding skill
evolution as a simplified time series forecasting problem. However, both
approaches overlook the sophisticated relationships among different skills and
the inner-connection between skill demand and supply variations. In this paper,
we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH)
framework for joint skill demand-supply prediction. Specifically, CHGH is an
encoder-decoder network consisting of i) a cross-view graph encoder to capture
the interconnection between skill demand and supply, ii) a hierarchical graph
encoder to model the co-evolution of skills from a cluster-wise perspective,
and iii) a conditional hyper-decoder to jointly predict demand and supply
variations by incorporating historical demand-supply gaps. Extensive
experiments on three real-world datasets demonstrate the superiority of the
proposed framework compared to seven baselines and the effectiveness of the
three modules.</div><div><a href='http://arxiv.org/abs/2401.17838v1'>2401.17838v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03134v1")'>TimeGraphs: Graph-based Temporal Reasoning</div>
<div id='2401.03134v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:26:49Z</div><div>Authors: Paridhi Maheshwari, Hongyu Ren, Yanan Wang, Rok Sosic, Jure Leskovec</div><div style='padding-top: 10px; width: 80ex'>Many real-world systems exhibit temporal, dynamic behaviors, which are
captured as time series of complex agent interactions. To perform temporal
reasoning, current methods primarily encode temporal dynamics through simple
sequence-based models. However, in general these models fail to efficiently
capture the full spectrum of rich dynamics in the input, since the dynamics is
not uniformly distributed. In particular, relevant information might be harder
to extract and computing power is wasted for processing all individual
timesteps, even if they contain no significant changes or no new information.
Here we propose TimeGraphs, a novel approach that characterizes dynamic
interactions as a hierarchical temporal graph, diverging from traditional
sequential representations. Our approach models the interactions using a
compact graph-based representation, enabling adaptive reasoning across diverse
time scales. Adopting a self-supervised method, TimeGraphs constructs a
multi-level event hierarchy from a temporal input, which is then used to
efficiently reason about the unevenly distributed dynamics. This construction
process is scalable and incremental to accommodate streaming data. We evaluate
TimeGraphs on multiple datasets with complex, dynamic agent interactions,
including a football simulator, the Resistance game, and the MOMA human
activity dataset. The results demonstrate both robustness and efficiency of
TimeGraphs on a range of temporal reasoning tasks. Our approach obtains
state-of-the-art performance and leads to a performance increase of up to 12.2%
on event prediction and recognition tasks over current approaches. Our
experiments further demonstrate a wide array of capabilities including
zero-shot generalization, robustness in case of data sparsity, and adaptability
to streaming data flow.</div><div><a href='http://arxiv.org/abs/2401.03134v1'>2401.03134v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03116v1")'>Feature-Action Design Patterns for Storytelling Visualizations with Time
  Series Data</div>
<div id='2402.03116v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:45:59Z</div><div>Authors: Saiful Khan, Scott Jones, Benjamin Bach, Jaehoon Cha, Min Chen, Julie Meikle, Jonathan C Roberts, Jeyan Thiyagalingam, Jo Wood, Panagiotis D. Ritsos</div><div style='padding-top: 10px; width: 80ex'>We present a method to create storytelling visualization with time series
data. Many personal decisions nowadays rely on access to dynamic data
regularly, as we have seen during the COVID-19 pandemic. It is thus desirable
to construct storytelling visualization for dynamic data that is selected by an
individual for a specific context. Because of the need to tell data-dependent
stories, predefined storyboards based on known data cannot accommodate dynamic
data easily nor scale up to many different individuals and contexts. Motivated
initially by the need to communicate time series data during the COVID-19
pandemic, we developed a novel computer-assisted method for meta-authoring of
stories, which enables the design of storyboards that include feature-action
patterns in anticipation of potential features that may appear in dynamically
arrived or selected data. In addition to meta-storyboards involving COVID-19
data, we also present storyboards for telling stories about progress in a
machine learning workflow. Our approach is complementary to traditional methods
for authoring storytelling visualization, and provides an efficient means to
construct data-dependent storyboards for different data-streams of similar
contexts.</div><div><a href='http://arxiv.org/abs/2402.03116v1'>2402.03116v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09239v1")'>Robust Training of Temporal GNNs using Nearest Neighbours based Hard
  Negatives</div>
<div id='2402.09239v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:27:53Z</div><div>Authors: Shubham Gupta, Srikanta Bedathur</div><div style='padding-top: 10px; width: 80ex'>Temporal graph neural networks Tgnn have exhibited state-of-art performance
in future-link prediction tasks. Training of these TGNNs is enumerated by
uniform random sampling based unsupervised loss. During training, in the
context of a positive example, the loss is computed over uninformative
negatives, which introduces redundancy and sub-optimal performance. In this
paper, we propose modified unsupervised learning of Tgnn, by replacing the
uniform negative sampling with importance-based negative sampling. We
theoretically motivate and define the dynamically computed distribution for a
sampling of negative examples. Finally, using empirical evaluations over three
real-world datasets, we show that Tgnn trained using loss based on proposed
negative sampling provides consistent superior performance.</div><div><a href='http://arxiv.org/abs/2402.09239v1'>2402.09239v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02355v1")'>Temporal Knowledge Graph Completion with Time-sensitive Relations in
  Hypercomplex Space</div>
<div id='2403.02355v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T16:50:48Z</div><div>Authors: Li Cai, Xin Mao, Zhihong Wang, Shangqing Zhao, Yuhao Zhou, Changxu Wu, Man Lan</div><div style='padding-top: 10px; width: 80ex'>Temporal knowledge graph completion (TKGC) aims to fill in missing facts
within a given temporal knowledge graph at a specific time. Existing methods,
operating in real or complex spaces, have demonstrated promising performance in
this task. This paper advances beyond conventional approaches by introducing
more expressive quaternion representations for TKGC within hypercomplex space.
Unlike existing quaternion-based methods, our study focuses on capturing
time-sensitive relations rather than time-aware entities. Specifically, we
model time-sensitive relations through time-aware rotation and periodic time
translation, effectively capturing complex temporal variability. Furthermore,
we theoretically demonstrate our method's capability to model symmetric,
asymmetric, inverse, compositional, and evolutionary relation patterns.
Comprehensive experiments on public datasets validate that our proposed
approach achieves state-of-the-art performance in the field of TKGC.</div><div><a href='http://arxiv.org/abs/2403.02355v1'>2403.02355v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02168v1")'>One Graph Model for Cross-domain Dynamic Link Prediction</div>
<div id='2402.02168v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T14:29:01Z</div><div>Authors: Xuanwen Huang, Wei Chow, Yang Wang, Ziwei Chai, Chunping Wang, Lei Chen, Yang Yang</div><div style='padding-top: 10px; width: 80ex'>This work proposes DyExpert, a dynamic graph model for cross-domain link
prediction. It can explicitly model historical evolving processes to learn the
evolution pattern of a specific downstream graph and subsequently make
pattern-specific link predictions. DyExpert adopts a decode-only transformer
and is capable of efficiently parallel training and inference by
\textit{conditioned link generation} that integrates both evolution modeling
and link prediction. DyExpert is trained by extensive dynamic graphs across
diverse domains, comprising 6M dynamic edges. Extensive experiments on eight
untrained graphs demonstrate that DyExpert achieves state-of-the-art
performance in cross-domain link prediction. Compared to the advanced baseline
under the same setting, DyExpert achieves an average of 11.40% improvement
Average Precision across eight graphs. More impressive, it surpasses the fully
supervised performance of 8 advanced baselines on 6 untrained graphs.</div><div><a href='http://arxiv.org/abs/2402.02168v1'>2402.02168v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03077v1")'>A Topology-aware Graph Coarsening Framework for Continual Graph Learning</div>
<div id='2401.03077v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T22:22:13Z</div><div>Authors: Xiaoxue Han, Zhuo Feng, Yue Ning</div><div style='padding-top: 10px; width: 80ex'>Continual learning on graphs tackles the problem of training a graph neural
network (GNN) where graph data arrive in a streaming fashion and the model
tends to forget knowledge from previous tasks when updating with new data.
Traditional continual learning strategies such as Experience Replay can be
adapted to streaming graphs, however, these methods often face challenges such
as inefficiency in preserving graph topology and incapability of capturing the
correlation between old and new tasks. To address these challenges, we propose
TA$\mathbb{CO}$, a (t)opology-(a)ware graph (co)arsening and (co)ntinual
learning framework that stores information from previous tasks as a reduced
graph. At each time period, this reduced graph expands by combining with a new
graph and aligning shared nodes, and then it undergoes a "zoom out" process by
reduction to maintain a stable size. We design a graph coarsening algorithm
based on node representation proximities to efficiently reduce a graph and
preserve topological information. We empirically demonstrate the learning
process on the reduced graph can approximate that of the original graph. Our
experiments validate the effectiveness of the proposed framework on three
real-world datasets using different backbone GNN models.</div><div><a href='http://arxiv.org/abs/2401.03077v1'>2401.03077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.18054v1")'>Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based
  Action Recognition</div>
<div id='2401.18054v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T18:20:42Z</div><div>Authors: Wei Wei, Tom De Schepper, Kevin Mets</div><div style='padding-top: 10px; width: 80ex'>Continual learning (CL) is the research field that aims to build machine
learning models that can accumulate knowledge continuously over different tasks
without retraining from scratch. Previous studies have shown that pre-training
graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)
after fine-tuning, a setting which is closely related to CL. Thus, we focus on
studying GNN in the continual graph learning (CGL) setting. We propose the
first continual graph learning benchmark for spatio-temporal graphs and use it
to benchmark well-known CGL methods in this novel setting. The benchmark is
based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action
recognition. Beyond benchmarking for standard performance metrics, we study the
class and task-order sensitivity of CGL methods, i.e., the impact of learning
order on each class/task's performance, and the architectural sensitivity of
CGL methods with backbone GNN at various widths and depths. We reveal that
task-order robust methods can still be class-order sensitive and observe
results that contradict previous empirical observations on architectural
sensitivity in CL.</div><div><a href='http://arxiv.org/abs/2401.18054v1'>2401.18054v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13200v2")'>Topology-aware Embedding Memory for Continual Learning on Expanding
  Networks</div>
<div id='2401.13200v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T03:03:17Z</div><div>Authors: Xikun Zhang, Dongjin Song, Yixin Chen, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Memory replay based techniques have shown great success for continual
learning with incrementally accumulated Euclidean data. Directly applying them
to continually expanding networks, however, leads to the potential memory
explosion problem due to the need to buffer representative nodes and their
associated topological neighborhood structures. To this end, we systematically
analyze the key challenges in the memory explosion problem, and present a
general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs)
with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed
framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$
to $\mathcal{O}(n)$, but also fully utilizes the topological information for
memory replay. Specifically, PDGNNs decouple trainable parameters from the
computation ego-subnetwork via $\textit{Topology-aware Embeddings}$ (TEs),
which compress ego-subnetworks into compact vectors (i.e., TEs) to reduce the
memory consumption. Based on this framework, we discover a unique
$\textit{pseudo-training effect}$ in continual learning on expanding networks
and this effect motivates us to develop a novel $\textit{coverage maximization
sampling}$ strategy that can enhance the performance with a tight memory
budget. Thorough empirical studies demonstrate that, by tackling the memory
explosion problem and incorporating topological information into memory replay,
PDGNNs with TEM significantly outperform state-of-the-art techniques,
especially in the challenging class-incremental setting.</div><div><a href='http://arxiv.org/abs/2401.13200v2'>2401.13200v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05944v1")'>Todyformer: Towards Holistic Dynamic Graph Transformers with
  Structure-Aware Tokenization</div>
<div id='2402.05944v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T23:05:30Z</div><div>Authors: Mahdi Biparva, Raika Karimi, Faezeh Faez, Yingxue Zhang</div><div style='padding-top: 10px; width: 80ex'>Temporal Graph Neural Networks have garnered substantial attention for their
capacity to model evolving structural and temporal patterns while exhibiting
impressive performance. However, it is known that these architectures are
encumbered by issues that constrain their performance, such as over-squashing
and over-smoothing. Meanwhile, Transformers have demonstrated exceptional
computational capacity to effectively address challenges related to long-range
dependencies. Consequently, we introduce Todyformer-a novel Transformer-based
neural network tailored for dynamic graphs. It unifies the local encoding
capacity of Message-Passing Neural Networks (MPNNs) with the global encoding of
Transformers through i) a novel patchifying paradigm for dynamic graphs to
improve over-squashing, ii) a structure-aware parametric tokenization strategy
leveraging MPNNs, iii) a Transformer with temporal positional-encoding to
capture long-range dependencies, and iv) an encoding architecture that
alternates between local and global contextualization, mitigating
over-smoothing in MPNNs. Experimental evaluations on public benchmark datasets
demonstrate that Todyformer consistently outperforms the state-of-the-art
methods for downstream tasks. Furthermore, we illustrate the underlying aspects
of the proposed model in effectively capturing extensive temporal dependencies
in dynamic graphs.</div><div><a href='http://arxiv.org/abs/2402.05944v1'>2402.05944v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15894v1")'>A Gated MLP Architecture for Learning Topological Dependencies in
  Spatio-Temporal Graphs</div>
<div id='2401.15894v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T05:26:17Z</div><div>Authors: Yun Young Choi, Minho Lee, Sun Woo Park, Seunghwan Lee, Joohwan Ko</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) and Transformer have been increasingly adopted
to learn the complex vector representations of spatio-temporal graphs,
capturing intricate spatio-temporal dependencies crucial for applications such
as traffic datasets. Although many existing methods utilize multi-head
attention mechanisms and message-passing neural networks (MPNNs) to capture
both spatial and temporal relations, these approaches encode temporal and
spatial relations independently, and reflect the graph's topological
characteristics in a limited manner. In this work, we introduce the Cycle to
Mixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivial
invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP).
The Cy2Mixer is composed of three blocks based on MLPs: A message-passing block
for encapsulating spatial information, a cycle message-passing block for
enriching topological information through cyclic subgraphs, and a temporal
block for capturing temporal properties. We bolster the effectiveness of
Cy2Mixer with mathematical evidence emphasizing that our cycle message-passing
block is capable of offering differentiated information to the deep learning
model compared to the message-passing block. Furthermore, empirical evaluations
substantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-art
performances across various traffic benchmark datasets.</div><div><a href='http://arxiv.org/abs/2401.15894v1'>2401.15894v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04135v1")'>Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New
  Framework For Traffic Flow Prediction</div>
<div id='2401.04135v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T05:28:36Z</div><div>Authors: Haiyang Liu, Chunjiang Zhu, Detian Zhang</div><div style='padding-top: 10px; width: 80ex'>Traffic flow prediction plays a crucial role in alleviating traffic
congestion and enhancing transport efficiency. While combining graph
convolution networks with recurrent neural networks for spatial-temporal
modeling is a common strategy in this realm, the restricted structure of
recurrent neural networks limits their ability to capture global information.
For spatial modeling, many prior studies learn a graph structure that is
assumed to be fixed and uniform at all time steps, which may not be true. This
paper introduces a novel traffic prediction framework, Global-Aware Enhanced
Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core
components: a spatial-temporal graph recurrent neural network and a global
awareness layer. Within this framework, three innovative prediction models are
formulated. A sequence-aware graph neural network is proposed and integrated
into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time
steps and capture local temporal relationships. To enhance the model's global
perception, three distinct global spatial-temporal transformer-like
architectures (GST^2) are devised for the global awareness layer. We conduct
extensive experiments on four real traffic datasets and the results demonstrate
the superiority of our framework and the three concrete models.</div><div><a href='http://arxiv.org/abs/2401.04135v1'>2401.04135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06040v3")'>Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for
  Traffic Forecasting</div>
<div id='2401.06040v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T16:55:48Z</div><div>Authors: Qipeng Qian, Tanwi Mallick</div><div style='padding-top: 10px; width: 80ex'>Traffic forecasting is the foundation for intelligent transportation systems.
Spatiotemporal graph neural networks have demonstrated state-of-the-art
performance in traffic forecasting. However, these methods do not explicitly
model some of the natural characteristics in traffic data, such as the
multiscale structure that encompasses spatial and temporal variations at
different levels of granularity or scale. To that end, we propose a
Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines
multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In
WavGCRN, the traffic data is decomposed into time-frequency components with
Discrete Wavelet Transformation (DWT), constructing a multi-stream input
structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as
encoders for each stream, extracting spatiotemporal features in different
scales; and finally the learnable Inversed DWT and GCRN are combined as the
decoder, fusing the information from all streams for traffic metrics
reconstruction and prediction. Furthermore, road-network-informed graphs and
data-driven graph learning are combined to accurately capture spatial
correlation. The proposed method can offer well-defined interpretability,
powerful learning capability, and competitive forecasting performance on
real-world traffic data sets.</div><div><a href='http://arxiv.org/abs/2401.06040v3'>2401.06040v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01091v1")'>COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for
  Traffic Forecasting</div>
<div id='2403.01091v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T04:30:09Z</div><div>Authors: Wei Ju, Yusheng Zhao, Yifang Qin, Siyu Yi, Jingyang Yuan, Zhiping Xiao, Xiao Luo, Xiting Yan, Ming Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper investigates traffic forecasting, which attempts to forecast the
future state of traffic based on historical situations. This problem has
received ever-increasing attention in various scenarios and facilitated the
development of numerous downstream applications such as urban planning and
transportation management. However, the efficacy of existing methods remains
sub-optimal due to their tendency to model temporal and spatial relationships
independently, thereby inadequately accounting for complex high-order
interactions of both worlds. Moreover, the diversity of transitional patterns
in traffic forecasting makes them challenging to capture for existing
approaches, warranting a deeper exploration of their diversity. Toward this
end, this paper proposes Conjoint Spatio-Temporal graph neural network
(abbreviated as COOL), which models heterogeneous graphs from prior and
posterior information to conjointly capture high-order spatio-temporal
relationships. On the one hand, heterogeneous graphs connecting sequential
observation are constructed to extract composite spatio-temporal relationships
via prior message passing. On the other hand, we model dynamic relationships
using constructed affinity and penalty graphs, which guide posterior message
passing to incorporate complementary semantic information into node
representations. Moreover, to capture diverse transitional properties to
enhance traffic forecasting, we propose a conjoint self-attention decoder that
models diverse temporal patterns from both multi-rank and multi-scale views.
Experimental results on four popular benchmark datasets demonstrate that our
proposed COOL provides state-of-the-art performance compared with the
competitive baselines.</div><div><a href='http://arxiv.org/abs/2403.01091v1'>2403.01091v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02600v1")'>TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of
  Experts</div>
<div id='2403.02600v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T02:27:52Z</div><div>Authors: Hyunwook Lee, Sungahn Ko</div><div style='padding-top: 10px; width: 80ex'>Accurate traffic forecasting is challenging due to the complex dependency on
road networks, various types of roads, and the abrupt speed change due to the
events. Recent works mainly focus on dynamic spatial modeling with adaptive
graph embedding or graph attention having less consideration for temporal
characteristics and in-situ modeling. In this paper, we propose a novel deep
learning model named TESTAM, which individually models recurring and
non-recurring traffic patterns by a mixture-of-experts model with three experts
on temporal modeling, spatio-temporal modeling with static graph, and dynamic
spatio-temporal dependency modeling with dynamic graph. By introducing
different experts and properly routing them, TESTAM could better model various
circumstances, including spatially isolated nodes, highly related nodes, and
recurring and non-recurring events. For the proper routing, we reformulate a
gating problem into a classification problem with pseudo labels. Experimental
results on three public traffic network datasets, METR-LA, PEMS-BAY, and
EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of
recurring and non-recurring traffic. We published the official code at
https://github.com/HyunWookL/TESTAM</div><div><a href='http://arxiv.org/abs/2403.02600v1'>2403.02600v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16453v1")'>Hybrid Transformer and Spatial-Temporal Self-Supervised Learning for
  Long-term Traffic Prediction</div>
<div id='2401.16453v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T06:17:23Z</div><div>Authors: Wang Zhu, Doudou Zhang, Baichao Long, Jianli Xiao</div><div style='padding-top: 10px; width: 80ex'>Long-term traffic prediction has always been a challenging task due to its
dynamic temporal dependencies and complex spatial dependencies. In this paper,
we propose a model that combines hybrid Transformer and spatio-temporal
self-supervised learning. The model enhances its robustness by applying
adaptive data augmentation techniques at the sequence-level and graph-level of
the traffic data. It utilizes Transformer to overcome the limitations of
recurrent neural networks in capturing long-term sequences, and employs
Chebyshev polynomial graph convolution to capture complex spatial dependencies.
Furthermore, considering the impact of spatio-temporal heterogeneity on traffic
speed, we design two self-supervised learning tasks to model the temporal and
spatial heterogeneity, thereby improving the accuracy and generalization
ability of the model. Experimental evaluations are conducted on two real-world
datasets, PeMS04 and PeMS08, and the results are visualized and analyzed,
demonstrating the superior performance of the proposed model.</div><div><a href='http://arxiv.org/abs/2401.16453v1'>2401.16453v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13794v1")'>Traffic Pattern Classification in Smart Cities Using Deep Recurrent
  Neural Network</div>
<div id='2401.13794v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T20:24:32Z</div><div>Authors: Ayad Ghany Ismaeel, Krishnadas Janardhanan, Manishankar Sankar, Yuvaraj Natarajan, Sarmad Nozad Mahmood, Sameer Alani, Akram H. Shather</div><div style='padding-top: 10px; width: 80ex'>This paper examines the use of deep recurrent neural networks to classify
traffic patterns in smart cities. We propose a novel approach to traffic
pattern classification based on deep recurrent neural networks, which can
effectively capture traffic patterns' dynamic and sequential features. The
proposed model combines convolutional and recurrent layers to extract features
from traffic pattern data and a SoftMax layer to classify traffic patterns.
Experimental results show that the proposed model outperforms existing methods
regarding accuracy, precision, recall, and F1 score. Furthermore, we provide an
in depth analysis of the results and discuss the implications of the proposed
model for smart cities. The results show that the proposed model can accurately
classify traffic patterns in smart cities with a precision of as high as 95%.
The proposed model is evaluated on a real world traffic pattern dataset and
compared with existing classification methods.</div><div><a href='http://arxiv.org/abs/2401.13794v1'>2401.13794v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00920v1")'>Deep Learning Approaches for Network Traffic Classification in the
  Internet of Things (IoT): A Survey</div>
<div id='2402.00920v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T14:33:24Z</div><div>Authors: Jawad Hussain Kalwar, Sania Bhatti</div><div style='padding-top: 10px; width: 80ex'>The Internet of Things (IoT) has witnessed unprecedented growth, resulting in
a massive influx of diverse network traffic from interconnected devices.
Effectively classifying this network traffic is crucial for optimizing resource
allocation, enhancing security measures, and ensuring efficient network
management in IoT systems. Deep learning has emerged as a powerful technique
for network traffic classification due to its ability to automatically learn
complex patterns and representations from raw data. This survey paper aims to
provide a comprehensive overview of the existing deep learning approaches
employed in network traffic classification specifically tailored for IoT
environments. By systematically analyzing and categorizing the latest research
contributions in this domain, we explore the strengths and limitations of
various deep learning models in handling the unique challenges posed by IoT
network traffic. Through this survey, we aim to offer researchers and
practitioners valuable insights, identify research gaps, and provide directions
for future research to further enhance the effectiveness and efficiency of deep
learning-based network traffic classification in IoT.</div><div><a href='http://arxiv.org/abs/2402.00920v1'>2402.00920v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10968v1")'>Enhancing IoT Security Against DDoS Attacks through Federated Learning</div>
<div id='2403.10968v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T16:45:28Z</div><div>Authors: Ghazaleh Shirvani, Saeid Ghasemshirazi, Mohammad Ali Alipour</div><div style='padding-top: 10px; width: 80ex'>The rapid proliferation of the Internet of Things (IoT) has ushered in
transformative connectivity between physical devices and the digital realm.
Nonetheless, the escalating threat of Distributed Denial of Service (DDoS)
attacks jeopardizes the integrity and reliability of IoT networks. Conventional
DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT
ecosystems, potentially compromising data privacy. This paper introduces an
innovative strategy to bolster the security of IoT networks against DDoS
attacks by harnessing the power of Federated Learning that allows multiple IoT
devices or edge nodes to collaboratively build a global model while preserving
data privacy and minimizing communication overhead. The research aims to
investigate Federated Learning's effectiveness in detecting and mitigating DDoS
attacks in IoT. Our proposed framework leverages IoT devices' collective
intelligence for real-time attack detection without compromising sensitive
data. This study proposes innovative deep autoencoder approaches for data
dimensionality reduction, retraining, and partial selection to enhance the
performance and stability of the proposed model. Additionally, two renowned
aggregation algorithms, FedAvg and FedAvgM, are employed in this research.
Various metrics, including true positive rate, false positive rate, and
F1-score, are employed to evaluate the model. The dataset utilized in this
research, N-BaIoT, exhibits non-IID data distribution, where data categories
are distributed quite differently. The negative impact of these distribution
disparities is managed by employing retraining and partial selection
techniques, enhancing the final model's stability. Furthermore, evaluation
results demonstrate that the FedAvgM aggregation algorithm outperforms FedAvg,
indicating that in non-IID datasets, FedAvgM provides better stability and
performance.</div><div><a href='http://arxiv.org/abs/2403.10968v1'>2403.10968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03270v1")'>Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT
  Protocol</div>
<div id='2402.03270v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:27:46Z</div><div>Authors: Hector Alaiz-Moreton, Jose Aveleira-Mata, Jorge Ondicol-Garcia, Angel Luis Muñoz-Castañeda, Isaías García, Carmen Benavides</div><div style='padding-top: 10px; width: 80ex'>The large number of sensors and actuators that make up the Internet of Things
obliges these systems to use diverse technologies and protocols. This means
that IoT networks are more heterogeneous than traditional networks. This gives
rise to new challenges in cybersecurity to protect these systems and devices
which are characterized by being connected continuously to the Internet.
Intrusion detection systems (IDS) are used to protect IoT systems from the
various anomalies and attacks at the network level. Intrusion Detection Systems
(IDS) can be improved through machine learning techniques. Our work focuses on
creating classification models that can feed an IDS using a dataset containing
frames under attacks of an IoT system that uses the MQTT protocol. We have
addressed two types of method for classifying the attacks, ensemble methods and
deep learning models, more specifically recurrent networks with very
satisfactory results.</div><div><a href='http://arxiv.org/abs/2402.03270v1'>2402.03270v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17546v1")'>Effective Multi-Stage Training Model For Edge Computing Devices In
  Intrusion Detection</div>
<div id='2401.17546v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T02:20:21Z</div><div>Authors: Thua Huynh Trong, Thanh Nguyen Hoang</div><div style='padding-top: 10px; width: 80ex'>Intrusion detection poses a significant challenge within expansive and
persistently interconnected environments. As malicious code continues to
advance and sophisticated attack methodologies proliferate, various advanced
deep learning-based detection approaches have been proposed. Nevertheless, the
complexity and accuracy of intrusion detection models still need further
enhancement to render them more adaptable to diverse system categories,
particularly within resource-constrained devices, such as those embedded in
edge computing systems. This research introduces a three-stage training
paradigm, augmented by an enhanced pruning methodology and model compression
techniques. The objective is to elevate the system's effectiveness,
concurrently maintaining a high level of accuracy for intrusion detection.
Empirical assessments conducted on the UNSW-NB15 dataset evince that this
solution notably reduces the model's dimensions, while upholding accuracy
levels equivalent to similar proposals.</div><div><a href='http://arxiv.org/abs/2401.17546v1'>2401.17546v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09432v1")'>An Enhanced Analysis of Traffic Intelligence in Smart Cities Using
  Sustainable Deep Radial Function</div>
<div id='2402.09432v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T22:28:14Z</div><div>Authors: Ayad Ghany Ismaeel, S. J. Jereesha Mary, C. Anitha, Jaganathan Logeshwaran, Sarmad Nozad Mahmood, Sameer Alani, Akram H. Shather</div><div style='padding-top: 10px; width: 80ex'>Smart cities have revolutionized urban living by incorporating sophisticated
technologies to optimize various aspects of urban infrastructure, such as
transportation systems. Effective traffic management is a crucial component of
smart cities, as it has a direct impact on the quality of life of residents and
tourists. Utilizing deep radial basis function (RBF) networks, this paper
describes a novel strategy for enhancing traffic intelligence in smart cities.
Traditional methods of traffic analysis frequently rely on simplistic models
that are incapable of capturing the intricate patterns and dynamics of urban
traffic systems. Deep learning techniques, such as deep RBF networks, have the
potential to extract valuable insights from traffic data and enable more
precise predictions and decisions. In this paper, we propose an RBF based
method for enhancing smart city traffic intelligence. Deep RBF networks combine
the adaptability and generalization capabilities of deep learning with the
discriminative capability of radial basis functions. The proposed method can
effectively learn intricate relationships and nonlinear patterns in traffic
data by leveraging the hierarchical structure of deep neural networks. The deep
RBF model can learn to predict traffic conditions, identify congestion
patterns, and make informed recommendations for optimizing traffic management
strategies by incorporating these rich and diverse data To evaluate the
efficacy of our proposed method, extensive experiments and comparisons with
real world traffic datasets from a smart city environment were conducted. In
terms of prediction accuracy and efficiency, the results demonstrate that the
deep RBF based approach outperforms conventional traffic analysis methods.
Smart city traffic intelligence is enhanced by the model capacity to capture
nonlinear relationships and manage large scale data sets.</div><div><a href='http://arxiv.org/abs/2402.09432v1'>2402.09432v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03397v2")'>Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic
  Forecasting</div>
<div id='2401.03397v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T06:51:26Z</div><div>Authors: Sina Ehsani, Elina Sergeeva, Wendy Murdy, Benjamin Fox</div><div style='padding-top: 10px; width: 80ex'>Accurate prediction of flight-level passenger traffic is of paramount
importance in airline operations, influencing key decisions from pricing to
route optimization. This study introduces a novel, multimodal deep learning
approach to the challenge of predicting flight-level passenger traffic,
yielding substantial accuracy improvements compared to traditional models.
Leveraging an extensive dataset from American Airlines, our model ingests
historical traffic data, fare closure information, and seasonality attributes
specific to each flight. Our proposed neural network integrates the strengths
of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN),
exploiting the temporal patterns and spatial relationships within the data to
enhance prediction performance. Crucial to the success of our model is a
comprehensive data processing strategy. We construct 3D tensors to represent
data, apply careful masking strategies to mirror real-world dynamics, and
employ data augmentation techniques to enrich the diversity of our training
set. The efficacy of our approach is borne out in the results: our model
demonstrates an approximate 33\% improvement in Mean Squared Error (MSE)
compared to traditional benchmarks. This study, therefore, highlights the
significant potential of deep learning techniques and meticulous data
processing in advancing the field of flight traffic prediction.</div><div><a href='http://arxiv.org/abs/2401.03397v2'>2401.03397v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09969v1")'>Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data
  Fusion and Deep Learning</div>
<div id='2403.09969v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T02:25:04Z</div><div>Authors: Xiaocai Zhang, Xiuju Fu, Zhe Xiao, Haiyan Xu, Xiaoyang Wei, Jimmy Koh, Daichi Ogawa, Zheng Qin</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the prediction of vessels' arrival time to the
pilotage area using multi-data fusion and deep learning approaches. Firstly,
the vessel arrival contour is extracted based on Multivariate Kernel Density
Estimation (MKDE) and clustering. Secondly, multiple data sources, including
Automatic Identification System (AIS), pilotage booking information, and
meteorological data, are fused before latent feature extraction. Thirdly, a
Temporal Convolutional Network (TCN) framework that incorporates a residual
mechanism is constructed to learn the hidden arrival patterns of the vessels.
Extensive tests on two real-world data sets from Singapore have been conducted
and the following promising results have been obtained: 1) fusion of pilotage
booking information and meteorological data improves the prediction accuracy,
with pilotage booking information having a more significant impact; 2) using
discrete embedding for the meteorological data performs better than using
continuous embedding; 3) the TCN outperforms the state-of-the-art baseline
methods in regression tasks, exhibiting Mean Absolute Error (MAE) ranging from
4.58 min to 4.86 min; and 4) approximately 89.41% to 90.61% of the absolute
prediction residuals fall within a time frame of 10 min.</div><div><a href='http://arxiv.org/abs/2403.09969v1'>2403.09969v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13651v2")'>Robustness of Deep Neural Networks for Micro-Doppler Radar
  Classification</div>
<div id='2402.13651v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:37:17Z</div><div>Authors: Mikolaj Czerkawski, Carmine Clemente, Craig Michie, Christos Tachtatzis</div><div style='padding-top: 10px; width: 80ex'>With the great capabilities of deep classifiers for radar data processing
come the risks of learning dataset-specific features that do not generalize
well. In this work, the robustness of two deep convolutional architectures,
trained and tested on the same data, is evaluated. When standard training
practice is followed, both classifiers exhibit sensitivity to subtle temporal
shifts of the input representation, an augmentation that carries minimal
semantic content. Furthermore, the models are extremely susceptible to
adversarial examples. Both small temporal shifts and adversarial examples are a
result of a model overfitting on features that do not generalize well. As a
remedy, it is shown that training on adversarial examples and temporally
augmented samples can reduce this effect and lead to models that generalise
better. Finally, models operating on cadence-velocity diagram representation
rather than Doppler-time are demonstrated to be naturally more immune to
adversarial examples.</div><div><a href='http://arxiv.org/abs/2402.13651v2'>2402.13651v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11176v2")'>Data-Driven Target Localization: Benchmarking Gradient Descent Using the
  Cramér-Rao Bound</div>
<div id='2401.11176v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T09:26:08Z</div><div>Authors: Shyam Venkatasubramanian, Sandeep Gogineni, Bosung Kang, Muralidhar Rangaswamy</div><div style='padding-top: 10px; width: 80ex'>In modern radar systems, precise target localization using azimuth and
velocity estimation is paramount. Traditional unbiased estimation methods have
utilized gradient descent algorithms to reach the theoretical limits of the
Cramer Rao Bound (CRB) for the error of the parameter estimates. As an
extension, we demonstrate on a realistic simulated example scenario that our
earlier presented data-driven neural network model outperforms these
traditional methods, yielding improved accuracies in target azimuth and
velocity estimation. We emphasize, however, that this improvement does not
imply that the neural network outperforms the CRB itself. Rather, the enhanced
performance is attributed to the biased nature of the neural network approach.
Our findings underscore the potential of employing deep learning methods in
radar systems to achieve more accurate localization in cluttered and dynamic
environments.</div><div><a href='http://arxiv.org/abs/2401.11176v2'>2401.11176v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01083v1")'>Aircraft Landing Time Prediction with Deep Learning on Trajectory Images</div>
<div id='2401.01083v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T07:56:05Z</div><div>Authors: Liping Huang, Sheng Zhang, Yicheng Zhang, Yi Zhang, Yifang Yin</div><div style='padding-top: 10px; width: 80ex'>Aircraft landing time (ALT) prediction is crucial for air traffic management,
especially for arrival aircraft sequencing on the runway. In this study, a
trajectory image-based deep learning method is proposed to predict ALTs for the
aircraft entering the research airspace that covers the Terminal Maneuvering
Area (TMA). Specifically, the trajectories of all airborne arrival aircraft
within the temporal capture window are used to generate an image with the
target aircraft trajectory labeled as red and all background aircraft
trajectory labeled as blue. The trajectory images contain various information,
including the aircraft position, speed, heading, relative distances, and
arrival traffic flows. It enables us to use state-of-the-art deep convolution
neural networks for ALT modeling. We also use real-time runway usage obtained
from the trajectory data and the external information such as aircraft types
and weather conditions as additional inputs. Moreover, a convolution neural
network (CNN) based module is designed for automatic holding-related
featurizing, which takes the trajectory images, the leading aircraft holding
status, and their time and speed gap at the research airspace boundary as its
inputs. Its output is further fed into the final end-to-end ALT prediction. The
proposed ALT prediction approach is applied to Singapore Changi Airport (ICAO
Code: WSSS) using one-month Automatic Dependent Surveillance-Broadcast (ADS-B)
data from November 1 to November 30, 2022. Experimental results show that by
integrating the holding featurization, we can reduce the mean absolute error
(MAE) from 82.23 seconds to 43.96 seconds, and achieve an average accuracy of
96.1\%, with 79.4\% of the predictions errors being less than 60 seconds.</div><div><a href='http://arxiv.org/abs/2401.01083v1'>2401.01083v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04872v1")'>Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction</div>
<div id='2401.04872v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T01:50:29Z</div><div>Authors: Yu Liu, Yuexin Zhang, Kunming Li, Yongliang Qiao, Stewart Worrall, You-Fu Li, He Kong</div><div style='padding-top: 10px; width: 80ex'>Predicting pedestrian motion trajectories is crucial for path planning and
motion control of autonomous vehicles. Accurately forecasting crowd
trajectories is challenging due to the uncertain nature of human motions in
different environments. For training, recent deep learning-based prediction
approaches mainly utilize information like trajectory history and interactions
between pedestrians, among others. This can limit the prediction performance
across various scenarios since the discrepancies between training datasets have
not been properly incorporated. To overcome this limitation, this paper
proposes a graph transformer structure to improve prediction performance,
capturing the differences between the various sites and scenarios contained in
the datasets. In particular, a self-attention mechanism and a domain adaption
module have been designed to improve the generalization ability of the model.
Moreover, an additional metric considering cross-dataset sequences is
introduced for training and performance evaluation purposes. The proposed
framework is validated and compared against existing methods using popular
public datasets, i.e., ETH and UCY. Experimental results demonstrate the
improved performance of our proposed scheme.</div><div><a href='http://arxiv.org/abs/2401.04872v1'>2401.04872v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11922v2")'>A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer
  Learning</div>
<div id='2402.11922v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:11:26Z</div><div>Authors: Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li</div><div style='padding-top: 10px; width: 80ex'>Spatio-temporal graph (STG) learning is foundational for smart city
applications, yet it is often hindered by data scarcity in many cities and
regions. To bridge this gap, we propose a novel generative pre-training
framework, GPDiff, for STG transfer learning. Unlike conventional approaches
that heavily rely on common feature extraction or intricate transfer learning
designs, our solution takes a novel approach by performing generative
pre-training on a collection of model parameters optimized with data from
source cities. We recast STG transfer learning as pre-training a generative
hypernetwork, which generates tailored model parameters guided by prompts,
allowing for adaptability to diverse data distributions and city-specific
characteristics. GPDiff employs a diffusion model with a transformer-based
denoising network, which is model-agnostic to integrate with powerful STG
models. By addressing challenges arising from data gaps and the complexity of
generalizing knowledge across cities, our framework consistently outperforms
state-of-the-art baselines on multiple real-world datasets for tasks such as
traffic speed prediction and crowd flow prediction. The implementation of our
approach is available: https://github.com/PLUTO-SCY/GPDiff.</div><div><a href='http://arxiv.org/abs/2402.11922v2'>2402.11922v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00397v2")'>Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic
  Forecasting</div>
<div id='2402.00397v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T07:33:31Z</div><div>Authors: Zhanyu Liu, Guanjie Zheng, Yanwei Yu</div><div style='padding-top: 10px; width: 80ex'>Traffic forecasting is crucial for intelligent transportation systems (ITS),
aiding in efficient resource allocation and effective traffic control. However,
its effectiveness often relies heavily on abundant traffic data, while many
cities lack sufficient data due to limited device support, posing a significant
challenge for traffic forecasting. Recognizing this challenge, we have made a
noteworthy observation: traffic patterns exhibit similarities across diverse
cities. Building on this key insight, we propose a solution for the cross-city
few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank
(MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich
source cities, effectively acquiring comprehensive traffic knowledge through a
spatial-temporal-aware pre-training process. Subsequently, the framework
employs advanced clustering techniques to systematically generate a multi-scale
traffic pattern bank derived from the learned knowledge. Next, the traffic data
of the data-scarce target city could query the traffic pattern bank,
facilitating the aggregation of meta-knowledge. This meta-knowledge, in turn,
assumes a pivotal role as a robust guide in subsequent processes involving
graph reconstruction and forecasting. Empirical assessments conducted on
real-world traffic datasets affirm the superior performance of MTPB, surpassing
existing methods across various categories and exhibiting numerous attributes
conducive to the advancement of cross-city few-shot forecasting methodologies.
The code is available in https://github.com/zhyliu00/MTPB.</div><div><a href='http://arxiv.org/abs/2402.00397v2'>2402.00397v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10518v1")'>Spatial-temporal Forecasting for Regions without Observations</div>
<div id='2401.10518v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T06:26:05Z</div><div>Authors: Xinyu Su, Jianzhong Qi, Egemen Tanin, Yanchuan Chang, Majid Sarvi</div><div style='padding-top: 10px; width: 80ex'>Spatial-temporal forecasting plays an important role in many real-world
applications, such as traffic forecasting, air pollutant forecasting,
crowd-flow forecasting, and so on. State-of-the-art spatial-temporal
forecasting models take data-driven approaches and rely heavily on data
availability. Such models suffer from accuracy issues when data is incomplete,
which is common in reality due to the heavy costs of deploying and maintaining
sensors for data collection. A few recent studies attempted to address the
issue of incomplete data. They typically assume some data availability in a
region of interest either for a short period or at a few locations. In this
paper, we further study spatial-temporal forecasting for a region of interest
without any historical observations, to address scenarios such as unbalanced
region development, progressive deployment of sensors or lack of open data. We
propose a model named STSM for the task. The model takes a contrastive
learning-based approach to learn spatial-temporal patterns from adjacent
regions that have recorded data. Our key insight is to learn from the locations
that resemble those in the region of interest, and we propose a selective
masking strategy to enable the learning. As a result, our model outperforms
adapted state-of-the-art models, reducing errors consistently over both traffic
and air pollutant forecasting tasks. The source code is available at
https://github.com/suzy0223/STSM.</div><div><a href='http://arxiv.org/abs/2401.10518v1'>2401.10518v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08735v1")'>A Framework for Scalable Ambient Air Pollution Concentration Estimation</div>
<div id='2401.08735v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:03:07Z</div><div>Authors: Liam J Berrisford, Lucy S Neal, Helen J Buttery, Benjamin R Evans, Ronaldo Menezes</div><div style='padding-top: 10px; width: 80ex'>Ambient air pollution remains a critical issue in the United Kingdom, where
data on air pollution concentrations form the foundation for interventions
aimed at improving air quality. However, the current air pollution monitoring
station network in the UK is characterized by spatial sparsity, heterogeneous
placement, and frequent temporal data gaps, often due to issues such as power
outages. We introduce a scalable data-driven supervised machine learning model
framework designed to address temporal and spatial data gaps by filling missing
measurements. This approach provides a comprehensive dataset for England
throughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning
techniques and real-world data from the sparsely distributed monitoring
stations, we generate 355,827 synthetic monitoring stations across the study
area, yielding data valued at approximately \pounds70 billion. Validation was
conducted to assess the model's performance in forecasting, estimating missing
locations, and capturing peak concentrations. The resulting dataset is of
particular interest to a diverse range of stakeholders engaged in downstream
assessments supported by outdoor air pollution concentration data for NO2, O3,
PM10, PM2.5, and SO2. This resource empowers stakeholders to conduct studies at
a higher resolution than was previously possible.</div><div><a href='http://arxiv.org/abs/2401.08735v1'>2401.08735v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10248v1")'>A Data-Driven Supervised Machine Learning Approach to Estimating Global
  Ambient Air Pollution Concentrations With Associated Prediction Intervals</div>
<div id='2402.10248v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T11:09:22Z</div><div>Authors: Liam J Berrisford, Hugo Barbosa, Ronaldo Menezes</div><div style='padding-top: 10px; width: 80ex'>Global ambient air pollution, a transboundary challenge, is typically
addressed through interventions relying on data from spatially sparse and
heterogeneously placed monitoring stations. These stations often encounter
temporal data gaps due to issues such as power outages. In response, we have
developed a scalable, data-driven, supervised machine learning framework. This
model is designed to impute missing temporal and spatial measurements, thereby
generating a comprehensive dataset for pollutants including NO$_2$, O$_3$,
PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of
0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for
each estimate, caters to a wide range of stakeholders relying on outdoor air
pollution data for downstream assessments. This enables more detailed studies.
Additionally, the model's performance across various geographical locations is
examined, providing insights and recommendations for strategic placement of
future monitoring stations to further enhance the model's accuracy.</div><div><a href='http://arxiv.org/abs/2402.10248v1'>2402.10248v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01788v1")'>Applications of machine learning and IoT for Outdoor Air Pollution
  Monitoring and Prediction: A Systematic Literature Review</div>
<div id='2401.01788v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T15:36:33Z</div><div>Authors: Ihsane Gryech, Chaimae Assad, Mounir Ghogho, Abdellatif Kobbane</div><div style='padding-top: 10px; width: 80ex'>According to the World Health Organization (WHO), air pollution kills seven
million people every year. Outdoor air pollution is a major environmental
health problem affecting low, middle, and high-income countries. In the past
few years, the research community has explored IoT-enabled machine learning
applications for outdoor air pollution prediction. The general objective of
this paper is to systematically review applications of machine learning and
Internet of Things (IoT) for outdoor air pollution prediction and the
combination of monitoring sensors and input features used. Two research
questions were formulated for this review. 1086 publications were collected in
the initial PRISMA stage. After the screening and eligibility phases, 37 papers
were selected for inclusion. A cost-based analysis was conducted on the
findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled
prediction. Three methods of prediction were identified: time series,
feature-based and spatio-temporal. This review's findings identify major
limitations in applications found in the literature, namely lack of coverage,
lack of diversity of data and lack of inclusion of context-specific features.
This review proposes directions for future research and underlines practical
implications in healthcare, urban planning, global synergy and smart cities.</div><div><a href='http://arxiv.org/abs/2401.01788v1'>2401.01788v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07657v1")'>Scalable Spatiotemporal Prediction with Bayesian Neural Fields</div>
<div id='2403.07657v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T13:47:50Z</div><div>Authors: Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs Köster, Rif A. Saurous, Matthew Hoffman</div><div style='padding-top: 10px; width: 80ex'>Spatiotemporal datasets, which consist of spatially-referenced time series,
are ubiquitous in many scientific and business-intelligence applications, such
as air pollution monitoring, disease tracking, and cloud-demand forecasting. As
modern datasets continue to increase in size and complexity, there is a growing
need for new statistical methods that are flexible enough to capture complex
spatiotemporal dynamics and scalable enough to handle large prediction
problems. This work presents the Bayesian Neural Field (BayesNF), a
domain-general statistical model for inferring rich probability distributions
over a spatiotemporal domain, which can be used for data-analysis tasks
including forecasting, interpolation, and variography. BayesNF integrates a
novel deep neural network architecture for high-capacity function estimation
with hierarchical Bayesian inference for robust uncertainty quantification. By
defining the prior through a sequence of smooth differentiable transforms,
posterior inference is conducted on large-scale data using variationally
learned surrogates trained via stochastic gradient descent. We evaluate BayesNF
against prominent statistical and machine-learning baselines, showing
considerable improvements on diverse prediction problems from climate and
public health datasets that contain tens to hundreds of thousands of
measurements. The paper is accompanied with an open-source software package
(https://github.com/google/bayesnf) that is easy-to-use and compatible with
modern GPU and TPU accelerators on the JAX machine learning platform.</div><div><a href='http://arxiv.org/abs/2403.07657v1'>2403.07657v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15359v1")'>Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of
  High Dimensional Categorical Observations</div>
<div id='2402.15359v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:52:05Z</div><div>Authors: J. E. San Soucie, H. M. Sosik, Y. Girdhar</div><div style='padding-top: 10px; width: 80ex'>We present the Streaming Gaussian Dirichlet Random Field (S-GDRF) model, a
novel approach for modeling a stream of spatiotemporally distributed, sparse,
high-dimensional categorical observations. The proposed approach efficiently
learns global and local patterns in spatiotemporal data, allowing for fast
inference and querying with a bounded time complexity. Using a high-resolution
data series of plankton images classified with a neural network, we demonstrate
the ability of the approach to make more accurate predictions compared to a
Variational Gaussian Process (VGP), and to learn a predictive distribution of
observations from streaming categorical data. S-GDRFs open the door to enabling
efficient informative path planning over high-dimensional categorical
observations, which until now has not been feasible.</div><div><a href='http://arxiv.org/abs/2402.15359v1'>2402.15359v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03449v1")'>SalienTime: User-driven Selection of Salient Time Steps for Large-Scale
  Geospatial Data Visualization</div>
<div id='2403.03449v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T04:27:10Z</div><div>Authors: Juntong Chen, Haiwen Huang, Huayuan Ye, Zhong Peng, Chenhui Li, Changbo Wang</div><div style='padding-top: 10px; width: 80ex'>The voluminous nature of geospatial temporal data from physical monitors and
simulation models poses challenges to efficient data access, often resulting in
cumbersome temporal selection experiences in web-based data portals. Thus,
selecting a subset of time steps for prioritized visualization and pre-loading
is highly desirable. Addressing this issue, this paper establishes a
multifaceted definition of salient time steps via extensive need-finding
studies with domain experts to understand their workflows. Building on this, we
propose a novel approach that leverages autoencoders and dynamic programming to
facilitate user-driven temporal selections. Structural features, statistical
variations, and distance penalties are incorporated to make more flexible
selections. User-specified priorities, spatial regions, and aggregations are
used to combine different perspectives. We design and implement a web-based
interface to enable efficient and context-aware selection of time steps and
evaluate its efficacy and usability through case studies, quantitative
evaluations, and expert interviews.</div><div><a href='http://arxiv.org/abs/2403.03449v1'>2403.03449v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00403v1")'>Fractal interpolation in the context of prediction accuracy optimization</div>
<div id='2403.00403v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T09:49:53Z</div><div>Authors: Alexandra Baicoianu, Cristina Gabriela Gavrilă, Cristina Maria Pacurar, Victor Dan Pacurar</div><div style='padding-top: 10px; width: 80ex'>This paper focuses on the hypothesis of optimizing time series predictions
using fractal interpolation techniques. In general, the accuracy of machine
learning model predictions is closely related to the quality and quantitative
aspects of the data used, following the principle of \textit{garbage-in,
garbage-out}. In order to quantitatively and qualitatively augment datasets,
one of the most prevalent concerns of data scientists is to generate synthetic
data, which should follow as closely as possible the actual pattern of the
original data.
  This study proposes three different data augmentation strategies based on
fractal interpolation, namely the \textit{Closest Hurst Strategy},
\textit{Closest Values Strategy} and \textit{Formula Strategy}. To validate the
strategies, we used four public datasets from the literature, as well as a
private dataset obtained from meteorological records in the city of Brasov,
Romania. The prediction results obtained with the LSTM model using the
presented interpolation strategies showed a significant accuracy improvement
compared to the raw datasets, thus providing a possible answer to practical
problems in the field of remote sensing and sensor sensitivity. Moreover, our
methodologies answer some optimization-related open questions for the fractal
interpolation step using \textit{Optuna} framework.</div><div><a href='http://arxiv.org/abs/2403.00403v1'>2403.00403v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10863v1")'>stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for
  Spatial Transcriptomics Data Imputation</div>
<div id='2403.10863v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T09:06:38Z</div><div>Authors: Xiaoyu Li, Wenwen Min, Shunfang Wang, Changmiao Wang, Taosheng Xu</div><div style='padding-top: 10px; width: 80ex'>Spatially resolved transcriptomics represents a significant advancement in
single-cell analysis by offering both gene expression data and their
corresponding physical locations. However, this high degree of spatial
resolution entails a drawback, as the resulting spatial transcriptomic data at
the cellular level is notably plagued by a high incidence of missing values.
Furthermore, most existing imputation methods either overlook the spatial
information between spots or compromise the overall gene expression data
distribution. To address these challenges, our primary focus is on effectively
utilizing the spatial location information within spatial transcriptomic data
to impute missing values, while preserving the overall data distribution. We
introduce \textbf{stMCDI}, a novel conditional diffusion model for spatial
transcriptomics data imputation, which employs a denoising network trained
using randomly masked data portions as guidance, with the unmasked data serving
as conditions. Additionally, it utilizes a GNN encoder to integrate the spatial
position information, thereby enhancing model performance. The results obtained
from spatial transcriptomics datasets elucidate the performance of our methods
relative to existing approaches.</div><div><a href='http://arxiv.org/abs/2403.10863v1'>2403.10863v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.11107v1")'>Dynamic nowcast of the New Zealand greenhouse gas inventory</div>
<div id='2402.11107v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T22:19:43Z</div><div>Authors: Malcolm Jones, Hannah Chorley, Flynn Owen, Tamsyn Hilder, Holly Trowland, Paul Bracewell</div><div style='padding-top: 10px; width: 80ex'>As efforts to mitigate the effects of climate change grow, reliable and
thorough reporting of greenhouse gas emissions are essential for measuring
progress towards international and domestic emissions reductions targets. New
Zealand's national emissions inventories are currently reported between 15 to
27 months out-of-date. We present a machine learning approach to nowcast
(dynamically estimate) national greenhouse gas emissions in New Zealand in
advance of the national emissions inventory's release, with just a two month
latency due to current data availability. Key findings include an estimated
0.2% decrease in national gross emissions since 2020 (as at July 2022). Our
study highlights the predictive power of a dynamic view of emissions intensive
activities. This methodology is a proof of concept that a machine learning
approach can make sub-annual estimates of national greenhouse gas emissions by
sector with a relatively low error that could be of value for policy makers.</div><div><a href='http://arxiv.org/abs/2402.11107v1'>2402.11107v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09553v1")'>Statistical and Machine Learning Models for Predicting Fire and Other
  Emergency Events</div>
<div id='2402.09553v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T20:10:30Z</div><div>Authors: Dilli Prasad Sharma, Nasim Beigi-Mohammadi, Hongxiang Geng, Dawn Dixon, Rob Madro, Phil Emmenegger, Carlos Tobar, Jeff Li, Alberto Leon-Garcia</div><div style='padding-top: 10px; width: 80ex'>Emergency events in a city cause considerable economic loss to individuals,
their families, and the community. Accurate and timely prediction of events can
help the emergency fire and rescue services in preparing for and mitigating the
consequences of emergency events. In this paper, we present a systematic
development of predictive models for various types of emergency events in the
City of Edmonton, Canada. We present methods for (i) data collection and
dataset development; (ii) descriptive analysis of each event type and its
characteristics at different spatiotemporal levels; (iii) feature analysis and
selection based on correlation coefficient analysis and feature importance
analysis; and (iv) development of prediction models for the likelihood of
occurrence of each event type at different temporal and spatial resolutions. We
analyze the association of event types with socioeconomic and demographic data
at the neighborhood level, identify a set of predictors for each event type,
and develop predictive models with negative binomial regression. We conduct
evaluations at neighborhood and fire station service area levels. Our results
show that the models perform well for most of the event types with acceptable
prediction errors for weekly and monthly periods. The evaluation shows that the
prediction accuracy is consistent at the level of the fire station, so the
predictions can be used in management by fire rescue service departments for
planning resource allocation for these time periods. We also examine the impact
of the COVID-19 pandemic on the occurrence of events and on the accuracy of
event predictor models. Our findings show that COVID-19 had a significant
impact on the performance of the event prediction models.</div><div><a href='http://arxiv.org/abs/2402.09553v1'>2402.09553v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05007v1")'>Temporal Analysis of World Disaster Risk:A Machine Learning Approach to
  Cluster Dynamics</div>
<div id='2401.05007v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T08:50:53Z</div><div>Authors: Christian Mulomba Mukendi, Hyebong Choi</div><div style='padding-top: 10px; width: 80ex'>he evaluation of the impact of actions undertaken is essential in management.
This paper assesses the impact of efforts considered to mitigate risk and
create safe environments on a global scale. We measure this impact by looking
at the probability of improvement over a specific short period of time. Using
the World Risk Index, we conduct a temporal analysis of global disaster risk
dynamics from 2011 to 2021. This temporal exploration through the lens of the
World Risk Index provides insights into the complex dynamics of disaster risk.
We found that, despite sustained efforts, the global landscape remains divided
into two main clusters: high susceptibility and moderate susceptibility,
regardless of geographical location. This clustering was achieved using a
semi-supervised approach through the Label Spreading algorithm, with 98%
accuracy. We also found that the prediction of clusters achieved through
supervised learning on the period considered in this study (one, three, and
five years) showed that the Logistic regression (almost 99% at each stage)
performed better than other classifiers. This suggests that the current
policies and mechanisms are not effective in helping countries move from a
hazardous position to a safer one during the period considered. In fact,
statistical projections using a scenario analysis indicate that there is only a
1% chance of such a shift occurring within a five-year timeframe. This sobering
reality highlights the need for a paradigm shift. Traditional long-term
disaster management strategies are not effective for countries that are highly
vulnerable. Our findings indicate the need for an innovative approach that is
tailored to the specific vulnerabilities of these nations. As the threat of
vulnerability persists, our research calls for the development of new
strategies that can effectively address the ongoing challenges of disaster risk
management</div><div><a href='http://arxiv.org/abs/2401.05007v1'>2401.05007v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14210v1")'>At the junction between deep learning and statistics of extremes:
  formalizing the landslide hazard definition</div>
<div id='2401.14210v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T14:48:08Z</div><div>Authors: Ashok Dahal, Raphaël Huser, Luigi Lombardo</div><div style='padding-top: 10px; width: 80ex'>The most adopted definition of landslide hazard combines spatial information
about landslide location (susceptibility), threat (intensity), and frequency
(return period). Only the first two elements are usually considered and
estimated when working over vast areas. Even then, separate models constitute
the standard, with frequency being rarely investigated. Frequency and intensity
are intertwined and depend on each other because larger events occur less
frequently and vice versa. However, due to the lack of multi-temporal
inventories and joint statistical models, modelling such properties via a
unified hazard model has always been challenging and has yet to be attempted.
Here, we develop a unified model to estimate landslide hazard at the slope unit
level to address such gaps. We employed deep learning, combined with a model
motivated by extreme-value theory to analyse an inventory of 30 years of
observed rainfall-triggered landslides in Nepal and assess landslide hazard for
multiple return periods. We also use our model to further explore landslide
hazard for the same return periods under different climate change scenarios up
to the end of the century. Our results show that the proposed model performs
excellently and can be used to model landslide hazard in a unified manner.
Geomorphologically, we find that under both climate change scenarios (SSP245
and SSP885), landslide hazard is likely to increase up to two times on average
in the lower Himalayan regions while remaining the same in the middle Himalayan
region whilst decreasing slightly in the upper Himalayan region areas.</div><div><a href='http://arxiv.org/abs/2401.14210v1'>2401.14210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.00781v1")'>Inferring Heterogeneous Treatment Effects of Crashes on Highway Traffic:
  A Doubly Robust Causal Machine Learning Approach</div>
<div id='2401.00781v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T15:03:14Z</div><div>Authors: Shuang Li, Ziyuan Pu, Zhiyong Cui, Seunghyeon Lee, Xiucheng Guo, Dong Ngoduy</div><div style='padding-top: 10px; width: 80ex'>Highway traffic crashes exert a considerable impact on both transportation
systems and the economy. In this context, accurate and dependable emergency
responses are crucial for effective traffic management. However, the influence
of crashes on traffic status varies across diverse factors and may be biased
due to selection bias. Therefore, there arises a necessity to accurately
estimate the heterogeneous causal effects of crashes, thereby providing
essential insights to facilitate individual-level emergency decision-making.
This paper proposes a novel causal machine learning framework to estimate the
causal effect of different types of crashes on highway speed. The Neyman-Rubin
Causal Model (RCM) is employed to formulate this problem from a causal
perspective. The Conditional Shapley Value Index (CSVI) is proposed based on
causal graph theory to filter adverse variables, and the Structural Causal
Model (SCM) is then adopted to define the statistical estimand for causal
effects. The treatment effects are estimated by Doubly Robust Learning (DRL)
methods, which combine doubly robust causal inference with classification and
regression machine learning models. Experimental results from 4815 crashes on
Highway Interstate 5 in Washington State reveal the heterogeneous treatment
effects of crashes at varying distances and durations. The rear-end crashes
cause more severe congestion and longer durations than other types of crashes,
and the sideswipe crashes have the longest delayed impact. Additionally, the
findings show that rear-end crashes affect traffic greater at night, while
crash to objects has the most significant influence during peak hours.
Statistical hypothesis tests, error metrics based on matched "counterfactual
outcomes", and sensitive analyses are employed for assessment, and the results
validate the accuracy and effectiveness of our method.</div><div><a href='http://arxiv.org/abs/2401.00781v1'>2401.00781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12415v1")'>Vehicle-group-based Crash Risk Formation and Propagation Analysis for
  Expressways</div>
<div id='2402.12415v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T07:47:23Z</div><div>Authors: Tianheng Zhu, Ling Wang, Yiheng Feng, Wanjing Ma, Mohamed Abdel-Aty</div><div style='padding-top: 10px; width: 80ex'>Previous studies in predicting crash risk primarily associated the number or
likelihood of crashes on a road segment with traffic parameters or geometric
characteristics of the segment, usually neglecting the impact of vehicles'
continuous movement and interactions with nearby vehicles. Advancements in
communication technologies have empowered driving information collected from
surrounding vehicles, enabling the study of group-based crash risks. Based on
high-resolution vehicle trajectory data, this research focused on vehicle
groups as the subject of analysis and explored risk formation and propagation
mechanisms considering features of vehicle groups and road segments. Several
key factors contributing to crash risks were identified, including past
high-risk vehicle-group states, complex vehicle behaviors, high percentage of
large vehicles, frequent lane changes within a vehicle group, and specific road
geometries. A multinomial logistic regression model was developed to analyze
the spatial risk propagation patterns, which were classified based on the trend
of high-risk occurrences within vehicle groups. The results indicated that
extended periods of high-risk states, increase in vehicle-group size, and
frequent lane changes are associated with adverse risk propagation patterns.
Conversely, smoother traffic flow and high initial crash risk values are linked
to risk dissipation. Furthermore, the study conducted sensitivity analysis on
different types of classifiers, prediction time intervalsss and adaptive TTC
thresholds. The highest AUC value for vehicle-group risk prediction surpassed
0.93. The findings provide valuable insights to researchers and practitioners
in understanding and prediction of vehicle-group safety, ultimately improving
active traffic safety management and operations of Connected and Autonomous
Vehicles.</div><div><a href='http://arxiv.org/abs/2402.12415v1'>2402.12415v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13421v1")'>Context-Aware Quantitative Risk Assessment Machine Learning Model for
  Drivers Distraction</div>
<div id='2402.13421v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T23:20:36Z</div><div>Authors: Adebamigbe Fasanmade, Ali H. Al-Bayatti, Jarrad Neil Morden, Fabio Caraffini</div><div style='padding-top: 10px; width: 80ex'>Risk mitigation techniques are critical to avoiding accidents associated with
driving behaviour. We provide a novel Multi-Class Driver Distraction Risk
Assessment (MDDRA) model that considers the vehicle, driver, and environmental
data during a journey. MDDRA categorises the driver on a risk matrix as safe,
careless, or dangerous. It offers flexibility in adjusting the parameters and
weights to consider each event on a specific severity level. We collect
real-world data using the Field Operation Test (TeleFOT), covering drivers
using the same routes in the East Midlands, United Kingdom (UK). The results
show that reducing road accidents caused by driver distraction is possible. We
also study the correlation between distraction (driver, vehicle, and
environment) and the classification severity based on a continuous distraction
severity score. Furthermore, we apply machine learning techniques to classify
and predict driver distraction according to severity levels to aid the
transition of control from the driver to the vehicle (vehicle takeover) when a
situation is deemed risky. The Ensemble Bagged Trees algorithm performed best,
with an accuracy of 96.2%.</div><div><a href='http://arxiv.org/abs/2402.13421v1'>2402.13421v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16537v2")'>Efficient Observation Time Window Segmentation for Administrative Data
  Machine Learning</div>
<div id='2401.16537v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T20:18:51Z</div><div>Authors: Musa Taib, Geoffrey G. Messier</div><div style='padding-top: 10px; width: 80ex'>Machine learning models benefit when allowed to learn from temporal trends in
time-stamped administrative data. These trends can be represented by dividing a
model's observation window into time segments or bins. Model training time and
performance can be improved by representing each feature with a different time
resolution. However, this causes the time bin size hyperparameter search space
to grow exponentially with the number of features. The contribution of this
paper is to propose a computationally efficient time series analysis to
investigate binning (TAIB) technique that determines which subset of data
features benefit the most from time bin size hyperparameter tuning. This
technique is demonstrated using hospital and housing/homelessness
administrative data sets. The results show that TAIB leads to models that are
not only more efficient to train but can perform better than models that
default to representing all features with the same time bin size.</div><div><a href='http://arxiv.org/abs/2401.16537v2'>2401.16537v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07022v1")'>A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary
  Modifiable Areal Units</div>
<div id='2403.07022v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T02:34:44Z</div><div>Authors: Liyue Chen, Jiangyi Fang, Tengfei Liu, Shaosheng Cao, Leye Wang</div><div style='padding-top: 10px; width: 80ex'>Spatio-Temporal (ST) prediction is crucial for making informed decisions in
urban location-based applications like ride-sharing. However, existing ST
models often require region partition as a prerequisite, resulting in two main
pitfalls. Firstly, location-based services necessitate ad-hoc regions for
various purposes, requiring multiple ST models with varying scales and zones,
which can be costly to support. Secondly, different ST models may produce
conflicting outputs, resulting in confusing predictions. In this paper, we
propose One4All-ST, a framework that can conduct ST prediction for arbitrary
modifiable areal units using only one model. To reduce the cost of getting
multi-scale predictions, we design an ST network with hierarchical spatial
modeling and scale normalization modules to efficiently and equally learn
multi-scale representations. To address prediction inconsistencies across
scales, we propose a dynamic programming scheme to solve the formulated optimal
combination problem, minimizing predicted error through theoretical analysis.
Besides, we suggest using an extended quad-tree to index the optimal
combinations for quick response to arbitrary modifiable areal units in
practical online scenarios. Extensive experiments on two real-world datasets
verify the efficiency and effectiveness of One4All-ST in ST prediction for
arbitrary modifiable areal units. The source codes and data of this work are
available at https://github.com/uctb/One4All-ST.</div><div><a href='http://arxiv.org/abs/2403.07022v1'>2403.07022v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12391v1")'>FairSTG: Countering performance heterogeneity via collaborative
  sample-level optimization</div>
<div id='2403.12391v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:59:50Z</div><div>Authors: Gengyu Lin, Zhengyang Zhou, Qihe Huang, Kuo Yang, Shifen Cheng, Yang Wang</div><div style='padding-top: 10px; width: 80ex'>Spatiotemporal learning plays a crucial role in mobile computing techniques
to empower smart cites. While existing research has made great efforts to
achieve accurate predictions on the overall dataset, they still neglect the
significant performance heterogeneity across samples. In this work, we
designate the performance heterogeneity as the reason for unfair spatiotemporal
learning, which not only degrades the practical functions of models, but also
brings serious potential risks to real-world urban applications. To fix this
gap, we propose a model-independent Fairness-aware framework for SpatioTemporal
Graph learning (FairSTG), which inherits the idea of exploiting advantages of
well-learned samples to challenging ones with collaborative mix-up.
Specifically, FairSTG consists of a spatiotemporal feature extractor for model
initialization, a collaborative representation enhancement for knowledge
transfer between well-learned samples and challenging ones, and fairness
objectives for immediately suppressing sample-level performance heterogeneity.
Experiments on four spatiotemporal datasets demonstrate that our FairSTG
significantly improves the fairness quality while maintaining comparable
forecasting accuracy. Case studies show FairSTG can counter both spatial and
temporal performance heterogeneity by our sample-level retrieval and
compensation, and our work can potentially alleviate the risks on
spatiotemporal resource allocation for underrepresented urban regions.</div><div><a href='http://arxiv.org/abs/2403.12391v1'>2403.12391v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12681v1")'>Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical
  Learning</div>
<div id='2401.12681v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T11:46:31Z</div><div>Authors: Zhishuai Li, Yunhao Nie, Ziyue Li, Lei Bai, Yisheng Lv, Rui Zhao</div><div style='padding-top: 10px; width: 80ex'>Kriging aims at estimating the attributes of unsampled geo-locations from
observations in the spatial vicinity or physical connections, which helps
mitigate skewed monitoring caused by under-deployed sensors. Existing works
assume that neighbors' information offers the basis for estimating the
attributes of the unobserved target while ignoring non-neighbors. However,
non-neighbors could also offer constructive information, and neighbors could
also be misleading. To this end, we propose ``Contrastive-Prototypical''
self-supervised learning for Kriging (KCP) to refine valuable information from
neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we
conduct the Kriging task from a new perspective of representation: we aim to
first learn robust and general representations and then recover attributes from
representations. A neighboring contrastive module is designed that coarsely
learns the representations by narrowing the representation distance between the
target and its neighbors while pushing away the non-neighbors. In parallel, a
prototypical module is introduced to identify similar representations via
exchanged prediction, thus refining the misleading neighbors and recycling the
useful non-neighbors from the neighboring contrast component. As a result, not
all the neighbors and some of the non-neighbors will be used to infer the
target. To encourage the two modules above to learn general and robust
representations, we design an adaptive augmentation module that incorporates
data-driven attribute augmentation and centrality-based topology augmentation
over the spatiotemporal Kriging graph data. Extensive experiments on real-world
datasets demonstrate the superior performance of KCP compared to its peers with
6% improvements and exceptional transferability and robustness. The code is
available at https://github.com/bonaldli/KCP</div><div><a href='http://arxiv.org/abs/2401.12681v1'>2401.12681v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01252v1")'>Target inductive methods for zero-shot regression</div>
<div id='2402.01252v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:19:45Z</div><div>Authors: Miriam Fdez-Díaz, José Ramón Quevedo, Elena Montañés</div><div style='padding-top: 10px; width: 80ex'>This research arises from the need to predict the amount of air pollutants in
meteorological stations. Air pollution depends on the location of the stations
(weather conditions and activities in the surroundings). Frequently, the
surrounding information is not considered in the learning process. This
information is known beforehand in the absence of unobserved weather conditions
and remains constant for the same station. Considering the surrounding
information as side information facilitates the generalization for predicting
pollutants in new stations, leading to a zero-shot regression scenario.
Available methods in zero-shot typically lean towards classification, and are
not easily extensible to regression. This paper proposes two zero-shot methods
for regression. The first method is a similarity based approach that learns
models from features and aggregates them using side information. However,
potential knowledge of the feature models may be lost in the aggregation. The
second method overcomes this drawback by replacing the aggregation procedure
and learning the correspondence between side information and feature-induced
models, instead. Both proposals are compared with a baseline procedure using
artificial datasets, UCI repository communities and crime datasets, and the
pollutants. Both approaches outperform the baseline method, but the parameter
learning approach manifests its superiority over the similarity based method.</div><div><a href='http://arxiv.org/abs/2402.01252v1'>2402.01252v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00222v1")'>Uncover the nature of overlapping community in cities</div>
<div id='2402.00222v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T22:50:49Z</div><div>Authors: Peng Luo, Di Zhu</div><div style='padding-top: 10px; width: 80ex'>Urban spaces, though often perceived as discrete communities, are shared by
various functional and social groups. Our study introduces a graph-based
physics-aware deep learning framework, illuminating the intricate overlapping
nature inherent in urban communities. Through analysis of individual mobile
phone positioning data at Twin Cities metro area (TCMA) in Minnesota, USA, our
findings reveal that 95.7 % of urban functional complexity stems from the
overlapping structure of communities during weekdays. Significantly, our
research not only quantifies these overlaps but also reveals their compelling
correlations with income and racial indicators, unraveling the complex
segregation patterns in U.S. cities. As the first to elucidate the overlapping
nature of urban communities, this work offers a unique geospatial perspective
on looking at urban structures, highlighting the nuanced interplay of
socioeconomic dynamics within cities.</div><div><a href='http://arxiv.org/abs/2402.00222v1'>2402.00222v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17905v1")'>Using Graph Neural Networks to Predict Local Culture</div>
<div id='2402.17905v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:43:14Z</div><div>Authors: Thiago H Silva, Daniel Silver</div><div style='padding-top: 10px; width: 80ex'>Urban research has long recognized that neighbourhoods are dynamic and
relational. However, lack of data, methodologies, and computer processing power
have hampered a formal quantitative examination of neighbourhood relational
dynamics. To make progress on this issue, this study proposes a graph neural
network (GNN) approach that permits combining and evaluating multiple sources
of information about internal characteristics of neighbourhoods, their past
characteristics, and flows of groups among them, potentially providing greater
expressive power in predictive models. By exploring a public large-scale
dataset from Yelp, we show the potential of our approach for considering
structural connectedness in predicting neighbourhood attributes, specifically
to predict local culture. Results are promising from a substantive and
methodologically point of view. Substantively, we find that either local area
information (e.g. area demographics) or group profiles (tastes of Yelp
reviewers) give the best results in predicting local culture, and they are
nearly equivalent in all studied cases. Methodologically, exploring group
profiles could be a helpful alternative where finding local information for
specific areas is challenging, since they can be extracted automatically from
many forms of online data. Thus, our approach could empower researchers and
policy-makers to use a range of data sources when other local area information
is lacking.</div><div><a href='http://arxiv.org/abs/2402.17905v1'>2402.17905v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.01147v1")'>A Hybrid Model for Traffic Incident Detection based on Generative
  Adversarial Networks and Transformer Model</div>
<div id='2403.01147v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T09:28:04Z</div><div>Authors: Xinying Lu, Doudou Zhang, Jianli Xiao</div><div style='padding-top: 10px; width: 80ex'>In addition to enhancing traffic safety and facilitating prompt emergency
response, traffic incident detection plays an indispensable role in intelligent
transportation systems by providing real-time traffic status information. This
enables the realization of intelligent traffic control and management. Previous
research has identified that apart from employing advanced algorithmic models,
the effectiveness of detection is also significantly influenced by challenges
related to acquiring large datasets and addressing dataset imbalances. A hybrid
model combining transformer and generative adversarial networks (GANs) is
proposed to address these challenges. Experiments are conducted on four real
datasets to validate the superiority of the transformer in traffic incident
detection. Additionally, GANs are utilized to expand the dataset and achieve a
balanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against
the baseline model. The results demonstrate that the proposed model enhances
the dataset size, balances the dataset, and improves the performance of traffic
incident detection in various aspects.</div><div><a href='http://arxiv.org/abs/2403.01147v1'>2403.01147v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18167v1")'>Decentralised Traffic Incident Detection via Network Lasso</div>
<div id='2402.18167v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T08:56:00Z</div><div>Authors: Qiyuan Zhu, A. K. Qin, Prabath Abeysekara, Hussein Dia, Hanna Grzybowska</div><div style='padding-top: 10px; width: 80ex'>Traffic incident detection plays a key role in intelligent transportation
systems, which has gained great attention in transport engineering. In the
past, traditional machine learning (ML) based detection methods achieved good
performance under a centralised computing paradigm, where all data are
transmitted to a central server for building ML models therein. Nowadays, deep
neural networks based federated learning (FL) has become a mainstream detection
approach to enable the model training in a decentralised manner while
warranting local data governance. Such neural networks-centred techniques,
however, have overshadowed the utility of well-established ML-based detection
methods. In this work, we aim to explore the potential of potent conventional
ML-based detection models in modern traffic scenarios featured by distributed
data. We leverage an elegant but less explored distributed optimisation
framework named Network Lasso, with guaranteed global convergence for convex
problem formulations, integrate the potent convex ML model with it, and compare
it with centralised learning, local learning, and federated learning methods
atop a well-known traffic incident detection dataset. Experimental results show
that the proposed network lasso-based approach provides a promising alternative
to the FL-based approach in data-decentralised traffic scenarios, with a strong
convergence guarantee while rekindling the significance of conventional
ML-based detection methods.</div><div><a href='http://arxiv.org/abs/2402.18167v1'>2402.18167v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04812v1")'>TrafPS: A Shapley-based Visual Analytics Approach to Interpret Traffic</div>
<div id='2403.04812v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T01:00:55Z</div><div>Authors: Zezheng Feng, Yifan Jiang, Hongjun Wang, Zipei Fan, Yuxin Ma, Shuang-Hua Yang, Huamin Qu, Xuan Song</div><div style='padding-top: 10px; width: 80ex'>Recent achievements in deep learning (DL) have shown its potential for
predicting traffic flows. Such predictions are beneficial for understanding the
situation and making decisions in traffic control. However, most
state-of-the-art DL models are considered "black boxes" with little to no
transparency for end users with respect to the underlying mechanisms. Some
previous work tried to "open the black boxes" and increase the interpretability
of how predictions are generated. However, it still remains challenging to
handle complex models on large-scale spatio-temporal data and discover salient
spatial and temporal patterns that significantly influence traffic flows. To
overcome the challenges, we present TrafPS, a visual analytics approach for
interpreting traffic prediction outcomes to support decision-making in traffic
management and urban planning. The measurements, region SHAP and trajectory
SHAP, are proposed to quantify the impact of flow patterns on urban traffic at
different levels. Based on the task requirement from the domain experts, we
employ an interactive visual interface for multi-aspect exploration and
analysis of significant flow patterns. Two real-world case studies demonstrate
the effectiveness of TrafPS in identifying key routes and decision-making
support for urban planning.</div><div><a href='http://arxiv.org/abs/2403.04812v1'>2403.04812v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15398v1")'>TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow
  Attention for Commuting Flow Prediction</div>
<div id='2402.15398v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:00:04Z</div><div>Authors: Yan Luo, Zhuoyue Wan, Yuzhong Chen, Gengchen Mai, Fu-lai Chung, Kent Larson</div><div style='padding-top: 10px; width: 80ex'>Understanding the link between urban planning and commuting flows is crucial
for guiding urban development and policymaking. This research, bridging
computer science and urban studies, addresses the challenge of integrating
these fields with their distinct focuses. Traditional urban studies methods,
like the gravity and radiation models, often underperform in complex scenarios
due to their limited handling of multiple variables and reliance on overly
simplistic and unrealistic assumptions, such as spatial isotropy. While deep
learning models offer improved accuracy, their black-box nature poses a
trade-off between performance and explainability -- both vital for analyzing
complex societal phenomena like commuting flows. To address this, we introduce
TransFlower, an explainable, transformer-based model employing flow-to-flow
attention to predict urban commuting patterns. It features a geospatial encoder
with an anisotropy-aware relative location encoder for nuanced flow
representation. Following this, the transformer-based flow predictor enhances
this by leveraging attention mechanisms to efficiently capture flow
interactions. Our model outperforms existing methods by up to 30.8% Common Part
of Commuters, offering insights into mobility dynamics crucial for urban
planning and policy decisions.</div><div><a href='http://arxiv.org/abs/2402.15398v1'>2402.15398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03457v1")'>Efficient and Interpretable Traffic Destination Prediction using
  Explainable Boosting Machines</div>
<div id='2402.03457v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T19:09:42Z</div><div>Authors: Yasin Yousif, Jörg Müller</div><div style='padding-top: 10px; width: 80ex'>Developing accurate models for traffic trajectory predictions is crucial for
achieving fully autonomous driving. Various deep neural network models have
been employed to address this challenge, but their black-box nature hinders
transparency and debugging capabilities in a deployed system. Glass-box models
offer a solution by providing full interpretability through methods like
\ac{GAM}. In this study, we evaluate an efficient additive model called
\ac{EBM} for traffic prediction on three popular mixed traffic datasets:
\ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models
perform competitively in predicting pedestrian destinations within \ac{SDD} and
\ac{InD} while providing modest predictions for vehicle-dominant Argoverse
dataset. Additionally, our transparent trained models allow us to analyse
feature importance and interactions, as well as provide qualitative examples of
predictions explanation. The full training code will be made public upon
publication.</div><div><a href='http://arxiv.org/abs/2402.03457v1'>2402.03457v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03585v1")'>RouteExplainer: An Explanation Framework for Vehicle Routing Problem</div>
<div id='2403.03585v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T10:01:35Z</div><div>Authors: Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuusuke Nakano</div><div style='padding-top: 10px; width: 80ex'>The Vehicle Routing Problem (VRP) is a widely studied combinatorial
optimization problem and has been applied to various practical problems. While
the explainability for VRP is significant for improving the reliability and
interactivity in practical VRP applications, it remains unexplored. In this
paper, we propose RouteExplainer, a post-hoc explanation framework that
explains the influence of each edge in a generated route. Our framework
realizes this by rethinking a route as the sequence of actions and extending
counterfactual explanations based on the action influence model to VRP. To
enhance the explanation, we additionally propose an edge classifier that infers
the intentions of each edge, a loss function to train the edge classifier, and
explanation-text generation by Large Language Models (LLMs). We quantitatively
evaluate our edge classifier on four different VRPs. The results demonstrate
its rapid computation while maintaining reasonable accuracy, thereby
highlighting its potential for deployment in practical applications. Moreover,
on the subject of a tourist route, we qualitatively evaluate explanations
generated by our framework. This evaluation not only validates our framework
but also shows the synergy between explanation frameworks and LLMs. See
https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.</div><div><a href='http://arxiv.org/abs/2403.03585v1'>2403.03585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.13098v2")'>Gravity-Informed Deep Learning Framework for Predicting Ship Traffic
  Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge</div>
<div id='2401.13098v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T21:22:51Z</div><div>Authors: Ruixin Song, Gabriel Spadon, Ronald Pelot, Stan Matwin, Amilcar Soares</div><div style='padding-top: 10px; width: 80ex'>Invasive species in water bodies pose a major threat to the environment and
biodiversity globally. Due to increased transportation and trade, non-native
species have been introduced to new environments, causing damage to ecosystems
and leading to economic losses in agriculture, forestry, and fisheries.
Therefore, there is a pressing need for risk assessment and management
techniques to mitigate the impact of these invasions. This study aims to
develop a new physics-inspired model to forecast maritime shipping traffic and
thus inform risk assessment of invasive species spread through global
transportation networks. Inspired by the gravity model for international
trades, our model considers various factors that influence the likelihood and
impact of vessel activities, such as shipping flux density, distance between
ports, trade flow, and centrality measures of transportation hubs.
Additionally, by analyzing the risk network of invasive species, we provide a
comprehensive framework for assessing the invasion threat level given a pair of
origin and destination. Accordingly, this paper introduces transformers to
gravity models to rebuild the short- and long-term dependencies that make the
risk analysis feasible. Thus, we introduce a physics-inspired framework that
achieves an 89% segmentation accuracy for existing and non-existing
trajectories and an 84.8% accuracy for the number of vessels flowing between
key port areas, representing more than 10% improvement over the traditional
deep-gravity model. Along these lines, this research contributes to a better
understanding of invasive species risk assessment. It allows policymakers,
conservationists, and stakeholders to prioritize management actions by
identifying high-risk invasion pathways. Besides, our model is versatile and
can include new data sources, making it suitable for assessing species invasion
risks in a changing global landscape.</div><div><a href='http://arxiv.org/abs/2401.13098v2'>2401.13098v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12418v1")'>STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space
  Model</div>
<div id='2403.12418v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T04:02:57Z</div><div>Authors: Lincan Li, Hanchen Wang, Wenjie Zhang, Adelle Coster</div><div style='padding-top: 10px; width: 80ex'>Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,
and non-stationary, leading to the continuous challenge of spatial-temporal
graph learning. In the past few years, various GNN-based methods have been
proposed to solely focus on mimicking the relationships among node individuals
of the STG network, ignoring the significance of modeling the intrinsic
features that exist in STG system over time. In contrast, modern Selective
State Space Models (SSSMs) present a new approach which treat STG Network as a
system, and meticulously explore the STG system's dynamic state evolution
across temporal dimension. In this work, we introduce Spatial-Temporal Graph
Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective
state space models for STG learning by treating STG Network as a system, and
employing the Graph Selective State Space Block (GS3B) to precisely
characterize the dynamic evolution of STG networks. STG-Mamba is formulated as
an Encoder-Decoder architecture, which takes GS3B as the basic module, for
efficient sequential data modeling. Furthermore, to strengthen GNN's ability of
modeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph
Neural Networks (KFGN) for adaptive graph structure upgrading. KFGN smoothly
fits in the context of selective state space evolution, and at the same time
keeps linear complexity. Extensive empirical studies are conducted on three
benchmark STG forecasting datasets, demonstrating the performance superiority
and computational efficiency of STG-Mamba. It not only surpasses existing
state-of-the-art methods in terms of STG forecasting performance, but also
effectively alleviate the computational bottleneck of large-scale graph
networks in reducing the computational cost of FLOPs and test inference time.</div><div><a href='http://arxiv.org/abs/2403.12418v1'>2403.12418v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16800v1")'>Online Algorithm for Node Feature Forecasting in Temporal Graphs</div>
<div id='2401.16800v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T07:31:51Z</div><div>Authors: Aniq Ur Rahman, Justin P. Coon</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose an online algorithm "mspace" for forecasting node
features in temporal graphs, which adeptly captures spatial cross-correlation
among different nodes as well as the temporal autocorrelation within a node.
The algorithm can be used for both probabilistic and deterministic multi-step
forecasting, making it applicable for estimation and generation tasks.
Comparative evaluations against various baselines, including graph neural
network (GNN) based models and classical Kalman filters, demonstrate that
mspace performs at par with the state-of-the-art and even surpasses them on
some datasets. Importantly, mspace demonstrates consistent robustness across
datasets with varying training sizes, a notable advantage over GNN-based
methods requiring abundant training samples to learn the spatiotemporal trends
in the data effectively. Therefore, employing mspace is advantageous in
scenarios where the training sample availability is limited. Additionally, we
establish theoretical bounds on multi-step forecasting error of mspace and show
that it scales as $O(q)$ for $q$-step forecast.</div><div><a href='http://arxiv.org/abs/2401.16800v1'>2401.16800v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07958v2")'>GD-CAF: Graph Dual-stream Convolutional Attention Fusion for
  Precipitation Nowcasting</div>
<div id='2401.07958v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T20:54:20Z</div><div>Authors: Lorand Vatamany, Siamak Mehrkanoon</div><div style='padding-top: 10px; width: 80ex'>Accurate precipitation nowcasting is essential for various applications,
including flood prediction, disaster management, optimizing agricultural
activities, managing transportation routes and renewable energy. While several
studies have addressed this challenging task from a sequence-to-sequence
perspective, most of them have focused on a single area without considering the
existing correlation between multiple disjoint regions. In this paper, we
formulate precipitation nowcasting as a spatiotemporal graph sequence
nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional
Attention Fusion (GD-CAF), a novel approach designed to learn from historical
spatiotemporal graph of precipitation maps and nowcast future time step ahead
precipitation at different spatial locations. GD-CAF consists of
spatio-temporal convolutional attention as well as gated fusion modules which
are equipped with depthwise-separable convolutional operations. This
enhancement enables the model to directly process the high-dimensional
spatiotemporal graph of precipitation maps and exploits higher-order
correlations between the data dimensions. We evaluate our model on seven years
of precipitation maps across Europe and its neighboring areas collected from
the ERA5 dataset, provided by Copernicus Climate Change Services. The
experimental results reveal the superior performance of the GD-CAF model
compared to the other examined models. Additionally, visualizations of averaged
seasonal spatial and temporal attention scores across the test set offer
valuable insights into the most robust connections between diverse regions or
time steps.</div><div><a href='http://arxiv.org/abs/2401.07958v2'>2401.07958v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07065v1")'>Tensor Graph Convolutional Network for Dynamic Graph Representation
  Learning</div>
<div id='2401.07065v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T12:49:56Z</div><div>Authors: Ling Wang, Ye Yuan</div><div style='padding-top: 10px; width: 80ex'>Dynamic graphs (DG) describe dynamic interactions between entities in many
practical scenarios. Most existing DG representation learning models combine
graph convolutional network and sequence neural network, which model
spatial-temporal dependencies through two different types of neural networks.
However, this hybrid design cannot well capture the spatial-temporal continuity
of a DG. In this paper, we propose a tensor graph convolutional network to
learn DG representations in one convolution framework based on the tensor
product with the following two-fold ideas: a) representing the information of
DG by tensor form; b) adopting tensor product to design a tensor graph
convolutional network modeling spatial-temporal feature simultaneously.
Experiments on real-world DG datasets demonstrate that our model obtains
state-of-the-art performance.</div><div><a href='http://arxiv.org/abs/2401.07065v1'>2401.07065v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13872v1")'>Spatial-Temporal Graph Representation Learning for Tactical Networks
  Future State Prediction</div>
<div id='2403.13872v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T15:27:17Z</div><div>Authors: Liu Junhua, Albrethsen Justin, Goh Lincoln, Yau David, Lim Kwan Hui</div><div style='padding-top: 10px; width: 80ex'>Resource allocation in tactical ad-hoc networks presents unique challenges
due to their dynamic and multi-hop nature. Accurate prediction of future
network connectivity is essential for effective resource allocation in such
environments. In this paper, we introduce the Spatial-Temporal Graph
Encoder-Decoder (STGED) framework for Tactical Communication Networks that
leverages both spatial and temporal features of network states to learn latent
tactical behaviors effectively. STGED hierarchically utilizes graph-based
attention mechanism to spatially encode a series of communication network
states, leverages a recurrent neural network to temporally encode the evolution
of states, and a fully-connected feed-forward network to decode the
connectivity in the future state. Through extensive experiments, we demonstrate
that STGED consistently outperforms baseline models by large margins across
different time-steps input, achieving an accuracy of up to 99.2\% for the
future state prediction task of tactical communication networks.</div><div><a href='http://arxiv.org/abs/2403.13872v1'>2403.13872v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02723v1")'>Predicting Traffic Flow with Federated Learning and Graph Neural with
  Asynchronous Computations Network</div>
<div id='2401.02723v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T09:36:42Z</div><div>Authors: Muhammad Yaqub, Shahzad Ahmad, Malik Abdul Manan, Imran Shabir Chuhan</div><div style='padding-top: 10px; width: 80ex'>Real-time traffic flow prediction holds significant importance within the
domain of Intelligent Transportation Systems (ITS). The task of achieving a
balance between prediction precision and computational efficiency presents a
significant challenge. In this article, we present a novel deep-learning method
called Federated Learning and Asynchronous Graph Convolutional Network
(FLAGCN). Our framework incorporates the principles of asynchronous graph
convolutional networks with federated learning to enhance the accuracy and
efficiency of real-time traffic flow prediction. The FLAGCN model employs a
spatial-temporal graph convolution technique to asynchronously address
spatio-temporal dependencies within traffic data effectively. To efficiently
handle the computational requirements associated with this deep learning model,
this study used a graph federated learning technique known as GraphFL. This
approach is designed to facilitate the training process. The experimental
results obtained from conducting tests on two distinct traffic datasets
demonstrate that the utilization of FLAGCN leads to the optimization of both
training and inference durations while maintaining a high level of prediction
accuracy. FLAGCN outperforms existing models with significant improvements by
achieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in
MAPE, compared to the best-performing existing models.</div><div><a href='http://arxiv.org/abs/2401.02723v1'>2401.02723v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00713v2")'>A Survey on Graph Neural Networks in Intelligent Transportation Systems</div>
<div id='2401.00713v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T09:53:24Z</div><div>Authors: Hourun Li, Yusheng Zhao, Zhengyang Mao, Yifang Qin, Zhiping Xiao, Jiaqi Feng, Yiyang Gu, Wei Ju, Xiao Luo, Ming Zhang</div><div style='padding-top: 10px; width: 80ex'>Intelligent Transportation System (ITS) is vital in improving traffic
congestion, reducing traffic accidents, optimizing urban planning, etc.
However, due to the complexity of the traffic network, traditional machine
learning and statistical methods are relegated to the background. With the
advent of the artificial intelligence era, many deep learning frameworks have
made remarkable progress in various fields and are now considered effective
methods in many areas. As a deep learning method, Graph Neural Networks (GNNs)
have emerged as a highly competitive method in the ITS field since 2019 due to
their strong ability to model graph-related problems. As a result, more and
more scholars pay attention to the applications of GNNs in transportation
domains, which have shown excellent performance. However, most of the research
in this area is still concentrated on traffic forecasting, while other ITS
domains, such as autonomous vehicles and urban planning, still require more
attention. This paper aims to review the applications of GNNs in six
representative and emerging ITS domains: traffic forecasting, autonomous
vehicles, traffic signal control, transportation safety, demand prediction, and
parking management. We have reviewed extensive graph-related studies from 2018
to 2023, summarized their methods, features, and contributions, and presented
them in informative tables or lists. Finally, we have identified the challenges
of applying GNNs to ITS and suggested potential future directions.</div><div><a href='http://arxiv.org/abs/2401.00713v2'>2401.00713v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03138v1")'>TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation
  and Prediction via Multifaceted Graph Modeling</div>
<div id='2401.03138v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:44:06Z</div><div>Authors: ChungYi Lin, Shen-Lung Tung, Hung-Ting Su, Winston H. Hsu</div><div style='padding-top: 10px; width: 80ex'>To address the limitations of traffic prediction from location-bound
detectors, we present Geographical Cellular Traffic (GCT) flow, a novel data
source that leverages the extensive coverage of cellular traffic to capture
mobility patterns. Our extensive analysis validates its potential for
transportation. Focusing on vehicle-related GCT flow prediction, we propose a
graph neural network that integrates multivariate, temporal, and spatial facets
for improved accuracy. Experiments reveal our model's superiority over
baselines, especially in long-term predictions. We also highlight the potential
for GCT flow integration into transportation systems.</div><div><a href='http://arxiv.org/abs/2401.03138v1'>2401.03138v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17095v1")'>Traffic estimation in unobserved network locations using data-driven
  macroscopic models</div>
<div id='2401.17095v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T15:21:50Z</div><div>Authors: Pablo Guarda, Sean Qian</div><div style='padding-top: 10px; width: 80ex'>This paper leverages macroscopic models and multi-source spatiotemporal data
collected from automatic traffic counters and probe vehicles to accurately
estimate traffic flow and travel time in links where these measurements are
unavailable. This problem is critical in transportation planning applications
where the sensor coverage is low and the planned interventions have
network-wide impacts. The proposed model, named the Macroscopic Traffic
Estimator (MaTE), can perform network-wide estimations of traffic flow and
travel time only using the set of observed measurements of these quantities.
Because MaTE is grounded in macroscopic flow theory, all parameters and
variables are interpretable. The estimated traffic flow satisfies fundamental
flow conservation constraints and exhibits an increasing monotonic relationship
with the estimated travel time. Using logit-based stochastic traffic assignment
as the principle for routing flow behavior makes the model fully differentiable
with respect to the model parameters. This property facilitates the application
of computational graphs to learn parameters from vast amounts of spatiotemporal
data. We also integrate neural networks and polynomial kernel functions to
capture link flow interactions and enrich the mapping of traffic flows into
travel times. MaTE also adds a destination choice model and a trip generation
model that uses historical data on the number of trips generated by location.
Experiments on synthetic data show that the model can accurately estimate
travel time and traffic flow in out-of-sample links. Results obtained using
real-world multi-source data from a large-scale transportation network suggest
that MaTE outperforms data-driven benchmarks, especially in travel time
estimation. The estimated parameters of MaTE are also informative about the
hourly change in travel demand and supply characteristics of the transportation
network.</div><div><a href='http://arxiv.org/abs/2401.17095v1'>2401.17095v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14504v1")'>Learning When to See for Long-term Traffic Data Collection on
  Power-constrained Devices</div>
<div id='2401.14504v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:50:34Z</div><div>Authors: Ruixuan Zhang, Wenyu Han, Zilin Bian, Kaan Ozbay, Chen Feng</div><div style='padding-top: 10px; width: 80ex'>Collecting traffic data is crucial for transportation systems and urban
planning, and is often more desirable through easy-to-deploy but
power-constrained devices, due to the unavailability or high cost of power and
network infrastructure. The limited power means an inevitable trade-off between
data collection duration and accuracy/resolution. We introduce a novel
learning-based framework that strategically decides observation timings for
battery-powered devices and reconstructs the full data stream from sparsely
sampled observations, resulting in minimal performance loss and a significantly
prolonged system lifetime. Our framework comprises a predictor, a controller,
and an estimator. The predictor utilizes historical data to forecast future
trends within a fixed time horizon. The controller uses the forecasts to
determine the next optimal timing for data collection. Finally, the estimator
reconstructs the complete data profile from the sampled observations. We
evaluate the performance of the proposed method on PeMS data by an RNN
(Recurrent Neural Network) predictor and estimator, and a DRQN (Deep Recurrent
Q-Network) controller, and compare it against the baseline that uses Kalman
filter and uniform sampling. The results indicate that our method outperforms
the baseline, primarily due to the inclusion of more representative data points
in the profile, resulting in an overall 10\% improvement in estimation
accuracy. Source code will be publicly available.</div><div><a href='http://arxiv.org/abs/2401.14504v1'>2401.14504v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11090v1")'>Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via
  NN-Driven Traffic Analysis at Line-Speed</div>
<div id='2403.11090v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T04:59:30Z</div><div>Authors: Jinzhu Yan, Haotian Xu, Zhuotao Liu, Qi Li, Ke Xu, Mingwei Xu, Jianping Wu</div><div style='padding-top: 10px; width: 80ex'>The emerging programmable networks sparked significant research on
Intelligent Network Data Plane (INDP), which achieves learning-based traffic
analysis at line-speed. Prior art in INDP focus on deploying tree/forest models
on the data plane. We observe a fundamental limitation in tree-based INDP
approaches: although it is possible to represent even larger tree/forest tables
on the data plane, the flow features that are computable on the data plane are
fundamentally limited by hardware constraints. In this paper, we present BoS to
push the boundaries of INDP by enabling Neural Network (NN) driven traffic
analysis at line-speed. Many types of NNs (such as Recurrent Neural Network
(RNN), and transformers) that are designed to work with sequential data have
advantages over tree-based models, because they can take raw network data as
input without complex feature computations on the fly. However, the challenge
is significant: the recurrent computation scheme used in RNN inference is
fundamentally different from the match-action paradigm used on the network data
plane. BoS addresses this challenge by (i) designing a novel data plane
friendly RNN architecture that can execute unlimited RNN time steps with
limited data plane stages, effectively achieving line-speed RNN inference; and
(ii) complementing the on-switch RNN model with an off-switch transformer-based
traffic analysis module to further boost the overall performance. We implement
a prototype of BoS using a P4 programmable switch as our data plane, and
extensively evaluate it over multiple traffic analysis tasks. The results show
that BoS outperforms state-of-the-art in both analysis accuracy and
scalability.</div><div><a href='http://arxiv.org/abs/2403.11090v1'>2403.11090v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15742v1")'>Efficient Data-Driven MPC for Demand Response of Commercial Buildings</div>
<div id='2401.15742v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T20:01:44Z</div><div>Authors: Marie-Christine Paré, Vasken Dermardiros, Antoine Lesage-Landry</div><div style='padding-top: 10px; width: 80ex'>Model predictive control (MPC) has been shown to significantly improve the
energy efficiency of buildings while maintaining thermal comfort. Data-driven
approaches based on neural networks have been proposed to facilitate system
modelling. However, such approaches are generally nonconvex and result in
computationally intractable optimization problems. In this work, we design a
readily implementable energy management method for small commercial buildings.
We then leverage our approach to formulate a real-time demand bidding strategy.
We propose a data-driven and mixed-integer convex MPC which is solved via
derivative-free optimization given a limited computational time of 5 minutes to
respect operational constraints. We consider rooftop unit heating, ventilation,
and air conditioning systems with discrete controls to accurately model the
operation of most commercial buildings. Our approach uses an input convex
recurrent neural network to model the thermal dynamics. We apply our approach
in several demand response (DR) settings, including a demand bidding, a
time-of-use, and a critical peak rebate program. Controller performance is
evaluated on a state-of-the-art building simulation. The proposed approach
improves thermal comfort while reducing energy consumption and cost through DR
participation, when compared to other data-driven approaches or a set-point
controller.</div><div><a href='http://arxiv.org/abs/2401.15742v1'>2401.15742v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12539v1")'>Impact of data usage for forecasting on performance of model predictive
  control in buildings with smart energy storage</div>
<div id='2402.12539v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T21:01:11Z</div><div>Authors: Max Langtry, Vijja Wichitwechkarn, Rebecca Ward, Chaoqun Zhuang, Monika J. Kreitmair, Nikolas Makasis, Zack Xuereb Conti, Ruchi Choudhary</div><div style='padding-top: 10px; width: 80ex'>Data is required to develop forecasting models for use in Model Predictive
Control (MPC) schemes in building energy systems. However, data usage incurs
costs from both its collection and exploitation. Determining cost optimal data
usage requires understanding of the forecast accuracy and resulting MPC
operational performance it enables. This study investigates the performance of
both simple and state-of-the-art machine learning prediction models for MPC in
a multi-building energy system simulation using historic building energy data.
The impact of data usage on forecast accuracy is quantified for the following
data efficiency measures: reuse of prediction models, reduction of training
data volumes, reduction of model data features, and online model training. A
simple linear multi-layer perceptron model is shown to provide equivalent
forecast accuracy to state-of-the-art models, with greater data efficiency and
generalisability. The use of more than 2 years of training data for load
prediction models provided no significant improvement in forecast accuracy.
Forecast accuracy and data efficiency were improved simultaneously by using
change-point analysis to screen training data. Reused models and those trained
with 3 months of data had on average 10% higher error than baseline, indicating
that deploying MPC systems without prior data collection may be economic.</div><div><a href='http://arxiv.org/abs/2402.12539v1'>2402.12539v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11877v1")'>Efficient Training of Learning-Based Thermal Power Flow for 4th
  Generation District Heating Grids</div>
<div id='2403.11877v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:31:09Z</div><div>Authors: Andreas Bott, Mario Beykirch, Florian Steinke</div><div style='padding-top: 10px; width: 80ex'>Thermal power flow (TPF) is an important task for various control purposes in
4 Th generation district heating grids with multiple decentral heat sources and
meshed grid structures. Computing the TPF, i.e., determining the grid state
consisting of temperatures, pressures, and mass flows for given supply and
demand values, is classically done by solving the nonlinear heat grid
equations, but can be sped up by orders of magnitude using learned models such
as neural networks. We propose a novel, efficient scheme to generate a
sufficiently large training data set covering relevant supply and demand
values. Instead of sampling supply and demand values, our approach generates
training examples from a proxy distribution over generator and consumer mass
flows, omitting the iterations needed for solving the heat grid equations. The
exact, but slightly different, training examples can be weighted to represent
the original training distribution. We show with simulations for typical grid
structures that the new approach can reduce training set generation times by
two orders of magnitude compared to sampling supply and demand values directly,
without loss of relevance for the training samples. Moreover, learning TPF with
a training data set is shown to outperform sample-free, physics-aware training
approaches significantly.</div><div><a href='http://arxiv.org/abs/2403.11877v1'>2403.11877v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00276v1")'>Graph Construction with Flexible Nodes for Traffic Demand Prediction</div>
<div id='2403.00276v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T04:38:51Z</div><div>Authors: Jinyan Hou, Shan Liu, Ya Zhang, Haotong Qin</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have been widely applied in traffic demand
prediction, and transportation modes can be divided into station-based mode and
free-floating traffic mode. Existing research in traffic graph construction
primarily relies on map matching to construct graphs based on the road network.
However, the complexity and inhomogeneity of data distribution in free-floating
traffic demand forecasting make road network matching inflexible. To tackle
these challenges, this paper introduces a novel graph construction method
tailored to free-floating traffic mode. We propose a novel density-based
clustering algorithm (HDPC-L) to determine the flexible positioning of nodes in
the graph, overcoming the computational bottlenecks of traditional clustering
algorithms and enabling effective handling of large-scale datasets.
Furthermore, we extract valuable information from ridership data to initialize
the edge weights of GNNs. Comprehensive experiments on two real-world datasets,
the Shenzhen bike-sharing dataset and the Haikou ride-hailing dataset, show
that the method significantly improves the performance of the model. On
average, our models show an improvement in accuracy of around 25\% and 19.5\%
on the two datasets. Additionally, it significantly enhances computational
efficiency, reducing training time by approximately 12% and 32.5% on the two
datasets. We make our code available at
https://github.com/houjinyan/HDPC-L-ODInit.</div><div><a href='http://arxiv.org/abs/2403.00276v1'>2403.00276v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11495v1")'>Semantic-Enhanced Representation Learning for Road Networks with
  Temporal Dynamics</div>
<div id='2403.11495v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T05:59:56Z</div><div>Authors: Yile Chen, Xiucheng Li, Gao Cong, Zhifeng Bao, Cheng Long</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce a novel framework called Toast for learning
general-purpose representations of road networks, along with its advanced
counterpart DyToast, designed to enhance the integration of temporal dynamics
to boost the performance of various time-sensitive downstream tasks.
Specifically, we propose to encode two pivotal semantic characteristics
intrinsic to road networks: traffic patterns and traveling semantics. To
achieve this, we refine the skip-gram module by incorporating auxiliary
objectives aimed at predicting the traffic context associated with a target
road segment. Moreover, we leverage trajectory data and design pre-training
strategies based on Transformer to distill traveling semantics on road
networks. DyToast further augments this framework by employing unified
trigonometric functions characterized by their beneficial properties, enabling
the capture of temporal evolution and dynamic nature of road networks more
effectively. With these proposed techniques, we can obtain representations that
encode multi-faceted aspects of knowledge within road networks, applicable
across both road segment-based applications and trajectory-based applications.
Extensive experiments on two real-world datasets across three tasks demonstrate
that our proposed framework consistently outperforms the state-of-the-art
baselines by a significant margin.</div><div><a href='http://arxiv.org/abs/2403.11495v1'>2403.11495v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16915v1")'>More Than Routing: Joint GPS and Route Modeling for Refine Trajectory
  Representation Learning</div>
<div id='2402.16915v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T18:27:25Z</div><div>Authors: Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou, Yilun Chen, Yu Zheng, Jiangtao Gong</div><div style='padding-top: 10px; width: 80ex'>Trajectory representation learning plays a pivotal role in supporting various
downstream tasks. Traditional methods in order to filter the noise in GPS
trajectories tend to focus on routing-based methods used to simplify the
trajectories. However, this approach ignores the motion details contained in
the GPS data, limiting the representation capability of trajectory
representation learning. To fill this gap, we propose a novel representation
learning framework that Joint GPS and Route Modelling based on self-supervised
technology, namely JGRM. We consider GPS trajectory and route as the two modes
of a single movement observation and fuse information through inter-modal
information interaction. Specifically, we develop two encoders, each tailored
to capture representations of route and GPS trajectories respectively. The
representations from the two modalities are fed into a shared transformer for
inter-modal information interaction. Eventually, we design three
self-supervised tasks to train the model. We validate the effectiveness of the
proposed method on two real datasets based on extensive experiments. The
experimental results demonstrate that JGRM outperforms existing methods in both
road segment representation and trajectory representation tasks. Our source
code is available at Anonymous Github.</div><div><a href='http://arxiv.org/abs/2402.16915v1'>2402.16915v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07232v2")'>GTM: General Trajectory Modeling with Auto-regressive Generation of
  Feature Domains</div>
<div id='2402.07232v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T15:49:50Z</div><div>Authors: Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen, Youfang Lin, Huaiyu Wan</div><div style='padding-top: 10px; width: 80ex'>Vehicle movement is frequently captured in the form of trajectories, i.e.,
sequences of timestamped locations. Numerous methods exist that target
different tasks involving trajectories such as travel-time estimation,
trajectory recovery, and trajectory prediction. However, most methods target
only one specific task and cannot be generalized to other tasks. Moreover,
existing methods often perform poorly on long trajectories, while also
underperforming on re-sampled, sparse trajectories.
  To address these shortcomings, we propose the General Trajectory Model (GTM)
that aims to support different tasks based on regular and sparse trajectories
without the need for retraining or extra prediction modules. GTM is designed
expressly to achieve adaptability and robustness. First, GTM separates the
features in trajectories into three distinct domains, such that each domain can
be masked and generated independently to meet specific input and output
requirements of a given task. Second, GTM is pre-trained by reconstructing
densely sampled trajectories given re-sampled sparse counterparts. This process
enables GTM to extract detailed spatio-temporal and road segment information
from sparse trajectories, ensuring consistent performance when trajectories are
sparse. Experiments involving three representative trajectory-related tasks on
two real-world trajectory datasets provide insight into the intended properties
performance of GTM and offer evidence that GTM is capable of meeting its
objectives.</div><div><a href='http://arxiv.org/abs/2402.07232v2'>2402.07232v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14151v1")'>Deep Learning for Trajectory Data Management and Mining: A Survey and
  Beyond</div>
<div id='2403.14151v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T05:57:27Z</div><div>Authors: Wei Chen, Yuxuan Liang, Yuanshao Zhu, Yanchuan Chang, Kang Luo, Haomin Wen, Lei Li, Yanwei Yu, Qingsong Wen, Chao Chen, Kai Zheng, Yunjun Gao, Xiaofang Zhou, Yu Zheng</div><div style='padding-top: 10px; width: 80ex'>Trajectory computing is a pivotal domain encompassing trajectory data
management and mining, garnering widespread attention due to its crucial role
in various practical applications such as location services, urban traffic, and
public safety. Traditional methods, focusing on simplistic spatio-temporal
features, face challenges of complex calculations, limited scalability, and
inadequate adaptability to real-world complexities. In this paper, we present a
comprehensive review of the development and recent advances in deep learning
for trajectory computing (DL4Traj). We first define trajectory data and provide
a brief overview of widely-used deep learning models. Systematically, we
explore deep learning applications in trajectory management (pre-processing,
storage, analysis, and visualization) and mining (trajectory-related
forecasting, trajectory-related recommendation, trajectory classification,
travel time estimation, anomaly detection, and mobility generation). Notably,
we encapsulate recent advancements in Large Language Models (LLMs) that hold
the potential to augment trajectory computing. Additionally, we summarize
application scenarios, public datasets, and toolkits. Finally, we outline
current challenges in DL4Traj research and propose future directions. Relevant
papers and open-source resources have been collated and are continuously
updated at:
\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.</div><div><a href='http://arxiv.org/abs/2403.14151v1'>2403.14151v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00732v1")'>MobilityDL: A Review of Deep Learning From Trajectory Data</div>
<div id='2402.00732v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:30:00Z</div><div>Authors: Anita Graser, Anahid Jalali, Jasmin Lampert, Axel Weißenfeld, Krzysztof Janowicz</div><div style='padding-top: 10px; width: 80ex'>Trajectory data combines the complexities of time series, spatial data, and
(sometimes irrational) movement behavior. As data availability and computing
power have increased, so has the popularity of deep learning from trajectory
data. This review paper provides the first comprehensive overview of deep
learning approaches for trajectory data. We have identified eight specific
mobility use cases which we analyze with regards to the deep learning models
and the training data used. Besides a comprehensive quantitative review of the
literature since 2018, the main contribution of our work is the data-centric
analysis of recent work in this field, placing it along the mobility data
continuum which ranges from detailed dense trajectories of individual movers
(quasi-continuous tracking data), to sparse trajectories (such as check-in
data), and aggregated trajectories (crowd information).</div><div><a href='http://arxiv.org/abs/2402.00732v1'>2402.00732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16310v1")'>REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility
  for Location Prediction over Sparse Trajectories</div>
<div id='2402.16310v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T05:28:36Z</div><div>Authors: Bangchao Deng, Bingqing Qu, Pengyang Wang, Dingqi Yang</div><div style='padding-top: 10px; width: 80ex'>Location prediction forecasts a user's location based on historical user
mobility traces. To tackle the intrinsic sparsity issue of real-world user
mobility traces, spatiotemporal contexts have been shown as significantly
useful. Existing solutions mostly incorporate spatiotemporal distances between
locations in mobility traces, either by feeding them as additional inputs to
Recurrent Neural Networks (RNNs) or by using them to search for informative
past hidden states for prediction. However, such distance-based methods fail to
capture the time-varying temporal regularities of human mobility, where human
mobility is often more regular in the morning than in other periods, for
example; this suggests the usefulness of the actual timestamps besides the
temporal distances. Against this background, we propose REPLAY, a general RNN
architecture learning to capture the time-varying temporal regularities for
location prediction. Specifically, REPLAY not only resorts to the
spatiotemporal distances in sparse trajectories to search for the informative
past hidden states, but also accommodates the time-varying temporal
regularities by incorporating smoothed timestamp embeddings using Gaussian
weighted averaging with timestamp-specific learnable bandwidths, which can
flexibly adapt to the temporal regularities of different strengths across
different timestamps. Our extensive evaluation compares REPLAY against a
sizable collection of state-of-the-art techniques on two real-world datasets.
Results show that REPLAY consistently and significantly outperforms
state-of-the-art methods by 7.7\%-10.9\% in the location prediction task, and
the bandwidths reveal interesting patterns of the time-varying temporal
regularities.</div><div><a href='http://arxiv.org/abs/2402.16310v1'>2402.16310v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12830v1")'>Enhancing Next Destination Prediction: A Novel LSTM Approach Using
  Real-World Airline Data</div>
<div id='2401.12830v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T15:07:49Z</div><div>Authors: Salih Salihoglu, Gulser Koksal, Orhan Abar</div><div style='padding-top: 10px; width: 80ex'>In the modern transportation industry, accurate prediction of travelers' next
destinations brings multiple benefits to companies, such as customer
satisfaction and targeted marketing. This study focuses on developing a precise
model that captures the sequential patterns and dependencies in travel data,
enabling accurate predictions of individual travelers' future destinations. To
achieve this, a novel model architecture with a sliding window approach based
on Long Short-Term Memory (LSTM) is proposed for destination prediction in the
transportation industry. The experimental results highlight satisfactory
performance and high scores achieved by the proposed model across different
data sizes and performance metrics. This research contributes to advancing
destination prediction methods, empowering companies to deliver personalized
recommendations and optimize customer experiences in the dynamic travel
landscape.</div><div><a href='http://arxiv.org/abs/2401.12830v1'>2401.12830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00284v1")'>A Survey of Route Recommendations: Methods, Applications, and
  Opportunities</div>
<div id='2403.00284v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T05:04:00Z</div><div>Authors: Shiming Zhang, Zhipeng Luo, Li Yang, Fei Teng, Tianrui Li</div><div style='padding-top: 10px; width: 80ex'>Nowadays, with advanced information technologies deployed citywide, large
data volumes and powerful computational resources are intelligentizing modern
city development. As an important part of intelligent transportation, route
recommendation and its applications are widely used, directly influencing
citizens` travel habits. Developing smart and efficient travel routes based on
big data (possibly multi-modal) has become a central challenge in route
recommendation research. Our survey offers a comprehensive review of route
recommendation work based on urban computing. It is organized by the following
three parts: 1) Methodology-wise. We categorize a large volume of traditional
machine learning and modern deep learning methods. Also, we discuss their
historical relations and reveal the edge-cutting progress. 2)
Application\-wise. We present numerous novel applications related to route
commendation within urban computing scenarios. 3) We discuss current problems
and challenges and envision several promising research directions. We believe
that this survey can help relevant researchers quickly familiarize themselves
with the current state of route recommendation research and then direct them to
future research trends.</div><div><a href='http://arxiv.org/abs/2403.00284v1'>2403.00284v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07369v1")'>Diff-RNTraj: A Structure-aware Diffusion Model for Road
  Network-constrained Trajectory Generation</div>
<div id='2402.07369v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T01:59:51Z</div><div>Authors: Tonglong Wei, Youfang Lin, Shengnan Guo, Yan Lin, Yiheng Huang, Chenyang Xiang, Yuqing Bai, Menglu Ya, Huaiyu Wan</div><div style='padding-top: 10px; width: 80ex'>Trajectory data is essential for various applications as it records the
movement of vehicles. However, publicly available trajectory datasets remain
limited in scale due to privacy concerns, which hinders the development of
trajectory data mining and trajectory-based applications. To address this
issue, some methods for generating synthetic trajectories have been proposed to
expand the scale of the dataset. However, all existing methods generate
trajectories in the geographical coordinate system, which poses two limitations
for their utilization in practical applications: 1) the inability to ensure
that the generated trajectories are constrained on the road. 2) the lack of
road-related information. In this paper, we propose a new problem to meet the
practical application need, \emph{i.e.}, road network-constrained trajectory
(RNTraj) generation, which can directly generate trajectories on the road
network with road-related information. RNTraj is a hybrid type of data, in
which each point is represented by a discrete road segment and a continuous
moving rate. To generate RNTraj, we design a diffusion model called
Diff-RNTraj. This model can effectively handle the hybrid RNTraj using a
continuous diffusion framework by incorporating a pre-training strategy to
embed hybrid RNTraj into continuous representations. During the sampling stage,
a RNTraj decoder is designed to map the continuous representation generated by
the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj
introduces a novel loss function to enhance the spatial validity of the
generated trajectories. Extensive experiments conducted on two real-world
trajectory datasets demonstrate the effectiveness of the proposed model.</div><div><a href='http://arxiv.org/abs/2402.07369v1'>2402.07369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03264v1")'>MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</div>
<div id='2402.03264v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:22:21Z</div><div>Authors: Ammar Haydari, Dongjie Chen, Zhengfeng Lai, Chen-Nee Chuah</div><div style='padding-top: 10px; width: 80ex'>Generative models have shown promising results in capturing human mobility
characteristics and generating synthetic trajectories. However, it remains
challenging to ensure that the generated geospatial mobility data is
semantically realistic, including consistent location sequences, and reflects
real-world characteristics, such as constraining on geospatial limits. To
address these issues, we reformat human mobility modeling as an autoregressive
generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure
its controllable generation to alleviate the above challenges, we propose a
geospatially-aware generative model, MobilityGPT. We propose a gravity-based
sampling method to train a transformer for semantic sequence similarity. Then,
we constrained the training process via a road connectivity matrix that
provides the connectivity of sequences in trajectory generation, thereby
keeping generated trajectories in geospatial limits. Lastly, we constructed a
Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel
distance between training and the synthetically generated trajectories. Our
experiments on real-world datasets demonstrate that MobilityGPT outperforms
state-of-the-art methods in generating high-quality mobility trajectories that
are closest to real data in terms of origin-destination similarity, trip
length, travel radius, link, and gravity distributions.</div><div><a href='http://arxiv.org/abs/2402.03264v1'>2402.03264v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03750v1")'>Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning
  Approach</div>
<div id='2402.03750v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:37:43Z</div><div>Authors: Xin Chen, Mingliang Hou, Tao Tang, Achhardeep Kaur, Feng Xia</div><div style='padding-top: 10px; width: 80ex'>With the arrival of the big data era, mobility profiling has become a viable
method of utilizing enormous amounts of mobility data to create an intelligent
transportation system. Mobility profiling can extract potential patterns in
urban traffic from mobility data and is critical for a variety of
traffic-related applications. However, due to the high level of complexity and
the huge amount of data, mobility profiling faces huge challenges. Digital Twin
(DT) technology paves the way for cost-effective and performance-optimised
management by digitally creating a virtual representation of the network to
simulate its behaviour. In order to capture the complex spatio-temporal
features in traffic scenario, we construct alignment diagrams to assist in
completing the spatio-temporal correlation representation and design dilated
alignment convolution network (DACN) to learn the fine-grained correlations,
i.e., spatio-temporal interactions. We propose a digital twin mobility
profiling (DTMP) framework to learn node profiles on a mobility network DT
model. Extensive experiments have been conducted upon three real-world
datasets. Experimental results demonstrate the effectiveness of DTMP.</div><div><a href='http://arxiv.org/abs/2402.03750v1'>2402.03750v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11966v1")'>Informed Spectral Normalized Gaussian Processes for Trajectory
  Prediction</div>
<div id='2403.11966v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:05:24Z</div><div>Authors: Christian Schlauch, Christian Wirth, Nadja Klein</div><div style='padding-top: 10px; width: 80ex'>Prior parameter distributions provide an elegant way to represent prior
expert and world knowledge for informed learning. Previous work has shown that
using such informative priors to regularize probabilistic deep learning (DL)
models increases their performance and data-efficiency. However, commonly used
sampling-based approximations for probabilistic DL models can be
computationally expensive, requiring multiple inference passes and longer
training times. Promising alternatives are compute-efficient last layer kernel
approximations like spectral normalized Gaussian processes (SNGPs). We propose
a novel regularization-based continual learning method for SNGPs, which enables
the use of informative priors that represent prior knowledge learned from
previous tasks. Our proposal builds upon well-established methods and requires
no rehearsal memory or parameter expansion. We apply our informed SNGP model to
the trajectory prediction problem in autonomous driving by integrating prior
drivability knowledge. On two public datasets, we investigate its performance
under diminishing training data and across locations, and thereby demonstrate
an increase in data-efficiency and robustness to location-transfers over
non-informed and informed baselines.</div><div><a href='http://arxiv.org/abs/2403.11966v1'>2403.11966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08698v1")'>AMEND: A Mixture of Experts Framework for Long-tailed Trajectory
  Prediction</div>
<div id='2402.08698v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T02:43:41Z</div><div>Authors: Ray Coden Mercurius, Ehsan Ahmadi, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli</div><div style='padding-top: 10px; width: 80ex'>Accurate prediction of pedestrians' future motions is critical for
intelligent driving systems. Developing models for this task requires rich
datasets containing diverse sets of samples. However, the existing naturalistic
trajectory prediction datasets are generally imbalanced in favor of simpler
samples and lack challenging scenarios. Such a long-tail effect causes
prediction models to underperform on the tail portion of the data distribution
containing safety-critical scenarios. Previous methods tackle the long-tail
problem using methods such as contrastive learning and class-conditioned
hypernetworks. These approaches, however, are not modular and cannot be applied
to many machine learning architectures. In this work, we propose a modular
model-agnostic framework for trajectory prediction that leverages a specialized
mixture of experts. In our approach, each expert is trained with a specialized
skill with respect to a particular part of the data. To produce predictions, we
utilise a router network that selects the best expert by generating relative
confidence scores. We conduct experimentation on common pedestrian trajectory
prediction datasets and show that besides achieving state-of-the-art
performance, our method significantly performs better on long-tail scenarios.
We further conduct ablation studies to highlight the contribution of different
proposed components.</div><div><a href='http://arxiv.org/abs/2402.08698v1'>2402.08698v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11643v1")'>Diffusion-Based Environment-Aware Trajectory Prediction</div>
<div id='2403.11643v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T10:35:15Z</div><div>Authors: Theodor Westny, Björn Olofsson, Erik Frisk</div><div style='padding-top: 10px; width: 80ex'>The ability to predict the future trajectories of traffic participants is
crucial for the safe and efficient operation of autonomous vehicles. In this
paper, a diffusion-based generative model for multi-agent trajectory prediction
is proposed. The model is capable of capturing the complex interactions between
traffic participants and the environment, accurately learning the multimodal
nature of the data. The effectiveness of the approach is assessed on
large-scale datasets of real-world traffic scenarios, showing that our model
outperforms several well-established methods in terms of prediction accuracy.
By the incorporation of differential motion constraints on the model output, we
illustrate that our model is capable of generating a diverse set of realistic
future trajectories. Through the use of an interaction-aware guidance signal,
we further demonstrate that the model can be adapted to predict the behavior of
less cooperative agents, emphasizing its practical applicability under
uncertain traffic conditions.</div><div><a href='http://arxiv.org/abs/2403.11643v1'>2403.11643v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15695v1")'>HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable
  In-The-Wild Navigation</div>
<div id='2401.15695v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T16:44:17Z</div><div>Authors: David Bethge, Daniel Bulanda, Adam Kozlowski, Thomas Kosch, Albrecht Schmidt, Tobias Grosse-Puppendahl</div><div style='padding-top: 10px; width: 80ex'>Routes represent an integral part of triggering emotions in drivers.
Navigation systems allow users to choose a navigation strategy, such as the
fastest or shortest route. However, they do not consider the driver's emotional
well-being. We present HappyRouting, a novel navigation-based empathic car
interface guiding drivers through real-world traffic while evoking positive
emotions. We propose design considerations, derive a technical architecture,
and implement a routing optimization framework. Our contribution is a machine
learning-based generated emotion map layer, predicting emotions along routes
based on static and dynamic contextual data. We evaluated HappyRouting in a
real-world driving study (N=13), finding that happy routes increase
subjectively perceived valence by 11% (p=.007). Although happy routes take 1.25
times longer on average, participants perceived the happy route as shorter,
presenting an emotion-enhanced alternative to today's fastest routing
mechanisms. We discuss how emotion-based routing can be integrated into
navigation apps, promoting emotional well-being for mobility use.</div><div><a href='http://arxiv.org/abs/2401.15695v1'>2401.15695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07139v1")'>Towards Robust Car Following Dynamics Modeling via Blackbox Models:
  Methodology, Analysis, and Recommendations</div>
<div id='2402.07139v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T09:46:15Z</div><div>Authors: Muhammad Bilal Shahid, Cody Fleming</div><div style='padding-top: 10px; width: 80ex'>The selection of the target variable is important while learning parameters
of the classical car following models like GIPPS, IDM, etc. There is a vast
body of literature on which target variable is optimal for classical car
following models, but there is no study that empirically evaluates the
selection of optimal target variables for black-box models, such as LSTM, etc.
The black-box models, like LSTM and Gaussian Process (GP) are increasingly
being used to model car following behavior without wise selection of target
variables. The current work tests different target variables, like
acceleration, velocity, and headway, for three black-box models, i.e., GP,
LSTM, and Kernel Ridge Regression. These models have different objective
functions and work in different vector spaces, e.g., GP works in function
space, and LSTM works in parameter space. The experiments show that the optimal
target variable recommendations for black-box models differ from classical car
following models depending on the objective function and the vector space. It
is worth mentioning that models and datasets used during evaluation are diverse
in nature: the datasets contained both automated and human-driven vehicle
trajectories; the black-box models belong to both parametric and non-parametric
classes of models. This diversity is important during the analysis of variance,
wherein we try to find the interaction between datasets, models, and target
variables. It is shown that the models and target variables interact and
recommended target variables don't depend on the dataset under consideration.</div><div><a href='http://arxiv.org/abs/2402.07139v1'>2402.07139v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06268v1")'>Physics-Guided Abnormal Trajectory Gap Detection</div>
<div id='2403.06268v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T17:07:28Z</div><div>Authors: Arun Sharma, Shashi Shekhar</div><div style='padding-top: 10px; width: 80ex'>Given trajectories with gaps (i.e., missing data), we investigate algorithms
to identify abnormal gaps in trajectories which occur when a given moving
object did not report its location, but other moving objects in the same
geographic region periodically did. The problem is important due to its
societal applications, such as improving maritime safety and regulatory
enforcement for global security concerns such as illegal fishing, illegal oil
transfers, and trans-shipments. The problem is challenging due to the
difficulty of bounding the possible locations of the moving object during a
trajectory gap, and the very high computational cost of detecting gaps in such
a large volume of location data. The current literature on anomalous trajectory
detection assumes linear interpolation within gaps, which may not be able to
detect abnormal gaps since objects within a given region may have traveled away
from their shortest path. In preliminary work, we introduced an abnormal gap
measure that uses a classical space-time prism model to bound an object's
possible movement during the trajectory gap and provided a scalable memoized
gap detection algorithm (Memo-AGD). In this paper, we propose a Space
Time-Aware Gap Detection (STAGD) approach to leverage space-time indexing and
merging of trajectory gaps. We also incorporate a Dynamic Region Merge-based
(DRM) approach to efficiently compute gap abnormality scores. We provide
theoretical proofs that both algorithms are correct and complete and also
provide analysis of asymptotic time complexity. Experimental results on
synthetic and real-world maritime trajectory data show that the proposed
approach substantially improves computation time over the baseline technique.</div><div><a href='http://arxiv.org/abs/2403.06268v1'>2403.06268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05778v1")'>Spatial Clustering Approach for Vessel Path Identification</div>
<div id='2403.05778v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T03:21:18Z</div><div>Authors: Mohamed Abuella, M. Amine Atoui, Slawomir Nowaczyk, Simon Johansson, Ethan Faghan</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the challenge of identifying the paths for vessels with
operating routes of repetitive paths, partially repetitive paths, and new
paths. We propose a spatial clustering approach for labeling the vessel paths
by using only position information. We develop a path clustering framework
employing two methods: a distance-based path modeling and a likelihood
estimation method. The former enhances the accuracy of path clustering through
the integration of unsupervised machine learning techniques, while the latter
focuses on likelihood-based path modeling and introduces segmentation for a
more detailed analysis. The result findings highlight the superior performance
and efficiency of the developed approach, as both methods for clustering vessel
paths into five classes achieve a perfect F1-score. The approach aims to offer
valuable insights for route planning, ultimately contributing to improving
safety and efficiency in maritime transportation.</div><div><a href='http://arxiv.org/abs/2403.05778v1'>2403.05778v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11838v1")'>UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal
  Prediction</div>
<div id='2402.11838v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T05:04:11Z</div><div>Authors: Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li</div><div style='padding-top: 10px; width: 80ex'>Urban spatio-temporal prediction is crucial for informed decision-making,
such as transportation management, resource optimization, and urban planning.
Although pretrained foundation models for natural languages have experienced
remarkable breakthroughs, wherein one general-purpose model can tackle multiple
tasks across various domains, urban spatio-temporal modeling lags behind.
Existing approaches for urban prediction are usually tailored for specific
spatio-temporal scenarios, requiring task-specific model designs and extensive
in-domain training data. In this work, we propose a universal model, UniST, for
urban spatio-temporal prediction. Drawing inspiration from large language
models, UniST achieves success through: (i) flexibility towards diverse
spatio-temporal data characteristics, (ii) effective generative pre-training
with elaborated masking strategies to capture complex spatio-temporal
relationships, (iii) spatio-temporal knowledge-guided prompts that align and
leverage intrinsic and shared knowledge across scenarios. These designs
together unlock the potential of a one-for-all model for spatio-temporal
prediction with powerful generalization capability. Extensive experiments on 15
cities and 6 domains demonstrate the universality of UniST in advancing
state-of-the-art prediction performance, especially in few-shot and zero-shot
scenarios.</div><div><a href='http://arxiv.org/abs/2402.11838v1'>2402.11838v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14192v1")'>How Can Large Language Models Understand Spatial-Temporal Data?</div>
<div id='2401.14192v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T14:03:15Z</div><div>Authors: Lei Liu, Shuo Yu, Runze Wang, Zhenxun Ma, Yanming Shen</div><div style='padding-top: 10px; width: 80ex'>While Large Language Models (LLMs) dominate tasks like natural language
processing and computer vision, harnessing their power for spatial-temporal
forecasting remains challenging. The disparity between sequential text and
complex spatial-temporal data hinders this application. To address this issue,
this paper introduces STG-LLM, an innovative approach empowering LLMs for
spatial-temporal forecasting. We tackle the data mismatch by proposing: 1)
STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph
data into concise tokens capturing both spatial and temporal relationships; 2)
STG-Adapter: This minimalistic adapter, consisting of linear encoding and
decoding layers, bridges the gap between tokenized data and LLM comprehension.
By fine-tuning only a small set of parameters, it can effectively grasp the
semantics of tokens generated by STG-Tokenizer, while preserving the original
natural language understanding capabilities of LLMs. Extensive experiments on
diverse spatial-temporal benchmark datasets show that STG-LLM successfully
unlocks LLM potential for spatial-temporal forecasting. Remarkably, our
approach achieves competitive performance on par with dedicated SOTA methods.</div><div><a href='http://arxiv.org/abs/2401.14192v1'>2401.14192v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06204v1")'>An Exploratory Assessment of LLM's Potential Toward Flight Trajectory
  Reconstruction Analysis</div>
<div id='2401.06204v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T17:59:18Z</div><div>Authors: Qilei Zhang, John H. Mott</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) hold transformative potential in aviation,
particularly in reconstructing flight trajectories. This paper investigates
this potential, grounded in the notion that LLMs excel at processing sequential
data and deciphering complex data structures. Utilizing the LLaMA 2 model, a
pre-trained open-source LLM, the study focuses on reconstructing flight
trajectories using Automatic Dependent Surveillance-Broadcast (ADS-B) data with
irregularities inherent in real-world scenarios. The findings demonstrate the
model's proficiency in filtering noise and estimating both linear and curved
flight trajectories. However, the analysis also reveals challenges in managing
longer data sequences, which may be attributed to the token length limitations
of LLM models. The study's insights underscore the promise of LLMs in flight
trajectory reconstruction and open new avenues for their broader application
across the aviation and transportation sectors.</div><div><a href='http://arxiv.org/abs/2401.06204v1'>2401.06204v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03372v1")'>TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for
  Terminal Airspace Operations</div>
<div id='2403.03372v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T23:37:43Z</div><div>Authors: Jay Patrikar, Joao Dantas, Brady Moon, Milad Hamidi, Sourish Ghosh, Nikhil Keetha, Ian Higgins, Atharva Chandak, Takashi Yoneyama, Sebastian Scherer</div><div style='padding-top: 10px; width: 80ex'>We introduce TartanAviation, an open-source multi-modal dataset focused on
terminal-area airspace operations. TartanAviation provides a holistic view of
the airport environment by concurrently collecting image, speech, and ADS-B
trajectory data using setups installed inside airport boundaries. The datasets
were collected at both towered and non-towered airfields across multiple months
to capture diversity in aircraft operations, seasons, aircraft types, and
weather conditions. In total, TartanAviation provides 3.1M images, 3374 hours
of Air Traffic Control speech data, and 661 days of ADS-B trajectory data. The
data was filtered, processed, and validated to create a curated dataset. In
addition to the dataset, we also open-source the code-base used to collect and
pre-process the dataset, further enhancing accessibility and usability. We
believe this dataset has many potential use cases and would be particularly
vital in allowing AI and machine learning technologies to be integrated into
air traffic control systems and advance the adoption of autonomous aircraft in
the airspace.</div><div><a href='http://arxiv.org/abs/2403.03372v1'>2403.03372v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14136v1")'>GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal
  Sensors</div>
<div id='2402.14136v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T21:24:57Z</div><div>Authors: Ho Lyun Jeong, Ziqi Wang, Colin Samplawski, Jason Wu, Shiwei Fang, Lance M. Kaplan, Deepak Ganesan, Benjamin Marlin, Mani Srivastava</div><div style='padding-top: 10px; width: 80ex'>Constantly locating moving objects, i.e., geospatial tracking, is essential
for autonomous building infrastructure. Accurate and robust geospatial tracking
often leverages multimodal sensor fusion algorithms, which require large
datasets with time-aligned, synchronized data from various sensor types.
However, such datasets are not readily available. Hence, we propose GDTM, a
nine-hour dataset for multimodal object tracking with distributed multimodal
sensors and reconfigurable sensor node placements. Our dataset enables the
exploration of several research problems, such as optimizing architectures for
processing multimodal data, and investigating models' robustness to adverse
sensing conditions and sensor placement variances. A GitHub repository
containing the code, sample data, and checkpoints of this work is available at
https://github.com/nesl/GDTM.</div><div><a href='http://arxiv.org/abs/2402.14136v1'>2402.14136v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12756v1")'>Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi
  Fingerprinting: A Discussion from a Data Perspective</div>
<div id='2402.12756v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T06:49:43Z</div><div>Authors: Zhe Tang, Ruocheng Gu, Sihao Li, Kyeong Soo Kim, Jeremy S. Smith</div><div style='padding-top: 10px; width: 80ex'>Wi-Fi fingerprinting has emerged as the most popular approach to indoor
localization. The use of ML algorithms has greatly improved the localization
performance of Wi-Fi fingerprinting, but its success depends on the
availability of fingerprint databases composed of a large number of RSSIs, the
MAC addresses of access points, and the other measurement information. However,
most fingerprint databases do not reflect well the time varying nature of
electromagnetic interferences in complicated modern indoor environment. This
could result in significant changes in statistical characteristics of
training/validation and testing datasets, which are often constructed at
different times, and even the characteristics of the testing datasets could be
different from those of the data submitted by users during the operation of
localization systems after their deployment. In this paper, we consider the
implications of time-varying Wi-Fi fingerprints on indoor localization from a
data-centric point of view and discuss the differences between static and
dynamic databases. As a case study, we have constructed a dynamic database
covering three floors of the IR building of XJTLU based on RSSI measurements,
over 44 days, and investigated the differences between static and dynamic
databases in terms of statistical characteristics and localization performance.
The analyses based on variance calculations and Isolation Forest show the
temporal shifts in RSSIs, which result in a noticeable trend of the increase in
the localization error of a Gaussian process regression model with the maximum
error of 6.65 m after 14 days of training without model adjustments. The
results of the case study with the XJTLU dynamic database clearly demonstrate
the limitations of static databases and the importance of the creation and
adoption of dynamic databases for future indoor localization research and
real-world deployment.</div><div><a href='http://arxiv.org/abs/2402.12756v1'>2402.12756v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11433v1")'>Improved Indoor Localization with Machine Learning Techniques for IoT
  applications</div>
<div id='2402.11433v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T02:55:19Z</div><div>Authors: M. W. P. Maduranga</div><div style='padding-top: 10px; width: 80ex'>The rise of the Internet of Things (IoT) and mobile internet applications has
spurred interest in location-based services (LBS) for commercial, military, and
social applications. While the global positioning system (GPS) dominates
outdoor localization, its efficacy wanes indoors due to signal challenges.
Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee,
Bluetooth, UWB, selecting based on context. Received signal strength indicator
(RSSI) technology, known for its accuracy and simplicity, is widely adopted.
This study employs machine learning algorithms in three phases: supervised
regressors, supervised classifiers, and ensemble methods for RSSI-based indoor
localization. Additionally, it introduces a weighted least squares technique
and pseudo-linear solution approach to address non-linear RSSI measurement
equations by approximating them with linear equations. An experimental testbed,
utilizing diverse wireless technologies and anchor nodes, is designed for data
collection, employing IoT cloud architectures. Pre-processing involves
investigating filters for data refinement before algorithm training. The study
employs machine learning models like linear regression, polynomial regression,
support vector regression, random forest regression, and decision tree
regressor across various wireless technologies. These models estimate the
geographical coordinates of a moving target node, and their performance is
evaluated using metrics such as accuracy, root mean square errors, precision,
recall, sensitivity, coefficient of determinant, and the f1-score. The
experiment's outcomes provide insights into the effectiveness of different
supervised machine learning techniques in terms of localization accuracy and
robustness in indoor environments.</div><div><a href='http://arxiv.org/abs/2402.11433v1'>2402.11433v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10077v1")'>Towards a large-scale fused and labeled dataset of human pose while
  interacting with robots in shared urban areas</div>
<div id='2402.10077v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T18:24:34Z</div><div>Authors: E. Sherafat, B. Farooq</div><div style='padding-top: 10px; width: 80ex'>Over the last decade, Autonomous Delivery Robots (ADRs) have transformed
conventional delivery methods, responding to the growing e-commerce demand.
However, the readiness of ADRs to navigate safely among pedestrians in shared
urban areas remains an open question. We contend that there are crucial
research gaps in understanding their interactions with pedestrians in such
environments. Human Pose Estimation is a vital stepping stone for various
downstream applications, including pose prediction and socially aware robot
path-planning. Yet, the absence of an enriched and pose-labeled dataset
capturing human-robot interactions in shared urban areas hinders this
objective. In this paper, we bridge this gap by repurposing, fusing, and
labeling two datasets, MOT17 and NCLT, focused on pedestrian tracking and
Simultaneous Localization and Mapping (SLAM), respectively. The resulting
unique dataset represents thousands of real-world indoor and outdoor
human-robot interaction scenarios. Leveraging YOLOv7, we obtained human pose
visual and numeric outputs and provided ground truth poses using manual
annotation. To overcome the distance bias present in the traditional MPJPE
metric, this study introduces a novel human pose estimation error metric called
Mean Scaled Joint Error (MSJE) by incorporating bounding box dimensions into
it. Findings demonstrate that YOLOv7 effectively estimates human pose in both
datasets. However, it exhibits weaker performance in specific scenarios, like
indoor, crowded scenes with a focused light source, where both MPJPE and MSJE
are recorded as 10.89 and 25.3, respectively. In contrast, YOLOv7 performs
better in single-person estimation (NCLT seq 2) and outdoor scenarios (MOT17
seq1), achieving MSJE values of 5.29 and 3.38, respectively.</div><div><a href='http://arxiv.org/abs/2402.10077v1'>2402.10077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12320v1")'>Landmark Stereo Dataset for Landmark Recognition and Moving Node
  Localization in a Non-GPS Battlefield Environment</div>
<div id='2402.12320v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T17:49:23Z</div><div>Authors: Ganesh Sapkota, Sanjay Madria</div><div style='padding-top: 10px; width: 80ex'>In this paper, we have proposed a new strategy of using the landmark anchor
node instead of a radio-based anchor node to obtain the virtual coordinates
(landmarkID, DISTANCE) of moving troops or defense forces that will help in
tracking and maneuvering the troops along a safe path within a GPS-denied
battlefield environment. The proposed strategy implements landmark recognition
using the Yolov5 model and landmark distance estimation using an efficient
Stereo Matching Algorithm. We consider that a moving node carrying a low-power
mobile device facilitated with a calibrated stereo vision camera that captures
stereo images of a scene containing landmarks within the battlefield region
whose locations are stored in an offline server residing within the device
itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34
landmark classes and another landmark stereo dataset of those 34 landmark
instances called MSTLandmarkStereov1. We trained the YOLOv5 model with
MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5:
0.95] IoU. We calculated the distance from a node to the landmark utilizing the
bounding box coordinates and the depth map generated by the improved SGM
algorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from
the detection result and the distances calculated by the SGM algorithm are
stored as the virtual coordinates of a node. In future work, we will use these
virtual coordinates to obtain the location of a node using an efficient
trilateration algorithm and optimize the node position using the appropriate
optimization method.</div><div><a href='http://arxiv.org/abs/2402.12320v1'>2402.12320v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14280v1")'>Secure Navigation using Landmark-based Localization in a GPS-denied
  Environment</div>
<div id='2402.14280v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T04:41:56Z</div><div>Authors: Ganesh Sapkota, Sanjay Madria</div><div style='padding-top: 10px; width: 80ex'>In modern battlefield scenarios, the reliance on GPS for navigation can be a
critical vulnerability. Adversaries often employ tactics to deny or deceive GPS
signals, necessitating alternative methods for the localization and navigation
of mobile troops. Range-free localization methods such as DV-HOP rely on
radio-based anchors and their average hop distance which suffers from accuracy
and stability in a dynamic and sparse network topology. Vision-based approaches
like SLAM and Visual Odometry use sensor fusion techniques for map generation
and pose estimation that are more sophisticated and computationally expensive.
This paper proposes a novel framework that integrates landmark-based
localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the
future state of moving entities along the battlefield. Our framework utilizes
safe trajectory information generated by the troop control center by
considering identifiable landmarks and pre-defined hazard maps. It performs
point inclusion tests on the convex hull of the trajectory segments to ensure
the safety and survivability of a moving entity and determines the next point
forward decisions. We present a simulated battlefield scenario for two
different approaches (with EKF and without EKF) that guide a moving entity
through an obstacle and hazard-free path. Using the proposed method, we
observed a percent error of 6.51% lengthwise in safe trajectory estimation with
an Average Displacement Error (ADE) of 2.97m and a Final Displacement Error
(FDE) of 3.27m. The results demonstrate that our approach not only ensures the
safety of the mobile units by keeping them within the secure trajectory but
also enhances operational effectiveness by adapting to the evolving threat
landscape.</div><div><a href='http://arxiv.org/abs/2402.14280v1'>2402.14280v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01749v1")'>Towards Urban General Intelligence: A Review and Outlook of Urban
  Foundation Models</div>
<div id='2402.01749v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T04:48:16Z</div><div>Authors: Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, Hui Xiong</div><div style='padding-top: 10px; width: 80ex'>Machine learning techniques are now integral to the advancement of
intelligent urban services, playing a crucial role in elevating the efficiency,
sustainability, and livability of urban environments. The recent emergence of
foundation models such as ChatGPT marks a revolutionary shift in the fields of
machine learning and artificial intelligence. Their unparalleled capabilities
in contextual understanding, problem solving, and adaptability across a wide
range of tasks suggest that integrating these models into urban domains could
have a transformative impact on the development of smart cities. Despite
growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces
challenges such as a lack of clear definitions, systematic reviews, and
universalizable solutions. To this end, this paper first introduces the concept
of UFM and discusses the unique challenges involved in building them. We then
propose a data-centric taxonomy that categorizes current UFM-related works,
based on urban data modalities and types. Furthermore, to foster advancement in
this field, we present a promising framework aimed at the prospective
realization of UFMs, designed to overcome the identified challenges.
Additionally, we explore the application landscape of UFMs, detailing their
potential impact in various urban contexts. Relevant papers and open-source
resources have been collated and are continuously updated at
https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.</div><div><a href='http://arxiv.org/abs/2402.01749v1'>2402.01749v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19348v1")'>Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy,
  Advances, and Outlook</div>
<div id='2402.19348v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T16:56:23Z</div><div>Authors: Xingchen Zou, Yibo Yan, Xixuan Hao, Yuehong Hu, Haomin Wen, Erdong Liu, Junbo Zhang, Yong Li, Tianrui Li, Yu Zheng, Yuxuan Liang</div><div style='padding-top: 10px; width: 80ex'>As cities continue to burgeon, Urban Computing emerges as a pivotal
discipline for sustainable development by harnessing the power of cross-domain
data fusion from diverse sources (e.g., geographical, traffic, social media,
and environmental data) and modalities (e.g., spatio-temporal, visual, and
textual modalities). Recently, we are witnessing a rising trend that utilizes
various deep-learning methods to facilitate cross-domain data fusion in smart
cities. To this end, we propose the first survey that systematically reviews
the latest advancements in deep learning-based data fusion methods tailored for
urban computing. Specifically, we first delve into data perspective to
comprehend the role of each modality and data source. Secondly, we classify the
methodology into four primary categories: feature-based, alignment-based,
contrast-based, and generation-based fusion methods. Thirdly, we further
categorize multi-modal urban applications into seven types: urban planning,
transportation, economy, public safety, society, environment, and energy.
Compared with previous surveys, we focus more on the synergy of deep learning
methods with urban computing applications. Furthermore, we shed light on the
interplay between Large Language Models (LLMs) and urban computing, postulating
future research directions that could revolutionize the field. We firmly
believe that the taxonomy, progress, and prospects delineated in our survey
stand poised to significantly enrich the research community. The summary of the
comprehensive and up-to-date paper list can be found at
https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.</div><div><a href='http://arxiv.org/abs/2402.19348v1'>2402.19348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10912v1")'>Automatic location detection based on deep learning</div>
<div id='2403.10912v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T12:25:30Z</div><div>Authors: Anjali Karangiya, Anirudh Sharma, Divax Shah, Kartavya Badgujar, Dr. Chintan Thacker, Dainik Dave</div><div style='padding-top: 10px; width: 80ex'>The proliferation of digital images and the advancements in deep learning
have paved the way for innovative solutions in various domains, especially in
the field of image classification. Our project presents an in-depth study and
implementation of an image classification system specifically tailored to
identify and classify images of Indian cities. Drawing from an extensive
dataset, our model classifies images into five major Indian cities: Ahmedabad,
Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and
characteristics of each city/state. To achieve high precision and recall rates,
we adopted two approaches. The first, a vanilla Convolutional Neural Network
(CNN) and then we explored the power of transfer learning by leveraging the
VGG16 model. The vanilla CNN achieved commendable accuracy and the VGG16 model
achieved a test accuracy of 63.6%. Evaluations highlighted the strengths and
potential areas of improvement, positioning our model as not only competitive
but also scalable for broader applications. With an emphasis on open-source
ethos, our work aims to contribute to the community, encouraging further
development and diverse applications. Our findings demonstrate the potential
applications in tourism, urban planning, and even real-time location
identification systems, among others.</div><div><a href='http://arxiv.org/abs/2403.10912v1'>2403.10912v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15017v1")'>Vehicle Detection Performance in Nordic Region</div>
<div id='2403.15017v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T08:03:10Z</div><div>Authors: Hamam Mokayed, Rajkumar Saini, Oluwatosin Adewumi, Lama Alkhaled, Bjorn Backe, Palaiahnakote Shivakumara, Olle Hagner, Yan Chai Hum</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the critical challenge of vehicle detection in the harsh
winter conditions in the Nordic regions, characterized by heavy snowfall,
reduced visibility, and low lighting. Due to their susceptibility to
environmental distortions and occlusions, traditional vehicle detection methods
have struggled in these adverse conditions. The advanced proposed deep learning
architectures brought promise, yet the unique difficulties of detecting
vehicles in Nordic winters remain inadequately addressed. This study uses the
Nordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to
evaluate the performance of state-of-the-art vehicle detection algorithms under
challenging weather conditions. Our methodology includes a comprehensive
evaluation of single-stage, two-stage, and transformer-based detectors against
the NVD. We propose a series of enhancements tailored to each detection
framework, including data augmentation, hyperparameter tuning, transfer
learning, and novel strategies designed explicitly for the DETR model. Our
findings not only highlight the limitations of current detection systems in the
Nordic environment but also offer promising directions for enhancing these
algorithms for improved robustness and accuracy in vehicle detection amidst the
complexities of winter landscapes. The code and the dataset are available at
https://nvd.ltu-ai.dev</div><div><a href='http://arxiv.org/abs/2403.15017v1'>2403.15017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10643v1")'>A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification:
  Models, Data Sets and Challenges</div>
<div id='2401.10643v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T11:45:10Z</div><div>Authors: Ali Amiri, Aydin Kaya, Ali Seydi Keceli</div><div style='padding-top: 10px; width: 80ex'>Vehicle re-identification (ReID) endeavors to associate vehicle images
collected from a distributed network of cameras spanning diverse traffic
environments. This task assumes paramount importance within the spectrum of
vehicle-centric technologies, playing a pivotal role in deploying Intelligent
Transportation Systems (ITS) and advancing smart city initiatives. Rapid
advancements in deep learning have significantly propelled the evolution of
vehicle ReID technologies in recent years. Consequently, undertaking a
comprehensive survey of methodologies centered on deep learning for vehicle
re-identification has become imperative and inescapable. This paper extensively
explores deep learning techniques applied to vehicle ReID. It outlines the
categorization of these methods, encompassing supervised and unsupervised
approaches, delves into existing research within these categories, introduces
datasets and evaluation criteria, and delineates forthcoming challenges and
potential research directions. This comprehensive assessment examines the
landscape of deep learning in vehicle ReID and establishes a foundation and
starting point for future works. It aims to serve as a complete reference by
highlighting challenges and emerging trends, fostering advancements and
applications in vehicle ReID utilizing deep learning models.</div><div><a href='http://arxiv.org/abs/2401.10643v1'>2401.10643v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07468v1")'>CarSpeedNet: A Deep Neural Network-based Car Speed Estimation from
  Smartphone Accelerometer</div>
<div id='2401.07468v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T04:51:34Z</div><div>Authors: Barak Or</div><div style='padding-top: 10px; width: 80ex'>In this study, a novel deep neural network (DNN) architecture, CarSpeedNet,
is introduced to estimate car speed using three-axis accelerometer data from
smartphones. Utilizing 13 hours of data collected from smartphones mounted in
vehicles navigating through various regions in Israel, the CarSpeedNet
effectively learns the relationship between measured smartphone acceleration
and car speed. Ground truth speed data was obtained at 1[Hz] from the GPS
receiver in the smartphones. The proposed model enables high-frequency speed
estimation, incorporating historical inputs. Our trained model demonstrates
exceptional accuracy in car speed estimation, achieving a precision of less
than 0.72[m/s] during an extended driving test, solely relying on smartphone
accelerometer data without any connectivity to the car.</div><div><a href='http://arxiv.org/abs/2401.07468v1'>2401.07468v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09427v1")'>DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT
  Applications</div>
<div id='2402.09427v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T05:28:29Z</div><div>Authors: Aleksei Zakharchenko, Sharon Farber, Itzik Klein</div><div style='padding-top: 10px; width: 80ex'>Many Internet of Things applications utilize low-cost, micro,
electro-mechanical inertial sensors. A common task is orientation estimation.
To tackle such a task, attitude and heading reference system algorithms are
applied. Relying on the gyroscope readings, the accelerometer readings are used
to update the attitude angles, and magnetometer measurements are utilized to
update the heading angle. In indoor environments, magnetometers suffer from
interference that degrades their performance. This mainly influences
applications focused on estimating the heading angle like finding the heading
angle of a closet or fridge door. To circumvent such situations, we propose
DoorINet, an end-to-end deep-learning framework to calculate the heading angle
from door-mounted, low-cost inertial sensors without using magnetometers. To
evaluate our approach, we record a unique dataset containing 391 minutes of
accelerometer and gyroscope measurements and corresponding ground-truth heading
angle. We show that our proposed approach outperforms commonly used, model
based approaches and data-driven methods.</div><div><a href='http://arxiv.org/abs/2402.09427v1'>2402.09427v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02456v1")'>A comprehensive survey of research towards AI-enabled unmanned aerial
  systems in pre-, active-, and post-wildfire management</div>
<div id='2401.02456v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T05:09:35Z</div><div>Authors: Sayed Pedram Haeri Boroujeni, Abolfazl Razi, Sahand Khoshdel, Fatemeh Afghah, Janice L. Coen, Leo ONeill, Peter Z. Fule, Adam Watts, Nick-Marios T. Kokolakis, Kyriakos G. Vamvoudakis</div><div style='padding-top: 10px; width: 80ex'>Wildfires have emerged as one of the most destructive natural disasters
worldwide, causing catastrophic losses in both human lives and forest wildlife.
Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by
the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models,
has created an unprecedented momentum to implement and develop more effective
wildfire management. Although some of the existing survey papers have explored
various learning-based approaches, a comprehensive review emphasizing the
application of AI-enabled UAV systems and their subsequent impact on
multi-stage wildfire management is notably lacking. This survey aims to bridge
these gaps by offering a systematic review of the recent state-of-the-art
technologies, highlighting the advancements of UAV systems and AI models from
pre-fire, through the active-fire stage, to post-fire management. To this aim,
we provide an extensive analysis of the existing remote sensing systems with a
particular focus on the UAV advancements, device specifications, and sensor
technologies relevant to wildfire management. We also examine the pre-fire and
post-fire management approaches, including fuel monitoring, prevention
strategies, as well as evacuation planning, damage assessment, and operation
strategies. Additionally, we review and summarize a wide range of computer
vision techniques in active-fire management, with an emphasis on Machine
Learning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms
for wildfire classification, segmentation, detection, and monitoring tasks.
Ultimately, we underscore the substantial advancement in wildfire modeling
through the integration of cutting-edge AI techniques and UAV-based data,
providing novel insights and enhanced predictive capabilities to understand
dynamic wildfire behavior.</div><div><a href='http://arxiv.org/abs/2401.02456v1'>2401.02456v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12871v1")'>Wildfire danger prediction optimization with transfer learning</div>
<div id='2403.12871v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T16:15:44Z</div><div>Authors: Spiros Maggioros, Nikos Tsalkitzis</div><div style='padding-top: 10px; width: 80ex'>Convolutional Neural Networks (CNNs) have proven instrumental across various
computer science domains, enabling advancements in object detection,
classification, and anomaly detection. This paper explores the application of
CNNs to analyze geospatial data specifically for identifying wildfire-affected
areas. Leveraging transfer learning techniques, we fine-tuned CNN
hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess
moisture conditions. The study establishes a methodology for computing wildfire
risk levels on a scale of 0 to 5, dynamically linked to weather patterns.
Notably, through the integration of transfer learning, the CNN model achieved
an impressive accuracy of 95\% in identifying burnt areas. This research sheds
light on the inner workings of CNNs and their practical, real-time utility in
predicting and mitigating wildfires. By combining transfer learning and CNNs,
this study contributes a robust approach to assess burnt areas, facilitating
timely interventions and preventative measures against conflagrations.</div><div><a href='http://arxiv.org/abs/2403.12871v1'>2403.12871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13545v1")'>Next day fire prediction via semantic segmentation</div>
<div id='2403.13545v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T12:31:13Z</div><div>Authors: Konstantinos Alexis, Stella Girtsou, Alexis Apostolakis, Giorgos Giannopoulos, Charalampos Kontoes</div><div style='padding-top: 10px; width: 80ex'>In this paper we present a deep learning pipeline for next day fire
prediction. The next day fire prediction task consists in learning models that
receive as input the available information for an area up until a certain day,
in order to predict the occurrence of fire for the next day. Starting from our
previous problem formulation as a binary classification task on instances
(daily snapshots of each area) represented by tabular feature vectors, we
reformulate the problem as a semantic segmentation task on images; there, each
pixel corresponds to a daily snapshot of an area, while its channels represent
the formerly tabular training features. We demonstrate that this problem
formulation, built within a thorough pipeline achieves state of the art
results.</div><div><a href='http://arxiv.org/abs/2403.13545v1'>2403.13545v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06762v1")'>Seeing the roads through the trees: A benchmark for modeling spatial
  dependencies with aerial imagery</div>
<div id='2401.06762v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:50:43Z</div><div>Authors: Caleb Robinson, Isaac Corley, Anthony Ortiz, Rahul Dodhia, Juan M. Lavista Ferres, Peyman Najafirad</div><div style='padding-top: 10px; width: 80ex'>Fully understanding a complex high-resolution satellite or aerial imagery
scene often requires spatial reasoning over a broad relevant context. The human
object recognition system is able to understand object in a scene over a
long-range relevant context. For example, if a human observes an aerial scene
that shows sections of road broken up by tree canopy, then they will be
unlikely to conclude that the road has actually been broken up into disjoint
pieces by trees and instead think that the canopy of nearby trees is occluding
the road. However, there is limited research being conducted to understand
long-range context understanding of modern machine learning models. In this
work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial
Context (RSC), for evaluating the spatial long-range context understanding of
geospatial machine learning models and show how commonly used semantic
segmentation models can fail at this task. For example, we show that a U-Net
trained to segment roads from background in aerial imagery achieves an 84%
recall on unoccluded roads, but just 63.5% recall on roads covered by tree
canopy despite being trained to model both the same way. We further analyze how
the performance of models changes as the relevant context for a decision
(unoccluded roads in our case) varies in distance. We release the code to
reproduce our experiments and dataset of imagery and masks to encourage future
research in this direction -- https://github.com/isaaccorley/ChesapeakeRSC.</div><div><a href='http://arxiv.org/abs/2401.06762v1'>2401.06762v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15223v1")'>Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis</div>
<div id='2401.15223v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T22:21:39Z</div><div>Authors: Mingshi Li, Dusan Grujicic, Steven De Saeger, Stien Heremans, Ben Somers, Matthew B. Blaschko</div><div style='padding-top: 10px; width: 80ex'>In recent years, machine learning has become crucial in remote sensing
analysis, particularly in the domain of Land-use/Land-cover (LULC). The synergy
of machine learning and satellite imagery analysis has demonstrated significant
productivity in this field, as evidenced by several studies. A notable
challenge within this area is the semantic segmentation mapping of land usage
over extensive territories, where the accessibility of accurate land-use data
and the reliability of ground truth land-use labels pose significant
difficulties. For example, providing a detailed and accurate pixel-wise labeled
dataset of the Flanders region, a first-level administrative division of
Belgium, can be particularly insightful. Yet there is a notable lack of
regulated, formalized datasets and workflows for such studies in many regions
globally. This paper introduces a comprehensive approach to addressing these
gaps. We present a densely labeled ground truth map of Flanders paired with
Sentinel-2 satellite imagery. Our methodology includes a formalized dataset
division and sampling method, utilizing the topographic map layout
'Kaartbladversnijdingen,' and a detailed semantic segmentation model training
pipeline. Preliminary benchmarking results are also provided to demonstrate the
efficacy of our approach.</div><div><a href='http://arxiv.org/abs/2401.15223v1'>2401.15223v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04464v2")'>PhilEO Bench: Evaluating Geo-Spatial Foundation Models</div>
<div id='2401.04464v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T09:58:42Z</div><div>Authors: Casper Fibaek, Luke Camilleri, Andreas Luyts, Nikolaos Dionelis, Bertrand Le Saux</div><div style='padding-top: 10px; width: 80ex'>Massive amounts of unlabelled data are captured by Earth Observation (EO)
satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.
This makes Remote Sensing a data-rich domain well suited to Machine Learning
(ML) solutions. However, a bottleneck in applying ML models to EO is the lack
of annotated data as annotation is a labour-intensive and costly process. As a
result, research in this domain has focused on Self-Supervised Learning and
Foundation Model approaches. This paper addresses the need to evaluate
different Foundation Models on a fair and uniform benchmark by introducing the
PhilEO Bench, a novel evaluation framework for EO Foundation Models. The
framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset
containing labels for three downstream tasks, building density estimation, road
segmentation, and land cover classification. We present experiments using our
framework evaluating different Foundation Models, including Prithvi and SatMAE,
at multiple n-shots and convergence rates.</div><div><a href='http://arxiv.org/abs/2401.04464v2'>2401.04464v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15739v1")'>SegmentAnyTree: A sensor and platform agnostic deep learning model for
  tree segmentation using laser scanning data</div>
<div id='2401.15739v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T19:47:17Z</div><div>Authors: Maciej Wielgosz, Stefano Puliti, Binbin Xiang, Konrad Schindler, Rasmus Astrup</div><div style='padding-top: 10px; width: 80ex'>This research advances individual tree crown (ITC) segmentation in lidar
data, using a deep learning model applicable to various laser scanning types:
airborne (ULS), terrestrial (TLS), and mobile (MLS). It addresses the challenge
of transferability across different data characteristics in 3D forest scene
analysis. The study evaluates the model's performance based on platform (ULS,
MLS) and data density, testing five scenarios with varying input data,
including sparse versions, to gauge adaptability and canopy layer efficacy. The
model, based on PointGroup architecture, is a 3D CNN with separate heads for
semantic and instance segmentation, validated on diverse point cloud datasets.
Results show point cloud sparsification enhances performance, aiding sparse
data handling and improving detection in dense forests. The model performs well
with &gt;50 points per sq. m densities but less so at 10 points per sq. m due to
higher omission rates. It outperforms existing methods (e.g., Point2Tree,
TLS2trees) in detection, omission, commission rates, and F1 score, setting new
benchmarks on LAUTx, Wytham Woods, and TreeLearn datasets. In conclusion, this
study shows the feasibility of a sensor-agnostic model for diverse lidar data,
surpassing sensor-specific approaches and setting new standards in tree
segmentation, particularly in complex forests. This contributes to future
ecological modeling and forest management advancements.</div><div><a href='http://arxiv.org/abs/2401.15739v1'>2401.15739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11413v1")'>A Multispectral Automated Transfer Technique (MATT) for machine-driven
  image labeling utilizing the Segment Anything Model (SAM)</div>
<div id='2402.11413v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T01:01:13Z</div><div>Authors: James E. Gallagher, Aryav Gogia, Edward J. Oughton</div><div style='padding-top: 10px; width: 80ex'>Segment Anything Model (SAM) is drastically accelerating the speed and
accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB)
imagery datasets. However, SAM is unable to segment and label images outside of
the visible light spectrum, for example, for multispectral or hyperspectral
imagery. Therefore, this paper outlines a method we call the Multispectral
Automated Transfer Technique (MATT). By transposing SAM segmentation masks from
RGB images we can automatically segment and label multispectral imagery with
high precision and efficiency. For example, the results demonstrate that
segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time
reduction of 87.8% in developing a trained model, reducing roughly 20 hours of
manual labeling, to only 2.4 hours. This efficiency gain is associated with
only a 6.7% decrease in overall mean average precision (mAP) when training
multispectral models via MATT, compared to a manually labeled dataset. We
consider this an acceptable level of precision loss when considering the time
saved during training, especially for rapidly prototyping experimental modeling
methods. This research greatly contributes to the study of multispectral object
detection by providing a novel and open-source method to rapidly segment,
label, and train multispectral object detection models with minimal human
interaction. Future research needs to focus on applying these methods to (i)
space-based multispectral, and (ii) drone-based hyperspectral imagery.</div><div><a href='http://arxiv.org/abs/2402.11413v1'>2402.11413v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09066v1")'>Solid Waste Detection in Remote Sensing Images: A Survey</div>
<div id='2402.09066v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:24:04Z</div><div>Authors: Piero Fraternali, Luca Morandini, Sergio Luis Herrera González</div><div style='padding-top: 10px; width: 80ex'>The detection and characterization of illegal solid waste disposal sites are
essential for environmental protection, particularly for mitigating pollution
and health hazards. Improperly managed landfills contaminate soil and
groundwater via rainwater infiltration, posing threats to both animals and
humans. Traditional landfill identification approaches, such as on-site
inspections, are time-consuming and expensive. Remote sensing is a
cost-effective solution for the identification and monitoring of solid waste
disposal sites that enables broad coverage and repeated acquisitions over time.
Earth Observation (EO) satellites, equipped with an array of sensors and
imaging capabilities, have been providing high-resolution data for several
decades. Researchers proposed specialized techniques that leverage remote
sensing imagery to perform a range of tasks such as waste site detection,
dumping site monitoring, and assessment of suitable locations for new
landfills. This review aims to provide a detailed illustration of the most
relevant proposals for the detection and monitoring of solid waste sites by
describing and comparing the approaches, the implemented techniques, and the
employed data. Furthermore, since the data sources are of the utmost importance
for developing an effective solid waste detection model, a comprehensive
overview of the satellites and publicly available data sets is presented.
Finally, this paper identifies the open issues in the state-of-the-art and
discusses the relevant research directions for reducing the costs and improving
the effectiveness of novel solid waste detection methods.</div><div><a href='http://arxiv.org/abs/2402.09066v1'>2402.09066v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02928v1")'>Instance Segmentation XXL-CT Challenge of a Historic Airplane</div>
<div id='2402.02928v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:47:45Z</div><div>Authors: Roland Gruber, Johann Christopher Engster, Markus Michen, Nele Blum, Maik Stille, Stefan Gerth, Thomas Wittenberg</div><div style='padding-top: 10px; width: 80ex'>Instance segmentation of compound objects in XXL-CT imagery poses a unique
challenge in non-destructive testing. This complexity arises from the lack of
known reference segmentation labels, limited applicable segmentation tools, as
well as partially degraded image quality. To asses recent advancements in the
field of machine learning-based image segmentation, the "Instance Segmentation
XXL-CT Challenge of a Historic Airplane" was conducted. The challenge aimed to
explore automatic or interactive instance segmentation methods for an efficient
delineation of the different aircraft components, such as screws, rivets, metal
sheets or pressure tubes. We report the organization and outcome of this
challenge and describe the capabilities and limitations of the submitted
segmentation methods.</div><div><a href='http://arxiv.org/abs/2402.02928v1'>2402.02928v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00897v1")'>VisRec: A Semi-Supervised Approach to Radio Interferometric Data
  Reconstruction</div>
<div id='2403.00897v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T16:27:33Z</div><div>Authors: Ruoqi Wang, Haitao Wang, Qiong Luo, Feng Wang, Hejun Wu</div><div style='padding-top: 10px; width: 80ex'>Radio telescopes produce visibility data about celestial objects, but these
data are sparse and noisy. As a result, images created on raw visibility data
are of low quality. Recent studies have used deep learning models to
reconstruct visibility data to get cleaner images. However, these methods rely
on a substantial amount of labeled training data, which requires significant
labeling effort from radio astronomers. Addressing this challenge, we propose
VisRec, a model-agnostic semi-supervised learning approach to the
reconstruction of visibility data. Specifically, VisRec consists of both a
supervised learning module and an unsupervised learning module. In the
supervised learning module, we introduce a set of data augmentation functions
to produce diverse training examples. In comparison, the unsupervised learning
module in VisRec augments unlabeled data and uses reconstructions from
non-augmented visibility data as pseudo-labels for training. This hybrid
approach allows VisRec to effectively leverage both labeled and unlabeled data.
This way, VisRec performs well even when labeled data is scarce. Our evaluation
results show that VisRec outperforms all baseline methods in reconstruction
quality, robustness against common observation perturbation, and
generalizability to different telescope configurations.</div><div><a href='http://arxiv.org/abs/2403.00897v1'>2403.00897v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.07931v2")'>Vertical Federated Image Segmentation</div>
<div id='2401.07931v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T19:47:14Z</div><div>Authors: Paul K. Mandal, Cole Leo</div><div style='padding-top: 10px; width: 80ex'>With the popularization of AI solutions for image based problems, there has
been a growing concern for both data privacy and acquisition. In a large number
of cases, information is located on separate data silos and it can be difficult
for a developer to consolidate all of it in a fashion that is appropriate for
machine learning model development. Alongside this, a portion of these
localized data regions may not have access to a labelled ground truth. This
indicates that they have the capacity to reach conclusions numerically, but are
not able to assign classifications amid a lack of pertinent information. Such a
determination is often negligible, especially when attempting to develop image
based solutions that often necessitate this capability. With this being the
case, we propose an innovative vertical federated learning (VFL) model
architecture that can operate under this common set of conditions. This is the
first (and currently the only) implementation of a system that can work under
the constraints of a VFL environment and perform image segmentation while
maintaining nominal accuracies. We achieved this by utilizing an FCN that
boasts the ability to operate on federates that lack labelled data and
privately share the respective weights with a central server, that of which
hosts the necessary features for classification. Tests were conducted on the
CamVid dataset in order to determine the impact of heavy feature compression
required for the transfer of information between federates, as well as to reach
nominal conclusions about the overall performance metrics when working under
such constraints.</div><div><a href='http://arxiv.org/abs/2401.07931v2'>2401.07931v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01969v2")'>Simulation-Enhanced Data Augmentation for Machine Learning Pathloss
  Prediction</div>
<div id='2402.01969v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T00:38:08Z</div><div>Authors: Ahmed P. Mohamed, Byunghyun Lee, Yaguang Zhang, Max Hollingsworth, C. Robert Anderson, James V. Krogmeier, David J. Love</div><div style='padding-top: 10px; width: 80ex'>Machine learning (ML) offers a promising solution to pathloss prediction.
However, its effectiveness can be degraded by the limited availability of data.
To alleviate these challenges, this paper introduces a novel
simulation-enhanced data augmentation method for ML pathloss prediction. Our
method integrates synthetic data generated from a cellular coverage simulator
and independently collected real-world datasets. These datasets were collected
through an extensive measurement campaign in different environments, including
farms, hilly terrains, and residential areas. This comprehensive data
collection provides vital ground truth for model training. A set of channel
features was engineered, including geographical attributes derived from LiDAR
datasets. These features were then used to train our prediction model,
incorporating the highly efficient and robust gradient boosting ML algorithm,
CatBoost. The integration of synthetic data, as demonstrated in our study,
significantly improves the generalizability of the model in different
environments, achieving a remarkable improvement of approximately 12dB in terms
of mean absolute error for the best-case scenario. Moreover, our analysis
reveals that even a small fraction of measurements added to the simulation
training set, with proper data balance, can significantly enhance the model's
performance.</div><div><a href='http://arxiv.org/abs/2402.01969v2'>2402.01969v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00878v1")'>Radio Map Estimation -- An Open Dataset with Directive Transmitter
  Antennas and Initial Experiments</div>
<div id='2402.00878v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T14:56:45Z</div><div>Authors: Fabian Jaensch, Giuseppe Caire, Begüm Demir</div><div style='padding-top: 10px; width: 80ex'>Over the last years, several works have explored the application of deep
learning algorithms to determine the large-scale signal fading (also referred
to as ``path loss'') between transmitter and receiver pairs in urban
communication networks. The central idea is to replace costly measurement
campaigns, inaccurate statistical models or computationally expensive
ray-tracing simulations by machine learning models which, once trained, produce
accurate predictions almost instantly. Although the topic has attracted
attention from many researchers, there are few open benchmark datasets and
codebases that would allow everyone to test and compare the developed methods
and algorithms. We take a step towards filling this gap by releasing a publicly
available dataset of simulated path loss radio maps together with realistic
city maps from real-world locations and aerial images from open datasources.
Initial experiments regarding model architectures, input feature design and
estimation of radio maps from aerial images are presented and the code is made
available.</div><div><a href='http://arxiv.org/abs/2402.00878v1'>2402.00878v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00229v1")'>Diffraction and Scattering Aware Radio Map and Environment
  Reconstruction using Geometry Model-Assisted Deep Learning</div>
<div id='2403.00229v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T02:20:01Z</div><div>Authors: Wangqian Chen, Junting Chen</div><div style='padding-top: 10px; width: 80ex'>Machine learning (ML) facilitates rapid channel modeling for 5G and beyond
wireless communication systems. Many existing ML techniques utilize a city map
to construct the radio map; however, an updated city map may not always be
available. This paper proposes to employ the received signal strength (RSS)
data to jointly construct the radio map and the virtual environment by
exploiting the geometry structure of the environment. In contrast to many
existing ML approaches that lack of an environment model, we develop a virtual
obstacle model and characterize the geometry relation between the propagation
paths and the virtual obstacles. A multi-screen knife-edge model is adopted to
extract the key diffraction features, and these features are fed into a neural
network (NN) for diffraction representation. To describe the scattering, as
oppose to most existing methods that directly input an entire city map, our
model focuses on the geometry structure from the local area surrounding the
TX-RX pair and the spatial invariance of such local geometry structure is
exploited. Numerical experiments demonstrate that, in addition to
reconstructing a 3D virtual environment, the proposed model outperforms the
state-of-the-art methods in radio map construction with 10%-18% accuracy
improvements. It can also reduce 20% data and 50% training epochs when
transferred to a new environment.</div><div><a href='http://arxiv.org/abs/2403.00229v1'>2403.00229v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17336v1")'>Outdoor Environment Reconstruction with Deep Learning on Radio
  Propagation Paths</div>
<div id='2402.17336v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T09:11:10Z</div><div>Authors: Hrant Khachatrian, Rafayel Mkrtchyan, Theofanis P. Raptis</div><div style='padding-top: 10px; width: 80ex'>Conventional methods for outdoor environment reconstruction rely
predominantly on vision-based techniques like photogrammetry and LiDAR, facing
limitations such as constrained coverage, susceptibility to environmental
conditions, and high computational and energy demands. These challenges are
particularly pronounced in applications like augmented reality navigation,
especially when integrated with wearable devices featuring constrained
computational resources and energy budgets. In response, this paper proposes a
novel approach harnessing ambient wireless signals for outdoor environment
reconstruction. By analyzing radio frequency (RF) data, the paper aims to
deduce the environmental characteristics and digitally reconstruct the outdoor
surroundings. Investigating the efficacy of selected deep learning (DL)
techniques on the synthetic RF dataset WAIR-D, the study endeavors to address
the research gap in this domain. Two DL-driven approaches are evaluated
(convolutional U-Net and CLIP+ based on vision transformers), with performance
assessed using metrics like intersection-over-union (IoU), Hausdorff distance,
and Chamfer distance. The results demonstrate promising performance of the
RF-based reconstruction method, paving the way towards lightweight and scalable
reconstruction solutions.</div><div><a href='http://arxiv.org/abs/2402.17336v1'>2402.17336v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09948v1")'>Neural 5G Indoor Localization with IMU Supervision</div>
<div id='2402.09948v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T13:51:21Z</div><div>Authors: Aleksandr Ermolov, Shreya Kadambi, Maximilian Arnold, Mohammed Hirzallah, Roohollah Amiri, Deepak Singh Mahendar Singh, Srinivas Yerramalli, Daniel Dijkman, Fatih Porikli, Taesang Yoo, Bence Major</div><div style='padding-top: 10px; width: 80ex'>Radio signals are well suited for user localization because they are
ubiquitous, can operate in the dark and maintain privacy. Many prior works
learn mappings between channel state information (CSI) and position
fully-supervised. However, that approach relies on position labels which are
very expensive to acquire. In this work, this requirement is relaxed by using
pseudo-labels during deployment, which are calculated from an inertial
measurement unit (IMU). We propose practical algorithms for IMU double
integration and training of the localization system. We show decimeter-level
accuracy on simulated and challenging real data of 5G measurements. Our
IMU-supervised method performs similarly to fully-supervised, but requires much
less effort to deploy.</div><div><a href='http://arxiv.org/abs/2402.09948v1'>2402.09948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01348v1")'>SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for
  Indoor Localization</div>
<div id='2403.01348v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T00:01:29Z</div><div>Authors: Danish Gufran, Saideep Tiku, Sudeep Pasricha</div><div style='padding-top: 10px; width: 80ex'>Indoor localization is a critical task in many embedded applications, such as
asset tracking, emergency response, and realtime navigation. In this article,
we propose a novel fingerprintingbased framework for indoor localization called
SANGRIA that uses stacked autoencoder neural networks with gradient boosted
trees. Our approach is designed to overcome the device heterogeneity challenge
that can create uncertainty in wireless signal measurements across embedded
devices used for localization. We compare SANGRIA to several state-of-the-art
frameworks and demonstrate 42.96% lower average localization error across
diverse indoor locales and heterogeneous devices.</div><div><a href='http://arxiv.org/abs/2403.01348v1'>2403.01348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08863v1")'>Robust Localization of Key Fob Using Channel Impulse Response of Ultra
  Wide Band Sensors for Keyless Entry Systems</div>
<div id='2401.08863v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T22:35:14Z</div><div>Authors: Abhiram Kolli, Filippo Casamassima, Horst Possegger, Horst Bischof</div><div style='padding-top: 10px; width: 80ex'>Using neural networks for localization of key fob within and surrounding a
car as a security feature for keyless entry is fast emerging. In this paper we
study: 1) the performance of pre-computed features of neural networks based UWB
(ultra wide band) localization classification forming the baseline of our
experiments. 2) Investigate the inherent robustness of various neural networks;
therefore, we include the study of robustness of the adversarial examples
without any adversarial training in this work. 3) Propose a multi-head
self-supervised neural network architecture which outperforms the baseline
neural networks without any adversarial training. The model's performance
improved by 67% at certain ranges of adversarial magnitude for fast gradient
sign method and 37% each for basic iterative method and projected gradient
descent method.</div><div><a href='http://arxiv.org/abs/2401.08863v1'>2401.08863v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09580v1")'>Complexity Reduction in Machine Learning-Based Wireless Positioning:
  Minimum Description Features</div>
<div id='2402.09580v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T21:03:08Z</div><div>Authors: Myeung Suk Oh, Anindya Bijoy Das, Taejoon Kim, David J. Love, Christopher G. Brinton</div><div style='padding-top: 10px; width: 80ex'>A recent line of research has been investigating deep learning approaches to
wireless positioning (WP). Although these WP algorithms have demonstrated high
accuracy and robust performance against diverse channel conditions, they also
have a major drawback: they require processing high-dimensional features, which
can be prohibitive for mobile applications. In this work, we design a
positioning neural network (P-NN) that substantially reduces the complexity of
deep learning-based WP through carefully crafted minimum description features.
Our feature selection is based on maximum power measurements and their temporal
locations to convey information needed to conduct WP. We also develop a novel
methodology for adaptively selecting the size of feature space, which optimizes
over balancing the expected amount of useful information and classification
capability, quantified using information-theoretic measures on the signal bin
selection. Numerical results show that P-NN achieves a significant advantage in
performance-complexity tradeoff over deep learning baselines that leverage the
full power delay profile (PDP).</div><div><a href='http://arxiv.org/abs/2402.09580v1'>2402.09580v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10685v1")'>Towards End-to-End GPS Localization with Neural Pseudorange Correction</div>
<div id='2401.10685v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T13:32:55Z</div><div>Authors: Xu Weng, KV Ling, Haochen Liu, Kun Cao</div><div style='padding-top: 10px; width: 80ex'>Pseudorange errors are the root cause of localization inaccuracy in GPS.
Previous data-driven methods regress and eliminate pseudorange errors using
handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS
localization framework, E2E-PrNet, to train a neural network for pseudorange
correction (PrNet) directly using the final task loss calculated with the
ground truth of GPS receiver states. The gradients of the loss with respect to
learnable parameters are backpropagated through a differentiable nonlinear
least squares optimizer to PrNet. The feasibility is verified with GPS data
collected by Android phones, showing that E2E-PrNet outperforms the
state-of-the-art end-to-end GPS localization methods.</div><div><a href='http://arxiv.org/abs/2401.10685v1'>2401.10685v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03309v1")'>AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion</div>
<div id='2402.03309v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:59:31Z</div><div>Authors: Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher A. Metzler</div><div style='padding-top: 10px; width: 80ex'>Underwater perception and 3D surface reconstruction are challenging problems
with broad applications in construction, security, marine archaeology, and
environmental monitoring. Treacherous operating conditions, fragile
surroundings, and limited navigation control often dictate that submersibles
restrict their range of motion and, thus, the baseline over which they can
capture measurements. In the context of 3D scene reconstruction, it is
well-known that smaller baselines make reconstruction more challenging. Our
work develops a physics-based multimodal acoustic-optical neural surface
reconstruction framework (AONeuS) capable of effectively integrating
high-resolution RGB measurements with low-resolution depth-resolved imaging
sonar measurements. By fusing these complementary modalities, our framework can
reconstruct accurate high-resolution 3D surfaces from measurements captured
over heavily-restricted baselines. Through extensive simulations and in-lab
experiments, we demonstrate that AONeuS dramatically outperforms recent
RGB-only and sonar-only inverse-differentiable-rendering--based surface
reconstruction methods. A website visualizing the results of our paper is
located at this address: https://aoneus.github.io/</div><div><a href='http://arxiv.org/abs/2402.03309v1'>2402.03309v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08976v1")'>ACT-GAN: Radio map construction based on generative adversarial networks
  with ACT blocks</div>
<div id='2401.08976v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T05:03:53Z</div><div>Authors: Chen Qi, Yang Jingjing, Huang Ming, Zhou Qiang</div><div style='padding-top: 10px; width: 80ex'>The radio map, serving as a visual representation of electromagnetic spatial
characteristics, plays a pivotal role in assessment of wireless communication
networks and radio monitoring coverage. Addressing the issue of low accuracy
existing in the current radio map construction, this paper presents a novel
radio map construction method based on generative adversarial network (GAN) in
which the Aggregated Contextual-Transformation (AOT) block, Convolutional Block
Attention Module (CBAM), and Transposed Convolution (T-Conv) block are applied
to the generator, and we name it as ACT-GAN. It significantly improves the
reconstruction accuracy and local texture of the radio maps. The performance of
ACT-GAN across three different scenarios is demonstrated. Experiment results
reveal that in the scenario without sparse discrete observations, the proposed
method reduces the root mean square error (RMSE) by 14.6% in comparison to the
state-of-the-art models. In the scenario with sparse discrete observations, the
RMSE is diminished by 13.2%. Furthermore, the predictive results of the
proposed model show a more lucid representation of electromagnetic spatial
field distribution. To verify the universality of this model in radio map
construction tasks, the scenario of unknown radio emission source is
investigated. The results indicate that the proposed model is robust radio map
construction and accurate in predicting the location of the emission source.</div><div><a href='http://arxiv.org/abs/2401.08976v1'>2401.08976v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18630v1")'>GNSS Positioning using Cost Function Regulated Multilateration and Graph
  Neural Networks</div>
<div id='2402.18630v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T19:00:01Z</div><div>Authors: Amir Jalalirad, Davide Belli, Bence Major, Songwon Jee, Himanshu Shah, Will Morrison</div><div style='padding-top: 10px; width: 80ex'>In urban environments, where line-of-sight signals from GNSS satellites are
frequently blocked by high-rise objects, GNSS receivers are subject to large
errors in measuring satellite ranges. Heuristic methods are commonly used to
estimate these errors and reduce the impact of noisy measurements on
localization accuracy. In our work, we replace these error estimation
heuristics with a deep learning model based on Graph Neural Networks.
Additionally, by analyzing the cost function of the multilateration process, we
derive an optimal method to utilize the estimated errors. Our approach
guarantees that the multilateration converges to the receiver's location as the
error estimation accuracy increases. We evaluate our solution on a real-world
dataset containing more than 100k GNSS epochs, collected from multiple cities
with diverse characteristics. The empirical results show improvements from 40%
to 80% in the horizontal localization error against recent deep learning
baselines as well as classical localization approaches.</div><div><a href='http://arxiv.org/abs/2402.18630v1'>2402.18630v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01259v1")'>Position Aware 60 GHz mmWave Beamforming for V2V Communications
  Utilizing Deep Learning</div>
<div id='2402.01259v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:30:27Z</div><div>Authors: Muhammad Baqer Mollah, Honggang Wang, Hua Fang</div><div style='padding-top: 10px; width: 80ex'>Beamforming techniques are considered as essential parts to compensate the
severe path loss in millimeter-wave (mmWave) communications by adopting large
antenna arrays and formulating narrow beams to obtain satisfactory received
powers. However, performing accurate beam alignment over such narrow beams for
efficient link configuration by traditional beam selection approaches, mainly
relied on channel state information, typically impose significant latency and
computing overheads, which is often infeasible in vehicle-to-vehicle (V2V)
communications like highly dynamic scenarios. In contrast, utilizing
out-of-band contextual information, such as vehicular position information, is
a potential alternative to reduce such overheads. In this context, this paper
presents a deep learning-based solution on utilizing the vehicular position
information for predicting the optimal beams having sufficient mmWave received
powers so that the best V2V line-of-sight links can be ensured proactively.
After experimental evaluation of the proposed solution on real-world measured
mmWave sensing and communications datasets, the results show that the solution
can achieve up to 84.58% of received power of link status on average, which
confirm a promising solution for beamforming in mmWave at 60 GHz enabled V2V
communications.</div><div><a href='http://arxiv.org/abs/2402.01259v1'>2402.01259v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12801v1")'>Deep Learning-based Target-To-User Association in Integrated Sensing and
  Communication Systems</div>
<div id='2401.12801v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T00:45:33Z</div><div>Authors: Lorenzo Cazzella, Marouan Mizmizi, Dario Tagliaferri, Damiano Badini, Matteo Matteucci, Umberto Spagnolini</div><div style='padding-top: 10px; width: 80ex'>In Integrated Sensing and Communication (ISAC) systems, matching the radar
targets with communication user equipments (UEs) is functional to several
communication tasks, such as proactive handover and beam prediction. In this
paper, we consider a radar-assisted communication system where a base station
(BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a
double aim: (i) associate vehicular radar targets to vehicular equipments (VEs)
in the communication beamspace and (ii) predict the beamforming vector for each
VE from radar data. The proposed target-to-user (T2U) association consists of
two stages. First, vehicular radar targets are detected from range-angle
images, and, for each, a beamforming vector is estimated. Then, the inferred
per-target beamforming vectors are matched with the ones utilized at the BS for
communication to perform target-to-user (T2U) association. Joint multi-target
detection and beam inference is obtained by modifying the you only look once
(YOLO) model, which is trained over simulated range-angle radar images.
Simulation results over different urban vehicular mobility scenarios show that
the proposed T2U method provides a probability of correct association that
increases with the size of the BS antenna array, highlighting the respective
increase of the separability of the VEs in the beamspace. Moreover, we show
that the modified YOLO architecture can effectively perform both beam
prediction and radar target detection, with similar performance in mean average
precision on the latter over different antenna array sizes.</div><div><a href='http://arxiv.org/abs/2401.12801v1'>2401.12801v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02323v1")'>Multi-Agent Context Learning Strategy for Interference-Aware Beam
  Allocation in mmWave Vehicular Communications</div>
<div id='2401.02323v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T15:43:55Z</div><div>Authors: Abdulkadir Kose, Haeyoung Lee, Chuan Heng Foh, Mohammad Shojafar</div><div style='padding-top: 10px; width: 80ex'>Millimeter wave (mmWave) has been recognized as one of key technologies for
5G and beyond networks due to its potential to enhance channel bandwidth and
network capacity. The use of mmWave for various applications including
vehicular communications has been extensively discussed. However, applying
mmWave to vehicular communications faces challenges of high mobility nodes and
narrow coverage along the mmWave beams. Due to high mobility in dense networks,
overlapping beams can cause strong interference which leads to performance
degradation. As a remedy, beam switching capability in mmWave can be utilized.
Then, frequent beam switching and cell change become inevitable to manage
interference, which increase computational and signalling complexity. In order
to deal with the complexity in interference control, we develop a new strategy
called Multi-Agent Context Learning (MACOL), which utilizes Contextual Bandit
to manage interference while allocating mmWave beams to serve vehicles in the
network. Our approach demonstrates that by leveraging knowledge of neighbouring
beam status, the machine learning agent can identify and avoid potential
interfering transmissions to other ongoing transmissions. Furthermore, we show
that even under heavy traffic loads, our proposed MACOL strategy is able to
maintain low interference levels at around 10%.</div><div><a href='http://arxiv.org/abs/2401.02323v1'>2401.02323v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17689v1")'>QoS prediction in radio vehicular environments via prior user
  information</div>
<div id='2402.17689v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:05:41Z</div><div>Authors: Noor Ul Ain, Rodrigo Hernangómez, Alexandros Palaios, Martin Kasparick, Sławomir Stańczak</div><div style='padding-top: 10px; width: 80ex'>Reliable wireless communications play an important role in the automotive
industry as it helps to enhance current use cases and enable new ones such as
connected autonomous driving, platooning, cooperative maneuvering, teleoperated
driving, and smart navigation. These and other use cases often rely on specific
quality of service (QoS) levels for communication. Recently, the area of
predictive quality of service (QoS) has received a great deal of attention as a
key enabler to forecast communication quality well enough in advance. However,
predicting QoS in a reliable manner is a notoriously difficult task. In this
paper, we evaluate ML tree-ensemble methods to predict QoS in the range of
minutes with data collected from a cellular test network. We discuss radio
environment characteristics and we showcase how these can be used to improve ML
performance and further support the uptake of ML in commercial networks.
Specifically, we use the correlations of the measurements coming from the radio
environment by including information of prior vehicles to enhance the
prediction of the target vehicles. Moreover, we are extending prior art by
showing how longer prediction horizons can be supported.</div><div><a href='http://arxiv.org/abs/2402.17689v1'>2402.17689v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14698v2")'>Using construction waste hauling trucks' GPS data to classify
  earthwork-related locations: A Chengdu case study</div>
<div id='2402.14698v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:50:32Z</div><div>Authors: Lei Yu, Ke Han</div><div style='padding-top: 10px; width: 80ex'>Earthwork-related locations (ERLs), such as construction sites, earth dumping
ground, and concrete mixing stations, are major sources of urban dust pollution
(particulate matters). The effective management of ERLs is crucial and requires
timely and efficient tracking of these locations throughout the city. This work
aims to identify and classify urban ERLs using GPS trajectory data of over
16,000 construction waste hauling trucks (CWHTs), as well as 58 urban features
encompassing geographic, land cover, POI and transport dimensions. We compare
several machine learning models and examine the impact of various
spatial-temporal features on classification performance using real-world data
in Chengdu, China. The results demonstrate that 77.8% classification accuracy
can be achieved with a limited number of features. This classification
framework was implemented in the Alpha MAPS system in Chengdu, which has
successfully identified 724 construction cites/earth dumping ground, 48
concrete mixing stations, and 80 truck parking locations in the city during
December 2023, which has enabled local authority to effectively manage urban
dust pollution at low personnel costs.</div><div><a href='http://arxiv.org/abs/2402.14698v2'>2402.14698v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02746v2")'>Learning without Exact Guidance: Updating Large-scale High-resolution
  Land Cover Maps from Low-resolution Historical Labels</div>
<div id='2403.02746v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:02:00Z</div><div>Authors: Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang</div><div style='padding-top: 10px; width: 80ex'>Large-scale high-resolution (HR) land-cover mapping is a vital task to survey
the Earth's surface and resolve many challenges facing humanity. However, it is
still a non-trivial task hindered by complex ground details, various landforms,
and the scarcity of accurate training labels over a wide-span geographic area.
In this paper, we propose an efficient, weakly supervised framework
(Paraformer) to guide large-scale HR land-cover mapping with easy-access
historical land-cover data of low resolution (LR). Specifically, existing
land-cover mapping approaches reveal the dominance of CNNs in preserving local
ground details but still suffer from insufficient global modeling in various
landforms. Therefore, we design a parallel CNN-Transformer feature extractor in
Paraformer, consisting of a downsampling-free CNN branch and a Transformer
branch, to jointly capture local and global contextual information. Besides,
facing the spatial mismatch of training data, a pseudo-label-assisted training
(PLAT) module is adopted to reasonably refine LR labels for weakly supervised
semantic segmentation of HR images. Experiments on two large-scale datasets
demonstrate the superiority of Paraformer over other state-of-the-art methods
for automatically updating HR land-cover maps from LR historical labels.</div><div><a href='http://arxiv.org/abs/2403.02746v2'>2403.02746v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03161v1")'>PalmProbNet: A Probabilistic Approach to Understanding Palm
  Distributions in Ecuadorian Tropical Forest via Transfer Learning</div>
<div id='2403.03161v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T17:54:22Z</div><div>Authors: Kangning Cui, Zishan Shao, Gregory Larsen, Victor Pauca, Sarra Alqahtani, David Segurado, João Pinheiro, Manqi Wang, David Lutz, Robert Plemmons, Miles Silman</div><div style='padding-top: 10px; width: 80ex'>Palms play an outsized role in tropical forests and are important resources
for humans and wildlife. A central question in tropical ecosystems is
understanding palm distribution and abundance. However, accurately identifying
and localizing palms in geospatial imagery presents significant challenges due
to dense vegetation, overlapping canopies, and variable lighting conditions in
mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a
probabilistic approach utilizing transfer learning to analyze high-resolution
UAV-derived orthomosaic imagery, enabling the detection of palm trees within
the dense canopy of the Ecuadorian Rainforest. This approach represents a
substantial advancement in automated palm detection, effectively pinpointing
palm presence and locality in mixed tropical rainforests. Our process begins by
generating an orthomosaic image from UAV images, from which we extract and
label palm and non-palm image patches in two distinct sizes. These patches are
then used to train models with an identical architecture, consisting of an
unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with
specifically trained parameters. Subsequently, PalmProbNet employs a sliding
window technique on the landscape orthomosaic, using both small and large
window sizes to generate a probability heatmap. This heatmap effectively
visualizes the distribution of palms, showcasing the scalability and
adaptability of our approach in various forest densities. Despite the
challenging terrain, our method demonstrated remarkable performance, achieving
an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.</div><div><a href='http://arxiv.org/abs/2403.03161v1'>2403.03161v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11670v1")'>Challenging the Black Box: A Comprehensive Evaluation of Attribution
  Maps of CNN Applications in Agriculture and Forestry</div>
<div id='2402.11670v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T18:16:43Z</div><div>Authors: Lars Nieradzik, Henrike Stephani, Jördis Sieburg-Rockel, Stephanie Helmling, Andrea Olbrich, Janis Keuper</div><div style='padding-top: 10px; width: 80ex'>In this study, we explore the explainability of neural networks in
agriculture and forestry, specifically in fertilizer treatment classification
and wood identification. The opaque nature of these models, often considered
'black boxes', is addressed through an extensive evaluation of state-of-the-art
Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency
maps. Our comprehensive qualitative and quantitative analysis of these AMs
uncovers critical practical limitations. Findings reveal that AMs frequently
fail to consistently highlight crucial features and often misalign with the
features considered important by domain experts. These discrepancies raise
substantial questions about the utility of AMs in understanding the
decision-making process of neural networks. Our study provides critical
insights into the trustworthiness and practicality of AMs within the
agriculture and forestry sectors, thus facilitating a better understanding of
neural networks in these application areas.</div><div><a href='http://arxiv.org/abs/2402.11670v1'>2402.11670v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10730v1")'>Counterfactual Analysis of Neural Networks Used to Create Fertilizer
  Management Zones</div>
<div id='2403.10730v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T23:29:32Z</div><div>Authors: Giorgio Morales, John Sheppard</div><div style='padding-top: 10px; width: 80ex'>In Precision Agriculture, the utilization of management zones (MZs) that take
into account within-field variability facilitates effective fertilizer
management. This approach enables the optimization of nitrogen (N) rates to
maximize crop yield production and enhance agronomic use efficiency. However,
existing works often neglect the consideration of responsivity to fertilizer as
a factor influencing MZ determination. In response to this gap, we present a MZ
clustering method based on fertilizer responsivity. We build upon the statement
that the responsivity of a given site to the fertilizer rate is described by
the shape of its corresponding N fertilizer-yield response (N-response) curve.
Thus, we generate N-response curves for all sites within the field using a
convolutional neural network (CNN). The shape of the approximated N-response
curves is then characterized using functional principal component analysis.
Subsequently, a counterfactual explanation (CFE) method is applied to discern
the impact of various variables on MZ membership. The genetic algorithm-based
CFE solves a multi-objective optimization problem and aims to identify the
minimum combination of features needed to alter a site's cluster assignment.
Results from two yield prediction datasets indicate that the features with the
greatest influence on MZ membership are associated with terrain characteristics
that either facilitate or impede fertilizer runoff, such as terrain slope or
topographic aspect.</div><div><a href='http://arxiv.org/abs/2403.10730v1'>2403.10730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02985v1")'>Unsupervised semantic segmentation of high-resolution UAV imagery for
  road scene parsing</div>
<div id='2402.02985v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:16:12Z</div><div>Authors: Zihan Ma, Yongshang Li, Ronggui Ma, Chen Liang</div><div style='padding-top: 10px; width: 80ex'>Two challenges are presented when parsing road scenes in UAV images. First,
the high resolution of UAV images makes processing difficult. Second,
supervised deep learning methods require a large amount of manual annotations
to train robust and accurate models. In this paper, an unsupervised road
parsing framework that leverages recent advances in vision language models and
fundamental computer vision model is introduced.Initially, a vision language
model is employed to efficiently process ultra-large resolution UAV images to
quickly detect road regions of interest in the images. Subsequently, the vision
foundation model SAM is utilized to generate masks for the road regions without
category information. Following that, a self-supervised representation learning
network extracts feature representations from all masked regions. Finally, an
unsupervised clustering algorithm is applied to cluster these feature
representations and assign IDs to each cluster. The masked regions are combined
with the corresponding IDs to generate initial pseudo-labels, which initiate an
iterative self-training process for regular semantic segmentation. The proposed
method achieves an impressive 89.96% mIoU on the development dataset without
relying on any manual annotation. Particularly noteworthy is the extraordinary
flexibility of the proposed method, which even goes beyond the limitations of
human-defined categories and is able to acquire knowledge of new categories
from the dataset itself.</div><div><a href='http://arxiv.org/abs/2402.02985v1'>2402.02985v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13379v1")'>Referee-Meta-Learning for Fast Adaptation of Locational Fairness</div>
<div id='2402.13379v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T21:09:04Z</div><div>Authors: Weiye Chen, Yiqun Xie, Xiaowei Jia, Erhu He, Han Bao, Bang An, Xun Zhou</div><div style='padding-top: 10px; width: 80ex'>When dealing with data from distinct locations, machine learning algorithms
tend to demonstrate an implicit preference of some locations over the others,
which constitutes biases that sabotage the spatial fairness of the algorithm.
This unfairness can easily introduce biases in subsequent decision-making given
broad adoptions of learning-based solutions in practice. However, locational
biases in AI are largely understudied. To mitigate biases over locations, we
propose a locational meta-referee (Meta-Ref) to oversee the few-shot
meta-training and meta-testing of a deep neural network. Meta-Ref dynamically
adjusts the learning rates for training samples of given locations to advocate
a fair performance across locations, through an explicit consideration of
locational biases and the characteristics of input data. We present a
three-phase training framework to learn both a meta-learning-based predictor
and an integrated Meta-Ref that governs the fairness of the model. Once trained
with a distribution of spatial tasks, Meta-Ref is applied to samples from new
spatial tasks (i.e., regions outside the training area) to promote fairness
during the fine-tune step. We carried out experiments with two case studies on
crop monitoring and transportation safety, which show Meta-Ref can improve
locational fairness while keeping the overall prediction quality at a similar
level.</div><div><a href='http://arxiv.org/abs/2402.13379v1'>2402.13379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16437v1")'>A Benchmark Dataset for Tornado Detection and Prediction using
  Full-Resolution Polarimetric Weather Radar Data</div>
<div id='2401.16437v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T21:47:39Z</div><div>Authors: Mark S. Veillette, James M. Kurdzo, Phillip M. Stepanian, John Y. N. Cho, Siddharth Samsi, Joseph McDonald</div><div style='padding-top: 10px; width: 80ex'>Weather radar is the primary tool used by forecasters to detect and warn for
tornadoes in near-real time. In order to assist forecasters in warning the
public, several algorithms have been developed to automatically detect tornadic
signatures in weather radar observations. Recently, Machine Learning (ML)
algorithms, which learn directly from large amounts of labeled data, have been
shown to be highly effective for this purpose. Since tornadoes are extremely
rare events within the corpus of all available radar observations, the
selection and design of training datasets for ML applications is critical for
the performance, robustness, and ultimate acceptance of ML algorithms. This
study introduces a new benchmark dataset, TorNet to support development of ML
algorithms in tornado detection and prediction. TorNet contains
full-resolution, polarimetric, Level-II WSR-88D data sampled from 10 years of
reported storm events. A number of ML baselines for tornado detection are
developed and compared, including a novel deep learning (DL) architecture
capable of processing raw radar imagery without the need for manual feature
extraction required for existing ML algorithms. Despite not benefiting from
manual feature engineering or other preprocessing, the DL model shows increased
detection performance compared to non-DL and operational baselines. The TorNet
dataset, as well as source code and model weights of the DL baseline trained in
this work, are made freely available.</div><div><a href='http://arxiv.org/abs/2401.16437v1'>2401.16437v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14974v1")'>Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An
  Application for MxIF Oncology Data</div>
<div id='2402.14974v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T21:22:21Z</div><div>Authors: Majid Farhadloo, Arun Sharma, Jayant Gupta, Alexey Leontovich, Svetomir N. Markovic, Shashi Shekhar</div><div style='padding-top: 10px; width: 80ex'>Given multi-category point sets from different place-types, our goal is to
develop a spatially-lucid classifier that can distinguish between two classes
based on the arrangements of their points. This problem is important for many
applications, such as oncology, for analyzing immune-tumor relationships and
designing new immunotherapies. It is challenging due to spatial variability and
interpretability needs. Previously proposed techniques require dense training
data or have limited ability to handle significant spatial variability within a
single place-type. Most importantly, these deep neural network (DNN) approaches
are not designed to work in non-Euclidean space, particularly point sets.
Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches.
We explore a spatial ensemble framework that explicitly uses different training
strategies, including weighted-distance learning rate and spatial domain
adaptation, on various place-types for spatially-lucid classification.
Experimental results on real-world datasets (e.g., MxIF oncology data) show
that the proposed framework provides higher prediction accuracy than baseline
methods.</div><div><a href='http://arxiv.org/abs/2402.14974v1'>2402.14974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15163v1")'>Studying the Impact of Stochasticity on the Evaluation of Deep Neural
  Networks for Forest-Fire Prediction</div>
<div id='2402.15163v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T07:54:20Z</div><div>Authors: Harshit Kumar, Biswadeep Chakraborty, Beomseok Kang, Saibal Mukhopadhyay</div><div style='padding-top: 10px; width: 80ex'>This paper presents the first systematic study of the evaluation of Deep
Neural Networks (DNNs) for discrete dynamical systems under stochastic
assumptions, with a focus on wildfire prediction. We develop a framework to
study the impact of stochasticity on two classes of evaluation metrics:
classification-based metrics, which assess fidelity to observed ground truth
(GT), and proper scoring rules, which test fidelity-to-statistic. Our findings
reveal that evaluating for fidelity-to-statistic is a reliable alternative in
highly stochastic scenarios. We extend our analysis to real-world wildfire
data, highlighting limitations in traditional wildfire prediction evaluation
methods, and suggest interpretable stochasticity-compatible alternatives.</div><div><a href='http://arxiv.org/abs/2402.15163v1'>2402.15163v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12991v1")'>Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free
  Traffic via a Spatio-Temporal Framework</div>
<div id='2403.12991v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:37:14Z</div><div>Authors: ChungYi Lin, Shen-Lung Tung, Hung-Ting Su, Winston H. Hsu</div><div style='padding-top: 10px; width: 80ex'>Vehicle flow, a crucial indicator for transportation, is often limited by
detector coverage. With the advent of extensive mobile network coverage, we can
leverage mobile user activities, or cellular traffic, on roadways as a proxy
for vehicle flow. However, as counts of cellular traffic may not directly align
with vehicle flow due to data from various user types, we present a new task:
predicting vehicle flow in camera-free areas using cellular traffic. To uncover
correlations within multi-source data, we deployed cameras on selected roadways
to establish the Tel2Veh dataset, consisting of extensive cellular traffic and
sparse vehicle flows. Addressing this challenge, we propose a framework that
independently extracts features and integrates them with a graph neural network
(GNN)-based fusion to discern disparities, thereby enabling the prediction of
unseen vehicle flows using cellular traffic. This work advances the use of
telecom data in transportation and pioneers the fusion of telecom and
vision-based data, offering solutions for traffic management.</div><div><a href='http://arxiv.org/abs/2403.12991v1'>2403.12991v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02906v1")'>Citizen Science and Machine Learning for Research and Nature
  Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects</div>
<div id='2403.02906v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T12:13:27Z</div><div>Authors: Kinga Skorupska, Rafał Stryjek, Izabela Wierzbowska, Piotr Bebas, Maciej Grzeszczuk, Piotr Gago, Jarosław Kowalski, Maciej Krzywicki, Jagoda Lazarek, Wiesław Kopeć</div><div style='padding-top: 10px; width: 80ex'>Technology is increasingly used in Nature Reserves and National Parks around
the world to support conservation efforts. Endangered species, such as the
Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps.
Yet, this method produces vast amounts of data, which needs to be prepared,
analyzed and interpreted. Therefore, researchers working in this area
increasingly need support to process this incoming information. One opportunity
is to seek support from volunteer Citizen Scientists who can help label the
data, however, it is challenging to retain their interest. Another way is to
automate the process with image recognition using convolutional neural
networks. During the panel, we will discuss considerations related to nature
research and conservation as well as opportunities for the use of Citizen
Science and Machine Learning to expedite the process of data preparation,
labelling and analysis.</div><div><a href='http://arxiv.org/abs/2403.02906v1'>2403.02906v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12072v1")'>Floralens: a Deep Learning Model for the Portuguese Native Flora</div>
<div id='2403.12072v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T15:23:21Z</div><div>Authors: António Filgueiras, Eduardo R. B. Marques, Luís M. B. Lopes, Miguel Marques, Hugo Silva</div><div style='padding-top: 10px; width: 80ex'>Machine-learning techniques, namely deep convolutional neural networks, are
pivotal for image-based identification of biological species in many Citizen
Science platforms. However, the construction of critically sized and sampled
datasets to train the networks and the choice of the network architectures
itself remains little documented and, therefore, does not lend itself to be
easily replicated. In this paper, we develop a streamlined methodology for
building datasets for biological taxa from publicly available research-grade
datasets and for deriving models from these datasets using off-the-shelf deep
convolutional neural networks such as those provided by Google's AutoML Vision
cloud service. Our case study is the Portuguese native flora, anchored in a
high-quality dataset, provided by the Sociedade Portuguesa de Bot\^anica,
scaled up by adding sampled data from iNaturalist, Pl@ntNet, and
Observation.org. We find that with a careful dataset design, off-the-shelf
machine-learning cloud services produce accurate models with relatively little
effort that rival those provided by state-of-the-art citizen science platforms.
The best model we derived, dubbed Floralens, has been integrated into the
public website of Project Biolens, where we gather models for other taxa as
well. The dataset used to train the model and its namesake is publicly
available on Zenodo.</div><div><a href='http://arxiv.org/abs/2403.12072v1'>2403.12072v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04691v1")'>AI-based Mapping of the Conservation Status of Orchid Assemblages at
  Global Scale</div>
<div id='2401.04691v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T17:38:19Z</div><div>Authors: Joaquim Estopinan, Maximilien Servajean, Pierre Bonnet, Alexis Joly, François Munoz</div><div style='padding-top: 10px; width: 80ex'>Although increasing threats on biodiversity are now widely recognised, there
are no accurate global maps showing whether and where species assemblages are
at risk. We hereby assess and map at kilometre resolution the conservation
status of the iconic orchid family, and discuss the insights conveyed at
multiple scales. We introduce a new Deep Species Distribution Model trained on
1M occurrences of 14K orchid species to predict their assemblages at global
scale and at kilometre resolution. We propose two main indicators of the
conservation status of the assemblages: (i) the proportion of threatened
species, and (ii) the status of the most threatened species in the assemblage.
We show and analyze the variation of these indicators at World scale and in
relation to currently protected areas in Sumatra island. Global and interactive
maps available online show the indicators of conservation status of orchid
assemblages, with sharp spatial variations at all scales. The highest level of
threat is found at Madagascar and the neighbouring islands. In Sumatra, we
found good correspondence of protected areas with our indicators, but
supplementing current IUCN assessments with status predictions results in
alarming levels of species threat across the island. Recent advances in deep
learning enable reliable mapping of the conservation status of species
assemblages on a global scale. As an umbrella taxon, orchid family provides a
reference for identifying vulnerable ecosystems worldwide, and prioritising
conservation actions both at international and local levels.</div><div><a href='http://arxiv.org/abs/2401.04691v1'>2401.04691v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05470v1")'>Modelling Species Distributions with Deep Learning to Predict Plant
  Extinction Risk and Assess Climate Change Impacts</div>
<div id='2401.05470v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T15:24:27Z</div><div>Authors: Joaquim Estopinan, Pierre Bonnet, Maximilien Servajean, François Munoz, Alexis Joly</div><div style='padding-top: 10px; width: 80ex'>The post-2020 global biodiversity framework needs ambitious, research-based
targets. Estimating the accelerated extinction risk due to climate change is
critical. The International Union for Conservation of Nature (IUCN) measures
the extinction risk of species. Automatic methods have been developed to
provide information on the IUCN status of under-assessed taxa. However, these
compensatory methods are based on current species characteristics, mainly
geographical, which precludes their use in future projections. Here, we
evaluate a novel method for classifying the IUCN status of species benefiting
from the generalisation power of species distribution models based on deep
learning. Our method matches state-of-the-art classification performance while
relying on flexible SDM-based features that capture species' environmental
preferences. Cross-validation yields average accuracies of 0.61 for status
classification and 0.78 for binary classification. Climate change will reshape
future species distributions. Under the species-environment equilibrium
hypothesis, SDM projections approximate plausible future outcomes. Two extremes
of species dispersal capacity are considered: unlimited or null. The projected
species distributions are translated into features feeding our IUCN
classification method. Finally, trends in threatened species are analysed over
time and i) by continent and as a function of average ii) latitude or iii)
altitude. The proportion of threatened species is increasing globally, with
critical rates in Africa, Asia and South America. Furthermore, the proportion
of threatened species is predicted to peak around the two Tropics, at the
Equator, in the lowlands and at altitudes of 800-1,500 m.</div><div><a href='http://arxiv.org/abs/2401.05470v1'>2401.05470v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07472v1")'>Imbalance-aware Presence-only Loss Function for Species Distribution
  Modeling</div>
<div id='2403.07472v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:08:36Z</div><div>Authors: Robin Zbinden, Nina van Tiel, Marc Rußwurm, Devis Tuia</div><div style='padding-top: 10px; width: 80ex'>In the face of significant biodiversity decline, species distribution models
(SDMs) are essential for understanding the impact of climate change on species
habitats by connecting environmental conditions to species occurrences.
Traditionally limited by a scarcity of species observations, these models have
significantly improved in performance through the integration of larger
datasets provided by citizen science initiatives. However, they still suffer
from the strong class imbalance between species within these datasets, often
resulting in the penalization of rare species--those most critical for
conservation efforts. To tackle this issue, this study assesses the
effectiveness of training deep learning models using a balanced presence-only
loss function on large citizen science-based datasets. We demonstrate that this
imbalance-aware loss function outperforms traditional loss functions across
various datasets and tasks, particularly in accurately modeling rare species
with limited observations.</div><div><a href='http://arxiv.org/abs/2403.07472v1'>2403.07472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02989v1")'>On the selection and effectiveness of pseudo-absences for species
  distribution modeling with deep learning</div>
<div id='2401.02989v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T16:06:30Z</div><div>Authors: Robin Zbinden, Nina van Tiel, Benjamin Kellenberger, Lloyd Hughes, Devis Tuia</div><div style='padding-top: 10px; width: 80ex'>Species distribution modeling is a highly versatile tool for understanding
the intricate relationship between environmental conditions and species
occurrences. However, the available data often lacks information on confirmed
species absence and is limited to opportunistically sampled, presence-only
observations. To overcome this limitation, a common approach is to employ
pseudo-absences, which are specific geographic locations designated as negative
samples. While pseudo-absences are well-established for single-species
distribution models, their application in the context of multi-species neural
networks remains underexplored. Notably, the significant class imbalance
between species presences and pseudo-absences is often left unaddressed.
Moreover, the existence of different types of pseudo-absences (e.g., random and
target-group background points) adds complexity to the selection process.
Determining the optimal combination of pseudo-absences types is difficult and
depends on the characteristics of the data, particularly considering that
certain types of pseudo-absences can be used to mitigate geographic biases. In
this paper, we demonstrate that these challenges can be effectively tackled by
integrating pseudo-absences in the training of multi-species neural networks
through modifications to the loss function. This adjustment involves assigning
different weights to the distinct terms of the loss function, thereby
addressing both the class imbalance and the choice of pseudo-absence types.
Additionally, we propose a strategy to set these loss weights using spatial
block cross-validation with presence-only data. We evaluate our approach using
a benchmark dataset containing independent presence-absence data from six
different regions and report improved results when compared to competing
approaches.</div><div><a href='http://arxiv.org/abs/2401.02989v1'>2401.02989v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14459v1")'>Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on
  Mediterranean Sea Water</div>
<div id='2402.14459v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T11:35:52Z</div><div>Authors: Celio Trois, Luciana Didonet Del Fabro, Vladimir A. Baulin</div><div style='padding-top: 10px; width: 80ex'>Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that
fosters biodiversity, stores carbon, releases oxygen, and provides habitat to
numerous sea organisms. Leveraging augmented research, we collected a
comprehensive dataset of 174 features compiled from diverse data sources.
Through machine learning analysis, we discovered the existence of a robust
correlation between the exact location of P. oceanica and water biogeochemical
properties. The model's feature importance, showed that carbon-related
variables as net biomass production and downward surface mass flux of carbon
dioxide have their values altered in the areas with P. oceanica, which in turn
can be used for indirect location of P. oceanica meadows. The study provides
the evidence of the plant's ability to exert a global impact on the environment
and underscores the crucial role of this plant in sea ecosystems, emphasizing
the need for its conservation and management.</div><div><a href='http://arxiv.org/abs/2402.14459v1'>2402.14459v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.08016v1")'>Aedes aegypti Egg Counting with Neural Networks for Object Detection</div>
<div id='2403.08016v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T18:28:13Z</div><div>Authors: Micheli Nayara de Oliveira Vicente, Gabriel Toshio Hirokawa Higa, João Vitor de Andrade Porto, Higor Henrique, Picoli Nucci, Asser Botelho Santana, Karla Rejane de Andrade Porto, Antonia Railda Roel, Hemerson Pistori</div><div style='padding-top: 10px; width: 80ex'>Aedes aegypti is still one of the main concerns when it comes to disease
vectors. Among the many ways to deal with it, there are important protocols
that make use of egg numbers in ovitraps to calculate indices, such as the
LIRAa and the Breteau Index, which can provide information on predictable
outbursts and epidemics. Also, there are many research lines that require egg
numbers, specially when mass production of mosquitoes is needed. Egg counting
is a laborious and error-prone task that can be automated via computer
vision-based techniques, specially deep learning-based counting with object
detection. In this work, we propose a new dataset comprising field and
laboratory eggs, along with test results of three neural networks applied to
the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.</div><div><a href='http://arxiv.org/abs/2403.08016v1'>2403.08016v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14744v1")'>Large Language Models as Urban Residents: An LLM Agent Framework for
  Personal Mobility Generation</div>
<div id='2402.14744v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:03:14Z</div><div>Authors: Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel approach using Large Language Models (LLMs)
integrated into an agent framework for flexible and efficient personal mobility
generation. LLMs overcome the limitations of previous models by efficiently
processing semantic data and offering versatility in modeling various tasks.
Our approach addresses the critical need to align LLMs with real-world urban
mobility data, focusing on three research questions: aligning LLMs with rich
activity data, developing reliable activity generation strategies, and
exploring LLM applications in urban mobility. The key technical contribution is
a novel LLM agent framework that accounts for individual activity patterns and
motivations, including a self-consistency approach to align LLMs with
real-world activity data and a retrieval-augmented strategy for interpretable
activity generation. In experimental studies, comprehensive validation is
performed using real-world data. This research marks the pioneering work of
designing an LLM agent framework for activity generation based on real-world
human activity data, offering a promising tool for urban mobility analysis.</div><div><a href='http://arxiv.org/abs/2402.14744v1'>2402.14744v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01968v1")'>A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges
  and Future Directions</div>
<div id='2402.01968v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T00:27:22Z</div><div>Authors: Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis</div><div style='padding-top: 10px; width: 80ex'>Research interest in autonomous agents is on the rise as an emerging topic.
The notable achievements of Large Language Models (LLMs) have demonstrated the
considerable potential to attain human-like intelligence in autonomous agents.
However, the challenge lies in enabling these agents to learn, reason, and
navigate uncertainties in dynamic environments. Context awareness emerges as a
pivotal element in fortifying multi-agent systems when dealing with dynamic
situations. Despite existing research focusing on both context-aware systems
and multi-agent systems, there is a lack of comprehensive surveys outlining
techniques for integrating context-aware systems with multi-agent systems. To
address this gap, this survey provides a comprehensive overview of
state-of-the-art context-aware multi-agent systems. First, we outline the
properties of both context-aware systems and multi-agent systems that
facilitate integration between these systems. Subsequently, we propose a
general process for context-aware systems, with each phase of the process
encompassing diverse approaches drawn from various application domains such as
collision avoidance in autonomous driving, disaster relief management, utility
management, supply chain management, human-AI interaction, and others. Finally,
we discuss the existing challenges of context-aware multi-agent systems and
provide future research directions in this field.</div><div><a href='http://arxiv.org/abs/2402.01968v1'>2402.01968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07744v2")'>Towards Unified Alignment Between Agents, Humans, and Environment</div>
<div id='2402.07744v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:14:22Z</div><div>Authors: Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang, Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo, Zhicheng Guo, Peng Li, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>The rapid progress of foundation models has led to the prosperity of
autonomous agents, which leverage the universal capabilities of foundation
models to conduct reasoning, decision-making, and environmental interaction.
However, the efficacy of agents remains limited when operating in intricate,
realistic environments. In this work, we introduce the principles of
$\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents
($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with
human intentions, environmental dynamics, and self-constraints such as the
limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we
review the current agent research and highlight the neglected factors in
existing agent benchmarks and method candidates. We also conduct
proof-of-concept studies by introducing realistic features to WebShop,
including user profiles to demonstrate intentions, personalized reranking for
complex environmental dynamics, and runtime cost statistics to reflect
self-constraints. We then follow the principles of $\mathbf{UA}^2$ to propose
an initial design of our agent, and benchmark its performance with several
candidate baselines in the retrofitted WebShop. The extensive experimental
results further prove the importance of the principles of $\mathbf{UA}^2$. Our
research sheds light on the next steps of autonomous agent research with
improved general problem-solving abilities.</div><div><a href='http://arxiv.org/abs/2402.07744v2'>2402.07744v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07462v2")'>A Hormetic Approach to the Value-Loading Problem: Preventing the
  Paperclip Apocalypse?</div>
<div id='2402.07462v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T07:49:48Z</div><div>Authors: Nathan I. N. Henry, Mangor Pedersen, Matt Williams, Jamin L. B. Martin, Liesje Donkin</div><div style='padding-top: 10px; width: 80ex'>The value-loading problem is a significant challenge for researchers aiming
to create artificial intelligence (AI) systems that align with human values and
preferences. This problem requires a method to define and regulate safe and
optimal limits of AI behaviors. In this work, we propose HALO (Hormetic
ALignment via Opponent processes), a regulatory paradigm that uses hormetic
analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a
phenomenon where low frequencies of a behavior have beneficial effects, while
high frequencies are harmful. By modeling behaviors as allostatic opponent
processes, we can use either Behavioral Frequency Response Analysis (BFRA) or
Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of
repeatable behaviors. We demonstrate how HALO can solve the 'paperclip
maximizer' scenario, a thought experiment where an unregulated AI tasked with
making paperclips could end up converting all matter in the universe into
paperclips. Our approach may be used to help create an evolving database of
'values' based on the hedonic calculus of repeatable behaviors with decreasing
marginal utility. This positions HALO as a promising solution for the
value-loading problem, which involves embedding human-aligned values into an AI
system, and the weak-to-strong generalization problem, which explores whether
weak models can supervise stronger models as they become more intelligent.
Hence, HALO opens several research avenues that may lead to the development of
a computational value system that allows an AI algorithm to learn whether the
decisions it makes are right or wrong.</div><div><a href='http://arxiv.org/abs/2402.07462v2'>2402.07462v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05715v1")'>A Framework for Effective AI Recommendations in Cyber-Physical-Human
  Systems</div>
<div id='2403.05715v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T23:02:20Z</div><div>Authors: Aditya Dave, Heeseung Bang, Andreas A. Malikopoulos</div><div style='padding-top: 10px; width: 80ex'>Many cyber-physical-human systems (CPHS) involve a human decision-maker who
may receive recommendations from an artificial intelligence (AI) platform while
holding the ultimate responsibility of making decisions. In such CPHS
applications, the human decision-maker may depart from an optimal recommended
decision and instead implement a different one for various reasons. In this
letter, we develop a rigorous framework to overcome this challenge. In our
framework, we consider that humans may deviate from AI recommendations as they
perceive and interpret the system's state in a different way than the AI
platform. We establish the structural properties of optimal recommendation
strategies and develop an approximate human model (AHM) used by the AI. We
provide theoretical bounds on the optimality gap that arises from an AHM and
illustrate the efficacy of our results in a numerical example.</div><div><a href='http://arxiv.org/abs/2403.05715v1'>2403.05715v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02451v1")'>Automation of Smart Homes with Multiple Rule Sources</div>
<div id='2401.02451v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T14:40:05Z</div><div>Authors: Kaufman Eran, Yigal Hoffner</div><div style='padding-top: 10px; width: 80ex'>Using rules for home automation presents several challenges, especially when
considering multiple stakeholders in addition to residents, such as homeowners,
local authorities, energy suppliers, and system providers, who will wish to
contribute rules to safeguard their interests. Managing rules from various
sources requires a structured procedure, a relevant policy, and a designated
authority to ensure authorized and correct contributions and address potential
conflicts. In addition, the smart home rule language needs to express
conditions and decisions at a high level of abstraction without specifying
implementation details such as interfaces, access protocols, and room layout.
Decoupling high-level decisions from these details supports the transferability
and adaptability of rules to similar homes. This separation also has important
implications for structuring the smart home system and the security
architecture. Our proposed approach and system implementation introduce a rule
management process, a rule administrator, and a domain-specific rule language
to address these challenges. In addition, the system provides a learning
process that observes residents, detects behavior patterns, and derives rules
which are then presented as recommendations to the system.</div><div><a href='http://arxiv.org/abs/2401.02451v1'>2401.02451v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07945v1")'>A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology</div>
<div id='2403.07945v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T03:44:18Z</div><div>Authors: Bryce Allen Bagley</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue -- which we term Cognitive Security -- but applied
efforts have been limited. A major barrier hampering scientific and engineering
efforts to address Cognitive Security is the lack of a clear means of
describing and analyzing relevant problems. In this paper we develop Cognitive
Security, a mathematical framework which enables such description and analysis
by drawing on methods and results from multiple fields. We demonstrate certain
statistical properties which have significant implications for Cognitive
Security, and then present descriptions of the algorithmic problems faced by
attackers attempting to violate privacy and autonomy, and defenders attempting
to obstruct such attempts.</div><div><a href='http://arxiv.org/abs/2403.07945v1'>2403.07945v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10648v1")'>Area Modeling using Stay Information for Large-Scale Users and Analysis
  for Influence of COVID-19</div>
<div id='2401.10648v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T11:48:52Z</div><div>Authors: Kazuyuki Shoji, Shunsuke Aoki, Takuro Yonezawa, Nobuo Kawaguchi</div><div style='padding-top: 10px; width: 80ex'>Understanding how people use area in a city can be a valuable information in
a wide range of fields, from marketing to urban planning. Area usage is subject
to change over time due to various events including seasonal shifts and
pandemics. Before the spread of smartphones, this data had been collected
through questionnaire survey. However, this is not a sustainable approach in
terms of time to results and cost. There are many existing studies on area
modeling, which characterize an area with some kind of information, using Point
of Interest (POI) or inter-area movement data. However, since POI is data that
is statically tied to space, and inter-area movement data ignores the behavior
of people within an area, existing methods are not sufficient in terms of
capturing area usage changes. In this paper, we propose a novel area modeling
method named Area2Vec, inspired by Word2Vec, which models areas based on
people's location data. This method is based on the discovery that it is
possible to characterize an area based on its usage by using people's stay
information in the area. And it is a novel method that can reflect the
dynamically changing people's behavior in an area in the modeling results. We
validated Area2vec by performing a functional classification of areas in a
district of Japan. The results show that Area2Vec can be usable in general area
analysis. We also investigated area usage changes due to COVID-19 in two
districts in Japan. We could find that COVID-19 made people refrain from
unnecessary going out, such as visiting entertainment areas.</div><div><a href='http://arxiv.org/abs/2401.10648v1'>2401.10648v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14639v1")'>On Defining Smart Cities using Transformer Neural Networks</div>
<div id='2403.14639v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:34:24Z</div><div>Authors: Andrei Khurshudov</div><div style='padding-top: 10px; width: 80ex'>Cities worldwide are rapidly adopting smart technologies, transforming urban
life. Despite this trend, a universally accepted definition of 'smart city'
remains elusive. Past efforts to define it have not yielded a consensus, as
evidenced by the numerous definitions in use. In this paper, we endeavored to
create a new 'compromise' definition that should resonate with most experts
previously involved in defining this concept and aimed to validate one of the
existing definitions. We reviewed 60 definitions of smart cities from industry,
academia, and various relevant organizations, employing transformer
architecture-based generative AI and semantic text analysis to reach this
compromise. We proposed a semantic similarity measure as an evaluation
technique, which could generally be used to compare different smart city
definitions, assessing their uniqueness or resemblance. Our methodology
employed generative AI to analyze various existing definitions of smart cities,
generating a list of potential new composite definitions. Each of these new
definitions was then tested against the pre-existing individual definitions we
have gathered, using cosine similarity as our metric. This process identified
smart city definitions with the highest average cosine similarity, semantically
positioning them as the closest on average to all the 60 individual definitions
selected.</div><div><a href='http://arxiv.org/abs/2403.14639v1'>2403.14639v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07204v1")'>Synergizing Spatial Optimization with Large Language Models for
  Open-Domain Urban Itinerary Planning</div>
<div id='2402.07204v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T13:30:53Z</div><div>Authors: Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Kebing Hou, Dingyi Zhuang, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma</div><div style='padding-top: 10px; width: 80ex'>In this paper, we for the first time propose the task of Open-domain Urban
Itinerary Planning (OUIP) for citywalk, which directly generates itineraries
based on users' requests described in natural language. OUIP is different from
conventional itinerary planning, which limits users from expressing more
detailed needs and hinders true personalization. Recently, large language
models (LLMs) have shown potential in handling diverse tasks. However, due to
non-real-time information, incomplete knowledge, and insufficient spatial
awareness, they are unable to independently deliver a satisfactory user
experience in OUIP. Given this, we present ItiNera, an OUIP system that
synergizes spatial optimization with Large Language Models (LLMs) to provide
services that customize urban itineraries based on users' needs. Specifically,
we develop an LLM-based pipeline for extracting and updating POI features to
create a user-owned personalized POI database. For each user request, we
leverage LLM in cooperation with an embedding-based module for retrieving
candidate POIs from the user's POI database. Then, a spatial optimization
module is used to order these POIs, followed by LLM crafting a personalized,
spatially coherent itinerary. To the best of our knowledge, this study marks
the first integration of LLMs to innovate itinerary planning solutions.
Extensive experiments on offline datasets and online subjective evaluation have
demonstrated the capacities of our system to deliver more responsive and
spatially coherent itineraries than current LLM-based solutions. Our system has
been deployed in production at the TuTu online travel service and has attracted
thousands of users for their urban travel planning.</div><div><a href='http://arxiv.org/abs/2402.07204v1'>2402.07204v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05322v1")'>Arrival Time Prediction for Autonomous Shuttle Services in the Real
  World: Evidence from Five Cities</div>
<div id='2401.05322v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T18:41:39Z</div><div>Authors: Carolin Schmidt, Mathias Tygesen, Filipe Rodrigues</div><div style='padding-top: 10px; width: 80ex'>Urban mobility is on the cusp of transformation with the emergence of shared,
connected, and cooperative automated vehicles. Yet, for them to be accepted by
customers, trust in their punctuality is vital. Many pilot initiatives operate
without a fixed schedule, thus enhancing the importance of reliable arrival
time (AT) predictions. This study presents an AT prediction system for
autonomous shuttles, utilizing separate models for dwell and running time
predictions, validated on real-world data from five cities. Alongside
established methods such as XGBoost, we explore the benefits of integrating
spatial data using graph neural networks (GNN). To accurately handle the case
of a shuttle bypassing a stop, we propose a hierarchical model combining a
random forest classifier and a GNN. The results for the final AT prediction are
promising, showing low errors even when predicting several stops ahead. Yet, no
single model emerges as universally superior, and we provide insights into the
characteristics of pilot sites that influence the model selection process.
Finally, we identify dwell time prediction as the key determinant in overall AT
prediction accuracy when autonomous shuttles are deployed in low-traffic areas
or under regulatory speed limits. This research provides insights into the
current state of autonomous public transport prediction models and paves the
way for more data-informed decision-making as the field advances.</div><div><a href='http://arxiv.org/abs/2401.05322v1'>2401.05322v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04019v1")'>Exploring the Effects of Population and Employment Characteristics on
  Truck Flows: An Analysis of NextGen NHTS Origin-Destination Data</div>
<div id='2402.04019v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T15:47:01Z</div><div>Authors: Majbah Uddin, Yuandong Liu, Hyeonsup Lim</div><div style='padding-top: 10px; width: 80ex'>Truck transportation remains the dominant mode of US freight transportation
because of its advantages, such as the flexibility of accessing pickup and
drop-off points and faster delivery. Because of the massive freight volume
transported by trucks, understanding the effects of population and employment
characteristics on truck flows is critical for better transportation planning
and investment decisions. The US Federal Highway Administration published a
truck travel origin-destination data set as part of the Next Generation
National Household Travel Survey program. This data set contains the total
number of truck trips in 2020 within and between 583 predefined zones
encompassing metropolitan and nonmetropolitan statistical areas within each
state and Washington, DC. In this study, origin-destination-level truck trip
flow data was augmented to include zone-level population and employment
characteristics from the US Census Bureau. Census population and County
Business Patterns data were included. The final data set was used to train a
machine learning algorithm-based model, Extreme Gradient Boosting (XGBoost),
where the target variable is the number of total truck trips. Shapley Additive
ExPlanation (SHAP) was adopted to explain the model results. Results showed
that the distance between the zones was the most important variable and had a
nonlinear relationship with truck flows.</div><div><a href='http://arxiv.org/abs/2402.04019v1'>2402.04019v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04072v1")'>Forecasting and Mitigating Disruptions in Public Bus Transit Services</div>
<div id='2403.04072v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T22:06:21Z</div><div>Authors: Chaeeun Han, Jose Paolo Talusan, Dan Freudberg, Ayan Mukhopadhyay, Abhishek Dubey, Aron Laszka</div><div style='padding-top: 10px; width: 80ex'>Public transportation systems often suffer from unexpected fluctuations in
demand and disruptions, such as mechanical failures and medical emergencies.
These fluctuations and disruptions lead to delays and overcrowding, which are
detrimental to the passengers' experience and to the overall performance of the
transit service. To proactively mitigate such events, many transit agencies
station substitute (reserve) vehicles throughout their service areas, which
they can dispatch to augment or replace vehicles on routes that suffer
overcrowding or disruption. However, determining the optimal locations where
substitute vehicles should be stationed is a challenging problem due to the
inherent randomness of disruptions and due to the combinatorial nature of
selecting locations across a city. In collaboration with the transit agency of
Nashville, TN, we address this problem by introducing data-driven statistical
and machine-learning models for forecasting disruptions and an effective
randomized local-search algorithm for selecting locations where substitute
vehicles are to be stationed. Our research demonstrates promising results in
proactive disruption management, offering a practical and easily implementable
solution for transit agencies to enhance the reliability of their services. Our
results resonate beyond mere operational efficiency: by advancing proactive
strategies, our approach fosters more resilient and accessible public
transportation, contributing to equitable urban mobility and ultimately
benefiting the communities that rely on public transportation the most.</div><div><a href='http://arxiv.org/abs/2403.04072v1'>2403.04072v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07371v1")'>A Data-driven Resilience Framework of Directionality Configuration based
  on Topological Credentials in Road Networks</div>
<div id='2401.07371v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T21:22:22Z</div><div>Authors: H M Imran Kays, Khondhaker Al Momin, K. K. "Muralee" Muraleetharan, Arif Mohaimin Sadri</div><div style='padding-top: 10px; width: 80ex'>Roadway reconfiguration is a crucial aspect of transportation planning,
aiming to enhance traffic flow, reduce congestion, and improve overall road
network performance with existing infrastructure and resources. This paper
presents a novel roadway reconfiguration technique by integrating optimization
based Brute Force search approach and decision support framework to rank
various roadway configurations for better performance. The proposed framework
incorporates a multi-criteria decision analysis (MCDA) approach, combining
input from generated scenarios during the optimization process. By utilizing
data from optimization, the model identifies total betweenness centrality
(TBC), system travel time (STT), and total link traffic flow (TLTF) as the most
influential decision variables. The developed framework leverages graph theory
to model the transportation network topology and apply network science metrics
as well as stochastic user equilibrium traffic assignment to assess the impact
of each roadway configuration on the overall network performance. To rank the
roadway configurations, the framework employs machine learning algorithms, such
as ridge regression, to determine the optimal weights for each criterion (i.e.,
TBC, STT, TLTF). Moreover, the network-based analysis ensures that the selected
configurations not only optimize individual roadway segments but also enhance
system-level efficiency, which is particularly helpful as the increasing
frequency and intensity of natural disasters and other disruptive events
underscore the critical need for resilient transportation networks. By
integrating multi-criteria decision analysis, machine learning, and network
science metrics, the proposed framework would enable transportation planners to
make informed and data-driven decisions, leading to more sustainable,
efficient, and resilient roadway configurations.</div><div><a href='http://arxiv.org/abs/2401.07371v1'>2401.07371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12074v2")'>Beyond Quantities: Machine Learning-based Characterization of Inequality
  in Infrastructure Quality Provision in Cities</div>
<div id='2403.12074v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T17:23:16Z</div><div>Authors: Bo Li, Ali Mostafavi</div><div style='padding-top: 10px; width: 80ex'>The objective of this study is to characterize inequality in infrastructure
quality across urban areas. While a growing of body of literature has
recognized the importance of characterizing infrastructure inequality in cities
and provided quantified metrics to inform urban development plans, the majority
of the existing approaches focus primarily on measuring the quantity of
infrastructure, assuming that more infrastructure is better. Also, the existing
research focuses primarily on index-based approaches in which the status of
infrastructure provision in urban areas is determined based on assumed
subjective weights. The focus on infrastructure quantity and use of indices
obtained from subjective weights has hindered the ability to properly examine
infrastructure inequality as it pertains to urban inequality and environmental
justice considerations. Recognizing this gap, we propose a machine
learning-based approach in which infrastructure features that shape
environmental hazard exposure are identified and we use the weights obtained by
the model to calculate an infrastructure quality provision for spatial areas of
cities and accordingly, quantify the extent of inequality in infrastructure
quality. The implementation of the model in five metropolitan areas in the U.S.
demonstrates the capability of the proposed approach in characterizing
inequality in infrastructure quality and capturing city-specific differences in
the weights of infrastructure features. The results also show that areas in
which low-income populations reside have lower infrastructure quality
provision, suggesting the lower infrastructure quality provision as a
determinant of urban disparities. Accordingly, the proposed approach can be
effectively used to inform integrated urban design strategies to promote
infrastructure equity and environmental justice based on data-driven and
machine intelligence-based insights.</div><div><a href='http://arxiv.org/abs/2403.12074v2'>2403.12074v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14671v1")'>Understanding the Transit Gap: A Comparative Study of On-Demand Bus
  Services and Urban Climate Resilience in South End, Charlotte, NC and
  Avondale, Chattanooga, TN</div>
<div id='2403.14671v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:50:21Z</div><div>Authors: Sanaz Sadat Hosseini, Babak Rahimi Ardabili, Mona Azarbayjani, Srinivas Pulugurtha, Hamed Tabkhi</div><div style='padding-top: 10px; width: 80ex'>Urban design significantly impacts sustainability, particularly in the
context of public transit efficiency and carbon emissions reduction. This study
explores two neighborhoods with distinct urban designs: South End, Charlotte,
NC, featuring a dynamic mixed-use urban design pattern, and Avondale,
Chattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM
tool, we assess the impact of increased bus utilization in these different
urban settings on traffic and CO2 emissions. Our results highlight the critical
role of urban design and planning in transit system efficiency. In South End,
the mixed-use design led to more substantial emission reductions, indicating
that urban layout can significantly influence public transit outcomes. Tailored
strategies that consider the unique urban design elements are essential for
climate resilience. Notably, doubling bus utilization decreased daily emissions
by 10.18% in South End and 8.13% in Avondale, with a corresponding reduction in
overall traffic. A target of 50% bus utilization saw emissions drop by 21.45%
in South End and 14.50% in Avondale. At an idealistic goal of 70% bus
utilization, South End and Avondale witnessed emission reductions of 37.22% and
27.80%, respectively. These insights are crucial for urban designers and
policymakers in developing sustainable urban landscapes.</div><div><a href='http://arxiv.org/abs/2403.14671v1'>2403.14671v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06466v1")'>RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling
  Approach</div>
<div id='2403.06466v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T07:07:05Z</div><div>Authors: Yingzhuo Liu</div><div style='padding-top: 10px; width: 80ex'>Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational
cost of bus company and guarantee service quality for passengers. Existing
approaches typically generate a bus scheduling scheme in an offline manner and
then schedule buses according to the scheme. In practice, uncertain events such
as traffic congestion occur frequently, which may make the pre-determined bus
scheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov
Decision Process (MDP). A Reinforcement Learning-based Multi-line bus
Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline
and online phases. At the offline phase, deadhead decision is integrated into
bus selection decision for the first time to simplify the learning problem. At
the online phase, deadhead decision is made through a time window mechanism
based on the policy learned at the offline phase. We develop several new and
useful state features including the features for control points, bus lines and
buses. A bus priority screening mechanism is invented to construct bus-related
features. Considering the interests of both the bus company and passengers, a
reward function combining the final reward and the step-wise reward is devised.
Experiments at the offline phase demonstrate that the number of buses used of
RL-MSA is decreased compared with offline optimization approaches. At the
online phase, RL-MSA can cover all departure times in a timetable (i.e.,
service quality) without increasing the number of buses used (i.e., operational
cost).</div><div><a href='http://arxiv.org/abs/2403.06466v1'>2403.06466v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07003v1")'>Evacuation Management Framework towards Smart City-wide Intelligent
  Emergency Interactive Response System</div>
<div id='2403.07003v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T12:10:19Z</div><div>Authors: Anuj Abraham, Yi Zhang, Shitala Prasad</div><div style='padding-top: 10px; width: 80ex'>A smart city solution toward future 6G network deployment allows small and
medium sized enterprises (SMEs), industry, and government entities to connect
with the infrastructures and play a crucial role in enhancing emergency
preparedness with advanced sensors. The objective of this work is to propose a
set of coordinated technological solutions to transform an existing emergency
response system into an intelligent interactive system, thereby improving the
public services and the quality of life for residents at home, on road, in
hospitals, transport hubs, etc. In this context, we consider a city wide view
from three different application scenes that are closely related to peoples
daily life, to optimize the actions taken at relevant departments. Therefore,
using artificial intelligence (AI) and machine learning (ML) techniques to
enable the next generation connected vehicle experiences, we specifically focus
on accidents happening in indoor households, urban roads, and at large public
facilities. This smart interactive response system will benefit from advanced
sensor fusion and AI by formulating a real time dynamic model.</div><div><a href='http://arxiv.org/abs/2403.07003v1'>2403.07003v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02146v1")'>Emergency Computing: An Adaptive Collaborative Inference Method Based on
  Hierarchical Reinforcement Learning</div>
<div id='2402.02146v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T13:28:35Z</div><div>Authors: Weiqi Fu, Lianming Xu, Xin Wu, Li Wang, Aiguo Fei</div><div style='padding-top: 10px; width: 80ex'>In achieving effective emergency response, the timely acquisition of
environmental information, seamless command data transmission, and prompt
decision-making are crucial. This necessitates the establishment of a resilient
emergency communication dedicated network, capable of providing communication
and sensing services even in the absence of basic infrastructure. In this
paper, we propose an Emergency Network with Sensing, Communication,
Computation, Caching, and Intelligence (E-SC3I). The framework incorporates
mechanisms for emergency computing, caching, integrated communication and
sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large
user base, reliable data transmission over unstable links, and dynamic network
deployment in a changing environment. However, these advantages come at the
cost of significant computation overhead. Therefore, we specifically
concentrate on emergency computing and propose an adaptive collaborative
inference method (ACIM) based on hierarchical reinforcement learning.
Experimental results demonstrate our method's ability to achieve rapid
inference of AI models with constrained computational and communication
resources.</div><div><a href='http://arxiv.org/abs/2402.02146v1'>2402.02146v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14941v1")'>Unifying Lane-Level Traffic Prediction from a Graph Structural
  Perspective: Benchmark and Baseline</div>
<div id='2403.14941v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T04:21:40Z</div><div>Authors: Shuhao Li, Yue Cui, Jingyi Xu, Libin Li, Lingkai Meng, Weidong Yang, Fan Zhang, Xiaofang Zhou</div><div style='padding-top: 10px; width: 80ex'>Traffic prediction has long been a focal and pivotal area in research,
witnessing both significant strides from city-level to road-level predictions
in recent years. With the advancement of Vehicle-to-Everything (V2X)
technologies, autonomous driving, and large-scale models in the traffic domain,
lane-level traffic prediction has emerged as an indispensable direction.
However, further progress in this field is hindered by the absence of
comprehensive and unified evaluation standards, coupled with limited public
availability of data and code. This paper extensively analyzes and categorizes
existing research in lane-level traffic prediction, establishes a unified
spatial topology structure and prediction tasks, and introduces a simple
baseline model, GraphMLP, based on graph structure and MLP networks. We have
replicated codes not publicly available in existing studies and, based on this,
thoroughly and fairly assessed various models in terms of effectiveness,
efficiency, and applicability, providing insights for practical applications.
Additionally, we have released three new datasets and corresponding codes to
accelerate progress in this field, all of which can be found on
https://github.com/ShuhaoLii/TITS24LaneLevel-Traffic-Benchmark.</div><div><a href='http://arxiv.org/abs/2403.14941v1'>2403.14941v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10155v2")'>A novel hybrid time-varying graph neural network for traffic flow
  forecasting</div>
<div id='2401.10155v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T07:21:36Z</div><div>Authors: Ben Ao Dai, Bao-Lin Ye</div><div style='padding-top: 10px; width: 80ex'>Real-time and accurate traffic flow prediction is the foundation for ensuring
the efficient operation of intelligent transportation systems.In existing
traffic flow prediction methods based on graph neural networks (GNNs),
pre-defined graphs were usually used to describe the spatial correlations of
different traffic nodes in urban road networks. However, the ability of
pre-defined graphs used to describe spatial correlation was limited by prior
knowledge and graph generation methods. Although time-varying graphs based on
data-driven learning can partially overcome the drawbacks of pre-defined
graphs, the learning ability of existing adaptive graphs was limited. For
example, time-varying graphs cannot adequately capture the inherent spatial
correlations in traffic flow data.In order to solve these problems, we have
proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow
prediction.</div><div><a href='http://arxiv.org/abs/2401.10155v2'>2401.10155v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14695v1")'>Continuously Evolving Graph Neural Controlled Differential Equations for
  Traffic Forecasting</div>
<div id='2401.14695v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T07:36:11Z</div><div>Authors: Jiajia Wu, Ling Chen</div><div style='padding-top: 10px; width: 80ex'>As a crucial technique for developing a smart city, traffic forecasting has
become a popular research focus in academic and industrial communities for
decades. This task is highly challenging due to complex and dynamic
spatial-temporal dependencies in traffic networks. Existing works ignore
continuous temporal dependencies and spatial dependencies evolving over time.
In this paper, we propose Continuously Evolving Graph Neural Controlled
Differential Equations (CEGNCDE) to capture continuous temporal dependencies
and spatial dependencies over time simultaneously. Specifically, a continuously
evolving graph generator (CEGG) based on NCDE is introduced to generate the
spatial dependencies graph that continuously evolves over time from discrete
historical observations. Then, a graph neural controlled differential equations
(GNCDE) framework is introduced to capture continuous temporal dependencies and
spatial dependencies over time simultaneously. Extensive experiments
demonstrate that CEGNCDE outperforms the SOTA methods by average 2.34% relative
MAE reduction, 0.97% relative RMSE reduction, and 3.17% relative MAPE
reduction.</div><div><a href='http://arxiv.org/abs/2401.14695v1'>2401.14695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01231v2")'>Unveiling Delay Effects in Traffic Forecasting: A Perspective from
  Spatial-Temporal Delay Differential Equations</div>
<div id='2402.01231v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T08:55:23Z</div><div>Authors: Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang, Yuanchun Zhou</div><div style='padding-top: 10px; width: 80ex'>Traffic flow forecasting is a fundamental research issue for transportation
planning and management, which serves as a canonical and typical example of
spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and
Recurrent Neural Networks (RNNs) have achieved great success in capturing
spatial-temporal correlations for traffic flow forecasting. Yet, two
non-ignorable issues haven't been well solved: 1) The message passing in GNNs
is immediate, while in reality the spatial message interactions among
neighboring nodes can be delayed. The change of traffic flow at one node will
take several minutes, i.e., time delay, to influence its connected neighbors.
2) Traffic conditions undergo continuous changes. The prediction frequency for
traffic flow forecasting may vary based on specific scenario requirements. Most
existing discretized models require retraining for each prediction horizon,
restricting their applicability. To tackle the above issues, we propose a
neural Spatial-Temporal Delay Differential Equation model, namely STDDE. It
includes both delay effects and continuity into a unified delay differential
equation framework, which explicitly models the time delay in spatial
information propagation. Furthermore, theoretical proofs are provided to show
its stability. Then we design a learnable traffic-graph time-delay estimator,
which utilizes the continuity of the hidden states to achieve the gradient
backward process. Finally, we propose a continuous output module, allowing us
to accurately predict traffic flow at various frequencies, which provides more
flexibility and adaptability to different scenarios. Extensive experiments show
the superiority of the proposed STDDE along with competitive computational
efficiency.</div><div><a href='http://arxiv.org/abs/2402.01231v2'>2402.01231v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08119v2")'>SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic
  Spatio-Temporal Traffic Forecasting</div>
<div id='2401.08119v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T05:23:34Z</div><div>Authors: Lequan Lin, Dai Shi, Andi Han, Junbin Gao</div><div style='padding-top: 10px; width: 80ex'>Traffic forecasting, a crucial application of spatio-temporal graph (STG)
learning, has traditionally relied on deterministic models for accurate point
estimations. Yet, these models fall short of identifying latent risks of
unexpected volatility in future observations. To address this gap,
probabilistic methods, especially variants of diffusion models, have emerged as
uncertainty-aware solutions. However, existing diffusion methods typically
focus on generating separate future time series for individual sensors in the
traffic network, resulting in insufficient involvement of spatial network
characteristics in the probabilistic learning process. To better leverage
spatial dependencies and systematic patterns inherent in traffic data, we
propose SpecSTG, a novel spectral diffusion framework. Our method generates the
Fourier representation of future time series, transforming the learning process
into the spectral domain enriched with spatial information. Additionally, our
approach incorporates a fast spectral graph convolution designed for Fourier
input, alleviating the computational burden associated with existing models.
Numerical experiments show that SpecSTG achieves outstanding performance with
traffic flow and traffic speed datasets compared to state-of-the-art baselines.
The source code for SpecSTG is available at
https://anonymous.4open.science/r/SpecSTG.</div><div><a href='http://arxiv.org/abs/2401.08119v2'>2401.08119v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02354v1")'>Spatio-Temporal Field Neural Networks for Air Quality Inference</div>
<div id='2403.02354v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T10:14:42Z</div><div>Authors: Yutong Feng, Qiongyan Wang, Yutong Xia, Junlin Huang, Siru Zhong, Kun Wang, Shifen Cheng, Yuxuan Liang</div><div style='padding-top: 10px; width: 80ex'>The air quality inference problem aims to utilize historical data from a
limited number of observation sites to infer the air quality index at an
unknown location. Considering the sparsity of data due to the high maintenance
cost of the stations, good inference algorithms can effectively save the cost
and refine the data granularity. While spatio-temporal graph neural networks
have made excellent progress on this problem, their non-Euclidean and discrete
data structure modeling of reality limits its potential. In this work, we make
the first attempt to combine two different spatio-temporal perspectives, fields
and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and
its corresponding new framework, Pyramidal Inference. Extensive experiments
validate that our model achieves state-of-the-art performance in nationwide air
quality inference in the Chinese Mainland, demonstrating the superiority of our
proposed model and framework.</div><div><a href='http://arxiv.org/abs/2403.02354v1'>2403.02354v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08727v2")'>MA2GCN: Multi Adjacency relationship Attention Graph Convolutional
  Networks for Traffic Prediction using Trajectory data</div>
<div id='2401.08727v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:22:44Z</div><div>Authors: Zhengke Sun, Yuliang Ma</div><div style='padding-top: 10px; width: 80ex'>The problem of traffic congestion not only causes a large amount of economic
losses, but also seriously endangers the urban environment. Predicting traffic
congestion has important practical significance. So far, most studies have been
based on historical data from sensors placed on different roads to predict
future traffic flow and speed, to analyze the traffic congestion conditions of
a certain road segment. However, due to the fixed position of sensors, it is
difficult to mine new information. On the other hand, vehicle trajectory data
is more flexible and can extract traffic information as needed. Therefore, we
proposed a new traffic congestion prediction model - Multi Adjacency
relationship Attention Graph Convolutional Networks(MA2GCN). This model
transformed vehicle trajectory data into graph structured data in grid form,
and proposed a vehicle entry and exit matrix based on the mobility between
different grids. At the same time, in order to improve the performance of the
model, this paper also built a new adaptive adjacency matrix generation method
and adjacency matrix attention module. This model mainly used gated temporal
convolution and graph convolution to extract temporal and spatial information,
respectively. Compared with multiple baselines, our model achieved the best
performance on Shanghai taxi GPS trajectory dataset. The code is available at
https://github.com/zachysun/Taxi_Traffic_Benchmark.</div><div><a href='http://arxiv.org/abs/2401.08727v2'>2401.08727v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.11037v1")'>Equivariant Graph Neural Operator for Modeling 3D Dynamics</div>
<div id='2401.11037v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T21:50:32Z</div><div>Authors: Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaifi, Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, Anima Anandkumar</div><div style='padding-top: 10px; width: 80ex'>Modeling the complex three-dimensional (3D) dynamics of relational systems is
an important problem in the natural sciences, with applications ranging from
molecular simulations to particle mechanics. Machine learning methods have
achieved good success by learning graph neural networks to model spatial
interactions. However, these approaches do not faithfully capture temporal
correlations since they only model next-step predictions. In this work, we
propose Equivariant Graph Neural Operator (EGNO), a novel and principled method
that directly models dynamics as trajectories instead of just next-step
prediction. Different from existing methods, EGNO explicitly learns the
temporal evolution of 3D dynamics where we formulate the dynamics as a function
over time and learn neural operators to approximate it. To capture the temporal
correlations while keeping the intrinsic SE(3)-equivariance, we develop
equivariant temporal convolutions parameterized in the Fourier space and build
EGNO by stacking the Fourier layers over equivariant networks. EGNO is the
first operator learning framework that is capable of modeling solution dynamics
functions over time while retaining 3D equivariance. Comprehensive experiments
in multiple domains, including particle simulations, human motion capture, and
molecular dynamics, demonstrate the significantly superior performance of EGNO
against existing methods, thanks to the equivariant temporal modeling.</div><div><a href='http://arxiv.org/abs/2401.11037v1'>2401.11037v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12196v1")'>Learning Dynamics from Multicellular Graphs with Deep Neural Networks</div>
<div id='2401.12196v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:36:29Z</div><div>Authors: Haiqian Yang, Florian Meyer, Shaoxun Huang, Liu Yang, Cristiana Lungu, Monilola A. Olayioye, Markus J. Buehler, Ming Guo</div><div style='padding-top: 10px; width: 80ex'>The inference of multicellular self-assembly is the central quest of
understanding morphogenesis, including embryos, organoids, tumors, and many
others. However, it has been tremendously difficult to identify structural
features that can indicate multicellular dynamics. Here we propose to harness
the predictive power of graph-based deep neural networks (GNN) to discover
important graph features that can predict dynamics. To demonstrate, we apply a
physically informed GNN (piGNN) to predict the motility of multicellular
collectives from a snapshot of their positions both in experiments and
simulations. We demonstrate that piGNN is capable of navigating through complex
graph features of multicellular living systems, which otherwise can not be
achieved by classical mechanistic models. With increasing amounts of
multicellular data, we propose that collaborative efforts can be made to create
a multicellular data bank (MDB) from which it is possible to construct a large
multicellular graph model (LMGM) for general-purposed predictions of
multicellular organization.</div><div><a href='http://arxiv.org/abs/2401.12196v1'>2401.12196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09039v1")'>Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly
  Detection in Dynamic Graphs</div>
<div id='2403.09039v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T02:26:10Z</div><div>Authors: Jie Liu, Xuequn Shang, Xiaolin Han, Wentao Zhang, Hongzhi Yin</div><div style='padding-top: 10px; width: 80ex'>Anomaly detection in dynamic graphs presents a significant challenge due to
the temporal evolution of graph structures and attributes. The conventional
approaches that tackle this problem typically employ an unsupervised learning
framework, capturing normality patterns with exclusive normal data during
training and identifying deviations as anomalies during testing. However, these
methods face critical drawbacks: they either only depend on proxy tasks for
general representation without directly pinpointing normal patterns, or they
neglect to differentiate between spatial and temporal normality patterns,
leading to diminished efficacy in anomaly detection. To address these
challenges, we introduce a novel Spatial-Temporal memories-enhanced graph
autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs)
and gated temporal convolution layers to extract spatial features and temporal
features, respectively. Then STRIPE incorporates separate spatial and temporal
memory networks, which capture and store prototypes of normal patterns, thereby
preserving the uniqueness of spatial and temporal normality. After that,
through a mutual attention mechanism, these stored patterns are then retrieved
and integrated with encoded graph embeddings. Finally, the integrated features
are fed into the decoder to reconstruct the graph streams which serve as the
proxy task for anomaly detection. This comprehensive approach not only
minimizes reconstruction errors but also refines the model by emphasizing the
compactness and distinctiveness of the embeddings in relation to the nearest
memory prototypes. Through extensive testing, STRIPE has demonstrated a
superior capability to discern anomalies by effectively leveraging the distinct
spatial and temporal dynamics of dynamic graphs, significantly outperforming
existing methodologies, with an average improvement of 15.39% on AUC values.</div><div><a href='http://arxiv.org/abs/2403.09039v1'>2403.09039v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13872v1")'>Edge Conditional Node Update Graph Neural Network for Multi-variate Time
  Series Anomaly Detection</div>
<div id='2401.13872v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T00:47:44Z</div><div>Authors: Hayoung Jo, Seong-Whan Lee</div><div style='padding-top: 10px; width: 80ex'>With the rapid advancement in cyber-physical systems, the increasing number
of sensors has significantly complicated manual monitoring of system states.
Consequently, graph-based time-series anomaly detection methods have gained
attention due to their ability to explicitly represent relationships between
sensors. However, these methods often apply a uniform source node
representation across all connected target nodes, even when updating different
target node representations. Moreover, the graph attention mechanism, commonly
used to infer unknown graph structures, could constrain the diversity of source
node representations. In this paper, we introduce the Edge Conditional
Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge
conditional node update module, dynamically transforms source node
representations based on connected edges to represent target nodes aptly. We
validate performance on three real-world datasets: SWaT, WADI, and PSM. Our
model demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively,
compared to best F1 baseline models.</div><div><a href='http://arxiv.org/abs/2401.13872v1'>2401.13872v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10220v1")'>From Chaos to Clarity: Time Series Anomaly Detection in Astronomical
  Observations</div>
<div id='2403.10220v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T11:39:12Z</div><div>Authors: Xinli Hao, Yile Chen, Chen Yang, Zhihui Du, Chaohong Ma, Chao Wu, Xiaofeng Meng</div><div style='padding-top: 10px; width: 80ex'>With the development of astronomical facilities, large-scale time series data
observed by these facilities is being collected. Analyzing anomalies in these
astronomical observations is crucial for uncovering potential celestial events
and physical phenomena, thus advancing the scientific research process.
However, existing time series anomaly detection methods fall short in tackling
the unique characteristics of astronomical observations where each star is
inherently independent but interfered by random concurrent noise, resulting in
a high rate of false alarms. To overcome the challenges, we propose AERO, a
novel two-stage framework tailored for unsupervised anomaly detection in
astronomical observations. In the first stage, we employ a Transformer-based
encoder-decoder architecture to learn the normal temporal patterns on each
variate (i.e., star) in alignment with the characteristic of variate
independence. In the second stage, we enhance the graph neural network with a
window-wise graph structure learning to tackle the occurrence of concurrent
noise characterized by spatial and temporal randomness. In this way, AERO is
not only capable of distinguishing normal temporal patterns from potential
anomalies but also effectively differentiating concurrent noise, thus
decreasing the number of false alarms. We conducted extensive experiments on
three synthetic datasets and three real-world datasets. The results demonstrate
that AERO outperforms the compared baselines. Notably, compared to the
state-of-the-art model, AERO improves the F1-score by up to 8.76% and 2.63% on
synthetic and real-world datasets respectively.</div><div><a href='http://arxiv.org/abs/2403.10220v1'>2403.10220v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05800v1")'>Graph Spatiotemporal Process for Multivariate Time Series Anomaly
  Detection with Missing Values</div>
<div id='2401.05800v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T10:10:16Z</div><div>Authors: Yu Zheng, Huan Yee Koh, Ming Jin, Lianhua Chi, Haishuai Wang, Khoa T. Phan, Yi-Ping Phoebe Chen, Shirui Pan, Wei Xiang</div><div style='padding-top: 10px; width: 80ex'>The detection of anomalies in multivariate time series data is crucial for
various practical applications, including smart power grids, traffic flow
forecasting, and industrial process control. However, real-world time series
data is usually not well-structured, posting significant challenges to existing
approaches: (1) The existence of missing values in multivariate time series
data along variable and time dimensions hinders the effective modeling of
interwoven spatial and temporal dependencies, resulting in important patterns
being overlooked during model training; (2) Anomaly scoring with
irregularly-sampled observations is less explored, making it difficult to use
existing detectors for multivariate series without fully-observed values. In
this work, we introduce a novel framework called GST-Pro, which utilizes a
graph spatiotemporal process and anomaly scorer to tackle the aforementioned
challenges in detecting anomalies on irregularly-sampled multivariate time
series. Our approach comprises two main components. First, we propose a graph
spatiotemporal process based on neural controlled differential equations. This
process enables effective modeling of multivariate time series from both
spatial and temporal perspectives, even when the data contains missing values.
Second, we present a novel distribution-based anomaly scoring mechanism that
alleviates the reliance on complete uniform observations. By analyzing the
predictions of the graph spatiotemporal process, our approach allows anomalies
to be easily detected. Our experimental results show that the GST-Pro method
can effectively detect anomalies in time series data and outperforms
state-of-the-art methods, regardless of whether there are missing values
present in the data. Our code is available: https://github.com/huankoh/GST-Pro.</div><div><a href='http://arxiv.org/abs/2401.05800v1'>2401.05800v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02616v1")'>Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive
  Anomaly Diagnosis of Industrial Cyber-physical Systems</div>
<div id='2403.02616v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T03:11:02Z</div><div>Authors: Haili Sun, Yan Huang, Lansheng Han, Cai Fu, Chunjie Zhou</div><div style='padding-top: 10px; width: 80ex'>Accurate detection and diagnosis of abnormal behaviors such as network
attacks from multivariate time series (MTS) are crucial for ensuring the stable
and effective operation of industrial cyber-physical systems (CPS). However,
existing researches pay little attention to the logical dependencies among
system working states, and have difficulties in explaining the evolution
mechanisms of abnormal signals. To reveal the spatio-temporal association
relationships and evolution mechanisms of the working states of industrial CPS,
this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e.
MAD-Transformer) to identify and diagnose anomalies in MTS. MAD-Transformer
first constructs a temporal state matrix to characterize and estimate the
change patterns of the system states in the temporal dimension. Then, to better
locate the anomalies, a spatial state matrix is also constructed to capture the
inter-sensor state correlation relationships within the system. Subsequently,
based on these two types of state matrices, a three-branch structure of
series-temporal-spatial attention module is designed to simultaneously capture
the series, temporal, and space dependencies among MTS. Afterwards, three
associated alignment loss functions and a reconstruction loss are constructed
to jointly optimize the model. Finally, anomalies are determined and diagnosed
by comparing the residual matrices with the original matrices. We conducted
comparative experiments on five publicly datasets spanning three application
domains (service monitoring, spatial and earth exploration, and water
treatment), along with a petroleum refining simulation dataset collected by
ourselves. The results demonstrate that MAD-Transformer can adaptively detect
fine-grained anomalies with short duration, and outperforms the
state-of-the-art baselines in terms of noise robustness and localization
performance.</div><div><a href='http://arxiv.org/abs/2403.02616v1'>2403.02616v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11773v2")'>Dynamic Multi-Network Mining of Tensor Time Series</div>
<div id='2402.11773v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:06:04Z</div><div>Authors: Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai</div><div style='padding-top: 10px; width: 80ex'>Subsequence clustering of time series is an essential task in data mining,
and interpreting the resulting clusters is also crucial since we generally do
not have prior knowledge of the data. Thus, given a large collection of tensor
time series consisting of multiple modes, including timestamps, how can we
achieve subsequence clustering for tensor time series and provide interpretable
insights? In this paper, we propose a new method, Dynamic Multi-network Mining
(DMM), that converts a tensor time series into a set of segment groups of
various lengths (i.e., clusters) characterized by a dependency network
constrained with l1-norm. Our method has the following properties. (a)
Interpretable: it characterizes the cluster with multiple networks, each of
which is a sparse dependency network of a corresponding non-temporal mode, and
thus provides visible and interpretable insights into the key relationships.
(b) Accurate: it discovers the clusters with distinct networks from tensor time
series according to the minimum description length (MDL). (c) Scalable: it
scales linearly in terms of the input data size when solving a non-convex
problem to optimize the number of segments and clusters, and thus it is
applicable to long-range and high-dimensional tensors. Extensive experiments
with synthetic datasets confirm that our method outperforms the
state-of-the-art methods in terms of clustering accuracy. We then use real
datasets to demonstrate that DMM is useful for providing interpretable insights
from tensor time series.</div><div><a href='http://arxiv.org/abs/2402.11773v2'>2402.11773v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08470v1")'>Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic
  Degradation Analysis at Scale</div>
<div id='2402.08470v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T14:00:59Z</div><div>Authors: Yangxin Fan, Raymond Wieser, Laura Bruckman, Roger French, Yinghui Wu</div><div style='padding-top: 10px; width: 80ex'>We propose a novel Spatio-Temporal Graph Neural Network empowered trend
analysis approach (ST-GTrend) to perform fleet-level performance degradation
analysis for Photovoltaic (PV) power networks. PV power stations have become an
integral component to the global sustainable energy production landscape.
Accurately estimating the performance of PV systems is critical to their
feasibility as a power generation technology and as a financial asset. One of
the most challenging problems in assessing the Levelized Cost of Energy (LCOE)
of a PV system is to understand and estimate the long-term Performance Loss
Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates
spatio-temporal coherence and graph attention to separate PLR as a long-term
"aging" trend from multiple fluctuation terms in the PV input data. To cope
with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled
graph autoencoder array to extract aging and fluctuation terms simultaneously.
ST-GTrend imposes flatness and smoothness regularization to ensure the
disentanglement between aging and fluctuation. To scale the analysis to large
PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate
the training and inference of ST-GTrend. We have evaluated ST-GTrend on three
large-scale PV datasets, spanning a time period of 10 years. Our results show
that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean
Distances by 34.74% and 33.66% compared to the SOTA methods. Our results
demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times. We
further verify the generality and effectiveness of ST-GTrend for trend analysis
using financial and economic datasets.</div><div><a href='http://arxiv.org/abs/2402.08470v1'>2402.08470v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01733v1")'>Investigating the Suitability of Concept Drift Detection for Detecting
  Leakages in Water Distribution Networks</div>
<div id='2401.01733v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T13:12:04Z</div><div>Authors: Valerie Vaquet, Fabian Hinder, Barbara Hammer</div><div style='padding-top: 10px; width: 80ex'>Leakages are a major risk in water distribution networks as they cause water
loss and increase contamination risks. Leakage detection is a difficult task
due to the complex dynamics of water distribution networks. In particular,
small leakages are hard to detect. From a machine-learning perspective,
leakages can be modeled as concept drift. Thus, a wide variety of drift
detection schemes seems to be a suitable choice for detecting leakages. In this
work, we explore the potential of model-loss-based and distribution-based drift
detection methods to tackle leakage detection. We additionally discuss the
issue of temporal dependencies in the data and propose a way to cope with it
when applying distribution-based detection. We evaluate different methods
systematically for leakages of different sizes and detection times.
Additionally, we propose a first drift-detection-based technique for localizing
leakages.</div><div><a href='http://arxiv.org/abs/2401.01733v1'>2401.01733v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.11271v1")'>DACR: Distribution-Augmented Contrastive Reconstruction for Time-Series
  Anomaly Detection</div>
<div id='2401.11271v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T16:56:52Z</div><div>Authors: Lixu Wang, Shichao Xu, Xinyu Du, Qi Zhu</div><div style='padding-top: 10px; width: 80ex'>Anomaly detection in time-series data is crucial for identifying faults,
failures, threats, and outliers across a range of applications. Recently, deep
learning techniques have been applied to this topic, but they often struggle in
real-world scenarios that are complex and highly dynamic, e.g., the normal data
may consist of multiple distributions, and various types of anomalies may
differ from the normal data to different degrees. In this work, to tackle these
challenges, we propose Distribution-Augmented Contrastive Reconstruction
(DACR). DACR generates extra data disjoint from the normal data distribution to
compress the normal data's representation space, and enhances the feature
extractor through contrastive learning to better capture the intrinsic
semantics from time-series data. Furthermore, DACR employs an attention
mechanism to model the semantic dependencies among multivariate time-series
features, thereby achieving more robust reconstruction for anomaly detection.
Extensive experiments conducted on nine benchmark datasets in various anomaly
detection scenarios demonstrate the effectiveness of DACR in achieving new
state-of-the-art time-series anomaly detection.</div><div><a href='http://arxiv.org/abs/2401.11271v1'>2401.11271v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03322v1")'>Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly
  Detection</div>
<div id='2401.03322v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T22:55:02Z</div><div>Authors: Seyed Amirhossein Najafi, Mohammad Hassan Asemani, Peyman Setoodeh</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a hybrid attention and autoencoder (AE) model for
unsupervised online anomaly detection in time series. The autoencoder captures
local structural patterns in short embeddings, while the attention model learns
long-term features, facilitating parallel computing with positional encoding.
Unique in its approach, our proposed hybrid model combines attention and
autoencoder for the first time in time series anomaly detection. It employs an
attention-based mechanism, akin to the deep transformer model, with key
architectural modifications for predicting the next time step window in the
autoencoder's latent space. The model utilizes a threshold from the validation
dataset for anomaly detection and introduces an alternative method based on
analyzing the first statistical moment of error, improving accuracy without
dependence on a validation dataset. Evaluation on diverse real-world benchmark
datasets and comparing with other well-established models, confirms the
effectiveness of our proposed model in anomaly detection.</div><div><a href='http://arxiv.org/abs/2401.03322v1'>2401.03322v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02429v1")'>Towards efficient deep autoencoders for multivariate time series anomaly
  detection</div>
<div id='2403.02429v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:22:09Z</div><div>Authors: Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo</div><div style='padding-top: 10px; width: 80ex'>Multivariate time series anomaly detection is a crucial problem in many
industrial and research applications. Timely detection of anomalies allows, for
instance, to prevent defects in manufacturing processes and failures in
cyberphysical systems. Deep learning methods are preferred among others for
their accuracy and robustness for the analysis of complex multivariate data.
However, a key aspect is being able to extract predictions in a timely manner,
to accommodate real-time requirements in different applications. In the case of
deep learning models, model reduction is extremely important to achieve optimal
results in real-time systems with limited time and memory constraints. In this
paper, we address this issue by proposing a novel compression method for deep
autoencoders that involves three key factors. First, pruning reduces the number
of weights, while preventing catastrophic drops in accuracy by means of a fast
search process that identifies high sparsity levels. Second, linear and
non-linear quantization reduces model complexity by reducing the number of bits
for every single weight. The combined contribution of these three aspects allow
the model size to be reduced, by removing a subset of the weights (pruning),
and decreasing their bit-width (quantization). As a result, the compressed
model is faster and easier to adopt in highly constrained hardware
environments. Experiments performed on popular multivariate anomaly detection
benchmarks, show that our method is capable of achieving significant model
compression ratio (between 80% and 95%) without a significant reduction in the
anomaly detection performance.</div><div><a href='http://arxiv.org/abs/2403.02429v1'>2403.02429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05943v1")'>A hybrid IndRNNLSTM approach for real-time anomaly detection in
  software-defined networks</div>
<div id='2402.05943v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:41:55Z</div><div>Authors: Sajjad Salem, Salman Asoudeh</div><div style='padding-top: 10px; width: 80ex'>Anomaly detection in SDN using data flow prediction is a difficult task. This
problem is included in the category of time series and regression problems.
Machine learning approaches are challenging in this field due to the manual
selection of features. On the other hand, deep learning approaches have
important features due to the automatic selection of features. Meanwhile,
RNN-based approaches have been used the most. The LSTM and GRU approaches learn
dependent entities well; on the other hand, the IndRNN approach learns
non-dependent entities in time series. The proposed approach tried to use a
combination of IndRNN and LSTM approaches to learn dependent and non-dependent
features. Feature selection approaches also provide a suitable view of features
for the models; for this purpose, four feature selection models, Filter,
Wrapper, Embedded, and Autoencoder were used. The proposed IndRNNLSTM
algorithm, in combination with Embedded, was able to achieve MAE=1.22 and
RMSE=9.92 on NSL-KDD data.</div><div><a href='http://arxiv.org/abs/2402.05943v1'>2402.05943v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06175v1")'>MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly
  Detection</div>
<div id='2401.06175v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T06:50:25Z</div><div>Authors: Jinyang Liu, Wenwei Gu, Zhuangbin Chen, Yichen Li, Yuxin Su, Michael R. Lyu</div><div style='padding-top: 10px; width: 80ex'>Key Performance Indicators (KPIs) are essential time-series metrics for
ensuring the reliability and stability of many software systems. They
faithfully record runtime states to facilitate the understanding of anomalous
system behaviors and provide informative clues for engineers to pinpoint the
root causes. The unprecedented scale and complexity of modern software systems,
however, make the volume of KPIs explode. Consequently, many traditional
methods of KPI anomaly detection become impractical, which serves as a catalyst
for the fast development of machine learning-based solutions in both academia
and industry. However, there is currently a lack of rigorous comparison among
these KPI anomaly detection methods, and re-implementation demands a
non-trivial effort. Moreover, we observe that different works adopt independent
evaluation processes with different metrics. Some of them may not fully reveal
the capability of a model and some are creating an illusion of progress. To
better understand the characteristics of different KPI anomaly detectors and
address the evaluation issue, in this paper, we provide a comprehensive review
and evaluation of twelve state-of-the-art methods, and propose a novel metric
called salience. Particularly, the selected methods include five traditional
machine learning-based methods and seven deep learning-based methods. These
methods are evaluated with five multivariate KPI datasets that are publicly
available. A unified toolkit with easy-to-use interfaces is also released. We
report the benchmark results in terms of accuracy, salience, efficiency, and
delay, which are of practical importance for industrial deployment. We believe
our work can contribute as a basis for future academic research and industrial
application.</div><div><a href='http://arxiv.org/abs/2401.06175v1'>2401.06175v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14738v1")'>A task of anomaly detection for a smart satellite Internet of things
  system</div>
<div id='2403.14738v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:26:29Z</div><div>Authors: Zilong Shao</div><div style='padding-top: 10px; width: 80ex'>When the equipment is working, real-time collection of environmental sensor
data for anomaly detection is one of the key links to prevent industrial
process accidents and network attacks and ensure system security. However,
under the environment with specific real-time requirements, the anomaly
detection for environmental sensors still faces the following difficulties: (1)
The complex nonlinear correlation characteristics between environmental sensor
data variables lack effective expression methods, and the distribution between
the data is difficult to be captured. (2) it is difficult to ensure the
real-time monitoring requirements by using complex machine learning models, and
the equipment cost is too high. (3) Too little sample data leads to less
labeled data in supervised learning. This paper proposes an unsupervised deep
learning anomaly detection system. Based on the generative adversarial network
and self-attention mechanism, considering the different feature information
contained in the local subsequences, it automatically learns the complex linear
and nonlinear dependencies between environmental sensor variables, and uses the
anomaly score calculation method combining reconstruction error and
discrimination error. It can monitor the abnormal points of real sensor data
with high real-time performance and can run on the intelligent satellite
Internet of things system, which is suitable for the real working environment.
Anomaly detection outperforms baseline methods in most cases and has good
interpretability, which can be used to prevent industrial accidents and
cyber-attacks for monitoring environmental sensors.</div><div><a href='http://arxiv.org/abs/2403.14738v1'>2403.14738v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16888v1")'>Chaotic attractor reconstruction using small reservoirs -- the influence
  of topology</div>
<div id='2402.16888v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T09:43:52Z</div><div>Authors: Lina Jaurigue</div><div style='padding-top: 10px; width: 80ex'>Forecasting timeseries based upon measured data is needed in a wide range of
applications and has been the subject of extensive research. A particularly
challenging task is the forecasting of timeseries generated by chaotic
dynamics. In recent years reservoir computing has been shown to be an effective
method of forecasting chaotic dynamics and reconstructing chaotic attractors
from data. In this work strides are made toward smaller and lower complexity
reservoirs with the goal of improved hardware implementability and more
reliable production of adequate surrogate models. We show that a reservoir of
uncoupled nodes more reliably produces long term timeseries predictions than
complex reservoir topologies. We then link the improved attractor
reconstruction of the uncoupled reservoir with smaller spectral radii of the
resulting surrogate systems. These results indicate that, the node degree plays
an important role in determining whether the desired dynamics will be stable in
the autonomous surrogate system which is attained via closed-loop operation of
the trained reservoir. In terms of hardware implementability, uncoupled nodes
would allow for greater freedom in the hardware architecture because no complex
coupling setups are needed and because, for uncoupled nodes, the system
response is equivalent for space and time multiplexing.</div><div><a href='http://arxiv.org/abs/2402.16888v1'>2402.16888v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14384v1")'>Generative Adversarial Network with Soft-Dynamic Time Warping and
  Parallel Reconstruction for Energy Time Series Anomaly Detection</div>
<div id='2402.14384v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T08:54:57Z</div><div>Authors: Hardik Prabhu, Jayaraman Valadi, Pandarasamy Arjunan</div><div style='padding-top: 10px; width: 80ex'>In this paper, we employ a 1D deep convolutional generative adversarial
network (DCGAN) for sequential anomaly detection in energy time series data.
Anomaly detection involves gradient descent to reconstruct energy
sub-sequences, identifying the noise vector that closely generates them through
the generator network. Soft-DTW is used as a differentiable alternative for the
reconstruction loss and is found to be superior to Euclidean distance.
Combining reconstruction loss and the latent space's prior probability
distribution serves as the anomaly score. Our novel method accelerates
detection by parallel computation of reconstruction of multiple points and
shows promise in identifying anomalous energy consumption in buildings, as
evidenced by performing experiments on hourly energy time series from 15
buildings.</div><div><a href='http://arxiv.org/abs/2402.14384v1'>2402.14384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03341v1")'>Weakly Augmented Variational Autoencoder in Time Series Anomaly
  Detection</div>
<div id='2401.03341v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T00:23:05Z</div><div>Authors: Zhangkai Wu, Longbing Cao, Qi Zhang, Junxian Zhou, Hui Chen</div><div style='padding-top: 10px; width: 80ex'>Due to their unsupervised training and uncertainty estimation, deep
Variational Autoencoders (VAEs) have become powerful tools for
reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based
TSAD methods, either statistical or deep, tune meta-priors to estimate the
likelihood probability for effectively capturing spatiotemporal dependencies in
the data. However, these methods confront the challenge of inherent data
scarcity, which is often the case in anomaly detection tasks. Such scarcity
easily leads to latent holes, discontinuous regions in latent space, resulting
in non-robust reconstructions on these discontinuous spaces. We propose a novel
generative framework that combines VAEs with self-supervised learning (SSL) to
address this issue.</div><div><a href='http://arxiv.org/abs/2401.03341v1'>2401.03341v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13023v1")'>Thwarting Cybersecurity Attacks with Explainable Concept Drift</div>
<div id='2403.13023v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T20:20:00Z</div><div>Authors: Ibrahim Shaer, Abdallah Shami</div><div style='padding-top: 10px; width: 80ex'>Cyber-security attacks pose a significant threat to the operation of
autonomous systems. Particularly impacted are the Heating, Ventilation, and Air
Conditioning (HVAC) systems in smart buildings, which depend on data gathered
by sensors and Machine Learning (ML) models using the captured data. As such,
attacks that alter the readings of these sensors can severely affect the HVAC
system operations impacting residents' comfort and energy reduction goals. Such
attacks may induce changes in the online data distribution being fed to the ML
models, violating the fundamental assumption of similarity in training and
testing data distribution. This leads to a degradation in model prediction
accuracy due to a phenomenon known as Concept Drift (CD) - the alteration in
the relationship between input features and the target variable. Addressing CD
requires identifying the source of drift to apply targeted mitigation
strategies, a process termed drift explanation. This paper proposes a Feature
Drift Explanation (FDE) module to identify the drifting features. FDE utilizes
an Auto-encoder (AE) that reconstructs the activation of the first layer of the
regression Deep Learning (DL) model and finds their latent representations.
When a drift is detected, each feature of the drifting data is replaced by its
representative counterpart from the training data. The Minkowski distance is
then used to measure the divergence between the altered drifting data and the
original training data. The results show that FDE successfully identifies 85.77
% of drifting features and showcases its utility in the DL adaptation method
under the CD phenomenon. As a result, the FDE method is an effective strategy
for identifying drifting features towards thwarting cyber-security attacks.</div><div><a href='http://arxiv.org/abs/2403.13023v1'>2403.13023v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15304v1")'>Adaptive Least Mean Squares Graph Neural Networks and Online Graph
  Signal Estimation</div>
<div id='2401.15304v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T05:47:12Z</div><div>Authors: Yi Yan, Changran Peng, Ercan Engin Kuruoglu</div><div style='padding-top: 10px; width: 80ex'>The online prediction of multivariate signals, existing simultaneously in
space and time, from noisy partial observations is a fundamental task in
numerous applications. We propose an efficient Neural Network architecture for
the online estimation of time-varying graph signals named the Adaptive Least
Mean Squares Graph Neural Networks (LMS-GNN). LMS-GNN aims to capture the time
variation and bridge the cross-space-time interactions under the condition that
signals are corrupted by noise and missing values. The LMS-GNN is a combination
of adaptive graph filters and Graph Neural Networks (GNN). At each time step,
the forward propagation of LMS-GNN is similar to adaptive graph filters where
the output is based on the error between the observation and the prediction
similar to GNN. The filter coefficients are updated via backpropagation as in
GNN. Experimenting on real-world temperature data reveals that our LMS-GNN
achieves more accurate online predictions compared to graph-based methods like
adaptive graph filters and graph convolutional neural networks.</div><div><a href='http://arxiv.org/abs/2401.15304v1'>2401.15304v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12598v1")'>Graph-based Virtual Sensing from Sparse and Partial Multivariate
  Observations</div>
<div id='2402.12598v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T23:22:30Z</div><div>Authors: Giovanni De Felice, Andrea Cini, Daniele Zambon, Vladimir V. Gusev, Cesare Alippi</div><div style='padding-top: 10px; width: 80ex'>Virtual sensing techniques allow for inferring signals at new unmonitored
locations by exploiting spatio-temporal measurements coming from physical
sensors at different locations. However, as the sensor coverage becomes sparse
due to costs or other constraints, physical proximity cannot be used to support
interpolation. In this paper, we overcome this challenge by leveraging
dependencies between the target variable and a set of correlated variables
(covariates) that can frequently be associated with each location of interest.
From this viewpoint, covariates provide partial observability, and the problem
consists of inferring values for unobserved channels by exploiting observations
at other locations to learn how such variables can correlate. We introduce a
novel graph-based methodology to exploit such relationships and design a graph
deep learning architecture, named GgNet, implementing the framework. The
proposed approach relies on propagating information over a nested graph
structure that is used to learn dependencies between variables as well as
locations. GgNet is extensively evaluated under different virtual sensing
scenarios, demonstrating higher reconstruction accuracy compared to the
state-of-the-art.</div><div><a href='http://arxiv.org/abs/2402.12598v1'>2402.12598v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08918v1")'>Graph Inference Acceleration by Learning MLPs on Graphs without
  Supervision</div>
<div id='2402.08918v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T03:16:13Z</div><div>Authors: Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph
learning tasks, yet their reliance on message-passing constraints their
deployment in latency-sensitive applications such as financial fraud detection.
Recent works have explored distilling knowledge from GNNs to Multi-Layer
Perceptrons (MLPs) to accelerate inference. However, this task-specific
supervised distillation limits generalization to unseen nodes, which are
prevalent in latency-sensitive applications. To this end, we present
\textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework
for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance
generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs
and MLPs to capture the fine-grained and generalizable correlation between node
features and graph structures, and proposes two strategies to alleviate the
risk of trivial solutions. Theoretically, we comprehensively analyze
\textsc{SimMLP} to demonstrate its equivalence to GNNs in the optimal case and
its generalization capability. Empirically, \textsc{SimMLP} outperforms
state-of-the-art baselines, especially in settings with unseen nodes. In
particular, it obtains significant performance gains {\bf (7$\sim$26\%)} over
MLPs and inference acceleration over GNNs {\bf (90$\sim$126$\times$)} on
large-scale graph datasets. Our codes are available at:
\url{https://github.com/Zehong-Wang/SimMLP}.</div><div><a href='http://arxiv.org/abs/2402.08918v1'>2402.08918v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03387v1")'>Overcoming Order in Autoregressive Graph Generation</div>
<div id='2402.03387v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:58:22Z</div><div>Authors: Edo Cohen-Karlik, Eyal Rozenberg, Daniel Freedman</div><div style='padding-top: 10px; width: 80ex'>Graph generation is a fundamental problem in various domains, including
chemistry and social networks. Recent work has shown that molecular graph
generation using recurrent neural networks (RNNs) is advantageous compared to
traditional generative approaches which require converting continuous latent
representations into graphs. One issue which arises when treating graph
generation as sequential generation is the arbitrary order of the sequence
which results from a particular choice of graph flattening method. In this work
we propose using RNNs, taking into account the non-sequential nature of graphs
by adding an Orderless Regularization (OLR) term that encourages the hidden
state of the recurrent model to be invariant to different valid orderings
present under the training distribution. We demonstrate that sequential graph
generation models benefit from our proposed regularization scheme, especially
when data is scarce. Our findings contribute to the growing body of research on
graph generation and provide a valuable tool for various applications requiring
the synthesis of realistic and diverse graph structures.</div><div><a href='http://arxiv.org/abs/2402.03387v1'>2402.03387v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03687v1")'>Pard: Permutation-Invariant Autoregressive Diffusion for Graph
  Generation</div>
<div id='2402.03687v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T04:17:44Z</div><div>Authors: Lingxiao Zhao, Xueying Ding, Leman Akoglu</div><div style='padding-top: 10px; width: 80ex'>Graph generation has been dominated by autoregressive models due to their
simplicity and effectiveness, despite their sensitivity to ordering. Yet
diffusion models have garnered increasing attention, as they offer comparable
performance while being permutation-invariant. Current graph diffusion models
generate graphs in a one-shot fashion, but they require extra features and
thousands of denoising steps to achieve optimal performance. We introduce PARD,
a Permutation-invariant Auto Regressive Diffusion model that integrates
diffusion models with autoregressive methods. PARD harnesses the effectiveness
and efficiency of the autoregressive model while maintaining permutation
invariance without ordering sensitivity. Specifically, we show that contrary to
sets, elements in a graph are not entirely unordered and there is a unique
partial order for nodes and edges. With this partial order, PARD generates a
graph in a block-by-block, autoregressive fashion, where each block's
probability is conditionally modeled by a shared diffusion model with an
equivariant network. To ensure efficiency while being expressive, we further
propose a higher-order graph transformer, which integrates transformer with
PPGN. Like GPT, we extend the higher-order graph transformer to support
parallel training of all blocks. Without any extra features, PARD achieves
state-of-the-art performance on molecular and non-molecular datasets, and
scales to large datasets like MOSES containing 1.9M molecules.</div><div><a href='http://arxiv.org/abs/2402.03687v1'>2402.03687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18974v1")'>Graph Generation via Spectral Diffusion</div>
<div id='2402.18974v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T09:26:46Z</div><div>Authors: Giorgia Minello, Alessandro Bicciato, Luca Rossi, Andrea Torsello, Luca Cosmo</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present GRASP, a novel graph generative model based on 1)
the spectral decomposition of the graph Laplacian matrix and 2) a diffusion
process. Specifically, we propose to use a denoising model to sample
eigenvectors and eigenvalues from which we can reconstruct the graph Laplacian
and adjacency matrix. Our permutation invariant model can also handle node
features by concatenating them to the eigenvectors of each node. Using the
Laplacian spectrum allows us to naturally capture the structural
characteristics of the graph and work directly in the node space while avoiding
the quadratic complexity bottleneck that limits the applicability of other
methods. This is achieved by truncating the spectrum, which as we show in our
experiments results in a faster yet accurate generative process. An extensive
set of experiments on both synthetic and real world graphs demonstrates the
strengths of our model against state-of-the-art alternatives.</div><div><a href='http://arxiv.org/abs/2402.18974v1'>2402.18974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15617v1")'>Diffusion-based graph generative methods</div>
<div id='2401.15617v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T10:09:05Z</div><div>Authors: Hongyang Chen, Can Xu, Lingyu Zheng, Qiang Zhang, Xuemin Lin</div><div style='padding-top: 10px; width: 80ex'>Being the most cutting-edge generative methods, diffusion methods have shown
great advances in wide generation tasks. Among them, graph generation attracts
significant research attention for its broad application in real life. In our
survey, we systematically and comprehensively review on diffusion-based graph
generative methods. We first make a review on three mainstream paradigms of
diffusion methods, which are denoising diffusion probabilistic models,
score-based genrative models, and stochastic differential equations. Then we
further categorize and introduce the latest applications of diffusion models on
graphs. In the end, we point out some limitations of current studies and future
directions of future explorations. The summary of existing methods metioned in
this survey is in
https://github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.</div><div><a href='http://arxiv.org/abs/2401.15617v1'>2401.15617v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00044v1")'>Scaling up Dynamic Edge Partition Models via Stochastic Gradient MCMC</div>
<div id='2403.00044v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:19:35Z</div><div>Authors: Sikun Yang, Heinz Koeppl</div><div style='padding-top: 10px; width: 80ex'>The edge partition model (EPM) is a generative model for extracting an
overlapping community structure from static graph-structured data. In the EPM,
the gamma process (GaP) prior is adopted to infer the appropriate number of
latent communities, and each vertex is endowed with a gamma distributed
positive memberships vector. Despite having many attractive properties,
inference in the EPM is typically performed using Markov chain Monte Carlo
(MCMC) methods that prevent it from being applied to massive network data. In
this paper, we generalize the EPM to account for dynamic enviroment by
representing each vertex with a positive memberships vector constructed using
Dirichlet prior specification, and capturing the time-evolving behaviour of
vertices via a Dirichlet Markov chain construction. A simple-to-implement Gibbs
sampler is proposed to perform posterior computation using Negative- Binomial
augmentation technique. For large network data, we propose a stochastic
gradient Markov chain Monte Carlo (SG-MCMC) algorithm for scalable inference in
the proposed model. The experimental results show that the novel methods
achieve competitive performance in terms of link prediction, while being much
faster.</div><div><a href='http://arxiv.org/abs/2403.00044v1'>2403.00044v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18697v1")'>Inferring Dynamic Networks from Marginals with Iterative Proportional
  Fitting</div>
<div id='2402.18697v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T20:24:56Z</div><div>Authors: Serina Chang, Frederic Koehler, Zhaonan Qu, Jure Leskovec, Johan Ugander</div><div style='padding-top: 10px; width: 80ex'>A common network inference problem, arising from real-world data constraints,
is how to infer a dynamic network from its time-aggregated adjacency matrix and
time-varying marginals (i.e., row and column sums). Prior approaches to this
problem have repurposed the classic iterative proportional fitting (IPF)
procedure, also known as Sinkhorn's algorithm, with promising empirical
results. However, the statistical foundation for using IPF has not been well
understood: under what settings does IPF provide principled estimation of a
dynamic network from its marginals, and how well does it estimate the network?
In this work, we establish such a setting, by identifying a generative network
model whose maximum likelihood estimates are recovered by IPF. Our model both
reveals implicit assumptions on the use of IPF in such settings and enables new
analyses, such as structure-dependent error bounds on IPF's parameter
estimates. When IPF fails to converge on sparse network data, we introduce a
principled algorithm that guarantees IPF converges under minimal changes to the
network structure. Finally, we conduct experiments with synthetic and
real-world data, which demonstrate the practical value of our theoretical and
algorithmic contributions.</div><div><a href='http://arxiv.org/abs/2402.18697v1'>2402.18697v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08412v1")'>Interacting Particle Systems on Networks: joint inference of the network
  and the interaction kernel</div>
<div id='2402.08412v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T12:29:38Z</div><div>Authors: Quanjun Lang, Xiong Wang, Fei Lu, Mauro Maggioni</div><div style='padding-top: 10px; width: 80ex'>Modeling multi-agent systems on networks is a fundamental challenge in a wide
variety of disciplines. We jointly infer the weight matrix of the network and
the interaction kernel, which determine respectively which agents interact with
which others and the rules of such interactions from data consisting of
multiple trajectories. The estimator we propose leads naturally to a non-convex
optimization problem, and we investigate two approaches for its solution: one
is based on the alternating least squares (ALS) algorithm; another is based on
a new algorithm named operator regression with alternating least squares
(ORALS). Both algorithms are scalable to large ensembles of data trajectories.
We establish coercivity conditions guaranteeing identifiability and
well-posedness. The ALS algorithm appears statistically efficient and robust
even in the small data regime but lacks performance and convergence guarantees.
The ORALS estimator is consistent and asymptotically normal under a coercivity
condition. We conduct several numerical experiments ranging from Kuramoto
particle systems on networks to opinion dynamics in leader-follower models.</div><div><a href='http://arxiv.org/abs/2402.08412v1'>2402.08412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01318v1")'>High-Dimensional Tail Index Regression: with An Application to Text
  Analyses of Viral Posts in Social Media</div>
<div id='2403.01318v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T21:37:40Z</div><div>Authors: Yuya Sasaki, Jing Tao, Yulong Wang</div><div style='padding-top: 10px; width: 80ex'>Motivated by the empirical power law of the distributions of credits (e.g.,
the number of "likes") of viral posts in social media, we introduce the
high-dimensional tail index regression and methods of estimation and inference
for its parameters. We propose a regularized estimator, establish its
consistency, and derive its convergence rate. To conduct inference, we propose
to debias the regularized estimate, and establish the asymptotic normality of
the debiased estimator. Simulation studies support our theory. These methods
are applied to text analyses of viral posts in X (formerly Twitter) concerning
LGBTQ+.</div><div><a href='http://arxiv.org/abs/2403.01318v1'>2403.01318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01404v3")'>Scalable network reconstruction in subquadratic time</div>
<div id='2401.01404v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T19:00:13Z</div><div>Authors: Tiago P. Peixoto</div><div style='padding-top: 10px; width: 80ex'>Network reconstruction consists in determining the unobserved pairwise
couplings between $N$ nodes given only observational data on the resulting
behavior that is conditioned on those couplings -- typically a time-series or
independent samples from a graphical model. A major obstacle to the scalability
of algorithms proposed for this problem is a seemingly unavoidable quadratic
complexity of $O(N^2)$, corresponding to the requirement of each possible
pairwise coupling being contemplated at least once, despite the fact that most
networks of interest are sparse, with a number of non-zero couplings that is
only $O(N)$. Here we present a general algorithm applicable to a broad range of
reconstruction problems that achieves its result in subquadratic time, with a
data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with
a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on
a stochastic second neighbor search that produces the best edge candidates with
high probability, thus bypassing an exhaustive quadratic search. In practice,
our algorithm achieves a performance that is many orders of magnitude faster
than the quadratic baseline, allows for easy parallelization, and thus enables
the reconstruction of networks with hundreds of thousands and even millions of
nodes and edges.</div><div><a href='http://arxiv.org/abs/2401.01404v3'>2401.01404v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03651v1")'>Temporal Graph Analysis with TGX</div>
<div id='2402.03651v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T02:56:07Z</div><div>Authors: Razieh Shirzadkhani, Shenyang Huang, Elahe Kooshafar, Reihaneh Rabbany, Farimah Poursafaei</div><div style='padding-top: 10px; width: 80ex'>Real-world networks, with their evolving relations, are best captured as
temporal graphs. However, existing software libraries are largely designed for
static graphs where the dynamic nature of temporal graphs is ignored. Bridging
this gap, we introduce TGX, a Python package specially designed for analysis of
temporal networks that encompasses an automated pipeline for data loading, data
processing, and analysis of evolving graphs. TGX provides access to eleven
built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as
well as any novel datasets in the .csv format. Beyond data loading, TGX
facilitates data processing functionalities such as discretization of temporal
graphs and node subsampling to accelerate working with larger datasets. For
comprehensive investigation, TGX offers network analysis by providing a diverse
set of measures, including average node degree and the evolving number of nodes
and edges per timestamp. Additionally, the package consolidates meaningful
visualization plots indicating the evolution of temporal patterns, such as
Temporal Edge Appearance (TEA) and Temporal Edge Trafficc (TET) plots. The TGX
package is a robust tool for examining the features of temporal graphs and can
be used in various areas like studying social networks, citation networks, and
tracking user interactions. We plan to continuously support and update TGX
based on community feedback. TGX is publicly available on:
https://github.com/ComplexData-MILA/TGX.</div><div><a href='http://arxiv.org/abs/2402.03651v1'>2402.03651v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06771v1")'>Redefining Event Types and Group Evolution in Temporal Data</div>
<div id='2403.06771v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T14:39:24Z</div><div>Authors: Andrea Failla, Rémy Cazabet, Giulio Rossetti, Salvatore Citraro</div><div style='padding-top: 10px; width: 80ex'>Groups -- such as clusters of points or communities of nodes -- are
fundamental when addressing various data mining tasks. In temporal data, the
predominant approach for characterizing group evolution has been through the
identification of ``events". However, the events usually described in the
literature, e.g., shrinks/growths, splits/merges, are often arbitrarily
defined, creating a gap between such theoretical/predefined types and real-data
group observations. Moving beyond existing taxonomies, we think of events as
``archetypes" characterized by a unique combination of quantitative dimensions
that we call ``facets". Group dynamics are defined by their position within the
facet space, where archetypal events occupy extremities. Thus, rather than
enforcing strict event types, our approach can allow for hybrid descriptions of
dynamics involving group proximity to multiple archetypes. We apply our
framework to evolving groups from several face-to-face interaction datasets,
showing it enables richer, more reliable characterization of group dynamics
with respect to state-of-the-art methods, especially when the groups are
subject to complex relationships. Our approach also offers intuitive solutions
to common tasks related to dynamic group analysis, such as choosing an
appropriate aggregation scale, quantifying partition stability, and evaluating
event quality.</div><div><a href='http://arxiv.org/abs/2403.06771v1'>2403.06771v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03194v1")'>Learning Persistent Community Structures in Dynamic Networks via
  Topological Data Analysis</div>
<div id='2401.03194v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T11:29:19Z</div><div>Authors: Dexu Kong, Anping Zhang, Yang Li</div><div style='padding-top: 10px; width: 80ex'>Dynamic community detection methods often lack effective mechanisms to ensure
temporal consistency, hindering the analysis of network evolution. In this
paper, we propose a novel deep graph clustering framework with temporal
consistency regularization on inter-community structures, inspired by the
concept of minimal network topological changes within short intervals.
Specifically, to address the representation collapse problem, we first
introduce MFC, a matrix factorization-based deep graph clustering algorithm
that preserves node embedding. Based on static clustering results, we construct
probabilistic community networks and compute their persistence homology, a
robust topological measure, to assess structural similarity between them.
Moreover, a novel neural network regularization TopoReg is introduced to ensure
the preservation of topological similarity between inter-community structures
over time intervals. Our approach enhances temporal consistency and clustering
accuracy on real-world datasets with both fixed and varying numbers of
communities. It is also a pioneer application of TDA in temporally persistent
community detection, offering an insightful contribution to field of network
analysis. Code and data are available at the public git repository:
https://github.com/kundtx/MFC_TopoReg</div><div><a href='http://arxiv.org/abs/2401.03194v1'>2401.03194v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02867v1")'>Scalable Continuous-time Diffusion Framework for Network Inference and
  Influence Estimation</div>
<div id='2403.02867v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:21:18Z</div><div>Authors: Keke Huang, Ruize Gao, Bogdan Cautis, Xiaokui Xiao</div><div style='padding-top: 10px; width: 80ex'>The study of continuous-time information diffusion has been an important area
of research for many applications in recent years. When only the diffusion
traces (cascades) are accessible, cascade-based network inference and influence
estimation are two essential problems to explore. Alas, existing methods
exhibit limited capability to infer and process networks with more than a few
thousand nodes, suffering from scalability issues. In this paper, we view the
diffusion process as a continuous-time dynamical system, based on which we
establish a continuous-time diffusion model. Subsequently, we instantiate the
model to a scalable and effective framework (FIM) to approximate the diffusion
propagation from available cascades, thereby inferring the underlying network
structure. Furthermore, we undertake an analysis of the approximation error of
FIM for network inference. To achieve the desired scalability for influence
estimation, we devise an advanced sampling technique and significantly boost
the efficiency. We also quantify the effect of the approximation error on
influence estimation theoretically. Experimental results showcase the
effectiveness and superior scalability of FIM on network inference and
influence estimation.</div><div><a href='http://arxiv.org/abs/2403.02867v1'>2403.02867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09755v1")'>Estimating the history of a random recursive tree</div>
<div id='2403.09755v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T14:02:00Z</div><div>Authors: Simon Briend, Christophe Giraud, Gábor Lugosi, Déborah Sulem</div><div style='padding-top: 10px; width: 80ex'>This paper studies the problem of estimating the order of arrival of the
vertices in a random recursive tree. Specifically, we study two fundamental
models: the uniform attachment model and the linear preferential attachment
model. We propose an order estimator based on the Jordan centrality measure and
define a family of risk measures to quantify the quality of the ordering
procedure. Moreover, we establish a minimax lower bound for this problem, and
prove that the proposed estimator is nearly optimal. Finally, we numerically
demonstrate that the proposed estimator outperforms degree-based and spectral
ordering procedures.</div><div><a href='http://arxiv.org/abs/2403.09755v1'>2403.09755v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06279v1")'>Sampling and Uniqueness Sets in Graphon Signal Processing</div>
<div id='2401.06279v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T22:31:48Z</div><div>Authors: Alejandro Parada-Mayorga, Alejandro Ribeiro</div><div style='padding-top: 10px; width: 80ex'>In this work, we study the properties of sampling sets on families of large
graphs by leveraging the theory of graphons and graph limits. To this end, we
extend to graphon signals the notion of removable and uniqueness sets, which
was developed originally for the analysis of signals on graphs. We state the
formal definition of a $\Lambda-$removable set and conditions under which a
bandlimited graphon signal can be represented in a unique way when its samples
are obtained from the complement of a given $\Lambda-$removable set in the
graphon. By leveraging such results we show that graphon representations of
graphs and graph signals can be used as a common framework to compare sampling
sets between graphs with different numbers of nodes and edges, and different
node labelings. Additionally, given a sequence of graphs that converges to a
graphon, we show that the sequences of sampling sets whose graphon
representation is identical in $[0,1]$ are convergent as well. We exploit the
convergence results to provide an algorithm that obtains approximately close to
optimal sampling sets. Performing a set of numerical experiments, we evaluate
the quality of these sampling sets. Our results open the door for the efficient
computation of optimal sampling sets in graphs of large size.</div><div><a href='http://arxiv.org/abs/2401.06279v1'>2401.06279v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11686v1")'>Learning the Topology and Behavior of Discrete Dynamical Systems</div>
<div id='2402.11686v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T19:31:26Z</div><div>Authors: Zirou Qiu, Abhijin Adiga, Madhav V. Marathe, S. S. Ravi, Daniel J. Rosenkrantz, Richard E. Stearns, Anil Vullikanti</div><div style='padding-top: 10px; width: 80ex'>Discrete dynamical systems are commonly used to model the spread of
contagions on real-world networks. Under the PAC framework, existing research
has studied the problem of learning the behavior of a system, assuming that the
underlying network is known. In this work, we focus on a more challenging
setting: to learn both the behavior and the underlying topology of a black-box
system. We show that, in general, this learning problem is computationally
intractable. On the positive side, we present efficient learning methods under
the PAC model when the underlying graph of the dynamical system belongs to some
classes. Further, we examine a relaxed setting where the topology of an unknown
system is partially observed. For this case, we develop an efficient PAC
learner to infer the system and establish the sample complexity. Lastly, we
present a formal analysis of the expressive power of the hypothesis class of
dynamical systems where both the topology and behavior are unknown, using the
well-known formalism of the Natarajan dimension. Our results provide a
theoretical foundation for learning both the behavior and topology of discrete
dynamical systems.</div><div><a href='http://arxiv.org/abs/2402.11686v1'>2402.11686v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15326v1")'>Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective
  of Operator Semigroup Theory</div>
<div id='2402.15326v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:44:57Z</div><div>Authors: Weichen Zhao, Chenguang Wang, Xinyan Wang, Congying Han, Tiande Guo, Tianshu Yu</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel study of the oversmoothing issue in
diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches
grounded in random walk analysis or particle systems, we approach this problem
through operator semigroup theory. This theoretical framework allows us to
rigorously prove that oversmoothing is intrinsically linked to the ergodicity
of the diffusion operator. This finding further poses a general and mild
ergodicity-breaking condition, encompassing the various specific solutions
previously offered, thereby presenting a more universal and theoretically
grounded approach to mitigating oversmoothing in diffusion-based GNNs.
Additionally, we offer a probabilistic interpretation of our theory, forging a
link with prior works and broadening the theoretical horizon. Our experimental
results reveal that this ergodicity-breaking term effectively mitigates
oversmoothing measured by Dirichlet energy, and simultaneously enhances
performance in node classification tasks.</div><div><a href='http://arxiv.org/abs/2402.15326v1'>2402.15326v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.15077v1")'>GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks</div>
<div id='2403.15077v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T10:02:13Z</div><div>Authors: Sukhdeep Singh, Anuj Sharma, Vinod Kumar Chauhan</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNN) have emerged as a popular and standard approach
for learning from graph-structured data. The literature on GNN highlights the
potential of this evolving research area and its widespread adoption in
real-life applications. However, most of the approaches are either new in
concept or derived from specific techniques. Therefore, the potential of more
than one approach in hybrid form has not been studied extensively, which can be
well utilized for sequenced data or static data together. We derive a hybrid
approach based on two established techniques as generalized aggregation
networks and topology adaptive graph convolution networks that solve our
purpose to apply on both types of sequenced and static nature of data,
effectively. The proposed method applies to both node and graph classification.
Our empirical analysis reveals that the results are at par with literature
results and better for handwritten strokes as sequenced data, where graph
structures have not been explored.</div><div><a href='http://arxiv.org/abs/2403.15077v1'>2403.15077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02464v2")'>A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer</div>
<div id='2402.02464v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T12:29:40Z</div><div>Authors: Zhangyang Gao, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Can we model non-Euclidean graphs as pure language or even Euclidean vectors
while retaining their inherent information? The non-Euclidean property have
posed a long term challenge in graph modeling. Despite recent GNN and
Graphformer efforts encoding graphs as Euclidean vectors, recovering original
graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a
Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph
words in a Euclidean space, along with a GraphGPT decoder that reconstructs the
original graph from graph words to ensure information equivalence. We pretrain
GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained
Graph2Seq excels in graph representation learning, achieving state-of-the-art
results on 8/9 graph classification and regression tasks. (2) Pretrained
GraphGPT serves as a strong graph generator, demonstrated by its ability to
perform both unconditional and conditional graph generation. (3)
Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space,
overcoming previously known non-Euclidean challenge. (4) Our proposed novel
edge-centric GPT pretraining task is effective in graph fields, underscoring
its success in both representation and generation.</div><div><a href='http://arxiv.org/abs/2402.02464v2'>2402.02464v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14358v1")'>Exploring the Potential of Large Language Models in Graph Generation</div>
<div id='2403.14358v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T12:37:54Z</div><div>Authors: Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu, Yuekui Yang, Wenwu Zhu, Hong Mei</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have achieved great success in many fields, and
recent works have studied exploring LLMs for graph discriminative tasks such as
node classification. However, the abilities of LLMs for graph generation remain
unexplored in the literature. Graph generation requires the LLM to generate
graphs with given properties, which has valuable real-world applications such
as drug discovery, while tends to be more challenging. In this paper, we
propose LLM4GraphGen to explore the ability of LLMs for graph generation with
systematical task designs and extensive experiments. Specifically, we propose
several tasks tailored with comprehensive experiments to address key questions
regarding LLMs' understanding of different graph structure rules, their ability
to capture structural type distributions, and their utilization of domain
knowledge for property-based graph generation. Our evaluations demonstrate that
LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation
tasks, including rule-based and distribution-based generation. We also observe
that popular prompting methods, such as few-shot and chain-of-thought
prompting, do not consistently enhance performance. Besides, LLMs show
potential in generating molecules with specific properties. These findings may
serve as foundations for designing good LLMs based models for graph generation
and provide valuable insights and further research.</div><div><a href='http://arxiv.org/abs/2403.14358v1'>2403.14358v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01071v1")'>GraphRCG: Self-conditioned Graph Generation via Bootstrapped
  Representations</div>
<div id='2403.01071v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T02:28:20Z</div><div>Authors: Song Wang, Zhen Tan, Xinyu Zhao, Tianlong Chen, Huan Liu, Jundong Li</div><div style='padding-top: 10px; width: 80ex'>Graph generation generally aims to create new graphs that closely align with
a specific graph distribution. Existing works often implicitly capture this
distribution through the optimization of generators, potentially overlooking
the intricacies of the distribution itself. Furthermore, these approaches
generally neglect the insights offered by the learned distribution for graph
generation. In contrast, in this work, we propose a novel self-conditioned
graph generation framework designed to explicitly model graph distributions and
employ these distributions to guide the generation process. We first perform
self-conditioned modeling to capture the graph distributions by transforming
each graph sample into a low-dimensional representation and optimizing a
representation generator to create new representations reflective of the
learned distribution. Subsequently, we leverage these bootstrapped
representations as self-conditioned guidance for the generation process,
thereby facilitating the generation of graphs that more accurately reflect the
learned distributions. We conduct extensive experiments on generic and
molecular graph datasets across various fields. Our framework demonstrates
superior performance over existing state-of-the-art graph generation methods in
terms of graph quality and fidelity to training data.</div><div><a href='http://arxiv.org/abs/2403.01071v1'>2403.01071v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17123v1")'>Unsupervised Discovery of Steerable Factors When Graph Deep Generative
  Models Are Entangled</div>
<div id='2401.17123v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:53:34Z</div><div>Authors: Shengchao Liu, Chengpeng Wang, Jiarui Lu, Weili Nie, Hanchen Wang, Zhuoxinran Li, Bolei Zhou, Jian Tang</div><div style='padding-top: 10px; width: 80ex'>Deep generative models (DGMs) have been widely developed for graph data.
However, much less investigation has been carried out on understanding the
latent space of such pretrained graph DGMs. These understandings possess the
potential to provide constructive guidelines for crucial tasks, such as graph
controllable generation. Thus in this work, we are interested in studying this
problem and propose GraphCG, a method for the unsupervised discovery of
steerable factors in the latent space of pretrained graph DGMs. We first
examine the representation space of three pretrained graph DGMs with six
disentanglement metrics, and we observe that the pretrained representation
space is entangled. Motivated by this observation, GraphCG learns the steerable
factors via maximizing the mutual information between semantic-rich directions,
where the controlled graph moving along the same direction will share the same
steerable factors. We quantitatively verify that GraphCG outperforms four
competitive baselines on two graph DGMs pretrained on two molecule datasets.
Additionally, we qualitatively illustrate seven steerable factors learned by
GraphCG on five pretrained DGMs over five graph datasets, including two for
molecules and three for point clouds.</div><div><a href='http://arxiv.org/abs/2401.17123v1'>2401.17123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07179v1")'>3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of
  Molecular Graphs</div>
<div id='2403.07179v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T21:44:54Z</div><div>Authors: Huaisheng Zhu, Teng Xiao, Vasant G Honavar</div><div style='padding-top: 10px; width: 80ex'>Generating molecules with desired properties is a critical task with broad
applications in drug discovery and materials design. Inspired by recent
advances in large language models, there is a growing interest in using natural
language descriptions of molecules to generate molecules with the desired
properties. Most existing methods focus on generating molecules that precisely
match the text description. However, practical applications call for methods
that generate diverse, and ideally novel, molecules with the desired
properties. We propose 3M-Diffusion, a novel multi-modal molecular graph
generation method, to address this challenge. 3M-Diffusion first encodes
molecular graphs into a graph latent space aligned with text descriptions. It
then reconstructs the molecular structure and atomic attributes based on the
given text descriptions using the molecule decoder. It then learns a
probabilistic mapping from the text space to the latent molecular graph space
using a diffusion model. The results of our extensive experiments on several
datasets demonstrate that 3M-Diffusion can generate high-quality, novel and
diverse molecular graphs that semantically match the textual description
provided.</div><div><a href='http://arxiv.org/abs/2403.07179v1'>2403.07179v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08790v1")'>Improving Molecule Generation and Drug Discovery with a
  Knowledge-enhanced Generative Model</div>
<div id='2402.08790v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T20:58:36Z</div><div>Authors: Aditya Malusare, Vaneet Aggarwal</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in generative models have established state-of-the-art
benchmarks in generating molecules and novel drug candidates. Despite these
successes, a significant gap persists between generative models and the
utilization of extensive biomedical knowledge, often systematized within
knowledge graphs, whose potential to inform and enhance generative processes
has not been realized. In this paper, we present a novel approach that bridges
this divide by developing a framework for knowledge-enhanced generative models
called K-DReAM. We develop a scalable methodology to extend the functionality
of knowledge graphs while preserving semantic integrity and incorporate this
contextual information into a generative framework to guide a diffusion-based
model. The integration of knowledge graph embeddings with our generative model
furnishes a robust mechanism for producing novel drug candidates possessing
specific characteristics while ensuring validity and synthesizability. K-DReAM
outperforms state-of-the-art generative models on both unconditional and
targeted generation tasks.</div><div><a href='http://arxiv.org/abs/2402.08790v1'>2402.08790v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11950v1")'>A novel molecule generative model of VAE combined with Transformer</div>
<div id='2402.11950v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:46:04Z</div><div>Authors: Yasuhiro Yoshikai, Tadahaya Mizuno, Shumpei Nemoto, Hiroyuki Kusuhara</div><div style='padding-top: 10px; width: 80ex'>Recently, molecule generation using deep learning has been actively
investigated in drug discovery. In this field, Transformer and VAE are widely
used as powerful models, but they are rarely used in combination due to
structural and performance mismatch of them. This study proposes a model that
combines these two models through structural and parameter optimization in
handling diverse molecules. The proposed model shows comparable performance to
existing models in generating molecules, and showed by far superior performance
in generating molecules with unseen structures. In addition, the proposed model
successfully predicted molecular properties using the latent representation of
VAE. Ablation studies suggested the advantage of VAE over other generative
models like language model in generating novel molecules, and that the
molecules can be described by ~32 dimensional variables, much smaller than
existing descriptors and models. This study is expected to provide a virtual
chemical library containing a wide variety of compounds for virtual screening
and to enable efficient screening.</div><div><a href='http://arxiv.org/abs/2402.11950v1'>2402.11950v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16771v1")'>MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups
  and their Linker Joints</div>
<div id='2401.16771v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:20:08Z</div><div>Authors: Mogan Gim, Jueon Park, Soyon Park, Sanghoon Lee, Seungheun Baek, Junhyun Lee, Ngoc-Quang Nguyen, Jaewoo Kang</div><div style='padding-top: 10px; width: 80ex'>Molecular core structures and R-groups are essential concepts in drug
development. Integration of these concepts with conventional graph pre-training
approaches can promote deeper understanding in molecules. We propose MolPLA, a
novel pre-training framework that employs masked graph contrastive learning in
understanding the underlying decomposable parts inmolecules that implicate
their core structure and peripheral R-groups. Furthermore, we formulate an
additional framework that grants MolPLA the ability to help chemists find
replaceable R-groups in lead optimization scenarios. Experimental results on
molecular property prediction show that MolPLA exhibits predictability
comparable to current state-of-the-art models. Qualitative analysis implicate
that MolPLA is capable of distinguishing core and R-group sub-structures,
identifying decomposable regions in molecules and contributing to lead
optimization scenarios by rationally suggesting R-group replacements given
various query core templates. The code implementation for MolPLA and its
pre-trained model checkpoint is available at https://github.com/dmis-lab/MolPLA</div><div><a href='http://arxiv.org/abs/2401.16771v1'>2401.16771v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13779v1")'>Contextual Molecule Representation Learning from Chemical Reaction
  Knowledge</div>
<div id='2402.13779v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:58:40Z</div><div>Authors: Han Tang, Shikun Feng, Bicheng Lin, Yuyan Ni, JIngjing Liu, Wei-Ying Ma, Yanyan Lan</div><div style='padding-top: 10px; width: 80ex'>In recent years, self-supervised learning has emerged as a powerful tool to
harness abundant unlabelled data for representation learning and has been
broadly adopted in diverse areas. However, when applied to molecular
representation learning (MRL), prevailing techniques such as masked sub-unit
reconstruction often fall short, due to the high degree of freedom in the
possible combinations of atoms within molecules, which brings insurmountable
complexity to the masking-reconstruction paradigm. To tackle this challenge, we
introduce REMO, a self-supervised learning framework that takes advantage of
well-defined atom-combination rules in common chemistry. Specifically, REMO
pre-trains graph/Transformer encoders on 1.7 million known chemical reactions
in the literature. We propose two pre-training objectives: Masked Reaction
Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO
offers a novel solution to MRL by exploiting the underlying shared patterns in
chemical reactions as \textit{context} for pre-training, which effectively
infers meaningful representations of common chemistry knowledge. Such
contextual representations can then be utilized to support diverse downstream
molecular tasks with minimum finetuning, such as affinity prediction and
drug-drug interaction prediction. Extensive experimental results on
MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type
classification show that across all tested downstream tasks, REMO outperforms
the standard baseline of single-molecule masked modeling used in current MRL.
Remarkably, REMO is the pioneering deep learning model surpassing
fingerprint-based methods in activity cliff benchmarks.</div><div><a href='http://arxiv.org/abs/2402.13779v1'>2402.13779v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11472v1")'>DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt
  Learning</div>
<div id='2402.11472v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T06:22:01Z</div><div>Authors: Yingying Wang, Yun Xiong, Xixi Wu, Xiangguo Sun, Jiawei Zhang</div><div style='padding-top: 10px; width: 80ex'>Recently, Graph Neural Networks have become increasingly prevalent in
predicting adverse drug-drug interactions (DDI) due to their proficiency in
modeling the intricate associations between atoms and functional groups within
and across drug molecules. However, they are still hindered by two significant
challenges: (1) the issue of highly imbalanced event distribution, which is a
common but critical problem in medical datasets where certain interactions are
vastly underrepresented. This imbalance poses a substantial barrier to
achieving accurate and reliable DDI predictions. (2) the scarcity of labeled
data for rare events, which is a pervasive issue in the medical field where
rare yet potentially critical interactions are often overlooked or
under-studied due to limited available data. In response, we offer DDIPrompt,
an innovative panacea inspired by the recent advancements in graph prompting.
Our framework aims to address these issues by leveraging the intrinsic
knowledge from pre-trained models, which can be efficiently deployed with
minimal downstream data. Specifically, to solve the first challenge, DDIPrompt
employs augmented links between drugs, considering both structural and
interactive proximity. It features a hierarchical pre-training strategy that
comprehends intra-molecular structures and inter-molecular interactions,
fostering a comprehensive and unbiased understanding of drug properties. For
the second challenge, we implement a prototype-enhanced prompting mechanism
during inference. This mechanism, refined by few-shot examples from each
category, effectively harnesses the rich pre-training knowledge to enhance
prediction accuracy, particularly for these rare but crucial interactions.
Comprehensive evaluations on two benchmark datasets demonstrate the superiority
of DDIPrompt, particularly in predicting rare DDI events.</div><div><a href='http://arxiv.org/abs/2402.11472v1'>2402.11472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18127v1")'>Hierarchical Multi-Relational Graph Representation Learning for
  Large-Scale Prediction of Drug-Drug Interactions</div>
<div id='2402.18127v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:36:16Z</div><div>Authors: Mengying Jiang, Guizhong Liu, Yuanchao Su, Weiqiang Jin, Biao Zhao</div><div style='padding-top: 10px; width: 80ex'>Most existing methods for predicting drug-drug interactions (DDI)
predominantly concentrate on capturing the explicit relationships among drugs,
overlooking the valuable implicit correlations present between drug pairs
(DPs), which leads to weak predictions. To address this issue, this paper
introduces a hierarchical multi-relational graph representation learning
(HMGRL) approach. Within the framework of HMGRL, we leverage a wealth of
drug-related heterogeneous data sources to construct heterogeneous graphs,
where nodes represent drugs and edges denote clear and various associations.
The relational graph convolutional network (RGCN) is employed to capture
diverse explicit relationships between drugs from these heterogeneous graphs.
Additionally, a multi-view differentiable spectral clustering (MVDSC) module is
developed to capture multiple valuable implicit correlations between DPs.
Within the MVDSC, we utilize multiple DP features to construct graphs, where
nodes represent DPs and edges denote different implicit correlations.
Subsequently, multiple DP representations are generated through graph cutting,
each emphasizing distinct implicit correlations. The graph-cutting strategy
enables our HMGRL to identify strongly connected communities of graphs, thereby
reducing the fusion of irrelevant features. By combining every representation
view of a DP, we create high-level DP representations for predicting DDIs. Two
genuine datasets spanning three distinct tasks are adopted to gauge the
efficacy of our HMGRL. Experimental outcomes unequivocally indicate that HMGRL
surpasses several leading-edge methods in performance.</div><div><a href='http://arxiv.org/abs/2402.18127v1'>2402.18127v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02786v1")'>Semi-Supervised Graph Representation Learning with Human-centric
  Explanation for Predicting Fatty Liver Disease</div>
<div id='2403.02786v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:59:45Z</div><div>Authors: So Yeon Kim, Sehee Wang, Eun Kyung Choe</div><div style='padding-top: 10px; width: 80ex'>Addressing the challenge of limited labeled data in clinical settings,
particularly in the prediction of fatty liver disease, this study explores the
potential of graph representation learning within a semi-supervised learning
framework. Leveraging graph neural networks (GNNs), our approach constructs a
subject similarity graph to identify risk patterns from health checkup data.
The effectiveness of various GNN approaches in this context is demonstrated,
even with minimal labeled samples. Central to our methodology is the inclusion
of human-centric explanations through explainable GNNs, providing personalized
feature importance scores for enhanced interpretability and clinical relevance,
thereby underscoring the potential of our approach in advancing healthcare
practices with a keen focus on graph representation learning and human-centric
explanation.</div><div><a href='http://arxiv.org/abs/2403.02786v1'>2403.02786v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05559v1")'>Improving Cognitive Diagnosis Models with Adaptive Relational Graph
  Neural Networks</div>
<div id='2403.05559v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:12:38Z</div><div>Authors: Pengyang Shao, Chen Gao, Lei Chen, Yonghui Yang, Kun Zhang, Meng Wang</div><div style='padding-top: 10px; width: 80ex'>Cognitive Diagnosis (CD) algorithms receive growing research interest in
intelligent education. Typically, these CD algorithms assist students by
inferring their abilities (i.e., their proficiency levels on various knowledge
concepts). The proficiency levels can enable further targeted skill training
and personalized exercise recommendations, thereby promoting students' learning
efficiency in online education. Recently, researchers have found that building
and incorporating a student-exercise bipartite graph is beneficial for
enhancing diagnostic performance. However, there are still limitations in their
studies. On one hand, researchers overlook the heterogeneity within edges,
where there can be both correct and incorrect answers. On the other hand, they
disregard the uncertainty within edges, e.g., a correct answer can indicate
true mastery or fortunate guessing. To address the limitations, we propose
Adaptive Semantic-aware Graph-based Cognitive Diagnosis model (ASG-CD), which
introduces a novel and effective way to leverage bipartite graph information in
CD. Specifically, we first map students, exercises, and knowledge concepts into
a latent representation space and combine these latent representations to
obtain student abilities and exercise difficulties. After that, we propose a
Semantic-aware Graph Neural Network Layer to address edge heterogeneity. This
layer splits the original bipartite graph into two subgraphs according to edge
semantics, and aggregates information based on these two subgraphs separately.
To mitigate the impact of edge uncertainties, we propose an Adaptive Edge
Differentiation Layer that dynamically differentiates edges, followed by
keeping reliable edges and filtering out uncertain edges. Extensive experiments
on three real-world datasets have demonstrated the effectiveness of ASG-CD.</div><div><a href='http://arxiv.org/abs/2403.05559v1'>2403.05559v1</a></div>
</div></div>
    <div><a href="arxiv_22.html">Prev (22)</a></div>
    <div><a href="arxiv_24.html">Next (24)</a></div>
    