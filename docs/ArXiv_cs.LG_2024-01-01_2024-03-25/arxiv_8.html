
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14123v1")'>AI and Memory Wall</div>
<div id='2403.14123v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T04:31:59Z</div><div>Authors: Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, Kurt Keutzer</div><div style='padding-top: 10px; width: 80ex'>The availability of unprecedented unsupervised training data, along with
neural scaling laws, has resulted in an unprecedented surge in model size and
compute requirements for serving/training LLMs. However, the main performance
bottleneck is increasingly shifting to memory bandwidth. Over the past 20
years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the
growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and
1.4 times every 2 years, respectively. This disparity has made memory, rather
than compute, the primary bottleneck in AI applications, particularly in
serving. Here, we analyze encoder and decoder Transformer models and show how
memory bandwidth can become the dominant bottleneck for decoder models. We
argue for a redesign in model architecture, training, and deployment strategies
to overcome this memory limitation.</div><div><a href='http://arxiv.org/abs/2403.14123v1'>2403.14123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17460v1")'>Rendering Wireless Environments Useful for Gradient Estimators: A
  Zero-Order Stochastic Federated Learning Method</div>
<div id='2401.17460v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T21:46:09Z</div><div>Authors: Elissa Mhanna, Mohamad Assaad</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL) is a novel approach to machine learning that allows
multiple edge devices to collaboratively train a model without disclosing their
raw data. However, several challenges hinder the practical implementation of
this approach, especially when devices and the server communicate over wireless
channels, as it suffers from communication and computation bottlenecks in this
case. By utilizing a communication-efficient framework, we propose a novel
zero-order (ZO) method with a one-point gradient estimator that harnesses the
nature of the wireless communication channel without requiring the knowledge of
the channel state coefficient. It is the first method that includes the
wireless channel in the learning algorithm itself instead of wasting resources
to analyze it and remove its impact. The two main difficulties of this work are
that in FL, the objective function is usually not convex, which makes the
extension of FL to ZO methods challenging, and that including the impact of
wireless channels requires extra attention. However, we overcome these
difficulties and comprehensively analyze the proposed zero-order federated
learning (ZOFL) framework. We establish its convergence theoretically, and we
prove a convergence rate of $O(\frac{1}{\sqrt[3]{K}})$ in the nonconvex
setting. We further demonstrate the potential of our algorithm with
experimental results, taking into account independent and identically
distributed (IID) and non-IID device data distributions.</div><div><a href='http://arxiv.org/abs/2401.17460v1'>2401.17460v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06528v1")'>Adaptive Federated Learning Over the Air</div>
<div id='2403.06528v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T09:10:37Z</div><div>Authors: Chenhao Wang, Zihan Chen, Nikolaos Pappas, Howard H. Yang, Tony Q. S. Quek, H. Vincent Poor</div><div style='padding-top: 10px; width: 80ex'>We propose a federated version of adaptive gradient methods, particularly
AdaGrad and Adam, within the framework of over-the-air model training. This
approach capitalizes on the inherent superposition property of wireless
channels, facilitating fast and scalable parameter aggregation. Meanwhile, it
enhances the robustness of the model training process by dynamically adjusting
the stepsize in accordance with the global gradient update. We derive the
convergence rate of the training algorithms, encompassing the effects of
channel fading and interference, for a broad spectrum of nonconvex loss
functions. Our analysis shows that the AdaGrad-based algorithm converges to a
stationary point at the rate of $\mathcal{O}( \ln{(T)} /{ T^{ 1 -
\frac{1}{\alpha} } } )$, where $\alpha$ represents the tail index of the
electromagnetic interference. This result indicates that the level of
heavy-tailedness in interference distribution plays a crucial role in the
training efficiency: the heavier the tail, the slower the algorithm converges.
In contrast, an Adam-like algorithm converges at the $\mathcal{O}( 1/T )$ rate,
demonstrating its advantage in expediting the model training process. We
conduct extensive experiments that corroborate our theoretical findings and
affirm the practical efficacy of our proposed federated adaptive gradient
methods.</div><div><a href='http://arxiv.org/abs/2403.06528v1'>2403.06528v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01442v1")'>Hierarchical Over-the-Air Federated Learning with Awareness of
  Interference and Data Heterogeneity</div>
<div id='2401.01442v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T21:43:01Z</div><div>Authors: Seyed Mohammad Azimi-Abarghouyi, Viktoria Fodor</div><div style='padding-top: 10px; width: 80ex'>When implementing hierarchical federated learning over wireless networks,
scalability assurance and the ability to handle both interference and device
data heterogeneity are crucial. This work introduces a learning method designed
to address these challenges, along with a scalable transmission scheme that
efficiently uses a single wireless resource through over-the-air computation.
To provide resistance against data heterogeneity, we employ gradient
aggregations. Meanwhile, the impact of interference is minimized through
optimized receiver normalizing factors. For this, we model a multi-cluster
wireless network using stochastic geometry, and characterize the mean squared
error of the aggregation estimations as a function of the network parameters.
We show that despite the interference and the data heterogeneity, the proposed
scheme achieves high learning accuracy and can significantly outperform the
conventional hierarchical algorithm.</div><div><a href='http://arxiv.org/abs/2401.01442v1'>2401.01442v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04431v1")'>Boosting Fairness and Robustness in Over-the-Air Federated Learning</div>
<div id='2403.04431v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T12:03:04Z</div><div>Authors: Halil Yigit Oksuz, Fabio Molinari, Henning Sprekeler, Joerg Raisch</div><div style='padding-top: 10px; width: 80ex'>Over-the-Air Computation is a beyond-5G communication strategy that has
recently been shown to be useful for the decentralized training of machine
learning models due to its efficiency. In this paper, we propose an
Over-the-Air federated learning algorithm that aims to provide fairness and
robustness through minmax optimization. By using the epigraph form of the
problem at hand, we show that the proposed algorithm converges to the optimal
solution of the minmax problem. Moreover, the proposed approach does not
require reconstructing channel coefficients by complex encoding-decoding
schemes as opposed to state-of-the-art approaches. This improves both
efficiency and privacy.</div><div><a href='http://arxiv.org/abs/2403.04431v1'>2403.04431v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00318v2")'>Analog-digital Scheduling for Federated Learning: A
  Communication-Efficient Approach</div>
<div id='2402.00318v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T04:05:24Z</div><div>Authors: Muhammad Faraz Ul Abrar, Nicolò Michelusi</div><div style='padding-top: 10px; width: 80ex'>Over-the-air (OTA) computation has recently emerged as a
communication-efficient Federated Learning (FL) paradigm to train machine
learning models over wireless networks. However, its performance is limited by
the device with the worst SNR, resulting in fast yet noisy updates. On the
other hand, allocating orthogonal resource blocks (RB) to individual devices
via digital channels mitigates the noise problem, at the cost of increased
communication latency. In this paper, we address this discrepancy and present
ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server
(PS) schedules each device to either upload its gradient via the analog OTA
scheme or transmit its quantized gradient over an orthogonal RB using the
``digital" scheme. Focusing on a single FL round, we cast the optimal
scheduling problem as the minimization of the mean squared error (MSE) on the
estimated global gradient at the PS, subject to a delay constraint, yielding
the optimal device scheduling configuration and quantization bits for the
digital devices. Our simulation results show that ADFL, by scheduling most of
the devices in the OTA scheme while also occasionally employing the digital
scheme for a few devices, consistently outperforms OTA-only and digital-only
schemes, in both i.i.d. and non-i.i.d. settings.</div><div><a href='http://arxiv.org/abs/2402.00318v2'>2402.00318v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09657v1")'>Digital versus Analog Transmissions for Federated Learning over Wireless
  Networks</div>
<div id='2402.09657v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T01:50:46Z</div><div>Authors: Jiacheng Yao, Wei Xu, Zhaohui Yang, Xiaohu You, Mehdi Bennis, H. Vincent Poor</div><div style='padding-top: 10px; width: 80ex'>In this paper, we quantitatively compare these two effective communication
schemes, i.e., digital and analog ones, for wireless federated learning (FL)
over resource-constrained networks, highlighting their essential differences as
well as their respective application scenarios. We first examine both digital
and analog transmission methods, together with a unified and fair comparison
scheme under practical constraints. A universal convergence analysis under
various imperfections is established for FL performance evaluation in wireless
networks. These analytical results reveal that the fundamental difference
between the two paradigms lies in whether communication and computation are
jointly designed or not. The digital schemes decouple the communication design
from specific FL tasks, making it difficult to support simultaneous uplink
transmission of massive devices with limited bandwidth. In contrast, the analog
communication allows over-the-air computation (AirComp), thus achieving
efficient spectrum utilization. However, computation-oriented analog
transmission reduces power efficiency, and its performance is sensitive to
computational errors. Finally, numerical simulations are conducted to verify
these theoretical observations.</div><div><a href='http://arxiv.org/abs/2402.09657v1'>2402.09657v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00321v1")'>DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things</div>
<div id='2403.00321v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T06:48:58Z</div><div>Authors: Yulin Shao</div><div style='padding-top: 10px; width: 80ex'>At the heart of the Internet of Things (IoT) -- a domain witnessing explosive
growth -- the imperative for energy efficiency and the extension of device
lifespans has never been more pressing. This paper presents DEEP-IoT, a
revolutionary communication paradigm poised to redefine how IoT devices
communicate. Through a pioneering "listen more, transmit less" strategy,
DEEP-IoT challenges and transforms the traditional transmitter (IoT
devices)-centric communication model to one where the receiver (the access
point) play a pivotal role, thereby cutting down energy use and boosting device
longevity. We not only conceptualize DEEP-IoT but also actualize it by
integrating deep learning-enhanced feedback channel codes within a narrow-band
system. Simulation results show a significant enhancement in the operational
lifespan of IoT cells -- surpassing traditional systems using Turbo and Polar
codes by up to 52.71%. This leap signifies a paradigm shift in IoT
communications, setting the stage for a future where IoT devices boast
unprecedented efficiency and durability.</div><div><a href='http://arxiv.org/abs/2403.00321v1'>2403.00321v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04837v2")'>T-PRIME: Transformer-based Protocol Identification for Machine-learning
  at the Edge</div>
<div id='2401.04837v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T22:01:55Z</div><div>Authors: Mauro Belgiovine, Joshua Groen, Miquel Sirera, Chinenye Tassie, Ayberk Yarkın Yıldız, Sage Trudeau, Stratis Ioannidis, Kaushik Chowdhury</div><div style='padding-top: 10px; width: 80ex'>Spectrum sharing allows different protocols of the same standard (e.g.,
802.11 family) or different standards (e.g., LTE and DVB) to coexist in
overlapping frequency bands. As this paradigm continues to spread, wireless
systems must also evolve to identify active transmitters and unauthorized
waveforms in real time under intentional distortion of preambles, extremely low
signal-to-noise ratios and challenging channel conditions. We overcome
limitations of correlation-based preamble matching methods in such conditions
through the design of T-PRIME: a Transformer-based machine learning approach.
T-PRIME learns the structural design of transmitted frames through its
attention mechanism, looking at sequence patterns that go beyond the preamble
alone. The paper makes three contributions: First, it compares Transformer
models and demonstrates their superiority over traditional methods and
state-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's
real-time feasibility on DeepWave's AIR-T platform. Third, it utilizes an
extensive 66 GB dataset of over-the-air (OTA) WiFi transmissions for training,
which is released along with the code for community use. Results reveal nearly
perfect (i.e. $&gt;98\%$) classification accuracy under simulated scenarios,
showing $100\%$ detection improvement over legacy methods in low SNR ranges,
$97\%$ classification accuracy for OTA single-protocol transmissions and up to
$75\%$ double-protocol classification accuracy in interference scenarios.</div><div><a href='http://arxiv.org/abs/2401.04837v2'>2401.04837v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03150v1")'>Deep-Learned Compression for Radio-Frequency Signal Classification</div>
<div id='2403.03150v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T17:42:39Z</div><div>Authors: Armani Rodriguez, Yagna Kaasaragadda, Silvija Kokalj-Filipovic</div><div style='padding-top: 10px; width: 80ex'>Next-generation cellular concepts rely on the processing of large quantities
of radio-frequency (RF) samples. This includes Radio Access Networks (RAN)
connecting the cellular front-end based on software defined radios (SDRs) and a
framework for the AI processing of spectrum-related data. The RF data collected
by the dense RAN radio units and spectrum sensors may need to be jointly
processed for intelligent decision making. Moving large amounts of data to AI
agents may result in significant bandwidth and latency costs. We propose a deep
learned compression (DLC) model, HQARF, based on learned vector quantization
(VQ), to compress the complex-valued samples of RF signals comprised of 6
modulation classes. We are assessing the effects of HQARF on the performance of
an AI model trained to infer the modulation class of the RF signal. Compression
of narrow-band RF samples for the training and off-the-site inference will
allow for an efficient use of the bandwidth and storage for non-real-time
analytics, and for a decreased delay in real-time applications. While exploring
the effectiveness of the HQARF signal reconstructions in modulation
classification tasks, we highlight the DLC optimization space and some open
problems related to the training of the VQ embedded in HQARF.</div><div><a href='http://arxiv.org/abs/2403.03150v1'>2403.03150v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04036v1")'>Unsupervised Contrastive Learning for Robust RF Device Fingerprinting
  Under Time-Domain Shift</div>
<div id='2403.04036v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T20:33:55Z</div><div>Authors: Jun Chen, Weng-Keen Wong, Bechir Hamdaoui</div><div style='padding-top: 10px; width: 80ex'>Radio Frequency (RF) device fingerprinting has been recognized as a potential
technology for enabling automated wireless device identification and
classification. However, it faces a key challenge due to the domain shift that
could arise from variations in the channel conditions and environmental
settings, potentially degrading the accuracy of RF-based device classification
when testing and training data is collected in different domains. This paper
introduces a novel solution that leverages contrastive learning to mitigate
this domain shift problem. Contrastive learning, a state-of-the-art
self-supervised learning approach from deep learning, learns a distance metric
such that positive pairs are closer (i.e. more similar) in the learned metric
space than negative pairs. When applied to RF fingerprinting, our model treats
RF signals from the same transmission as positive pairs and those from
different transmissions as negative pairs. Through experiments on wireless and
wired RF datasets collected over several days, we demonstrate that our
contrastive learning approach captures domain-invariant features, diminishing
the effects of domain-specific variations. Our results show large and
consistent improvements in accuracy (10.8\% to 27.8\%) over baseline models,
thus underscoring the effectiveness of contrastive learning in improving device
classification under domain shift.</div><div><a href='http://arxiv.org/abs/2403.04036v1'>2403.04036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09710v1")'>Preserving Data Privacy for ML-driven Applications in Open Radio Access
  Networks</div>
<div id='2402.09710v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T05:06:53Z</div><div>Authors: Pranshav Gajjar, Azuka Chiejina, Vijay K. Shah</div><div style='padding-top: 10px; width: 80ex'>Deep learning offers a promising solution to improve spectrum access
techniques by utilizing data-driven approaches to manage and share limited
spectrum resources for emerging applications. For several of these
applications, the sensitive wireless data (such as spectrograms) are stored in
a shared database or multistakeholder cloud environment and are therefore prone
to privacy leaks. This paper aims to address such privacy concerns by examining
the representative case study of shared database scenarios in 5G Open Radio
Access Network (O-RAN) networks where we have a shared database within the
near-real-time (near-RT) RAN intelligent controller. We focus on securing the
data that can be used by machine learning (ML) models for spectrum sharing and
interference mitigation applications without compromising the model and network
performances. The underlying idea is to leverage a (i) Shuffling-based
learnable encryption technique to encrypt the data, following which, (ii)
employ a custom Vision transformer (ViT) as the trained ML model that is
capable of performing accurate inferences on such encrypted data. The paper
offers a thorough analysis and comparisons with analogous convolutional neural
networks (CNN) as well as deeper architectures (such as ResNet-50) as
baselines. Our experiments showcase that the proposed approach significantly
outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the
percent accuracy and F1-Score respectively when operated on encrypted data.
Though deeper ResNet-50 architecture is obtained as a slightly more accurate
model, with an increase of 4.4%, the proposed approach boasts a reduction of
parameters by 99.32%, and thus, offers a much-improved prediction time by
nearly 60%.</div><div><a href='http://arxiv.org/abs/2402.09710v1'>2402.09710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05436v1")'>Deep OFDM Channel Estimation: Capturing Frequency Recurrence</div>
<div id='2401.05436v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T14:13:08Z</div><div>Authors: Abu Shafin Mohammad Mahdee Jameel, Akshay Malhotra, Aly El Gamal, Shahab Hamidi-Rad</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a deep-learning-based channel estimation scheme in
an orthogonal frequency division multiplexing (OFDM) system. Our proposed
method, named Single Slot Recurrence Along Frequency Network (SisRafNet), is
based on a novel study of recurrent models for exploiting sequential behavior
of channels across frequencies. Utilizing the fact that wireless channels have
a high degree of correlation across frequencies, we employ recurrent neural
network techniques within a single OFDM slot, thus overcoming the latency and
memory constraints typically associated with recurrence based methods. The
proposed SisRafNet delivers superior estimation performance compared to
existing deep-learning-based channel estimation techniques and the performance
has been validated on a wide range of 3rd Generation Partnership Project (3GPP)
compliant channel scenarios at multiple signal-to-noise ratios.</div><div><a href='http://arxiv.org/abs/2401.05436v1'>2401.05436v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12645v1")'>On the Robustness of Deep Learning-aided Symbol Detectors to Varying
  Conditions and Imperfect Channel Knowledge</div>
<div id='2401.12645v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:55:29Z</div><div>Authors: Chin-Hung Chen, Boris Karanov, Wim van Houtum, Wu Yan, Alex Young, Alex Alvarado</div><div style='padding-top: 10px; width: 80ex'>Recently, a data-driven Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm tailored to
channels with intersymbol interference has been introduced. This so-called
BCJRNet algorithm utilizes neural networks to calculate channel likelihoods.
BCJRNet has demonstrated resilience against inaccurate channel tap estimations
when applied to a time-invariant channel with ideal exponential decay profiles.
However, its generalization capabilities for practically-relevant time-varying
channels, where the receiver can only access incorrect channel parameters,
remain largely unexplored. The primary contribution of this paper is to expand
upon the results from existing literature to encompass a variety of imperfect
channel knowledge cases that appear in real-world transmissions. Our findings
demonstrate that BCJRNet significantly outperforms the conventional BCJR
algorithm for stationary transmission scenarios when learning from noisy
channel data and with imperfect channel decay profiles. However, this advantage
is shown to diminish when the operating channel is also rapidly time-varying.
Our results also show the importance of memory assumptions for conventional
BCJR and BCJRNet. An underestimation of the memory largely degrades the
performance of both BCJR and BCJRNet, especially in a slow-decaying channel. To
mimic a situation closer to a practical scenario, we also combined channel tap
uncertainty with imperfect channel memory knowledge. Somewhat surprisingly, our
results revealed improved performance when employing the conventional BCJR with
an underestimated memory assumption. BCJRNet, on the other hand, showed a
consistent performance improvement as the level of accurate memory knowledge
increased.</div><div><a href='http://arxiv.org/abs/2401.12645v1'>2401.12645v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12400v1")'>Finding the Missing Data: A BERT-inspired Approach Against Package Loss
  in Wireless Sensing</div>
<div id='2403.12400v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T03:16:52Z</div><div>Authors: Zijian Zhao, Tingwei Chen, Fanyi Meng, Hang Li, Xiaoyang Li, Guangxu Zhu</div><div style='padding-top: 10px; width: 80ex'>Despite the development of various deep learning methods for Wi-Fi sensing,
package loss often results in noncontinuous estimation of the Channel State
Information (CSI), which negatively impacts the performance of the learning
models. To overcome this challenge, we propose a deep learning model based on
Bidirectional Encoder Representations from Transformers (BERT) for CSI
recovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner
on the target dataset without the need for additional data. Furthermore, unlike
traditional interpolation methods that focus on one subcarrier at a time,
CSI-BERT captures the sequential relationships across different subcarriers.
Experimental results demonstrate that CSI-BERT achieves lower error rates and
faster speed compared to traditional interpolation methods, even when facing
with high loss rates. Moreover, by harnessing the recovered CSI obtained from
CSI-BERT, other deep learning models like Residual Network and Recurrent Neural
Network can achieve an average increase in accuracy of approximately 15\% in
Wi-Fi sensing tasks. The collected dataset WiGesture and code for our model are
publicly available at https://github.com/RS2002/CSI-BERT.</div><div><a href='http://arxiv.org/abs/2403.12400v1'>2403.12400v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08318v2")'>OpenDPD: An Open-Source End-to-End Learning &amp; Benchmarking Framework for
  Wideband Power Amplifier Modeling and Digital Pre-Distortion</div>
<div id='2401.08318v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T12:36:17Z</div><div>Authors: Yizhuo Wu, Gagan Deep Singh, Mohammadreza Beikmirza, Leo C. N. de Vreede, Morteza Alavi, Chang Gao</div><div style='padding-top: 10px; width: 80ex'>With the rise in communication capacity, deep neural networks (DNN) for
digital pre-distortion (DPD) to correct non-linearity in wideband power
amplifiers (PAs) have become prominent. Yet, there is a void in open-source and
measurement-setup-independent platforms for fast DPD exploration and objective
DPD model comparison. This paper presents an open-source framework, OpenDPD,
crafted in PyTorch, with an associated dataset for PA modeling and DPD
learning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a
novel end-to-end learning architecture, outperforming previous DPD models on a
digital PA (DPA) in the new digital transmitter (DTX) architecture with
unconventional transfer characteristics compared to analog PAs. Measurements
show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB
for 200 MHz OFDM signals. OpenDPD code, datasets, and documentation are
publicly available at https://github.com/lab-emi/OpenDPD.</div><div><a href='http://arxiv.org/abs/2401.08318v2'>2401.08318v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15288v1")'>Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical
  Communications</div>
<div id='2402.15288v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:33:27Z</div><div>Authors: Jonas Ney, Patrick Matalla, Vincent Lauinger, Laurent Schmalen, Sebastian Randel, Norbert Wehn</div><div style='padding-top: 10px; width: 80ex'>In this work, we present a high-throughput field programmable gate array
(FPGA) demonstrator of an artificial neural network (ANN)-based equalizer. The
equalization is performed and illustrated in real-time for a 30 GBd, two-level
pulse amplitude modulation (PAM2) optical communication system.</div><div><a href='http://arxiv.org/abs/2402.15288v1'>2402.15288v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01531v1")'>Will 6G be Semantic Communications? Opportunities and Challenges from
  Task Oriented and Secure Communications to Integrated Sensing</div>
<div id='2401.01531v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T04:01:20Z</div><div>Authors: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</div><div style='padding-top: 10px; width: 80ex'>This paper explores opportunities and challenges of task (goal)-oriented and
semantic communications for next-generation (NextG) communication networks
through the integration of multi-task learning. This approach employs deep
neural networks representing a dedicated encoder at the transmitter and
multiple task-specific decoders at the receiver, collectively trained to handle
diverse tasks including semantic information preservation, source input
reconstruction, and integrated sensing and communications. To extend the
applicability from point-to-point links to multi-receiver settings, we envision
the deployment of decoders at various receivers, where decentralized learning
addresses the challenges of communication load and privacy concerns, leveraging
federated learning techniques that distribute model updates across
decentralized nodes. However, the efficacy of this approach is contingent on
the robustness of the employed deep learning models. We scrutinize potential
vulnerabilities stemming from adversarial attacks during both training and
testing phases. These attacks aim to manipulate both the inputs at the encoder
at the transmitter and the signals received over the air on the receiver side,
highlighting the importance of fortifying semantic communications against
potential multi-domain exploits. Overall, the joint and robust design of
task-oriented communications, semantic communications, and integrated sensing
and communications in a multi-task learning framework emerges as the key
enabler for context-aware, resource-efficient, and secure communications
ultimately needed in NextG network systems.</div><div><a href='http://arxiv.org/abs/2401.01531v1'>2401.01531v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11656v1")'>Integrating Pre-Trained Language Model with Physical Layer
  Communications</div>
<div id='2402.11656v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T17:27:51Z</div><div>Authors: Ju-Hyung Lee, Dong-Ho Lee, Joohan Lee, Jay Pujara</div><div style='padding-top: 10px; width: 80ex'>The burgeoning field of on-device AI communication, where devices exchange
information directly through embedded foundation models, such as language
models (LMs), requires robust, efficient, and generalizable communication
frameworks. However, integrating these frameworks with existing wireless
systems and effectively managing noise and bit errors pose significant
challenges. In this work, we introduce a practical on-device AI communication
framework, integrated with physical layer (PHY) communication functions,
demonstrated through its performance on a link-level simulator. Our framework
incorporates end-to-end training with channel noise to enhance resilience,
incorporates vector quantized variational autoencoders (VQ-VAE) for efficient
and robust communication, and utilizes pre-trained encoder-decoder transformers
for improved generalization capabilities. Simulations, across various
communication scenarios, reveal that our framework achieves a 50% reduction in
transmission size while demonstrating substantial generalization ability and
noise robustness under standardized 3GPP channel models.</div><div><a href='http://arxiv.org/abs/2402.11656v1'>2402.11656v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10072v1")'>Deep Joint Source-Channel Coding for Efficient and Reliable
  Cross-Technology Communication</div>
<div id='2402.10072v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T04:55:37Z</div><div>Authors: Shumin Yao, Xiaodong Xu, Hao Chen, Yaping Sun, Qinglin Zhao</div><div style='padding-top: 10px; width: 80ex'>Cross-technology communication (CTC) is a promising technique that enables
direct communications among incompatible wireless technologies without needing
hardware modification. However, it has not been widely adopted in real-world
applications due to its inefficiency and unreliability. To address this issue,
this paper proposes a deep joint source-channel coding (DJSCC) scheme to enable
efficient and reliable CTC. The proposed scheme builds a neural-network-based
encoder and decoder at the sender side and the receiver side, respectively, to
achieve two critical tasks simultaneously: 1) compressing the messages to the
point where only their essential semantic meanings are preserved; 2) ensuring
the robustness of the semantic meanings when they are transmitted across
incompatible technologies. The scheme incorporates existing CTC coding
algorithms as domain knowledge to guide the encoder-decoder pair to learn the
characteristics of CTC links better. Moreover, the scheme constructs shared
semantic knowledge for the encoder and decoder, allowing semantic meanings to
be converted into very few bits for cross-technology transmissions, thus
further improving the efficiency of CTC. Extensive simulations verify that the
proposed scheme can reduce the transmission overhead by up to 97.63\% and
increase the structural similarity index measure by up to 734.78%, compared
with the state-of-the-art CTC scheme.</div><div><a href='http://arxiv.org/abs/2402.10072v1'>2402.10072v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02645v2")'>DT-DDNN: A Physical Layer Security Attack Detector in 5G RF Domain for
  CAVs</div>
<div id='2403.02645v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T04:29:31Z</div><div>Authors: Ghazal Asemian, Mohammadreza Amini, Burak Kantarci, Melike Erol-Kantarci</div><div style='padding-top: 10px; width: 80ex'>The Synchronization Signal Block (SSB) is a fundamental component of the 5G
New Radio (NR) air interface, crucial for the initial access procedure of
Connected and Automated Vehicles (CAVs), and serves several key purposes in the
network's operation. However, due to the predictable nature of SSB
transmission, including the Primary and Secondary Synchronization Signals (PSS
and SSS), jamming attacks are critical threats. These attacks, which can be
executed without requiring high power or complex equipment, pose substantial
risks to the 5G network, particularly as a result of the unencrypted
transmission of control signals. Leveraging RF domain knowledge, this work
presents a novel deep learning-based technique for detecting jammers in CAV
networks. Unlike the existing jamming detection algorithms that mostly rely on
network parameters, we introduce a double-threshold deep learning jamming
detector by focusing on the SSB. The detection method is focused on RF domain
features and improves the robustness of the network without requiring
integration with the pre-existing network infrastructure. By integrating a
preprocessing block to extract PSS correlation and energy per null resource
elements (EPNRE) characteristics, our method distinguishes between normal and
jammed received signals with high precision. Additionally, by incorporating of
Discrete Wavelet Transform (DWT), the efficacy of training and detection are
optimized. A double-threshold double Deep Neural Network (DT-DDNN) is also
introduced to the architecture complemented by a deep cascade learning model to
increase the sensitivity of the model to variations of signal-to-jamming noise
ratio (SJNR). Results show that the proposed method achieves 96.4% detection
rate in extra low jamming power, i.e., SJNR between 15 to 30 dB. Further,
performance of DT-DDNN is validated by analyzing real 5G signals obtained from
a practical testbed.</div><div><a href='http://arxiv.org/abs/2403.02645v2'>2403.02645v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10724v1")'>Real-Time Zero-Day Intrusion Detection System for Automotive Controller
  Area Network on FPGAs</div>
<div id='2401.10724v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T14:36:01Z</div><div>Authors: Shashwat Khandelwal, Shreejith Shanker</div><div style='padding-top: 10px; width: 80ex'>Increasing automation in vehicles enabled by increased connectivity to the
outside world has exposed vulnerabilities in previously siloed automotive
networks like controller area networks (CAN). Attributes of CAN such as
broadcast-based communication among electronic control units (ECUs) that
lowered deployment costs are now being exploited to carry out active injection
attacks like denial of service (DoS), fuzzing, and spoofing attacks. Research
literature has proposed multiple supervised machine learning models deployed as
Intrusion detection systems (IDSs) to detect such malicious activity; however,
these are largely limited to identifying previously known attack vectors. With
the ever-increasing complexity of active injection attacks, detecting zero-day
(novel) attacks in these networks in real-time (to prevent propagation) becomes
a problem of particular interest. This paper presents an
unsupervised-learning-based convolutional autoencoder architecture for
detecting zero-day attacks, which is trained only on benign (attack-free) CAN
messages. We quantise the model using Vitis-AI tools from AMD/Xilinx targeting
a resource-constrained Zynq Ultrascale platform as our IDS-ECU system for
integration. The proposed model successfully achieves equal or higher
classification accuracy (&gt; 99.5%) on unseen DoS, fuzzing, and spoofing attacks
from a publicly available attack dataset when compared to the state-of-the-art
unsupervised learning-based IDSs. Additionally, by cleverly overlapping IDS
operation on a window of CAN messages with the reception, the model is able to
meet line-rate detection (0.43 ms per window) of high-speed CAN, which when
coupled with the low energy consumption per inference, makes this architecture
ideally suited for detecting zero-day attacks on critical CAN networks.</div><div><a href='http://arxiv.org/abs/2401.10724v1'>2401.10724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10689v1")'>A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid
  FPGAs</div>
<div id='2401.10689v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T13:39:05Z</div><div>Authors: Shashwat Khandelwal, Shreejith Shanker</div><div style='padding-top: 10px; width: 80ex'>Rising connectivity in vehicles is enabling new capabilities like connected
autonomous driving and advanced driver assistance systems (ADAS) for improving
the safety and reliability of next-generation vehicles. This increased access
to in-vehicle functions compromises critical capabilities that use legacy
invehicle networks like Controller Area Network (CAN), which has no inherent
security or authentication mechanism. Intrusion detection and mitigation
approaches, particularly using machine learning models, have shown promising
results in detecting multiple attack vectors in CAN through their ability to
generalise to new vectors. However, most deployments require dedicated
computing units like GPUs to perform line-rate detection, consuming much higher
power. In this paper, we present a lightweight multi-attack quantised machine
learning model that is deployed using Xilinx's Deep Learning Processing Unit IP
on a Zynq Ultrascale+ (XCZU3EG) FPGA, which is trained and validated using the
public CAN Intrusion Detection dataset. The quantised model detects denial of
service and fuzzing attacks with an accuracy of above 99 % and a false positive
rate of 0.07%, which are comparable to the state-of-the-art techniques in the
literature. The Intrusion Detection System (IDS) execution consumes just 2.0 W
with software tasks running on the ECU and achieves a 25 % reduction in
per-message processing latency over the state-of-the-art implementations. This
deployment allows the ECU function to coexist with the IDS with minimal changes
to the tasks, making it ideal for real-time IDS in in-vehicle systems.</div><div><a href='http://arxiv.org/abs/2401.10689v1'>2401.10689v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12234v1")'>A Lightweight FPGA-based IDS-ECU Architecture for Automotive CAN</div>
<div id='2401.12234v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T13:51:24Z</div><div>Authors: Shashwat Khandelwal, Shreejith Shanker</div><div style='padding-top: 10px; width: 80ex'>Recent years have seen an exponential rise in complex software-driven
functionality in vehicles, leading to a rising number of electronic control
units (ECUs), network capabilities, and interfaces. These expanded capabilities
also bring-in new planes of vulnerabilities making intrusion detection and
management a critical capability; however, this can often result in more ECUs
and network elements due to the high computational overheads. In this paper, we
present a consolidated ECU architecture incorporating an Intrusion Detection
System (IDS) for Automotive Controller Area Network (CAN) along with
traditional ECU functionality on an off-the-shelf hybrid FPGA device, with
near-zero overhead for the ECU functionality. We propose two quantised
multi-layer perceptrons (QMLP's) as isolated IDSs for detecting a range of
attack vectors including Denial-of-Service, Fuzzing and Spoofing, which are
accelerated using off-the-shelf deep-learning processing unit (DPU) IP block
from Xilinx, operating fully transparently to the software on the ECU. The
proposed models achieve the state-of-the-art classification accuracy for all
the attacks, while we observed a 15x reduction in power consumption when
compared against the GPU-based implementation of the same models quantised
using Nvidia libraries. We also achieved a 2.3x speed up in per-message
processing latency (at 0.24 ms from the arrival of a CAN message) to meet the
strict end-to-end latency on critical CAN nodes and a 2.6x reduction in power
consumption for inference when compared to the state-of-the-art IDS models on
embedded IDS and loosely coupled IDS accelerators (GPUs) discussed in the
literature.</div><div><a href='http://arxiv.org/abs/2401.12234v1'>2401.12234v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12240v1")'>Quantised Neural Network Accelerators for Low-Power IDS in Automotive
  Networks</div>
<div id='2401.12240v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T21:19:48Z</div><div>Authors: Shashwat Khandelwal, Anneliese Walsh, Shanker Shreejith</div><div style='padding-top: 10px; width: 80ex'>In this paper, we explore low-power custom quantised Multi-Layer Perceptrons
(MLPs) as an Intrusion Detection System (IDS) for automotive controller area
network (CAN). We utilise the FINN framework from AMD/Xilinx to quantise, train
and generate hardware IP of our MLP to detect denial of service (DoS) and
fuzzying attacks on CAN network, using ZCU104 (XCZU7EV) FPGA as our target ECU
architecture with integrated IDS capabilities. Our approach achieves
significant improvements in latency (0.12 ms per-message processing latency)
and inference energy consumption (0.25 mJ per inference) while achieving
similar classification performance as state-of-the-art approaches in the
literature.</div><div><a href='http://arxiv.org/abs/2401.12240v1'>2401.12240v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10674v1")'>Deep Learning-based Embedded Intrusion Detection System for Automotive
  CAN</div>
<div id='2401.10674v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T13:13:38Z</div><div>Authors: Shashwat Khandelwal, Eashan Wadhwa, Shreejith Shanker</div><div style='padding-top: 10px; width: 80ex'>Rising complexity of in-vehicle electronics is enabling new capabilities like
autonomous driving and active safety. However, rising automation also increases
risk of security threats which is compounded by lack of in-built security
measures in legacy networks like CAN, allowing attackers to observe, tamper and
modify information shared over such broadcast networks. Various intrusion
detection approaches have been proposed to detect and tackle such threats, with
machine learning models proving highly effective. However, deploying machine
learning models will require high processing power through high-end processors
or GPUs to perform them close to line rate. In this paper, we propose a hybrid
FPGA-based ECU approach that can transparently integrate IDS functionality
through a dedicated off-the-shelf hardware accelerator that implements a
deep-CNN intrusion detection model. Our results show that the proposed approach
provides an average accuracy of over 99% across multiple attack datasets with
0.64% false detection rates while consuming 94% less energy and achieving 51.8%
reduction in per-message processing latency when compared to IDS
implementations on GPUs.</div><div><a href='http://arxiv.org/abs/2401.10674v1'>2401.10674v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13563v1")'>DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced
  Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs</div>
<div id='2403.13563v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T12:56:40Z</div><div>Authors: Haoyu Wang, Basel Halak, Jianjie Ren, Ahmad Atamli</div><div style='padding-top: 10px; width: 80ex'>This study introduces a refined Flooding Injection Rate-adjustable
Denial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly
presents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame
Fusion (2F) for DoS detection and localization. Two Convolutional Neural
Networks models for classification and segmentation were developed to detect
and localize DoS respectively. It achieves detection and localization
accuracies of 95.8\% and 91.7\%, and precision rates of 98.5\% and 99.3\% in a
16x16 mesh NoC. The framework's hardware overhead notably decreases by 76.3\%
when scaling from 8x8 to 16x16 NoCs, and it requires 42.4\% less hardware
compared to state-of-the-arts. This advancement demonstrates DL2Fence's
effectiveness in balancing outstanding detection performance in large-scale
NoCs with extremely low hardware overhead.</div><div><a href='http://arxiv.org/abs/2403.13563v1'>2403.13563v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.11030v1")'>Exploring Highly Quantised Neural Networks for Intrusion Detection in
  Automotive CAN</div>
<div id='2401.11030v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T21:11:02Z</div><div>Authors: Shashwat Khandelwal, Shreejith Shanker</div><div style='padding-top: 10px; width: 80ex'>Vehicles today comprise intelligent systems like connected autonomous driving
and advanced driving assistance systems (ADAS) to enhance the driving
experience, which is enabled through increased connectivity to infrastructure
and fusion of information from different sensing modes. However, the rising
connectivity coupled with the legacy network architecture within vehicles can
be exploited for launching active and passive attacks on critical vehicle
systems and directly affecting the safety of passengers. Machine learning-based
intrusion detection models have been shown to successfully detect multiple
targeted attack vectors in recent literature, whose deployments are enabled
through quantised neural networks targeting low-power platforms. Multiple
models are often required to simultaneously detect multiple attack vectors,
increasing the area, (resource) cost, and energy consumption. In this paper, we
present a case for utilising custom-quantised MLP's (CQMLP) as a multi-class
classification model, capable of detecting multiple attacks from the benign
flow of controller area network (CAN) messages. The specific quantisation and
neural architecture are determined through a joint design space exploration,
resulting in our choice of the 2-bit precision and the n-layer MLP. Our 2-bit
version is trained using Brevitas and optimised as a dataflow hardware model
through the FINN toolflow from AMD/Xilinx, targeting an XCZU7EV device. We show
that the 2-bit CQMLP model, when integrated as the IDS, can detect malicious
attack messages (DoS, fuzzing, and spoofing attack) with a very high accuracy
of 99.9%, on par with the state-of-the-art methods in the literature.
Furthermore, the dataflow model can perform line rate detection at a latency of
0.11 ms from message reception while consuming 0.23 mJ/inference, making it
ideally suited for integration with an ECU in critical CAN networks.</div><div><a href='http://arxiv.org/abs/2401.11030v1'>2401.11030v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14862v1")'>SISSA: Real-time Monitoring of Hardware Functional Safety and
  Cybersecurity with In-vehicle SOME/IP Ethernet Traffic</div>
<div id='2402.14862v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T03:31:40Z</div><div>Authors: Qi Liu, Xingyu Li, Ke Sun, Yufeng Li, Yanchen Liu</div><div style='padding-top: 10px; width: 80ex'>Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet
communication standard protocol in the Automotive Open System Architecture
(AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However,
SOME/IP lacks a robust security architecture, making it susceptible to
potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP
communication. In this paper, we propose SISSA, a SOME/IP communication
traffic-based approach for modeling and analyzing in-vehicle functional safety
and cyber security. Specifically, SISSA models hardware failures with the
Weibull distribution and addresses five potential attacks on SOME/IP
communication, including Distributed Denial-of-Services, Man-in-the-Middle, and
abnormal communication processes, assuming a malicious user accesses the
in-vehicle network. Subsequently, SISSA designs a series of deep learning
models with various backbones to extract features from SOME/IP sessions among
ECUs. We adopt residual self-attention to accelerate the model's convergence
and enhance detection accuracy, determining whether an ECU is under attack,
facing functional failure, or operating normally. Additionally, we have created
and annotated a dataset encompassing various classes, including indicators of
attack, functionality, and normalcy. This contribution is noteworthy due to the
scarcity of publicly accessible datasets with such characteristics.Extensive
experimental results show the effectiveness and efficiency of SISSA.</div><div><a href='http://arxiv.org/abs/2402.14862v1'>2402.14862v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12803v1")'>Enhancements for 5G NR PRACH Reception: An AI/ML Approach</div>
<div id='2401.12803v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T10:44:23Z</div><div>Authors: Rohit Singh, Anil Kumar Yerrapragada, Jeeva Keshav S, Radha Krishna Ganti</div><div style='padding-top: 10px; width: 80ex'>Random Access is an important step in enabling the initial attachment of a
User Equipment (UE) to a Base Station (gNB). The UE identifies itself by
embedding a Preamble Index (RAPID) in the phase rotation of a known base
sequence, which it transmits on the Physical Random Access Channel (PRACH). The
signal on the PRACH also enables the estimation of propagation delay, often
known as Timing Advance (TA), which is induced by virtue of the UE's position.
Traditional receivers estimate the RAPID and TA using correlation-based
techniques. This paper presents an alternative receiver approach that uses
AI/ML models, wherein two neural networks are proposed, one for the RAPID and
one for the TA. Different from other works, these two models can run in
parallel as opposed to sequentially. Experiments with both simulated data and
over-the-air hardware captures highlight the improved performance of the
proposed AI/ML-based techniques compared to conventional correlation methods.</div><div><a href='http://arxiv.org/abs/2401.12803v1'>2401.12803v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12149v1")'>Personalized Over-the-Air Federated Learning with Personalized
  Reconfigurable Intelligent Surfaces</div>
<div id='2401.12149v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T17:36:23Z</div><div>Authors: Jiayu Mao, Aylin Yener</div><div style='padding-top: 10px; width: 80ex'>Over-the-air federated learning (OTA-FL) provides bandwidth-efficient
learning by leveraging the inherent superposition property of wireless
channels. Personalized federated learning balances performance for users with
diverse datasets, addressing real-life data heterogeneity. We propose the first
personalized OTA-FL scheme through multi-task learning, assisted by personal
reconfigurable intelligent surfaces (RIS) for each user. We take a cross-layer
approach that optimizes communication and computation resources for global and
personalized tasks in time-varying channels with imperfect channel state
information, using multi-task learning for non-i.i.d data. Our PROAR-PFed
algorithm adaptively designs power, local iterations, and RIS configurations.
We present convergence analysis for non-convex objectives and demonstrate that
PROAR-PFed outperforms state-of-the-art on the Fashion-MNIST dataset.</div><div><a href='http://arxiv.org/abs/2401.12149v1'>2401.12149v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01023v1")'>Federated Learning via Lattice Joint Source-Channel Coding</div>
<div id='2403.01023v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T22:53:57Z</div><div>Authors: Seyed Mohammad Azimi-Abarghouyi, Lav R. Varshney</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a universal federated learning framework that enables
over-the-air computation via digital communications, using a new joint
source-channel coding scheme. Without relying on channel state information at
devices, this scheme employs lattice codes to both quantize model parameters
and exploit interference from the devices. A novel two-layer receiver structure
at the server is designed to reliably decode an integer combination of the
quantized model parameters as a lattice point for the purpose of aggregation.
Numerical experiments validate the effectiveness of the proposed scheme. Even
with the challenges posed by channel conditions and device heterogeneity, the
proposed scheme markedly surpasses other over-the-air FL strategies.</div><div><a href='http://arxiv.org/abs/2403.01023v1'>2403.01023v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03157v1")'>Rethinking Clustered Federated Learning in NOMA Enhanced Wireless
  Networks</div>
<div id='2403.03157v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T17:49:09Z</div><div>Authors: Yushen Lin, Kaidi Wang, Zhiguo Ding</div><div style='padding-top: 10px; width: 80ex'>This study explores the benefits of integrating the novel clustered federated
learning (CFL) approach with non-orthogonal multiple access (NOMA) under
non-independent and identically distributed (non-IID) datasets, where multiple
devices participate in the aggregation with time limitations and a finite
number of sub-channels. A detailed theoretical analysis of the generalization
gap that measures the degree of non-IID in the data distribution is presented.
Following that, solutions to address the challenges posed by non-IID conditions
are proposed with the analysis of the properties. Specifically, users' data
distributions are parameterized as concentration parameters and grouped using
spectral clustering, with Dirichlet distribution serving as the prior. The
investigation into the generalization gap and convergence rate guides the
design of sub-channel assignments through the matching-based algorithm, and the
power allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the
derived closed-form solution. The extensive simulation results show that the
proposed cluster-based FL framework can outperform FL baselines in terms of
both test accuracy and convergence rate. Moreover, jointly optimizing
sub-channel and power allocation in NOMA-enhanced networks can lead to a
significant improvement.</div><div><a href='http://arxiv.org/abs/2403.03157v1'>2403.03157v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16775v2")'>Activity Detection for Massive Connectivity in Cell-free Networks with
  Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity
  Probability: A Bayesian Approach</div>
<div id='2401.16775v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:27:11Z</div><div>Authors: Hao Zhang, Qingfeng Lin, Yang Li, Lei Cheng, Yik-Chung Wu</div><div style='padding-top: 10px; width: 80ex'>Activity detection is an important task in the next generation grant-free
multiple access. While there are a number of existing algorithms designed for
this purpose, they mostly require precise information about the network, such
as large-scale fading coefficients, small-scale fading channel statistics,
noise variance at the access points, and user activity probability. Acquiring
these information would take a significant overhead and their estimated values
might not be accurate. This problem is even more severe in cell-free networks
as there are many of these parameters to be acquired. Therefore, this paper
sets out to investigate the activity detection problem without the
above-mentioned information. In order to handle so many unknown parameters,
this paper employs the Bayesian approach, where the unknown variables are
endowed with prior distributions which effectively act as regularizations.
Together with the likelihood function, a maximum a posteriori (MAP) estimator
and a variational inference algorithm are derived. Extensive simulations
demonstrate that the proposed methods, even without the knowledge of these
system parameters, perform better than existing state-of-the-art methods, such
as covariance-based and approximate message passing methods.</div><div><a href='http://arxiv.org/abs/2401.16775v2'>2401.16775v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.04896v1")'>Learning from the Best: Active Learning for Wireless Communications</div>
<div id='2402.04896v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T12:21:57Z</div><div>Authors: Nasim Soltani, Jifan Zhang, Batool Salehi, Debashri Roy, Robert Nowak, Kaushik Chowdhury</div><div style='padding-top: 10px; width: 80ex'>Collecting an over-the-air wireless communications training dataset for deep
learning-based communication tasks is relatively simple. However, labeling the
dataset requires expert involvement and domain knowledge, may involve private
intellectual properties, and is often computationally and financially
expensive. Active learning is an emerging area of research in machine learning
that aims to reduce the labeling overhead without accuracy degradation. Active
learning algorithms identify the most critical and informative samples in an
unlabeled dataset and label only those samples, instead of the complete set. In
this paper, we introduce active learning for deep learning applications in
wireless communications, and present its different categories. We present a
case study of deep learning-based mmWave beam selection, where labeling is
performed by a compute-intensive algorithm based on exhaustive search. We
evaluate the performance of different active learning algorithms on a publicly
available multi-modal dataset with different modalities including image and
LiDAR. Our results show that using an active learning algorithm for
class-imbalanced datasets can reduce labeling overhead by up to 50% for this
dataset while maintaining the same accuracy as classical training.</div><div><a href='http://arxiv.org/abs/2402.04896v1'>2402.04896v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12167v1")'>Dynamic Semantic Compression for CNN Inference in Multi-access Edge
  Computing: A Graph Reinforcement Learning-based Autoencoder</div>
<div id='2401.12167v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T15:19:47Z</div><div>Authors: Nan Li, Alexandros Iosifidis, Qi Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper studies the computational offloading of CNN inference in dynamic
multi-access edge computing (MEC) networks. To address the uncertainties in
communication time and computation resource availability, we propose a novel
semantic compression method, autoencoder-based CNN architecture (AECNN), for
effective semantic extraction and compression in partial offloading. In the
semantic encoder, we introduce a feature compression module based on the
channel attention mechanism in CNNs, to compress intermediate data by selecting
the most informative features. In the semantic decoder, we design a lightweight
decoder to reconstruct the intermediate data through learning from the received
compressed data to improve accuracy. To effectively trade-off communication,
computation, and inference accuracy, we design a reward function and formulate
the offloading problem of CNN inference as a maximization problem with the goal
of maximizing the average inference accuracy and throughput over the long term.
To address this maximization problem, we propose a graph reinforcement
learning-based AECNN (GRL-AECNN) method, which outperforms existing works
DROO-AECNN, GRL-BottleNet++ and GRL-DeepJSCC under different dynamic scenarios.
This highlights the advantages of GRL-AECNN in offloading decision-making in
dynamic MEC.</div><div><a href='http://arxiv.org/abs/2401.12167v1'>2401.12167v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14718v1")'>FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness
  in IoT System</div>
<div id='2403.14718v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T09:34:01Z</div><div>Authors: Jianjun Huang, Lixin Ye, Li Kang</div><div style='padding-top: 10px; width: 80ex'>In the Industrial Internet of Things (IoT), a large amount of data will be
generated every day. Due to privacy and security issues, it is difficult to
collect all these data together to train deep learning models, thus the
federated learning, a distributed machine learning paradigm that protects data
privacy, has been widely used in IoT. However, in practical federated learning,
the data distributions usually have large differences across devices, and the
heterogeneity of data will deteriorate the performance of the model. Moreover,
federated learning in IoT usually has a large number of devices involved in
training, and the limited communication resource of cloud servers become a
bottleneck for training. To address the above issues, in this paper, we combine
centralized federated learning with decentralized federated learning to design
a semi-decentralized cloud-edge-device hierarchical federated learning
framework, which can mitigate the impact of data heterogeneity, and can be
deployed at lage scale in IoT. To address the effect of data heterogeneity, we
use an incremental subgradient optimization algorithm in each ring cluster to
improve the generalization ability of the ring cluster models. Our extensive
experiments show that our approach can effectively mitigate the impact of data
heterogeneity and alleviate the communication bottleneck in cloud servers.</div><div><a href='http://arxiv.org/abs/2403.14718v1'>2403.14718v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09629v1")'>Smart Information Exchange for Unsupervised Federated Learning via
  Reinforcement Learning</div>
<div id='2402.09629v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T00:14:41Z</div><div>Authors: Seohyun Lee, Anindya Bijoy Das, Satyavrat Wagle, Christopher G. Brinton</div><div style='padding-top: 10px; width: 80ex'>One of the main challenges of decentralized machine learning paradigms such
as Federated Learning (FL) is the presence of local non-i.i.d. datasets.
Device-to-device transfers (D2D) between distributed devices has been shown to
be an effective tool for dealing with this problem and robust to stragglers. In
an unsupervised case, however, it is not obvious how data exchanges should take
place due to the absence of labels. In this paper, we propose an approach to
create an optimal graph for data transfer using Reinforcement Learning. The
goal is to form links that will provide the most benefit considering the
environment's constraints and improve convergence speed in an unsupervised FL
environment. Numerical analysis shows the advantages in terms of convergence
speed and straggler resilience of the proposed method to different available FL
schemes and benchmark datasets.</div><div><a href='http://arxiv.org/abs/2402.09629v1'>2402.09629v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02619v2")'>Training Machine Learning models at the Edge: A Survey</div>
<div id='2403.02619v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T03:18:43Z</div><div>Authors: Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal</div><div style='padding-top: 10px; width: 80ex'>Edge Computing (EC) has gained significant traction in recent years,
promising enhanced efficiency by integrating Artificial Intelligence (AI)
capabilities at the edge. While the focus has primarily been on the deployment
and inference of Machine Learning (ML) models at the edge, the training aspect
remains less explored. This survey delves into Edge Learning (EL), specifically
the optimization of ML model training at the edge. The objective is to
comprehensively explore diverse approaches and methodologies in EL, synthesize
existing knowledge, identify challenges, and highlight future trends. Utilizing
Scopus' advanced search, relevant literature on EL was identified, revealing a
concentration of research efforts in distributed learning methods, particularly
Federated Learning (FL). This survey further provides a guideline for comparing
techniques used to optimize ML for edge learning, along with an exploration of
different frameworks, libraries, and simulation tools available for EL. In
doing so, the paper contributes to a holistic understanding of the current
landscape and future directions in the intersection of edge computing and
machine learning, paving the way for informed comparisons between optimization
methods and techniques designed for edge learning.</div><div><a href='http://arxiv.org/abs/2403.02619v2'>2403.02619v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11259v1")'>A learning-based solution approach to the application placement problem
  in mobile edge computing under uncertainty</div>
<div id='2403.11259v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T16:23:00Z</div><div>Authors: Taha-Hossein Hejazi, Zahra Ghadimkhani, Arezoo Borji</div><div style='padding-top: 10px; width: 80ex'>Placing applications in mobile edge computing servers presents a complex
challenge involving many servers, users, and their requests. Existing
algorithms take a long time to solve high-dimensional problems with significant
uncertainty scenarios. Therefore, an efficient approach is required to maximize
the quality of service while considering all technical constraints. One of
these approaches is machine learning, which emulates optimal solutions for
application placement in edge servers. Machine learning models are expected to
learn how to allocate user requests to servers based on the spatial positions
of users and servers. In this study, the problem is formulated as a two-stage
stochastic programming. A sufficient amount of training records is generated by
varying parameters such as user locations, their request rates, and solving the
optimization model. Then, based on the distance features of each user from the
available servers and their request rates, machine learning models generate
decision variables for the first stage of the stochastic optimization model,
which is the user-to-server request allocation, and are employed as independent
decision agents that reliably mimic the optimization model. Support Vector
Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to
achieve practical decisions from the stochastic optimization models. The
performance of each model has shown an execution effectiveness of over 80%.
This research aims to provide a more efficient approach for tackling
high-dimensional problems and scenarios with uncertainties in mobile edge
computing by leveraging machine learning models for optimal decision-making in
request allocation to edge servers. These results suggest that machine-learning
models can significantly improve solution times compared to conventional
approaches.</div><div><a href='http://arxiv.org/abs/2403.11259v1'>2403.11259v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02668v1")'>Towards Integrated Fine-tuning and Inference when Generative AI meets
  Edge Intelligence</div>
<div id='2401.02668v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T06:52:55Z</div><div>Authors: Ning Chen, Zhipeng Cheng, Xuwei Fan, Xiaoyu Xia, Lianfen Huang</div><div style='padding-top: 10px; width: 80ex'>The high-performance generative artificial intelligence (GAI) represents the
latest evolution of computational intelligence, while the blessing of future 6G
networks also makes edge intelligence (EI) full of development potential. The
inevitable encounter between GAI and EI can unleash new opportunities, where
GAI's pre-training based on massive computing resources and large-scale
unlabeled corpora can provide strong foundational knowledge for EI, while EI
can harness fragmented computing resources to aggregate personalized knowledge
for GAI. However, the natural contradictory features pose significant
challenges to direct knowledge sharing. To address this, in this paper, we
propose the GAI-oriented synthetical network (GaisNet), a collaborative
cloud-edge-end intelligence framework that buffers contradiction leveraging
data-free knowledge relay, where the bidirectional knowledge flow enables GAI's
virtuous-cycle model fine-tuning and task inference, achieving mutualism
between GAI and EI with seamless fusion and collaborative evolution.
Experimental results demonstrate the effectiveness of the proposed mechanisms.
Finally, we discuss the future challenges and directions in the interplay
between GAI and EI.</div><div><a href='http://arxiv.org/abs/2401.02668v1'>2401.02668v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06989v1")'>Gradient Coreset for Federated Learning</div>
<div id='2401.06989v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T06:17:17Z</div><div>Authors: Durga Sivasubramanian, Lokesh Nagalapatti, Rishabh Iyer, Ganesh Ramakrishnan</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) is used to learn machine learning models with data
that is partitioned across multiple clients, including resource-constrained
edge devices. It is therefore important to devise solutions that are efficient
in terms of compute, communication, and energy consumption, while ensuring
compliance with the FL framework's privacy requirements. Conventional
approaches to these problems select a weighted subset of the training dataset,
known as coreset, and learn by fitting models on it. Such coreset selection
approaches are also known to be robust to data noise. However, these approaches
rely on the overall statistics of the training data and are not easily
extendable to the FL setup.
  In this paper, we propose an algorithm called Gradient based Coreset for
Robust and Efficient Federated Learning (GCFL) that selects a coreset at each
client, only every $K$ communication rounds and derives updates only from it,
assuming the availability of a small validation dataset at the server. We
demonstrate that our coreset selection technique is highly effective in
accounting for noise in clients' data. We conduct experiments using four
real-world datasets and show that GCFL is (1) more compute and energy efficient
than FL, (2) robust to various kinds of noise in both the feature space and
labels, (3) preserves the privacy of the validation dataset, and (4) introduces
a small communication overhead but achieves significant gains in performance,
particularly in cases when the clients' data is noisy.</div><div><a href='http://arxiv.org/abs/2401.06989v1'>2401.06989v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18606v1")'>Impact of network topology on the performance of Decentralized Federated
  Learning</div>
<div id='2402.18606v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T11:13:53Z</div><div>Authors: Luigi Palmieri, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti</div><div style='padding-top: 10px; width: 80ex'>Fully decentralized learning is gaining momentum for training AI models at
the Internet's edge, addressing infrastructure challenges and privacy concerns.
In a decentralized machine learning system, data is distributed across multiple
nodes, with each node training a local model based on its respective dataset.
The local models are then shared and combined to form a global model capable of
making accurate predictions on new data. Our exploration focuses on how
different types of network structures influence the spreading of knowledge -
the process by which nodes incorporate insights gained from learning patterns
in data available on other nodes across the network. Specifically, this study
investigates the intricate interplay between network structure and learning
performance using three network topologies and six data distribution methods.
These methods consider different vertex properties, including degree
centrality, betweenness centrality, and clustering coefficient, along with
whether nodes exhibit high or low values of these metrics. Our findings
underscore the significance of global centrality metrics (degree, betweenness)
in correlating with learning performance, while local clustering proves less
predictive. We highlight the challenges in transferring knowledge from
peripheral to central nodes, attributed to a dilution effect during model
aggregation. Additionally, we observe that central nodes exert a pull effect,
facilitating the spread of knowledge. In examining degree distribution, hubs in
Barabasi-Albert networks positively impact learning for central nodes but
exacerbate dilution when knowledge originates from peripheral nodes. Finally,
we demonstrate the formidable challenge of knowledge circulation outside of
segregated communities.</div><div><a href='http://arxiv.org/abs/2402.18606v1'>2402.18606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04784v1")'>Analysis of Privacy Leakage in Federated Large Language Models</div>
<div id='2403.04784v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T20:25:38Z</div><div>Authors: Minh N. Vu, Truc Nguyen, Tre' R. Jeter, My T. Thai</div><div style='padding-top: 10px; width: 80ex'>With the rapid adoption of Federated Learning (FL) as the training and tuning
protocol for applications utilizing Large Language Models (LLMs), recent
research highlights the need for significant modifications to FL to accommodate
the large-scale of LLMs. While substantial adjustments to the protocol have
been introduced as a response, comprehensive privacy analysis for the adapted
FL protocol is currently lacking.
  To address this gap, our work delves into an extensive examination of the
privacy analysis of FL when used for training LLMs, both from theoretical and
practical perspectives. In particular, we design two active membership
inference attacks with guaranteed theoretical success rates to assess the
privacy leakages of various adapted FL configurations. Our theoretical findings
are translated into practical attacks, revealing substantial privacy
vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and
OpenAI's GPTs, across multiple real-world language datasets. Additionally, we
conduct thorough experiments to evaluate the privacy leakage of these models
when data is protected by state-of-the-art differential privacy (DP)
mechanisms.</div><div><a href='http://arxiv.org/abs/2403.04784v1'>2403.04784v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07334v1")'>Differentially Private Training of Mixture of Experts Models</div>
<div id='2402.07334v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T23:57:09Z</div><div>Authors: Pierre Tholoniat, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim</div><div style='padding-top: 10px; width: 80ex'>This position paper investigates the integration of Differential Privacy (DP)
in the training of Mixture of Experts (MoE) models within the field of natural
language processing. As Large Language Models (LLMs) scale to billions of
parameters, leveraging expansive datasets, they exhibit enhanced linguistic
capabilities and emergent abilities. However, this growth raises significant
computational and privacy concerns. Our study addresses these issues by
exploring the potential of MoE models, known for their computational
efficiency, and the application of DP, a standard for privacy preservation. We
present the first known attempt to train MoE models under the constraints of
DP, addressing the unique challenges posed by their architecture and the
complexities of DP integration. Our initial experimental studies demonstrate
that MoE models can be effectively trained with DP, achieving performance that
is competitive with their non-private counterparts. This initial study aims to
provide valuable insights and ignite further research in the domain of
privacy-preserving MoE models, softly laying the groundwork for prospective
developments in this evolving field.</div><div><a href='http://arxiv.org/abs/2402.07334v1'>2402.07334v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04451v1")'>Membership Inference Attacks and Privacy in Topic Modeling</div>
<div id='2403.04451v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T12:43:42Z</div><div>Authors: Nico Manzonelli, Wanrong Zhang, Salil Vadhan</div><div style='padding-top: 10px; width: 80ex'>Recent research shows that large language models are susceptible to privacy
attacks that infer aspects of the training data. However, it is unclear if
simpler generative models, like topic models, share similar vulnerabilities. In
this work, we propose an attack against topic models that can confidently
identify members of the training data in Latent Dirichlet Allocation. Our
results suggest that the privacy risks associated with generative modeling are
not restricted to large neural models. Additionally, to mitigate these
vulnerabilities, we explore differentially private (DP) topic modeling. We
propose a framework for private topic modeling that incorporates DP vocabulary
selection as a pre-processing step, and show that it improves privacy while
having limited effects on practical utility.</div><div><a href='http://arxiv.org/abs/2403.04451v1'>2403.04451v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00932v1")'>Differentially Private Knowledge Distillation via Synthetic Text
  Generation</div>
<div id='2403.00932v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T19:22:24Z</div><div>Authors: James Flemings, Murali Annavaram</div><div style='padding-top: 10px; width: 80ex'>Large Language models (LLMs) are achieving state-of-the-art performance in
many different downstream tasks. However, the increasing urgency of data
privacy requires LLMs to train with Differential Privacy (DP) on private data.
Concurrently it is also necessary to compress LLMs for real-life deployments on
resource-constrained devices or latency-sensitive applications. Differential
privacy and model compression generally must trade off utility loss to achieve
their objectives. Moreover, concurrently achieving both can result in even more
utility loss. To this end, we propose a novel differentially private knowledge
distillation algorithm that exploits synthetic data generated by a
differentially private LLM. The knowledge of a teacher model is transferred
onto the student in two ways: one way from the synthetic data itself, the hard
labels, and the other way by the output distribution of the teacher model
evaluated on the synthetic data, the soft labels. Furthermore, if the teacher
and student share a similar architectural structure, we can further distill
knowledge by exploiting hidden representations. Our results show that our
framework substantially improves the utility over existing baselines with
strong privacy parameters, {\epsilon} = 2, validating that we can successfully
compress autoregressive LLMs while preserving the privacy of training data.</div><div><a href='http://arxiv.org/abs/2403.00932v1'>2403.00932v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01071v1")'>Chameleon: Foundation Models for Fairness-aware Multi-modal Data
  Augmentation to Enhance Coverage of Minorities</div>
<div id='2402.01071v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T00:16:45Z</div><div>Authors: Mahdi Erfanian, H. V. Jagadish, Abolfazl Asudeh</div><div style='padding-top: 10px; width: 80ex'>The potential harms of the under-representation of minorities in training
data, particularly in multi-modal settings, is a well-recognized concern. While
there has been extensive effort in detecting such under-representation,
resolution has remained a challenge. With recent advancements in generative AI,
large language models and foundation models have emerged as versatile tools
across various domains. In this paper, we propose Chameleon, a system that
efficiently utilizes these tools to augment a data set with a minimal addition
of synthetically generated tuples, in order to enhance the coverage of the
under-represented groups. Our system follows a rejection sampling approach to
ensure the generated tuples have a high quality and follow the underlying
distribution. In order to minimize the rejection chance of the generated
tuples, we propose multiple strategies for providing a guide for the foundation
model. Our experiment results, in addition to confirming the efficiency of our
proposed algorithms, illustrate the effectiveness of our approach, as the
unfairness of the model in a downstream task significantly dropped after data
repair using Chameleon.</div><div><a href='http://arxiv.org/abs/2402.01071v1'>2402.01071v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02694v1")'>Privacy-Aware Semantic Cache for Large Language Models</div>
<div id='2403.02694v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:23:50Z</div><div>Authors: Waris Gill, Mohamed Elidrisi, Pallavi Kalapatapu, Ali Anwar, Muhammad Ali Gulzar</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2
have revolutionized natural language processing and search engine dynamics.
However, these models incur exceptionally high computational costs. For
instance, GPT-3 consists of 175 billion parameters and inference on these
models also demands billions of floating-point operations. Caching is a natural
solution to reduce LLM inference costs on repeated queries. However, existing
caching methods are incapable of finding semantic similarities among LLM
queries, leading to unacceptable false hit-and-miss rates.
  This paper introduces MeanCache, a semantic cache for LLMs that identifies
semantically similar queries to determine cache hit or miss. Using MeanCache,
the response to a user's semantically similar query can be retrieved from a
local cache rather than re-querying the LLM, thus reducing costs, service
provider load, and environmental impact. MeanCache leverages Federated Learning
(FL) to collaboratively train a query similarity model in a distributed manner
across numerous users without violating privacy. By placing a local cache in
each user's device and using FL, MeanCache reduces the latency and costs and
enhances model performance, resulting in lower cache false hit rates. Our
experiments, benchmarked against the GPTCache, reveal that MeanCache attains an
approximately 17% higher F-score and a 20% increase in precision during
semantic cache hit-and-miss decisions. Furthermore, MeanCache reduces the
storage requirement by 83% and accelerates semantic cache hit-and-miss
decisions by 11%, while still surpassing GPTCache.</div><div><a href='http://arxiv.org/abs/2403.02694v1'>2403.02694v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00873v1")'>Blockchain-empowered Federated Learning: Benefits, Challenges, and
  Solutions</div>
<div id='2403.00873v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T07:41:05Z</div><div>Authors: Zeju Cai, Jianguo Chen, Yuting Fan, Zibin Zheng, Keqin Li</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL) is a distributed machine learning approach that
protects user data privacy by training models locally on clients and
aggregating them on a parameter server. While effective at preserving privacy,
FL systems face limitations such as single points of failure, lack of
incentives, and inadequate security. To address these challenges, blockchain
technology is integrated into FL systems to provide stronger security,
fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems
introduce additional demands on network, computing, and storage resources. This
survey provides a comprehensive review of recent research on BC-FL systems,
analyzing the benefits and challenges associated with blockchain integration.
We explore why blockchain is applicable to FL, how it can be implemented, and
the challenges and existing solutions for its integration. Additionally, we
offer insights on future research directions for the BC-FL system.</div><div><a href='http://arxiv.org/abs/2403.00873v1'>2403.00873v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15917v1")'>Blockchain-enabled Trustworthy Federated Unlearning</div>
<div id='2401.15917v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T07:04:48Z</div><div>Authors: Yijing Lin, Zhipeng Gao, Hongyang Du, Jinke Ren, Zhiqiang Xie, Dusit Niyato</div><div style='padding-top: 10px; width: 80ex'>Federated unlearning is a promising paradigm for protecting the data
ownership of distributed clients. It allows central servers to remove
historical data effects within the machine learning model as well as address
the "right to be forgotten" issue in federated learning. However, existing
works require central servers to retain the historical model parameters from
distributed clients, such that allows the central server to utilize these
parameters for further training even, after the clients exit the training
process. To address this issue, this paper proposes a new blockchain-enabled
trustworthy federated unlearning framework. We first design a proof of
federated unlearning protocol, which utilizes the Chameleon hash function to
verify data removal and eliminate the data contributions stored in other
clients' models. Then, an adaptive contribution-based retraining mechanism is
developed to reduce the computational overhead and significantly improve the
training efficiency. Extensive experiments demonstrate that the proposed
framework can achieve a better data removal effect than the state-of-the-art
frameworks, marking a significant stride towards trustworthy federated
unlearning.</div><div><a href='http://arxiv.org/abs/2401.15917v1'>2401.15917v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15006v1")'>opp/ai: Optimistic Privacy-Preserving AI on Blockchain</div>
<div id='2402.15006v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T22:54:41Z</div><div>Authors: Cathie So, KD Conway, Xiaohang Yu, Suning Yao, Kartin Wong</div><div style='padding-top: 10px; width: 80ex'>The convergence of Artificial Intelligence (AI) and blockchain technology is
reshaping the digital world, offering decentralized, secure, and efficient AI
services on blockchain platforms. Despite the promise, the high computational
demands of AI on blockchain raise significant privacy and efficiency concerns.
The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a
pioneering solution to these issues, striking a balance between privacy
protection and computational efficiency. The framework integrates
Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine
Learning (opML) for efficiency, creating a hybrid model tailored for blockchain
AI services. This study presents the opp/ai framework, delves into the privacy
features of zkML, and assesses the framework's performance and adaptability
across different scenarios.</div><div><a href='http://arxiv.org/abs/2402.15006v1'>2402.15006v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06319v1")'>Fake or Compromised? Making Sense of Malicious Clients in Federated
  Learning</div>
<div id='2403.06319v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T21:37:21Z</div><div>Authors: Hamid Mozaffari, Sunav Choudhary, Amir Houmansadr</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL) is a distributed machine learning paradigm that
enables training models on decentralized data. The field of FL security against
poisoning attacks is plagued with confusion due to the proliferation of
research that makes different assumptions about the capabilities of adversaries
and the adversary models they operate under. Our work aims to clarify this
confusion by presenting a comprehensive analysis of the various poisoning
attacks and defensive aggregation rules (AGRs) proposed in the literature, and
connecting them under a common framework. To connect existing adversary models,
we present a hybrid adversary model, which lies in the middle of the spectrum
of adversaries, where the adversary compromises a few clients, trains a
generative (e.g., DDPM) model with their compromised samples, and generates new
synthetic data to solve an optimization for a stronger (e.g., cheaper, more
practical) attack against different robust aggregation rules. By presenting the
spectrum of FL adversaries, we aim to provide practitioners and researchers
with a clear understanding of the different types of threats they need to
consider when designing FL systems, and identify areas where further research
is needed.</div><div><a href='http://arxiv.org/abs/2403.06319v1'>2403.06319v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02846v1")'>FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive
  Models</div>
<div id='2403.02846v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T10:36:27Z</div><div>Authors: Younghan Lee, Yungi Cho, Woorim Han, Ho Bae, Yunheung Paek</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) thrives in training a global model with numerous
clients by only sharing the parameters of their local models trained with their
private training datasets. Therefore, without revealing the private dataset,
the clients can obtain a deep learning (DL) model with high performance.
However, recent research proposed poisoning attacks that cause a catastrophic
loss in the accuracy of the global model when adversaries, posed as benign
clients, are present in a group of clients. Therefore, recent studies suggested
byzantine-robust FL methods that allow the server to train an accurate global
model even with the adversaries present in the system. However, many existing
methods require the knowledge of the number of malicious clients or the
auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when
the private dataset was non-independently and identically distributed
(non-IID). In this work, we propose FLGuard, a novel byzantine-robust FL method
that detects malicious clients and discards malicious local updates by
utilizing the contrastive learning technique, which showed a tremendous
improvement as a self-supervised learning method. With contrastive models, we
design FLGuard as an ensemble scheme to maximize the defensive capability. We
evaluate FLGuard extensively under various poisoning attacks and compare the
accuracy of the global model with existing byzantine-robust FL methods. FLGuard
outperforms the state-of-the-art defense methods in most cases and shows
drastic improvement, especially in non-IID settings.
https://github.com/201younghanlee/FLGuard</div><div><a href='http://arxiv.org/abs/2403.02846v1'>2403.02846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08984v1")'>A GAN-based data poisoning framework against anomaly detection in
  vertical federated learning</div>
<div id='2401.08984v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T05:31:08Z</div><div>Authors: Xiaolin Chen, Daoguang Zan, Wei Li, Bei Guan, Yongji Wang</div><div style='padding-top: 10px; width: 80ex'>In vertical federated learning (VFL), commercial entities collaboratively
train a model while preserving data privacy. However, a malicious participant's
poisoning attack may degrade the performance of this collaborative model. The
main challenge in achieving the poisoning attack is the absence of access to
the server-side top model, leaving the malicious participant without a clear
target model. To address this challenge, we introduce an innovative end-to-end
poisoning framework P-GAN. Specifically, the malicious participant initially
employs semi-supervised learning to train a surrogate target model.
Subsequently, this participant employs a GAN-based method to produce
adversarial perturbations to degrade the surrogate target model's performance.
Finally, the generator is obtained and tailored for VFL poisoning. Besides, we
develop an anomaly detection algorithm based on a deep auto-encoder (DAE),
offering a robust defense mechanism to VFL scenarios. Through extensive
experiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the
factors that influence their performance.</div><div><a href='http://arxiv.org/abs/2401.08984v1'>2401.08984v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02983v1")'>Federated Learning Under Attack: Exposing Vulnerabilities through Data
  Poisoning Attacks in Computer Networks</div>
<div id='2403.02983v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T14:03:15Z</div><div>Authors: Ehsan Nowroozi, Imran Haider, Rahim Taheri, Mauro Conti</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) is a machine learning (ML) approach that enables
multiple decentralized devices or edge servers to collaboratively train a
shared model without exchanging raw data. During the training and sharing of
model updates between clients and servers, data and models are susceptible to
different data-poisoning attacks.
  In this study, our motivation is to explore the severity of data poisoning
attacks in the computer network domain because they are easy to implement but
difficult to detect. We considered two types of data-poisoning attacks, label
flipping (LF) and feature poisoning (FP), and applied them with a novel
approach. In LF, we randomly flipped the labels of benign data and trained the
model on the manipulated data. For FP, we randomly manipulated the highly
contributing features determined using the Random Forest algorithm. The
datasets used in this experiment were CIC and UNSW related to computer
networks. We generated adversarial samples using the two attacks mentioned
above, which were applied to a small percentage of datasets. Subsequently, we
trained and tested the accuracy of the model on adversarial datasets. We
recorded the results for both benign and manipulated datasets and observed
significant differences between the accuracy of the models on different
datasets. From the experimental results, it is evident that the LF attack
failed, whereas the FP attack showed effective results, which proved its
significance in fooling a server. With a 1% LF attack on the CIC, the accuracy
was approximately 0.0428 and the ASR was 0.9564; hence, the attack is easily
detectable, while with a 1% FP attack, the accuracy and ASR were both
approximately 0.9600, hence, FP attacks are difficult to detect. We repeated
the experiment with different poisoning percentages.</div><div><a href='http://arxiv.org/abs/2403.02983v1'>2403.02983v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07151v1")'>Don't Forget What I did?: Assessing Client Contributions in Federated
  Learning</div>
<div id='2403.07151v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:39:32Z</div><div>Authors: Bishwamittra Ghosh, Debabrota Basu, Fu Huazhu, Wang Yuan, Renuga Kanagavelu, Jiang Jin Peng, Liu Yong, Goh Siow Mong Rick, Wei Qingsong</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) is a collaborative machine learning (ML) approach,
where multiple clients participate in training an ML model without exposing the
private data. Fair and accurate assessment of client contributions is an
important problem in FL to facilitate incentive allocation and encouraging
diverse clients to participate in a unified model training. Existing methods
for assessing client contribution adopts co-operative game-theoretic concepts,
such as Shapley values, but under simplified assumptions. In this paper, we
propose a history-aware game-theoretic framework, called FLContrib, to assess
client contributions when a subset of (potentially non-i.i.d.) clients
participate in each epoch of FL training. By exploiting the FL training process
and linearity of Shapley value, we develop FLContrib that yields a historical
timeline of client contributions as FL training progresses over epochs.
Additionally, to assess client contribution under limited computational budget,
we propose a scheduling procedure that considers a two-sided fairness criteria
to perform expensive Shapley value computation only in a subset of training
epochs. In experiments, we demonstrate a controlled trade-off between the
correctness and efficiency of client contributions assessed via FLContrib. To
demonstrate the benefits of history-aware client contributions, we apply
FLContrib to detect dishonest clients conducting data poisoning in FL training.</div><div><a href='http://arxiv.org/abs/2403.07151v1'>2403.07151v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03149v1")'>Robust Federated Learning Mitigates Client-side Training Data
  Distribution Inference Attacks</div>
<div id='2403.03149v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T17:41:35Z</div><div>Authors: Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong</div><div style='padding-top: 10px; width: 80ex'>Recent studies have revealed that federated learning (FL), once considered
secure due to clients not sharing their private data with the server, is
vulnerable to attacks such as client-side training data distribution inference,
where a malicious client can recreate the victim's data. While various
countermeasures exist, they are not practical, often assuming server access to
some training data or knowledge of label distribution before the attack.
  In this work, we bridge the gap by proposing InferGuard, a novel
Byzantine-robust aggregation rule aimed at defending against client-side
training data distribution inference attacks. In our proposed InferGuard, the
server first calculates the coordinate-wise median of all the model updates it
receives. A client's model update is considered malicious if it significantly
deviates from the computed median update. We conduct a thorough evaluation of
our proposed InferGuard on five benchmark datasets and perform a comparison
with ten baseline methods. The results of our experiments indicate that our
defense mechanism is highly effective in protecting against client-side
training data distribution inference attacks, even against strong adaptive
attacks. Furthermore, our method substantially outperforms the baseline methods
in various practical FL scenarios.</div><div><a href='http://arxiv.org/abs/2403.03149v1'>2403.03149v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00809v1")'>A review on different techniques used to combat the non-IID and
  heterogeneous nature of data in FL</div>
<div id='2401.00809v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T16:34:00Z</div><div>Authors: Venkataraman Natarajan Iyer</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) is a machine-learning approach enabling collaborative
model training across multiple decentralized edge devices that hold local data
samples, all without exchanging these samples. This collaborative process
occurs under the supervision of a central server orchestrating the training or
via a peer-to-peer network. The significance of FL is particularly pronounced
in industries such as healthcare and finance, where data privacy holds
paramount importance. However, training a model under the Federated learning
setting brings forth several challenges, with one of the most prominent being
the heterogeneity of data distribution among the edge devices. The data is
typically non-independently and non-identically distributed (non-IID), thereby
presenting challenges to model convergence. This report delves into the issues
arising from non-IID and heterogeneous data and explores current algorithms
designed to address these challenges.</div><div><a href='http://arxiv.org/abs/2401.00809v1'>2401.00809v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19105v1")'>CollaFuse: Navigating Limited Resources and Privacy in Collaborative
  Generative AI</div>
<div id='2402.19105v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T12:36:10Z</div><div>Authors: Domenique Zipperling, Simeon Allmendinger, Lukas Struppek, Niklas Kühl</div><div style='padding-top: 10px; width: 80ex'>In the landscape of generative artificial intelligence, diffusion-based
models present challenges for socio-technical systems in data requirements and
privacy. Traditional approaches like federated learning distribute the learning
process but strain individual clients, especially with constrained resources
(e.g., edge devices). In response to these challenges, we introduce CollaFuse,
a novel framework inspired by split learning. Tailored for efficient and
collaborative use of denoising diffusion probabilistic models, CollaFuse
enables shared server training and inference, alleviating client computational
burdens. This is achieved by retaining data and computationally inexpensive GPU
processes locally at each client while outsourcing the computationally
expensive processes to the shared server. Demonstrated in a healthcare context,
CollaFuse enhances privacy by highly reducing the need for sensitive
information sharing. These capabilities hold the potential to impact various
application areas, such as the design of edge computing solutions, healthcare
research, or autonomous driving. In essence, our work advances distributed
machine learning, shaping the future of collaborative GenAI networks.</div><div><a href='http://arxiv.org/abs/2402.19105v1'>2402.19105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01923v2")'>IoT in the Era of Generative AI: Vision and Challenges</div>
<div id='2401.01923v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T18:08:57Z</div><div>Authors: Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul Alam, Mi Zhang, Bhaskar Krishnamachari</div><div style='padding-top: 10px; width: 80ex'>Equipped with sensing, networking, and computing capabilities, Internet of
Things (IoT) such as smartphones, wearables, smart speakers, and household
robots have been seamlessly weaved into our daily lives. Recent advancements in
Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold
immense promise to push IoT to the next level. In this article, we share our
vision and views on the benefits that Generative AI brings to IoT, and discuss
some of the most important applications of Generative AI in IoT-related
domains. Fully harnessing Generative AI in IoT is a complex challenge. We
identify some of the most critical challenges including high resource demands
of the Generative AI models, prompt engineering, on-device inference,
offloading, on-device fine-tuning, federated learning, security, as well as
development tools and benchmarks, and discuss current gaps as well as promising
opportunities on enabling Generative AI for IoT. We hope this article can
inspire new research on IoT in the era of Generative AI.</div><div><a href='http://arxiv.org/abs/2401.01923v2'>2401.01923v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02523v1")'>Image-based Deep Learning for Smart Digital Twins: a Review</div>
<div id='2401.02523v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T20:17:25Z</div><div>Authors: Md Ruman Islam, Mahadevan Subramaniam, Pei-Chi Huang</div><div style='padding-top: 10px; width: 80ex'>Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.</div><div><a href='http://arxiv.org/abs/2401.02523v1'>2401.02523v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00238v1")'>CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano
  Things and Digital Twins</div>
<div id='2402.00238v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T23:40:44Z</div><div>Authors: Mohammad, Jamshidi, Dinh Thai Hoang, Diep N. Nguyen</div><div style='padding-top: 10px; width: 80ex'>Digital twins (DTs) are revolutionizing the biotechnology industry by
enabling sophisticated digital representations of biological assets,
microorganisms, drug development processes, and digital health applications.
However, digital twinning at micro and nano scales, particularly in modeling
complex entities like bacteria, presents significant challenges in terms of
requiring advanced Internet of Things (IoT) infrastructure and computing
approaches to achieve enhanced accuracy and scalability. In this work, we
propose a novel framework that integrates the Internet of Bio-Nano Things
(IoBNT) with advanced machine learning techniques, specifically convolutional
neural networks (CNN) and federated learning (FL), to effectively tackle the
identified challenges. Within our framework, IoBNT devices are deployed to
gather image-based biological data across various physical environments,
leveraging the strong capabilities of CNNs for robust machine vision and
pattern recognition. Subsequently, FL is utilized to aggregate insights from
these disparate data sources, creating a refined global model that continually
enhances accuracy and predictive reliability, which is crucial for the
effective deployment of DTs in biotechnology. The primary contribution is the
development of a novel framework that synergistically combines CNN and FL,
augmented by the capabilities of the IoBNT. This novel approach is specifically
tailored to enhancing DTs in the biotechnology industry. The results showcase
enhancements in the reliability and safety of microorganism DTs, while
preserving their accuracy. Furthermore, the proposed framework excels in energy
efficiency and security, offering a user-friendly and adaptable solution. This
broadens its applicability across diverse sectors, including biotechnology and
pharmaceutical industries, as well as clinical and hospital settings.</div><div><a href='http://arxiv.org/abs/2402.00238v1'>2402.00238v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02275v2")'>SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing
  Applications using a Generative Approach</div>
<div id='2402.02275v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T22:08:11Z</div><div>Authors: Tianshi Wang, Jinyang Li, Ruijie Wang, Denizhan Kara, Shengzhong Liu, Davis Wertheimer, Antoni Viros-i-Martin, Raghu Ganti, Mudhakar Srivatsa, Tarek Abdelzaher</div><div style='padding-top: 10px; width: 80ex'>This paper introduces SudokuSens, a generative framework for automated
generation of training data in machine-learning-based Internet-of-Things (IoT)
applications, such that the generated synthetic data mimic experimental
configurations not encountered during actual sensor data collection. The
framework improves the robustness of resulting deep learning models, and is
intended for IoT applications where data collection is expensive. The work is
motivated by the fact that IoT time-series data entangle the signatures of
observed objects with the confounding intrinsic properties of the surrounding
environment and the dynamic environmental disturbances experienced. To
incorporate sufficient diversity into the IoT training data, one therefore
needs to consider a combinatorial explosion of training cases that are
multiplicative in the number of objects considered and the possible
environmental conditions in which such objects may be encountered. Our
framework substantially reduces these multiplicative training needs. To
decouple object signatures from environmental conditions, we employ a
Conditional Variational Autoencoder (CVAE) that allows us to reduce data
collection needs from multiplicative to (nearly) linear, while synthetically
generating (data for) the missing conditions. To obtain robustness with respect
to dynamic disturbances, a session-aware temporal contrastive learning approach
is taken. Integrating the aforementioned two approaches, SudokuSens
significantly improves the robustness of deep learning for IoT applications. We
explore the degree to which SudokuSens benefits downstream inference tasks in
different data sets and discuss conditions under which the approach is
particularly effective.</div><div><a href='http://arxiv.org/abs/2402.02275v2'>2402.02275v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.11018v1")'>Communication Efficient and Provable Federated Unlearning</div>
<div id='2401.11018v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T20:35:02Z</div><div>Authors: Youming Tao, Cheng-Long Wang, Miao Pan, Dongxiao Yu, Xiuzhen Cheng, Di Wang</div><div style='padding-top: 10px; width: 80ex'>We study federated unlearning, a novel problem to eliminate the impact of
specific clients or data points on the global model learned via federated
learning (FL). This problem is driven by the right to be forgotten and the
privacy challenges in FL. We introduce a new framework for exact federated
unlearning that meets two essential criteria: \textit{communication efficiency}
and \textit{exact unlearning provability}. To our knowledge, this is the first
work to tackle both aspects coherently. We start by giving a rigorous
definition of \textit{exact} federated unlearning, which guarantees that the
unlearned model is statistically indistinguishable from the one trained without
the deleted data. We then pinpoint the key property that enables fast exact
federated unlearning: total variation (TV) stability, which measures the
sensitivity of the model parameters to slight changes in the dataset.
Leveraging this insight, we develop a TV-stable FL algorithm called
\texttt{FATS}, which modifies the classical
\texttt{\underline{F}ed\underline{A}vg} algorithm for \underline{T}V
\underline{S}tability and employs local SGD with periodic averaging to lower
the communication round. We also design efficient unlearning algorithms for
\texttt{FATS} under two settings: client-level and sample-level unlearning. We
provide theoretical guarantees for our learning and unlearning algorithms,
proving that they achieve exact federated unlearning with reasonable
convergence rates for both the original and unlearned models. We empirically
validate our framework on 6 benchmark datasets, and show its superiority over
state-of-the-art methods in terms of accuracy, communication cost, computation
cost, and unlearning efficacy.</div><div><a href='http://arxiv.org/abs/2401.11018v1'>2401.11018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12265v1")'>On the Byzantine-Resilience of Distillation-Based Federated Learning</div>
<div id='2402.12265v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T16:26:40Z</div><div>Authors: Christophe Roux, Max Zimmer, Sebastian Pokutta</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) algorithms using Knowledge Distillation (KD) have
received increasing attention due to their favorable properties with respect to
privacy, non-i.i.d. data and communication cost. These methods depart from
transmitting model parameters and, instead, communicate information about a
learning task by sharing predictions on a public dataset. In this work, we
study the performance of such approaches in the byzantine setting, where a
subset of the clients act in an adversarial manner aiming to disrupt the
learning process. We show that KD-based FL algorithms are remarkably resilient
and analyze how byzantine clients can influence the learning process compared
to Federated Averaging. Based on these insights, we introduce two new byzantine
attacks and demonstrate that they are effective against prior
byzantine-resilient methods. Additionally, we propose FilterExp, a novel method
designed to enhance the byzantine resilience of KD-based FL algorithms and
demonstrate its efficacy. Finally, we provide a general method to make attacks
harder to detect, improving their effectiveness.</div><div><a href='http://arxiv.org/abs/2402.12265v1'>2402.12265v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13108v1")'>Analyzing the Impact of Partial Sharing on the Resilience of Online
  Federated Learning Against Model Poisoning Attacks</div>
<div id='2403.13108v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:15:38Z</div><div>Authors: Ehsan Lari, Vinay Chakravarthi Gogineni, Reza Arablouei, Stefan Werner</div><div style='padding-top: 10px; width: 80ex'>We scrutinize the resilience of the partial-sharing online federated learning
(PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the
communication load by enabling clients to exchange only a fraction of their
model estimates with the server at each update round. Partial sharing of model
estimates also enhances the robustness of the algorithm against model-poisoning
attacks. To gain better insights into this phenomenon, we analyze the
performance of the PSO-Fed algorithm in the presence of Byzantine clients,
malicious actors who may subtly tamper with their local models by adding noise
before sharing them with the server. Through our analysis, we demonstrate that
PSO-Fed maintains convergence in both mean and mean-square senses, even under
the strain of model-poisoning attacks. We further derive the theoretical mean
square error (MSE) of PSO-Fed, linking it to various parameters such as
stepsize, attack probability, number of Byzantine clients, client participation
rate, partial-sharing ratio, and noise variance. We also show that there is a
non-trivial optimal stepsize for PSO-Fed when faced with model-poisoning
attacks. The results of our extensive numerical experiments affirm our
theoretical assertions and highlight the superior ability of PSO-Fed to
counteract Byzantine attacks, outperforming other related leading algorithms.</div><div><a href='http://arxiv.org/abs/2403.13108v1'>2403.13108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01268v1")'>Defending Against Data Reconstruction Attacks in Federated Learning: An
  Information Theory Approach</div>
<div id='2403.01268v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T17:12:32Z</div><div>Authors: Qi Tan, Qi Li, Yi Zhao, Zhuotao Liu, Xiaobing Guo, Ke Xu</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) trains a black-box and high-dimensional model among
different clients by exchanging parameters instead of direct data sharing,
which mitigates the privacy leak incurred by machine learning. However, FL
still suffers from membership inference attacks (MIA) or data reconstruction
attacks (DRA). In particular, an attacker can extract the information from
local datasets by constructing DRA, which cannot be effectively throttled by
existing techniques, e.g., Differential Privacy (DP).
  In this paper, we aim to ensure a strong privacy guarantee for FL under DRA.
We prove that reconstruction errors under DRA are constrained by the
information acquired by an attacker, which means that constraining the
transmitted information can effectively throttle DRA. To quantify the
information leakage incurred by FL, we establish a channel model, which depends
on the upper bound of joint mutual information between the local dataset and
multiple transmitted parameters. Moreover, the channel model indicates that the
transmitted information can be constrained through data space operation, which
can improve training efficiency and the model accuracy under constrained
information. According to the channel model, we propose algorithms to constrain
the information transmitted in a single round of local training. With a limited
number of training rounds, the algorithms ensure that the total amount of
transmitted information is limited. Furthermore, our channel model can be
applied to various privacy-enhancing techniques (such as DP) to enhance privacy
guarantees against DRA. Extensive experiments with real-world datasets validate
the effectiveness of our methods.</div><div><a href='http://arxiv.org/abs/2403.01268v1'>2403.01268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13421v2")'>Federated learning with distributed fixed design quantum chips and
  quantum channels</div>
<div id='2401.13421v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T12:32:08Z</div><div>Authors: Ammar Daskin</div><div style='padding-top: 10px; width: 80ex'>The privacy in classical federated learning can be breached through the use
of local gradient results along with engineered queries to the clients.
However, quantum communication channels are considered more secure because a
measurement on the channel causes a loss of information, which can be detected
by the sender. Therefore, the quantum version of federated learning can be used
to provide more privacy. Additionally, sending an $N$ dimensional data vector
through a quantum channel requires sending $\log N$ entangled qubits, which can
potentially provide exponential efficiency if the data vector is utilized as
quantum states.
  In this paper, we propose a quantum federated learning model where fixed
design quantum chips are operated based on the quantum states sent by a
centralized server. Based on the coming superposition states, the clients
compute and then send their local gradients as quantum states to the server,
where they are aggregated to update parameters. Since the server does not send
model parameters, but instead sends the operator as a quantum state, the
clients are not required to share the model. This allows for the creation of
asynchronous learning models. In addition, the model as a quantum state is fed
into client-side chips directly; therefore, it does not require measurements on
the upcoming quantum state to obtain model parameters in order to compute
gradients. This can provide efficiency over the models where the parameter
vector is sent via classical or quantum channels and local gradients are
obtained through the obtained values of these parameters.</div><div><a href='http://arxiv.org/abs/2401.13421v2'>2401.13421v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08216v2")'>Towards Efficient and Certified Recovery from Poisoning Attacks in
  Federated Learning</div>
<div id='2401.08216v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T09:02:34Z</div><div>Authors: Yu Jiang, Jiyuan Shen, Ziyao Liu, Chee Wei Tan, Kwok-Yan Lam</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL) is vulnerable to poisoning attacks, where malicious
clients manipulate their updates to affect the global model. Although various
methods exist for detecting those clients in FL, identifying malicious clients
requires sufficient model updates, and hence by the time malicious clients are
detected, FL models have been already poisoned. Thus, a method is needed to
recover an accurate global model after malicious clients are identified.
Current recovery methods rely on (i) all historical information from
participating FL clients and (ii) the initial model unaffected by the malicious
clients, leading to a high demand for storage and computational resources. In
this paper, we show that highly effective recovery can still be achieved based
on (i) selective historical information rather than all historical information
and (ii) a historical model that has not been significantly affected by
malicious clients rather than the initial model. In this scenario, while
maintaining comparable recovery performance, we can accelerate the recovery
speed and decrease memory consumption. Following this concept, we introduce
Crab, an efficient and certified recovery method, which relies on selective
information storage and adaptive model rollback. Theoretically, we demonstrate
that the difference between the global model recovered by Crab and the one
recovered by train-from-scratch can be bounded under certain assumptions. Our
empirical evaluation, conducted across three datasets over multiple machine
learning models, and a variety of untargeted and targeted poisoning attacks
reveals that Crab is both accurate and efficient, and consistently outperforms
previous approaches in terms of both recovery speed and memory consumption.</div><div><a href='http://arxiv.org/abs/2401.08216v2'>2401.08216v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16934v1")'>FedReview: A Review Mechanism for Rejecting Poisoned Updates in
  Federated Learning</div>
<div id='2402.16934v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T17:53:15Z</div><div>Authors: Tianhang Zheng, Baochun Li</div><div style='padding-top: 10px; width: 80ex'>Federated learning has recently emerged as a decentralized approach to learn
a high-performance model without access to user data. Despite its
effectiveness, federated learning gives malicious users opportunities to
manipulate the model by uploading poisoned model updates to the server. In this
paper, we propose a review mechanism called FedReview to identify and decline
the potential poisoned updates in federated learning. Under our mechanism, the
server randomly assigns a subset of clients as reviewers to evaluate the model
updates on their training datasets in each round. The reviewers rank the model
updates based on the evaluation results and count the number of the updates
with relatively low quality as the estimated number of poisoned updates. Based
on review reports, the server employs a majority voting mechanism to integrate
the rankings and remove the potential poisoned updates in the model aggregation
process. Extensive evaluation on multiple datasets demonstrate that FedReview
can assist the server to learn a well-performed global model in an adversarial
environment.</div><div><a href='http://arxiv.org/abs/2402.16934v1'>2402.16934v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05562v1")'>Brave: Byzantine-Resilient and Privacy-Preserving Peer-to-Peer Federated
  Learning</div>
<div id='2401.05562v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:07:40Z</div><div>Authors: Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Radha Poovendran</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL) enables multiple participants to train a global
machine learning model without sharing their private training data.
Peer-to-peer (P2P) FL advances existing centralized FL paradigms by eliminating
the server that aggregates local models from participants and then updates the
global model. However, P2P FL is vulnerable to (i) honest-but-curious
participants whose objective is to infer private training data of other
participants, and (ii) Byzantine participants who can transmit arbitrarily
manipulated local models to corrupt the learning process. P2P FL schemes that
simultaneously guarantee Byzantine resilience and preserve privacy have been
less studied. In this paper, we develop Brave, a protocol that ensures
Byzantine Resilience And privacy-preserving property for P2P FL in the presence
of both types of adversaries. We show that Brave preserves privacy by
establishing that any honest-but-curious adversary cannot infer other
participants' private data by observing their models. We further prove that
Brave is Byzantine-resilient, which guarantees that all benign participants
converge to an identical model that deviates from a global model trained
without Byzantine adversaries by a bounded distance. We evaluate Brave against
three state-of-the-art adversaries on a P2P FL for image classification tasks
on benchmark datasets CIFAR10 and MNIST. Our results show that the global model
learned with Brave in the presence of adversaries achieves comparable
classification accuracy to a global model trained in the absence of any
adversary.</div><div><a href='http://arxiv.org/abs/2401.05562v1'>2401.05562v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13700v1")'>On the Conflict of Robustness and Learning in Collaborative Machine
  Learning</div>
<div id='2402.13700v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T11:04:23Z</div><div>Authors: Mathilde Raynal, Carmela Troncoso</div><div style='padding-top: 10px; width: 80ex'>Collaborative Machine Learning (CML) allows participants to jointly train a
machine learning model while keeping their training data private. In scenarios
where privacy is a strong requirement, such as health-related applications,
safety is also a primary concern. This means that privacy-preserving CML
processes must produce models that output correct and reliable decisions
\emph{even in the presence of potentially untrusted participants}. In response
to this issue, researchers propose to use \textit{robust aggregators} that rely
on metrics which help filter out malicious contributions that could compromise
the training process. In this work, we formalize the landscape of robust
aggregators in the literature. Our formalization allows us to show that
existing robust aggregators cannot fulfill their goal: either they use
distance-based metrics that cannot accurately identify targeted malicious
updates; or propose methods whose success is in direct conflict with the
ability of CML participants to learn from others and therefore cannot eliminate
the risk of manipulation without preventing learning.</div><div><a href='http://arxiv.org/abs/2402.13700v1'>2402.13700v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04803v1")'>Enhancing Security in Federated Learning through Adaptive
  Consensus-Based Model Update Validation</div>
<div id='2403.04803v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T20:54:56Z</div><div>Authors: Zahir Alsulaimawi</div><div style='padding-top: 10px; width: 80ex'>This paper introduces an advanced approach for fortifying Federated Learning
(FL) systems against label-flipping attacks. We propose a simplified
consensus-based verification process integrated with an adaptive thresholding
mechanism. This dynamic thresholding is designed to adjust based on the
evolving landscape of model updates, offering a refined layer of anomaly
detection that aligns with the real-time needs of distributed learning
environments. Our method necessitates a majority consensus among participating
clients to validate updates, ensuring that only vetted and consensual
modifications are applied to the global model. The efficacy of our approach is
validated through experiments on two benchmark datasets in deep learning,
CIFAR-10 and MNIST. Our results indicate a significant mitigation of
label-flipping attacks, bolstering the FL system's resilience. This method
transcends conventional techniques that depend on anomaly detection or
statistical validation by incorporating a verification layer reminiscent of
blockchain's participatory validation without the associated cryptographic
overhead. The innovation of our approach rests in striking an optimal balance
between heightened security measures and the inherent limitations of FL
systems, such as computational efficiency and data privacy. Implementing a
consensus mechanism specifically tailored for FL environments paves the way for
more secure, robust, and trustworthy distributed machine learning applications,
where safeguarding data integrity and model robustness is critical.</div><div><a href='http://arxiv.org/abs/2403.04803v1'>2403.04803v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12572v1")'>FairProof : Confidential and Certifiable Fairness for Neural Networks</div>
<div id='2402.12572v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T21:53:43Z</div><div>Authors: Chhavi Yadav, Amrita Roy Chowdhury, Dan Boneh, Kamalika Chaudhuri</div><div style='padding-top: 10px; width: 80ex'>Machine learning models are increasingly used in societal applications, yet
legal and privacy concerns demand that they very often be kept confidential.
Consequently, there is a growing distrust about the fairness properties of
these models in the minds of consumers, who are often at the receiving end of
model predictions. To this end, we propose FairProof - a system that uses
Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the
fairness of a model, while maintaining confidentiality. We also propose a
fairness certification algorithm for fully-connected neural networks which is
befitting to ZKPs and is used in this system. We implement FairProof in Gnark
and demonstrate empirically that our system is practically feasible.</div><div><a href='http://arxiv.org/abs/2402.12572v1'>2402.12572v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03629v1")'>Disparate Impact on Group Accuracy of Linearization for Private
  Inference</div>
<div id='2402.03629v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T01:56:29Z</div><div>Authors: Saswat Das, Marco Romanelli, Ferdinando Fioretto</div><div style='padding-top: 10px; width: 80ex'>Ensuring privacy-preserving inference on cryptographically secure data is a
well-known computational challenge. To alleviate the bottleneck of costly
cryptographic computations in non-linear activations, recent methods have
suggested linearizing a targeted portion of these activations in neural
networks. This technique results in significantly reduced runtimes with often
negligible impacts on accuracy. In this paper, we demonstrate that such
computational benefits may lead to increased fairness costs. Specifically, we
find that reducing the number of ReLU activations disproportionately decreases
the accuracy for minority groups compared to majority groups. To explain these
observations, we provide a mathematical interpretation under restricted
assumptions about the nature of the decision boundary, while also showing the
prevalence of this problem across widely used datasets and architectures.
Finally, we show how a simple procedure altering the fine-tuning step for
linearized models can serve as an effective mitigation strategy.</div><div><a href='http://arxiv.org/abs/2402.03629v1'>2402.03629v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01096v1")'>Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance</div>
<div id='2402.01096v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T01:58:58Z</div><div>Authors: Wenqi Wei, Ling Liu</div><div style='padding-top: 10px; width: 80ex'>Emerging Distributed AI systems are revolutionizing big data computing and
data processing capabilities with growing economic and societal impact.
However, recent studies have identified new attack surfaces and risks caused by
security, privacy, and fairness issues in AI systems. In this paper, we review
representative techniques, algorithms, and theoretical foundations for
trustworthy distributed AI through robustness guarantee, privacy protection,
and fairness awareness in distributed learning. We first provide a brief
overview of alternative architectures for distributed learning, discuss
inherent vulnerabilities for security, privacy, and fairness of AI algorithms
in distributed learning, and analyze why these problems are present in
distributed learning regardless of specific architectures. Then we provide a
unique taxonomy of countermeasures for trustworthy distributed AI, covering (1)
robustness to evasion attacks and irregular queries at inference, and
robustness to poisoning attacks, Byzantine attacks, and irregular data
distribution during training; (2) privacy protection during distributed
learning and model inference at deployment; and (3) AI fairness and governance
with respect to both data and models. We conclude with a discussion on open
challenges and future research directions toward trustworthy distributed AI,
such as the need for trustworthy AI policy guidelines, the AI
responsibility-utility co-design, and incentives and compliance.</div><div><a href='http://arxiv.org/abs/2402.01096v1'>2402.01096v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15957v1")'>Scalable Federated Unlearning via Isolated and Coded Sharding</div>
<div id='2401.15957v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:41:45Z</div><div>Authors: Yijing Lin, Zhipeng Gao, Hongyang Du, Dusit Niyato, Gui Gui, Shuguang Cui, Jinke Ren</div><div style='padding-top: 10px; width: 80ex'>Federated unlearning has emerged as a promising paradigm to erase the
client-level data effect without affecting the performance of collaborative
learning models. However, the federated unlearning process often introduces
extensive storage overhead and consumes substantial computational resources,
thus hindering its implementation in practice. To address this issue, this
paper proposes a scalable federated unlearning framework based on isolated
sharding and coded computing. We first divide distributed clients into multiple
isolated shards across stages to reduce the number of clients being affected.
Then, to reduce the storage overhead of the central server, we develop a coded
computing mechanism by compressing the model parameters across different
shards. In addition, we provide the theoretical analysis of time efficiency and
storage effectiveness for the isolated and coded sharding. Finally, extensive
experiments on two typical learning tasks, i.e., classification and generation,
demonstrate that our proposed framework can achieve better performance than
three state-of-the-art frameworks in terms of accuracy, retraining time,
storage overhead, and F1 scores for resisting membership inference attacks.</div><div><a href='http://arxiv.org/abs/2401.15957v1'>2401.15957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16158v1")'>Distribution-Free Fair Federated Learning with Small Samples</div>
<div id='2402.16158v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T17:37:53Z</div><div>Authors: Qichuan Yin, Junzhou Huang, Huaxiu Yao, Linjun Zhang</div><div style='padding-top: 10px; width: 80ex'>As federated learning gains increasing importance in real-world applications
due to its capacity for decentralized data training, addressing fairness
concerns across demographic groups becomes critically important. However, most
existing machine learning algorithms for ensuring fairness are designed for
centralized data environments and generally require large-sample and
distributional assumptions, underscoring the urgent need for fairness
techniques adapted for decentralized and heterogeneous systems with
finite-sample and distribution-free guarantees. To address this issue, this
paper introduces FedFaiREE, a post-processing algorithm developed specifically
for distribution-free fair learning in decentralized settings with small
samples. Our approach accounts for unique challenges in decentralized
environments, such as client heterogeneity, communication costs, and small
sample sizes. We provide rigorous theoretical guarantees for both fairness and
accuracy, and our experimental results further provide robust empirical
validation for our proposed method.</div><div><a href='http://arxiv.org/abs/2402.16158v1'>2402.16158v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10795v1")'>Diversified Ensembling: An Experiment in Crowdsourced Machine Learning</div>
<div id='2402.10795v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:20:43Z</div><div>Authors: Ira Globus-Harris, Declan Harrison, Michael Kearns, Pietro Perona, Aaron Roth</div><div style='padding-top: 10px; width: 80ex'>Crowdsourced machine learning on competition platforms such as Kaggle is a
popular and often effective method for generating accurate models. Typically,
teams vie for the most accurate model, as measured by overall error on a
holdout set, and it is common towards the end of such competitions for teams at
the top of the leaderboard to ensemble or average their models outside the
platform mechanism to get the final, best global model. In arXiv:2201.10408,
the authors developed an alternative crowdsourcing framework in the context of
fair machine learning, in order to integrate community feedback into models
when subgroup unfairness is present and identifiable. There, unlike in
classical crowdsourced ML, participants deliberately specialize their efforts
by working on subproblems, such as demographic subgroups in the service of
fairness. Here, we take a broader perspective on this work: we note that within
this framework, participants may both specialize in the service of fairness and
simply to cater to their particular expertise (e.g., focusing on identifying
bird species in an image classification task). Unlike traditional
crowdsourcing, this allows for the diversification of participants' efforts and
may provide a participation mechanism to a larger range of individuals (e.g. a
machine learning novice who has insight into a specific fairness concern). We
present the first medium-scale experimental evaluation of this framework, with
46 participating teams attempting to generate models to predict income from
American Community Survey data. We provide an empirical analysis of teams'
approaches, and discuss the novel system architecture we developed. From here,
we give concrete guidance for how best to deploy such a framework.</div><div><a href='http://arxiv.org/abs/2402.10795v1'>2402.10795v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13567v1")'>Spot Check Equivalence: an Interpretable Metric for Information
  Elicitation Mechanisms</div>
<div id='2402.13567v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T06:57:07Z</div><div>Authors: Shengwei Xu, Yichi Zhang, Paul Resnick, Grant Schoenebeck</div><div style='padding-top: 10px; width: 80ex'>Because high-quality data is like oxygen for AI systems, effectively
eliciting information from crowdsourcing workers has become a first-order
problem for developing high-performance machine learning algorithms. Two
prevalent paradigms, spot-checking and peer prediction, enable the design of
mechanisms to evaluate and incentivize high-quality data from human labelers.
So far, at least three metrics have been proposed to compare the performances
of these techniques [33, 8, 3]. However, different metrics lead to divergent
and even contradictory results in various contexts. In this paper, we harmonize
these divergent stories, showing that two of these metrics are actually the
same within certain contexts and explain the divergence of the third. Moreover,
we unify these different contexts by introducing \textit{Spot Check
Equivalence}, which offers an interpretable metric for the effectiveness of a
peer prediction mechanism. Finally, we present two approaches to compute spot
check equivalence in various contexts, where simulation results verify the
effectiveness of our proposed metric.</div><div><a href='http://arxiv.org/abs/2402.13567v1'>2402.13567v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18129v1")'>On the Inductive Biases of Demographic Parity-based Fair Learning
  Algorithms</div>
<div id='2402.18129v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:39:58Z</div><div>Authors: Haoyu Lei, Amin Gohari, Farzan Farnia</div><div style='padding-top: 10px; width: 80ex'>Fair supervised learning algorithms assigning labels with little dependence
on a sensitive attribute have attracted great attention in the machine learning
community. While the demographic parity (DP) notion has been frequently used to
measure a model's fairness in training fair classifiers, several studies in the
literature suggest potential impacts of enforcing DP in fair learning
algorithms. In this work, we analytically study the effect of standard DP-based
regularization methods on the conditional distribution of the predicted label
given the sensitive attribute. Our analysis shows that an imbalanced training
dataset with a non-uniform distribution of the sensitive attribute could lead
to a classification rule biased toward the sensitive attribute outcome holding
the majority of training data. To control such inductive biases in DP-based
fair learning, we propose a sensitive attribute-based distributionally robust
optimization (SA-DRO) method improving robustness against the marginal
distribution of the sensitive attribute. Finally, we present several numerical
results on the application of DP-based learning methods to standard centralized
and distributed learning problems. The empirical findings support our
theoretical results on the inductive biases in DP-based fair learning
algorithms and the debiasing effects of the proposed SA-DRO method.</div><div><a href='http://arxiv.org/abs/2402.18129v1'>2402.18129v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06812v1")'>Monotone Individual Fairness</div>
<div id='2403.06812v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:32:56Z</div><div>Authors: Yahav Bechavod</div><div style='padding-top: 10px; width: 80ex'>We revisit the problem of online learning with individual fairness, where an
online learner strives to maximize predictive accuracy while ensuring that
similar individuals are treated similarly. We first extend the frameworks of
Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human
auditors regarding fairness violations, as we consider auditing schemes that
are capable of aggregating feedback from any number of auditors, using a rich
class we term monotone aggregation functions. We then prove a characterization
for such auditing schemes, practically reducing the analysis of auditing for
individual fairness by multiple auditors to that of auditing by
(instance-specific) single auditors. Using our generalized framework, we
present an oracle-efficient algorithm achieving an upper bound frontier of
$(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret,
number of fairness violations, for $0\leq b \leq 1/4$. We then study an online
classification setting where label feedback is available for
positively-predicted individuals only, and present an oracle-efficient
algorithm achieving an upper bound frontier of
$(\mathcal{O}(T^{2/3+2b}),\mathcal{O}(T^{5/6-b}))$ for regret, number of
fairness violations, for $0\leq b \leq 1/6$. In both settings, our algorithms
improve on the best known bounds for oracle-efficient algorithms. Furthermore,
our algorithms offer significant improvements in computational efficiency,
greatly reducing the number of required calls to an (offline) optimization
oracle per round, to $\tilde{\mathcal{O}}(\alpha^{-2})$ in the full information
setting, and $\tilde{\mathcal{O}}(\alpha^{-2} + k^2T^{1/3})$ in the partial
information setting, where $\alpha$ is the sensitivity for reporting fairness
violations, and $k$ is the number of individuals in a round.</div><div><a href='http://arxiv.org/abs/2403.06812v1'>2403.06812v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10632v2")'>Interventional Fairness on Partially Known Causal Graphs: A Constrained
  Optimization Approach</div>
<div id='2401.10632v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T11:20:31Z</div><div>Authors: Aoqi Zuo, Yiqing Li, Susan Wei, Mingming Gong</div><div style='padding-top: 10px; width: 80ex'>Fair machine learning aims to prevent discrimination against individuals or
sub-populations based on sensitive attributes such as gender and race. In
recent years, causal inference methods have been increasingly used in fair
machine learning to measure unfairness by causal effects. However, current
methods assume that the true causal graph is given, which is often not true in
real-world applications. To address this limitation, this paper proposes a
framework for achieving causal fairness based on the notion of interventions
when the true causal graph is partially known. The proposed approach involves
modeling fair prediction using a Partially Directed Acyclic Graph (PDAG),
specifically, a class of causal DAGs that can be learned from observational
data combined with domain knowledge. The PDAG is used to measure causal
fairness, and a constrained optimization problem is formulated to balance
between fairness and accuracy. Results on both simulated and real-world
datasets demonstrate the effectiveness of this method.</div><div><a href='http://arxiv.org/abs/2401.10632v2'>2401.10632v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00625v1")'>Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness
  and Efficiency</div>
<div id='2403.00625v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T16:01:28Z</div><div>Authors: Yixuan Zhang, Feng Zhou</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning pre-trained models is a widely employed technique in numerous
real-world applications. However, fine-tuning these models on new tasks can
lead to unfair outcomes. This is due to the absence of generalization
guarantees for fairness properties, regardless of whether the original
pre-trained model was developed with fairness considerations. To tackle this
issue, we introduce an efficient and robust fine-tuning framework specifically
designed to mitigate biases in new tasks. Our empirical analysis shows that the
parameters in the pre-trained model that affect predictions for different
demographic groups are different, so based on this observation, we employ a
transfer learning strategy that neutralizes the importance of these influential
weights, determined using Fisher information across demographic groups.
Additionally, we integrate this weight importance neutralization strategy with
a matrix factorization technique, which provides a low-rank approximation of
the weight matrix using fewer parameters, reducing the computational demands.
Experiments on multiple pre-trained models and new tasks demonstrate the
effectiveness of our method.</div><div><a href='http://arxiv.org/abs/2403.00625v1'>2403.00625v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15638v1")'>Fair Resource Allocation in Multi-Task Learning</div>
<div id='2402.15638v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T22:46:14Z</div><div>Authors: Hao Ban, Kaiyi Ji</div><div style='padding-top: 10px; width: 80ex'>By jointly learning multiple tasks, multi-task learning (MTL) can leverage
the shared knowledge across tasks, resulting in improved data efficiency and
generalization performance. However, a major challenge in MTL lies in the
presence of conflicting gradients, which can hinder the fair optimization of
some tasks and subsequently impede MTL's ability to achieve better overall
performance. Inspired by fair resource allocation in communication networks, we
formulate the optimization of MTL as a utility maximization problem, where the
loss decreases across tasks are maximized under different fairness
measurements. To solve this problem, we propose FairGrad, a novel MTL
optimization method. FairGrad not only enables flexible emphasis on certain
tasks but also achieves a theoretical convergence guarantee. Extensive
experiments demonstrate that our method can achieve state-of-the-art
performance among gradient manipulation methods on a suite of multi-task
benchmarks in supervised learning and reinforcement learning. Furthermore, we
incorporate the idea of $\alpha$-fairness into loss functions of various MTL
methods. Extensive empirical studies demonstrate that their performance can be
significantly enhanced. Code is provided at
\url{https://github.com/OptMN-Lab/fairgrad}.</div><div><a href='http://arxiv.org/abs/2402.15638v1'>2402.15638v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07780v1")'>FairRR: Pre-Processing for Group Fairness through Randomized Response</div>
<div id='2403.07780v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:08:47Z</div><div>Authors: Xianli Zeng, Joshua Ward, Guang Cheng</div><div style='padding-top: 10px; width: 80ex'>The increasing usage of machine learning models in consequential
decision-making processes has spurred research into the fairness of these
systems. While significant work has been done to study group fairness in the
in-processing and post-processing setting, there has been little that
theoretically connects these results to the pre-processing domain. This paper
proposes that achieving group fairness in downstream models can be formulated
as finding the optimal design matrix in which to modify a response variable in
a Randomized Response framework. We show that measures of group fairness can be
directly controlled for with optimal model utility, proposing a pre-processing
algorithm called FairRR that yields excellent downstream model utility and
fairness.</div><div><a href='http://arxiv.org/abs/2403.07780v1'>2403.07780v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12237v1")'>Learning to Defer in Content Moderation: The Human-AI Interplay</div>
<div id='2402.12237v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:47:47Z</div><div>Authors: Thodoris Lykouris, Wentao Weng</div><div style='padding-top: 10px; width: 80ex'>Successful content moderation in online platforms relies on a human-AI
collaboration approach. A typical heuristic estimates the expected harmfulness
of a post and uses fixed thresholds to decide whether to remove it and whether
to send it for human review. This disregards the prediction uncertainty, the
time-varying element of human review capacity and post arrivals, and the
selective sampling in the dataset (humans only review posts filtered by the
admission algorithm).
  In this paper, we introduce a model to capture the human-AI interplay in
content moderation. The algorithm observes contextual information for incoming
posts, makes classification and admission decisions, and schedules posts for
human review. Only admitted posts receive human reviews on their harmfulness.
These reviews help educate the machine-learning algorithms but are delayed due
to congestion in the human review system. The classical learning-theoretic way
to capture this human-AI interplay is via the framework of learning to defer,
where the algorithm has the option to defer a classification task to humans for
a fixed cost and immediately receive feedback. Our model contributes to this
literature by introducing congestion in the human review system. Moreover,
unlike work on online learning with delayed feedback where the delay in the
feedback is exogenous to the algorithm's decisions, the delay in our model is
endogenous to both the admission and the scheduling decisions.
  We propose a near-optimal learning algorithm that carefully balances the
classification loss from a selectively sampled dataset, the idiosyncratic loss
of non-reviewed posts, and the delay loss of having congestion in the human
review system. To the best of our knowledge, this is the first result for
online learning in contextual queueing systems and hence our analytical
framework may be of independent interest.</div><div><a href='http://arxiv.org/abs/2402.12237v1'>2402.12237v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03540v1")'>Regulation Games for Trustworthy Machine Learning</div>
<div id='2402.03540v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T21:54:28Z</div><div>Authors: Mohammad Yaghini, Patty Liu, Franziska Boenisch, Nicolas Papernot</div><div style='padding-top: 10px; width: 80ex'>Existing work on trustworthy machine learning (ML) often concentrates on
individual aspects of trust, such as fairness or privacy. Additionally, many
techniques overlook the distinction between those who train ML models and those
responsible for assessing their trustworthiness. To address these issues, we
propose a framework that views trustworthy ML as a multi-objective multi-agent
optimization problem. This naturally lends itself to a game-theoretic
formulation we call regulation games. We illustrate a particular game instance,
the SpecGame in which we model the relationship between an ML model builder and
fairness and privacy regulators. Regulators wish to design penalties that
enforce compliance with their specification, but do not want to discourage
builders from participation. Seeking such socially optimal (i.e., efficient for
all agents) solutions to the game, we introduce ParetoPlay. This novel
equilibrium search algorithm ensures that agents remain on the Pareto frontier
of their objectives and avoids the inefficiencies of other equilibria.
Simulating SpecGame through ParetoPlay can provide policy guidance for ML
Regulation. For instance, we show that for a gender classification application,
regulators can enforce a differential privacy budget that is on average 4.0
lower if they take the initiative to specify their desired guarantee first.</div><div><a href='http://arxiv.org/abs/2402.03540v1'>2402.03540v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15270v2")'>SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models</div>
<div id='2401.15270v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T02:36:30Z</div><div>Authors: Zhihao Wang, Yiqun Xie, Zhili Li, Xiaowei Jia, Zhe Jiang, Aolin Jia, Shuo Xu</div><div style='padding-top: 10px; width: 80ex'>Fairness-awareness has emerged as an essential building block for the
responsible use of artificial intelligence in real applications. In many cases,
inequity in performance is due to the change in distribution over different
regions. While techniques have been developed to improve the transferability of
fairness, a solution to the problem is not always feasible with no samples from
the new regions, which is a bottleneck for pure data-driven attempts.
Fortunately, physics-based mechanistic models have been studied for many
problems with major social impacts. We propose SimFair, a physics-guided
fairness-aware learning framework, which bridges the data limitation by
integrating physical-rule-based simulation and inverse modeling into the
training design. Using temperature prediction as an example, we demonstrate the
effectiveness of the proposed SimFair in fairness preservation.</div><div><a href='http://arxiv.org/abs/2401.15270v2'>2401.15270v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07697v1")'>Data vs. Model Machine Learning Fairness Testing: An Empirical Study</div>
<div id='2401.07697v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T14:14:16Z</div><div>Authors: Arumoy Shome, Luis Cruz, Arie van Deursen</div><div style='padding-top: 10px; width: 80ex'>Although several fairness definitions and bias mitigation techniques exist in
the literature, all existing solutions evaluate fairness of Machine Learning
(ML) systems after the training stage. In this paper, we take the first steps
towards evaluating a more holistic approach by testing for fairness both before
and after model training. We evaluate the effectiveness of the proposed
approach and position it within the ML development lifecycle, using an
empirical analysis of the relationship between model dependent and independent
fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5
real-world datasets and 1600 fairness evaluation cycles. We find a linear
relationship between data and model fairness metrics when the distribution and
the size of the training data changes. Our results indicate that testing for
fairness prior to training can be a ``cheap'' and effective means of catching a
biased data collection process early; detecting data drifts in production
systems and minimising execution of full training cycles thus reducing
development time and costs.</div><div><a href='http://arxiv.org/abs/2401.07697v1'>2401.07697v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09328v1")'>Connecting Algorithmic Fairness to Quality Dimensions in Machine
  Learning in Official Statistics and Survey Production</div>
<div id='2402.09328v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T17:18:03Z</div><div>Authors: Patrick Oliver Schenk, Christoph Kern</div><div style='padding-top: 10px; width: 80ex'>National Statistical Organizations (NSOs) increasingly draw on Machine
Learning (ML) to improve the timeliness and cost-effectiveness of their
products. When introducing ML solutions, NSOs must ensure that high standards
with respect to robustness, reproducibility, and accuracy are upheld as
codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA;
Yung et al. 2022). At the same time, a growing body of research focuses on
fairness as a pre-condition of a safe deployment of ML to prevent disparate
social impacts in practice. However, fairness has not yet been explicitly
discussed as a quality aspect in the context of the application of ML at NSOs.
We employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of
its quality dimensions to algorithmic fairness. We thereby extend the QF4SA
framework in several ways: we argue for fairness as its own quality dimension,
we investigate the interaction of fairness with other dimensions, and we
explicitly address data, both on its own and its interaction with applied
methodology. In parallel with empirical illustrations, we show how our mapping
can contribute to methodology in the domains of official statistics,
algorithmic fairness, and trustworthy machine learning.</div><div><a href='http://arxiv.org/abs/2402.09328v1'>2402.09328v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05240v2")'>Decoupling Decision-Making in Fraud Prevention through Classifier
  Calibration for Business Logic Action</div>
<div id='2401.05240v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T16:13:21Z</div><div>Authors: Emanuele Luzio, Moacir Antonelli Ponti, Christian Ramirez Arevalo, Luis Argerich</div><div style='padding-top: 10px; width: 80ex'>Machine learning models typically focus on specific targets like creating
classifiers, often based on known population feature distributions in a
business context. However, models calculating individual features adapt over
time to improve precision, introducing the concept of decoupling: shifting from
point evaluation to data distribution. We use calibration strategies as
strategy for decoupling machine learning (ML) classifiers from score-based
actions within business logic frameworks. To evaluate these strategies, we
perform a comparative analysis using a real-world business scenario and
multiple ML models. Our findings highlight the trade-offs and performance
implications of the approach, offering valuable insights for practitioners
seeking to optimize their decoupling efforts. In particular, the Isotonic and
Beta calibration methods stand out for scenarios in which there is shift
between training and testing data.</div><div><a href='http://arxiv.org/abs/2401.05240v2'>2401.05240v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06906v2")'>Cost-Sensitive Learning to Defer to Multiple Experts with Workload
  Constraints</div>
<div id='2403.06906v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:57:20Z</div><div>Authors: Jean V. Alves, Diogo Leitão, Sérgio Jesus, Marco O. P. Sampaio, Javier Liébana, Pedro Saleiro, Mário A. T. Figueiredo, Pedro Bizarro</div><div style='padding-top: 10px; width: 80ex'>Learning to defer (L2D) aims to improve human-AI collaboration systems by
learning how to defer decisions to humans when they are more likely to be
correct than an ML classifier. Existing research in L2D overlooks key aspects
of real-world systems that impede its practical adoption, namely: i) neglecting
cost-sensitive scenarios, where type 1 and type 2 errors have different costs;
ii) requiring concurrent human predictions for every instance of the training
dataset and iii) not dealing with human work capacity constraints. To address
these issues, we propose the deferral under cost and capacity constraints
framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised
learning to model the probability of human error under less restrictive data
requirements (only one expert prediction per instance) and using constraint
programming to globally minimize the error cost subject to workload
limitations. We test DeCCaF in a series of cost-sensitive fraud detection
scenarios with different teams of 9 synthetic fraud analysts, with individual
work capacity constraints. The results demonstrate that our approach performs
significantly better than the baselines in a wide array of scenarios, achieving
an average 8.4% reduction in the misclassification cost.</div><div><a href='http://arxiv.org/abs/2403.06906v2'>2403.06906v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14389v1")'>Securing Transactions: A Hybrid Dependable Ensemble Machine Learning
  Model using IHT-LR and Grid Search</div>
<div id='2402.14389v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:01:42Z</div><div>Authors: Md. Alamin Talukder, Rakib Hossen, Md Ashraf Uddin, Mohammed Nasir Uddin, Uzzal Kumar Acharjee</div><div style='padding-top: 10px; width: 80ex'>Financial institutions and businesses face an ongoing challenge from
fraudulent transactions, prompting the need for effective detection methods.
Detecting credit card fraud is crucial for identifying and preventing
unauthorized transactions.Timely detection of fraud enables investigators to
take swift actions to mitigate further losses. However, the investigation
process is often time-consuming, limiting the number of alerts that can be
thoroughly examined each day. Therefore, the primary objective of a fraud
detection model is to provide accurate alerts while minimizing false alarms and
missed fraud cases. In this paper, we introduce a state-of-the-art hybrid
ensemble (ENS) dependable Machine learning (ML) model that intelligently
combines multiple algorithms with proper weighted optimization using Grid
search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor
(KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To
address the data imbalance issue, we employ the Instant Hardness Threshold
(IHT) technique in conjunction with Logistic Regression (LR), surpassing
conventional approaches. Our experiments are conducted on a publicly available
credit card dataset comprising 284,807 transactions. The proposed model
achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a
perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid
ensemble model outperforms existing works, establishing a new benchmark for
detecting fraudulent transactions in high-frequency scenarios. The results
highlight the effectiveness and reliability of our approach, demonstrating
superior performance metrics and showcasing its exceptional potential for
real-world fraud detection applications.</div><div><a href='http://arxiv.org/abs/2402.14389v1'>2402.14389v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03530v1")'>Detecting Anomalies in Blockchain Transactions using Machine Learning
  Classifiers and Explainability Analysis</div>
<div id='2401.03530v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T16:01:51Z</div><div>Authors: Mohammad Hasan, Mohammad Shahriar Rahman, Helge Janicke, Iqbal H. Sarker</div><div style='padding-top: 10px; width: 80ex'>As the use of Blockchain for digital payments continues to rise in
popularity, it also becomes susceptible to various malicious attacks.
Successfully detecting anomalies within Blockchain transactions is essential
for bolstering trust in digital payments. However, the task of anomaly
detection in Blockchain transaction data is challenging due to the infrequent
occurrence of illicit transactions. Although several studies have been
conducted in the field, a limitation persists: the lack of explanations for the
model's predictions. This study seeks to overcome this limitation by
integrating eXplainable Artificial Intelligence (XAI) techniques and anomaly
rules into tree-based ensemble classifiers for detecting anomalous Bitcoin
transactions. The Shapley Additive exPlanation (SHAP) method is employed to
measure the contribution of each feature, and it is compatible with ensemble
models. Moreover, we present rules for interpreting whether a Bitcoin
transaction is anomalous or not. Additionally, we have introduced an
under-sampling algorithm named XGBCLUS, designed to balance anomalous and
non-anomalous transaction data. This algorithm is compared against other
commonly used under-sampling and over-sampling techniques. Finally, the
outcomes of various tree-based single classifiers are compared with those of
stacking and voting ensemble classifiers. Our experimental results demonstrate
that: (i) XGBCLUS enhances TPR and ROC-AUC scores compared to state-of-the-art
under-sampling and over-sampling techniques, and (ii) our proposed ensemble
classifiers outperform traditional single tree-based machine learning
classifiers in terms of accuracy, TPR, and FPR scores.</div><div><a href='http://arxiv.org/abs/2401.03530v1'>2401.03530v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02995v1")'>Mitigating Label Flipping Attacks in Malicious URL Detectors Using
  Ensemble Trees</div>
<div id='2403.02995v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T14:21:57Z</div><div>Authors: Ehsan Nowroozi, Nada Jadalla, Samaneh Ghelichkhani, Alireza Jolfaei</div><div style='padding-top: 10px; width: 80ex'>Malicious URLs provide adversarial opportunities across various industries,
including transportation, healthcare, energy, and banking which could be
detrimental to business operations. Consequently, the detection of these URLs
is of crucial importance; however, current Machine Learning (ML) models are
susceptible to backdoor attacks. These attacks involve manipulating a small
percentage of training data labels, such as Label Flipping (LF), which changes
benign labels to malicious ones and vice versa. This manipulation results in
misclassification and leads to incorrect model behavior. Therefore, integrating
defense mechanisms into the architecture of ML models becomes an imperative
consideration to fortify against potential attacks.
  The focus of this study is on backdoor attacks in the context of URL
detection using ensemble trees. By illuminating the motivations behind such
attacks, highlighting the roles of attackers, and emphasizing the critical
importance of effective defense strategies, this paper contributes to the
ongoing efforts to fortify ML models against adversarial threats within the ML
domain in network security. We propose an innovative alarm system that detects
the presence of poisoned labels and a defense mechanism designed to uncover the
original class labels with the aim of mitigating backdoor attacks on ensemble
tree classifiers. We conducted a case study using the Alexa and Phishing Site
URL datasets and showed that LF attacks can be addressed using our proposed
defense mechanism. Our experimental results prove that the LF attack achieved
an Attack Success Rate (ASR) between 50-65% within 2-5%, and the innovative
defense method successfully detected poisoned labels with an accuracy of up to
100%.</div><div><a href='http://arxiv.org/abs/2403.02995v1'>2403.02995v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16912v1")'>An Adversarial Robustness Benchmark for Enterprise Network Intrusion
  Detection</div>
<div id='2402.16912v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T16:45:39Z</div><div>Authors: João Vitorino, Miguel Silva, Eva Maia, Isabel Praça</div><div style='padding-top: 10px; width: 80ex'>As cyber-attacks become more sophisticated, improving the robustness of
Machine Learning (ML) models must be a priority for enterprises of all sizes.
To reliably compare the robustness of different ML models for cyber-attack
detection in enterprise computer networks, they must be evaluated in
standardized conditions. This work presents a methodical adversarial robustness
benchmark of multiple decision tree ensembles with constrained adversarial
examples generated from standard datasets. The robustness of regularly and
adversarially trained RF, XGB, LGBM, and EBM models was evaluated on the
original CICIDS2017 dataset, a corrected version of it designated as NewCICIDS,
and the HIKARI dataset, which contains more recent network traffic. NewCICIDS
led to models with a better performance, especially XGB and EBM, but RF and
LGBM were less robust against the more recent cyber-attacks of HIKARI. Overall,
the robustness of the models to adversarial cyber-attack examples was improved
without their generalization to regular traffic being affected, enabling a
reliable detection of suspicious activity without costly increases of false
alarms.</div><div><a href='http://arxiv.org/abs/2402.16912v1'>2402.16912v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10974v1")'>On the Cross-Dataset Generalization of Machine Learning for Network
  Intrusion Detection</div>
<div id='2402.10974v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:39:58Z</div><div>Authors: Marco Cantone, Claudio Marrocco, Alessandro Bria</div><div style='padding-top: 10px; width: 80ex'>Network Intrusion Detection Systems (NIDS) are a fundamental tool in
cybersecurity. Their ability to generalize across diverse networks is a
critical factor in their effectiveness and a prerequisite for real-world
applications. In this study, we conduct a comprehensive analysis on the
generalization of machine-learning-based NIDS through an extensive
experimentation in a cross-dataset framework. We employ four machine learning
classifiers and utilize four datasets acquired from different networks:
CIC-IDS-2017, CSE-CIC-IDS2018, LycoS-IDS2017, and LycoS-Unicas-IDS2018.
Notably, the last dataset is a novel contribution, where we apply corrections
based on LycoS-IDS2017 to the well-known CSE-CIC-IDS2018 dataset. The results
show nearly perfect classification performance when the models are trained and
tested on the same dataset. However, when training and testing the models in a
cross-dataset fashion, the classification accuracy is largely commensurate with
random chance except for a few combinations of attacks and datasets. We employ
data visualization techniques in order to provide valuable insights on the
patterns in the data. Our analysis unveils the presence of anomalies in the
data that directly hinder the classifiers capability to generalize the learned
knowledge to new scenarios. This study enhances our comprehension of the
generalization capabilities of machine-learning-based NIDS, highlighting the
significance of acknowledging data heterogeneity.</div><div><a href='http://arxiv.org/abs/2402.10974v1'>2402.10974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11180v1")'>usfAD Based Effective Unknown Attack Detection Focused IDS Framework</div>
<div id='2403.11180v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T11:49:57Z</div><div>Authors: Md. Ashraf Uddin, Sunil Aryal, Mohamed Reda Bouadjenek, Muna Al-Hawawreh, Md. Alamin Talukder</div><div style='padding-top: 10px; width: 80ex'>The rapid expansion of varied network systems, including the Internet of
Things (IoT) and Industrial Internet of Things (IIoT), has led to an increasing
range of cyber threats. Ensuring robust protection against these threats
necessitates the implementation of an effective Intrusion Detection System
(IDS). For more than a decade, researchers have delved into supervised machine
learning techniques to develop IDS to classify normal and attack traffic.
However, building effective IDS models using supervised learning requires a
substantial number of benign and attack samples. To collect a sufficient number
of attack samples from real-life scenarios is not possible since cyber attacks
occur occasionally. Further, IDS trained and tested on known datasets fails in
detecting zero-day or unknown attacks due to the swift evolution of attack
patterns. To address this challenge, we put forth two strategies for
semi-supervised learning based IDS where training samples of attacks are not
required: 1) training a supervised machine learning model using randomly and
uniformly dispersed synthetic attack samples; 2) building a One Class
Classification (OCC) model that is trained exclusively on benign network
traffic. We have implemented both approaches and compared their performances
using 10 recent benchmark IDS datasets. Our findings demonstrate that the OCC
model based on the state-of-art anomaly detection technique called usfAD
significantly outperforms conventional supervised classification and other OCC
based techniques when trained and tested considering real-life scenarios,
particularly to detect previously unseen attacks.</div><div><a href='http://arxiv.org/abs/2403.11180v1'>2403.11180v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13010v1")'>A Dual-Tier Adaptive One-Class Classification IDS for Emerging
  Cyberthreats</div>
<div id='2403.13010v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T12:26:30Z</div><div>Authors: Md. Ashraf Uddin, Sunil Aryal, Mohamed Reda Bouadjenek, Muna Al-Hawawreh, Md. Alamin Talukder</div><div style='padding-top: 10px; width: 80ex'>In today's digital age, our dependence on IoT (Internet of Things) and IIoT
(Industrial IoT) systems has grown immensely, which facilitates sensitive
activities such as banking transactions and personal, enterprise data, and
legal document exchanges. Cyberattackers consistently exploit weak security
measures and tools. The Network Intrusion Detection System (IDS) acts as a
primary tool against such cyber threats. However, machine learning-based IDSs,
when trained on specific attack patterns, often misclassify new emerging
cyberattacks. Further, the limited availability of attack instances for
training a supervised learner and the ever-evolving nature of cyber threats
further complicate the matter. This emphasizes the need for an adaptable IDS
framework capable of recognizing and learning from unfamiliar/unseen attacks
over time. In this research, we propose a one-class classification-driven IDS
system structured on two tiers. The first tier distinguishes between normal
activities and attacks/threats, while the second tier determines if the
detected attack is known or unknown. Within this second tier, we also embed a
multi-classification mechanism coupled with a clustering algorithm. This model
not only identifies unseen attacks but also uses them for retraining them by
clustering unseen attacks. This enables our model to be future-proofed, capable
of evolving with emerging threat patterns. Leveraging one-class classifiers
(OCC) at the first level, our approach bypasses the need for attack samples,
addressing data imbalance and zero-day attack concerns and OCC at the second
level can effectively separate unknown attacks from the known attacks. Our
methodology and evaluations indicate that the presented framework exhibits
promising potential for real-world deployments.</div><div><a href='http://arxiv.org/abs/2403.13010v1'>2403.13010v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13013v1")'>Hierarchical Classification for Intrusion Detection System: Effective
  Design and Empirical Analysis</div>
<div id='2403.13013v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T17:16:55Z</div><div>Authors: Md. Ashraf Uddin, Sunil Aryal, Mohamed Reda Bouadjenek, Muna Al-Hawawreh, Md. Alamin Talukder</div><div style='padding-top: 10px; width: 80ex'>With the increased use of network technologies like Internet of Things (IoT)
in many real-world applications, new types of cyberattacks have been emerging.
To safeguard critical infrastructures from these emerging threats, it is
crucial to deploy an Intrusion Detection System (IDS) that can detect different
types of attacks accurately while minimizing false alarms. Machine learning
approaches have been used extensively in IDS and they are mainly using flat
multi-class classification to differentiate normal traffic and different types
of attacks. Though cyberattack types exhibit a hierarchical structure where
similar granular attack subtypes can be grouped into more high-level attack
types, hierarchical classification approach has not been explored well. In this
paper, we investigate the effectiveness of hierarchical classification approach
in IDS. We use a three-level hierarchical classification model to classify
various network attacks, where the first level classifies benign or attack, the
second level classifies coarse high-level attack types, and the third level
classifies a granular level attack types. Our empirical results of using 10
different classification algorithms in 10 different datasets show that there is
no significant difference in terms of overall classification performance (i.e.,
detecting normal and different types of attack correctly) of hierarchical and
flat classification approaches. However, flat classification approach
misclassify attacks as normal whereas hierarchical approach misclassify one
type of attack as another attack type. In other words, the hierarchical
classification approach significantly minimises attacks from misclassified as
normal traffic, which is more important in critical systems.</div><div><a href='http://arxiv.org/abs/2403.13013v1'>2403.13013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17045v1")'>An Investigation into the Performances of the State-of-the-art Machine
  Learning Approaches for Various Cyber-attack Detection: A Survey</div>
<div id='2402.17045v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T22:04:25Z</div><div>Authors: Tosin Ige, Christopher Kiekintveld, Aritran Piplai</div><div style='padding-top: 10px; width: 80ex'>To secure computers and information systems from attackers taking advantage
of vulnerabilities in the system to commit cybercrime, several methods have
been proposed for real-time detection of vulnerabilities to improve security
around information systems. Of all the proposed methods, machine learning had
been the most effective method in securing a system with capabilities ranging
from early detection of software vulnerabilities to real-time detection of
ongoing compromise in a system. As there are different types of cyberattacks,
each of the existing state-of-the-art machine learning models depends on
different algorithms for training which also impact their suitability for
detection of a particular type of cyberattack. In this research, we analyzed
each of the current state-of-theart machine learning models for different types
of cyberattack detection from the past 10 years with a major emphasis on the
most recent works for comparative study to identify the knowledge gap where
work is still needed to be done with regard to detection of each category of
cyberattack</div><div><a href='http://arxiv.org/abs/2402.17045v1'>2402.17045v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11206v1")'>CBR -- Boosting Adaptive Classification By Retrieval of Encrypted
  Network Traffic with Out-of-distribution</div>
<div id='2403.11206v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T13:14:09Z</div><div>Authors: Amir Lukach, Ran Dubin, Amit Dvir, Chen Hajaj</div><div style='padding-top: 10px; width: 80ex'>Encrypted network traffic Classification tackles the problem from different
approaches and with different goals. One of the common approaches is using
Machine learning or Deep Learning-based solutions on a fixed number of classes,
leading to misclassification when an unknown class is given as input. One of
the solutions for handling unknown classes is to retrain the model, however,
retraining models every time they become obsolete is both resource and
time-consuming. Therefore, there is a growing need to allow classification
models to detect and adapt to new classes dynamically, without retraining, but
instead able to detect new classes using few shots learning [1]. In this paper,
we introduce Adaptive Classification By Retrieval CBR, a novel approach for
encrypted network traffic classification. Our new approach is based on an
ANN-based method, which allows us to effectively identify new and existing
classes without retraining the model. The novel approach is simple, yet
effective and achieved similar results to RF with up to 5% difference (usually
less than that) in the classification tasks while having a slight decrease in
the case of new samples (from new classes) without retraining. To summarize,
the new method is a real-time classification, which can classify new classes
without retraining. Furthermore, our solution can be used as a complementary
solution alongside RF or any other machine/deep learning classification method,
as an aggregated solution.</div><div><a href='http://arxiv.org/abs/2403.11206v1'>2403.11206v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08063v1")'>Locality Sensitive Hashing for Network Traffic Fingerprinting</div>
<div id='2402.08063v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T21:14:37Z</div><div>Authors: Nowfel Mashnoor, Jay Thom, Abdur Rouf, Shamik Sengupta, Batyr Charyyev</div><div style='padding-top: 10px; width: 80ex'>The advent of the Internet of Things (IoT) has brought forth additional
intricacies and difficulties to computer networks. These gadgets are
particularly susceptible to cyber-attacks because of their simplistic design.
Therefore, it is crucial to recognise these devices inside a network for the
purpose of network administration and to identify any harmful actions. Network
traffic fingerprinting is a crucial technique for identifying devices and
detecting anomalies. Currently, the predominant methods for this depend heavily
on machine learning (ML). Nevertheless, machine learning (ML) methods need the
selection of features, adjustment of hyperparameters, and retraining of models
to attain optimal outcomes and provide resilience to concept drifts detected in
a network. In this research, we suggest using locality-sensitive hashing (LSH)
for network traffic fingerprinting as a solution to these difficulties. Our
study focuses on examining several design options for the Nilsimsa LSH
function. We then use this function to create unique fingerprints for network
data, which may be used to identify devices. We also compared it with ML-based
traffic fingerprinting and observed that our method increases the accuracy of
state-of-the-art by 12% achieving around 94% accuracy in identifying devices in
a network.</div><div><a href='http://arxiv.org/abs/2402.08063v1'>2402.08063v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16843v1")'>Evaluating ML-Based Anomaly Detection Across Datasets of Varied
  Integrity: A Case Study</div>
<div id='2401.16843v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T09:34:15Z</div><div>Authors: Adrian Pekar, Richard Jozsa</div><div style='padding-top: 10px; width: 80ex'>Cybersecurity remains a critical challenge in the digital age, with network
traffic flow anomaly detection being a key pivotal instrument in the fight
against cyber threats. In this study, we address the prevalent issue of data
integrity in network traffic datasets, which are instrumental in developing
machine learning (ML) models for anomaly detection. We introduce two refined
versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed
using NFStream to ensure methodologically sound flow expiration and labeling.
Our research contrasts the performance of the Random Forest (RF) algorithm
across the original CICIDS-2017, its refined counterparts WTMC-2021 and
CRiSIS-2022, and our NFStream-generated datasets, in both binary and
multi-class classification contexts. We observe that the RF model exhibits
exceptional robustness, achieving consistent high-performance metrics
irrespective of the underlying dataset quality, which prompts a critical
discussion on the actual impact of data integrity on ML efficacy. Our study
underscores the importance of continual refinement and methodological rigor in
dataset generation for network security research. As the landscape of network
threats evolves, so must the tools and techniques used to detect and analyze
them.</div><div><a href='http://arxiv.org/abs/2401.16843v1'>2401.16843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08564v1")'>ADVENT: Attack/Anomaly Detection in VANETs</div>
<div id='2401.08564v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:49:08Z</div><div>Authors: Hamideh Baharlouei, Adetokunbo Makanju, Nur Zincir-Heywood</div><div style='padding-top: 10px; width: 80ex'>In the domain of Vehicular Ad hoc Networks (VANETs), where the imperative of
having a real-world malicious detector capable of detecting attacks in
real-time and unveiling their perpetrators is crucial, our study introduces a
system with this goal. This system is designed for real-time detection of
malicious behavior, addressing the critical need to first identify the onset of
attacks and subsequently the responsible actors. Prior work in this area have
never addressed both requirements, which we believe are necessary for real
world deployment, simultaneously. By seamlessly integrating statistical and
machine learning techniques, the proposed system prioritizes simplicity and
efficiency. It excels in swiftly detecting attack onsets with a remarkable
F1-score of 99.66%, subsequently identifying malicious vehicles with an average
F1-score of approximately 97.85%. Incorporating federated learning in both
stages enhances privacy and improves the efficiency of malicious node
detection, effectively reducing the false negative rate.</div><div><a href='http://arxiv.org/abs/2401.08564v1'>2401.08564v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08135v1")'>Machine Learning-Based Malicious Vehicle Detection for Security Threats
  and Attacks in Vehicle Ad-hoc Network (VANET) Communications</div>
<div id='2401.08135v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T06:01:02Z</div><div>Authors: Thanh Nguyen Canh, Xiem HoangVan</div><div style='padding-top: 10px; width: 80ex'>With the rapid growth of Vehicle Ad-hoc Network (VANET) as a promising
technology for efficient and reliable communication among vehicles and
infrastructure, the security and integrity of VANET communications has become a
critical concern. One of the significant threats to VANET is the presence of
blackhole attacks, where malicious nodes disrupt the network's functionality
and compromise data confidentiality, integrity, and availability. In this
paper, we propose a machine learning-based approach for blackhole detection in
VANET. To achieve this task, we first create a comprehensive dataset comprising
normal and malicious traffic flows. Afterward, we study and define a promising
set of features to discriminate the blackhole attacks. Finally, we evaluate
various machine learning algorithms, including Gradient Boosting, Random
Forest, Support Vector Machines, k-Nearest Neighbors, Gaussian Naive Bayes, and
Logistic Regression. Experimental results demonstrate the effectiveness of
these algorithms in distinguishing between normal and malicious nodes. Our
findings also highlight the potential of machine learning based approach in
enhancing the security of VANET by detecting and mitigating blackhole attacks.</div><div><a href='http://arxiv.org/abs/2401.08135v1'>2401.08135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13848v1")'>A V2X-based Privacy Preserving Federated Measuring and Learning System</div>
<div id='2401.13848v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T23:11:11Z</div><div>Authors: Levente Alekszejenkó, Tadeusz Dobrowiecki</div><div style='padding-top: 10px; width: 80ex'>Future autonomous vehicles (AVs) will use a variety of sensors that generate
a vast amount of data. Naturally, this data not only serves self-driving
algorithms; but can also assist other vehicles or the infrastructure in
real-time decision-making. Consequently, vehicles shall exchange their
measurement data over Vehicle-to-Everything (V2X) technologies. Moreover,
predicting the state of the road network might be beneficial too. With such a
prediction, we might mitigate road congestion, balance parking lot usage, or
optimize the traffic flow. That would decrease transportation costs as well as
reduce its environmental impact.
  In this paper, we propose a federated measurement and learning system that
provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V)
communication while also operating a federated learning (FL) scheme over the
Vehicle-to-Network (V2N) link to create a predictive model of the
transportation network. As we are yet to have real-world AV data, we model it
with a non-IID (independent and identically distributed) dataset to evaluate
the capabilities of the proposed system in terms of performance and privacy.
Results indicate that the proposed FL scheme improves learning performance and
prevents eavesdropping at the aggregator server side.</div><div><a href='http://arxiv.org/abs/2401.13848v1'>2401.13848v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06953v1")'>FedDriveScore: Federated Scoring Driving Behavior with a Mixture of
  Metric Distributions</div>
<div id='2401.06953v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T02:15:41Z</div><div>Authors: Lin Lu</div><div style='padding-top: 10px; width: 80ex'>Scoring the driving performance of various drivers on a unified scale, based
on how safe or economical they drive on their daily trips, is essential for the
driver profile task. Connected vehicles provide the opportunity to collect
real-world driving data, which is advantageous for constructing scoring models.
However, the lack of pre-labeled scores impede the use of supervised regression
models and the data privacy issues hinder the way of traditionally
data-centralized learning on the cloud side for model training. To address
them, an unsupervised scoring method is presented without the need for labels
while still preserving fairness and objectiveness compared to subjective
scoring strategies. Subsequently, a federated learning framework based on
vehicle-cloud collaboration is proposed as a privacy-friendly alternative to
centralized learning. This framework includes a consistently federated version
of the scoring method to reduce the performance degradation of the global
scoring model caused by the statistical heterogeneous challenge of local data.
Theoretical and experimental analysis demonstrate that our federated scoring
model is consistent with the utility of the centrally learned counterpart and
is effective in evaluating driving performance.</div><div><a href='http://arxiv.org/abs/2401.06953v1'>2401.06953v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.15285v1")'>Blockchain-based Pseudonym Management for Vehicle Twin Migrations in
  Vehicular Edge Metaverse</div>
<div id='2403.15285v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T15:31:37Z</div><div>Authors: Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou, Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie</div><div style='padding-top: 10px; width: 80ex'>Driven by the great advances in metaverse and edge computing technologies,
vehicular edge metaverses are expected to disrupt the current paradigm of
intelligent transportation systems. As highly computerized avatars of Vehicular
Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can
provide valuable metaverse services to improve driving safety and on-board
satisfaction for their VMUs throughout journeys. To maintain uninterrupted
metaverse experiences, VTs must be migrated among edge servers following the
movements of vehicles. This can raise concerns about privacy breaches during
the dynamic communications among vehicular edge metaverses. To address these
concerns and safeguard location privacy, pseudonyms as temporary identifiers
can be leveraged by both VMUs and VTs to realize anonymous communications in
the physical space and virtual spaces. However, existing pseudonym management
methods fall short in meeting the extensive pseudonym demands in vehicular edge
metaverses, thus dramatically diminishing the performance of privacy
preservation. To this end, we present a cross-metaverse empowered dual
pseudonym management framework. We utilize cross-chain technology to enhance
management efficiency and data security for pseudonyms. Furthermore, we propose
a metric to assess the privacy level and employ a Multi-Agent Deep
Reinforcement Learning (MADRL) approach to obtain an optimal pseudonym
generating strategy. Numerical results demonstrate that our proposed schemes
are high-efficiency and cost-effective, showcasing their promising applications
in vehicular edge metaverses.</div><div><a href='http://arxiv.org/abs/2403.15285v1'>2403.15285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15285v2")'>Ransomware threat mitigation through network traffic analysis and
  machine learning techniques</div>
<div id='2401.15285v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T03:55:28Z</div><div>Authors: Ali Mehrban, Shirin Karimi Geransayeh</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a noticeable increase in cyberattacks using
ransomware. Attackers use this malicious software to break into networks and
harm computer systems. This has caused significant and lasting damage to
various organizations, including government, private companies, and regular
users. These attacks often lead to the loss or exposure of sensitive
information, disruptions in normal operations, and persistent vulnerabilities.
This paper focuses on a method for recognizing and identifying ransomware in
computer networks. The approach relies on using machine learning algorithms and
analyzing the patterns of network traffic. By collecting and studying this
traffic, and then applying machine learning models, we can accurately identify
and detect ransomware. The results of implementing this method show that
machine learning algorithms can effectively pinpoint ransomware based on
network traffic, achieving high levels of precision and accuracy.</div><div><a href='http://arxiv.org/abs/2401.15285v2'>2401.15285v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00780v1")'>Empirical and Experimental Insights into Data Mining Techniques for
  Crime Prediction: A Comprehensive Survey</div>
<div id='2403.00780v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T15:00:45Z</div><div>Authors: Kamal Taha</div><div style='padding-top: 10px; width: 80ex'>This survey paper presents a comprehensive analysis of crime prediction
methodologies, exploring the various techniques and technologies utilized in
this area. The paper covers the statistical methods, machine learning
algorithms, and deep learning techniques employed to analyze crime data, while
also examining their effectiveness and limitations. We propose a methodological
taxonomy that classifies crime prediction algorithms into specific techniques.
This taxonomy is structured into four tiers, including methodology category,
methodology sub-category, methodology techniques, and methodology
sub-techniques. Empirical and experimental evaluations are provided to rank the
different techniques. The empirical evaluation assesses the crime prediction
techniques based on four criteria, while the experimental evaluation ranks the
algorithms that employ the same sub-technique, the different sub-techniques
that employ the same technique, the different techniques that employ the same
methodology sub-category, the different methodology sub-categories within the
same category, and the different methodology categories. The combination of
methodological taxonomy, empirical evaluations, and experimental comparisons
allows for a nuanced and comprehensive understanding of crime prediction
algorithms, aiding researchers in making informed decisions. Finally, the paper
provides a glimpse into the future of crime prediction techniques, highlighting
potential advancements and opportunities for further research in this field</div><div><a href='http://arxiv.org/abs/2403.00780v1'>2403.00780v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03116v1")'>Advancing DDoS Attack Detection: A Synergistic Approach Using Deep
  Residual Neural Networks and Synthetic Oversampling</div>
<div id='2401.03116v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T03:03:52Z</div><div>Authors: Ali Alfatemi, Mohamed Rahouti, Ruhul Amin, Sarah ALJamal, Kaiqi Xiong, Yufeng Xin</div><div style='padding-top: 10px; width: 80ex'>Distributed Denial of Service (DDoS) attacks pose a significant threat to the
stability and reliability of online systems. Effective and early detection of
such attacks is pivotal for safeguarding the integrity of networks. In this
work, we introduce an enhanced approach for DDoS attack detection by leveraging
the capabilities of Deep Residual Neural Networks (ResNets) coupled with
synthetic oversampling techniques. Because of the inherent class imbalance in
many cyber-security datasets, conventional methods often struggle with false
negatives, misclassifying subtle DDoS patterns as benign. By applying the
Synthetic Minority Over-sampling Technique (SMOTE) to the CICIDS dataset, we
balance the representation of benign and malicious data points, enabling the
model to better discern intricate patterns indicative of an attack. Our deep
residual network, tailored for this specific task, further refines the
detection process. Experimental results on a real-world dataset demonstrate
that our approach achieves an accuracy of 99.98%, significantly outperforming
traditional methods. This work underscores the potential of combining advanced
data augmentation techniques with deep learning models to bolster
cyber-security defenses.</div><div><a href='http://arxiv.org/abs/2401.03116v1'>2401.03116v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03488v1")'>Data-Driven Subsampling in the Presence of an Adversarial Actor</div>
<div id='2401.03488v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T14:02:22Z</div><div>Authors: Abu Shafin Mohammad Mahdee Jameel, Ahmed P. Mohamed, Jinho Yi, Aly El Gamal, Akshay Malhotra</div><div style='padding-top: 10px; width: 80ex'>Deep learning based automatic modulation classification (AMC) has received
significant attention owing to its potential applications in both military and
civilian use cases. Recently, data-driven subsampling techniques have been
utilized to overcome the challenges associated with computational complexity
and training time for AMC. Beyond these direct advantages of data-driven
subsampling, these methods also have regularizing properties that may improve
the adversarial robustness of the modulation classifier. In this paper, we
investigate the effects of an adversarial attack on an AMC system that employs
deep learning models both for AMC and for subsampling. Our analysis shows that
subsampling itself is an effective deterrent to adversarial attacks. We also
uncover the most efficient subsampling strategy when an adversarial attack on
both the classifier and the subsampler is anticipated.</div><div><a href='http://arxiv.org/abs/2401.03488v1'>2401.03488v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03646v2")'>Lens: A Foundation Model for Network Traffic in Cybersecurity</div>
<div id='2402.03646v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T02:45:13Z</div><div>Authors: Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Huajie Shao</div><div style='padding-top: 10px; width: 80ex'>Network traffic refers to the amount of data being sent and received over the
internet or any system that connects computers. Analyzing and understanding
network traffic is vital for improving network security and management.
However, the analysis of network traffic is challenging due to the diverse
nature of data packets, which often feature heterogeneous headers and encrypted
payloads lacking semantics. To capture the latent semantics of traffic, a few
studies have adopted pre-training techniques based on the Transformer encoder
or decoder to learn the representations from massive traffic data. However,
these methods typically excel in traffic understanding (classification) or
traffic generation tasks. To address this issue, we develop Lens, a foundation
model for network traffic that leverages the T5 architecture to learn the
pre-trained representations from large-scale unlabeled data. Harnessing the
strength of the encoder-decoder framework, which captures the global
information while preserving the generative ability, our model can better learn
the representations from raw data. To further enhance pre-training
effectiveness, we design a novel loss that combines three distinct tasks:
Masked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous
Traffic Prediction (HTP). Evaluation results across various benchmark datasets
demonstrate that the proposed Lens outperforms the baselines in most downstream
tasks related to both traffic understanding and generation. Notably, it also
requires much less labeled data for fine-tuning compared to current methods.</div><div><a href='http://arxiv.org/abs/2402.03646v2'>2402.03646v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07501v1")'>One Train for Two Tasks: An Encrypted Traffic Classification Framework
  Using Supervised Contrastive Learning</div>
<div id='2402.07501v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T09:10:09Z</div><div>Authors: Haozhen Zhang, Xi Xiao, Le Yu, Qing Li, Zhen Ling, Ye Zhang</div><div style='padding-top: 10px; width: 80ex'>As network security receives widespread attention, encrypted traffic
classification has become the current research focus. However, existing methods
conduct traffic classification without sufficiently considering the common
characteristics between data samples, leading to suboptimal performance.
Moreover, they train the packet-level and flow-level classification tasks
independently, which is redundant because the packet representations learned in
the packet-level task can be exploited by the flow-level task. Therefore, in
this paper, we propose an effective model named a Contrastive Learning Enhanced
Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised
contrastive learning to enhance the packet-level and flow-level representations
and perform graph data augmentation on the byte-level traffic graph so that the
fine-grained semantic-invariant characteristics between bytes can be captured
through contrastive learning. We also propose cross-level multi-task learning,
which simultaneously accomplishes the packet-level and flow-level
classification tasks in the same model with one training. Further experiments
show that CLE-TFE achieves the best overall performance on the two tasks, while
its computational overhead (i.e., floating point operations, FLOPs) is only
about 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at
https://github.com/ViktorAxelsen/CLE-TFE</div><div><a href='http://arxiv.org/abs/2402.07501v1'>2402.07501v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05822v2")'>TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic
  Analysis and Generation</div>
<div id='2403.05822v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T07:19:37Z</div><div>Authors: Jian Qu, Xiaobo Ma, Jianfeng Li</div><div style='padding-top: 10px; width: 80ex'>Over the years, network traffic analysis and generation have advanced
significantly. From traditional statistical methods, the field has progressed
to sophisticated deep learning techniques. This progress has improved the
ability to detect complex patterns and security threats, as well as to test and
optimize network performance. However, obstacles persist, such as the
dependence on labeled data for analysis and the difficulty of generating
traffic samples that follow realistic patterns. Pre-trained deep neural
networks have emerged as powerful tools to resolve these issues, offering
improved performance by learning robust data representations from large
unlabeled datasets. Despite their benefits, existing pre-trained models face
challenges like token length limitation, which restricts their usefulness in
comprehensive traffic analysis and realistic traffic generation. To address
these challenges, we introduce TrafficGPT, a deep learning model that can
tackle complex challenges related to long flow classification and generation
tasks. This model uses generative pre-training with the linear attention
mechanism, which allows for a substantially increased capacity of up to 12,032
tokens from the previous limit of only 512 tokens. TrafficGPT demonstrates
superior performance in classification tasks, reaching state-of-the-art levels.
In generation tasks, it closely resembles real traffic flows, with low JS
divergence and an F1 score close to 0.5 (representing a random guess) in
discriminating generated data. These advancements hold promise for future
applications in both traffic flow classification and generation tasks.</div><div><a href='http://arxiv.org/abs/2403.05822v2'>2403.05822v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18329v1")'>Living-off-The-Land Reverse-Shell Detection by Informed Data
  Augmentation</div>
<div id='2402.18329v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T13:49:23Z</div><div>Authors: Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Fabio Roli</div><div style='padding-top: 10px; width: 80ex'>The living-off-the-land (LOTL) offensive methodologies rely on the
perpetration of malicious actions through chains of commands executed by
legitimate applications, identifiable exclusively by analysis of system logs.
LOTL techniques are well hidden inside the stream of events generated by common
legitimate activities, moreover threat actors often camouflage activity through
obfuscation, making them particularly difficult to detect without incurring in
plenty of false alarms, even using machine learning. To improve the performance
of models in such an harsh environment, we propose an augmentation framework to
enhance and diversify the presence of LOTL malicious activity inside legitimate
logs. Guided by threat intelligence, we generate a dataset by injecting attack
templates known to be employed in the wild, further enriched by malleable
patterns of legitimate activities to replicate the behavior of evasive threat
actors. We conduct an extensive ablation study to understand which models
better handle our augmented dataset, also manipulated to mimic the presence of
model-agnostic evasion and poisoning attacks. Our results suggest that
augmentation is needed to maintain high-predictive capabilities, robustness to
attack is achieved through specific hardening techniques like adversarial
training, and it is possible to deploy near-real-time models with almost-zero
false alarms.</div><div><a href='http://arxiv.org/abs/2402.18329v1'>2402.18329v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01883v1")'>Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports</div>
<div id='2401.01883v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T18:53:22Z</div><div>Authors: Md Rayhanur Rahman, Brandon Wroblewski, Quinn Matthews, Brantley Morgan, Tim Menzies, Laurie Williams</div><div style='padding-top: 10px; width: 80ex'>Defending from cyberattacks requires practitioners to operate on high-level
adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack
incidents describe the chain of malicious actions with respect to time. To
avoid repeating cyberattack incidents, practitioners must proactively identify
and defend against recurring chain of actions - which we refer to as temporal
attack patterns. Automatically mining the patterns among actions provides
structured and actionable information on the adversary behavior of past
cyberattacks. The goal of this paper is to aid security practitioners in
prioritizing and proactive defense against cyberattacks by mining temporal
attack patterns from cyberthreat intelligence reports. To this end, we propose
ChronoCTI, an automated pipeline for mining temporal attack patterns from
cyberthreat intelligence (CTI) reports of past cyberattacks. To construct
ChronoCTI, we build the ground truth dataset of temporal attack patterns and
apply state-of-the-art large language models, natural language processing, and
machine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,
where we identify 124 temporal attack patterns - which we categorize into nine
pattern categories. We identify that the most prevalent pattern category is to
trick victim users into executing malicious code to initiate the attack,
followed by bypassing the anti-malware system in the victim network. Based on
the observed patterns, we advocate organizations to train users about
cybersecurity best practices, introduce immutable operating systems with
limited functionalities, and enforce multi-user authentications. Moreover, we
advocate practitioners to leverage the automated mining capability of ChronoCTI
and design countermeasures against the recurring attack patterns.</div><div><a href='http://arxiv.org/abs/2401.01883v1'>2401.01883v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05641v1")'>When eBPF Meets Machine Learning: On-the-fly OS Kernel
  Compartmentalization</div>
<div id='2401.05641v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T03:30:50Z</div><div>Authors: Zicheng Wang, Tiejin Chen, Qinrun Dai, Yueqi Chen, Hua Wei, Qingkai Zeng</div><div style='padding-top: 10px; width: 80ex'>Compartmentalization effectively prevents initial corruption from turning
into a successful attack. This paper presents O2C, a pioneering system designed
to enforce OS kernel compartmentalization on the fly. It not only provides
immediate remediation for sudden threats but also maintains consistent system
availability through the enforcement process.
  O2C is empowered by the newest advancements of the eBPF ecosystem which
allows to instrument eBPF programs that perform enforcement actions into the
kernel at runtime. O2C takes the lead in embedding a machine learning model
into eBPF programs, addressing unique challenges in on-the-fly
compartmentalization. Our comprehensive evaluation shows that O2C effectively
confines damage within the compartment. Further, we validate that decision tree
is optimally suited for O2C owing to its advantages in processing tabular data,
its explainable nature, and its compliance with the eBPF ecosystem. Last but
not least, O2C is lightweight, showing negligible overhead and excellent
sacalability system-wide.</div><div><a href='http://arxiv.org/abs/2401.05641v1'>2401.05641v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11126v1")'>CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive
  Attackers for Security Applications</div>
<div id='2401.11126v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T05:37:09Z</div><div>Authors: Hangsheng Zhang, Jiqiang Liu, Jinsong Dong</div><div style='padding-top: 10px; width: 80ex'>Ensemble defenses, are widely employed in various security-related
applications to enhance model performance and robustness. The widespread
adoption of these techniques also raises many questions: Are general ensembles
defenses guaranteed to be more robust than individuals? Will stronger adaptive
attacks defeat existing ensemble defense strategies as the cybersecurity arms
race progresses? Can ensemble defenses achieve adversarial robustness to
different types of attacks simultaneously and resist the continually adjusted
adaptive attacks? Unfortunately, these critical questions remain unresolved as
there are no platforms for comprehensive evaluation of ensemble adversarial
attacks and defenses in the cybersecurity domain. In this paper, we propose a
general Cybersecurity Adversarial Robustness Evaluation (CARE) platform aiming
to bridge this gap.</div><div><a href='http://arxiv.org/abs/2401.11126v1'>2401.11126v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.03097v1")'>Adaptive Boosting with Fairness-aware Reweighting Technique for Fair
  Classification</div>
<div id='2401.03097v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T00:00:49Z</div><div>Authors: Xiaobin Song, Zeyuan Liu, Benben Jiang</div><div style='padding-top: 10px; width: 80ex'>Machine learning methods based on AdaBoost have been widely applied to
various classification problems across many mission-critical applications
including healthcare, law and finance. However, there is a growing concern
about the unfairness and discrimination of data-driven classification models,
which is inevitable for classical algorithms including AdaBoost. In order to
achieve fair classification, a novel fair AdaBoost (FAB) approach is proposed
that is an interpretable fairness-improving variant of AdaBoost. We mainly
investigate binary classification problems and focus on the fairness of three
different indicators (i.e., accuracy, false positive rate and false negative
rate). By utilizing a fairness-aware reweighting technique for base
classifiers, the proposed FAB approach can achieve fair classification while
maintaining the advantage of AdaBoost with negligible sacrifice of predictive
performance. In addition, a hyperparameter is introduced in FAB to show
preferences for the fairness-accuracy trade-off. An upper bound for the target
loss function that quantifies error rate and unfairness is theoretically
derived for FAB, which provides a strict theoretical support for the
fairness-improving methods designed for AdaBoost. The effectiveness of the
proposed method is demonstrated on three real-world datasets (i.e., Adult,
COMPAS and HSLS) with respect to the three fairness indicators. The results are
accordant with theoretic analyses, and show that (i) FAB significantly improves
classification fairness at a small cost of accuracy compared with AdaBoost; and
(ii) FAB outperforms state-of-the-art fair classification methods including
equalized odds method, exponentiated gradient method, and disparate
mistreatment method in terms of the fairness-accuracy trade-off.</div><div><a href='http://arxiv.org/abs/2401.03097v1'>2401.03097v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14483v1")'>Utilizing the LightGBM Algorithm for Operator User Credit Assessment
  Research</div>
<div id='2403.14483v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T15:29:24Z</div><div>Authors: Shaojie Li, Xinqi Dong, Danqing Ma, Bo Dang, Hengyi Zang, Yulu Gong</div><div style='padding-top: 10px; width: 80ex'>Mobile Internet user credit assessment is an important way for communication
operators to establish decisions and formulate measures, and it is also a
guarantee for operators to obtain expected benefits. However, credit evaluation
methods have long been monopolized by financial industries such as banks and
credit. As supporters and providers of platform network technology and network
resources, communication operators are also builders and maintainers of
communication networks. Internet data improves the user's credit evaluation
strategy. This paper uses the massive data provided by communication operators
to carry out research on the operator's user credit evaluation model based on
the fusion LightGBM algorithm. First, for the massive data related to user
evaluation provided by operators, key features are extracted by data
preprocessing and feature engineering methods, and a multi-dimensional feature
set with statistical significance is constructed; then, linear regression,
decision tree, LightGBM, and other machine learning algorithms build multiple
basic models to find the best basic model; finally, integrates Averaging,
Voting, Blending, Stacking and other integrated algorithms to refine multiple
fusion models, and finally establish the most suitable fusion model for
operator user evaluation.</div><div><a href='http://arxiv.org/abs/2403.14483v1'>2403.14483v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17106v1")'>Dataset Fairness: Achievable Fairness on Your Data With Utility
  Guarantees</div>
<div id='2402.17106v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T00:59:32Z</div><div>Authors: Muhammad Faaiz Taufiq, Jean-Francois Ton, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>In machine learning fairness, training models which minimize disparity across
different sensitive groups often leads to diminished accuracy, a phenomenon
known as the fairness-accuracy trade-off. The severity of this trade-off
fundamentally depends on dataset characteristics such as dataset imbalances or
biases. Therefore using a uniform fairness requirement across datasets remains
questionable and can often lead to models with substantially low utility. To
address this, we present a computationally efficient approach to approximate
the fairness-accuracy trade-off curve tailored to individual datasets, backed
by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO)
framework, our approach mitigates the computational burden of having to train
multiple models when approximating the trade-off curve. Moreover, we quantify
the uncertainty in our approximation by introducing confidence intervals around
this curve, offering a statistically grounded perspective on the acceptable
range of fairness violations for any given accuracy threshold. Our empirical
evaluation spanning tabular, image and language datasets underscores that our
approach provides practitioners with a principled framework for
dataset-specific fairness decisions across various data modalities.</div><div><a href='http://arxiv.org/abs/2402.17106v1'>2402.17106v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.11081v1")'>Learning from Aggregate responses: Instance Level versus Bag Level Loss
  Functions</div>
<div id='2401.11081v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T02:14:11Z</div><div>Authors: Adel Javanmard, Lin Chen, Vahab Mirrokni, Ashwinkumar Badanidiyuru, Gang Fu</div><div style='padding-top: 10px; width: 80ex'>Due to the rise of privacy concerns, in many practical applications the
training data is aggregated before being shared with the learner, in order to
protect privacy of users' sensitive responses. In an aggregate learning
framework, the dataset is grouped into bags of samples, where each bag is
available only with an aggregate response, providing a summary of individuals'
responses in that bag. In this paper, we study two natural loss functions for
learning from aggregate responses: bag-level loss and the instance-level loss.
In the former, the model is learnt by minimizing a loss between aggregate
responses and aggregate model predictions, while in the latter the model aims
to fit individual predictions to the aggregate responses. In this work, we show
that the instance-level loss can be perceived as a regularized form of the
bag-level loss. This observation lets us compare the two approaches with
respect to bias and variance of the resulting estimators, and introduce a novel
interpolating estimator which combines the two approaches. For linear
regression tasks, we provide a precise characterization of the risk of the
interpolating estimator in an asymptotic regime where the size of the training
set grows in proportion to the features dimension. Our analysis allows us to
theoretically understand the effect of different factors, such as bag size on
the model prediction risk. In addition, we propose a mechanism for
differentially private learning from aggregate responses and derive the optimal
bag size in terms of prediction risk-privacy trade-off. We also carry out
thorough experiments to corroborate our theory and show the efficacy of the
interpolating estimator.</div><div><a href='http://arxiv.org/abs/2401.11081v1'>2401.11081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09560v1")'>Distribution-Free Rates in Neyman-Pearson Classification</div>
<div id='2402.09560v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T20:21:43Z</div><div>Authors: Mohammadreza M. Kalan, Samory Kpotufe</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of Neyman-Pearson classification which models
unbalanced classification settings where error w.r.t. a distribution $\mu_1$ is
to be minimized subject to low error w.r.t. a different distribution $\mu_0$.
Given a fixed VC class $\mathcal{H}$ of classifiers to be minimized over, we
provide a full characterization of possible distribution-free rates, i.e.,
minimax rates over the space of all pairs $(\mu_0, \mu_1)$. The rates involve a
dichotomy between hard and easy classes $\mathcal{H}$ as characterized by a
simple geometric condition, a three-points-separation condition, loosely
related to VC dimension.</div><div><a href='http://arxiv.org/abs/2402.09560v1'>2402.09560v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08831v1")'>Majority-of-Three: The Simplest Optimal Learner?</div>
<div id='2403.08831v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T18:01:30Z</div><div>Authors: Ishaq Aden-Ali, Mikael Møller Høgsgaard, Kasper Green Larsen, Nikita Zhivotovskiy</div><div style='padding-top: 10px; width: 80ex'>Developing an optimal PAC learning algorithm in the realizable setting, where
empirical risk minimization (ERM) is suboptimal, was a major open problem in
learning theory for decades. The problem was finally resolved by Hanneke a few
years ago. Unfortunately, Hanneke's algorithm is quite complex as it returns
the majority vote of many ERM classifiers that are trained on carefully
selected subsets of the data. It is thus a natural goal to determine the
simplest algorithm that is optimal. In this work we study the arguably simplest
algorithm that could be optimal: returning the majority vote of three ERM
classifiers. We show that this algorithm achieves the optimal in-expectation
bound on its error which is provably unattainable by a single ERM classifier.
Furthermore, we prove a near-optimal high-probability bound on this algorithm's
error. We conjecture that a better analysis will prove that this algorithm is
in fact optimal in the high-probability regime.</div><div><a href='http://arxiv.org/abs/2403.08831v1'>2403.08831v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07310v2")'>How does promoting the minority fraction affect generalization? A
  theoretical study of the one-hidden-layer neural network on group imbalance</div>
<div id='2403.07310v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T04:38:05Z</div><div>Authors: Hongkang Li, Shuai Zhang, Yihua Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen</div><div style='padding-top: 10px; width: 80ex'>Group imbalance has been a known problem in empirical risk minimization
(ERM), where the achieved high average accuracy is accompanied by low accuracy
in a minority group. Despite algorithmic efforts to improve the minority group
accuracy, a theoretical generalization analysis of ERM on individual groups
remains elusive. By formulating the group imbalance problem with the Gaussian
Mixture Model, this paper quantifies the impact of individual groups on the
sample complexity, the convergence rate, and the average and group-level
testing performance. Although our theoretical framework is centered on binary
classification using a one-hidden-layer neural network, to the best of our
knowledge, we provide the first theoretical analysis of the group-level
generalization of ERM in addition to the commonly studied average
generalization performance. Sample insights of our theoretical results include
that when all group-level co-variance is in the medium regime and all mean are
close to zero, the learning performance is most desirable in the sense of a
small sample complexity, a fast training rate, and a high average and
group-level testing accuracy. Moreover, we show that increasing the fraction of
the minority group in the training data does not necessarily improve the
generalization performance of the minority group. Our theoretical results are
validated on both synthetic and empirical datasets, such as CelebA and CIFAR-10
in image classification.</div><div><a href='http://arxiv.org/abs/2403.07310v2'>2403.07310v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12235v1")'>The Fundamental Limits of Least-Privilege Learning</div>
<div id='2402.12235v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:44:54Z</div><div>Authors: Theresa Stadler, Bogdan Kulynych, Nicoals Papernot, Michael Gastpar, Carmela Troncoso</div><div style='padding-top: 10px; width: 80ex'>The promise of least-privilege learning -- to find feature representations
that are useful for a learning task but prevent inference of any sensitive
information unrelated to this task -- is highly appealing. However, so far this
concept has only been stated informally. It thus remains an open question
whether and how we can achieve this goal. In this work, we provide the first
formalisation of the least-privilege principle for machine learning and
characterise its feasibility. We prove that there is a fundamental trade-off
between a representation's utility for a given task and its leakage beyond the
intended task: it is not possible to learn representations that have high
utility for the intended task but, at the same time prevent inference of any
attribute other than the task label itself. This trade-off holds regardless of
the technique used to learn the feature mappings that produce these
representations. We empirically validate this result for a wide range of
learning techniques, model architectures, and datasets.</div><div><a href='http://arxiv.org/abs/2402.12235v1'>2402.12235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15207v1")'>Robust optimization for adversarial learning with finite sample
  complexity guarantees</div>
<div id='2403.15207v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T13:49:53Z</div><div>Authors: André Bertolace, Konstatinos Gatsis, Kostas Margellos</div><div style='padding-top: 10px; width: 80ex'>Decision making and learning in the presence of uncertainty has attracted
significant attention in view of the increasing need to achieve robust and
reliable operations. In the case where uncertainty stems from the presence of
adversarial attacks this need is becoming more prominent. In this paper we
focus on linear and nonlinear classification problems and propose a novel
adversarial training method for robust classifiers, inspired by Support Vector
Machine (SVM) margins. We view robustness under a data driven lens, and derive
finite sample complexity bounds for both linear and non-linear classifiers in
binary and multi-class scenarios. Notably, our bounds match natural
classifiers' complexity. Our algorithm minimizes a worst-case surrogate loss
using Linear Programming (LP) and Second Order Cone Programming (SOCP) for
linear and non-linear models. Numerical experiments on the benchmark MNIST and
CIFAR10 datasets show our approach's comparable performance to state-of-the-art
methods, without needing adversarial examples during training. Our work offers
a comprehensive framework for enhancing binary linear and non-linear classifier
robustness, embedding robustness in learning under the presence of adversaries.</div><div><a href='http://arxiv.org/abs/2403.15207v1'>2403.15207v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14988v1")'>Verifiable Boosted Tree Ensembles</div>
<div id='2402.14988v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T21:56:20Z</div><div>Authors: Stefano Calzavara, Lorenzo Cazzaro, Claudio Lucchese, Giulio Ermanno Pibiri</div><div style='padding-top: 10px; width: 80ex'>Verifiable learning advocates for training machine learning models amenable
to efficient security verification. Prior research demonstrated that specific
classes of decision tree ensembles -- called large-spread ensembles -- allow
for robustness verification in polynomial time against any norm-based attacker.
This study expands prior work on verifiable learning from basic ensemble
methods (i.e., hard majority voting) to advanced boosted tree ensembles, such
as those trained using XGBoost or LightGBM. Our formal results indicate that
robustness verification is achievable in polynomial time when considering
attackers based on the $L_\infty$-norm, but remains NP-hard for other
norm-based attackers. Nevertheless, we present a pseudo-polynomial time
algorithm to verify robustness against attackers based on the $L_p$-norm for
any $p \in \mathbb{N} \cup \{0\}$, which in practice grants excellent
performance. Our experimental evaluation shows that large-spread boosted
ensembles are accurate enough for practical adoption, while being amenable to
efficient security verification.</div><div><a href='http://arxiv.org/abs/2402.14988v1'>2402.14988v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17443v1")'>Liquid Democracy for Low-Cost Ensemble Pruning</div>
<div id='2401.17443v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T21:11:35Z</div><div>Authors: Ben Armstrong, Kate Larson</div><div style='padding-top: 10px; width: 80ex'>We argue that there is a strong connection between ensemble learning and a
delegative voting paradigm -- liquid democracy -- that can be leveraged to
reduce ensemble training costs. We present an incremental training procedure
that identifies and removes redundant classifiers from an ensemble via
delegation mechanisms inspired by liquid democracy. Through both analysis and
extensive experiments we show that this process greatly reduces the
computational cost of training compared to training a full ensemble. By
carefully selecting the underlying delegation mechanism, weight centralization
in the classifier population is avoided, leading to higher accuracy than some
boosting methods. Furthermore, this work serves as an exemplar of how
frameworks from computational social choice literature can be applied to
problems in nontraditional domains.</div><div><a href='http://arxiv.org/abs/2401.17443v1'>2401.17443v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16157v1")'>Consensus learning: A novel decentralised ensemble learning paradigm</div>
<div id='2402.16157v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T17:35:31Z</div><div>Authors: Horia Magureanu, Naïri Usher</div><div style='padding-top: 10px; width: 80ex'>The widespread adoption of large-scale machine learning models in recent
years highlights the need for distributed computing for efficiency and
scalability. This work introduces a novel distributed machine learning paradigm
-- \emph{consensus learning} -- which combines classical ensemble methods with
consensus protocols deployed in peer-to-peer systems. These algorithms consist
of two phases: first, participants develop their models and submit predictions
for any new data inputs; second, the individual predictions are used as inputs
for a communication phase, which is governed by a consensus protocol. Consensus
learning ensures user data privacy, while also inheriting the safety measures
against Byzantine attacks from the underlying consensus mechanism. We provide a
detailed theoretical analysis for a particular consensus protocol and compare
the performance of the consensus learning ensemble with centralised ensemble
learning algorithms. The discussion is supplemented by various numerical
simulations, which describe the robustness of the algorithms against Byzantine
participants.</div><div><a href='http://arxiv.org/abs/2402.16157v1'>2402.16157v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12142v1")'>Federated Bayesian Network Ensembles</div>
<div id='2402.12142v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T13:52:37Z</div><div>Authors: Florian van Daalen, Lianne Ippel, Andre Dekker, Inigo Bermejo</div><div style='padding-top: 10px; width: 80ex'>Federated learning allows us to run machine learning algorithms on
decentralized data when data sharing is not permitted due to privacy concerns.
Ensemble-based learning works by training multiple (weak) classifiers whose
output is aggregated. Federated ensembles are ensembles applied to a federated
setting, where each classifier in the ensemble is trained on one data location.
  In this article, we explore the use of federated ensembles of Bayesian
networks (FBNE) in a range of experiments and compare their performance with
locally trained models and models trained with VertiBayes, a federated learning
algorithm to train Bayesian networks from decentralized data. Our results show
that FBNE outperforms local models and provides a significant increase in
training speed compared with VertiBayes while maintaining a similar performance
in most settings, among other advantages. We show that FBNE is a potentially
useful tool within the federated learning toolbox, especially when local
populations are heavily biased, or there is a strong imbalance in population
size across parties. We discuss the advantages and disadvantages of this
approach in terms of time complexity, model accuracy, privacy protection, and
model interpretability.</div><div><a href='http://arxiv.org/abs/2402.12142v1'>2402.12142v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02904v1")'>Class-wise Generalization Error: an Information-Theoretic Analysis</div>
<div id='2401.02904v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T17:05:14Z</div><div>Authors: Firas Laakom, Yuheng Bu, Moncef Gabbouj</div><div style='padding-top: 10px; width: 80ex'>Existing generalization theories of supervised learning typically take a
holistic approach and provide bounds for the expected generalization over the
whole data distribution, which implicitly assumes that the model generalizes
similarly for all the classes. In practice, however, there are significant
variations in generalization performance among different classes, which cannot
be captured by the existing generalization bounds. In this work, we tackle this
problem by theoretically studying the class-generalization error, which
quantifies the generalization performance of each individual class. We derive a
novel information-theoretic bound for class-generalization error using the KL
divergence, and we further obtain several tighter bounds using the conditional
mutual information (CMI), which are significantly easier to estimate in
practice. We empirically validate our proposed bounds in different neural
networks and show that they accurately capture the complex class-generalization
error behavior. Moreover, we show that the theoretical tools developed in this
paper can be applied in several applications beyond this context.</div><div><a href='http://arxiv.org/abs/2401.02904v1'>2401.02904v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15500v1")'>Data-Driven Estimation of the False Positive Rate of the Bayes Binary
  Classifier via Soft Labels</div>
<div id='2401.15500v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T20:41:55Z</div><div>Authors: Minoh Jeong, Martina Cardone, Alex Dytso</div><div style='padding-top: 10px; width: 80ex'>Classification is a fundamental task in many applications on which
data-driven methods have shown outstanding performances. However, it is
challenging to determine whether such methods have achieved the optimal
performance. This is mainly because the best achievable performance is
typically unknown and hence, effectively estimating it is of prime importance.
In this paper, we consider binary classification problems and we propose an
estimator for the false positive rate (FPR) of the Bayes classifier, that is,
the optimal classifier with respect to accuracy, from a given dataset. Our
method utilizes soft labels, or real-valued labels, which are gaining
significant traction thanks to their properties. We thoroughly examine various
theoretical properties of our estimator, including its consistency,
unbiasedness, rate of convergence, and variance. To enhance the versatility of
our estimator beyond soft labels, we also consider noisy labels, which
encompass binary labels. For noisy labels, we develop effective FPR estimators
by leveraging a denoising technique and the Nadaraya-Watson estimator. Due to
the symmetry of the problem, our results can be readily applied to estimate the
false negative rate of the Bayes classifier.</div><div><a href='http://arxiv.org/abs/2401.15500v1'>2401.15500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05069v1")'>MISS: Multiclass Interpretable Scoring Systems</div>
<div id='2401.05069v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T10:57:12Z</div><div>Authors: Michal K. Grzeszczyk, Tomasz Trzciński, Arkadiusz Sitek</div><div style='padding-top: 10px; width: 80ex'>In this work, we present a novel, machine-learning approach for constructing
Multiclass Interpretable Scoring Systems (MISS) - a fully data-driven
methodology for generating single, sparse, and user-friendly scoring systems
for multiclass classification problems. Scoring systems are commonly utilized
as decision support models in healthcare, criminal justice, and other domains
where interpretability of predictions and ease of use are crucial. Prior
methods for data-driven scoring, such as SLIM (Supersparse Linear Integer
Model), were limited to binary classification tasks and extensions to
multiclass domains were primarily accomplished via one-versus-all-type
techniques. The scores produced by our method can be easily transformed into
class probabilities via the softmax function. We demonstrate techniques for
dimensionality reduction and heuristics that enhance the training efficiency
and decrease the optimality gap, a measure that can certify the optimality of
the model. Our approach has been extensively evaluated on datasets from various
domains, and the results indicate that it is competitive with other machine
learning models in terms of classification performance metrics and provides
well-calibrated class probabilities.</div><div><a href='http://arxiv.org/abs/2401.05069v1'>2401.05069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15005v1")'>Comparison of Machine Learning Classification Algorithms and Application
  to the Framingham Heart Study</div>
<div id='2402.15005v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T22:49:35Z</div><div>Authors: Nabil Kahouadji</div><div style='padding-top: 10px; width: 80ex'>The use of machine learning algorithms in healthcare can amplify social
injustices and health inequities. While the exacerbation of biases can occur
and compound during the problem selection, data collection, and outcome
definition, this research pertains to some generalizability impediments that
occur during the development and the post-deployment of machine learning
classification algorithms. Using the Framingham coronary heart disease data as
a case study, we show how to effectively select a probability cutoff to convert
a regression model for a dichotomous variable into a classifier. We then
compare the sampling distribution of the predictive performance of eight
machine learning classification algorithms under four training/testing
scenarios to test their generalizability and their potential to perpetuate
biases. We show that both the Extreme Gradient Boosting, and Support Vector
Machine are flawed when trained on an unbalanced dataset. We introduced and
show that the double discriminant scoring of type I is the most generalizable
as it consistently outperforms the other classification algorithms regardless
of the training/testing scenario. Finally, we introduce a methodology to
extract an optimal variable hierarchy for a classification algorithm, and
illustrate it on the overall, male and female Framingham coronary heart disease
data.</div><div><a href='http://arxiv.org/abs/2402.15005v1'>2402.15005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10999v1")'>Analysis and Mortality Prediction using Multiclass Classification for
  Older Adults with Type 2 Diabetes</div>
<div id='2402.10999v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:47:48Z</div><div>Authors: Ruchika Desure, Gutha Jaya Krishna</div><div style='padding-top: 10px; width: 80ex'>Designing proper treatment plans to manage diabetes requires health
practitioners to pay heed to the individuals remaining life along with the
comorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM)
are prone to experience premature death or even hypoglycaemia. The structured
dataset utilized has 68 potential mortality predictors for 275,190 diabetic
U.S. military Veterans aged 65 years or older. A new target variable is
invented by combining the two original target variables. Outliers are handled
by discretizing the continuous variables. Categorical variables have been dummy
encoded. Class balancing is achieved by random under-sampling. A benchmark
regression model is built using Multinomial Logistic Regression with LASSO.
Chi-Squared and Information Gain are the filter-based feature selection
techniques utilized. Classifiers such as Multinomial Logistic Regression,
Random Forest, Extreme Gradient Boosting (XGBoost), and One-vs-Rest classifier
are employed to build various models. Contrary to expectations, all the models
have constantly underperformed. XGBoost has given the highest accuracy of 53.03
percent with Chi-Squared feature selection. All the models have consistently
shown an acceptable performance for Class 3 (remaining life is more than 10
years), significantly low for Class 1 (remaining life is up to 5 years), and
the worst for Class 2 (remaining life is more than 5 but up to 10 years).
Features analysis has deduced that almost all input variables are associated
with multiple target classes. The high dimensionality of the input data after
dummy encoding seems to have confused the models, leading to
misclassifications. The approach taken in this study is ineffective in
producing a high-performing predictive model but lays a foundation as this
problem has never been viewed from a multiclass classification perspective.</div><div><a href='http://arxiv.org/abs/2402.10999v1'>2402.10999v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00965v1")'>Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to
  Advance ML-based Clinical Decision Support Systems for Early Prediction of
  Dialysis Among CKD Patients</div>
<div id='2403.00965v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T20:32:17Z</div><div>Authors: Hamed Khosravi, Srinjoy Das, Abdullah Al-Mamun, Imtiaz Ahmed</div><div style='padding-top: 10px; width: 80ex'>The Center for Disease Control estimates that over 37 million US adults
suffer from chronic kidney disease (CKD), yet 9 out of 10 of these individuals
are unaware of their condition due to the absence of symptoms in the early
stages. It has a significant impact on patients' quality of life, particularly
when it progresses to the need for dialysis. Early prediction of dialysis is
crucial as it can significantly improve patient outcomes and assist healthcare
providers in making timely and informed decisions. However, developing an
effective machine learning (ML)-based Clinical Decision Support System (CDSS)
for early dialysis prediction poses a key challenge due to the imbalanced
nature of data. To address this challenge, this study evaluates various data
augmentation techniques to understand their effectiveness on real-world
datasets. We propose a new approach named Binary Gaussian Copula Synthesis
(BGCS). BGCS is tailored for binary medical datasets and excels in generating
synthetic minority data that mirrors the distribution of the original data.
BGCS enhances early dialysis prediction by outperforming traditional methods in
detecting dialysis patients. For the best ML model, Random Forest, BCGS
achieved a 72% improvement, surpassing the state-of-the-art augmentation
approaches. Also, we present a ML-based CDSS, designed to aid clinicians in
making informed decisions. CDSS, which utilizes decision tree models, is
developed to improve patient outcomes, identify critical variables, and thereby
enable clinicians to make proactive decisions, and strategize treatment plans
effectively for CKD patients who are more likely to require dialysis in the
near future. Through comprehensive feature analysis and meticulous data
preparation, we ensure that the CDSS's dialysis predictions are not only
accurate but also actionable, providing a valuable tool in the management and
treatment of CKD.</div><div><a href='http://arxiv.org/abs/2403.00965v1'>2403.00965v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13756v1")'>NLICE: Synthetic Medical Record Generation for Effective Primary
  Healthcare Differential Diagnosis</div>
<div id='2401.13756v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T19:17:45Z</div><div>Authors: Zaid Al-Ars, Obinna Agba, Zhuoran Guo, Christiaan Boerkamp, Ziyaad Jaber, Tareq Jaber</div><div style='padding-top: 10px; width: 80ex'>This paper offers a systematic method for creating medical knowledge-grounded
patient records for use in activities involving differential diagnosis.
Additionally, an assessment of machine learning models that can differentiate
between various conditions based on given symptoms is also provided. We use a
public disease-symptom data source called SymCat in combination with Synthea to
construct the patients records. In order to increase the expressive nature of
the synthetic data, we use a medically-standardized symptom modeling method
called NLICE to augment the synthetic data with additional contextual
information for each condition. In addition, Naive Bayes and Random Forest
models are evaluated and compared on the synthetic data. The paper shows how to
successfully construct SymCat-based and NLICE-based datasets. We also show
results for the effectiveness of using the datasets to train predictive disease
models. The SymCat-based dataset is able to train a Naive Bayes and Random
Forest model yielding a 58.8% and 57.1% Top-1 accuracy score, respectively. In
contrast, the NLICE-based dataset improves the results, with a Top-1 accuracy
of 82.0% and Top-5 accuracy values of more than 90% for both models. Our
proposed data generation approach solves a major barrier to the application of
artificial intelligence methods in the healthcare domain. Our novel NLICE
symptom modeling approach addresses the incomplete and insufficient information
problem in the current binary symptom representation approach. The NLICE code
is open sourced at https://github.com/guozhuoran918/NLICE.</div><div><a href='http://arxiv.org/abs/2401.13756v1'>2401.13756v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01598v2")'>Learning from Two Decades of Blood Pressure Data: Demography-Specific
  Patterns Across 75 Million Patient Encounters</div>
<div id='2402.01598v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T17:51:49Z</div><div>Authors: Seyedeh Somayyeh Mousavi, Yuting Guo, Abeed Sarker, Reza Sameni</div><div style='padding-top: 10px; width: 80ex'>Hypertension remains a global health concern with a rising prevalence,
necessitating effective monitoring and understanding of blood pressure (BP)
dynamics. This study delves into the wealth of information derived from BP
measurement, a crucial approach in informing our understanding of hypertensive
trends. Numerous studies have reported on the relationship between BP variation
and various factors. In this research, we leveraged an extensive dataset
comprising 75 million records spanning two decades, offering a unique
opportunity to explore and analyze BP variations across demographic features
such as age, race, and gender. Our findings revealed that gender-based BP
variation was not statistically significant, challenging conventional
assumptions. Interestingly, systolic blood pressure (SBP) consistently
increased with age, while diastolic blood pressure (DBP) displayed a
distinctive peak in the forties age group. Moreover, our analysis uncovered
intriguing similarities in the distribution of BP among some of the racial
groups. This comprehensive investigation contributes to the ongoing discourse
on hypertension and underscores the importance of considering diverse
demographic factors in understanding BP variations. Our results provide
valuable insights that may inform personalized healthcare approaches tailored
to specific demographic profiles.</div><div><a href='http://arxiv.org/abs/2402.01598v2'>2402.01598v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12558v1")'>Evaluation of Country Dietary Habits Using Machine Learning Techniques
  in Relation to Deaths from COVID-19</div>
<div id='2402.12558v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T21:32:56Z</div><div>Authors: María Teresa García-Ordás, Natalia Arias, Carmen Benavides, Oscar García-Olalla, José Alberto Benítez-Andrades</div><div style='padding-top: 10px; width: 80ex'>COVID-19 disease has affected almost every country in the world. The large
number of infected people and the different mortality rates between countries
has given rise to many hypotheses about the key points that make the virus so
lethal in some places. In this study, the eating habits of 170 countries were
evaluated in order to find correlations between these habits and mortality
rates caused by COVID-19 using machine learning techniques that group the
countries together according to the different distribution of fat, energy, and
protein across 23 different types of food, as well as the amount ingested in
kilograms. Results shown how obesity and the high consumption of fats appear in
countries with the highest death rates, whereas countries with a lower rate
have a higher level of cereal consumption accompanied by a lower total average
intake of kilocalories.</div><div><a href='http://arxiv.org/abs/2402.12558v1'>2402.12558v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.15012v1")'>Empirical investigation of multi-source cross-validation in clinical
  machine learning</div>
<div id='2403.15012v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T07:56:31Z</div><div>Authors: Tuija Leinonen, David Wong, Ali Wahab, Ramesh Nadarajah, Matti Kaisti, Antti Airola</div><div style='padding-top: 10px; width: 80ex'>Traditionally, machine learning-based clinical prediction models have been
trained and evaluated on patient data from a single source, such as a hospital.
Cross-validation methods can be used to estimate the accuracy of such models on
new patients originating from the same source, by repeated random splitting of
the data. However, such estimates tend to be highly overoptimistic when
compared to accuracy obtained from deploying models to sources not represented
in the dataset, such as a new hospital. The increasing availability of
multi-source medical datasets provides new opportunities for obtaining more
comprehensive and realistic evaluations of expected accuracy through
source-level cross-validation designs.
  In this study, we present a systematic empirical evaluation of standard
K-fold cross-validation and leave-source-out cross-validation methods in a
multi-source setting. We consider the task of electrocardiogram based
cardiovascular disease classification, combining and harmonizing the openly
available PhysioNet CinC Challenge 2021 and the Shandong Provincial Hospital
datasets for our study.
  Our results show that K-fold cross-validation, both on single-source and
multi-source data, systemically overestimates prediction performance when the
end goal is to generalize to new sources. Leave-source-out cross-validation
provides more reliable performance estimates, having close to zero bias though
larger variability. The evaluation highlights the dangers of obtaining
misleading cross-validation results on medical data and demonstrates how these
issues can be mitigated when having access to multi-source data.</div><div><a href='http://arxiv.org/abs/2403.15012v1'>2403.15012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07079v1")'>The Relevance Feature and Vector Machine for health applications</div>
<div id='2402.07079v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T01:21:56Z</div><div>Authors: Albert Belenguer-Llorens, Carlos Sevilla-Salcedo, Emilio Parrado-Hernández, Vanessa Gómez-Verdejo</div><div style='padding-top: 10px; width: 80ex'>This paper presents the Relevance Feature and Vector Machine (RFVM), a novel
model that addresses the challenges of the fat-data problem when dealing with
clinical prospective studies. The fat-data problem refers to the limitations of
Machine Learning (ML) algorithms when working with databases in which the
number of features is much larger than the number of samples (a common scenario
in certain medical fields). To overcome such limitations, the RFVM incorporates
different characteristics: (1) A Bayesian formulation which enables the model
to infer its parameters without overfitting thanks to the Bayesian model
averaging. (2) A joint optimisation that overcomes the limitations arising from
the fat-data characteristic by simultaneously including the variables that
define the primal space (features) and those that define the dual space
(observations). (3) An integrated prunning that removes the irrelevant features
and samples during the training iterative optimization. Also, this last point
turns out crucial when performing medical prospective studies, enabling
researchers to exclude unnecessary medical tests, reducing costs and
inconvenience for patients, and identifying the critical patients/subjects that
characterize the disorder and, subsequently, optimize the patient recruitment
process that leads to a balanced cohort. The model capabilities are tested
against state-of-the-art models in several medical datasets with fat-data
problems. These experimental works show that RFVM is capable of achieving
competitive classification accuracies while providing the most compact subset
of data (in both terms of features and samples). Moreover, the selected
features (medical tests) seem to be aligned with the existing medical
literature.</div><div><a href='http://arxiv.org/abs/2402.07079v1'>2402.07079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05765v2")'>A new computationally efficient algorithm to solve Feature Selection for
  Functional Data Classification in high-dimensional spaces</div>
<div id='2401.05765v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T09:17:25Z</div><div>Authors: Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Alessandra Pascale, Jonathan Epperlein</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel methodology for Feature Selection for
Functional Classification, FSFC, that addresses the challenge of jointly
performing feature selection and classification of functional data in scenarios
with categorical responses and multivariate longitudinal features. FSFC tackles
a newly defined optimization problem that integrates logistic loss and
functional features to identify the most crucial variables for classification.
To address the minimization procedure, we employ functional principal
components and develop a new adaptive version of the Dual Augmented Lagrangian
algorithm. The computational efficiency of FSFC enables handling
high-dimensional scenarios where the number of features may considerably exceed
the number of statistical units. Simulation experiments demonstrate that FSFC
outperforms other machine learning and deep learning methods in computational
time and classification accuracy. Furthermore, the FSFC feature selection
capability can be leveraged to significantly reduce the problem's
dimensionality and enhance the performances of other classification algorithms.
The efficacy of FSFC is also demonstrated through a real data application,
analyzing relationships between four chronic diseases and other health and
demographic factors.</div><div><a href='http://arxiv.org/abs/2401.05765v2'>2401.05765v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06172v1")'>CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine
  Learning Methods</div>
<div id='2401.06172v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T04:07:14Z</div><div>Authors: Yue Chen, Xingyi Andrew, Salintip Supasanya</div><div style='padding-top: 10px; width: 80ex'>Historically, the economic recession often came abruptly and disastrously.
For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from
October 2007 to March 2009. If we could detect the signals of the crisis
earlier, we could have taken preventive measures. Therefore, driven by such
motivation, we use advanced machine learning techniques, including Random
Forest and Extreme Gradient Boosting, to predict any potential market crashes
mainly in the US market. Also, we would like to compare the performance of
these methods and examine which model is better for forecasting US stock market
crashes. We apply our models on the daily financial market data, which tend to
be more responsive with higher reporting frequencies. We consider 75
explanatory variables, including general US stock market indexes, SP 500 sector
indexes, as well as market indicators that can be used for the purpose of
crisis prediction. Finally, we conclude, with selected classification metrics,
that the Extreme Gradient Boosting method performs the best in predicting US
stock market crisis events.</div><div><a href='http://arxiv.org/abs/2401.06172v1'>2401.06172v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09596v1")'>Pulmonologists-Level lung cancer detection based on standard blood test
  results and smoking status using an explainable machine learning approach</div>
<div id='2402.09596v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T22:00:57Z</div><div>Authors: Ricco Noel Hansen Flyckt, Louise Sjodsholm, Margrethe Høstgaard Bang Henriksen, Claus Lohman Brasen, Ali Ebrahimi, Ole Hilberg, Torben Frøstrup Hansen, Uffe Kock Wiil, Lars Henrik Jensen, Abdolrahman Peimankar</div><div style='padding-top: 10px; width: 80ex'>Lung cancer (LC) remains the primary cause of cancer-related mortality,
largely due to late-stage diagnoses. Effective strategies for early detection
are therefore of paramount importance. In recent years, machine learning (ML)
has demonstrated considerable potential in healthcare by facilitating the
detection of various diseases. In this retrospective development and validation
study, we developed an ML model based on dynamic ensemble selection (DES) for
LC detection. The model leverages standard blood sample analysis and smoking
history data from a large population at risk in Denmark. The study includes all
patients examined on suspicion of LC in the Region of Southern Denmark from
2009 to 2018. We validated and compared the predictions by the DES model with
diagnoses provided by five pulmonologists. Among the 38,944 patients, 9,940 had
complete data of which 2,505 (25\%) had LC. The DES model achieved an area
under the roc curve of 0.77$\pm$0.01, sensitivity of 76.2\%$\pm$2.4\%,
specificity of 63.8\%$\pm$2.3\%, positive predictive value of 41.6\%$\pm$1.2\%,
and F\textsubscript{1}-score of 53.8\%$\pm$1.1\%. The DES model outperformed
all five pulmonologists, achieving a sensitivity 9\% higher than their average.
The model identified smoking status, age, total calcium levels, neutrophil
count, and lactate dehydrogenase as the most important factors for the
detection of LC. The results highlight the successful application of the ML
approach in detecting LC, surpassing pulmonologists' performance. Incorporating
clinical and laboratory data in future risk assessment models can improve
decision-making and facilitate timely referrals.</div><div><a href='http://arxiv.org/abs/2402.09596v1'>2402.09596v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06498v1")'>Temporal and Between-Group Variability in College Dropout Prediction</div>
<div id='2401.06498v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T10:43:55Z</div><div>Authors: Dominik Glandorf, Hye Rin Lee, Gabe Avakian Orona, Marina Pumptow, Renzhe Yu, Christian Fischer</div><div style='padding-top: 10px; width: 80ex'>Large-scale administrative data is a common input in early warning systems
for college dropout in higher education. Still, the terminology and methodology
vary significantly across existing studies, and the implications of different
modeling decisions are not fully understood. This study provides a systematic
evaluation of contributing factors and predictive performance of machine
learning models over time and across different student groups. Drawing on
twelve years of administrative data at a large public university in the US, we
find that dropout prediction at the end of the second year has a 20% higher AUC
than at the time of enrollment in a Random Forest model. Also, most predictive
factors at the time of enrollment, including demographics and high school
performance, are quickly superseded in predictive importance by college
performance and in later stages by enrollment behavior. Regarding variability
across student groups, college GPA has more predictive value for students from
traditionally disadvantaged backgrounds than their peers. These results can
help researchers and administrators understand the comparative value of
different data sources when building early warning systems and optimizing
decisions under specific policy goals.</div><div><a href='http://arxiv.org/abs/2401.06498v1'>2401.06498v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14663v1")'>Machine Learning Predicts Upper Secondary Education Dropout as Early as
  the End of Primary School</div>
<div id='2403.14663v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:18:08Z</div><div>Authors: Maria Psyridou, Fabi Prezja, Minna Torppa, Marja-Kristiina Lerkkanen, Anna-Maija Poikkeus, Kati Vasalampi</div><div style='padding-top: 10px; width: 80ex'>Education plays a pivotal role in alleviating poverty, driving economic
growth, and empowering individuals, thereby significantly influencing societal
and personal development. However, the persistent issue of school dropout poses
a significant challenge, with its effects extending beyond the individual.
While previous research has employed machine learning for dropout
classification, these studies often suffer from a short-term focus, relying on
data collected only a few years into the study period. This study expanded the
modeling horizon by utilizing a 13-year longitudinal dataset, encompassing data
from kindergarten to Grade 9. Our methodology incorporated a comprehensive
range of parameters, including students' academic and cognitive skills,
motivation, behavior, well-being, and officially recorded dropout data. The
machine learning models developed in this study demonstrated notable
classification ability, achieving a mean area under the curve (AUC) of 0.61
with data up to Grade 6 and an improved AUC of 0.65 with data up to Grade 9.
Further data collection and independent correlational and causal analyses are
crucial. In future iterations, such models may have the potential to
proactively support educators' processes and existing protocols for identifying
at-risk students, thereby potentially aiding in the reinvention of student
retention and success strategies and ultimately contributing to improved
educational outcomes.</div><div><a href='http://arxiv.org/abs/2403.14663v1'>2403.14663v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01710v1")'>Exploring Educational Equity: A Machine Learning Approach to Unravel
  Achievement Disparities in Georgia</div>
<div id='2402.01710v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T15:05:52Z</div><div>Authors: Yichen Ma, Dima Nazzal</div><div style='padding-top: 10px; width: 80ex'>The COVID-19 pandemic has significantly exacerbated existing educational
disparities in Georgia's K-12 system, particularly in terms of racial and
ethnic achievement gaps. Utilizing machine learning methods, the study conducts
a comprehensive analysis of student achievement rates across different
demographics, regions, and subjects. The findings highlight a significant
decline in proficiency in English and Math during the pandemic, with a
noticeable contraction in score distribution and a greater impact on
economically disadvantaged and Black students. Socio-economic status, as
represented by the Directly Certified Percentage -- the percentage of students
eligible for free lunch, emerges as the most crucial factor, with additional
insights drawn from faculty resources such as teacher salaries and expenditure
on instruction. The study also identifies disparities in achievement rates
between urban and rural settings, as well as variations across counties,
underscoring the influence of geographical and socio-economic factors. The data
suggests that targeted interventions and resource allocation, particularly in
schools with higher percentages of economically disadvantaged students, are
essential for mitigating educational disparities.</div><div><a href='http://arxiv.org/abs/2402.01710v1'>2402.01710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13819v1")'>A machine learning approach to predict university enrolment choices
  through students' high school background in Italy</div>
<div id='2403.13819v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T10:05:37Z</div><div>Authors: Andrea Priulla, Alessandro Albano, Nicoletta D'Angelo, Massimo Attanasio</div><div style='padding-top: 10px; width: 80ex'>This paper explores the influence of Italian high school students'
proficiency in mathematics and the Italian language on their university
enrolment choices, specifically focusing on STEM (Science, Technology,
Engineering, and Mathematics) courses. We distinguish between students from
scientific and humanistic backgrounds in high school, providing valuable
insights into their enrolment preferences. Furthermore, we investigate
potential gender differences in response to similar previous educational
choices and achievements. The study employs gradient boosting methodology,
known for its high predicting performance and ability to capture non-linear
relationships within data, and adjusts for variables related to the
socio-demographic characteristics of the students and their previous
educational achievements. Our analysis reveals significant differences in the
enrolment choices based on previous high school achievements. The findings shed
light on the complex interplay of academic proficiency, gender, and high school
background in shaping students' choices regarding university education, with
implications for educational policy and future research endeavours.</div><div><a href='http://arxiv.org/abs/2403.13819v1'>2403.13819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01655v1")'>A Deep Learning Approach Towards Student Performance Prediction in
  Online Courses: Challenges Based on a Global Perspective</div>
<div id='2402.01655v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T19:13:19Z</div><div>Authors: Abdallah Moubayed, MohammadNoor Injadat, Nouh Alhindawi, Ghassan Samara, Sara Abuasal, Raed Alazaidah</div><div style='padding-top: 10px; width: 80ex'>Analyzing and evaluating students' progress in any learning environment is
stressful and time consuming if done using traditional analysis methods. This
is further exasperated by the increasing number of students due to the shift of
focus toward integrating the Internet technologies in education and the focus
of academic institutions on moving toward e-Learning, blended, or online
learning models. As a result, the topic of student performance prediction has
become a vibrant research area in recent years. To address this, machine
learning and data mining techniques have emerged as a viable solution. To that
end, this work proposes the use of deep learning techniques (CNN and RNN-LSTM)
to predict the students' performance at the midpoint stage of the online course
delivery using three distinct datasets collected from three different regions
of the world. Experimental results show that deep learning models have
promising performance as they outperform other optimized traditional ML models
in two of the three considered datasets while also having comparable
performance for the third dataset.</div><div><a href='http://arxiv.org/abs/2402.01655v1'>2402.01655v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06843v1")'>Towards an educational tool for supporting neonatologists in the
  delivery room</div>
<div id='2403.06843v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:03:21Z</div><div>Authors: Giorgio Leonardi, Clara Maldarizzi, Stefania Montani, Manuel Striani, Mariachiara Martina Strozzi</div><div style='padding-top: 10px; width: 80ex'>Nowadays, there is evidence that several factors may increase the risk, for
an infant, to require stabilisation or resuscitation manoeuvres at birth.
However, this risk factors are not completely known, and a universally
applicable model for predicting high-risk situations is not available yet.
Considering both these limitations and the fact that the need for resuscitation
at birth is a rare event, periodic training of the healthcare personnel
responsible for newborn caring in the delivery room is mandatory.
  In this paper, we propose a machine learning approach for identifying risk
factors and their impact on the birth event from real data, which can be used
by personnel to progressively increase and update their knowledge. Our final
goal will be the one of designing a user-friendly mobile application, able to
improve the recognition rate and the planning of the appropriate interventions
on high-risk patients.</div><div><a href='http://arxiv.org/abs/2403.06843v1'>2403.06843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06091v2")'>A Closer Look at AUROC and AUPRC under Class Imbalance</div>
<div id='2401.06091v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:11:42Z</div><div>Authors: Matthew B. A. McDermott, Lasse Hyldig Hansen, Haoran Zhang, Giovanni Angelotti, Jack Gallifant</div><div style='padding-top: 10px; width: 80ex'>In machine learning (ML), a widespread adage is that the area under the
precision-recall curve (AUPRC) is a superior metric for model comparison to the
area under the receiver operating characteristic (AUROC) for binary
classification tasks with class imbalance. This paper challenges this notion
through novel mathematical analysis, illustrating that AUROC and AUPRC can be
concisely related in probabilistic terms. We demonstrate that AUPRC, contrary
to popular belief, is not superior in cases of class imbalance and might even
be a harmful metric, given its inclination to unduly favor model improvements
in subpopulations with more frequent positive labels. This bias can
inadvertently heighten algorithmic disparities. Prompted by these insights, a
thorough review of existing ML literature was conducted, utilizing large
language models to analyze over 1.5 million papers from arXiv. Our
investigation focused on the prevalence and substantiation of the purported
AUPRC superiority. The results expose a significant deficit in empirical
backing and a trend of misattributions that have fuelled the widespread
acceptance of AUPRC's supposed advantages. Our findings represent a dual
contribution: a significant technical advancement in understanding metric
behaviors and a stark warning about unchecked assumptions in the ML community.
All experiments are accessible at
https://github.com/mmcdermott/AUC_is_all_you_need.</div><div><a href='http://arxiv.org/abs/2401.06091v2'>2401.06091v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02249v1")'>Don't Label Twice: Quantity Beats Quality when Comparing Binary
  Classifiers on a Budget</div>
<div id='2402.02249v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T19:40:41Z</div><div>Authors: Florian E. Dorner, Moritz Hardt</div><div style='padding-top: 10px; width: 80ex'>We study how to best spend a budget of noisy labels to compare the accuracy
of two binary classifiers. It's common practice to collect and aggregate
multiple noisy labels for a given data point into a less noisy label via a
majority vote. We prove a theorem that runs counter to conventional wisdom. If
the goal is to identify the better of two classifiers, we show it's best to
spend the budget on collecting a single label for more samples. Our result
follows from a non-trivial application of Cram\'er's theorem, a staple in the
theory of large deviations. We discuss the implications of our work for the
design of machine learning benchmarks, where they overturn some time-honored
recommendations. In addition, our results provide sample size bounds superior
to what follows from Hoeffding's bound.</div><div><a href='http://arxiv.org/abs/2402.02249v1'>2402.02249v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07453v1")'>Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs</div>
<div id='2402.07453v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T07:20:05Z</div><div>Authors: Yuval Filmus, Steve Hanneke, Idan Mehalel, Shay Moran</div><div style='padding-top: 10px; width: 80ex'>Consider the domain of multiclass classification within the adversarial
online setting. What is the price of relying on bandit feedback as opposed to
full information? To what extent can an adaptive adversary amplify the loss
compared to an oblivious one? To what extent can a randomized learner reduce
the loss compared to a deterministic one? We study these questions in the
mistake bound model and provide nearly tight answers.
  We demonstrate that the optimal mistake bound under bandit feedback is at
most $O(k)$ times higher than the optimal mistake bound in the full information
case, where $k$ represents the number of labels. This bound is tight and
provides an answer to an open question previously posed and studied by Daniely
and Helbertal ['13] and by Long ['17, '20], who focused on deterministic
learners.
  Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap
between randomized and deterministic learners, as well as between adaptive and
oblivious adversaries in the bandit feedback setting. This stands in contrast
to the full information scenario, where adaptive and oblivious adversaries are
equivalent, and the gap in mistake bounds between randomized and deterministic
learners is a constant multiplicative factor of $2$.
  In addition, our results imply that in some cases the optimal randomized
mistake bound is approximately the square-root of its deterministic parallel.
Previous results show that this is essentially the smallest it can get.</div><div><a href='http://arxiv.org/abs/2402.07453v1'>2402.07453v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08180v1")'>Online Structured Prediction with Fenchel--Young Losses and Improved
  Surrogate Regret for Online Multiclass Classification with Logistic Loss</div>
<div id='2402.08180v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T02:36:41Z</div><div>Authors: Shinsaku Sakaue, Han Bao, Taira Tsuchiya, Taihei Oki</div><div style='padding-top: 10px; width: 80ex'>This paper studies online structured prediction with full-information
feedback. For online multiclass classification, van der Hoeven (2020) has
obtained surrogate regret bounds independent of the time horizon, or
\emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap}
framework. However, this framework has been limited to multiclass
classification primarily because it relies on a classification-specific
procedure for converting estimated scores to outputs. We extend the
exploit-the-surrogate-gap framework to online structured prediction with
\emph{Fenchel--Young losses}, a large family of surrogate losses including the
logistic loss for multiclass classification, obtaining finite surrogate regret
bounds in various structured prediction problems. To this end, we propose and
analyze \emph{randomized decoding}, which converts estimated scores to general
structured outputs. Moreover, by applying our decoding to online multiclass
classification with the logistic loss, we obtain a surrogate regret bound of
$O(B^2)$, where $B$ is the $\ell_2$-diameter of the domain. This bound is tight
up to logarithmic factors and improves the previous bound of $O(dB^2)$ due to
van der Hoeven (2020) by a factor of $d$, the number of classes.</div><div><a href='http://arxiv.org/abs/2402.08180v1'>2402.08180v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19303v1")'>Learnability Gaps of Strategic Classification</div>
<div id='2402.19303v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T16:09:19Z</div><div>Authors: Lee Cohen, Yishay Mansour, Shay Moran, Han Shao</div><div style='padding-top: 10px; width: 80ex'>In contrast with standard classification tasks, strategic classification
involves agents strategically modifying their features in an effort to receive
favorable predictions. For instance, given a classifier determining loan
approval based on credit scores, applicants may open or close their credit
cards to fool the classifier. The learning goal is to find a classifier robust
against strategic manipulations. Various settings, based on what and when
information is known, have been explored in strategic classification. In this
work, we focus on addressing a fundamental question: the learnability gaps
between strategic classification and standard learning.
  We essentially show that any learnable class is also strategically learnable:
we first consider a fully informative setting, where the manipulation structure
(which is modeled by a manipulation graph $G^\star$) is known and during
training time the learner has access to both the pre-manipulation data and
post-manipulation data. We provide nearly tight sample complexity and regret
bounds, offering significant improvements over prior results. Then, we relax
the fully informative setting by introducing two natural types of uncertainty.
First, following Ahmadi et al. (2023), we consider the setting in which the
learner only has access to the post-manipulation data. We improve the results
of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower
bound raised by them. Our second relaxation of the fully informative setting
introduces uncertainty to the manipulation structure. That is, we assume that
the manipulation graph is unknown but belongs to a known class of graphs. We
provide nearly tight bounds on the learning complexity in various unknown
manipulation graph settings. Notably, our algorithm in this setting is of
independent interest and can be applied to other problems such as multi-label
learning.</div><div><a href='http://arxiv.org/abs/2402.19303v1'>2402.19303v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08758v1")'>Bayesian Strategic Classification</div>
<div id='2402.08758v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:51:49Z</div><div>Authors: Lee Cohen, Saeed Sharifi-Malvajerdi, Kevin Stangl, Ali Vakilian, Juba Ziani</div><div style='padding-top: 10px; width: 80ex'>In strategic classification, agents modify their features, at a cost, to
ideally obtain a positive classification from the learner's classifier. The
typical response of the learner is to carefully modify their classifier to be
robust to such strategic behavior. When reasoning about agent manipulations,
most papers that study strategic classification rely on the following strong
assumption: agents fully know the exact parameters of the deployed classifier
by the learner. This often is an unrealistic assumption when using complex or
proprietary machine learning techniques in real-world prediction tasks.
  We initiate the study of partial information release by the learner in
strategic classification. We move away from the traditional assumption that
agents have full knowledge of the classifier. Instead, we consider agents that
have a common distributional prior on which classifier the learner is using.
The learner in our model can reveal truthful, yet not necessarily complete,
information about the deployed classifier to the agents. The learner's goal is
to release just enough information about the classifier to maximize accuracy.
We show how such partial information release can, counter-intuitively, benefit
the learner's accuracy, despite increasing agents' abilities to manipulate.
  We show that while it is intractable to compute the best response of an agent
in the general case, there exist oracle-efficient algorithms that can solve the
best response of the agents when the learner's hypothesis class is the class of
linear classifiers, or when the agents' cost function satisfies a natural
notion of submodularity as we define. We then turn our attention to the
learner's optimization problem and provide both positive and negative results
on the algorithmic problem of how much information the learner should release
about the classifier to maximize their expected accuracy.</div><div><a href='http://arxiv.org/abs/2402.08758v1'>2402.08758v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15274v1")'>Classification Under Strategic Self-Selection</div>
<div id='2402.15274v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T11:37:56Z</div><div>Authors: Guy Horowitz, Yonatan Sommer, Moran Koren, Nir Rosenfeld</div><div style='padding-top: 10px; width: 80ex'>When users stand to gain from certain predictions, they are prone to act
strategically to obtain favorable predictive outcomes. Whereas most works on
strategic classification consider user actions that manifest as feature
modifications, we study a novel setting in which users decide -- in response to
the learned classifier -- whether to at all participate (or not). For learning
approaches of increasing strategic awareness, we study the effects of
self-selection on learning, and the implications of learning on the composition
of the self-selected population. We then propose a differentiable framework for
learning under self-selective behavior, which can be optimized effectively. We
conclude with experiments on real data and simulated behavior that both
complement our analysis and demonstrate the utility of our approach.</div><div><a href='http://arxiv.org/abs/2402.15274v1'>2402.15274v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16422v2")'>Strategic Usage in a Multi-Learner Setting</div>
<div id='2401.16422v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:59:22Z</div><div>Authors: Eliot Shekhtman, Sarah Dean</div><div style='padding-top: 10px; width: 80ex'>Real-world systems often involve some pool of users choosing between a set of
services. With the increase in popularity of online learning algorithms, these
services can now self-optimize, leveraging data collected on users to maximize
some reward such as service quality. On the flipside, users may strategically
choose which services to use in order to pursue their own reward functions, in
the process wielding power over which services can see and use their data.
Extensive prior research has been conducted on the effects of strategic users
in single-service settings, with strategic behavior manifesting in the
manipulation of observable features to achieve a desired classification;
however, this can often be costly or unattainable for users and fails to
capture the full behavior of multi-service dynamic systems. As such, we analyze
a setting in which strategic users choose among several available services in
order to pursue positive classifications, while services seek to minimize loss
functions on their observations. We focus our analysis on realizable settings,
and show that naive retraining can still lead to oscillation even if all users
are observed at different times; however, if this retraining uses memory of
past observations, convergent behavior can be guaranteed for certain loss
function classes. We provide results obtained from synthetic and real-world
data to empirically validate our theoretical findings.</div><div><a href='http://arxiv.org/abs/2401.16422v2'>2401.16422v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07213v1")'>Which LLM to Play? Convergence-Aware Online Model Selection with
  Time-Increasing Bandits</div>
<div id='2403.07213v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T23:52:46Z</div><div>Authors: Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A. Rossi, Sungchul Kim, Shuai Li</div><div style='padding-top: 10px; width: 80ex'>Web-based applications such as chatbots, search engines and news
recommendations continue to grow in scale and complexity with the recent surge
in the adoption of LLMs. Online model selection has thus garnered increasing
attention due to the need to choose the best model among a diverse set while
balancing task reward and exploration cost. Organizations faces decisions like
whether to employ a costly API-based LLM or a locally finetuned small LLM,
weighing cost against performance. Traditional selection methods often evaluate
every candidate model before choosing one, which are becoming impractical given
the rising costs of training and finetuning LLMs. Moreover, it is undesirable
to allocate excessive resources towards exploring poor-performing models. While
some recent works leverage online bandit algorithm to manage such
exploration-exploitation trade-off in model selection, they tend to overlook
the increasing-then-converging trend in model performances as the model is
iteratively finetuned, leading to less accurate predictions and suboptimal
model selections.
  In this paper, we propose a time-increasing bandit algorithm TI-UCB, which
effectively predicts the increase of model performances due to finetuning and
efficiently balances exploration and exploitation in model selection. To
further capture the converging points of models, we develop a change detection
mechanism by comparing consecutive increase predictions. We theoretically prove
that our algorithm achieves a logarithmic regret upper bound in a typical
increasing bandit setting, which implies a fast convergence rate. The advantage
of our method is also empirically validated through extensive experiments on
classification model selection and online selection of LLMs. Our results
highlight the importance of utilizing increasing-then-converging pattern for
more efficient and economic model selection in the deployment of LLMs.</div><div><a href='http://arxiv.org/abs/2403.07213v1'>2403.07213v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10942v1")'>Machine Unlearning for Recommendation Systems: An Insight</div>
<div id='2401.10942v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T18:35:44Z</div><div>Authors: Bhavika Sachdeva, Harshita Rathee, Sristi, Arun Sharma, Witold Wydmański</div><div style='padding-top: 10px; width: 80ex'>This review explores machine unlearning (MUL) in recommendation systems,
addressing adaptability, personalization, privacy, and bias challenges. Unlike
traditional models, MUL dynamically adjusts system knowledge based on shifts in
user preferences and ethical considerations. The paper critically examines
MUL's basics, real-world applications, and challenges like algorithmic
transparency. It sifts through literature, offering insights into how MUL could
transform recommendations, discussing user trust, and suggesting paths for
future research in responsible and user-focused artificial intelligence (AI).
The document guides researchers through challenges involving the trade-off
between personalization and privacy, encouraging contributions to meet
practical demands for targeted data removal. Emphasizing MUL's role in secure
and adaptive machine learning, the paper proposes ways to push its boundaries.
The novelty of this paper lies in its exploration of the limitations of the
methods, which highlights exciting prospects for advancing the field.</div><div><a href='http://arxiv.org/abs/2401.10942v1'>2401.10942v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17236v1")'>A Review of Data Mining in Personalized Education: Current Trends and
  Future Prospects</div>
<div id='2402.17236v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T06:09:48Z</div><div>Authors: Zhang Xiong, Haoxuan Li, Zhuang Liu, Zhuofan Chen, Hao Zhou, Wenge Rong, Yuanxin Ouyang</div><div style='padding-top: 10px; width: 80ex'>Personalized education, tailored to individual student needs, leverages
educational technology and artificial intelligence (AI) in the digital age to
enhance learning effectiveness. The integration of AI in educational platforms
provides insights into academic performance, learning preferences, and
behaviors, optimizing the personal learning process. Driven by data mining
techniques, it not only benefits students but also provides educators and
institutions with tools to craft customized learning experiences. To offer a
comprehensive review of recent advancements in personalized educational data
mining, this paper focuses on four primary scenarios: educational
recommendation, cognitive diagnosis, knowledge tracing, and learning analysis.
This paper presents a structured taxonomy for each area, compiles commonly used
datasets, and identifies future research directions, emphasizing the role of
data mining in enhancing personalized education and paving the way for future
exploration and innovation.</div><div><a href='http://arxiv.org/abs/2402.17236v1'>2402.17236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14664v1")'>ClickTree: A Tree-based Method for Predicting Math Students' Performance
  Based on Clickstream Data</div>
<div id='2403.14664v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T23:39:03Z</div><div>Authors: Narjes Rohani, Behnam Rohani, Areti Manataki</div><div style='padding-top: 10px; width: 80ex'>The prediction of student performance and the analysis of students' learning
behavior play an important role in enhancing online courses. By analysing a
massive amount of clickstream data that captures student behavior, educators
can gain valuable insights into the factors that influence academic outcomes
and identify areas of improvement in courses. In this study, we developed
ClickTree, a tree-based methodology, to predict student performance in
mathematical assignments based on students' clickstream data. We extracted a
set of features, including problem-level, assignment-level and student-level
features, from the extensive clickstream data and trained a CatBoost tree to
predict whether a student successfully answers a problem in an assignment. The
developed method achieved an AUC of 0.78844 in the Educational Data Mining Cup
2023 and ranked second in the competition. Furthermore, our results indicate
that students encounter more difficulties in the problem types that they must
select a subset of answers from a given set as well as problem subjects of
Algebra II. Additionally, students who performed well in answering end-unit
assignment problems engaged more with in-unit assignments and answered more
problems correctly, while those who struggled had higher tutoring request rate.
The proposed method can be utilized to improve students' learning experiences,
and the above insights can be integrated into mathematical courses to enhance
students' learning outcomes.</div><div><a href='http://arxiv.org/abs/2403.14664v1'>2403.14664v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14597v1")'>Learning Style Identification Using Semi-Supervised Self-Taught Labeling</div>
<div id='2402.14597v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T11:56:49Z</div><div>Authors: Hani Y. Ayyoub, Omar S. Al-Kadi</div><div style='padding-top: 10px; width: 80ex'>Education is a dynamic field that must be adaptable to sudden changes and
disruptions caused by events like pandemics, war, and natural disasters related
to climate change. When these events occur, traditional classrooms with
traditional or blended delivery can shift to fully online learning, which
requires an efficient learning environment that meets students' needs. While
learning management systems support teachers' productivity and creativity, they
typically provide the same content to all learners in a course, ignoring their
unique learning styles. To address this issue, we propose a semi-supervised
machine learning approach that detects students' learning styles using a data
mining technique. We use the commonly used Felder Silverman learning style
model and demonstrate that our semi-supervised method can produce reliable
classification models with few labeled data. We evaluate our approach on two
different courses and achieve an accuracy of 88.83% and 77.35%, respectively.
Our work shows that educational data mining and semi-supervised machine
learning techniques can identify different learning styles and create a
personalized learning environment.</div><div><a href='http://arxiv.org/abs/2402.14597v1'>2402.14597v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05555v1")'>Subgroup Discovery in MOOCs: A Big Data Application for Describing
  Different Types of Learners</div>
<div id='2403.05555v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T16:07:38Z</div><div>Authors: J. M. Luna, H. M. Fardoun, F. Padillo, C. Romero, S. Ventura</div><div style='padding-top: 10px; width: 80ex'>The aim of this paper is to categorize and describe different types of
learners in massive open online courses (MOOCs) by means of a subgroup
discovery approach based on MapReduce. The final objective is to discover
IF-THEN rules that appear in different MOOCs. The proposed subgroup discovery
approach, which is an extension of the well-known FP-Growth algorithm,
considers emerging parallel methodologies like MapReduce to be able to cope
with extremely large datasets. As an additional feature, the proposal includes
a threshold value to denote the number of courses that each discovered rule
should satisfy. A post-processing step is also included so redundant subgroups
can be removed. The experimental stage is carried out by considering
de-identified data from the first year of 16 MITx and HarvardX courses on the
edX platform. Experimental results demonstrate that the proposed MapReduce
approach outperforms traditional sequential subgroup discovery approaches,
achieving a runtime that is almost constant for different courses.
Additionally, thanks to the final post-processing step, only interesting and
not-redundant rules are discovered, hence reducing the number of subgroups in
one or two orders of magnitude. Finally, the discovered subgroups are easily
used by courses' instructors not only for descriptive purposes but also for
additional tasks such as recommendation or personalization.</div><div><a href='http://arxiv.org/abs/2403.05555v1'>2403.05555v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00961v1")'>Data Science Education in Undergraduate Physics: Lessons Learned from a
  Community of Practice</div>
<div id='2403.00961v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T20:21:42Z</div><div>Authors: Karan Shah, Julie Butler, Alexis Knaub, Anıl Zenginoğlu, William Ratcliff, Mohammad Soltanieh-ha</div><div style='padding-top: 10px; width: 80ex'>With the increasing availability of diverse datasets, ranging from
small-scale experimental data points to large and complex data repositories and
powerful data analysis tools, it is increasingly important that physics
educators equip their students with the skills to work with data effectively.
However, many educators may lack the necessary training and expertise in data
science to teach these skills. To address this gap, we created the Data Science
Education Community of Practice (DSECOP), bringing together graduate students
and physics educators from different institutions and backgrounds to share best
practices and lessons learned in integrating data science into undergraduate
physics education. In this article, we present insights and experiences from
this community of practice, highlighting key strategies and challenges in
incorporating data science into the introductory physics curriculum. Our goal
is to provide guidance and inspiration to educators who seek to integrate data
science into their teaching, helping to prepare the next generation of
physicists for a data-driven world.</div><div><a href='http://arxiv.org/abs/2403.00961v1'>2403.00961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14689v1")'>Developing and Deploying Industry Standards for Artificial Intelligence
  in Education (AIED): Challenges, Strategies, and Future Directions</div>
<div id='2403.14689v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-13T22:38:08Z</div><div>Authors: Richard Tong, Haoyang Li, Joleen Liang, Qingsong Wen</div><div style='padding-top: 10px; width: 80ex'>The adoption of Artificial Intelligence in Education (AIED) holds the promise
of revolutionizing educational practices by offering personalized learning
experiences, automating administrative and pedagogical tasks, and reducing the
cost of content creation. However, the lack of standardized practices in the
development and deployment of AIED solutions has led to fragmented ecosystems,
which presents challenges in interoperability, scalability, and ethical
governance. This article aims to address the critical need to develop and
implement industry standards in AIED, offering a comprehensive analysis of the
current landscape, challenges, and strategic approaches to overcome these
obstacles. We begin by examining the various applications of AIED in various
educational settings and identify key areas lacking in standardization,
including system interoperability, ontology mapping, data integration,
evaluation, and ethical governance. Then, we propose a multi-tiered framework
for establishing robust industry standards for AIED. In addition, we discuss
methodologies for the iterative development and deployment of standards,
incorporating feedback loops from real-world applications to refine and adapt
standards over time. The paper also highlights the role of emerging
technologies and pedagogical theories in shaping future standards for AIED.
Finally, we outline a strategic roadmap for stakeholders to implement these
standards, fostering a cohesive and ethical AIED ecosystem. By establishing
comprehensive industry standards, such as those by IEEE Artificial Intelligence
Standards Committee (AISC) and International Organization for Standardization
(ISO), we can accelerate and scale AIED solutions to improve educational
outcomes, ensuring that technological advances align with the principles of
inclusivity, fairness, and educational excellence.</div><div><a href='http://arxiv.org/abs/2403.14689v1'>2403.14689v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14601v2")'>Bringing Generative AI to Adaptive Learning in Education</div>
<div id='2402.14601v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T23:54:51Z</div><div>Authors: Hang Li, Tianlong Xu, Chaoli Zhang, Eason Chen, Jing Liang, Xing Fan, Haoyang Li, Jiliang Tang, Qingsong Wen</div><div style='padding-top: 10px; width: 80ex'>The recent surge in generative AI technologies, such as large language models
and diffusion models, have boosted the development of AI applications in
various domains, including science, finance, and education. Concurrently,
adaptive learning, a concept that has gained substantial interest in the
educational sphere, has proven its efficacy in enhancing students' learning
efficiency. In this position paper, we aim to shed light on the intersectional
studies of these two methods, which combine generative AI with adaptive
learning concepts. By presenting discussions about the benefits, challenges,
and potentials in this field, we argue that this union will contribute
significantly to the development of the next stage learning format in
education.</div><div><a href='http://arxiv.org/abs/2402.14601v2'>2402.14601v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14642v1")'>Revolutionising Distance Learning: A Comparative Study of Learning
  Progress with AI-Driven Tutoring</div>
<div id='2403.14642v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:15:58Z</div><div>Authors: Moritz Möller, Gargi Nirmal, Dario Fabietti, Quintus Stierstorfer, Mark Zakhvatkin, Holger Sommerfeld, Sven Schütt</div><div style='padding-top: 10px; width: 80ex'>Generative AI is expected to have a vast, positive impact on education;
however, at present, this potential has not yet been demonstrated at scale at
university level. In this study, we present first evidence that generative AI
can increase the speed of learning substantially in university students. We
tested whether using the AI-powered teaching assistant Syntea affected the
speed of learning of hundreds of distance learning students across more than 40
courses at the IU International University of Applied Sciences. Our analysis
suggests that using Syntea reduced their study time substantially--by about
27\% on average--in the third month after the release of Syntea. Taken
together, the magnitude of the effect and the scalability of the approach
implicate generative AI as a key lever to significantly improve and accelerate
learning by personalisation.</div><div><a href='http://arxiv.org/abs/2403.14642v1'>2403.14642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01669v1")'>Improved Performances and Motivation in Intelligent Tutoring Systems:
  Combining Machine Learning and Learner Choice</div>
<div id='2402.01669v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T13:41:00Z</div><div>Authors: Benjamin Clément, Hélène Sauzéon, Didier Roy, Pierre-Yves Oudeyer</div><div style='padding-top: 10px; width: 80ex'>Large class sizes pose challenges to personalized learning in schools, which
educational technologies, especially intelligent tutoring systems (ITS), aim to
address. In this context, the ZPDES algorithm, based on the Learning Progress
Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences
exercises that maximize learning progress (LP). This algorithm was previously
shown in field studies to boost learning performances for a wider diversity of
students compared to a hand-designed curriculum. However, its motivational
impact was not assessed. Also, ZPDES did not allow students to express choices.
This limitation in agency is at odds with the LPH theory concerned with
modeling curiosity-driven learning. We here study how the introduction of such
choice possibilities impact both learning efficiency and motivation. The given
choice concerns dimensions that are orthogonal to exercise difficulty, acting
as a playful feature.
  In an extensive field study (265 7-8 years old children, RCT design), we
compare systems based either on ZPDES or a hand-designed curriculum, both with
and without self-choice. We first show that ZPDES improves learning performance
and produces a positive and motivating learning experience. We then show that
the addition of choice triggers intrinsic motivation and reinforces the
learning effectiveness of the LP-based personalization. In doing so, it
strengthens the links between intrinsic motivation and performance progress
during the serious game. Conversely, deleterious effects of the playful feature
are observed for hand-designed linear paths. Thus, the intrinsic motivation
elicited by a playful feature is beneficial only if the curriculum
personalization is effective for the learner. Such a result deserves great
attention due to increased use of playful features in non adaptive educational
technologies.</div><div><a href='http://arxiv.org/abs/2402.01669v1'>2402.01669v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12068v1")'>Process mining for self-regulated learning assessment in e-learning</div>
<div id='2403.12068v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T11:51:32Z</div><div>Authors: R. Cerezo, A. Bogarin, M. Esteban, C. Romero</div><div style='padding-top: 10px; width: 80ex'>Content assessment has broadly improved in e-learning scenarios in recent
decades. However, the eLearning process can give rise to a spatial and temporal
gap that poses interesting challenges for assessment of not only content, but
also students' acquisition of core skills such as self-regulated learning. Our
objective was to discover students' self-regulated learning processes during an
eLearning course by using Process Mining Techniques. We applied a new algorithm
in the educational domain called Inductive Miner over the interaction traces
from 101 university students in a course given over one semester on the Moodle
2.0 platform. Data was extracted from the platform's event logs with 21629
traces in order to discover students' self-regulation models that contribute to
improving the instructional process. The Inductive Miner algorithm discovered
optimal models in terms of fitness for both Pass and Fail students in this
dataset, as well as models at a certain level of granularity that can be
interpreted in educational terms, which are the most important achievement in
model discovery. We can conclude that although students who passed did not
follow the instructors' suggestions exactly, they did follow the logic of a
successful self-regulated learning process as opposed to their failing
classmates. The Process Mining models also allow us to examine which specific
actions the students performed, and it was particularly interesting to see a
high presence of actions related to forum-supported collaborative learning in
the Pass group and an absence of those in the Fail group.</div><div><a href='http://arxiv.org/abs/2403.12068v1'>2403.12068v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05556v1")'>Modeling and predicting students' engagement behaviors using mixture
  Markov models</div>
<div id='2403.05556v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T19:03:06Z</div><div>Authors: R. Maqsood, P. Ceravolo, C. Romero, S. Ventura</div><div style='padding-top: 10px; width: 80ex'>Students' engagements reflect their level of involvement in an ongoing
learning process which can be estimated through their interactions with a
computer-based learning or assessment system. A pre-requirement for stimulating
student engagement lies in the capability to have an approximate representation
model for comprehending students' varied (dis)engagement behaviors. In this
paper, we utilized model-based clustering for this purpose which generates K
mixture Markov models to group students' traces containing their
(dis)engagement behavioral patterns. To prevent the Expectation-Maximization
(EM) algorithm from getting stuck in a local maxima, we also introduced a
K-means-based initialization method named as K-EM. We performed an experimental
work on two real datasets using the three variants of the EM algorithm: the
original EM, emEM, K-EM; and, non-mixture baseline models for both datasets.
The proposed K-EM has shown very promising results and achieved significant
performance difference in comparison with the other approaches particularly
using the Dataset. Hence, we suggest to perform further experiments using large
dataset(s) to validate our method. Additionally, visualization of the resultant
clusters through first-order Markov chains reveals very useful insights about
(dis)engagement behaviors depicted by the students. We conclude the paper with
a discussion on the usefulness of our approach, limitations and potential
extensions of this work.</div><div><a href='http://arxiv.org/abs/2403.05556v1'>2403.05556v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08371v1")'>Helping university students to choose elective courses by using a hybrid
  multi-criteria recommendation system with genetic optimization</div>
<div id='2402.08371v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T11:02:12Z</div><div>Authors: A. Esteban, A. Zafra, C. Romero</div><div style='padding-top: 10px; width: 80ex'>The wide availability of specific courses together with the flexibility of
academic plans in university studies reveal the importance of Recommendation
Systems (RSs) in this area. These systems appear as tools that help students to
choose courses that suit to their personal interests and their academic
performance. This paper presents a hybrid RS that combines Collaborative
Filtering (CF) and Content-based Filtering (CBF) using multiple criteria
related both to student and course information to recommend the most suitable
courses to the students. A Genetic Algorithm (GA) has been developed to
automatically discover the optimal RS configuration which include both the most
relevant criteria and the configuration of the rest of parameters. The
experimental study has used real information of Computer Science Degree of
University of Cordoba (Spain) including information gathered from students
during three academic years, counting on 2500 entries of 95 students and 63
courses. Experimental results show a study of the most relevant criteria for
the course recommendation, the importance of using a hybrid model that combines
both student information and course information to increase the reliability of
the recommendations as well as an excellent performance compared to previous
models.</div><div><a href='http://arxiv.org/abs/2402.08371v1'>2402.08371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13646v1")'>A Large Dimensional Analysis of Multi-task Semi-Supervised Learning</div>
<div id='2402.13646v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:27:44Z</div><div>Authors: Victor Leger, Romain Couillet</div><div style='padding-top: 10px; width: 80ex'>This article conducts a large dimensional study of a simple yet quite
versatile classification model, encompassing at once multi-task and
semi-supervised learning, and taking into account uncertain labeling. Using
tools from random matrix theory, we characterize the asymptotics of some key
functionals, which allows us on the one hand to predict the performances of the
algorithm, and on the other hand to reveal some counter-intuitive guidance on
how to use it efficiently. The model, powerful enough to provide good
performance guarantees, is also straightforward enough to provide strong
insights into its behavior.</div><div><a href='http://arxiv.org/abs/2402.13646v1'>2402.13646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00645v1")'>Spectrally Transformed Kernel Regression</div>
<div id='2402.00645v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:07:31Z</div><div>Authors: Runtian Zhai, Rattana Pukdee, Roger Jin, Maria-Florina Balcan, Pradeep Ravikumar</div><div style='padding-top: 10px; width: 80ex'>Unlabeled data is a key component of modern machine learning. In general, the
role of unlabeled data is to impose a form of smoothness, usually from the
similarity information encoded in a base kernel, such as the
$\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work
revisits the classical idea of spectrally transformed kernel regression (STKR),
and provides a new class of general and scalable STKR estimators able to
leverage unlabeled data. Intuitively, via spectral transformation, STKR
exploits the data distribution for which unlabeled data can provide additional
information. First, we show that STKR is a principled and general approach, by
characterizing a universal type of "target smoothness", and proving that any
sufficiently smooth function can be learned by STKR. Second, we provide
scalable STKR implementations for the inductive setting and a general
transformation function, while prior work is mostly limited to the transductive
setting. Third, we derive statistical guarantees for two scenarios: STKR with a
known polynomial transformation, and STKR with kernel PCA when the
transformation is unknown. Overall, we believe that this work helps deepen our
understanding of how to work with unlabeled data, and its generality makes it
easier to inspire new methods.</div><div><a href='http://arxiv.org/abs/2402.00645v1'>2402.00645v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12478v1")'>Mini-batch Submodular Maximization</div>
<div id='2401.12478v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T04:16:58Z</div><div>Authors: Gregory Schwartzman</div><div style='padding-top: 10px; width: 80ex'>We present the first mini-batch algorithm for maximizing a non-negative
monotone decomposable submodular function, $F=\sum_{i=1}^N f^i$, under a set of
constraints. We improve over the sparsifier based approach both in theory and
in practice. We experimentally observe that our algorithm generates solutions
that are far superior to those generated by the sparsifier based approach.</div><div><a href='http://arxiv.org/abs/2401.12478v1'>2401.12478v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16442v1")'>On Distributed Larger-Than-Memory Subset Selection With Pairwise
  Submodular Functions</div>
<div id='2402.16442v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T09:38:39Z</div><div>Authors: Maximilian Böther, Abraham Sebastian, Pranjal Awasthi, Ana Klimovic, Srikumar Ramalingam</div><div style='padding-top: 10px; width: 80ex'>Many learning problems hinge on the fundamental problem of subset selection,
i.e., identifying a subset of important and representative points. For example,
selecting the most significant samples in ML training cannot only reduce
training costs but also enhance model quality. Submodularity, a discrete
analogue of convexity, is commonly used for solving subset selection problems.
However, existing algorithms for optimizing submodular functions are
sequential, and the prior distributed methods require at least one central
machine to fit the target subset. In this paper, we relax the requirement of
having a central machine for the target subset by proposing a novel distributed
bounding algorithm with provable approximation guarantees. The algorithm
iteratively bounds the minimum and maximum utility values to select high
quality points and discard the unimportant ones. When bounding does not find
the complete subset, we use a multi-round, partition-based distributed greedy
algorithm to identify the remaining subset. We show that these algorithms find
high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in
quality compared to centralized methods, and scale to a dataset with 13 billion
points.</div><div><a href='http://arxiv.org/abs/2402.16442v1'>2402.16442v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13857v1")'>Replicable Learning of Large-Margin Halfspaces</div>
<div id='2402.13857v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T15:06:51Z</div><div>Authors: Alkis Kalavasis, Amin Karbasi, Kasper Green Larsen, Grigoris Velegkas, Felix Zhou</div><div style='padding-top: 10px; width: 80ex'>We provide efficient replicable algorithms for the problem of learning
large-margin halfspaces. Our results improve upon the algorithms provided by
Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first
dimension-independent replicable algorithms for this task which runs in
polynomial time, is proper, and has strictly improved sample complexity
compared to the one achieved by Impagliazzo et al. [2022] with respect to all
the relevant parameters. Moreover, our first algorithm has sample complexity
that is optimal with respect to the accuracy parameter $\epsilon$. We also
design an SGD-based replicable algorithm that, in some parameters' regimes,
achieves better sample and time complexity than our first algorithm.
  Departing from the requirement of polynomial time algorithms, using the
DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei,
Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a
replicable algorithm for large-margin halfspaces with improved sample
complexity with respect to the margin parameter $\tau$, but running time doubly
exponential in $1/\tau^2$ and worse sample complexity dependence on $\epsilon$
than one of our previous algorithms. We then design an improved algorithm with
better sample complexity than all three of our previous algorithms and running
time exponential in $1/\tau^{2}$.</div><div><a href='http://arxiv.org/abs/2402.13857v1'>2402.13857v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13454v1")'>Theoretical Analysis of Submodular Information Measures for Targeted
  Data Subset Selection</div>
<div id='2402.13454v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T01:18:32Z</div><div>Authors: Nathan Beck, Truong Pham, Rishabh Iyer</div><div style='padding-top: 10px; width: 80ex'>With increasing volume of data being used across machine learning tasks, the
capability to target specific subsets of data becomes more important. To aid in
this capability, the recently proposed Submodular Mutual Information (SMI) has
been effectively applied across numerous tasks in literature to perform
targeted subset selection with the aid of a exemplar query set. However, all
such works are deficient in providing theoretical guarantees for SMI in terms
of its sensitivity to a subset's relevance and coverage of the targeted data.
For the first time, we provide such guarantees by deriving similarity-based
bounds on quantities related to relevance and coverage of the targeted data.
With these bounds, we show that the SMI functions, which have empirically shown
success in multiple applications, are theoretically sound in achieving good
query relevance and query coverage.</div><div><a href='http://arxiv.org/abs/2402.13454v1'>2402.13454v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10332v1")'>GreedyML: A Parallel Algorithm for Maximizing Submodular Functions</div>
<div id='2403.10332v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T14:19:09Z</div><div>Authors: Shivaram Gopal, S M Ferdous, Hemanta K. Maji, Alex Pothen</div><div style='padding-top: 10px; width: 80ex'>We describe a parallel approximation algorithm for maximizing monotone
submodular functions subject to hereditary constraints on distributed memory
multiprocessors. Our work is motivated by the need to solve submodular
optimization problems on massive data sets, for practical applications in areas
such as data summarization, machine learning, and graph sparsification. Our
work builds on the randomized distributed RandGreedI algorithm, proposed by
Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed
solution by randomly partitioning the data among all the processors and then
employing a single accumulation step in which all processors send their partial
solutions to one processor. However, for large problems, the accumulation step
could exceed the memory available on a processor, and the processor which
performs the accumulation could become a computational bottleneck.
  Here, we propose a generalization of the RandGreedI algorithm that employs
multiple accumulation steps to reduce the memory required. We analyze the
approximation ratio and the time complexity of the algorithm (in the BSP
model). We also evaluate the new GreedyML algorithm on three classes of
problems, and report results from massive data sets with millions of elements.
The results show that the GreedyML algorithm can solve problems where the
sequential Greedy and distributed RandGreedI algorithms fail due to memory
constraints. For certain computationally intensive problems, the GreedyML
algorithm can be faster than the RandGreedI algorithm. The observed
approximation quality of the solutions computed by the GreedyML algorithm
closely matches those obtained by the RandGreedI algorithm on these problems.</div><div><a href='http://arxiv.org/abs/2403.10332v1'>2403.10332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01280v1")'>GEqO: ML-Accelerated Semantic Equivalence Detection</div>
<div id='2401.01280v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T16:37:42Z</div><div>Authors: Brandon Haynes, Rana Alotaibi, Anna Pavlenko, Jyoti Leeka, Alekh Jindal, Yuanyuan Tian</div><div style='padding-top: 10px; width: 80ex'>Large scale analytics engines have become a core dependency for modern
data-driven enterprises to derive business insights and drive actions. These
engines support a large number of analytic jobs processing huge volumes of data
on a daily basis, and workloads are often inundated with overlapping
computations across multiple jobs. Reusing common computation is crucial for
efficient cluster resource utilization and reducing job execution time.
Detecting common computation is the first and key step for reducing this
computational redundancy. However, detecting equivalence on large-scale
analytics engines requires efficient and scalable solutions that are fully
automated. In addition, to maximize computation reuse, equivalence needs to be
detected at the semantic level instead of just the syntactic level (i.e., the
ability to detect semantic equivalence of seemingly different-looking queries).
Unfortunately, existing solutions fall short of satisfying these requirements.
  In this paper, we take a major step towards filling this gap by proposing
GEqO, a portable and lightweight machine-learning-based framework for
efficiently identifying semantically equivalent computations at scale. GEqO
introduces two machine-learning-based filters that quickly prune out
nonequivalent subexpressions and employs a semi-supervised learning feedback
loop to iteratively improve its model with an intelligent sampling mechanism.
Further, with its novel database-agnostic featurization method, GEqO can
transfer the learning from one workload and database to another. Our extensive
empirical evaluation shows that, on TPC-DS-like queries, GEqO yields
significant performance gains-up to 200x faster than automated verifiers-and
finds up to 2x more equivalences than optimizer and signature-based equivalence
detection approaches.</div><div><a href='http://arxiv.org/abs/2401.01280v1'>2401.01280v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05821v1")'>Optimizing LLM Queries in Relational Workloads</div>
<div id='2403.05821v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T07:01:44Z</div><div>Authors: Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph E. Gonzalez, Ion Stoica, Matei Zaharia</div><div style='padding-top: 10px; width: 80ex'>Analytical database providers (e.g., Redshift, Databricks, BigQuery) have
rapidly added support for invoking Large Language Models (LLMs) through native
user-defined functions (UDFs) to help users perform natural language tasks,
such as classification, entity extraction, and translation, inside analytical
workloads. For instance, an analyst might want to extract customer sentiments
on millions of product reviews. However, LLM inference is highly expensive in
both computational and economic terms: for example, an NVIDIA L4 GPU running
Llama2-7B can only process 6 KB of text per second. In this paper, we explore
how to optimize LLM inference for analytical workloads that invoke LLMs within
relational queries. We show that relational queries present novel opportunities
for accelerating LLM inference, including reordering rows to maximize key-value
(KV) cache reuse within the LLM inference engine, reordering columns within a
row to further increase cache reuse, and deduplicating redundant inference
requests. We implement these optimizations in Apache Spark, with vLLM as the
model serving backend and achieve up to 4.4x improvement in end-to-end latency
on a benchmark of diverse LLM-based queries on real datasets. To the best of
our knowledge, this is the first work to explicitly address the problem of
optimizing LLM invocations within SQL queries.</div><div><a href='http://arxiv.org/abs/2403.05821v1'>2403.05821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11472v1")'>Accelerating String-Key Learned Index Structures via Memoization-based
  Incremental Training</div>
<div id='2403.11472v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T04:44:00Z</div><div>Authors: Minsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan, Jongse Park</div><div style='padding-top: 10px; width: 80ex'>Learned indexes use machine learning models to learn the mappings between
keys and their corresponding positions in key-value indexes. These indexes use
the mapping information as training data. Learned indexes require frequent
retrainings of their models to incorporate the changes introduced by update
queries. To efficiently retrain the models, existing learned index systems
often harness a linear algebraic QR factorization technique that performs
matrix decomposition. This factorization approach processes all key-position
pairs during each retraining, resulting in compute operations that grow
linearly with the total number of keys and their lengths. Consequently, the
retrainings create a severe performance bottleneck, especially for
variable-length string keys, while the retrainings are crucial for maintaining
high prediction accuracy and in turn, ensuring low query service latency.
  To address this performance problem, we develop an algorithm-hardware
co-designed string-key learned index system, dubbed SIA. In designing SIA, we
leverage a unique algorithmic property of the matrix decomposition-based
training method. Exploiting the property, we develop a memoization-based
incremental training scheme, which only requires computation over updated keys,
while decomposition results of non-updated keys from previous computations can
be reused. We further enhance SIA to offload a portion of this training process
to an FPGA accelerator to not only relieve CPU resources for serving index
queries (i.e., inference), but also accelerate the training itself. Our
evaluation shows that compared to ALEX, LIPP, and SIndex, a state-of-the-art
learned index systems, SIA-accelerated learned indexes offer 2.6x and 3.4x
higher throughput on the two real-world benchmark suites, YCSB and Twitter
cache trace, respectively.</div><div><a href='http://arxiv.org/abs/2403.11472v1'>2403.11472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06311v1")'>How much data do you need? Part 2: Predicting DL class specific training
  dataset sizes</div>
<div id='2403.06311v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T21:08:29Z</div><div>Authors: Thomas Mühlenstädt, Jelena Frtunikj</div><div style='padding-top: 10px; width: 80ex'>This paper targets the question of predicting machine learning classification
model performance, when taking into account the number of training examples per
class and not just the overall number of training examples. This leads to the a
combinatorial question, which combinations of number of training examples per
class should be considered, given a fixed overall training dataset size. In
order to solve this question, an algorithm is suggested which is motivated from
special cases of space filling design of experiments. The resulting data are
modeled using models like powerlaw curves and similar models, extended like
generalized linear models i.e. by replacing the overall training dataset size
by a parametrized linear combination of the number of training examples per
label class. The proposed algorithm has been applied on the CIFAR10 and the
EMNIST datasets.</div><div><a href='http://arxiv.org/abs/2403.06311v1'>2403.06311v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00542v1")'>Machine Learning Training Optimization using the Barycentric Correction
  Procedure</div>
<div id='2403.00542v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:56:36Z</div><div>Authors: Sofia Ramos-Pulido, Neil Hernandez-Gress, Hector G. Ceballos-Cancino</div><div style='padding-top: 10px; width: 80ex'>Machine learning (ML) algorithms are predictively competitive algorithms with
many human-impact applications. However, the issue of long execution time
remains unsolved in the literature for high-dimensional spaces. This study
proposes combining ML algorithms with an efficient methodology known as the
barycentric correction procedure (BCP) to address this issue. This study uses
synthetic data and an educational dataset from a private university to show the
benefits of the proposed method. It was found that this combination provides
significant benefits related to time in synthetic and real data without losing
accuracy when the number of instances and dimensions increases. Additionally,
for high-dimensional spaces, it was proved that BCP and linear support vector
classification (LinearSVC), after an estimated feature map for the gaussian
radial basis function (RBF) kernel, were unfeasible in terms of computational
time and accuracy.</div><div><a href='http://arxiv.org/abs/2403.00542v1'>2403.00542v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15108v1")'>Active Learning for Regression based on Wasserstein distance and
  GroupSort Neural Networks</div>
<div id='2403.15108v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T10:51:55Z</div><div>Authors: Benjamin Bobbia, Matthias Picard</div><div style='padding-top: 10px; width: 80ex'>This paper addresses a new active learning strategy for regression problems.
The presented Wasserstein active regression model is based on the principles of
distribution-matching to measure the representativeness of the labeled dataset.
The Wasserstein distance is computed using GroupSort Neural Networks. The use
of such networks provides theoretical foundations giving a way to quantify
errors with explicit bounds for their size and depth. This solution is combined
with another uncertainty-based approach that is more outlier-tolerant to
complete the query strategy. Finally, this method is compared with other
classical and recent solutions. The study empirically shows the pertinence of
such a representativity-uncertainty approach, which provides good estimation
all along the query procedure. Moreover, the Wasserstein active regression
often achieves more precise estimations and tends to improve accuracy faster
than other models.</div><div><a href='http://arxiv.org/abs/2403.15108v1'>2403.15108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06424v1")'>Bridging Domains with Approximately Shared Features</div>
<div id='2403.06424v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T04:25:41Z</div><div>Authors: Ziliang Samuel Zhong, Xiang Pan, Qi Lei</div><div style='padding-top: 10px; width: 80ex'>Multi-source domain adaptation aims to reduce performance degradation when
applying machine learning models to unseen domains. A fundamental challenge is
devising the optimal strategy for feature selection. Existing literature is
somewhat paradoxical: some advocate for learning invariant features from source
domains, while others favor more diverse features. To address the challenge, we
propose a statistical framework that distinguishes the utilities of features
based on the variance of their correlation to label $y$ across domains. Under
our framework, we design and analyze a learning procedure consisting of
learning approximately shared feature representation from source tasks and
fine-tuning it on the target task. Our theoretical analysis necessitates the
importance of learning approximately shared features instead of only the
strictly invariant features and yields an improved population risk compared to
previous results on both source and target tasks, thus partly resolving the
paradox mentioned above. Inspired by our theory, we proposed a more practical
way to isolate the content (invariant+approximately shared) from environmental
features and further consolidate our theoretical findings.</div><div><a href='http://arxiv.org/abs/2403.06424v1'>2403.06424v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05209v1")'>Overcoming Data Inequality across Domains with Semi-Supervised Domain
  Generalization</div>
<div id='2403.05209v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T10:49:37Z</div><div>Authors: Jinha Park, Wonguk Cho, Taesup Kim</div><div style='padding-top: 10px; width: 80ex'>While there have been considerable advancements in machine learning driven by
extensive datasets, a significant disparity still persists in the availability
of data across various sources and populations. This inequality across domains
poses challenges in modeling for those with limited data, which can lead to
profound practical and ethical concerns. In this paper, we address a
representative case of data inequality problem across domains termed
Semi-Supervised Domain Generalization (SSDG), in which only one domain is
labeled while the rest are unlabeled. We propose a novel algorithm, ProUD,
which can effectively learn domain-invariant features via domain-aware
prototypes along with progressive generalization via uncertainty-adaptive
mixing of labeled and unlabeled domains. Our experiments on three different
benchmark datasets demonstrate the effectiveness of ProUD, outperforming all
baseline models including single domain generalization and semi-supervised
learning. Source code will be released upon acceptance of the paper.</div><div><a href='http://arxiv.org/abs/2403.05209v1'>2403.05209v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06174v1")'>Domain Adversarial Active Learning for Domain Generalization
  Classification</div>
<div id='2403.06174v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T10:59:22Z</div><div>Authors: Jianting Chen, Ling Ding, Yunxiao Yang, Zaiyuan Di, Yang Xiang</div><div style='padding-top: 10px; width: 80ex'>Domain generalization models aim to learn cross-domain knowledge from source
domain data, to improve performance on unknown target domains. Recent research
has demonstrated that diverse and rich source domain samples can enhance domain
generalization capability. This paper argues that the impact of each sample on
the model's generalization ability varies. Despite its small scale, a
high-quality dataset can still attain a certain level of generalization
ability. Motivated by this, we propose a domain-adversarial active learning
(DAAL) algorithm for classification tasks in domain generalization. First, we
analyze that the objective of tasks is to maximize the inter-class distance
within the same domain and minimize the intra-class distance across different
domains. To achieve this objective, we design a domain adversarial selection
method that prioritizes challenging samples. Second, we posit that even in a
converged model, there are subsets of features that lack discriminatory power
within each domain. We attempt to identify these feature subsets and optimize
them by a constraint loss. We validate and analyze our DAAL algorithm on
multiple domain generalization datasets, comparing it with various domain
generalization algorithms and active learning algorithms. Our results
demonstrate that the DAAL algorithm can achieve strong generalization ability
with fewer data resources, thereby reducing data annotation costs in domain
generalization tasks.</div><div><a href='http://arxiv.org/abs/2403.06174v1'>2403.06174v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03170v1")'>Preserving Silent Features for Domain Generalization</div>
<div id='2401.03170v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T09:11:41Z</div><div>Authors: Chujie Zhao, Tianren Zhang, Feng Chen</div><div style='padding-top: 10px; width: 80ex'>Domain generalization (DG) aims to improve the generalization ability of the
model trained on several known training domains over unseen test domains.
Previous work has shown that self-supervised contrastive pre-training improves
the robustness of the model on downstream tasks. However, in this paper, we
find that self-supervised models do not exhibit better generalization
performance than supervised models pre-trained on the same dataset in the DG
setting. We argue that this is owing to the fact that the richer intra-class
discriminative features extracted by self-supervised contrastive learning,
which we term silent features, are suppressed during supervised fine-tuning.
These silent features are likely to contain features that are more
generalizable on the test domain. In this work, we model and analyze this
feature suppression phenomenon and theoretically prove that preserving silent
features can achieve lower expected test domain risk under certain conditions.
In light of this, we propose a simple yet effective method termed STEP (Silent
Feature Preservation) to improve the generalization performance of the
self-supervised contrastive learning pre-trained model by alleviating the
suppression of silent features during the supervised fine-tuning process.
Experimental results show that STEP exhibits state-of-the-art performance on
standard DG benchmarks with significant distribution shifts.</div><div><a href='http://arxiv.org/abs/2401.03170v1'>2401.03170v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07785v2")'>HYPO: Hyperspherical Out-of-Distribution Generalization</div>
<div id='2402.07785v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:50:07Z</div><div>Authors: Yifei Ming, Haoyue Bai, Julian Katz-Samuels, Yixuan Li</div><div style='padding-top: 10px; width: 80ex'>Out-of-distribution (OOD) generalization is critical for machine learning
models deployed in the real world. However, achieving this can be fundamentally
challenging, as it requires the ability to learn invariant features across
different domains or environments. In this paper, we propose a novel framework
HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant
representations in a hyperspherical space. In particular, our hyperspherical
learning algorithm is guided by intra-class variation and inter-class
separation principles -- ensuring that features from the same class (across
different training domains) are closely aligned with their class prototypes,
while different class prototypes are maximally separated. We further provide
theoretical justifications on how our prototypical learning objective improves
the OOD generalization bound. Through extensive experiments on challenging OOD
benchmarks, we demonstrate that our approach outperforms competitive baselines
and achieves superior performance. Code is available at
https://github.com/deeplearning-wisc/hypo.</div><div><a href='http://arxiv.org/abs/2402.07785v2'>2402.07785v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03569v1")'>On Transfer in Classification: How Well do Subsets of Classes
  Generalize?</div>
<div id='2403.03569v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T09:25:22Z</div><div>Authors: Raphael Baena, Lucas Drumetz, Vincent Gripon</div><div style='padding-top: 10px; width: 80ex'>In classification, it is usual to observe that models trained on a given set
of classes can generalize to previously unseen ones, suggesting the ability to
learn beyond the initial task. This ability is often leveraged in the context
of transfer learning where a pretrained model can be used to process new
classes, with or without fine tuning. Surprisingly, there are a few papers
looking at the theoretical roots beyond this phenomenon. In this work, we are
interested in laying the foundations of such a theoretical framework for
transferability between sets of classes. Namely, we establish a partially
ordered set of subsets of classes. This tool allows to represent which subset
of classes can generalize to others. In a more practical setting, we explore
the ability of our framework to predict which subset of classes can lead to the
best performance when testing on all of them. We also explore few-shot
learning, where transfer is the golden standard. Our work contributes to better
understanding of transfer mechanics and model generalization.</div><div><a href='http://arxiv.org/abs/2403.03569v1'>2403.03569v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11742v1")'>Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with
  Spectral Imbalance</div>
<div id='2402.11742v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T23:59:54Z</div><div>Authors: Chiraag Kaushik, Ran Liu, Chi-Heng Lin, Amrit Khera, Matthew Y Jin, Wenrui Ma, Vidya Muthukumar, Eva L Dyer</div><div style='padding-top: 10px; width: 80ex'>Classification models are expected to perform equally well for different
classes, yet in practice, there are often large gaps in their performance. This
issue of class bias is widely studied in cases of datasets with sample
imbalance, but is relatively overlooked in balanced datasets. In this work, we
introduce the concept of spectral imbalance in features as a potential source
for class disparities and study the connections between spectral imbalance and
class bias in both theory and practice. To build the connection between
spectral imbalance and class gap, we develop a theoretical framework for
studying class disparities and derive exact expressions for the per-class error
in a high-dimensional mixture model setting. We then study this phenomenon in
11 different state-of-the-art pretrained encoders and show how our proposed
framework can be used to compare the quality of encoders, as well as evaluate
and combine data augmentation strategies to mitigate the issue. Our work sheds
light on the class-dependent effects of learning, and provides new insights
into how state-of-the-art pretrained features may have unknown biases that can
be diagnosed through their spectra.</div><div><a href='http://arxiv.org/abs/2402.11742v1'>2402.11742v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14696v1")'>Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening</div>
<div id='2401.14696v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T07:36:57Z</div><div>Authors: Hoyong Kim, Semi Lee, Kangil Kim</div><div style='padding-top: 10px; width: 80ex'>In the feature space, the collapse between features invokes critical problems
in representation learning by remaining the features undistinguished.
Interpolation-based augmentation methods such as mixup have shown their
effectiveness in relieving the collapse problem between different classes,
called inter-class collapse. However, intra-class collapse raised in
coarse-to-fine transfer learning has not been discussed in the augmentation
approach. To address them, we propose a better feature augmentation method,
asymptotic midpoint mixup. The method generates augmented features by
interpolation but gradually moves them toward the midpoint of inter-class
feature pairs. As a result, the method induces two effects: 1) balancing the
margin for all classes and 2) only moderately broadening the margin until it
holds maximal confidence. We empirically analyze the collapse effects by
measuring alignment and uniformity with visualizing representations. Then, we
validate the intra-class collapse effects in coarse-to-fine transfer learning
and the inter-class collapse effects in imbalanced learning on long-tailed
datasets. In both tasks, our method shows better performance than other
augmentation methods.</div><div><a href='http://arxiv.org/abs/2401.14696v1'>2401.14696v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14137v1")'>Improving Image Classification Accuracy through Complementary
  Intra-Class and Inter-Class Mixup</div>
<div id='2403.14137v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T05:13:12Z</div><div>Authors: Ye Xu, Ya Gao, Xiaorong Qiu, Yang Chen, Ying Ji</div><div style='padding-top: 10px; width: 80ex'>MixUp and its variants, such as Manifold MixUp, have two key limitations in
image classification tasks. First, they often neglect mixing within the same
class (intra-class mixup), leading to an underutilization of the relationships
among samples within the same class. Second, although these methods effectively
enhance inter-class separability by mixing between different classes
(inter-class mixup), they fall short in improving intra-class cohesion through
their mixing operations, limiting their classification performance. To tackle
these issues, we propose a novel mixup method and a comprehensive integrated
solution.Our mixup approach specifically targets intra-class mixup, an aspect
commonly overlooked, to strengthen intra-class cohesion-a feature not provided
by current mixup techniques.For each mini-batch, our method utilizes feature
representations of unaugmented original images from each class within the
mini-batch to generate a single synthesized feature representation through
random linear interpolation. All synthesized representations for this
mini-batch are then fed into the classification and loss layers to calculate an
average classification loss that can markedly enhance intra-class cohesion.
Moreover, our integrated solution seamlessly combines our intra-class mixup
method with an existing mixup approach such as MixUp or Manifold MixUp. This
comprehensive solution incorporates inter- and intra-class mixup in a balanced
manner while concurrently improving intra-class cohesion and inter-class
separability. Experimental results on six public datasets demonstrate that our
integrated solution achieves a 0.1% to 3.43% higher accuracy than the best of
either MixUp or our intra-class mixup method, averaging a 1.16% gain. It also
outperforms the better performer of either Manifold MixUp or our intra-class
mixup method by 0.12% to 5.16%, with an average gain of 1.11%.</div><div><a href='http://arxiv.org/abs/2403.14137v1'>2403.14137v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07329v1")'>Unknown Domain Inconsistency Minimization for Domain Generalization</div>
<div id='2403.07329v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T05:29:48Z</div><div>Authors: Seungjae Shin, HeeSun Bae, Byeonghu Na, Yoon-Yeong Kim, Il-Chul Moon</div><div style='padding-top: 10px; width: 80ex'>The objective of domain generalization (DG) is to enhance the transferability
of the model learned from a source domain to unobserved domains. To prevent
overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces
source domain's loss sharpness. Although SAM variants have delivered
significant improvements in DG, we highlight that there's still potential for
improvement in generalizing to unknown domains through the exploration on data
space. This paper introduces an objective rooted in both parameter and data
perturbed regions for domain generalization, coined Unknown Domain
Inconsistency Minimization (UDIM). UDIM reduces the loss landscape
inconsistency between source domain and unknown domains. As unknown domains are
inaccessible, these domains are empirically crafted by perturbing instances
from the source domain dataset. In particular, by aligning the loss landscape
acquired in the source domain to the loss landscape of perturbed domains, we
expect to achieve generalization grounded on these flat minima for the unknown
domains. Theoretically, we validate that merging SAM optimization with the UDIM
objective establishes an upper bound for the true objective of the DG task. In
an empirical aspect, UDIM consistently outperforms SAM variants across multiple
DG benchmark datasets. Notably, UDIM shows statistically significant
improvements in scenarios with more restrictive domain information,
underscoring UDIM's generalization capability in unseen domains. Our code is
available at \url{https://github.com/SJShin-AI/UDIM}.</div><div><a href='http://arxiv.org/abs/2403.07329v1'>2403.07329v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14846v2")'>Understanding Domain Generalization: A Noise Robustness Perspective</div>
<div id='2401.14846v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T13:27:15Z</div><div>Authors: Rui Qiao, Bryan Kian Hsiang Low</div><div style='padding-top: 10px; width: 80ex'>Despite the rapid development of machine learning algorithms for domain
generalization (DG), there is no clear empirical evidence that the existing DG
algorithms outperform the classic empirical risk minimization (ERM) across
standard benchmarks. To better understand this phenomenon, we investigate
whether there are benefits of DG algorithms over ERM through the lens of label
noise. Specifically, our finite-sample analysis reveals that label noise
exacerbates the effect of spurious correlations for ERM, undermining
generalization. Conversely, we illustrate that DG algorithms exhibit implicit
label-noise robustness during finite-sample training even when spurious
correlation is present. Such desirable property helps mitigate spurious
correlations and improve generalization in synthetic experiments. However,
additional comprehensive experiments on real-world benchmark datasets indicate
that label-noise robustness does not necessarily translate to better
performance compared to ERM. We conjecture that the failure mode of ERM arising
from spurious correlations may be less pronounced in practice.</div><div><a href='http://arxiv.org/abs/2401.14846v2'>2401.14846v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17541v1")'>Towards Understanding Variants of Invariant Risk Minimization through
  the Lens of Calibration</div>
<div id='2401.17541v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T02:08:43Z</div><div>Authors: Kotaro Yoshida, Hiroki Naganuma</div><div style='padding-top: 10px; width: 80ex'>Machine learning models traditionally assume that training and test data are
independently and identically distributed. However, in real-world applications,
the test distribution often differs from training. This problem, known as
out-of-distribution generalization, challenges conventional models. Invariant
Risk Minimization (IRM) emerges as a solution, aiming to identify features
invariant across different environments to enhance out-of-distribution
robustness. However, IRM's complexity, particularly its bi-level optimization,
has led to the development of various approximate methods. Our study
investigates these approximate IRM techniques, employing the Expected
Calibration Error (ECE) as a key metric. ECE, which measures the reliability of
model prediction, serves as an indicator of whether models effectively capture
environment-invariant features. Through a comparative analysis of datasets with
distributional shifts, we observe that Information Bottleneck-based IRM, which
condenses representational information, achieves a balance in improving ECE
while preserving accuracy relatively. This finding is pivotal, as it
demonstrates a feasible path to maintaining robustness without compromising
accuracy. Nonetheless, our experiments also caution against
over-regularization, which can diminish accuracy. This underscores the
necessity for a systematic approach in evaluating out-of-distribution
generalization metrics, one that beyond mere accuracy to address the nuanced
interplay between accuracy and calibration.</div><div><a href='http://arxiv.org/abs/2401.17541v1'>2401.17541v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14027v1")'>The Risk of Federated Learning to Skew Fine-Tuning Features and
  Underperform Out-of-Distribution Robustness</div>
<div id='2401.14027v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T09:18:51Z</div><div>Authors: Mengyao Du, Miao Zhang, Yuwen Pu, Kai Xu, Shouling Ji, Quanjun Yin</div><div style='padding-top: 10px; width: 80ex'>To tackle the scarcity and privacy issues associated with domain-specific
datasets, the integration of federated learning in conjunction with fine-tuning
has emerged as a practical solution. However, our findings reveal that
federated learning has the risk of skewing fine-tuning features and
compromising the out-of-distribution robustness of the model. By introducing
three robustness indicators and conducting experiments across diverse robust
datasets, we elucidate these phenomena by scrutinizing the diversity,
transferability, and deviation within the model feature space. To mitigate the
negative impact of federated learning on model robustness, we introduce GNP, a
\underline{G}eneral \underline{N}oisy \underline{P}rojection-based robust
algorithm, ensuring no deterioration of accuracy on the target distribution.
Specifically, the key strategy for enhancing model robustness entails the
transfer of robustness from the pre-trained model to the fine-tuned model,
coupled with adding a small amount of Gaussian noise to augment the
representative capacity of the model. Comprehensive experimental results
demonstrate that our approach markedly enhances the robustness across diverse
scenarios, encompassing various parameter-efficient fine-tuning methods and
confronting different levels of data heterogeneity.</div><div><a href='http://arxiv.org/abs/2401.14027v1'>2401.14027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16255v1")'>Watch Your Head: Assembling Projection Heads to Save the Reliability of
  Federated Models</div>
<div id='2402.16255v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T02:37:39Z</div><div>Authors: Jinqian Chen, Jihua Zhu, Qinghai Zheng, Zhongyu Li, Zhiqiang Tian</div><div style='padding-top: 10px; width: 80ex'>Federated learning encounters substantial challenges with heterogeneous data,
leading to performance degradation and convergence issues. While considerable
progress has been achieved in mitigating such an impact, the reliability aspect
of federated models has been largely disregarded. In this study, we conduct
extensive experiments to investigate the reliability of both generic and
personalized federated models. Our exploration uncovers a significant finding:
\textbf{federated models exhibit unreliability when faced with heterogeneous
data}, demonstrating poor calibration on in-distribution test data and low
uncertainty levels on out-of-distribution data. This unreliability is primarily
attributed to the presence of biased projection heads, which introduce
miscalibration into the federated models. Inspired by this observation, we
propose the "Assembled Projection Heads" (APH) method for enhancing the
reliability of federated models. By treating the existing projection head
parameters as priors, APH randomly samples multiple initialized parameters of
projection heads from the prior and further performs targeted fine-tuning on
locally available data under varying learning rates. Such a head ensemble
introduces parameter diversity into the deterministic model, eliminating the
bias and producing reliable predictions via head averaging. We evaluate the
effectiveness of the proposed APH method across three prominent federated
benchmarks. Experimental results validate the efficacy of APH in model
calibration and uncertainty estimation. Notably, APH can be seamlessly
integrated into various federated approaches but only requires less than 30\%
additional computation cost for 100$\times$ inferences within large models.</div><div><a href='http://arxiv.org/abs/2402.16255v1'>2402.16255v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00194v1")'>Ask Your Distribution Shift if Pre-Training is Right for You</div>
<div id='2403.00194v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T23:46:28Z</div><div>Authors: Benjamin Cohen-Wang, Joshua Vendrow, Aleksander Madry</div><div style='padding-top: 10px; width: 80ex'>Pre-training is a widely used approach to develop models that are robust to
distribution shifts. However, in practice, its effectiveness varies:
fine-tuning a pre-trained model improves robustness significantly in some cases
but not at all in others (compared to training from scratch). In this work, we
seek to characterize the failure modes that pre-training can and cannot
address. In particular, we focus on two possible failure modes of models under
distribution shift: poor extrapolation (e.g., they cannot generalize to a
different domain) and biases in the training data (e.g., they rely on spurious
features). Our study suggests that, as a rule of thumb, pre-training can help
mitigate poor extrapolation but not dataset biases. After providing theoretical
motivation and empirical evidence for this finding, we explore two of its
implications for developing robust models: (1) pre-training and interventions
designed to prevent exploiting biases have complementary robustness benefits,
and (2) fine-tuning on a (very) small, non-diverse but de-biased dataset can
result in significantly more robust models than fine-tuning on a large and
diverse but biased dataset. Code is available at
https://github.com/MadryLab/pretraining-distribution-shift-robustness.</div><div><a href='http://arxiv.org/abs/2403.00194v1'>2403.00194v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04529v1")'>Enhancing Data Quality in Federated Fine-Tuning of Foundation Models</div>
<div id='2403.04529v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:28:04Z</div><div>Authors: Wanru Zhao, Yaxin Du, Nicholas Donald Lane, Siheng Chen, Yanfeng Wang</div><div style='padding-top: 10px; width: 80ex'>In the current landscape of foundation model training, there is a significant
reliance on public domain data, which is nearing exhaustion according to recent
research. To further scale up, it is crucial to incorporate collaboration among
multiple specialized and high-quality private domain data sources. However, the
challenge of training models locally without sharing private data presents
numerous obstacles in data quality control. To tackle this issue, we propose a
data quality control pipeline for federated fine-tuning of foundation models.
This pipeline computes scores reflecting the quality of training data and
determines a global threshold for a unified standard, aiming for improved
global performance. Our experiments show that the proposed quality control
pipeline facilitates the effectiveness and reliability of the model training,
leading to better performance.</div><div><a href='http://arxiv.org/abs/2403.04529v1'>2403.04529v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14427v1")'>Beimingwu: A Learnware Dock System</div>
<div id='2401.14427v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T09:27:51Z</div><div>Authors: Zhi-Hao Tan, Jian-Dong Liu, Xiao-Dong Bi, Peng Tan, Qin-Cheng Zheng, Hai-Tian Liu, Yi Xie, Xiao-Chuan Zou, Yang Yu, Zhi-Hua Zhou</div><div style='padding-top: 10px; width: 80ex'>The learnware paradigm proposed by Zhou [2016] aims to enable users to reuse
numerous existing well-trained models instead of building machine learning
models from scratch, with the hope of solving new user tasks even beyond
models' original purposes. In this paradigm, developers worldwide can submit
their high-performing models spontaneously to the learnware dock system
(formerly known as learnware market) without revealing their training data.
Once the dock system accepts the model, it assigns a specification and
accommodates the model. This specification allows the model to be adequately
identified and assembled to reuse according to future users' needs, even if
they have no prior knowledge of the model. This paradigm greatly differs from
the current big model direction and it is expected that a learnware dock system
housing millions or more high-performing models could offer excellent
capabilities for both planned tasks where big models are applicable; and
unplanned, specialized, data-sensitive scenarios where big models are not
present or applicable.
  This paper describes Beimingwu, the first open-source learnware dock system
providing foundational support for future research of learnware paradigm.The
system significantly streamlines the model development for new user tasks,
thanks to its integrated architecture and engine design, extensive engineering
implementations and optimizations, and the integration of various algorithms
for learnware identification and reuse. Notably, this is possible even for
users with limited data and minimal expertise in machine learning, without
compromising the raw data's security. Beimingwu supports the entire process of
learnware paradigm. The system lays the foundation for future research in
learnware-related algorithms and systems, and prepares the ground for hosting a
vast array of learnwares and establishing a learnware ecosystem.</div><div><a href='http://arxiv.org/abs/2401.14427v1'>2401.14427v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11039v1")'>Robustness to Subpopulation Shift with Domain Label Noise via
  Regularized Annotation of Domains</div>
<div id='2402.11039v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T19:35:42Z</div><div>Authors: Nathan Stromberg, Rohan Ayyagari, Monica Welfert, Sanmi Koyejo, Lalitha Sankar</div><div style='padding-top: 10px; width: 80ex'>Existing methods for last layer retraining that aim to optimize worst-group
accuracy (WGA) rely heavily on well-annotated groups in the training data. We
show, both in theory and practice, that annotation-based data augmentations
using either downsampling or upweighting for WGA are susceptible to domain
annotation noise, and in high-noise regimes approach the WGA of a model trained
with vanilla empirical risk minimization. We introduce Regularized Annotation
of Domains (RAD) in order to train robust last layer classifiers without the
need for explicit domain annotations. Our results show that RAD is competitive
with other recently proposed domain annotation-free techniques. Most
importantly, RAD outperforms state-of-the-art annotation-reliant methods even
with only 5% noise in the training data for several publicly available
datasets.</div><div><a href='http://arxiv.org/abs/2402.11039v1'>2402.11039v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03421v1")'>LEAD: Learning Decomposition for Source-free Universal Domain Adaptation</div>
<div id='2403.03421v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T03:08:20Z</div><div>Authors: Sanqing Qu, Tianpei Zou, Lianghua He, Florian Röhrbein, Alois Knoll, Guang Chen, Changjun Jiang</div><div style='padding-top: 10px; width: 80ex'>Universal Domain Adaptation (UniDA) targets knowledge transfer in the
presence of both covariate and label shifts. Recently, Source-free Universal
Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to
source data, which tends to be more practical due to data protection policies.
The main challenge lies in determining whether covariate-shifted samples belong
to target-private unknown categories. Existing methods tackle this either
through hand-crafted thresholding or by developing time-consuming iterative
clustering strategies. In this paper, we propose a new idea of LEArning
Decomposition (LEAD), which decouples features into source-known and -unknown
components to identify target-private data. Technically, LEAD initially
leverages the orthogonal decomposition analysis for feature decomposition.
Then, LEAD builds instance-level decision boundaries to adaptively identify
target-private data. Extensive experiments across various UniDA scenarios have
demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA
scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and
reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD
is also appealing in that it is complementary to most existing methods. The
code is available at https://github.com/ispc-lab/LEAD.</div><div><a href='http://arxiv.org/abs/2403.03421v1'>2403.03421v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06826v1")'>Direct Distillation between Different Domains</div>
<div id='2401.06826v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T02:48:51Z</div><div>Authors: Jialiang Tang, Shuo Chen, Gang Niu, Hongyuan Zhu, Joey Tianyi Zhou, Chen Gong, Masashi Sugiyama</div><div style='padding-top: 10px; width: 80ex'>Knowledge Distillation (KD) aims to learn a compact student network using
knowledge from a large pre-trained teacher network, where both networks are
trained on data from the same distribution. However, in practical applications,
the student network may be required to perform in a new scenario (i.e., the
target domain), which usually exhibits significant differences from the known
scenario of the teacher network (i.e., the source domain). The traditional
domain adaptation techniques can be integrated with KD in a two-stage process
to bridge the domain gap, but the ultimate reliability of two-stage approaches
tends to be limited due to the high computational consumption and the
additional errors accumulated from both stages. To solve this problem, we
propose a new one-stage method dubbed ``Direct Distillation between Different
Domains" (4Ds). We first design a learnable adapter based on the Fourier
transform to separate the domain-invariant knowledge from the domain-specific
knowledge. Then, we build a fusion-activation mechanism to transfer the
valuable domain-invariant knowledge to the student network, while
simultaneously encouraging the adapter within the teacher network to learn the
domain-specific knowledge of the target data. As a result, the teacher network
can effectively transfer categorical knowledge that aligns with the target
domain of the student network. Intensive experiments on various benchmark
datasets demonstrate that our proposed 4Ds method successfully produces
reliable student networks and outperforms state-of-the-art approaches.</div><div><a href='http://arxiv.org/abs/2401.06826v1'>2401.06826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11148v2")'>Knowledge Distillation Based on Transformed Teacher Matching</div>
<div id='2402.11148v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:28:06Z</div><div>Authors: Kaixiang Zheng, En-Hui Yang</div><div style='padding-top: 10px; width: 80ex'>As a technique to bridge logit matching and probability distribution
matching, temperature scaling plays a pivotal role in knowledge distillation
(KD). Conventionally, temperature scaling is applied to both teacher's logits
and student's logits in KD. Motivated by some recent works, in this paper, we
drop instead temperature scaling on the student side, and systematically study
the resulting variant of KD, dubbed transformed teacher matching (TTM). By
reinterpreting temperature scaling as a power transform of probability
distribution, we show that in comparison with the original KD, TTM has an
inherent R\'enyi entropy term in its objective function, which serves as an
extra regularization term. Extensive experiment results demonstrate that thanks
to this inherent regularization, TTM leads to trained students with better
generalization than the original KD. To further enhance student's capability to
match teacher's power transformed probability distribution, we introduce a
sample-adaptive weighting coefficient into TTM, yielding a novel distillation
approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments,
that although WTTM is simple, it is effective, improves upon TTM, and achieves
state-of-the-art accuracy performance. Our source code is available at
https://github.com/zkxufo/TTM.</div><div><a href='http://arxiv.org/abs/2402.11148v2'>2402.11148v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07030v2")'>AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge
  Distillation</div>
<div id='2403.07030v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T03:34:14Z</div><div>Authors: Zihao Tang, Zheqi Lv, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu, Kun Kuang</div><div style='padding-top: 10px; width: 80ex'>Due to privacy or patent concerns, a growing number of large models are
released without granting access to their training data, making transferring
their knowledge inefficient and problematic. In response, Data-Free Knowledge
Distillation (DFKD) methods have emerged as direct solutions. However, simply
adopting models derived from DFKD for real-world applications suffers
significant performance degradation, due to the discrepancy between teachers'
training data and real-world scenarios (student domain). The degradation stems
from the portions of teachers' knowledge that are not applicable to the student
domain. They are specific to the teacher domain and would undermine students'
performance. Hence, selectively transferring teachers' appropriate knowledge
becomes the primary challenge in DFKD. In this work, we propose a simple but
effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific
anchor to align student-domain data with the teacher domain and leverages a
generative method to progressively trade off the learning process between OOD
knowledge distillation and domain-specific information learning via mixup
learning. Extensive experiments in 3 datasets and 8 settings demonstrate the
stability and superiority of our approach. Code available at
https://github.com/IshiKura-a/AuG-KD .</div><div><a href='http://arxiv.org/abs/2403.07030v2'>2403.07030v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12406v1")'>Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge
  Distillation</div>
<div id='2402.12406v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T08:13:57Z</div><div>Authors: Hyunjune Shin, Dong-Wan Choi</div><div style='padding-top: 10px; width: 80ex'>Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge
to a student model with the help of a generator without using original data. In
such data-free scenarios, achieving stable performance of DFKD is essential due
to the unavailability of validation data. Unfortunately, this paper has
discovered that existing DFKD methods are quite sensitive to different teacher
models, occasionally showing catastrophic failures of distillation, even when
using well-trained teacher models. Our observation is that the generator in
DFKD is not always guaranteed to produce precise yet diverse samples using the
existing representative strategy of minimizing both class-prior and adversarial
losses. Through our empirical study, we focus on the fact that class-prior not
only decreases the diversity of generated samples, but also cannot completely
address the problem of generating unexpectedly low-quality samples depending on
teacher models. In this paper, we propose the teacher-agnostic data-free
knowledge distillation (TA-DFKD) method, with the goal of more robust and
stable performance regardless of teacher models. Our basic idea is to assign
the teacher model a lenient expert role for evaluating samples, rather than a
strict supervisor that enforces its class-prior on the generator. Specifically,
we design a sample selection approach that takes only clean samples verified by
the teacher model without imposing restrictions on the power of generating
diverse samples. Through extensive experiments, we show that our method
successfully achieves both robustness and training stability across various
teacher models, while outperforming the existing DFKD methods.</div><div><a href='http://arxiv.org/abs/2402.12406v1'>2402.12406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05942v1")'>Cooperative Knowledge Distillation: A Learner Agnostic Approach</div>
<div id='2402.05942v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T17:31:50Z</div><div>Authors: Michael Livanos, Ian Davidson, Stephen Wong</div><div style='padding-top: 10px; width: 80ex'>Knowledge distillation is a simple but powerful way to transfer knowledge
between a teacher model to a student model. Existing work suffers from at least
one of the following key limitations in terms of direction and scope of
transfer which restrict its use: all knowledge is transferred from teacher to
student regardless of whether or not that knowledge is useful, the student is
the only one learning in this exchange, and typically distillation transfers
knowledge only from a single teacher to a single student. We formulate a novel
form of knowledge distillation in which many models can act as both students
and teachers which we call cooperative distillation. The models cooperate as
follows: a model (the student) identifies specific deficiencies in it's
performance and searches for another model (the teacher) who encodes learned
knowledge into instructional virtual instances via counterfactual instance
generation. Because different models may have different strengths and
weaknesses, all models can act as either students or teachers (cooperation)
when appropriate and only distill knowledge in areas specific to their
strengths (focus). Since counterfactuals as a paradigm are not tied to any
specific algorithm, we can use this method to distill knowledge between
learners of different architectures, algorithms, and even feature spaces. We
demonstrate that our approach not only outperforms baselines such as transfer
learning, self-supervised learning, and multiple knowledge distillation
algorithms on several datasets, but it can also be used in settings where the
aforementioned techniques cannot.</div><div><a href='http://arxiv.org/abs/2402.05942v1'>2402.05942v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03119v1")'>Good Teachers Explain: Explanation-Enhanced Knowledge Distillation</div>
<div id='2402.03119v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:47:54Z</div><div>Authors: Amin Parchami-Araghi, Moritz Böhle, Sukrut Rao, Bernt Schiele</div><div style='padding-top: 10px; width: 80ex'>Knowledge Distillation (KD) has proven effective for compressing large
teacher models into smaller student models. While it is well known that student
models can achieve similar accuracies as the teachers, it has also been shown
that they nonetheless often do not learn the same function. It is, however,
often highly desirable that the student's and teacher's functions share similar
properties such as basing the prediction on the same input features, as this
ensures that students learn the 'right features' from the teachers. In this
work, we explore whether this can be achieved by not only optimizing the
classic KD loss but also the similarity of the explanations generated by the
teacher and the student. Despite the idea being simple and intuitive, we find
that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides
large gains in terms of accuracy and student-teacher agreement, (2) ensures
that the student learns from the teacher to be right for the right reasons and
to give similar explanations, and (3) is robust with respect to the model
architectures, the amount of training data, and even works with 'approximate',
pre-computed explanations.</div><div><a href='http://arxiv.org/abs/2402.03119v1'>2402.03119v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14035v2")'>Wisdom of Committee: Distilling from Foundation Model to Specialized
  Application Model</div>
<div id='2402.14035v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T04:33:26Z</div><div>Authors: Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in foundation models have yielded impressive performance
across a wide range of tasks. Meanwhile, for specific applications,
practitioners have been developing specialized application models. To enjoy the
benefits of both kinds of models, one natural path is to transfer the knowledge
in foundation models into specialized application models, which are generally
more efficient for serving. Techniques from knowledge distillation may be
applied here, where the application model learns to mimic the foundation model.
However, specialized application models and foundation models have substantial
gaps in capacity, employing distinct architectures, using different input
features from different modalities, and being optimized on different
distributions. These differences in model characteristics lead to significant
challenges for distillation methods. In this work, we propose creating a
teaching committee comprising both foundation model teachers and complementary
teachers. Complementary teachers possess model characteristics akin to the
student's, aiming to bridge the gap between the foundation model and
specialized application models for a smoother knowledge transfer. Further, to
accommodate the dissimilarity among the teachers in the committee, we introduce
DiverseDistill, which allows the student to understand the expertise of each
teacher and extract task knowledge. Our evaluations demonstrate that adding
complementary teachers enhances student performance. Finally, DiverseDistill
consistently outperforms baseline distillation methods, regardless of the
teacher choices, resulting in significantly improved student performance.</div><div><a href='http://arxiv.org/abs/2402.14035v2'>2402.14035v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14410v1")'>GLC++: Source-Free Universal Domain Adaptation through Global-Local
  Clustering and Contrastive Affinity Learning</div>
<div id='2403.14410v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T13:57:45Z</div><div>Authors: Sanqing Qu, Tianpei Zou, Florian Röhrbein, Cewu Lu, Guang Chen, Dacheng Tao, Changjun Jiang</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks often exhibit sub-optimal performance under covariate
and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising
solution to this dilemma, yet most SFDA approaches are restricted to closed-set
scenarios. In this paper, we explore Source-Free Universal Domain Adaptation
(SF-UniDA) aiming to accurately classify "known" data belonging to common
categories and segregate them from target-private "unknown" data. We propose a
novel Global and Local Clustering (GLC) technique, which comprises an adaptive
one-vs-all global clustering algorithm to discern between target classes,
complemented by a local k-NN clustering strategy to mitigate negative transfer.
Despite the effectiveness, the inherent closed-set source architecture leads to
uniform treatment of "unknown" data, impeding the identification of distinct
"unknown" categories. To address this, we evolve GLC to GLC++, integrating a
contrastive affinity learning strategy. We examine the superiority of GLC and
GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in
the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by
16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel
category clustering accuracy of GLC by 4.3% in open-set scenarios on
Office-Home. Furthermore, the introduced contrastive learning strategy not only
enhances GLC but also significantly facilitates existing methodologies.</div><div><a href='http://arxiv.org/abs/2403.14410v1'>2403.14410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18853v1")'>Rethinking Multi-domain Generalization with A General Learning Objective</div>
<div id='2402.18853v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T05:00:30Z</div><div>Authors: Zhaorui Tan, Xi Yang, Kaizhu Huang</div><div style='padding-top: 10px; width: 80ex'>Multi-domain generalization (mDG) is universally aimed to minimize the
discrepancy between training and testing distributions to enhance
marginal-to-label distribution mapping. However, existing mDG literature lacks
a general learning objective paradigm and often imposes constraints on static
target marginal distributions. In this paper, we propose to leverage a
$Y$-mapping to relax the constraint. We rethink the learning objective for mDG
and design a new \textbf{general learning objective} to interpret and analyze
most existing mDG wisdom. This general objective is bifurcated into two
synergistic amis: learning domain-independent conditional features and
maximizing a posterior. Explorations also extend to two effective
regularization terms that incorporate prior information and suppress invalid
causality, alleviating the issues that come with relaxed constraints. We
theoretically contribute an upper bound for the domain alignment of
domain-independent conditional features, disclosing that many previous mDG
endeavors actually \textbf{optimize partially the objective} and thus lead to
limited performance. As such, our study distills a general learning objective
into four practical components, providing a general, robust, and flexible
mechanism to handle complex domain shifts. Extensive empirical results indicate
that the proposed objective with $Y$-mapping leads to substantially better mDG
performance in various downstream tasks, including regression, segmentation,
and classification.</div><div><a href='http://arxiv.org/abs/2402.18853v1'>2402.18853v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06392v1")'>Towards Robust Out-of-Distribution Generalization Bounds via Sharpness</div>
<div id='2403.06392v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T02:57:27Z</div><div>Authors: Yingtian Zou, Kenji Kawaguchi, Yingnan Liu, Jiashuo Liu, Mong-Li Lee, Wynne Hsu</div><div style='padding-top: 10px; width: 80ex'>Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD
generalization, still lacks appropriate theoretical guarantees. Canonical OOD
bounds focus on different distance measurements between source and target
domains but fail to consider the optimization property of the learned model. As
empirically shown in recent work, the sharpness of learned minima influences
OOD generalization. To bridge this gap between optimization and OOD
generalization, we study the effect of sharpness on how a model tolerates data
change in domain shift which is usually captured by "robustness" in
generalization. In this paper, we give a rigorous connection between sharpness
and robustness, which gives better OOD guarantees for robust algorithms. It
also provides a theoretical backing for "flat minima leads to better OOD
generalization". Overall, we propose a sharpness-based OOD generalization bound
by taking robustness into consideration, resulting in a tighter bound than
non-robust guarantees. Our findings are supported by the experiments on a ridge
regression model, as well as the experiments on deep learning classification
tasks.</div><div><a href='http://arxiv.org/abs/2403.06392v1'>2403.06392v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15952v1")'>A Class-aware Optimal Transport Approach with Higher-Order Moment
  Matching for Unsupervised Domain Adaptation</div>
<div id='2401.15952v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:27:31Z</div><div>Authors: Tuan Nguyen, Van Nguyen, Trung Le, He Zhao, Quan Hung Tran, Dinh Phung</div><div style='padding-top: 10px; width: 80ex'>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. In this paper, we
introduce a novel approach called class-aware optimal transport (OT), which
measures the OT distance between a distribution over the source
class-conditional distributions and a mixture of source and target data
distribution. Our class-aware OT leverages a cost function that determines the
matching extent between a given data example and a source class-conditional
distribution. By optimizing this cost function, we find the optimal matching
between target examples and source class-conditional distributions, effectively
addressing the data and label shifts that occur between the two domains. To
handle the class-aware OT efficiently, we propose an amortization solution that
employs deep neural networks to formulate the transportation probabilities and
the cost function. Additionally, we propose minimizing class-aware Higher-order
Moment Matching (HMM) to align the corresponding class regions on the source
and target domains. The class-aware HMM component offers an economical
computational approach for accurately evaluating the HMM distance between the
two distributions. Extensive experiments on benchmark datasets demonstrate that
our proposed method significantly outperforms existing state-of-the-art
baselines.</div><div><a href='http://arxiv.org/abs/2401.15952v1'>2401.15952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01048v1")'>PAC-Bayesian Domain Adaptation Bounds for Multi-view learning</div>
<div id='2401.01048v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T06:00:47Z</div><div>Authors: Mehdi Hennequin, Khalid Benabdeslem, Haytham Elghazel</div><div style='padding-top: 10px; width: 80ex'>This paper presents a series of new results for domain adaptation in the
multi-view learning setting. The incorporation of multiple views in the domain
adaptation was paid little attention in the previous studies. In this way, we
propose an analysis of generalization bounds with Pac-Bayesian theory to
consolidate the two paradigms, which are currently treated separately. Firstly,
building on previous work by Germain et al., we adapt the distance between
distribution proposed by Germain et al. for domain adaptation with the concept
of multi-view learning. Thus, we introduce a novel distance that is tailored
for the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds
for estimating the introduced divergence. Finally, we compare the different new
bounds with the previous studies.</div><div><a href='http://arxiv.org/abs/2401.01048v1'>2401.01048v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01887v1")'>On f-Divergence Principled Domain Adaptation: An Improved Framework</div>
<div id='2402.01887v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:22:44Z</div><div>Authors: Ziqiao Wang, Yongyi Mao</div><div style='padding-top: 10px; width: 80ex'>Unsupervised domain adaptation (UDA) plays a crucial role in addressing
distribution shifts in machine learning. In this work, we improve the
theoretical foundations of UDA proposed by Acuna et al. (2021) by refining
their f-divergence-based discrepancy and additionally introducing a new
measure, f-domain discrepancy (f-DD). By removing the absolute value function
and incorporating a scaling parameter, f-DD yields novel target error and
sample complexity bounds, allowing us to recover previous KL-based results and
bridging the gap between algorithms and theory presented in Acuna et al.
(2021). Leveraging a localization technique, we also develop a fast-rate
generalization bound. Empirical results demonstrate the superior performance of
f-DD-based domain learning algorithms over previous works in popular UDA
benchmarks.</div><div><a href='http://arxiv.org/abs/2402.01887v1'>2402.01887v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00580v1")'>Continuous Unsupervised Domain Adaptation Using Stabilized
  Representations and Experience Replay</div>
<div id='2402.00580v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T05:09:14Z</div><div>Authors: Mohammad Rostami</div><div style='padding-top: 10px; width: 80ex'>We introduce an algorithm for tackling the problem of unsupervised domain
adaptation (UDA) in continual learning (CL) scenarios. The primary objective is
to maintain model generalization under domain shift when new domains arrive
continually through updating a base model when only unlabeled data is
accessible in subsequent tasks. While there are many existing UDA algorithms,
they typically require access to both the source and target domain datasets
simultaneously. Conversely, existing CL approaches can handle tasks that all
have labeled data. Our solution is based on stabilizing the learned internal
distribution to enhances the model generalization on new domains. The internal
distribution is modeled by network responses in hidden layer. We model this
internal distribution using a Gaussian mixture model (GMM ) and update the
model by matching the internally learned distribution of new domains to the
estimated GMM. Additionally, we leverage experience replay to overcome the
problem of catastrophic forgetting, where the model loses previously acquired
knowledge when learning new tasks. We offer theoretical analysis to explain why
our algorithm would work. We also offer extensive comparative and analytic
experiments to demonstrate that our method is effective. We perform experiments
on four benchmark datasets to demonstrate that our approach is effective.</div><div><a href='http://arxiv.org/abs/2402.00580v1'>2402.00580v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08464v1")'>Enhancing Evolving Domain Generalization through Dynamic Latent
  Representations</div>
<div id='2401.08464v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T16:16:42Z</div><div>Authors: Binghui Xie, Yongqiang Chen, Jiaqi Wang, Kaiwen Zhou, Bo Han, Wei Meng, James Cheng</div><div style='padding-top: 10px; width: 80ex'>Domain generalization is a critical challenge for machine learning systems.
Prior domain generalization methods focus on extracting domain-invariant
features across several stationary domains to enable generalization to new
domains. However, in non-stationary tasks where new domains evolve in an
underlying continuous structure, such as time, merely extracting the invariant
features is insufficient for generalization to the evolving new domains.
Nevertheless, it is non-trivial to learn both evolving and invariant features
within a single model due to their conflicts. To bridge this gap, we build
causal models to characterize the distribution shifts concerning the two
patterns, and propose to learn both dynamic and invariant features via a new
framework called Mutual Information-Based Sequential Autoencoders (MISTS).
MISTS adopts information theoretic constraints onto sequential autoencoders to
disentangle the dynamic and invariant features, and leverage a domain adaptive
classifier to make predictions based on both evolving and invariant
information. Our experimental results on both synthetic and real-world datasets
demonstrate that MISTS succeeds in capturing both evolving and invariant
information, and present promising results in evolving domain generalization
tasks.</div><div><a href='http://arxiv.org/abs/2401.08464v1'>2401.08464v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07834v2")'>Generalizing across Temporal Domains with Koopman Operators</div>
<div id='2402.07834v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:45:40Z</div><div>Authors: Qiuhao Zeng, Wei Wang, Fan Zhou, Gezheng Xu, Ruizhi Pu, Changjian Shui, Christian Gagne, Shichun Yang, Boyu Wang, Charles X. Ling</div><div style='padding-top: 10px; width: 80ex'>In the field of domain generalization, the task of constructing a predictive
model capable of generalizing to a target domain without access to target data
remains challenging. This problem becomes further complicated when considering
evolving dynamics between domains. While various approaches have been proposed
to address this issue, a comprehensive understanding of the underlying
generalization theory is still lacking. In this study, we contribute novel
theoretic results that aligning conditional distribution leads to the reduction
of generalization bounds. Our analysis serves as a key motivation for solving
the Temporal Domain Generalization (TDG) problem through the application of
Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By
employing Koopman Operators, we effectively address the time-evolving
distributions encountered in TDG using the principles of Koopman theory, where
measurement functions are sought to establish linear transition relations
between evolving domains. Through empirical evaluations conducted on synthetic
and real-world datasets, we validate the effectiveness of our proposed
approach.</div><div><a href='http://arxiv.org/abs/2402.07834v2'>2402.07834v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03588v1")'>Continual Domain Adversarial Adaptation via Double-Head Discriminators</div>
<div id='2402.03588v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T23:46:03Z</div><div>Authors: Yan Shen, Zhanghexuan Ji, Chunwei Ma, Mingchen Gao</div><div style='padding-top: 10px; width: 80ex'>Domain adversarial adaptation in a continual setting poses a significant
challenge due to the limitations on accessing previous source domain data.
Despite extensive research in continual learning, the task of adversarial
adaptation cannot be effectively accomplished using only a small number of
stored source domain data, which is a standard setting in memory replay
approaches. This limitation arises from the erroneous empirical estimation of
$\gH$-divergence with few source domain samples. To tackle this problem, we
propose a double-head discriminator algorithm, by introducing an addition
source-only domain discriminator that are trained solely on source learning
phase. We prove that with the introduction of a pre-trained source-only domain
discriminator, the empirical estimation error of $\gH$-divergence related
adversarial loss is reduced from the source domain side. Further experiments on
existing domain adaptation benchmark show that our proposed algorithm achieves
more than 2$\%$ improvement on all categories of target domain adaptation task
while significantly mitigating the forgetting on source domain.</div><div><a href='http://arxiv.org/abs/2402.03588v1'>2402.03588v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03082v1")'>Recall-Oriented Continual Learning with Generative Adversarial
  Meta-Model</div>
<div id='2403.03082v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T16:08:59Z</div><div>Authors: Haneol Kang, Dong-Wan Choi</div><div style='padding-top: 10px; width: 80ex'>The stability-plasticity dilemma is a major challenge in continual learning,
as it involves balancing the conflicting objectives of maintaining performance
on previous tasks while learning new tasks. In this paper, we propose the
recall-oriented continual learning framework to address this challenge.
Inspired by the human brain's ability to separate the mechanisms responsible
for stability and plasticity, our framework consists of a two-level
architecture where an inference network effectively acquires new knowledge and
a generative network recalls past knowledge when necessary. In particular, to
maximize the stability of past knowledge, we investigate the complexity of
knowledge depending on different representations, and thereby introducing
generative adversarial meta-model (GAMM) that incrementally learns
task-specific parameters instead of input data samples of the task. Through our
experiments, we show that our framework not only effectively learns new
knowledge without any disruption but also achieves high stability of previous
knowledge in both task-aware and task-agnostic learning scenarios. Our code is
available at: https://github.com/bigdata-inha/recall-oriented-cl-framework.</div><div><a href='http://arxiv.org/abs/2403.03082v1'>2403.03082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16681v2")'>Enhancing Continuous Domain Adaptation with Multi-Path Transfer
  Curriculum</div>
<div id='2402.16681v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T15:59:38Z</div><div>Authors: Hanbing Liu, Jingge Wang, Xuan Zhang, Ye Guo, Yang Li</div><div style='padding-top: 10px; width: 80ex'>Addressing the large distribution gap between training and testing data has
long been a challenge in machine learning, giving rise to fields such as
transfer learning and domain adaptation. Recently, Continuous Domain Adaptation
(CDA) has emerged as an effective technique, closing this gap by utilizing a
series of intermediate domains. This paper contributes a novel CDA method,
W-MPOT, which rigorously addresses the domain ordering and error accumulation
problems overlooked by previous studies. Specifically, we construct a transfer
curriculum over the source and intermediate domains based on Wasserstein
distance, motivated by theoretical analysis of CDA. Then we transfer the source
model to the target domain through multiple valid paths in the curriculum using
a modified version of continuous optimal transport. A bidirectional path
consistency constraint is introduced to mitigate the impact of accumulated
mapping errors during continuous transfer. We extensively evaluate W-MPOT on
multiple datasets, achieving up to 54.1\% accuracy improvement on multi-session
Alzheimer MR image classification and 94.7\% MSE reduction on battery capacity
estimation.</div><div><a href='http://arxiv.org/abs/2402.16681v2'>2402.16681v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13847v1")'>Optimal Transport for Domain Adaptation through Gaussian Mixture Models</div>
<div id='2403.13847v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T09:32:33Z</div><div>Authors: Eduardo Fernandes Montesuma, Fred Maurice Ngolè Mboula, Antoine Souloumiac</div><div style='padding-top: 10px; width: 80ex'>In this paper we explore domain adaptation through optimal transport. We
propose a novel approach, where we model the data distributions through
Gaussian mixture models. This strategy allows us to solve continuous optimal
transport through an equivalent discrete problem. The optimal transport
solution gives us a matching between source and target domain mixture
components. From this matching, we can map data points between domains, or
transfer the labels from the source domain components towards the target
domain. We experiment with 2 domain adaptation benchmarks in fault diagnosis,
showing that our methods have state-of-the-art performance.</div><div><a href='http://arxiv.org/abs/2403.13847v1'>2403.13847v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03281v1")'>Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits</div>
<div id='2403.03281v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T19:25:55Z</div><div>Authors: Sahil Sidheekh, Pranuthi Tenali, Saurabh Mathur, Erik Blasch, Kristian Kersting, Sriraam Natarajan</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of late multi-modal fusion for discriminative
learning. Motivated by noisy, multi-source domains that require understanding
the reliability of each data source, we explore the notion of credibility in
the context of multi-modal fusion. We propose a combination function that uses
probabilistic circuits (PCs) to combine predictive distributions over
individual modalities. We also define a probabilistic measure to evaluate the
credibility of each modality via inference queries over the PC. Our
experimental evaluation demonstrates that our fusion method can reliably infer
credibility while maintaining competitive performance with the
state-of-the-art.</div><div><a href='http://arxiv.org/abs/2403.03281v1'>2403.03281v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13721v2")'>Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in
  Regression</div>
<div id='2401.13721v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T14:55:02Z</div><div>Authors: Ismail Nejjar, Gaetan Frusque, Florent Forest, Olga Fink</div><div style='padding-top: 10px; width: 80ex'>Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model
from a labeled source domain to an unlabeled target domain for regression
tasks. Recent successful works in UDAR mostly focus on subspace alignment,
involving the alignment of a selected subspace within the entire feature space.
This contrasts with the feature alignment methods used for classification,
which aim at aligning the entire feature space and have proven effective but
are less so in regression settings. Specifically, while classification aims to
identify separate clusters across the entire embedding dimension, regression
induces less structure in the data representation, necessitating additional
guidance for efficient alignment. In this paper, we propose an effective method
for UDAR by incorporating guidance from uncertainty. Our approach serves a dual
purpose: providing a measure of confidence in predictions and acting as a
regularization of the embedding space. Specifically, we leverage the Deep
Evidential Learning framework, which outputs both predictions and uncertainties
for each input sample. We propose aligning the parameters of higher-order
evidential distributions between the source and target domains using
traditional alignment methods at the feature or posterior level. Additionally,
we propose to augment the feature space representation by mixing source samples
with pseudo-labeled target samples based on label similarity. This cross-domain
mixing strategy produces more realistic samples than random mixing and
introduces higher uncertainty, facilitating further alignment. We demonstrate
the effectiveness of our approach on four benchmarks for UDAR, on which we
outperform existing methods.</div><div><a href='http://arxiv.org/abs/2401.13721v2'>2401.13721v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12715v1")'>Spurious Correlations in Machine Learning: A Survey</div>
<div id='2402.12715v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:49:34Z</div><div>Authors: Wenqian Ye, Guangtao Zheng, Xu Cao, Yunsheng Ma, Xia Hu, Aidong Zhang</div><div style='padding-top: 10px; width: 80ex'>Machine learning systems are known to be sensitive to spurious correlations
between biased features of the inputs (e.g., background, texture, and secondary
objects) and the corresponding labels. These features and their correlations
with the labels are known as "spurious" because they tend to change with shifts
in real-world data distributions, which can negatively impact the model's
generalization and robustness. In this survey, we provide a comprehensive
review of this issue, along with a taxonomy of current state-of-the-art methods
for addressing spurious correlations in machine learning models. Additionally,
we summarize existing datasets, benchmarks, and metrics to aid future research.
The paper concludes with a discussion of the recent advancements and future
research challenges in this field, aiming to provide valuable insights for
researchers in the related domains.</div><div><a href='http://arxiv.org/abs/2402.12715v1'>2402.12715v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07390v1")'>Knee or ROC</div>
<div id='2401.07390v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T23:25:44Z</div><div>Authors: Veronica Wendt, Byunggu Yu, Caleb Kelly, Junwhan Kim</div><div style='padding-top: 10px; width: 80ex'>Self-attention transformers have demonstrated accuracy for image
classification with smaller data sets. However, a limitation is that tests
to-date are based upon single class image detection with known representation
of image populations. For instances where the input image classes may be
greater than one and test sets that lack full information on representation of
image populations, accuracy calculations must adapt. The Receiver Operating
Characteristic (ROC) accuracy thresh-old can address the instances of
multi-class input images. However, this approach is unsuitable in instances
where image population representation is unknown. We consider calculating
accuracy using the knee method to determine threshold values on an ad-hoc
basis. Results of ROC curve and knee thresholds for a multi-class data set,
created from CIFAR-10 images, are discussed for multi-class image detection.</div><div><a href='http://arxiv.org/abs/2401.07390v1'>2401.07390v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14435v1")'>Biased Binary Attribute Classifiers Ignore the Majority Classes</div>
<div id='2403.14435v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:41:58Z</div><div>Authors: Xinyi Zhang, Johanna Sophie Bieri, Manuel Günther</div><div style='padding-top: 10px; width: 80ex'>To visualize the regions of interest that classifiers base their decisions
on, different Class Activation Mapping (CAM) methods have been developed.
However, all of these techniques target categorical classifiers only, though
most real-world tasks are binary classification. In this paper, we extend
gradient-based CAM techniques to work with binary classifiers and visualize the
active regions for binary facial attribute classifiers. When training an
unbalanced binary classifier on an imbalanced dataset, it is well-known that
the majority class, i.e. the class with many training samples, is mostly
predicted much better than minority class with few training instances. In our
experiments on the CelebA dataset, we verify these results, when training an
unbalanced classifier to extract 40 facial attributes simultaneously. One would
expect that the biased classifier has learned to extract features mainly for
the majority classes and that the proportional energy of the activations mainly
reside in certain specific regions of the image where the attribute is located.
However, we find very little regular activation for samples of majority
classes, while the active regions for minority classes seem mostly reasonable
and overlap with our expectations. These results suggest that biased
classifiers mainly rely on bias activation for majority classes. When training
a balanced classifier on the imbalanced data by employing attribute-specific
class weights, majority and minority classes are classified similarly well and
show expected activations for almost all attributes</div><div><a href='http://arxiv.org/abs/2403.14435v1'>2403.14435v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06546v1")'>Optimizing Feature Selection for Binary Classification with Noisy
  Labels: A Genetic Algorithm Approach</div>
<div id='2401.06546v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T12:42:26Z</div><div>Authors: Vandad Imani, Elaheh Moradi, Carlos Sevilla-Salcedo, Vittorio Fortino, Jussi Tohka</div><div style='padding-top: 10px; width: 80ex'>Feature selection in noisy label scenarios remains an understudied topic. We
propose a novel genetic algorithm-based approach, the Noise-Aware
Multi-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selecting
optimal feature subsets in binary classification with noisy labels. NMFS-GA
offers a unified framework for selecting feature subsets that are both accurate
and interpretable. We evaluate NMFS-GA on synthetic datasets with label noise,
a Breast Cancer dataset enriched with noisy features, and a real-world ADNI
dataset for dementia conversion prediction. Our results indicate that NMFS-GA
can effectively select feature subsets that improve the accuracy and
interpretability of binary classifiers in scenarios with noisy labels.</div><div><a href='http://arxiv.org/abs/2401.06546v1'>2401.06546v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17760v2")'>Regularized Linear Discriminant Analysis Using a Nonlinear Covariance
  Matrix Estimator</div>
<div id='2401.17760v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T11:37:14Z</div><div>Authors: Maaz Mahadi, Tarig Ballal, Muhammad Moinuddin, Tareq Y. Al-Naffouri, Ubaid M. Al-Saggaf</div><div style='padding-top: 10px; width: 80ex'>Linear discriminant analysis (LDA) is a widely used technique for data
classification. The method offers adequate performance in many classification
problems, but it becomes inefficient when the data covariance matrix is
ill-conditioned. This often occurs when the feature space's dimensionality is
higher than or comparable to the training data size. Regularized LDA (RLDA)
methods based on regularized linear estimators of the data covariance matrix
have been proposed to cope with such a situation. The performance of RLDA
methods is well studied, with optimal regularization schemes already proposed.
In this paper, we investigate the capability of a positive semidefinite
ridge-type estimator of the inverse covariance matrix that coincides with a
nonlinear (NL) covariance matrix estimator. The estimator is derived by
reformulating the score function of the optimal classifier utilizing linear
estimation methods, which eventually results in the proposed NL-RLDA
classifier. We derive asymptotic and consistent estimators of the proposed
technique's misclassification rate under the assumptions of a double-asymptotic
regime and multivariate Gaussian model for the classes. The consistent
estimator, coupled with a one-dimensional grid search, is used to set the value
of the regularization parameter required for the proposed NL-RLDA classifier.
Performance evaluations based on both synthetic and real data demonstrate the
effectiveness of the proposed classifier. The proposed technique outperforms
state-of-art methods over multiple datasets. When compared to state-of-the-art
methods across various datasets, the proposed technique exhibits superior
performance.</div><div><a href='http://arxiv.org/abs/2401.17760v2'>2401.17760v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12842v1")'>Iterated Relevance Matrix Analysis (IRMA) for the identification of
  class-discriminative subspaces</div>
<div id='2401.12842v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T15:23:13Z</div><div>Authors: Sofie Lövdal, Michael Biehl</div><div style='padding-top: 10px; width: 80ex'>We introduce and investigate the iterated application of Generalized Matrix
Learning Vector Quantizaton for the analysis of feature relevances in
classification problems, as well as for the construction of
class-discriminative subspaces. The suggested Iterated Relevance Matrix
Analysis (IRMA) identifies a linear subspace representing the classification
specific information of the considered data sets using Generalized Matrix
Learning Vector Quantization (GMLVQ). By iteratively determining a new
discriminative subspace while projecting out all previously identified ones, a
combined subspace carrying all class-specific information can be found. This
facilitates a detailed analysis of feature relevances, and enables improved
low-dimensional representations and visualizations of labeled data sets.
Additionally, the IRMA-based class-discriminative subspace can be used for
dimensionality reduction and the training of robust classifiers with
potentially improved performance.</div><div><a href='http://arxiv.org/abs/2401.12842v1'>2401.12842v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02438v1")'>Fast and interpretable Support Vector Classification based on the
  truncated ANOVA decomposition</div>
<div id='2402.02438v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:27:42Z</div><div>Authors: Kseniya Akhalaya, Franziska Nestler, Daniel Potts</div><div style='padding-top: 10px; width: 80ex'>Support Vector Machines (SVMs) are an important tool for performing
classification on scattered data, where one usually has to deal with many data
points in high-dimensional spaces. We propose solving SVMs in primal form using
feature maps based on trigonometric functions or wavelets. In small dimensional
settings the Fast Fourier Transform (FFT) and related methods are a powerful
tool in order to deal with the considered basis functions. For growing
dimensions the classical FFT-based methods become inefficient due to the curse
of dimensionality. Therefore, we restrict ourselves to multivariate basis
functions, each one of them depends only on a small number of dimensions. This
is motivated by the well-known sparsity of effects and recent results regarding
the reconstruction of functions from scattered data in terms of truncated
analysis of variance (ANOVA) decomposition, which makes the resulting model
even interpretable in terms of importance of the features as well as their
couplings. The usage of small superposition dimensions has the consequence that
the computational effort no longer grows exponentially but only polynomially
with respect to the dimension. In order to enforce sparsity regarding the basis
coefficients, we use the frequently applied $\ell_2$-norm and, in addition,
$\ell_1$-norm regularization. The found classifying function, which is the
linear combination of basis functions, and its variance can then be analyzed in
terms of the classical ANOVA decomposition of functions. Based on numerical
examples we show that we are able to recover the signum of a function that
perfectly fits our model assumptions. We obtain better results with
$\ell_1$-norm regularization, both in terms of accuracy and clarity of
interpretability.</div><div><a href='http://arxiv.org/abs/2402.02438v1'>2402.02438v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13103v1")'>Multivariate Functional Linear Discriminant Analysis for the
  Classification of Short Time Series with Missing Data</div>
<div id='2402.13103v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:58:45Z</div><div>Authors: Rahul Bordoloi, Clémence Réda, Orell Trautmann, Saptarshi Bej, Olaf Wolkenhauer</div><div style='padding-top: 10px; width: 80ex'>Functional linear discriminant analysis (FLDA) is a powerful tool that
extends LDA-mediated multiclass classification and dimension reduction to
univariate time-series functions. However, in the age of large multivariate and
incomplete data, statistical dependencies between features must be estimated in
a computationally tractable way, while also dealing with missing data. There is
a need for a computationally tractable approach that considers the statistical
dependencies between features and can handle missing values. We here develop a
multivariate version of FLDA (MUDRA) to tackle this issue and describe an
efficient expectation/conditional-maximization (ECM) algorithm to infer its
parameters. We assess its predictive power on the "Articulary Word Recognition"
data set and show its improvement over the state-of-the-art, especially in the
case of missing data. MUDRA allows interpretable classification of data sets
with large proportions of missing data, which will be particularly useful for
medical or psychological data sets.</div><div><a href='http://arxiv.org/abs/2402.13103v1'>2402.13103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13176v1")'>Castor: Competing shapelets for fast and accurate time series
  classification</div>
<div id='2403.13176v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T22:05:32Z</div><div>Authors: Isak Samsten, Zed Lee</div><div style='padding-top: 10px; width: 80ex'>Shapelets are discriminative subsequences, originally embedded in
shapelet-based decision trees but have since been extended to shapelet-based
transformations. We propose Castor, a simple, efficient, and accurate time
series classification algorithm that utilizes shapelets to transform time
series. The transformation organizes shapelets into groups with varying
dilation and allows the shapelets to compete over the time context to construct
a diverse feature representation. By organizing the shapelets into groups, we
enable the transformation to transition between levels of competition,
resulting in methods that more closely resemble distance-based transformations
or dictionary-based transformations. We demonstrate, through an extensive
empirical investigation, that Castor yields transformations that result in
classifiers that are significantly more accurate than several state-of-the-art
classifiers. In an extensive ablation study, we examine the effect of choosing
hyperparameters and suggest accurate and efficient default values.</div><div><a href='http://arxiv.org/abs/2403.13176v1'>2403.13176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14081v1")'>Robust Learning of Noisy Time Series Collections Using Stochastic
  Process Models with Motion Codes</div>
<div id='2402.14081v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:10:08Z</div><div>Authors: Chandrajit Bajaj, Minh Nguyen</div><div style='padding-top: 10px; width: 80ex'>While time series classification and forecasting problems have been
extensively studied, the cases of noisy time series data with arbitrary time
sequence lengths have remained challenging. Each time series instance can be
thought of as a sample realization of a noisy dynamical model, which is
characterized by a continuous stochastic process. For many applications, the
data are mixed and consist of several types of noisy time series sequences
modeled by multiple stochastic processes, making the forecasting and
classification tasks even more challenging. Instead of regressing data naively
and individually to each time series type, we take a latent variable model
approach using a mixtured Gaussian processes with learned spectral kernels.
More specifically, we auto-assign each type of noisy time series data a
signature vector called its motion code. Then, conditioned on each assigned
motion code, we infer a sparse approximation of the corresponding time series
using the concept of the most informative timestamps. Our unmixing
classification approach involves maximizing the likelihood across all the mixed
noisy time series sequences of varying lengths. This stochastic approach allows
us to learn not only within a single type of noisy time series data but also
across many underlying stochastic processes, giving us a way to learn multiple
dynamical models in an integrated and robust manner. The different learned
latent stochastic models allow us to generate specific sub-type forecasting. We
provide several quantitative comparisons demonstrating the performance of our
approach.</div><div><a href='http://arxiv.org/abs/2402.14081v1'>2402.14081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13099v1")'>Sparse identification of nonlinear dynamics in the presence of library
  and system uncertainty</div>
<div id='2401.13099v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T21:23:51Z</div><div>Authors: Andrew O'Brien</div><div style='padding-top: 10px; width: 80ex'>The SINDy algorithm has been successfully used to identify the governing
equations of dynamical systems from time series data. However, SINDy assumes
the user has prior knowledge of the variables in the system and of a function
library that can act as a basis for the system. In this paper, we demonstrate
on real world data how the Augmented SINDy algorithm outperforms SINDy in the
presence of system variable uncertainty. We then show SINDy can be further
augmented to perform robustly when both kinds of uncertainty are present.</div><div><a href='http://arxiv.org/abs/2401.13099v1'>2401.13099v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03827v1")'>Linear and nonlinear system identification under $\ell_1$- and
  group-Lasso regularization via L-BFGS-B</div>
<div id='2403.03827v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:17:34Z</div><div>Authors: Alberto Bemporad</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose an approach for identifying linear and nonlinear
discrete-time state-space models, possibly under $\ell_1$- and group-Lasso
regularization, based on the L-BFGS-B algorithm. For the identification of
linear models, we show that, compared to classical linear subspace methods, the
approach often provides better results, is much more general in terms of the
loss and regularization terms used, and is also more stable from a numerical
point of view. The proposed method not only enriches the existing set of linear
system identification tools but can be also applied to identifying a very broad
class of parametric nonlinear state-space models, including recurrent neural
networks. We illustrate the approach on synthetic and experimental datasets and
apply it to solve the challenging industrial robot benchmark for nonlinear
multi-input/multi-output system identification proposed by Weigand et al.
(2022). A Python implementation of the proposed identification method is
available in the package \texttt{jax-sysid}, available at
\url{https://github.com/bemporad/jax-sysid}.</div><div><a href='http://arxiv.org/abs/2403.03827v1'>2403.03827v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00578v1")'>SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study</div>
<div id='2403.00578v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T14:58:36Z</div><div>Authors: Aurelio Raffa Ugolini, Valentina Breschi, Andrea Manzoni, Mara Tanelli</div><div style='padding-top: 10px; width: 80ex'>In this work we analyze the effectiveness of the Sparse Identification of
Nonlinear Dynamics (SINDy) technique on three benchmark datasets for nonlinear
identification, to provide a better understanding of its suitability when
tackling real dynamical systems. While SINDy can be an appealing strategy for
pursuing physics-based learning, our analysis highlights difficulties in
dealing with unobserved states and non-smooth dynamics. Due to the ubiquity of
these features in real systems in general, and control applications in
particular, we complement our analysis with hands-on approaches to tackle these
issues in order to exploit SINDy also in these challenging contexts.</div><div><a href='http://arxiv.org/abs/2403.00578v1'>2403.00578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00646v1")'>Stability-Certified Learning of Control Systems with Quadratic
  Nonlinearities</div>
<div id='2403.00646v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T16:26:47Z</div><div>Authors: Igor Pontes Duff, Pawan Goyal, Peter Benner</div><div style='padding-top: 10px; width: 80ex'>This work primarily focuses on an operator inference methodology aimed at
constructing low-dimensional dynamical models based on a priori hypotheses
about their structure, often informed by established physics or expert
insights. Stability is a fundamental attribute of dynamical systems, yet it is
not always assured in models derived through inference. Our main objective is
to develop a method that facilitates the inference of quadratic control
dynamical systems with inherent stability guarantees. To this aim, we
investigate the stability characteristics of control systems with
energy-preserving nonlinearities, thereby identifying conditions under which
such systems are bounded-input bounded-state stable. These insights are
subsequently applied to the learning process, yielding inferred models that are
inherently stable by design. The efficacy of our proposed framework is
demonstrated through a couple of numerical examples.</div><div><a href='http://arxiv.org/abs/2403.00646v1'>2403.00646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03145v1")'>SafEDMD: A certified learning architecture tailored to data-driven
  control of nonlinear dynamical systems</div>
<div id='2402.03145v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:12:36Z</div><div>Authors: Robin Strässer, Manuel Schaller, Karl Worthmann, Julian Berberich, Frank Allgöwer</div><div style='padding-top: 10px; width: 80ex'>The Koopman operator serves as the theoretical backbone for machine learning
of dynamical control systems, where the operator is heuristically approximated
by extended dynamic mode decomposition (EDMD). In this paper, we propose
Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning
architecture which comes along with rigorous certificates, resulting in a
reliable surrogate model generated in a data-driven fashion. To ensure
trustworthiness of SafEDMD, we derive proportional error bounds, which vanish
at the origin and are tailored for control tasks, leading to certified
controller design based on semi-definite programming. We illustrate the
developed machinery by means of several benchmark examples and highlight the
advantages over state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2402.03145v1'>2402.03145v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10013v1")'>LyZNet: A Lightweight Python Tool for Learning and Verifying Neural
  Lyapunov Functions and Regions of Attraction</div>
<div id='2403.10013v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T04:35:56Z</div><div>Authors: Jun Liu, Yiming Meng, Maxwell Fitzsimmons, Ruikun Zhou</div><div style='padding-top: 10px; width: 80ex'>In this paper, we describe a lightweight Python framework that provides
integrated learning and verification of neural Lyapunov functions for stability
analysis. The proposed tool, named LyZNet, learns neural Lyapunov functions
using physics-informed neural networks (PINNs) to solve Zubov's equation and
verifies them using satisfiability modulo theories (SMT) solvers. What
distinguishes this tool from others in the literature is its ability to provide
verified regions of attraction close to the domain of attraction. This is
achieved by encoding Zubov's partial differential equation (PDE) into the PINN
approach. By embracing the non-convex nature of the underlying optimization
problems, we demonstrate that in cases where convex optimization, such as
semidefinite programming, fails to capture the domain of attraction, our neural
network framework proves more successful. The tool also offers automatic
decomposition of coupled nonlinear systems into a network of low-dimensional
subsystems for compositional verification. We illustrate the tool's usage and
effectiveness with several numerical examples, including both non-trivial
low-dimensional nonlinear systems and high-dimensional systems. The repository
of the tool can be found at https://git.uwaterloo.ca/hybrid-systems-lab/lyznet.</div><div><a href='http://arxiv.org/abs/2403.10013v1'>2403.10013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07621v1")'>Correctness Verification of Neural Networks Approximating Differential
  Equations</div>
<div id='2402.07621v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T12:55:35Z</div><div>Authors: Petros Ellinas, Rahul Nellikath, Ignasi Ventura, Jochen Stiasny, Spyros Chatzivasileiadis</div><div style='padding-top: 10px; width: 80ex'>Verification of Neural Networks (NNs) that approximate the solution of
Partial Differential Equations (PDEs) is a major milestone towards enhancing
their trustworthiness and accelerating their deployment, especially for
safety-critical systems. If successful, such NNs can become integral parts of
simulation software tools which can accelerate the simulation of complex
dynamic systems more than 100 times. However, the verification of these
functions poses major challenges; it is not straightforward how to efficiently
bound them or how to represent the derivative of the NN. This work addresses
both these problems. First, we define the NN derivative as a finite difference
approximation. Then, we formulate the PDE residual bounding problem alongside
the Initial Value Problem's error propagation. Finally, for the first time, we
tackle the problem of bounding an NN function without a priori knowledge of the
output domain. For this, we build a parallel branching algorithm that combines
the incomplete CROWN solver and Gradient Attack for termination and domain
rejection conditions. We demonstrate the strengths and weaknesses of the
proposed framework, and we suggest further work to enhance its efficiency.</div><div><a href='http://arxiv.org/abs/2402.07621v1'>2402.07621v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10998v1")'>Provably Safe Neural Network Controllers via Differential Dynamic Logic</div>
<div id='2402.10998v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:15:25Z</div><div>Authors: Samuel Teuber, Stefan Mitsch, André Platzer</div><div style='padding-top: 10px; width: 80ex'>While neural networks (NNs) have a large potential as goal-oriented
controllers for Cyber-Physical Systems, verifying the safety of neural network
based control systems (NNCSs) poses significant challenges for the practical
use of NNs -- especially when safety is needed for unbounded time horizons. One
reason for this is the intractability of NN and hybrid system analysis. We
introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The
first approach for the combination of differential dynamic logic (dL) and NN
verification. By joining forces, we can exploit the efficiency of NN
verification tools while retaining the rigor of dL. We reflect a safety proof
for a controller envelope in an NN to prove the safety of concrete NNCS on an
infinite-time horizon. The NN verification properties resulting from VerSAILLE
typically require nonlinear arithmetic while efficient NN verification tools
merely support linear arithmetic. To overcome this divide, we present Mosaic:
The first sound and complete verification approach for polynomial real
arithmetic properties on piece-wise linear NNs. Mosaic lifts off-the-shelf
tools for linear properties to the nonlinear setting. An evaluation on case
studies, including adaptive cruise control and airborne collision avoidance,
demonstrates the versatility of VerSAILLE and Mosaic: It supports the
certification of infinite-time horizon safety and the exhaustive enumeration of
counterexample regions while significantly outperforming State-of-the-Art tools
in closed-loop NNV.</div><div><a href='http://arxiv.org/abs/2402.10998v1'>2402.10998v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03314v1")'>Collision Avoidance Verification of Multiagent Systems with Learned
  Policies</div>
<div id='2403.03314v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T20:36:26Z</div><div>Authors: Zihao Dong, Shayegan Omidshafiei, Michael Everett</div><div style='padding-top: 10px; width: 80ex'>For many multiagent control problems, neural networks (NNs) have enabled
promising new capabilities. However, many of these systems lack formal
guarantees (e.g., collision avoidance, robustness), which prevents leveraging
these advances in safety-critical settings. While there is recent work on
formal verification of NN-controlled systems, most existing techniques cannot
handle scenarios with more than one agent. To address this research gap, this
paper presents a backward reachability-based approach for verifying the
collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs).
Given the dynamics models and trained control policies of each agent, the
proposed algorithm computes relative backprojection sets by solving a series of
Mixed Integer Linear Programs (MILPs) offline for each pair of agents. Our
pair-wise approach is parallelizable and thus scales well with increasing
number of agents, and we account for state measurement uncertainties, making it
well aligned with real-world scenarios. Using those results, the agents can
quickly check for collision avoidance online by solving low-dimensional Linear
Programs (LPs). We demonstrate the proposed algorithm can verify collision-free
properties of a MA-NFL with agents trained to imitate a collision avoidance
algorithm (Reciprocal Velocity Obstacles). We further demonstrate the
computational scalability of the approach on systems with up to 10 agents.</div><div><a href='http://arxiv.org/abs/2403.03314v1'>2403.03314v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15107v1")'>Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups</div>
<div id='2401.15107v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T16:26:44Z</div><div>Authors: Yannik P. Wotte, Federico Califano, Stefano Stramigioli</div><div style='padding-top: 10px; width: 80ex'>This work presents a novel approach for the optimization of dynamic systems
on finite-dimensional Lie groups. We rephrase dynamic systems as so-called
neural ordinary differential equations (neural ODEs), and formulate the
optimization problem on Lie groups. A gradient descent optimization algorithm
is presented to tackle the optimization numerically. Our algorithm is scalable,
and applicable to any finite dimensional Lie group, including matrix Lie
groups. By representing the system at the Lie algebra level, we reduce the
computational cost of the gradient computation. In an extensive example,
optimal potential energy shaping for control of a rigid body is treated. The
optimal control problem is phrased as an optimization of a neural ODE on the
Lie group SE(3), and the controller is iteratively optimized. The final
controller is validated on a state-regulation task.</div><div><a href='http://arxiv.org/abs/2401.15107v1'>2401.15107v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14029v1")'>Towards a Systems Theory of Algorithms</div>
<div id='2401.14029v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T09:20:21Z</div><div>Authors: Florian Dörfler, Zhiyu He, Giuseppe Belgioioso, Saverio Bolognani, John Lygeros, Michael Muehlebach</div><div style='padding-top: 10px; width: 80ex'>Traditionally, numerical algorithms are seen as isolated pieces of code
confined to an {\em in silico} existence. However, this perspective is not
appropriate for many modern computational approaches in control, learning, or
optimization, wherein {\em in vivo} algorithms interact with their environment.
Examples of such {\em open} include various real-time optimization-based
control strategies, reinforcement learning, decision-making architectures,
online optimization, and many more. Further, even {\em closed} algorithms in
learning or optimization are increasingly abstracted in block diagrams with
interacting dynamic modules and pipelines. In this opinion paper, we state our
vision on a to-be-cultivated {\em systems theory of algorithms} and argue in
favour of viewing algorithms as open dynamical systems interacting with other
algorithms, physical systems, humans, or databases. Remarkably, the manifold
tools developed under the umbrella of systems theory also provide valuable
insights into this burgeoning paradigm shift and its accompanying challenges in
the algorithmic world. We survey various instances where the principles of
algorithmic systems theory are being developed and outline pertinent modeling,
analysis, and design challenges.</div><div><a href='http://arxiv.org/abs/2401.14029v1'>2401.14029v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15611v1")'>Data/moment-driven approaches for fast predictive control of collective
  dynamics</div>
<div id='2402.15611v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T21:21:16Z</div><div>Authors: Giacomo Albi, Sara Bicego, Michael Herty, Yuyang Huang, Dante Kalise, Chiara Segala</div><div style='padding-top: 10px; width: 80ex'>Feedback control synthesis for large-scale particle systems is reviewed in
the framework of model predictive control (MPC). The high-dimensional character
of collective dynamics hampers the performance of traditional MPC algorithms
based on fast online dynamic optimization at every time step. Two alternatives
to MPC are proposed. First, the use of supervised learning techniques for the
offline approximation of optimal feedback laws is discussed. Then, a procedure
based on sequential linearization of the dynamics based on macroscopic
quantities of the particle ensemble is reviewed. Both approaches circumvent the
online solution of optimal control problems enabling fast, real-time, feedback
synthesis for large-scale particle systems. Numerical experiments assess the
performance of the proposed algorithms.</div><div><a href='http://arxiv.org/abs/2402.15611v1'>2402.15611v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12476v1")'>Bayesian identification of nonseparable Hamiltonians with multiplicative
  noise using deep learning and reduced-order modeling</div>
<div id='2401.12476v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T04:05:26Z</div><div>Authors: Nicholas Galioto, Harsh Sharma, Boris Kramer, Alex Arkady Gorodetsky</div><div style='padding-top: 10px; width: 80ex'>This paper presents a structure-preserving Bayesian approach for learning
nonseparable Hamiltonian systems using stochastic dynamic models allowing for
statistically-dependent, vector-valued additive and multiplicative measurement
noise. The approach is comprised of three main facets. First, we derive a
Gaussian filter for a statistically-dependent, vector-valued, additive and
multiplicative noise model that is needed to evaluate the likelihood within the
Bayesian posterior. Second, we develop a novel algorithm for cost-effective
application of Bayesian system identification to high-dimensional systems.
Third, we demonstrate how structure-preserving methods can be incorporated into
the proposed framework, using nonseparable Hamiltonians as an illustrative
system class. We compare the Bayesian method to a state-of-the-art machine
learning method on a canonical nonseparable Hamiltonian model and a chaotic
double pendulum model with small, noisy training datasets. The results show
that using the Bayesian posterior as a training objective can yield upwards of
724 times improvement in Hamiltonian mean squared error using training data
with up to 10% multiplicative noise compared to a standard training objective.
Lastly, we demonstrate the utility of the novel algorithm for parameter
estimation of a 64-dimensional model of the spatially-discretized nonlinear
Schr\"odinger equation with data corrupted by up to 20% multiplicative noise.</div><div><a href='http://arxiv.org/abs/2401.12476v1'>2401.12476v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14833v1")'>Model order reduction of deep structured state-space models: A
  system-theoretic approach</div>
<div id='2403.14833v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T21:05:59Z</div><div>Authors: Marco Forgione, Manas Mejari, Dario Piga</div><div style='padding-top: 10px; width: 80ex'>With a specific emphasis on control design objectives, achieving accurate
system modeling with limited complexity is crucial in parametric system
identification. The recently introduced deep structured state-space models
(SSM), which feature linear dynamical blocks as key constituent components,
offer high predictive performance. However, the learned representations often
suffer from excessively large model orders, which render them unsuitable for
control design purposes. The current paper addresses this challenge by means of
system-theoretic model order reduction techniques that target the linear
dynamical blocks of SSMs. We introduce two regularization terms which can be
incorporated into the training loss for improved model order reduction. In
particular, we consider modal $\ell_1$ and Hankel nuclear norm regularization
to promote sparsity, allowing one to retain only the relevant states without
sacrificing accuracy. The presented regularizers lead to advantages in terms of
parsimonious representations and faster inference resulting from the reduced
order models. The effectiveness of the proposed methodology is demonstrated
using real-world ground vibration data from an aircraft.</div><div><a href='http://arxiv.org/abs/2403.14833v1'>2403.14833v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14429v1")'>[Re] The Discriminative Kalman Filter for Bayesian Filtering with
  Nonlinear and Non-Gaussian Observation Models</div>
<div id='2401.14429v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T21:00:42Z</div><div>Authors: Josue Casco-Rodriguez, Caleb Kemere, Richard G. Baraniuk</div><div style='padding-top: 10px; width: 80ex'>Kalman filters provide a straightforward and interpretable means to estimate
hidden or latent variables, and have found numerous applications in control,
robotics, signal processing, and machine learning. One such application is
neural decoding for neuroprostheses. In 2020, Burkhart et al. thoroughly
evaluated their new version of the Kalman filter that leverages Bayes' theorem
to improve filter performance for highly non-linear or non-Gaussian observation
models. This work provides an open-source Python alternative to the authors'
MATLAB algorithm. Specifically, we reproduce their most salient results for
neuroscientific contexts and further examine the efficacy of their filter using
multiple random seeds and previously unused trials from the authors' dataset.
All experiments were performed offline on a single computer.</div><div><a href='http://arxiv.org/abs/2401.14429v1'>2401.14429v1</a></div>
</div></div>
    <div><a href="arxiv_7.html">Prev (7)</a></div>
    <div><a href="arxiv_9.html">Next (9)</a></div>
    