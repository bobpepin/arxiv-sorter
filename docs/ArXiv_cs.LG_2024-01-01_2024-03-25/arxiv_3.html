
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15492v1")'>Mechanics-Informed Autoencoder Enables Automated Detection and
  Localization of Unforeseen Structural Damage</div>
<div id='2402.15492v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:31:02Z</div><div>Authors: Xuyang Li, Hamed Bolandi, Mahdi Masmoudi, Talal Salem, Nizar Lajnef, Vishnu Naresh Boddeti</div><div style='padding-top: 10px; width: 80ex'>Structural health monitoring (SHM) is vital for ensuring the safety and
longevity of structures like buildings and bridges. As the volume and scale of
structures and the impact of their failure continue to grow, there is a dire
need for SHM techniques that are scalable, inexpensive, operate passively
without human intervention, and customized for each mechanical structure
without the need for complex baseline models. We present a novel
"deploy-and-forget" approach for automated detection and localization of
damages in structures. It is based on a synergistic combination of fully
passive measurements from inexpensive sensors and a mechanics-informed
autoencoder. Once deployed, our solution continuously learns and adapts a
bespoke baseline model for each structure, learning from its undamaged state's
response characteristics. After learning from just 3 hours of data, it can
autonomously detect and localize different types of unforeseen damage. Results
from numerical simulations and experiments indicate that incorporating the
mechanical characteristics into the variational autoencoder allows for up to
35\% earlier detection and localization of damage over a standard autoencoder.
Our approach holds substantial promise for a significant reduction in human
intervention and inspection costs and enables proactive and preventive
maintenance strategies, thus extending the lifespan, reliability, and
sustainability of civil infrastructures.</div><div><a href='http://arxiv.org/abs/2402.15492v1'>2402.15492v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17657v1")'>An attempt to generate new bridge types from latent space of
  energy-based model</div>
<div id='2401.17657v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T08:21:35Z</div><div>Authors: Hongjun Zhang</div><div style='padding-top: 10px; width: 80ex'>Use energy-based model for bridge-type innovation. The loss function is
explained by the game theory, the logic is clear and the formula is simple and
clear. Thus avoid the use of maximum likelihood estimation to explain the loss
function and eliminate the need for Monte Carlo methods to solve the normalized
denominator. Assuming that the bridge-type population follows a Boltzmann
distribution, a neural network is constructed to represent the energy function.
Use Langevin dynamics technology to generate a new sample with low energy
value, thus a generative model of bridge-type based on energy is established.
Train energy function on symmetric structured image dataset of three span beam
bridge, arch bridge, cable-stayed bridge, and suspension bridge to accurately
calculate the energy values of real and fake samples. Sampling from latent
space, using gradient descent algorithm, the energy function transforms the
sampling points into low energy score samples, thereby generating new bridge
types different from the dataset. Due to unstable and slow training in this
attempt, the possibility of generating new bridge types is rare and the image
definition of generated images is low.</div><div><a href='http://arxiv.org/abs/2401.17657v1'>2401.17657v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05964v1")'>An attempt to generate new bridge types from latent space of PixelCNN</div>
<div id='2401.05964v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T15:06:25Z</div><div>Authors: Hongjun Zhang</div><div style='padding-top: 10px; width: 80ex'>Try to generate new bridge types using generative artificial intelligence
technology. Using symmetric structured image dataset of three-span beam bridge,
arch bridge, cable-stayed bridge and suspension bridge , based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
PixelCNN is constructed and trained. The model can capture the statistical
structure of the images and calculate the probability distribution of the next
pixel when the previous pixels are given. From the obtained latent space
sampling, new bridge types different from the training dataset can be
generated. PixelCNN can organically combine different structural components on
the basis of human original bridge types, creating new bridge types that have a
certain degree of human original ability. Autoregressive models cannot
understand the meaning of the sequence, while multimodal models combine
regression and autoregressive models to understand the sequence. Multimodal
models should be the way to achieve artificial general intelligence in the
future.</div><div><a href='http://arxiv.org/abs/2401.05964v1'>2401.05964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07129v1")'>An attempt to generate new bridge types from latent space of denoising
  diffusion Implicit model</div>
<div id='2402.07129v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T08:54:37Z</div><div>Authors: Hongjun Zhang</div><div style='padding-top: 10px; width: 80ex'>Use denoising diffusion implicit model for bridge-type innovation. The
process of adding noise and denoising to an image can be likened to the process
of a corpse rotting and a detective restoring the scene of a victim being
killed, to help beginners understand. Through an easy-to-understand algebraic
method, derive the function formulas for adding noise and denoising, making it
easier for beginners to master the mathematical principles of the model. Using
symmetric structured image dataset of three-span beam bridge, arch bridge,
cable-stayed bridge and suspension bridge , based on Python programming
language, TensorFlow and Keras deep learning platform framework , denoising
diffusion implicit model is constructed and trained. From the latent space
sampling, new bridge types with asymmetric structures can be generated.
Denoising diffusion implicit model can organically combine different structural
components on the basis of human original bridge types, and create new bridge
types.</div><div><a href='http://arxiv.org/abs/2402.07129v1'>2402.07129v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00700v1")'>An attempt to generate new bridge types from latent space of generative
  adversarial network</div>
<div id='2401.00700v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T08:46:29Z</div><div>Authors: Hongjun Zhang</div><div style='padding-top: 10px; width: 80ex'>Try to generate new bridge types using generative artificial intelligence
technology. Symmetric structured image dataset of three-span beam bridge, arch
bridge, cable-stayed bridge and suspension bridge are used . Based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
as well as Wasserstein loss function and Lipschitz constraints, generative
adversarial network is constructed and trained. From the obtained low
dimensional bridge-type latent space sampling, new bridge types with asymmetric
structures can be generated. Generative adversarial network can create new
bridge types by organically combining different structural components on the
basis of human original bridge types. It has a certain degree of human original
ability. Generative artificial intelligence technology can open up imagination
space and inspire humanity.</div><div><a href='http://arxiv.org/abs/2401.00700v1'>2401.00700v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05972v1")'>Learning physics-based reduced models from data for the
  Hasegawa-Wakatani equations</div>
<div id='2401.05972v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T15:20:06Z</div><div>Authors: Constatin Gahr, Ionut-Gabriel Farcas, Frank Jenko</div><div style='padding-top: 10px; width: 80ex'>This paper focuses on the construction of non-intrusive Scientific Machine
Learning (SciML) Reduced-Order Models (ROMs) for nonlinear, chaotic plasma
turbulence simulations. In particular, we propose using Operator Inference
(OpInf) to build low-cost physics-based ROMs from data for such simulations. As
a representative example, we focus on the Hasegawa-Wakatani (HW) equations used
for modeling two-dimensional electrostatic drift-wave plasma turbulence. For a
comprehensive perspective of the potential of OpInf to construct accurate ROMs
for this model, we consider a setup for the HW equations that leads to the
formation of complex, nonlinear, and self-driven dynamics, and perform two sets
of experiments. We first use the data obtained via a direct numerical
simulation of the HW equations starting from a specific initial condition and
train OpInf ROMs for predictions beyond the training time horizon. In the
second, more challenging set of experiments, we train ROMs using the same
dataset as before but this time perform predictions for six other initial
conditions. Our results show that the OpInf ROMs capture the important features
of the turbulent dynamics and generalize to new and unseen initial conditions
while reducing the evaluation time of the high-fidelity model by up to five
orders of magnitude in single-core performance. In the broader context of
fusion research, this shows that non-intrusive SciML ROMs have the potential to
drastically accelerate numerical studies, which can ultimately enable tasks
such as the design and real-time control of optimized fusion devices.</div><div><a href='http://arxiv.org/abs/2401.05972v1'>2401.05972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05815v1")'>Cheetah: Bridging the Gap Between Machine Learning and Particle
  Accelerator Physics with High-Speed, Differentiable Simulations</div>
<div id='2401.05815v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T10:30:40Z</div><div>Authors: Jan Kaiser, Chenran Xu, Annika Eichler, Andrea Santamaria Garcia</div><div style='padding-top: 10px; width: 80ex'>Machine learning has emerged as a powerful solution to the modern challenges
in accelerator physics. However, the limited availability of beam time, the
computational cost of simulations, and the high-dimensionality of optimisation
problems pose significant challenges in generating the required data for
training state-of-the-art machine learning models. In this work, we introduce
Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code.
Cheetah enables the fast collection of large data sets by reducing computation
times by multiple orders of magnitude and facilitates efficient gradient-based
optimisation for accelerator tuning and system identification. This positions
Cheetah as a user-friendly, readily extensible tool that integrates seamlessly
with widely adopted machine learning tools. We showcase the utility of Cheetah
through five examples, including reinforcement learning training,
gradient-based beamline tuning, gradient-based system identification,
physics-informed Bayesian optimisation priors, and modular neural network
surrogate modelling of space charge effects. The use of such a high-speed
differentiable simulation code will simplify the development of machine
learning-based methods for particle accelerators and fast-track their
integration into everyday operations of accelerator facilities.</div><div><a href='http://arxiv.org/abs/2401.05815v1'>2401.05815v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02879v1")'>Efficient Parameter Optimisation for Quantum Kernel Alignment: A
  Sub-sampling Approach in Variational Training</div>
<div id='2401.02879v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T16:11:34Z</div><div>Authors: M. Emre Sahin, Benjamin C. B. Symons, Pushpak Pati, Fayyaz Minhas, Declan Millar, Maria Gabrani, Jan Lukas Robertus, Stefano Mensa</div><div style='padding-top: 10px; width: 80ex'>Quantum machine learning with quantum kernels for classification problems is
a growing area of research. Recently, quantum kernel alignment techniques that
parameterise the kernel have been developed, allowing the kernel to be trained
and therefore aligned with a specific dataset. While quantum kernel alignment
is a promising technique, it has been hampered by considerable training costs
because the full kernel matrix must be constructed at every training iteration.
Addressing this challenge, we introduce a novel method that seeks to balance
efficiency and performance. We present a sub-sampling training approach that
uses a subset of the kernel matrix at each training step, thereby reducing the
overall computational cost of the training. In this work, we apply the
sub-sampling method to synthetic datasets and a real-world breast cancer
dataset and demonstrate considerable reductions in the number of circuits
required to train the quantum kernel while maintaining classification accuracy.</div><div><a href='http://arxiv.org/abs/2401.02879v1'>2401.02879v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02940v1")'>Digital-analog quantum learning on Rydberg atom arrays</div>
<div id='2401.02940v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T18:31:00Z</div><div>Authors: Jonathan Z. Lu, Lucy Jiao, Kristina Wolinski, Milan Kornjača, Hong-Ye Hu, Sergio Cantu, Fangli Liu, Susanne F. Yelin, Sheng-Tao Wang</div><div style='padding-top: 10px; width: 80ex'>We propose hybrid digital-analog learning algorithms on Rydberg atom arrays,
combining the potentially practical utility and near-term realizability of
quantum learning with the rapidly scaling architectures of neutral atoms. Our
construction requires only single-qubit operations in the digital setting and
global driving according to the Rydberg Hamiltonian in the analog setting. We
perform a comprehensive numerical study of our algorithm on both classical and
quantum data, given respectively by handwritten digit classification and
unsupervised quantum phase boundary learning. We show in the two representative
problems that digital-analog learning is not only feasible in the near term,
but also requires shorter circuit depths and is more robust to realistic error
models as compared to digital learning schemes. Our results suggest that
digital-analog learning opens a promising path towards improved variational
quantum learning experiments in the near term.</div><div><a href='http://arxiv.org/abs/2401.02940v1'>2401.02940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12993v1")'>Simple Full-Spectrum Correlated k-Distribution Model based on Multilayer
  Perceptron</div>
<div id='2403.12993v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:04:01Z</div><div>Authors: Xin Wang, Yucheng Kuang, Chaojun Wang, Hongyuan Di, Boshu He</div><div style='padding-top: 10px; width: 80ex'>While neural networks have been successfully applied to the full-spectrum
k-distribution (FSCK) method at a large range of thermodynamics with k-values
predicted by a trained multilayer perceptron (MLP) model, the required a-values
still need to be calculated on-the-fly, which theoretically degrades the FSCK
method and may lead to errors. On the other hand, too complicated structure of
the current MLP model inevitably slows down the calculation efficiency.
Therefore, to compensate among accuracy, efficiency and storage, the simple MLP
designed based on the nature of FSCK method are developed, i.e., the simple
FSCK MLP (SFM) model, from which those correlated k-values and corresponding
ka-values can be efficiently obtained. Several test cases have been carried out
to compare the developed SFM model and other FSCK tools including look-up
tables and traditional FSCK MLP (TFM) model. Results show that the SFM model
can achieve excellent accuracy that is even better than look-up tables at a
tiny computational cost that is far less than that of TFM model. Considering
accuracy, efficiency and portability, the SFM model is not only an excellent
tool for the prediction of spectral properties, but also provides a method to
reduce the errors due to nonlinear effects.</div><div><a href='http://arxiv.org/abs/2403.12993v1'>2403.12993v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02810v2")'>Physics-Informed Neural Networks for High-Frequency and Multi-Scale
  Problems using Transfer Learning</div>
<div id='2401.02810v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T13:45:08Z</div><div>Authors: Abdul Hannan Mustajab, Hao Lyu, Zarghaam Rizvi, Frank Wuttke</div><div style='padding-top: 10px; width: 80ex'>Physics-informed neural network (PINN) is a data-driven solver for partial
and ordinary differential equations(ODEs/PDEs). It provides a unified framework
to address both forward and inverse problems. However, the complexity of the
objective function often leads to training failures. This issue is particularly
prominent when solving high-frequency and multi-scale problems. We proposed
using transfer learning to boost the robustness and convergence of training
PINN, starting training from low-frequency problems and gradually approaching
high-frequency problems. Through two case studies, we discovered that transfer
learning can effectively train PINN to approximate solutions from low-frequency
problems to high-frequency problems without increasing network parameters.
Furthermore, it requires fewer data points and less training time. We
elaborately described our training strategy, including optimizer selection, and
suggested guidelines for using transfer learning to train neural networks for
solving more complex problems.</div><div><a href='http://arxiv.org/abs/2401.02810v2'>2401.02810v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14609v1")'>Physically Informed Synchronic-adaptive Learning for Industrial Systems
  Modeling in Heterogeneous Media with Unavailable Time-varying Interface</div>
<div id='2401.14609v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T02:48:45Z</div><div>Authors: Aina Wang, Pan Qin, Xi-Ming Sun</div><div style='padding-top: 10px; width: 80ex'>Partial differential equations (PDEs) are commonly employed to model complex
industrial systems characterized by multivariable dependence. Existing
physics-informed neural networks (PINNs) excel in solving PDEs in a homogeneous
medium. However, their feasibility is diminished when PDE parameters are
unknown due to a lack of physical attributions and time-varying interface is
unavailable arising from heterogeneous media. To this end, we propose a
data-physics-hybrid method, physically informed synchronic-adaptive learning
(PISAL), to solve PDEs for industrial systems modeling in heterogeneous media.
First, Net1, Net2, and NetI, are constructed to approximate the solutions
satisfying PDEs and the interface. Net1 and Net2 are utilized to synchronously
learn each solution satisfying PDEs with diverse parameters, while NetI is
employed to adaptively learn the unavailable time-varying interface. Then, a
criterion combined with NetI is introduced to adaptively distinguish the
attributions of measurements and collocation points. Furthermore, NetI is
integrated into a data-physics-hybrid loss function. Accordingly, a
synchronic-adaptive learning (SAL) strategy is proposed to decompose and
optimize each subdomain. Besides, we theoretically prove the approximation
capability of PISAL. Extensive experimental results verify that the proposed
PISAL can be used for industrial systems modeling in heterogeneous media, which
faces the challenges of lack of physical attributions and unavailable
time-varying interface.</div><div><a href='http://arxiv.org/abs/2401.14609v1'>2401.14609v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03444v1")'>Uncertainty quantification for deeponets with ensemble kalman inversion</div>
<div id='2403.03444v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T04:02:30Z</div><div>Authors: Andrew Pensoneault, Xueyu Zhu</div><div style='padding-top: 10px; width: 80ex'>In recent years, operator learning, particularly the DeepONet, has received
much attention for efficiently learning complex mappings between input and
output functions across diverse fields. However, in practical scenarios with
limited and noisy data, accessing the uncertainty in DeepONet predictions
becomes essential, especially in mission-critical or safety-critical
applications. Existing methods, either computationally intensive or yielding
unsatisfactory uncertainty quantification, leave room for developing efficient
and informative uncertainty quantification (UQ) techniques tailored for
DeepONets. In this work, we proposed a novel inference approach for efficient
UQ for operator learning by harnessing the power of the Ensemble Kalman
Inversion (EKI) approach. EKI, known for its derivative-free, noise-robust, and
highly parallelizable feature, has demonstrated its advantages for UQ for
physics-informed neural networks [28]. Our innovative application of EKI
enables us to efficiently train ensembles of DeepONets while obtaining
informative uncertainty estimates for the output of interest. We deploy a
mini-batch variant of EKI to accommodate larger datasets, mitigating the
computational demand due to large datasets during the training stage.
Furthermore, we introduce a heuristic method to estimate the artificial
dynamics covariance, thereby improving our uncertainty estimates. Finally, we
demonstrate the effectiveness and versatility of our proposed methodology
across various benchmark problems, showcasing its potential to address the
pressing challenges of uncertainty quantification in DeepONets, especially for
practical applications with limited and noisy data.</div><div><a href='http://arxiv.org/abs/2403.03444v1'>2403.03444v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15406v1")'>Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty
  Quantification in Deep Operator Networks</div>
<div id='2402.15406v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:07:39Z</div><div>Authors: Christian Moya, Amirhossein Mollaali, Zecheng Zhang, Lu Lu, Guang Lin</div><div style='padding-top: 10px; width: 80ex'>In this paper, we adopt conformal prediction, a distribution-free uncertainty
quantification (UQ) framework, to obtain confidence prediction intervals with
coverage guarantees for Deep Operator Network (DeepONet) regression. Initially,
we enhance the uncertainty quantification frameworks (B-DeepONet and
Prob-DeepONet) previously proposed by the authors by using split conformal
prediction. By combining conformal prediction with our Prob- and B-DeepONets,
we effectively quantify uncertainty by generating rigorous confidence intervals
for DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that
allows for a more natural use of split conformal prediction. We refer to this
distribution-free effective uncertainty quantification framework as split
conformal Quantile-DeepONet regression. Finally, we demonstrate the
effectiveness of the proposed methods using various ordinary, partial
differential equation numerical examples, and multi-fidelity learning.</div><div><a href='http://arxiv.org/abs/2402.15406v1'>2402.15406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01960v2")'>Calibrated Uncertainty Quantification for Operator Learning via
  Conformal Prediction</div>
<div id='2402.01960v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T23:43:28Z</div><div>Authors: Ziqi Ma, Kamyar Azizzadenesheli, Anima Anandkumar</div><div style='padding-top: 10px; width: 80ex'>Operator learning has been increasingly adopted in scientific and engineering
applications, many of which require calibrated uncertainty quantification.
Since the output of operator learning is a continuous function, quantifying
uncertainty simultaneously at all points in the domain is challenging. Current
methods consider calibration at a single point or over one scalar function or
make strong assumptions such as Gaussianity. We propose a risk-controlling
quantile neural operator, a distribution-free, finite-sample functional
calibration conformal prediction method. We provide a theoretical calibration
guarantee on the coverage rate, defined as the expected percentage of points on
the function domain whose true value lies within the predicted uncertainty
ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure
prediction task validate our theoretical results, demonstrating calibrated
coverage and efficient uncertainty bands outperforming baseline methods. In
particular, on the 3D problem, our method is the only one that meets the target
calibration percentage (percentage of test samples for which the uncertainty
estimates are calibrated) of 98%.</div><div><a href='http://arxiv.org/abs/2402.01960v2'>2402.01960v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12475v1")'>Diffeomorphism Neural Operator for various domains and parameters of
  partial differential equations</div>
<div id='2402.12475v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:21:45Z</div><div>Authors: Zhiwei Zhao, Changqing Liu, Yingguang Li, Zhibin Chen, Xu Liu</div><div style='padding-top: 10px; width: 80ex'>Many science and engineering applications demand partial differential
equations (PDE) evaluations that are traditionally computed with
resource-intensive numerical solvers. Neural operator models provide an
efficient alternative by learning the governing physical laws directly from
data in a class of PDEs with different parameters, but constrained in a fixed
boundary (domain). Many applications, such as design and manufacturing, would
benefit from neural operators with flexible domains when studied at scale. Here
we present a diffeomorphism neural operator learning framework towards
developing domain-flexible models for physical systems with various and complex
domains. Specifically, a neural operator trained in a shared domain mapped from
various domains of fields by diffeomorphism is proposed, which transformed the
problem of learning function mappings in varying domains (spaces) into the
problem of learning operators on a shared diffeomorphic domain. Meanwhile, an
index is provided to evaluate the generalization of diffeomorphism neural
operators in different domains by the domain diffeomorphism similarity.
Experiments on statics scenarios (Darcy flow, mechanics) and dynamic scenarios
(pipe flow, airfoil flow) demonstrate the advantages of our approach for neural
operator learning under various domains, where harmonic and volume
parameterization are used as the diffeomorphism for 2D and 3D domains. Our
diffeomorphism neural operator approach enables strong learning capability and
robust generalization across varying domains and parameters.</div><div><a href='http://arxiv.org/abs/2402.12475v1'>2402.12475v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06342v1")'>Separable Physics-informed Neural Networks for Solving the BGK Model of
  the Boltzmann Equation</div>
<div id='2403.06342v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T23:44:55Z</div><div>Authors: Jaemin Oh, Seung Yeon Cho, Seok-Bae Yun, Eunbyung Park, Youngjoon Hong</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce a method based on Separable Physics-Informed
Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann
equation. While the mesh-free nature of PINNs offers significant advantages in
handling high-dimensional partial differential equations (PDEs), challenges
arise when applying quadrature rules for accurate integral evaluation in the
BGK operator, which can compromise the mesh-free benefit and increase
computational costs. To address this, we leverage the canonical polyadic
decomposition structure of SPINNs and the linear nature of moment calculation,
achieving a substantial reduction in computational expense for quadrature rule
application. The multi-scale nature of the particle density function poses
difficulties in precisely approximating macroscopic moments using neural
networks. To improve SPINN training, we introduce the integration of Gaussian
functions into SPINNs, coupled with a relative loss approach. This modification
enables SPINNs to decay as rapidly as Maxwellian distributions, thereby
enhancing the accuracy of macroscopic moment approximations. The relative loss
design further ensures that both large and small-scale features are effectively
captured by the SPINNs. The efficacy of our approach is demonstrated through a
series of five numerical experiments, including the solution to a challenging
3D Riemann problem. These results highlight the potential of our novel method
in efficiently and accurately addressing complex challenges in computational
physics.</div><div><a href='http://arxiv.org/abs/2403.06342v1'>2403.06342v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09084v1")'>Sobolev Training for Operator Learning</div>
<div id='2402.09084v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:57:29Z</div><div>Authors: Namkyeong Cho, Junseung Ryu, Hyung Ju Hwang</div><div style='padding-top: 10px; width: 80ex'>This study investigates the impact of Sobolev Training on operator learning
frameworks for improving model performance. Our research reveals that
integrating derivative information into the loss function enhances the training
process, and we propose a novel framework to approximate derivatives on
irregular meshes in operator learning. Our findings are supported by both
experimental evidence and theoretical analysis. This demonstrates the
effectiveness of Sobolev Training in approximating the solution operators
between infinite-dimensional spaces.</div><div><a href='http://arxiv.org/abs/2402.09084v1'>2402.09084v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15097v2")'>Learning solution operators of PDEs defined on varying domains via
  MIONet</div>
<div id='2402.15097v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T05:11:34Z</div><div>Authors: Shanshan Xiao, Pengzhan Jin, Yifa Tang</div><div style='padding-top: 10px; width: 80ex'>In this work, we propose a method to learn the solution operators of PDEs
defined on varying domains via MIONet, and theoretically justify this method.
We first extend the approximation theory of MIONet to further deal with metric
spaces, establishing that MIONet can approximate mappings with multiple inputs
in metric spaces. Subsequently, we construct a set consisting of some
appropriate regions and provide a metric on this set thus make it a metric
space, which satisfies the approximation condition of MIONet. Building upon the
theoretical foundation, we are able to learn the solution mapping of a PDE with
all the parameters varying, including the parameters of the differential
operator, the right-hand side term, the boundary condition, as well as the
domain. Without loss of generality, we for example perform the experiments for
2-d Poisson equations, where the domains and the right-hand side terms are
varying. The results provide insights into the performance of this method
across convex polygons, polar regions with smooth boundary, and predictions for
different levels of discretization on one task. We also show the additional
result of the fully-parameterized case in the appendix for interested readers.
Reasonably, we point out that this is a meshless method, hence can be flexibly
used as a general solver for a type of PDE.</div><div><a href='http://arxiv.org/abs/2402.15097v2'>2402.15097v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07250v1")'>DIMON: Learning Solution Operators of Partial Differential Equations on
  a Diffeomorphic Family of Domains</div>
<div id='2402.07250v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T17:32:23Z</div><div>Authors: Minglang Yin, Nicolas Charon, Ryan Brody, Lu Lu, Natalia Trayanova, Mauro Maggioni</div><div style='padding-top: 10px; width: 80ex'>The solution of a PDE over varying initial/boundary conditions on multiple
domains is needed in a wide variety of applications, but it is computationally
expensive if the solution is computed de novo whenever the initial/boundary
conditions of the domain change. We introduce a general operator learning
framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn
approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$,
that learns the map from initial/boundary conditions and domain $\Omega_\theta$
to the solution of the PDE, or to specified functionals thereof. DIMON is based
on transporting a given problem (initial/boundary conditions and domain
$\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where
training data from multiple problems is used to learn the map to the solution
on $\Omega_{0}$, which is then re-mapped to the original domain
$\Omega_{\theta}$. We consider several problems to demonstrate the performance
of the framework in learning both static and time-dependent PDEs on non-rigid
geometries; these include solving the Laplace equation, reaction-diffusion
equations, and a multiscale PDE that characterizes the electrical propagation
on the left ventricle. This work paves the way toward the fast prediction of
PDE solutions on a family of domains and the application of neural operators in
engineering and precision medicine.</div><div><a href='http://arxiv.org/abs/2402.07250v1'>2402.07250v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07494v2")'>Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks</div>
<div id='2401.07494v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T06:26:53Z</div><div>Authors: Zihao Wang, P S Pravin, Zhe Wu</div><div style='padding-top: 10px; width: 80ex'>Computational efficiency and adversarial robustness are critical factors in
real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model outperforms existing recurrent units across a spectrum of
engineering tasks in terms of computational efficiency and adversarial
robustness. These tasks encompass a benchmark MNIST image classification,
real-world solar irradiance prediction for Solar PV system planning at LHT
Holdings in Singapore, and real-time Model Predictive Control optimization for
a chemical reactor.</div><div><a href='http://arxiv.org/abs/2401.07494v2'>2401.07494v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17733v1")'>Towards Physical Plausibility in Neuroevolution Systems</div>
<div id='2401.17733v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T10:54:34Z</div><div>Authors: Gabriel Cortês, Nuno Lourenço, Penousal Machado</div><div style='padding-top: 10px; width: 80ex'>The increasing usage of Artificial Intelligence (AI) models, especially Deep
Neural Networks (DNNs), is increasing the power consumption during training and
inference, posing environmental concerns and driving the need for more
energy-efficient algorithms and hardware solutions. This work addresses the
growing energy consumption problem in Machine Learning (ML), particularly
during the inference phase. Even a slight reduction in power usage can lead to
significant energy savings, benefiting users, companies, and the environment.
Our approach focuses on maximizing the accuracy of Artificial Neural Network
(ANN) models using a neuroevolutionary framework whilst minimizing their power
consumption. To do so, power consumption is considered in the fitness function.
We introduce a new mutation strategy that stochastically reintroduces modules
of layers, with power-efficient modules having a higher chance of being chosen.
We introduce a novel technique that allows training two separate models in a
single training step whilst promoting one of them to be more power efficient
than the other while maintaining similar accuracy. The results demonstrate a
reduction in power consumption of ANN models by up to 29.2% without a
significant decrease in predictive performance.</div><div><a href='http://arxiv.org/abs/2401.17733v1'>2401.17733v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11674v1")'>A Fast Algorithm to Simulate Nonlinear Resistive Networks</div>
<div id='2402.11674v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T18:33:48Z</div><div>Authors: Benjamin Scellier</div><div style='padding-top: 10px; width: 80ex'>In the quest for energy-efficient artificial intelligence systems, resistor
networks are attracting interest as an alternative to conventional GPU-based
neural networks. These networks leverage the physics of electrical circuits for
inference and can be optimized with local training techniques such as
equilibrium propagation. Despite their potential advantage in terms of power
consumption, the challenge of efficiently simulating these resistor networks
has been a significant bottleneck to assess their scalability, with current
methods either being limited to linear networks or relying on realistic, yet
slow circuit simulators like SPICE. Assuming ideal circuit elements, we
introduce a novel approach for the simulation of nonlinear resistive networks,
which we frame as a quadratic programming problem with linear inequality
constraints, and which we solve using a fast, exact coordinate descent
algorithm. Our simulation methodology significantly outperforms existing
SPICE-based simulations, enabling the training of networks up to 325 times
larger at speeds 150 times faster, resulting in a 50,000-fold improvement in
the ratio of network size to epoch duration. Our approach, adaptable to other
electrical components, can foster more rapid progress in the simulations of
nonlinear electrical networks.</div><div><a href='http://arxiv.org/abs/2402.11674v1'>2402.11674v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04807v1")'>Mathematics of Neural Networks (Lecture Notes Graduate Course)</div>
<div id='2403.04807v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:45:29Z</div><div>Authors: Bart M. N. Smets</div><div style='padding-top: 10px; width: 80ex'>These are the lecture notes that accompanied the course of the same name that
I taught at the Eindhoven University of Technology from 2021 to 2023. The
course is intended as an introduction to neural networks for mathematics
students at the graduate level and aims to make mathematics students interested
in further researching neural networks. It consists of two parts: first a
general introduction to deep learning that focuses on introducing the field in
a formal mathematical way. The second part provides an introduction to the
theory of Lie groups and homogeneous spaces and how it can be applied to design
neural networks with desirable geometric equivariances. The lecture notes were
made to be as self-contained as possible so as to accessible for any student
with a moderate mathematics background. The course also included coding
tutorials and assignments in the form of a set of Jupyter notebooks that are
publicly available at
https://gitlab.com/bsmetsjr/mathematics_of_neural_networks.</div><div><a href='http://arxiv.org/abs/2403.04807v1'>2403.04807v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01058v1")'>Towards an Algebraic Framework For Approximating Functions Using Neural
  Network Polynomials</div>
<div id='2402.01058v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T23:06:50Z</div><div>Authors: Shakil Rafi, Joshua Lee Padgett, Ukash Nakarmi</div><div style='padding-top: 10px; width: 80ex'>We make the case for neural network objects and extend an already existing
neural network calculus explained in detail in Chapter 2 on \cite{bigbook}. Our
aim will be to show that, yes, indeed, it makes sense to talk about neural
network polynomials, neural network exponentials, sine, and cosines in the
sense that they do indeed approximate their real number counterparts subject to
limitations on certain of their parameters, $q$, and $\varepsilon$. While doing
this, we show that the parameter and depth growth are only polynomial on their
desired accuracy (defined as a 1-norm difference over $\mathbb{R}$), thereby
showing that this approach to approximating, where a neural network in some
sense has the structural properties of the function it is approximating is not
entire intractable.</div><div><a href='http://arxiv.org/abs/2402.01058v1'>2402.01058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00094v1")'>Deep Neural Networks: A Formulation Via Non-Archimedean Analysis</div>
<div id='2402.00094v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T14:49:44Z</div><div>Authors: W. A. Zúñiga-Galindo</div><div style='padding-top: 10px; width: 80ex'>We introduce a new class of deep neural networks (DNNs) with multilayered
tree-like architectures. The architectures are codified using numbers from the
ring of integers of non-Archimdean local fields. These rings have a natural
hierarchical organization as infinite rooted trees. Natural morphisms on these
rings allow us to construct finite multilayered architectures. The new DNNs are
robust universal approximators of real-valued functions defined on the
mentioned rings. We also show that the DNNs are robust universal approximators
of real-valued square-integrable functions defined in the unit interval.</div><div><a href='http://arxiv.org/abs/2402.00094v1'>2402.00094v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07248v1")'>Depth Separations in Neural Networks: Separating the Dimension from the
  Accuracy</div>
<div id='2402.07248v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T17:27:26Z</div><div>Authors: Itay Safran, Daniel Reichman, Paul Valiant</div><div style='padding-top: 10px; width: 80ex'>We prove an exponential separation between depth 2 and depth 3 neural
networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to
constant accuracy, with respect to a distribution with support in $[0,1]^{d}$,
assuming exponentially bounded weights. This addresses an open problem posed in
\citet{safran2019depth}, and proves that the curse of dimensionality manifests
in depth 2 approximation, even in cases where the target function can be
represented efficiently using depth 3. Previously, lower bounds that were used
to separate depth 2 from depth 3 required that at least one of the Lipschitz
parameter, target accuracy or (some measure of) the size of the domain of
approximation scale polynomially with the input dimension, whereas we fix the
former two and restrict our domain to the unit hypercube. Our lower bound holds
for a wide variety of activation functions, and is based on a novel application
of an average- to worst-case random self-reducibility argument, to reduce the
problem to threshold circuits lower bounds.</div><div><a href='http://arxiv.org/abs/2402.07248v1'>2402.07248v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08808v1")'>Depth Separation in Norm-Bounded Infinite-Width Neural Networks</div>
<div id='2402.08808v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T21:26:38Z</div><div>Authors: Suzanna Parkinson, Greg Ongie, Rebecca Willett, Ohad Shamir, Nathan Srebro</div><div style='padding-top: 10px; width: 80ex'>We study depth separation in infinite-width neural networks, where complexity
is controlled by the overall squared $\ell_2$-norm of the weights (sum of
squares of all weights in the network). Whereas previous depth separation
results focused on separation in terms of width, such results do not give
insight into whether depth determines if it is possible to learn a network that
generalizes well even when the network width is unbounded. Here, we study
separation in terms of the sample complexity required for learnability.
Specifically, we show that there are functions that are learnable with sample
complexity polynomial in the input dimension by norm-controlled depth-3 ReLU
networks, yet are not learnable with sub-exponential sample complexity by
norm-controlled depth-2 ReLU networks (with any value for the norm). We also
show that a similar statement in the reverse direction is not possible: any
function learnable with polynomial sample complexity by a norm-controlled
depth-2 ReLU network with infinite width is also learnable with polynomial
sample complexity by a norm-controlled depth-3 ReLU network.</div><div><a href='http://arxiv.org/abs/2402.08808v1'>2402.08808v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15315v1")'>On Minimal Depth in Neural Networks</div>
<div id='2402.15315v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:34:03Z</div><div>Authors: Juan L. Valerdi</div><div style='padding-top: 10px; width: 80ex'>A characterization of the representability of neural networks is relevant to
comprehend their success in artificial intelligence. This study investigate two
topics on ReLU neural network expressivity and their connection with a
conjecture related to the minimum depth required for representing any
continuous piecewise linear function (CPWL). The topics are the minimal depth
representation of the sum and max operations, as well as the exploration of
polytope neural networks. For the sum operation, we establish a sufficient
condition on the minimal depth of the operands to find the minimal depth of the
operation. In contrast, regarding the max operation, a comprehensive set of
examples is presented, demonstrating that no sufficient conditions, depending
solely on the depth of the operands, would imply a minimal depth for the
operation. The study also examine the minimal depth relationship between convex
CPWL functions. On polytope neural networks, we investigate several fundamental
properties, deriving results equivalent to those of ReLU networks, such as
depth inclusions and depth computation from vertices. Notably, we compute the
minimal depth of simplices, which is strictly related to the minimal depth
conjecture in ReLU networks.</div><div><a href='http://arxiv.org/abs/2402.15315v1'>2402.15315v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11871v1")'>The Real Tropical Geometry of Neural Networks</div>
<div id='2403.11871v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:24:47Z</div><div>Authors: Marie-Charlotte Brandenburg, Georg Loho, Guido Montúfar</div><div style='padding-top: 10px; width: 80ex'>We consider a binary classifier defined as the sign of a tropical rational
function, that is, as the difference of two convex piecewise linear functions.
The parameter space of ReLU neural networks is contained as a semialgebraic set
inside the parameter space of tropical rational functions. We initiate the
study of two different subdivisions of this parameter space: a subdivision into
semialgebraic sets, on which the combinatorial type of the decision boundary is
fixed, and a subdivision into a polyhedral fan, capturing the combinatorics of
the partitions of the dataset. The sublevel sets of the 0/1-loss function arise
as subfans of this classification fan, and we show that the level-sets are not
necessarily connected. We describe the classification fan i) geometrically, as
normal fan of the activation polytope, and ii) combinatorially through a list
of properties of associated bipartite graphs, in analogy to covector axioms of
oriented matroids and tropical oriented matroids. Our findings extend and
refine the connection between neural networks and tropical geometry by
observing structures established in real tropical geometry, such as positive
tropicalizations of hypersurfaces and tropical semialgebraic sets.</div><div><a href='http://arxiv.org/abs/2403.11871v1'>2403.11871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00949v1")'>Geometry of Polynomial Neural Networks</div>
<div id='2402.00949v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T19:06:06Z</div><div>Authors: Kaie Kubjas, Jiayi Li, Maximilian Wiesmann</div><div style='padding-top: 10px; width: 80ex'>We study the expressivity and learning process for polynomial neural networks
(PNNs) with monomial activation functions. The weights of the network
parametrize the neuromanifold. In this paper, we study certain neuromanifolds
using tools from algebraic geometry: we give explicit descriptions as
semialgebraic sets and characterize their Zariski closures, called
neurovarieties. We study their dimension and associate an algebraic degree, the
learning degree, to the neurovariety. The dimension serves as a geometric
measure for the expressivity of the network, the learning degree is a measure
for the complexity of training the network and provides upper bounds on the
number of learnable functions. These theoretical results are accompanied with
experiments.</div><div><a href='http://arxiv.org/abs/2402.00949v1'>2402.00949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16613v1")'>Algebraic Complexity and Neurovariety of Linear Convolutional Networks</div>
<div id='2401.16613v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T23:00:15Z</div><div>Authors: Vahid Shahverdi</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study linear convolutional networks with one-dimensional
filters and arbitrary strides. The neuromanifold of such a network is a
semialgebraic set, represented by a space of polynomials admitting specific
factorizations. Introducing a recursive algorithm, we generate polynomial
equations whose common zero locus corresponds to the Zariski closure of the
corresponding neuromanifold. Furthermore, we explore the algebraic complexity
of training these networks employing tools from metric algebraic geometry. Our
findings reveal that the number of all complex critical points in the
optimization of such a network is equal to the generic Euclidean distance
degree of a Segre variety. Notably, this count significantly surpasses the
number of critical points encountered in the training of a fully connected
linear network with the same number of parameters.</div><div><a href='http://arxiv.org/abs/2401.16613v1'>2401.16613v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16655v2")'>Rademacher Complexity of Neural ODEs via Chen-Fliess Series</div>
<div id='2401.16655v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T01:18:41Z</div><div>Authors: Joshua Hanson, Maxim Raginsky</div><div style='padding-top: 10px; width: 80ex'>We show how continuous-depth neural ODE models can be framed as single-layer,
infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs.
In this net, the output "weights" are taken from the signature of the control
input -- a tool used to represent infinite-dimensional paths as a sequence of
tensors -- which comprises iterated integrals of the control input over a
simplex. The "features" are taken to be iterated Lie derivatives of the output
function with respect to the vector fields in the controlled ODE model. The
main result of this work applies this framework to derive compact expressions
for the Rademacher complexity of ODE models that map an initial condition to a
scalar output at some terminal time. The result leverages the straightforward
analysis afforded by single-layer architectures. We conclude with some examples
instantiating the bound for some specific systems and discuss potential
follow-up work.</div><div><a href='http://arxiv.org/abs/2401.16655v2'>2401.16655v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11938v1")'>State space representations of the Roesser type for convolutional layers</div>
<div id='2403.11938v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:35:13Z</div><div>Authors: Patricia Pauli, Dennis Gramlich, Fran Allgöwer</div><div style='padding-top: 10px; width: 80ex'>From the perspective of control theory, convolutional layers (of neural
networks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual
representation of convolutional layers by the convolution kernel corresponds to
the representation of a dynamical system by its impulse response. However, many
analysis tools from control theory, e.g., involving linear matrix inequalities,
require a state space representation. For this reason, we explicitly provide a
state space representation of the Roesser type for 2-D convolutional layers
with $c_\mathrm{in}r_1 + c_\mathrm{out}r_2$ states, where
$c_\mathrm{in}$/$c_\mathrm{out}$ is the number of input/output channels of the
layer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel.
This representation is shown to be minimal for $c_\mathrm{in} =
c_\mathrm{out}$. We further construct state space representations for dilated,
strided, and N-D convolutions.</div><div><a href='http://arxiv.org/abs/2403.11938v1'>2403.11938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08751v1")'>Nearest Neighbor Representations of Neural Circuits</div>
<div id='2402.08751v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:38:01Z</div><div>Authors: Kordag Mehmet Kilic, Jin Sima, Jehoshua Bruck</div><div style='padding-top: 10px; width: 80ex'>Neural networks successfully capture the computational power of the human
brain for many tasks. Similarly inspired by the brain architecture, Nearest
Neighbor (NN) representations is a novel approach of computation. We establish
a firmer correspondence between NN representations and neural networks.
Although it was known how to represent a single neuron using NN
representations, there were no results even for small depth neural networks.
Specifically, for depth-2 threshold circuits, we provide explicit constructions
for their NN representation with an explicit bound on the number of bits to
represent it. Example functions include NN representations of convex polytopes
(AND of threshold gates), IP2, OR of threshold gates, and linear or exact
decision lists.</div><div><a href='http://arxiv.org/abs/2402.08751v1'>2402.08751v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08748v1")'>Nearest Neighbor Representations of Neurons</div>
<div id='2402.08748v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:33:41Z</div><div>Authors: Kordag Mehmet Kilic, Jin Sima, Jehoshua Bruck</div><div style='padding-top: 10px; width: 80ex'>The Nearest Neighbor (NN) Representation is an emerging computational model
that is inspired by the brain. We study the complexity of representing a neuron
(threshold function) using the NN representations. It is known that two anchors
(the points to which NN is computed) are sufficient for a NN representation of
a threshold function, however, the resolution (the maximum number of bits
required for the entries of an anchor) is $O(n\log{n})$. In this work, the
trade-off between the number of anchors and the resolution of a NN
representation of threshold functions is investigated. We prove that the
well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which
require 2 or 3 anchors and resolution of $O(n)$, can be represented by
polynomially large number of anchors in $n$ and $O(\log{n})$ resolution. We
conjecture that for all threshold functions, there are NN representations with
polynomially large size and logarithmic resolution in $n$.</div><div><a href='http://arxiv.org/abs/2402.08748v1'>2402.08748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15121v1")'>Expressive Power of ReLU and Step Networks under Floating-Point
  Operations</div>
<div id='2401.15121v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T05:59:40Z</div><div>Authors: Yeachan Park, Geonho Hwang, Wonyeol Lee, Sejun Park</div><div style='padding-top: 10px; width: 80ex'>The study of the expressive power of neural networks has investigated the
fundamental limits of neural networks. Most existing results assume real-valued
inputs and parameters as well as exact operations during the evaluation of
neural networks. However, neural networks are typically executed on computers
that can only represent a tiny subset of the reals and apply inexact
operations. In this work, we analyze the expressive power of neural networks
under a more realistic setup: when we use floating-point numbers and
operations. Our first set of results assumes floating-point operations where
the significand of a float is represented by finite bits but its exponent can
take any integer value. Under this setup, we show that neural networks using a
binary threshold unit or ReLU can memorize any finite input/output pairs and
can approximate any continuous function within a small error. We also show
similar results on memorization and universal approximation when floating-point
operations use finite bits for both significand and exponent; these results are
applicable to many popular floating-point formats such as those defined in the
IEEE 754 standard (e.g., 32-bit single-precision format) and bfloat16.</div><div><a href='http://arxiv.org/abs/2401.15121v1'>2401.15121v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16418v1")'>Boolean Logic as an Error feedback mechanism</div>
<div id='2401.16418v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:56:21Z</div><div>Authors: Louis Leconte</div><div style='padding-top: 10px; width: 80ex'>The notion of Boolean logic backpropagation was introduced to build neural
networks with weights and activations being Boolean numbers. Most of
computations can be done with Boolean logic instead of real arithmetic, both
during training and inference phases. But the underlying discrete optimization
problem is NP-hard, and the Boolean logic has no guarantee. In this work we
propose the first convergence analysis, under standard non-convex assumptions.</div><div><a href='http://arxiv.org/abs/2401.16418v1'>2401.16418v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02277v1")'>Universal Approximation Theorem for Vector- and Hypercomplex-Valued
  Neural Networks</div>
<div id='2401.02277v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T13:56:13Z</div><div>Authors: Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira</div><div style='padding-top: 10px; width: 80ex'>The universal approximation theorem states that a neural network with one
hidden layer can approximate continuous functions on compact sets with any
desired precision. This theorem supports using neural networks for various
applications, including regression and classification tasks. Furthermore, it is
valid for real-valued neural networks and some hypercomplex-valued neural
networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural
networks. However, hypercomplex-valued neural networks are a type of
vector-valued neural network defined on an algebra with additional algebraic or
geometric properties. This paper extends the universal approximation theorem
for a wide range of vector-valued neural networks, including
hypercomplex-valued models as particular instances. Precisely, we introduce the
concept of non-degenerate algebra and state the universal approximation theorem
for neural networks defined on such algebras.</div><div><a href='http://arxiv.org/abs/2401.02277v1'>2401.02277v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15092v1")'>A note on the capacity of the binary perceptron</div>
<div id='2401.15092v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T20:15:12Z</div><div>Authors: Dylan J. Altschuler, Konstantin Tikhomirov</div><div style='padding-top: 10px; width: 80ex'>Determining the capacity $\alpha_c$ of the Binary Perceptron is a
long-standing problem. Krauth and Mezard (1989) conjectured an explicit value
of $\alpha_c$, approximately equal to .833, and a rigorous lower bound matching
this prediction was recently established by Ding and Sun (2019). Regarding the
upper bound, Kim and Roche (1998) and Talagrand (1999) independently showed
that $\alpha_c$ &lt; .996, while Krauth and Mezard outlined an argument which can
be used to show that $\alpha_c$ &lt; .847. The purpose of this expository note is
to record a complete proof of the bound $\alpha_c$ &lt; .847. The proof is a
conditional first moment method combined with known results on the spherical
perceptron</div><div><a href='http://arxiv.org/abs/2401.15092v1'>2401.15092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08856v1")'>Approximation of relation functions and attention mechanisms</div>
<div id='2402.08856v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T23:53:47Z</div><div>Authors: Awni Altabaa, John Lafferty</div><div style='padding-top: 10px; width: 80ex'>Inner products of neural network feature maps arises in a wide variety of
machine learning frameworks as a method of modeling relations between inputs.
This work studies the approximation properties of inner products of neural
networks. It is shown that the inner product of a multi-layer perceptron with
itself is a universal approximator for symmetric positive-definite relation
functions. In the case of asymmetric relation functions, it is shown that the
inner product of two different multi-layer perceptrons is a universal
approximator. In both cases, a bound is obtained on the number of neurons
required to achieve a given accuracy of approximation. In the symmetric case,
the function class can be identified with kernels of reproducing kernel Hilbert
spaces, whereas in the asymmetric case the function class can be identified
with kernels of reproducing kernel Banach spaces. Finally, these approximation
results are applied to analyzing the attention mechanism underlying
Transformers, showing that any retrieval mechanism defined by an abstract
preorder can be approximated by attention through its inner product relations.
This result uses the Debreu representation theorem in economics to represent
preference relations in terms of utility functions.</div><div><a href='http://arxiv.org/abs/2402.08856v1'>2402.08856v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05249v1")'>On Representing Electronic Wave Functions with Sign Equivariant Neural
  Networks</div>
<div id='2403.05249v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T12:13:11Z</div><div>Authors: Nicholas Gao, Stephan Günnemann</div><div style='padding-top: 10px; width: 80ex'>Recent neural networks demonstrated impressively accurate approximations of
electronic ground-state wave functions. Such neural networks typically consist
of a permutation-equivariant neural network followed by a
permutation-antisymmetric operation to enforce the electronic exchange
symmetry. While accurate, such neural networks are computationally expensive.
In this work, we explore the flipped approach, where we first compute
antisymmetric quantities based on the electronic coordinates and then apply
sign equivariant neural networks to preserve the antisymmetry. While this
approach promises acceleration thanks to the lower-dimensional representation,
we demonstrate that it reduces to a Jastrow factor, a commonly used
permutation-invariant multiplicative factor in the wave function. Our empirical
results support this further, finding little to no improvements over baselines.
We conclude with neither theoretical nor empirical advantages of sign
equivariant functions for representing electronic wave functions within the
evaluation of this work.</div><div><a href='http://arxiv.org/abs/2403.05249v1'>2403.05249v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11942v3")'>The effect of Leaky ReLUs on the training and generalization of
  overparameterized networks</div>
<div id='2402.11942v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:30:06Z</div><div>Authors: Yinglong Guo, Shaohan Li, Gilad Lerman</div><div style='padding-top: 10px; width: 80ex'>We investigate the training and generalization errors of overparameterized
neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU)
functions. More specifically, we carefully upper bound both the convergence
rate of the training error and the generalization error of such NNs and
investigate the dependence of these bounds on the Leaky ReLU parameter,
$\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value
activation function, is optimal for the training error bound. Furthermore, in
special settings, it is also optimal for the generalization error bound.
Numerical experiments empirically support the practical choices guided by the
theory.</div><div><a href='http://arxiv.org/abs/2402.11942v3'>2402.11942v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12610v1")'>The twin peaks of learning neural networks</div>
<div id='2401.12610v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:09:14Z</div><div>Authors: Elizaveta Demyanenko, Christoph Feinauer, Enrico M. Malatesta, Luca Saglietti</div><div style='padding-top: 10px; width: 80ex'>Recent works demonstrated the existence of a double-descent phenomenon for
the generalization error of neural networks, where highly overparameterized
models escape overfitting and achieve good test performance, at odds with the
standard bias-variance trade-off described by statistical learning theory. In
the present work, we explore a link between this phenomenon and the increase of
complexity and sensitivity of the function represented by neural networks. In
particular, we study the Boolean mean dimension (BMD), a metric developed in
the context of Boolean function analysis. Focusing on a simple teacher-student
setting for the random feature model, we derive a theoretical analysis based on
the replica method that yields an interpretable expression for the BMD, in the
high dimensional regime where the number of data points, the number of
features, and the input size grow to infinity. We find that, as the degree of
overparameterization of the network is increased, the BMD reaches an evident
peak at the interpolation threshold, in correspondence with the generalization
error peak, and then slowly approaches a low asymptotic value. The same
phenomenology is then traced in numerical experiments with different model
classes and training setups. Moreover, we find empirically that adversarially
initialized models tend to show higher BMD values, and that models that are
more robust to adversarial attacks exhibit a lower BMD.</div><div><a href='http://arxiv.org/abs/2401.12610v1'>2401.12610v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09184v1")'>A Two-Scale Complexity Measure for Deep Learning Models</div>
<div id='2401.09184v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T12:50:50Z</div><div>Authors: Massimiliano Datres, Gian Paolo Leonardi, Alessio Figalli, David Sutter</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel capacity measure 2sED for statistical models based on
the effective dimension. The new quantity provably bounds the generalization
error under mild assumptions on the model. Furthermore, simulations on standard
data sets and popular model architectures show that 2sED correlates well with
the training error. For Markovian models, we show how to efficiently
approximate 2sED from below through a layerwise iterative approach, which
allows us to tackle deep learning models with a large number of parameters.
Simulation results suggest that the approximation is good for different
prominent models and data sets.</div><div><a href='http://arxiv.org/abs/2401.09184v1'>2401.09184v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06235v1")'>Probabilistic Neural Circuits</div>
<div id='2403.06235v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T15:25:49Z</div><div>Authors: Pedro Zuidberg Dos Martires</div><div style='padding-top: 10px; width: 80ex'>Probabilistic circuits (PCs) have gained prominence in recent years as a
versatile framework for discussing probabilistic models that support tractable
queries and are yet expressive enough to model complex probability
distributions. Nevertheless, tractability comes at a cost: PCs are less
expressive than neural networks. In this paper we introduce probabilistic
neural circuits (PNCs), which strike a balance between PCs and neural nets in
terms of tractability and expressive power. Theoretically, we show that PNCs
can be interpreted as deep mixtures of Bayesian networks. Experimentally, we
demonstrate that PNCs constitute powerful function approximators.</div><div><a href='http://arxiv.org/abs/2403.06235v1'>2403.06235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00759v1")'>Building Expressive and Tractable Probabilistic Generative Models: A
  Review</div>
<div id='2402.00759v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:49:27Z</div><div>Authors: Sahil Sidheekh, Sriraam Natarajan</div><div style='padding-top: 10px; width: 80ex'>We present a comprehensive survey of the advancements and techniques in the
field of tractable probabilistic generative modeling, primarily focusing on
Probabilistic Circuits (PCs). We provide a unified perspective on the inherent
trade-offs between expressivity and the tractability, highlighting the design
principles and algorithmic extensions that have enabled building expressive and
efficient PCs, and provide a taxonomy of the field. We also discuss recent
efforts to build deep and hybrid PCs by fusing notions from deep neural models,
and outline the challenges and open questions that can guide future research in
this evolving field.</div><div><a href='http://arxiv.org/abs/2402.00759v1'>2402.00759v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13125v1")'>Probabilistic Circuits with Constraints via Convex Optimization</div>
<div id='2403.13125v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:55:38Z</div><div>Authors: Soroush Ghandi, Benjamin Quost, Cassio de Campos</div><div style='padding-top: 10px; width: 80ex'>This work addresses integrating probabilistic propositional logic constraints
into the distribution encoded by a probabilistic circuit (PC). PCs are a class
of tractable models that allow efficient computations (such as conditional and
marginal probabilities) while achieving state-of-the-art performance in some
domains. The proposed approach takes both a PC and constraints as inputs, and
outputs a new PC that satisfies the constraints. This is done efficiently via
convex optimization without the need to retrain the entire model. Empirical
evaluations indicate that the combination of constraints and PCs can have
multiple use cases, including the improvement of model performance under scarce
or incomplete data, as well as the enforcement of machine learning fairness
measures into the model without compromising model fitness. We believe that
these ideas will open possibilities for multiple other applications involving
the combination of logics and deep probabilistic models.</div><div><a href='http://arxiv.org/abs/2403.13125v1'>2403.13125v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03621v1")'>Neural Network Approximators for Marginal MAP in Probabilistic Circuits</div>
<div id='2402.03621v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T01:15:06Z</div><div>Authors: Shivvrat Arya, Tahrima Rahman, Vibhav Gogate</div><div style='padding-top: 10px; width: 80ex'>Probabilistic circuits (PCs) such as sum-product networks efficiently
represent large multi-variate probability distributions. They are preferred in
practice over other probabilistic representations such as Bayesian and Markov
networks because PCs can solve marginal inference (MAR) tasks in time that
scales linearly in the size of the network. Unfortunately, the
maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in
these models. Inspired by the recent work on using neural networks for
generating near-optimal solutions to optimization problems such as integer
linear programming, we propose an approach that uses neural networks to
approximate (M)MAP inference in PCs. The key idea in our approach is to
approximate the cost of an assignment to the query variables using a continuous
multilinear function, and then use the latter as a loss function. The two main
benefits of our new method are that it is self-supervised and after the neural
network is learned, it requires only linear time to output a solution. We
evaluate our new approach on several benchmark datasets and show that it
outperforms three competing linear time approximations, max-product inference,
max-marginal inference and sequential estimation, which are used in practice to
solve MMAP tasks in PCs.</div><div><a href='http://arxiv.org/abs/2402.03621v1'>2402.03621v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12113v1")'>Extracting Formulae in Many-Valued Logic from Deep Neural Networks</div>
<div id='2401.12113v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:51:01Z</div><div>Authors: Yani Zhang, Helmut Bölcskei</div><div style='padding-top: 10px; width: 80ex'>We propose a new perspective on deep ReLU networks, namely as circuit
counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV)
generalization of Boolean logic. An algorithm for extracting formulae in MV
logic from deep ReLU networks is presented. As the algorithm applies to
networks with general, in particular also real-valued, weights, it can be used
to extract logical formulae from deep ReLU networks trained on data.</div><div><a href='http://arxiv.org/abs/2401.12113v1'>2401.12113v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03254v1")'>Minimum Description Length and Generalization Guarantees for
  Representation Learning</div>
<div id='2402.03254v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:12:28Z</div><div>Authors: Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski</div><div style='padding-top: 10px; width: 80ex'>A major challenge in designing efficient statistical supervised learning
algorithms is finding representations that perform well not only on available
training samples but also on unseen data. While the study of representation
learning has spurred much interest, most existing such approaches are
heuristic; and very little is known about theoretical generalization
guarantees.
  In this paper, we establish a compressibility framework that allows us to
derive upper bounds on the generalization error of a representation learning
algorithm in terms of the "Minimum Description Length" (MDL) of the labels or
the latent variables (representations). Rather than the mutual information
between the encoder's input and the representation, which is often believed to
reflect the algorithm's generalization capability in the related literature but
in fact, falls short of doing so, our new bounds involve the "multi-letter"
relative entropy between the distribution of the representations (or labels) of
the training and test sets and a fixed prior. In particular, these new bounds
reflect the structure of the encoder and are not vacuous for deterministic
algorithms. Our compressibility approach, which is information-theoretic in
nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two
essential ingredients: block-coding and lossy-compression. The latter allows
our approach to subsume the so-called geometrical compressibility as a special
case. To the best knowledge of the authors, the established generalization
bounds are the first of their kind for Information Bottleneck (IB) type
encoders and representation learning. Finally, we partly exploit the
theoretical results by introducing a new data-dependent prior. Numerical
simulations illustrate the advantages of well-chosen such priors over classical
priors used in IB.</div><div><a href='http://arxiv.org/abs/2402.03254v1'>2402.03254v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12588v1")'>Machine Learning of the Prime Distribution</div>
<div id='2403.12588v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T09:47:54Z</div><div>Authors: Alexander Kolpakov, Aidan Rocke</div><div style='padding-top: 10px; width: 80ex'>In the present work we use maximum entropy methods to derive several theorems
in probabilistic number theory, including a version of the Hardy-Ramanujan
Theorem. We also provide a theoretical argument explaining the experimental
observations of Y.-H. He about the learnability of primes, and posit that the
Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning
techniques. Numerical experiments that we perform corroborate our theoretical
findings.</div><div><a href='http://arxiv.org/abs/2403.12588v1'>2403.12588v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08946v1")'>Measuring Sharpness in Grokking</div>
<div id='2402.08946v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T05:22:53Z</div><div>Authors: Jack Miller, Patrick Gleeson, Charles O'Neill, Thang Bui, Noam Levi</div><div style='padding-top: 10px; width: 80ex'>Neural networks sometimes exhibit grokking, a phenomenon where perfect or
near-perfect performance is achieved on a validation set well after the same
performance has been obtained on the corresponding training set. In this
workshop paper, we introduce a robust technique for measuring grokking, based
on fitting an appropriate functional form. We then use this to investigate the
sharpness of transitions in training and validation accuracy under two
settings. The first setting is the theoretical framework developed by Levi et
al. (2023) where closed form expressions are readily accessible. The second
setting is a two-layer MLP trained to predict the parity of bits, with grokking
induced by the concealment strategy of Miller et al. (2023). We find that
trends between relative grokking gap and grokking sharpness are similar in both
settings when using absolute and relative measures of sharpness. Reflecting on
this, we make progress toward explaining some trends and identify the need for
further study to untangle the various mechanisms which influence the sharpness
of grokking.</div><div><a href='http://arxiv.org/abs/2402.08946v1'>2402.08946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08573v1")'>Two Tales of Single-Phase Contrastive Hebbian Learning</div>
<div id='2402.08573v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:21:18Z</div><div>Authors: Rasmus Kjær Høier, Christopher Zach</div><div style='padding-top: 10px; width: 80ex'>The search for "biologically plausible" learning algorithms has converged on
the idea of representing gradients as activity differences. However, most
approaches require a high degree of synchronization (distinct phases during
learning) and introduce substantial computational overhead, which raises doubts
regarding their biological plausibility as well as their potential utility for
neuromorphic computing. Furthermore, they commonly rely on applying
infinitesimal perturbations (nudges) to output units, which is impractical in
noisy environments. Recently it has been shown that by modelling artificial
neurons as dyads with two oppositely nudged compartments, it is possible for a
fully local learning algorithm named ``dual propagation'' to bridge the
performance gap to backpropagation, without requiring separate learning phases
or infinitesimal nudging. However, the algorithm has the drawback that its
numerical stability relies on symmetric nudging, which may be restrictive in
biological and analog implementations. In this work we first provide a solid
foundation for the objective underlying the dual propagation method, which also
reveals a surprising connection with adversarial robustness. Second, we
demonstrate how dual propagation is related to a particular adjoint state
method, which is stable regardless of asymmetric nudging.</div><div><a href='http://arxiv.org/abs/2402.08573v1'>2402.08573v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03625v1")'>Convex Relaxations of ReLU Neural Networks Approximate Global Optima in
  Polynomial Time</div>
<div id='2402.03625v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T01:29:35Z</div><div>Authors: Sungyoon Kim, Mert Pilanci</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the optimality gap between two-layer ReLU networks
regularized with weight decay and their convex relaxations. We show that when
the training data is random, the relative optimality gap between the original
problem and its relaxation can be bounded by a factor of $O(\sqrt{\log n})$,
where $n$ is the number of training samples. A simple application leads to a
tractable polynomial-time algorithm that is guaranteed to solve the original
non-convex problem up to a logarithmic factor. Moreover, under mild
assumptions, we show that with random initialization on the parameters local
gradient methods almost surely converge to a point that has low training loss.
Our result is an exponential improvement compared to existing results and sheds
new light on understanding why local gradient methods work well.</div><div><a href='http://arxiv.org/abs/2402.03625v1'>2402.03625v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07606v1")'>RedEx: Beyond Fixed Representation Methods via Convex Optimization</div>
<div id='2401.07606v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T11:30:11Z</div><div>Authors: Amit Daniely, Mariano Schain, Gilad Yehudai</div><div style='padding-top: 10px; width: 80ex'>Optimizing Neural networks is a difficult task which is still not well
understood. On the other hand, fixed representation methods such as kernels and
random features have provable optimization guarantees but inferior performance
due to their inherent inability to learn the representations. In this paper, we
aim at bridging this gap by presenting a novel architecture called RedEx
(Reduced Expander Extractor) that is as expressive as neural networks and can
also be trained in a layer-wise fashion via a convex program with semi-definite
constraints and optimization guarantees. We also show that RedEx provably
surpasses fixed representation methods, in the sense that it can efficiently
learn a family of target functions which fixed representation methods cannot.</div><div><a href='http://arxiv.org/abs/2401.07606v1'>2401.07606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15141v1")'>A note on the adjoint method for neural ordinary differential equation
  network</div>
<div id='2402.15141v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:55:34Z</div><div>Authors: Pipi Hu</div><div style='padding-top: 10px; width: 80ex'>Perturbation and operator adjoint method are used to give the right adjoint
form rigourously. From the derivation, we can have following results: 1) The
loss gradient is not an ODE, it is an integral and we shows the reason; 2) The
traditional adjoint form is not equivalent with the back propagation results.
3) The adjoint operator analysis shows that if and only if the discrete adjoint
has the same scheme with the discrete neural ODE, the adjoint form would give
the same results as BP does.</div><div><a href='http://arxiv.org/abs/2402.15141v1'>2402.15141v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13108v1")'>On the Stability of Gradient Descent for Large Learning Rate</div>
<div id='2402.13108v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T16:01:42Z</div><div>Authors: Alexandru Crăciun, Debarghya Ghoshdastidar</div><div style='padding-top: 10px; width: 80ex'>There currently is a significant interest in understanding the Edge of
Stability (EoS) phenomenon, which has been observed in neural networks
training, characterized by a non-monotonic decrease of the loss function over
epochs, while the sharpness of the loss (spectral norm of the Hessian)
progressively approaches and stabilizes around 2/(learning rate). Reasons for
the existence of EoS when training using gradient descent have recently been
proposed -- a lack of flat minima near the gradient descent trajectory together
with the presence of compact forward-invariant sets. In this paper, we show
that linear neural networks optimized under a quadratic loss function satisfy
the first assumption and also a necessary condition for the second assumption.
More precisely, we prove that the gradient descent map is non-singular, the set
of global minimizers of the loss function forms a smooth manifold, and the
stable minima form a bounded subset in parameter space. Additionally, we prove
that if the step-size is too big, then the set of initializations from which
gradient descent converges to a critical point has measure zero.</div><div><a href='http://arxiv.org/abs/2402.13108v1'>2402.13108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07874v1")'>Do stable neural networks exist for classification problems? -- A new
  view on stability in AI</div>
<div id='2401.07874v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T18:08:31Z</div><div>Authors: Z. N. D. Liu, A. C. Hansen</div><div style='padding-top: 10px; width: 80ex'>In deep learning (DL) the instability phenomenon is widespread and well
documented, most commonly using the classical measure of stability, the
Lipschitz constant. While a small Lipchitz constant is traditionally viewed as
guarantying stability, it does not capture the instability phenomenon in DL for
classification well. The reason is that a classification function -- which is
the target function to be approximated -- is necessarily discontinuous, thus
having an 'infinite' Lipchitz constant. As a result, the classical approach
will deem every classification function unstable, yet basic classification
functions a la 'is there a cat in the image?' will typically be locally very
'flat' -- and thus locally stable -- except at the decision boundary. The lack
of an appropriate measure of stability hinders a rigorous theory for stability
in DL, and consequently, there are no proper approximation theoretic results
that can guarantee the existence of stable networks for classification
functions. In this paper we introduce a novel stability measure
$\mathscr{S}(f)$, for any classification function $f$, appropriate to study the
stability of discontinuous functions and their approximations. We further prove
two approximation theorems: First, for any $\epsilon &gt; 0$ and any
classification function $f$ on a \emph{compact set}, there is a neural network
(NN) $\psi$, such that $\psi - f \neq 0$ only on a set of measure $&lt; \epsilon$,
moreover, $\mathscr{S}(\psi) \geq \mathscr{S}(f) - \epsilon$ (as accurate and
stable as $f$ up to $\epsilon$). Second, for any classification function $f$
and $\epsilon &gt; 0$, there exists a NN $\psi$ such that $\psi = f$ on the set of
points that are at least $\epsilon$ away from the decision boundary.</div><div><a href='http://arxiv.org/abs/2401.07874v1'>2401.07874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07250v1")'>Stabilizing Sharpness-aware Minimization Through A Simple
  Renormalization Strategy</div>
<div id='2401.07250v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T10:53:36Z</div><div>Authors: Chengli Tan, Jiangshe Zhang, Junmin Liu, Yicheng Wang, Yunda Hao</div><div style='padding-top: 10px; width: 80ex'>Recently, sharpness-aware minimization (SAM) has attracted a lot of attention
because of its surprising effectiveness in improving generalization
performance.However, training neural networks with SAM can be highly unstable
since the loss does not decrease along the direction of the exact gradient at
the current point, but instead follows the direction of a surrogate gradient
evaluated at another point nearby. To address this issue, we propose a simple
renormalization strategy, dubbed StableSAM, so that the norm of the surrogate
gradient maintains the same as that of the exact gradient. Our strategy is easy
to implement and flexible enough to integrate with SAM and its variants, almost
at no computational cost. With elementary tools from convex optimization and
learning theory, we also conduct a theoretical analysis of sharpness-aware
training, revealing that compared to stochastic gradient descent (SGD), the
effectiveness of SAM is only assured in a limited regime of learning rate. In
contrast, we show how StableSAM extends this regime of learning rate and when
it can consistently perform better than SAM with minor modification. Finally,
we demonstrate the improved performance of StableSAM on several representative
data sets and tasks.</div><div><a href='http://arxiv.org/abs/2401.07250v1'>2401.07250v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12350v1")'>Friendly Sharpness-Aware Minimization</div>
<div id='2403.12350v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T01:39:33Z</div><div>Authors: Tao Li, Pan Zhou, Zhengbao He, Xinwen Cheng, Xiaolin Huang</div><div style='padding-top: 10px; width: 80ex'>Sharpness-Aware Minimization (SAM) has been instrumental in improving deep
neural network training by minimizing both training loss and loss sharpness.
Despite the practical success, the mechanisms behind SAM's generalization
enhancements remain elusive, limiting its progress in deep learning
optimization. In this work, we investigate SAM's core components for
generalization improvement and introduce "Friendly-SAM" (F-SAM) to further
enhance SAM's generalization. Our investigation reveals the key role of
batch-specific stochastic gradient noise within the adversarial perturbation,
i.e., the current minibatch gradient, which significantly influences SAM's
generalization performance. By decomposing the adversarial perturbation in SAM
into full gradient and stochastic gradient noise components, we discover that
relying solely on the full gradient component degrades generalization while
excluding it leads to improved performance. The possible reason lies in the
full gradient component's increase in sharpness loss for the entire dataset,
creating inconsistencies with the subsequent sharpness minimization step solely
on the current minibatch data. Inspired by these insights, F-SAM aims to
mitigate the negative effects of the full gradient component. It removes the
full gradient estimated by an exponentially moving average (EMA) of historical
stochastic gradients, and then leverages stochastic gradient noise for improved
generalization. Moreover, we provide theoretical validation for the EMA
approximation and prove the convergence of F-SAM on non-convex problems.
Extensive experiments demonstrate the superior generalization performance and
robustness of F-SAM over vanilla SAM. Code is available at
https://github.com/nblt/F-SAM.</div><div><a href='http://arxiv.org/abs/2403.12350v1'>2403.12350v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15152v1")'>On the Duality Between Sharpness-Aware Minimization and Adversarial
  Training</div>
<div id='2402.15152v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T07:22:55Z</div><div>Authors: Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, Zeming Wei</div><div style='padding-top: 10px; width: 80ex'>Adversarial Training (AT), which adversarially perturb the input samples
during training, has been acknowledged as one of the most effective defenses
against adversarial attacks, yet suffers from a fundamental tradeoff that
inevitably decreases clean accuracy. Instead of perturbing the samples,
Sharpness-Aware Minimization (SAM) perturbs the model weights during training
to find a more flat loss landscape and improve generalization. However, as SAM
is designed for better clean accuracy, its effectiveness in enhancing
adversarial robustness remains unexplored. In this work, considering the
duality between SAM and AT, we investigate the adversarial robustness derived
from SAM. Intriguingly, we find that using SAM alone can improve adversarial
robustness. To understand this unexpected property of SAM, we first provide
empirical and theoretical insights into how SAM can implicitly learn more
robust features, and conduct comprehensive experiments to show that SAM can
improve adversarial robustness notably without sacrificing any clean accuracy,
shedding light on the potential of SAM to be a substitute for AT when accuracy
comes at a higher priority. Code is available at
https://github.com/weizeming/SAM_AT.</div><div><a href='http://arxiv.org/abs/2402.15152v1'>2402.15152v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14707v1")'>Mitigating Feature Gap for Adversarial Robustness by Feature
  Disentanglement</div>
<div id='2401.14707v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T08:38:57Z</div><div>Authors: Nuoyan Zhou, Dawei Zhou, Decheng Liu, Xinbo Gao, Nannan Wang</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks are vulnerable to adversarial samples. Adversarial
fine-tuning methods aim to enhance adversarial robustness through fine-tuning
the naturally pre-trained model in an adversarial training manner. However, we
identify that some latent features of adversarial samples are confused by
adversarial perturbation and lead to an unexpectedly increasing gap between
features in the last hidden layer of natural and adversarial samples. To
address this issue, we propose a disentanglement-based approach to explicitly
model and further remove the latent features that cause the feature gap.
Specifically, we introduce a feature disentangler to separate out the latent
features from the features of the adversarial samples, thereby boosting
robustness by eliminating the latent features. Besides, we align features in
the pre-trained model with features of adversarial samples in the fine-tuned
model, to further benefit from the features from natural samples without
confusion. Empirical evaluations on three benchmark datasets demonstrate that
our approach surpasses existing adversarial fine-tuning methods and adversarial
training baselines.</div><div><a href='http://arxiv.org/abs/2401.14707v1'>2401.14707v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12187v1")'>Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep
  Learning via Adversarial Training</div>
<div id='2402.12187v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:51:20Z</div><div>Authors: Leo Hyun Park, Jaeuk Kim, Myung Gyo Oh, Jaewoo Park, Taekyoung Kwon</div><div style='padding-top: 10px; width: 80ex'>Deep learning models continue to advance in accuracy, yet they remain
vulnerable to adversarial attacks, which often lead to the misclassification of
adversarial examples. Adversarial training is used to mitigate this problem by
increasing robustness against these attacks. However, this approach typically
reduces a model's standard accuracy on clean, non-adversarial samples. The
necessity for deep learning models to balance both robustness and accuracy for
security is obvious, but achieving this balance remains challenging, and the
underlying reasons are yet to be clarified. This paper proposes a novel
adversarial training method called Adversarial Feature Alignment (AFA), to
address these problems. Our research unveils an intriguing insight:
misalignment within the feature space often leads to misclassification,
regardless of whether the samples are benign or adversarial. AFA mitigates this
risk by employing a novel optimization algorithm based on contrastive learning
to alleviate potential feature misalignment. Through our evaluations, we
demonstrate the superior performance of AFA. The baseline AFA delivers higher
robust accuracy than previous adversarial contrastive learning methods while
minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and
CIFAR100, respectively, in comparison to cross-entropy. We also show that joint
optimization of AFA and TRADES, accompanied by data augmentation using a recent
diffusion model, achieves state-of-the-art accuracy and robustness.</div><div><a href='http://arxiv.org/abs/2402.12187v1'>2402.12187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18211v1")'>Catastrophic Overfitting: A Potential Blessing in Disguise</div>
<div id='2402.18211v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T10:01:44Z</div><div>Authors: Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin</div><div style='padding-top: 10px; width: 80ex'>Fast Adversarial Training (FAT) has gained increasing attention within the
research community owing to its efficacy in improving adversarial robustness.
Particularly noteworthy is the challenge posed by catastrophic overfitting (CO)
in this field. Although existing FAT approaches have made strides in mitigating
CO, the ascent of adversarial robustness occurs with a non-negligible decline
in classification accuracy on clean samples. To tackle this issue, we initially
employ the feature activation differences between clean and adversarial
examples to analyze the underlying causes of CO. Intriguingly, our findings
reveal that CO can be attributed to the feature coverage induced by a few
specific pathways. By intentionally manipulating feature activation differences
in these pathways with well-designed regularization terms, we can effectively
mitigate and induce CO, providing further evidence for this observation.
Notably, models trained stably with these terms exhibit superior performance
compared to prior FAT work. On this basis, we harness CO to achieve `attack
obfuscation', aiming to bolster model performance. Consequently, the models
suffering from CO can attain optimal classification accuracy on both clean and
adversarial data when adding random noise to inputs during evaluation. We also
validate their robustness against transferred adversarial examples and the
necessity of inducing CO to improve robustness. Hence, CO may not be a problem
that has to be solved.</div><div><a href='http://arxiv.org/abs/2402.18211v1'>2402.18211v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17229v1")'>Preserving Fairness Generalization in Deepfake Detection</div>
<div id='2402.17229v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T05:47:33Z</div><div>Authors: Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu</div><div style='padding-top: 10px; width: 80ex'>Although effective deepfake detection models have been developed in recent
years, recent studies have revealed that these models can result in unfair
performance disparities among demographic groups, such as race and gender. This
can lead to particular groups facing unfair targeting or exclusion from
detection, potentially allowing misclassified deepfakes to manipulate public
opinion and undermine trust in the model. The existing method for addressing
this problem is providing a fair loss function. It shows good fairness
performance for intra-domain evaluation but does not maintain fairness for
cross-domain testing. This highlights the significance of fairness
generalization in the fight against deepfakes. In this work, we propose the
first method to address the fairness generalization problem in deepfake
detection by simultaneously considering features, loss, and optimization
aspects. Our method employs disentanglement learning to extract demographic and
domain-agnostic forgery features, fusing them to encourage fair learning across
a flattened loss landscape. Extensive experiments on prominent deepfake
datasets demonstrate our method's effectiveness, surpassing state-of-the-art
approaches in preserving fairness during cross-domain deepfake detection. The
code is available at https://github.com/Purdue-M2/Fairness-Generalization</div><div><a href='http://arxiv.org/abs/2402.17229v1'>2402.17229v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02012v1")'>Fast &amp; Fair: Efficient Second-Order Robust Optimization for Fairness in
  Machine Learning</div>
<div id='2401.02012v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T01:02:55Z</div><div>Authors: Allen Minch, Hung Anh Vu, Anne Marie Warren</div><div style='padding-top: 10px; width: 80ex'>This project explores adversarial training techniques to develop fairer Deep
Neural Networks (DNNs) to mitigate the inherent bias they are known to exhibit.
DNNs are susceptible to inheriting bias with respect to sensitive attributes
such as race and gender, which can lead to life-altering outcomes (e.g.,
demographic bias in facial recognition software used to arrest a suspect). We
propose a robust optimization problem, which we demonstrate can improve
fairness in several datasets, both synthetic and real-world, using an affine
linear model. Leveraging second order information, we are able to find a
solution to our optimization problem more efficiently than a purely first order
method.</div><div><a href='http://arxiv.org/abs/2401.02012v1'>2401.02012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00418v3")'>Benchmarking Transferable Adversarial Attacks</div>
<div id='2402.00418v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T08:36:16Z</div><div>Authors: Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen</div><div style='padding-top: 10px; width: 80ex'>The robustness of deep learning models against adversarial attacks remains a
pivotal concern. This study presents, for the first time, an exhaustive review
of the transferability aspect of adversarial attacks. It systematically
categorizes and critically evaluates various methodologies developed to augment
the transferability of adversarial attacks. This study encompasses a spectrum
of techniques, including Generative Structure, Semantic Similarity, Gradient
Editing, Target Modification, and Ensemble Approach. Concurrently, this paper
introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading
methodologies for adversarial attack transferability, thereby providing a
standardized and systematic platform for comparative analysis across diverse
model architectures. Through comprehensive scrutiny, we delineate the efficacy
and constraints of each method, shedding light on their underlying operational
principles and practical utility. This review endeavors to be a quintessential
resource for both scholars and practitioners in the field, charting the complex
terrain of adversarial transferability and setting a foundation for future
explorations in this vital sector. The associated codebase is accessible at:
https://github.com/KxPlaug/TAA-Bench</div><div><a href='http://arxiv.org/abs/2402.00418v3'>2402.00418v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08734v1")'>Bag of Tricks to Boost Adversarial Transferability</div>
<div id='2401.08734v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:42:36Z</div><div>Authors: Zeliang Zhang, Rongyi Zhu, Wei Yao, Xiaosen Wang, Chenliang Xu</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks are widely known to be vulnerable to adversarial
examples. However, vanilla adversarial examples generated under the white-box
setting often exhibit low transferability across different models. Since
adversarial transferability poses more severe threats to practical
applications, various approaches have been proposed for better transferability,
including gradient-based, input transformation-based, and model-related
attacks, \etc. In this work, we find that several tiny changes in the existing
adversarial attacks can significantly affect the attack performance, \eg, the
number of iterations and step size. Based on careful studies of existing
adversarial attacks, we propose a bag of tricks to enhance adversarial
transferability, including momentum initialization, scheduled step size, dual
example, spectral-based input transformation, and several ensemble strategies.
Extensive experiments on the ImageNet dataset validate the high effectiveness
of our proposed tricks and show that combining them can further boost
adversarial transferability. Our work provides practical insights and
techniques to enhance adversarial transferability, and offers guidance to
improve the attack performance on the real-world application through simple
adjustments.</div><div><a href='http://arxiv.org/abs/2401.08734v1'>2401.08734v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17018v1")'>A Curious Case of Remarkable Resilience to Gradient Attacks via Fully
  Convolutional and Differentiable Front End with a Skip Connection</div>
<div id='2402.17018v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T20:55:47Z</div><div>Authors: Leonid Boytsov, Ameya Joshi, Filipe Condessa</div><div style='padding-top: 10px; width: 80ex'>We tested front-end enhanced neural models where a frozen classifier was
prepended by a differentiable and fully convolutional model with a skip
connection. By training them using a small learning rate for about one epoch,
we obtained models that retained the accuracy of the backbone classifier while
being unusually resistant to gradient attacks including APGD and FAB-T attacks
from the AutoAttack package, which we attributed to gradient masking. The
gradient masking phenomenon is not new, but the degree of masking was quite
remarkable for fully differentiable models that did not have
gradient-shattering components such as JPEG compression or components that are
expected to cause diminishing gradients.
  Though black box attacks can be partially effective against gradient masking,
they are easily defeated by combining models into randomized ensembles. We
estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10,
CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive
attacks. Adversarial training of the backbone classifier can further increase
resistance of the front-end enhanced model to gradient attacks. On CIFAR10, the
respective randomized ensemble achieved 90.8$\pm 2.5$% (99% CI) accuracy under
AutoAttack while having only 18.2$\pm 3.6$% accuracy under the adaptive attack.
  We do not establish SOTA in adversarial robustness. Instead, we make
methodological contributions and further supports the thesis that adaptive
attacks designed with the complete knowledge of model architecture are crucial
in demonstrating model robustness and that even the so-called white-box
gradient attacks can have limited applicability. Although gradient attacks can
be complemented with black-box attack such as the SQUARE attack or the
zero-order PGD, black-box attacks can be weak against randomized ensembles,
e.g., when ensemble models mask gradients.</div><div><a href='http://arxiv.org/abs/2402.17018v1'>2402.17018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05100v1")'>Exploring the Adversarial Frontier: Quantifying Robustness via
  Adversarial Hypervolume</div>
<div id='2403.05100v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T07:03:18Z</div><div>Authors: Ping Guo, Cheng Gong, Xi Lin, Zhiyuan Yang, Qingfu Zhang</div><div style='padding-top: 10px; width: 80ex'>The escalating threat of adversarial attacks on deep learning models,
particularly in security-critical fields, has underscored the need for robust
deep learning systems. Conventional robustness evaluations have relied on
adversarial accuracy, which measures a model's performance under a specific
perturbation intensity. However, this singular metric does not fully
encapsulate the overall resilience of a model against varying degrees of
perturbation. To address this gap, we propose a new metric termed adversarial
hypervolume, assessing the robustness of deep learning models comprehensively
over a range of perturbation intensities from a multi-objective optimization
standpoint. This metric allows for an in-depth comparison of defense mechanisms
and recognizes the trivial improvements in robustness afforded by less potent
defensive strategies. Additionally, we adopt a novel training algorithm that
enhances adversarial robustness uniformly across various perturbation
intensities, in contrast to methods narrowly focused on optimizing adversarial
accuracy. Our extensive empirical studies validate the effectiveness of the
adversarial hypervolume metric, demonstrating its ability to reveal subtle
differences in robustness that adversarial accuracy overlooks. This research
contributes a new measure of robustness and establishes a standard for
assessing and benchmarking the resilience of current and future defensive
models against adversarial threats.</div><div><a href='http://arxiv.org/abs/2403.05100v1'>2403.05100v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04070v1")'>Improving Adversarial Training using Vulnerability-Aware Perturbation
  Budget</div>
<div id='2403.04070v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T21:50:52Z</div><div>Authors: Olukorede Fakorede, Modeste Atsague, Jin Tian</div><div style='padding-top: 10px; width: 80ex'>Adversarial Training (AT) effectively improves the robustness of Deep Neural
Networks (DNNs) to adversarial attacks. Generally, AT involves training DNN
models with adversarial examples obtained within a pre-defined, fixed
perturbation bound. Notably, individual natural examples from which these
adversarial examples are crafted exhibit varying degrees of intrinsic
vulnerabilities, and as such, crafting adversarial examples with fixed
perturbation radius for all instances may not sufficiently unleash the potency
of AT. Motivated by this observation, we propose two simple, computationally
cheap vulnerability-aware reweighting functions for assigning perturbation
bounds to adversarial examples used for AT, named Margin-Weighted Perturbation
Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The
proposed methods assign perturbation radii to individual adversarial samples
based on the vulnerability of their corresponding natural examples.
Experimental results show that the proposed methods yield genuine improvements
in the robustness of AT algorithms against various adversarial attacks.</div><div><a href='http://arxiv.org/abs/2403.04070v1'>2403.04070v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13751v1")'>A Systematic Approach to Robustness Modelling for Deep Convolutional
  Neural Networks</div>
<div id='2401.13751v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T19:12:37Z</div><div>Authors: Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy Löfstedt, Erik Elmroth</div><div style='padding-top: 10px; width: 80ex'>Convolutional neural networks have shown to be widely applicable to a large
number of fields when large amounts of labelled data are available. The recent
trend has been to use models with increasingly larger sets of tunable
parameters to increase model accuracy, reduce model loss, or create more
adversarially robust models -- goals that are often at odds with one another.
In particular, recent theoretical work raises questions about the ability for
even larger models to generalize to data outside of the controlled train and
test sets. As such, we examine the role of the number of hidden layers in the
ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a
variety of parameters including the size of the model, the floating point
precision, and the noise level of both the training data and the model output.
To encapsulate the model's predictive power and computational cost, we provide
a method that uses induced failures to model the probability of failure as a
function of time and relate that to a novel metric that allows us to quickly
determine whether or not the cost of training a model outweighs the cost of
attacking it. Using this approach, we are able to approximate the expected
failure rate using a small number of specially crafted samples rather than
increasingly larger benchmark datasets. We demonstrate the efficacy of this
technique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit
floating-point numbers, various data pre-processing techniques, and several
attacks on five configurations of the ResNet model. Then, using empirical
measurements, we examine the various trade-offs between cost, robustness,
latency, and reliability to find that larger models do not significantly aid in
adversarial robustness despite costing significantly more to train.</div><div><a href='http://arxiv.org/abs/2401.13751v1'>2401.13751v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18787v1")'>Enhancing the "Immunity" of Mixture-of-Experts Networks for Adversarial
  Defense</div>
<div id='2402.18787v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T01:27:38Z</div><div>Authors: Qiao Han, yong huang, xinling Guo, Yiteng Zhai, Yu Qin, Yao Yang</div><div style='padding-top: 10px; width: 80ex'>Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs)
to adversarial examples, which can easily fool DNNs into making incorrect
predictions. To mitigate this deficiency, we propose a novel adversarial
defense method called "Immunity" (Innovative MoE with MUtual information \&amp;
positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture
in this work. The key enhancements to the standard MoE are two-fold: 1)
integrating of Random Switch Gates (RSGs) to obtain diverse network structures
via random permutation of RSG parameters at evaluation time, despite of RSGs
being determined after one-time training; 2) devising innovative Mutual
Information (MI)-based and Position Stability-based loss functions by
capitalizing on Grad-CAM's explanatory power to increase the diversity and the
causality of expert networks. Notably, our MI-based loss operates directly on
the heatmaps, thereby inducing subtler negative impacts on the classification
performance when compared to other losses of the same type, theoretically.
Extensive evaluation validates the efficacy of the proposed approach in
improving adversarial robustness against a wide range of attacks.</div><div><a href='http://arxiv.org/abs/2402.18787v1'>2402.18787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09101v1")'>Soften to Defend: Towards Adversarial Robustness via Self-Guided Label
  Refinement</div>
<div id='2403.09101v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T04:48:31Z</div><div>Authors: Daiwei Yu, Zhuorong Li, Lina Wei, Canghong Jin, Yun Zhang, Sixian Chan</div><div style='padding-top: 10px; width: 80ex'>Adversarial training (AT) is currently one of the most effective ways to
obtain the robustness of deep neural networks against adversarial attacks.
However, most AT methods suffer from robust overfitting, i.e., a significant
generalization gap in adversarial robustness between the training and testing
curves. In this paper, we first identify a connection between robust
overfitting and the excessive memorization of noisy labels in AT from a view of
gradient norm. As such label noise is mainly caused by a distribution mismatch
and improper label assignments, we are motivated to propose a label refinement
approach for AT. Specifically, our Self-Guided Label Refinement first
self-refines a more accurate and informative label distribution from
over-confident hard labels, and then it calibrates the training by dynamically
incorporating knowledge from self-distilled models into the current model and
thus requiring no external teachers. Empirical results demonstrate that our
method can simultaneously boost the standard accuracy and robust performance
across multiple benchmark datasets, attack types, and architectures. In
addition, we also provide a set of analyses from the perspectives of
information theory to dive into our method and suggest the importance of soft
labels for robust generalization.</div><div><a href='http://arxiv.org/abs/2403.09101v1'>2403.09101v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13523v1")'>Have You Poisoned My Data? Defending Neural Networks against Data
  Poisoning</div>
<div id='2403.13523v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T11:50:16Z</div><div>Authors: Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini</div><div style='padding-top: 10px; width: 80ex'>The unprecedented availability of training data fueled the rapid development
of powerful neural networks in recent years. However, the need for such large
amounts of data leads to potential threats such as poisoning attacks:
adversarial manipulations of the training data aimed at compromising the
learned model to achieve a given adversarial goal.
  This paper investigates defenses against clean-label poisoning attacks and
proposes a novel approach to detect and filter poisoned datapoints in the
transfer learning setting. We define a new characteristic vector representation
of datapoints and show that it effectively captures the intrinsic properties of
the data distribution. Through experimental analysis, we demonstrate that
effective poisons can be successfully differentiated from clean points in the
characteristic vector space. We thoroughly evaluate our proposed approach and
compare it to existing state-of-the-art defenses using multiple architectures,
datasets, and poison budgets. Our evaluation shows that our proposal
outperforms existing approaches in defense rate and final trained model
performance across all experimental settings.</div><div><a href='http://arxiv.org/abs/2403.13523v1'>2403.13523v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07991v2")'>Robustness Against Adversarial Attacks via Learning Confined Adversarial
  Polytopes</div>
<div id='2401.07991v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T22:31:15Z</div><div>Authors: Shayan Mohajer Hamidi, Linfeng Ye</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks (DNNs) could be deceived by generating
human-imperceptible perturbations of clean samples. Therefore, enhancing the
robustness of DNNs against adversarial attacks is a crucial task. In this
paper, we aim to train robust DNNs by limiting the set of outputs reachable via
a norm-bounded perturbation added to a clean sample. We refer to this set as
adversarial polytope, and each clean sample has a respective adversarial
polytope. Indeed, if the respective polytopes for all the samples are compact
such that they do not intersect the decision boundaries of the DNN, then the
DNN is robust against adversarial samples. Hence, the inner-working of our
algorithm is based on learning \textbf{c}onfined \textbf{a}dversarial
\textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we
demonstrate the effectiveness of CAP over existing adversarial robustness
methods in improving the robustness of models against state-of-the-art attacks
including AutoAttack.</div><div><a href='http://arxiv.org/abs/2401.07991v2'>2401.07991v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17523v1")'>Game-Theoretic Unlearnable Example Generator</div>
<div id='2401.17523v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T00:43:30Z</div><div>Authors: Shuang Liu, Yihan Wang, Xiao-Shan Gao</div><div style='padding-top: 10px; width: 80ex'>Unlearnable example attacks are data poisoning attacks aiming to degrade the
clean test accuracy of deep learning by adding imperceptible perturbations to
the training samples, which can be formulated as a bi-level optimization
problem. However, directly solving this optimization problem is intractable for
deep neural networks. In this paper, we investigate unlearnable example attacks
from a game-theoretic perspective, by formulating the attack as a nonzero sum
Stackelberg game. First, the existence of game equilibria is proved under the
normal setting and the adversarial training setting. It is shown that the game
equilibrium gives the most powerful poison attack in that the victim has the
lowest test accuracy among all networks within the same hypothesis space, when
certain loss functions are used. Second, we propose a novel attack method,
called the Game Unlearnable Example (GUE), which has three main gradients. (1)
The poisons are obtained by directly solving the equilibrium of the Stackelberg
game with a first-order algorithm. (2) We employ an autoencoder-like generative
network model as the poison attacker. (3) A novel payoff function is introduced
to evaluate the performance of the poison. Comprehensive experiments
demonstrate that GUE can effectively poison the model in various scenarios.
Furthermore, the GUE still works by using a relatively small percentage of the
training data to train the generator, and the poison generator can generalize
to unseen data well. Our implementation code can be found at
https://github.com/hong-xian/gue.</div><div><a href='http://arxiv.org/abs/2401.17523v1'>2401.17523v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06668v1")'>PeerAiD: Improving Adversarial Distillation from a Specialized Peer
  Tutor</div>
<div id='2403.06668v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T12:36:14Z</div><div>Authors: Jaewon Jung, Hongsun Jang, Jaeyong Song, Jinho Lee</div><div style='padding-top: 10px; width: 80ex'>Adversarial robustness of the neural network is a significant concern when it
is applied to security-critical domains. In this situation, adversarial
distillation is a promising option which aims to distill the robustness of the
teacher network to improve the robustness of a small student network. Previous
works pretrain the teacher network to make it robust to the adversarial
examples aimed at itself. However, the adversarial examples are dependent on
the parameters of the target network. The fixed teacher network inevitably
degrades its robustness against the unseen transferred adversarial examples
which targets the parameters of the student network in the adversarial
distillation process. We propose PeerAiD to make a peer network learn the
adversarial examples of the student network instead of adversarial examples
aimed at itself. PeerAiD is an adversarial distillation that trains the peer
network and the student network simultaneously in order to make the peer
network specialized for defending the student network. We observe that such
peer networks surpass the robustness of pretrained robust teacher network
against student-attacked adversarial samples. With this peer network and
adversarial distillation, PeerAiD achieves significantly higher robustness of
the student network with AutoAttack (AA) accuracy up to 1.66%p and improves the
natural accuracy of the student network up to 4.72%p with ResNet-18 and
TinyImageNet dataset.</div><div><a href='http://arxiv.org/abs/2403.06668v1'>2403.06668v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13134v1")'>Robust NAS under adversarial training: benchmark, theory, and beyond</div>
<div id='2403.13134v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T20:10:23Z</div><div>Authors: Yongtao Wu, Fanghui Liu, Carl-Johann Simon-Gabriel, Grigorios G Chrysos, Volkan Cevher</div><div style='padding-top: 10px; width: 80ex'>Recent developments in neural architecture search (NAS) emphasize the
significance of considering robust architectures against malicious data.
However, there is a notable absence of benchmark evaluations and theoretical
guarantees for searching these robust architectures, especially when
adversarial training is considered. In this work, we aim to address these two
challenges, making twofold contributions. First, we release a comprehensive
data set that encompasses both clean accuracy and robust accuracy for a vast
array of adversarially trained networks from the NAS-Bench-201 search space on
image datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep
learning theory, we establish a generalization theory for searching
architecture in terms of clean accuracy and robust accuracy under
multi-objective adversarial training. We firmly believe that our benchmark and
theoretical insights will significantly benefit the NAS community through
reliable reproducibility, efficient assessment, and theoretical foundation,
particularly in the pursuit of robust architectures.</div><div><a href='http://arxiv.org/abs/2403.13134v1'>2403.13134v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00935v1")'>Transfer Learning for Security: Challenges and Future Directions</div>
<div id='2403.00935v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T19:27:53Z</div><div>Authors: Adrian Shuai Li, Arun Iyengar, Ashish Kundu, Elisa Bertino</div><div style='padding-top: 10px; width: 80ex'>Many machine learning and data mining algorithms rely on the assumption that
the training and testing data share the same feature space and distribution.
However, this assumption may not always hold. For instance, there are
situations where we need to classify data in one domain, but we only have
sufficient training data available from a different domain. The latter data may
follow a distinct distribution. In such cases, successfully transferring
knowledge across domains can significantly improve learning performance and
reduce the need for extensive data labeling efforts. Transfer learning (TL) has
thus emerged as a promising framework to tackle this challenge, particularly in
security-related tasks. This paper aims to review the current advancements in
utilizing TL techniques for security. The paper includes a discussion of the
existing research gaps in applying TL in the security domain, as well as
exploring potential future research directions and issues that arise in the
context of TL-assisted security solutions.</div><div><a href='http://arxiv.org/abs/2403.00935v1'>2403.00935v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07362v1")'>Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine
  Unlearning</div>
<div id='2403.07362v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T06:50:32Z</div><div>Authors: Chongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu</div><div style='padding-top: 10px; width: 80ex'>The trustworthy machine learning (ML) community is increasingly recognizing
the crucial need for models capable of selectively 'unlearning' data points
after training. This leads to the problem of machine unlearning (MU), aiming to
eliminate the influence of chosen data points on model performance, while still
maintaining the model's utility post-unlearning. Despite various MU methods for
data influence erasure, evaluations have largely focused on random data
forgetting, ignoring the vital inquiry into which subset should be chosen to
truly gauge the authenticity of unlearning performance. To tackle this issue,
we introduce a new evaluative angle for MU from an adversarial viewpoint. We
propose identifying the data subset that presents the most significant
challenge for influence erasure, i.e., pinpointing the worst-case forget set.
Utilizing a bi-level optimization principle, we amplify unlearning challenges
at the upper optimization level to emulate worst-case scenarios, while
simultaneously engaging in standard training and unlearning at the lower level,
achieving a balance between data influence erasure and model utility. Our
proposal offers a worst-case evaluation of MU's resilience and effectiveness.
Through extensive experiments across different datasets (including CIFAR-10,
100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image
classifiers and generative models), we expose critical pros and cons in
existing (approximate) unlearning strategies. Our results illuminate the
complex challenges of MU in practice, guiding the future development of more
accurate and robust unlearning algorithms. The code is available at
https://github.com/OPTML-Group/Unlearn-WorstCase.</div><div><a href='http://arxiv.org/abs/2403.07362v1'>2403.07362v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00195v1")'>Dataset Condensation Driven Machine Unlearning</div>
<div id='2402.00195v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T21:48:25Z</div><div>Authors: Junaid Iqbal Khan</div><div style='padding-top: 10px; width: 80ex'>The current trend in data regulation requirements and privacy-preserving
machine learning has emphasized the importance of machine unlearning. The naive
approach to unlearning training data by retraining over the complement of the
forget samples is susceptible to computational challenges. These challenges
have been effectively addressed through a collection of techniques falling
under the umbrella of machine unlearning. However, there still exists a lack of
sufficiency in handling persistent computational challenges in harmony with the
utility and privacy of unlearned model. We attribute this to the lack of work
on improving the computational complexity of approximate unlearning from the
perspective of the training dataset. In this paper, we aim to fill this gap by
introducing dataset condensation as an essential component of machine
unlearning in the context of image classification. To achieve this goal, we
propose new dataset condensation techniques and an innovative unlearning scheme
that strikes a balance between machine unlearning privacy, utility, and
efficiency. Furthermore, we present a novel and effective approach to
instrumenting machine unlearning and propose its application in defending
against membership inference and model inversion attacks. Additionally, we
explore a new application of our approach, which involves removing data from
`condensed model', which can be employed to quickly train any arbitrary model
without being influenced by unlearning samples.</div><div><a href='http://arxiv.org/abs/2402.00195v1'>2402.00195v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12830v1")'>Has Approximate Machine Unlearning been evaluated properly? From
  Auditing to Side Effects</div>
<div id='2403.12830v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T15:37:27Z</div><div>Authors: Cheng-Long Wang, Qi Li, Zihang Xiang, Di Wang</div><div style='padding-top: 10px; width: 80ex'>The growing concerns surrounding data privacy and security have underscored
the critical necessity for machine unlearning--aimed at fully removing data
lineage from machine learning models. MLaaS providers expect this to be their
ultimate safeguard for regulatory compliance. Despite its critical importance,
the pace at which privacy communities have been developing and implementing
strong methods to verify the effectiveness of machine unlearning has been
disappointingly slow, with this vital area often receiving insufficient focus.
This paper seeks to address this shortfall by introducing well-defined and
effective metrics for black-box unlearning auditing tasks. We transform the
auditing challenge into a question of non-membership inference and develop
efficient metrics for auditing. By relying exclusively on the original and
unlearned models--eliminating the need to train additional shadow models--our
approach simplifies the evaluation of unlearning at the individual data point
level. Utilizing these metrics, we conduct an in-depth analysis of current
approximate machine unlearning algorithms, identifying three key directions
where these approaches fall short: utility, resilience, and equity. Our aim is
that this work will greatly improve our understanding of approximate machine
unlearning methods, taking a significant stride towards converting the
theoretical right to data erasure into a auditable reality.</div><div><a href='http://arxiv.org/abs/2403.12830v1'>2403.12830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14015v1")'>Corrective Machine Unlearning</div>
<div id='2402.14015v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:54:37Z</div><div>Authors: Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal</div><div style='padding-top: 10px; width: 80ex'>Machine Learning models increasingly face data integrity challenges due to
the use of large-scale training datasets drawn from the internet. We study what
model developers can do if they detect that some data was manipulated or
incorrect. Such manipulated data can cause adverse effects like vulnerability
to backdoored samples, systematic biases, and in general, reduced accuracy on
certain input domains. Often, all manipulated training samples are not known,
and only a small, representative subset of the affected data is flagged.
  We formalize "Corrective Machine Unlearning" as the problem of mitigating the
impact of data affected by unknown manipulations on a trained model, possibly
knowing only a subset of impacted samples. We demonstrate that the problem of
corrective unlearning has significantly different requirements from traditional
privacy-oriented unlearning. We find most existing unlearning methods,
including the gold-standard retraining-from-scratch, require most of the
manipulated data to be identified for effective corrective unlearning. However,
one approach, SSD, achieves limited success in unlearning adverse effects with
just a small portion of the manipulated samples, showing the tractability of
this setting. We hope our work spurs research towards developing better methods
for corrective unlearning and offers practitioners a new strategy to handle
data integrity challenges arising from web-scale training.</div><div><a href='http://arxiv.org/abs/2402.14015v1'>2402.14015v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09477v1")'>PANORAMIA: Privacy Auditing of Machine Learning Models without
  Retraining</div>
<div id='2402.09477v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:56:07Z</div><div>Authors: Mishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco, Qiaoyue Tang, Tao Wang, Sébastien Gambs, Mathias Lécuyer</div><div style='padding-top: 10px; width: 80ex'>We introduce a privacy auditing scheme for ML models that relies on
membership inference attacks using generated data as "non-members". This
scheme, which we call PANORAMIA, quantifies the privacy leakage for large-scale
ML models without control of the training process or model re-training and only
requires access to a subset of the training data. To demonstrate its
applicability, we evaluate our auditing scheme across multiple ML domains,
ranging from image and tabular data classification to large-scale language
models.</div><div><a href='http://arxiv.org/abs/2402.09477v1'>2402.09477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11348v1")'>COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via
  Probabilistic Circuits</div>
<div id='2403.11348v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T21:23:45Z</div><div>Authors: Mintong Kang, Nezihe Merve Gürel, Linyi Li, Bo Li</div><div style='padding-top: 10px; width: 80ex'>Conformal prediction has shown spurring performance in constructing
statistically rigorous prediction sets for arbitrary black-box machine learning
models, assuming the data is exchangeable. However, even small adversarial
perturbations during the inference can violate the exchangeability assumption,
challenge the coverage guarantees, and result in a subsequent decline in
empirical coverage. In this work, we propose a certifiably robust
learning-reasoning conformal prediction framework (COLEP) via probabilistic
circuits, which comprise a data-driven learning component that trains
statistical models to learn different semantic concepts, and a reasoning
component that encodes knowledge and characterizes the relationships among the
trained models for logic reasoning. To achieve exact and efficient reasoning,
we employ probabilistic circuits (PCs) within the reasoning component.
Theoretically, we provide end-to-end certification of prediction coverage for
COLEP in the presence of bounded adversarial perturbations. We also provide
certified coverage considering the finite size of the calibration set.
Furthermore, we prove that COLEP achieves higher prediction coverage and
accuracy over a single model as long as the utilities of knowledge models are
non-trivial. Empirically, we show the validity and tightness of our certified
coverage, demonstrating the robust conformal prediction of COLEP on various
datasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to
12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% on
AwA2.</div><div><a href='http://arxiv.org/abs/2403.11348v1'>2403.11348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01218v1")'>Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense
  of Privacy</div>
<div id='2403.01218v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T14:22:40Z</div><div>Authors: Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas Papernot</div><div style='padding-top: 10px; width: 80ex'>The high cost of model training makes it increasingly desirable to develop
techniques for unlearning. These techniques seek to remove the influence of a
training example without having to retrain the model from scratch. Intuitively,
once a model has unlearned, an adversary that interacts with the model should
no longer be able to tell whether the unlearned example was included in the
model's training set or not. In the privacy literature, this is known as
membership inference. In this work, we discuss adaptations of Membership
Inference Attacks (MIAs) to the setting of unlearning (leading to their
``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into
``population U-MIAs'', where the same attacker is instantiated for all
examples, and ``per-example U-MIAs'', where a dedicated attacker is
instantiated for each example. We show that the latter category, wherein the
attacker tailors its membership prediction to each example under attack, is
significantly stronger. Indeed, our results show that the commonly used U-MIAs
in the unlearning literature overestimate the privacy protection afforded by
existing unlearning techniques on both vision and language models. Our
investigation reveals a large variance in the vulnerability of different
examples to per-example U-MIAs. In fact, several unlearning algorithms lead to
a reduced vulnerability for some, but not all, examples that we wish to
unlearn, at the expense of increasing it for other examples. Notably, we find
that the privacy protection for the remaining training examples may worsen as a
consequence of unlearning. We also discuss the fundamental difficulty of
equally protecting all examples using existing unlearning schemes, due to the
different rates at which examples are unlearned. We demonstrate that naive
attempts at tailoring unlearning stopping criteria to different examples fail
to alleviate these issues.</div><div><a href='http://arxiv.org/abs/2403.01218v1'>2403.01218v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17390v1")'>Robustness-Congruent Adversarial Training for Secure Machine Learning
  Model Updates</div>
<div id='2402.17390v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T10:37:13Z</div><div>Authors: Daniele Angioni, Luca Demetrio, Maura Pintor, Luca Oneto, Davide Anguita, Battista Biggio, Fabio Roli</div><div style='padding-top: 10px; width: 80ex'>Machine-learning models demand for periodic updates to improve their average
accuracy, exploiting novel architectures and additional data. However, a
newly-updated model may commit mistakes that the previous model did not make.
Such misclassifications are referred to as negative flips, and experienced by
users as a regression of performance. In this work, we show that this problem
also affects robustness to adversarial examples, thereby hindering the
development of secure model update practices. In particular, when updating a
model to improve its adversarial robustness, some previously-ineffective
adversarial examples may become misclassified, causing a regression in the
perceived security of the system. We propose a novel technique, named
robustness-congruent adversarial training, to address this issue. It amounts to
fine-tuning a model with adversarial training, while constraining it to retain
higher robustness on the adversarial examples that were correctly classified
before the update. We show that our algorithm and, more generally, learning
with non-regression constraints, provides a theoretically-grounded framework to
train consistent estimators. Our experiments on robust models for computer
vision confirm that (i) both accuracy and robustness, even if improved after
model update, can be affected by negative flips, and (ii) our
robustness-congruent adversarial training can mitigate the problem,
outperforming competing baseline methods.</div><div><a href='http://arxiv.org/abs/2402.17390v1'>2402.17390v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00899v3")'>Weakly Supervised Learners for Correction of AI Errors with Provable
  Performance Guarantees</div>
<div id='2402.00899v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T20:36:13Z</div><div>Authors: Ivan Y. Tyukin, Tatiana Tyukina, Daniel van Helden, Zedong Zheng, Evgeny M. Mirkes, Oliver J. Sutton, Qinghua Zhou, Alexander N. Gorban, Penelope Allison</div><div style='padding-top: 10px; width: 80ex'>We present a new methodology for handling AI errors by introducing weakly
supervised AI error correctors with a priori performance guarantees. These AI
correctors are auxiliary maps whose role is to moderate the decisions of some
previously constructed underlying classifier by either approving or rejecting
its decisions. The rejection of a decision can be used as a signal to suggest
abstaining from making a decision. A key technical focus of the work is in
providing performance guarantees for these new AI correctors through bounds on
the probabilities of incorrect decisions. These bounds are distribution
agnostic and do not rely on assumptions on the data dimension. Our empirical
example illustrates how the framework can be applied to improve the performance
of an image classifier in a challenging real-world task where training data are
scarce.</div><div><a href='http://arxiv.org/abs/2402.00899v3'>2402.00899v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14648v1")'>Rethinking Invariance Regularization in Adversarial Training to Improve
  Robustness-Accuracy Trade-off</div>
<div id='2402.14648v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T15:53:46Z</div><div>Authors: Futa Waseda, Isao Echizen</div><div style='padding-top: 10px; width: 80ex'>Although adversarial training has been the state-of-the-art approach to
defend against adversarial examples (AEs), they suffer from a
robustness-accuracy trade-off. In this work, we revisit representation-based
invariance regularization to learn discriminative yet adversarially invariant
representations, aiming to mitigate this trade-off. We empirically identify two
key issues hindering invariance regularization: (1) a "gradient conflict"
between invariance loss and classification objectives, indicating the existence
of "collapsing solutions," and (2) the mixture distribution problem arising
from diverged distributions of clean and adversarial inputs. To address these
issues, we propose Asymmetrically Representation-regularized Adversarial
Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor
in the invariance loss to avoid "collapsing solutions," inspired by a recent
non-contrastive self-supervised learning approach, and a split-BatchNorm (BN)
structure to resolve the mixture distribution problem. Our method significantly
improves the robustness-accuracy trade-off by learning adversarially invariant
representations without sacrificing discriminative power. Furthermore, we
discuss the relevance of our findings to knowledge-distillation-based defense
methods, contributing to a deeper understanding of their relative successes.</div><div><a href='http://arxiv.org/abs/2402.14648v1'>2402.14648v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11120v1")'>DART: A Principled Approach to Adversarially Robust Unsupervised Domain
  Adaptation</div>
<div id='2402.11120v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T22:48:38Z</div><div>Authors: Yunjuan Wang, Hussein Hazimeh, Natalia Ponomareva, Alexey Kurakin, Ibrahim Hammoud, Raman Arora</div><div style='padding-top: 10px; width: 80ex'>Distribution shifts and adversarial examples are two major challenges for
deploying machine learning models. While these challenges have been studied
individually, their combination is an important topic that remains relatively
under-explored. In this work, we study the problem of adversarial robustness
under a common setting of distribution shift - unsupervised domain adaptation
(UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled
target domain $D_T$ with related but different distributions, the goal is to
obtain an adversarially robust model for $D_T$. The absence of target domain
labels poses a unique challenge, as conventional adversarial robustness
defenses cannot be directly applied to $D_T$. To address this challenge, we
first establish a generalization bound for the adversarial target loss, which
consists of (i) terms related to the loss on the data, and (ii) a measure of
worst-case domain divergence. Motivated by this bound, we develop a novel
unified defense framework called Divergence Aware adveRsarial Training (DART),
which can be used in conjunction with a variety of standard UDA methods; e.g.,
DANN [Ganin and Lempitsky, 2015]. DART is applicable to general threat models,
including the popular $\ell_p$-norm model, and does not require heuristic
regularizers or architectural changes. We also release DomainRobust: a testbed
for evaluating robustness of UDA models to adversarial attacks. DomainRobust
consists of 4 multi-domain benchmark datasets (with 46 source-target pairs) and
7 meta-algorithms with a total of 11 variants. Our large-scale experiments
demonstrate that on average, DART significantly enhances model robustness on
all benchmarks compared to the state of the art, while maintaining competitive
standard accuracy. The relative improvement in robustness from DART reaches up
to 29.2% on the source-target domain pairs considered.</div><div><a href='http://arxiv.org/abs/2402.11120v1'>2402.11120v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09191v1")'>An Optimal Transport Approach for Computing Adversarial Training Lower
  Bounds in Multiclass Classification</div>
<div id='2401.09191v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T13:03:47Z</div><div>Authors: Nicolas Garcia Trillos, Matt Jacobs, Jakwang Kim, Matthew Werenski</div><div style='padding-top: 10px; width: 80ex'>Despite the success of deep learning-based algorithms, it is widely known
that neural networks may fail to be robust. A popular paradigm to enforce
robustness is adversarial training (AT), however, this introduces many
computational and theoretical difficulties. Recent works have developed a
connection between AT in the multiclass classification setting and
multimarginal optimal transport (MOT), unlocking a new set of tools to study
this problem. In this paper, we leverage the MOT connection to propose
computationally tractable numerical algorithms for computing universal lower
bounds on the optimal adversarial risk and identifying optimal classifiers. We
propose two main algorithms based on linear programming (LP) and entropic
regularization (Sinkhorn). Our key insight is that one can harmlessly truncate
the higher order interactions between classes, preventing the combinatorial run
times typically encountered in MOT problems. We validate these results with
experiments on MNIST and CIFAR-$10$, which demonstrate the tractability of our
approach.</div><div><a href='http://arxiv.org/abs/2401.09191v1'>2401.09191v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13624v1")'>Can overfitted deep neural networks in adversarial training generalize?
  -- An approximation viewpoint</div>
<div id='2401.13624v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T17:54:55Z</div><div>Authors: Zhongjie Shi, Fanghui Liu, Yuan Cao, Johan A. K. Suykens</div><div style='padding-top: 10px; width: 80ex'>Adversarial training is a widely used method to improve the robustness of
deep neural networks (DNNs) over adversarial perturbations. However, it is
empirically observed that adversarial training on over-parameterized networks
often suffers from the \textit{robust overfitting}: it can achieve almost zero
adversarial training error while the robust generalization performance is not
promising. In this paper, we provide a theoretical understanding of the
question of whether overfitted DNNs in adversarial training can generalize from
an approximation viewpoint. Specifically, our main results are summarized into
three folds: i) For classification, we prove by construction the existence of
infinitely many adversarial training classifiers on over-parameterized DNNs
that obtain arbitrarily small adversarial training error (overfitting), whereas
achieving good robust generalization error under certain conditions concerning
the data quality, well separated, and perturbation level. ii) Linear
over-parameterization (meaning that the number of parameters is only slightly
larger than the sample size) is enough to ensure such existence if the target
function is smooth enough. iii) For regression, our results demonstrate that
there also exist infinitely many overfitted DNNs with linear
over-parameterization in adversarial training that can achieve almost optimal
rates of convergence for the standard generalization error. Overall, our
analysis points out that robust overfitting can be avoided but the required
model capacity will depend on the smoothness of the target function, while a
robust generalization gap is inevitable. We hope our analysis will give a
better understanding of the mathematical foundations of robustness in DNNs from
an approximation view.</div><div><a href='http://arxiv.org/abs/2401.13624v1'>2401.13624v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15555v1")'>Deep Networks Always Grok and Here is Why</div>
<div id='2402.15555v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:59:31Z</div><div>Authors: Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk</div><div style='padding-top: 10px; width: 80ex'>Grokking, or delayed generalization, is a phenomenon where generalization in
a deep neural network (DNN) occurs long after achieving near zero training
error. Previous studies have reported the occurrence of grokking in specific
controlled settings, such as DNNs initialized with large-norm parameters or
transformers trained on algorithmic datasets. We demonstrate that grokking is
actually much more widespread and materializes in a wide range of practical
settings, such as training of a convolutional neural network (CNN) on CIFAR10
or a Resnet on Imagenette. We introduce the new concept of delayed robustness,
whereby a DNN groks adversarial examples and becomes robust, long after
interpolation and/or generalization. We develop an analytical explanation for
the emergence of both delayed generalization and delayed robustness based on a
new measure of the local complexity of a DNN's input-output mapping. Our local
complexity measures the density of the so-called 'linear regions' (aka, spline
partition regions) that tile the DNN input space, and serves as a utile
progress measure for training. We provide the first evidence that for
classification problems, the linear regions undergo a phase transition during
training whereafter they migrate away from the training samples (making the DNN
mapping smoother there) and towards the decision boundary (making the DNN
mapping less smooth there). Grokking occurs post phase transition as a robust
partition of the input space emerges thanks to the linearization of the DNN
mapping around the training points. Website: https://bit.ly/grok-adversarial</div><div><a href='http://arxiv.org/abs/2402.15555v1'>2402.15555v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05395v1")'>Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems
  trained with Gradient Descent</div>
<div id='2403.05395v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T15:45:13Z</div><div>Authors: Nathan Buskulic, Jalal Fadili, Yvain Quéau</div><div style='padding-top: 10px; width: 80ex'>Advanced machine learning methods, and more prominently neural networks, have
become standard to solve inverse problems over the last years. However, the
theoretical recovery guarantees of such methods are still scarce and difficult
to achieve. Only recently did unsupervised methods such as Deep Image Prior
(DIP) get equipped with convergence and recovery guarantees for generic loss
functions when trained through gradient flow with an appropriate
initialization. In this paper, we extend these results by proving that these
guarantees hold true when using gradient descent with an appropriately chosen
step-size/learning rate. We also show that the discretization only affects the
overparametrization bound for a two-layer DIP network by a constant and thus
that the different guarantees found for the gradient flow will hold for
gradient descent.</div><div><a href='http://arxiv.org/abs/2403.05395v1'>2403.05395v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01052v1")'>Weakly Convex Regularisers for Inverse Problems: Convergence of Critical
  Points and Primal-Dual Optimisation</div>
<div id='2402.01052v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T22:54:45Z</div><div>Authors: Zakhar Shumaylov, Jeremy Budd, Subhadip Mukherjee, Carola-Bibiane Schönlieb</div><div style='padding-top: 10px; width: 80ex'>Variational regularisation is the primary method for solving inverse
problems, and recently there has been considerable work leveraging deeply
learned regularisation for enhanced performance. However, few results exist
addressing the convergence of such regularisation, particularly within the
context of critical points as opposed to global minima. In this paper, we
present a generalised formulation of convergent regularisation in terms of
critical points, and show that this is achieved by a class of weakly convex
regularisers. We prove convergence of the primal-dual hybrid gradient method
for the associated variational problem, and, given a Kurdyka-Lojasiewicz
condition, an $\mathcal{O}(\log{k}/k)$ ergodic convergence rate. Finally,
applying this theory to learned regularisation, we prove universal
approximation for input weakly convex neural networks (IWCNN), and show
empirically that IWCNNs can lead to improved performance of learned adversarial
regularisers for computed tomography (CT) reconstruction.</div><div><a href='http://arxiv.org/abs/2402.01052v1'>2402.01052v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14069v1")'>Neural Sinkhorn Gradient Flow</div>
<div id='2401.14069v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T10:44:50Z</div><div>Authors: Huminhao Zhu, Fangyikang Wang, Chao Zhang, Hanbin Zhao, Hui Qian</div><div style='padding-top: 10px; width: 80ex'>Wasserstein Gradient Flows (WGF) with respect to specific functionals have
been widely used in the machine learning literature. Recently, neural networks
have been adopted to approximate certain intractable parts of the underlying
Wasserstein gradient flow and result in efficient inference procedures. In this
paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which
parametrizes the time-varying velocity field of the Wasserstein gradient flow
w.r.t. the Sinkhorn divergence to the target distribution starting a given
source distribution. We utilize the velocity field matching training scheme in
NSGF, which only requires samples from the source and target distribution to
compute an empirical velocity field approximation. Our theoretical analyses
show that as the sample size increases to infinity, the mean-field limit of the
empirical approximation converges to the true underlying velocity field. To
further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++
model is devised, which first follows the Sinkhorn flow to approach the image
manifold quickly ($\le 5$ NFEs) and then refines the samples along a simple
straight flow. Numerical experiments with synthetic and real-world benchmark
datasets support our theoretical results and demonstrate the effectiveness of
the proposed methods.</div><div><a href='http://arxiv.org/abs/2401.14069v1'>2401.14069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11722v1")'>Invertible Fourier Neural Operators for Tackling Both Forward and
  Inverse Problems</div>
<div id='2402.11722v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T22:16:43Z</div><div>Authors: Da Long, Shandian Zhe</div><div style='padding-top: 10px; width: 80ex'>Fourier Neural Operator (FNO) is a popular operator learning method, which
has demonstrated state-of-the-art performance across many tasks. However, FNO
is mainly used in forward prediction, yet a large family of applications rely
on solving inverse problems. In this paper, we propose an invertible Fourier
Neural Operator (iFNO) that tackles both the forward and inverse problems. We
designed a series of invertible Fourier blocks in the latent channel space to
share the model parameters, efficiently exchange the information, and mutually
regularize the learning for the bi-directional tasks. We integrated a
variational auto-encoder to capture the intrinsic structures within the input
space and to enable posterior inference so as to overcome challenges of
illposedness, data shortage, noises, etc. We developed a three-step process for
pre-training and fine tuning for efficient training. The evaluations on five
benchmark problems have demonstrated the effectiveness of our approach.</div><div><a href='http://arxiv.org/abs/2402.11722v1'>2402.11722v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12236v2")'>The Surprising Harmfulness of Benign Overfitting for Adversarial
  Robustness</div>
<div id='2401.12236v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T15:40:46Z</div><div>Authors: Yifan Hao, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>Recent empirical and theoretical studies have established the generalization
capabilities of large machine learning models that are trained to
(approximately or exactly) fit noisy data. In this work, we prove a surprising
result that even if the ground truth itself is robust to adversarial examples,
and the benignly overfitted model is benign in terms of the ``standard''
out-of-sample risk objective, this benign overfitting process can be harmful
when out-of-sample data are subject to adversarial manipulation. More
specifically, our main results contain two parts: (i) the min-norm estimator in
overparameterized linear model always leads to adversarial vulnerability in the
``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result
between the standard risk and the ``adversarial'' risk of every ridge
regression estimator, implying that under suitable conditions these two items
cannot both be small at the same time by any single choice of the ridge
regularization parameter. Furthermore, under the lazy training regime, we
demonstrate parallel results on two-layer neural tangent kernel (NTK) model,
which align with empirical observations in deep neural networks. Our finding
provides theoretical insights into the puzzling phenomenon observed in
practice, where the true target function (e.g., human) is robust against
adverasrial attack, while beginly overfitted neural networks lead to models
that are not robust.</div><div><a href='http://arxiv.org/abs/2401.12236v2'>2401.12236v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06903v1")'>Benign overfitting in leaky ReLU networks with moderate input dimension</div>
<div id='2403.06903v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:56:01Z</div><div>Authors: Kedar Karhadkar, Erin George, Michael Murray, Guido Montúfar, Deanna Needell</div><div style='padding-top: 10px; width: 80ex'>The problem of benign overfitting asks whether it is possible for a model to
perfectly fit noisy training data and still generalize well. We study benign
overfitting in two-layer leaky ReLU networks trained with the hinge loss on a
binary classification task. We consider input data which can be decomposed into
the sum of a common signal and a random noise component, which lie on subspaces
orthogonal to one another. We characterize conditions on the signal to noise
ratio (SNR) of the model parameters giving rise to benign versus non-benign, or
harmful, overfitting: in particular, if the SNR is high then benign overfitting
occurs, conversely if the SNR is low then harmful overfitting occurs. We
attribute both benign and non-benign overfitting to an approximate margin
maximization property and show that leaky ReLU networks trained on hinge loss
with Gradient Descent (GD) satisfy this property. In contrast to prior work we
do not require near orthogonality conditions on the training data: notably, for
input dimension $d$ and training sample size $n$, while prior work shows
asymptotically optimal error when $d = \Omega(n^2 \log n)$, here we require
only $d = \Omega\left(n \log \frac{1}{\epsilon}\right)$ to obtain error within
$\epsilon$ of optimal.</div><div><a href='http://arxiv.org/abs/2403.06903v1'>2403.06903v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03967v1")'>Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability</div>
<div id='2403.03967v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:41:21Z</div><div>Authors: Rajdeep Haldar, Yue Xing, Qifan Song</div><div style='padding-top: 10px; width: 80ex'>The existence of adversarial attacks on machine learning models imperceptible
to a human is still quite a mystery from a theoretical perspective. In this
work, we introduce two notions of adversarial attacks: natural or on-manifold
attacks, which are perceptible by a human/oracle, and unnatural or off-manifold
attacks, which are not. We argue that the existence of the off-manifold attacks
is a natural consequence of the dimension gap between the intrinsic and ambient
dimensions of the data. For 2-layer ReLU networks, we prove that even though
the dimension gap does not affect generalization performance on samples drawn
from the observed data space, it makes the clean-trained model more vulnerable
to adversarial perturbations in the off-manifold direction of the data space.
Our main results provide an explicit relationship between the
$\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the
dimension gap.</div><div><a href='http://arxiv.org/abs/2403.03967v1'>2403.03967v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03576v1")'>Generalization Properties of Adversarial Training for $\ell_0$-Bounded
  Adversarial Attacks</div>
<div id='2402.03576v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:57:33Z</div><div>Authors: Payam Delgosha, Hamed Hassani, Ramtin Pedarsani</div><div style='padding-top: 10px; width: 80ex'>We have widely observed that neural networks are vulnerable to small additive
perturbations to the input causing misclassification. In this paper, we focus
on the $\ell_0$-bounded adversarial attacks, and aim to theoretically
characterize the performance of adversarial training for an important class of
truncated classifiers. Such classifiers are shown to have strong performance
empirically, as well as theoretically in the Gaussian mixture model, in the
$\ell_0$-adversarial setting. The main contribution of this paper is to prove a
novel generalization bound for the binary classification setting with
$\ell_0$-bounded adversarial perturbation that is distribution-independent.
Deriving a generalization bound in this setting has two main challenges: (i)
the truncated inner product which is highly non-linear; and (ii) maximization
over the $\ell_0$ ball due to adversarial training is non-convex and highly
non-smooth. To tackle these challenges, we develop new coding techniques for
bounding the combinatorial dimension of the truncated hypothesis class.</div><div><a href='http://arxiv.org/abs/2402.03576v1'>2402.03576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07639v1")'>Tighter Bounds on the Information Bottleneck with Application to Deep
  Learning</div>
<div id='2402.07639v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T13:24:32Z</div><div>Authors: Nir Weingarten, Zohar Yakhini, Moshe Butman, Ran Gilad-Bachrach</div><div style='padding-top: 10px; width: 80ex'>Deep Neural Nets (DNNs) learn latent representations induced by their
downstream task, objective function, and other parameters. The quality of the
learned representations impacts the DNN's generalization ability and the
coherence of the emerging latent space. The Information Bottleneck (IB)
provides a hypothetically optimal framework for data modeling, yet it is often
intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired
variational methods to approximate bounds on mutual information, resulting in
improved robustness to adversarial attacks. This work introduces a new and
tighter variational bound for the IB, improving performance of previous
IB-inspired DNNs. These advancements strengthen the case for the IB and its
variational approximations as a data modeling framework, and provide a simple
method to significantly enhance the adversarial robustness of classifier DNNs.</div><div><a href='http://arxiv.org/abs/2402.07639v1'>2402.07639v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15262v1")'>Asymptotic Behavior of Adversarial Training Estimator under
  $\ell_\infty$-Perturbation</div>
<div id='2401.15262v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T01:16:33Z</div><div>Authors: Yiling Xie, Xiaoming Huo</div><div style='padding-top: 10px; width: 80ex'>Adversarial training has been proposed to hedge against adversarial attacks
in machine learning and statistical models. This paper focuses on adversarial
training under $\ell_\infty$-perturbation, which has recently attracted much
research attention. The asymptotic behavior of the adversarial training
estimator is investigated in the generalized linear model. The results imply
that the limiting distribution of the adversarial training estimator under
$\ell_\infty$-perturbation could put a positive probability mass at $0$ when
the true parameter is $0$, providing a theoretical guarantee of the associated
sparsity-recovery ability. Alternatively, a two-step procedure is proposed --
adaptive adversarial training, which could further improve the performance of
adversarial training under $\ell_\infty$-perturbation. Specifically, the
proposed procedure could achieve asymptotic unbiasedness and variable-selection
consistency. Numerical experiments are conducted to show the sparsity-recovery
ability of adversarial training under $\ell_\infty$-perturbation and to compare
the empirical performance between classic adversarial training and adaptive
adversarial training.</div><div><a href='http://arxiv.org/abs/2401.15262v1'>2401.15262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03156v1")'>Data-Dependent Stability Analysis of Adversarial Training</div>
<div id='2401.03156v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:18:04Z</div><div>Authors: Yihan Wang, Shuang Liu, Xiao-Shan Gao</div><div style='padding-top: 10px; width: 80ex'>Stability analysis is an essential aspect of studying the generalization
ability of deep learning, as it involves deriving generalization bounds for
stochastic gradient descent-based training algorithms. Adversarial training is
the most widely used defense against adversarial example attacks. However,
previous generalization bounds for adversarial training have not included
information regarding the data distribution. In this paper, we fill this gap by
providing generalization bounds for stochastic gradient descent-based
adversarial training that incorporate data distribution information. We utilize
the concepts of on-average stability and high-order approximate Lipschitz
conditions to examine how changes in data distribution and adversarial budget
can affect robust generalization gaps. Our derived generalization bounds for
both convex and non-convex losses are at least as good as the uniform
stability-based counterparts which do not include data distribution
information. Furthermore, our findings demonstrate how distribution shifts from
data poisoning attacks can impact robust generalization.</div><div><a href='http://arxiv.org/abs/2401.03156v1'>2401.03156v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14948v1")'>Conserve-Update-Revise to Cure Generalization and Robustness Trade-off
  in Adversarial Training</div>
<div id='2401.14948v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T15:33:39Z</div><div>Authors: Shruthi Gowda, Bahram Zonooz, Elahe Arani</div><div style='padding-top: 10px; width: 80ex'>Adversarial training improves the robustness of neural networks against
adversarial attacks, albeit at the expense of the trade-off between standard
and robust generalization. To unveil the underlying factors driving this
phenomenon, we examine the layer-wise learning capabilities of neural networks
during the transition from a standard to an adversarial setting. Our empirical
findings demonstrate that selectively updating specific layers while preserving
others can substantially enhance the network's learning capacity. We therefore
propose CURE, a novel training framework that leverages a gradient prominence
criterion to perform selective conservation, updating, and revision of weights.
Importantly, CURE is designed to be dataset- and architecture-agnostic,
ensuring its applicability across various scenarios. It effectively tackles
both memorization and overfitting issues, thus enhancing the trade-off between
robustness and generalization and additionally, this training approach also
aids in mitigating "robust overfitting". Furthermore, our study provides
valuable insights into the mechanisms of selective adversarial training and
offers a promising avenue for future research.</div><div><a href='http://arxiv.org/abs/2401.14948v1'>2401.14948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11733v1")'>The Effectiveness of Random Forgetting for Robust Generalization</div>
<div id='2402.11733v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T23:14:40Z</div><div>Authors: Vijaya Raghavan T Ramkumar, Bahram Zonooz, Elahe Arani</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks are susceptible to adversarial attacks, which can
compromise their performance and accuracy. Adversarial Training (AT) has
emerged as a popular approach for protecting neural networks against such
attacks. However, a key challenge of AT is robust overfitting, where the
network's robust performance on test data deteriorates with further training,
thus hindering generalization. Motivated by the concept of active forgetting in
the brain, we introduce a novel learning paradigm called "Forget to Mitigate
Overfitting (FOMO)". FOMO alternates between the forgetting phase, which
randomly forgets a subset of weights and regulates the model's information
through weight reinitialization, and the relearning phase, which emphasizes
learning generalizable features. Our experiments on benchmark datasets and
adversarial attacks show that FOMO alleviates robust overfitting by
significantly reducing the gap between the best and last robust test accuracy
while improving the state-of-the-art robustness. Furthermore, FOMO provides a
better trade-off between standard and robust accuracy, outperforming baseline
adversarial methods. Finally, our framework is robust to AutoAttacks and
increases generalization in many real-world scenarios.</div><div><a href='http://arxiv.org/abs/2402.11733v1'>2402.11733v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11196v1")'>Maintaining Adversarial Robustness in Continuous Learning</div>
<div id='2402.11196v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T05:14:47Z</div><div>Authors: Xiaolei Ru, Xiaowei Cao, Zijia Liu, Jack Murdoch Moore, Xin-Ya Zhang, Xia Zhu, Wenjia Wei, Gang Yan</div><div style='padding-top: 10px; width: 80ex'>Adversarial robustness is essential for security and reliability of machine
learning systems. However, the adversarial robustness gained by sophisticated
defense algorithms is easily erased as the neural network evolves to learn new
tasks. This vulnerability can be addressed by fostering a novel capability for
neural networks, termed continual robust learning, which focuses on both the
(classification) performance and adversarial robustness on previous tasks
during continuous learning. To achieve continuous robust learning, we propose
an approach called Double Gradient Projection that projects the gradients for
weight updates orthogonally onto two crucial subspaces -- one for stabilizing
the smoothed sample gradients and another for stabilizing the final outputs of
the neural network. The experimental results on four benchmarks demonstrate
that the proposed approach effectively maintains continuous robustness against
strong adversarial attacks, outperforming the baselines formed by combining the
existing defense strategies and continual learning methods.</div><div><a href='http://arxiv.org/abs/2402.11196v1'>2402.11196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10461v1")'>Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance
  ML Robustness</div>
<div id='2403.10461v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T16:52:25Z</div><div>Authors: Mohamed elShehaby, Aditya Kotha, Ashraf Matrawy</div><div style='padding-top: 10px; width: 80ex'>Machine Learning (ML) is susceptible to adversarial attacks that aim to trick
ML models, making them produce faulty predictions. Adversarial training was
found to increase the robustness of ML models against these attacks. However,
in network and cybersecurity, obtaining labeled training and adversarial
training data is challenging and costly. Furthermore, concept drift deepens the
challenge, particularly in dynamic domains like network and cybersecurity, and
requires various models to conduct periodic retraining. This letter introduces
Adaptive Continuous Adversarial Training (ACAT) to continuously integrate
adversarial training samples into the model during ongoing learning sessions,
using real-world detected adversarial data, to enhance model resilience against
evolving adversarial threats. ACAT is an adaptive defense mechanism that
utilizes periodic retraining to effectively counter adversarial attacks while
mitigating catastrophic forgetting. Our approach also reduces the total time
required for adversarial sample detection, especially in environments such as
network security where the rate of attacks could be very high. Traditional
detection processes that involve two stages may result in lengthy procedures.
Experimental results using a SPAM detection dataset demonstrate that with ACAT,
the accuracy of the SPAM filter increased from 69% to over 88% after just three
retraining sessions. Furthermore, ACAT outperforms conventional adversarial
sample detectors, providing faster decision times, up to four times faster in
some cases.</div><div><a href='http://arxiv.org/abs/2403.10461v1'>2403.10461v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19322v2")'>Verification of Neural Networks' Global Robustness</div>
<div id='2402.19322v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T16:27:59Z</div><div>Authors: Anan Kabaha, Dana Drachsler-Cohen</div><div style='padding-top: 10px; width: 80ex'>Neural networks are successful in various applications but are also
susceptible to adversarial attacks. To show the safety of network classifiers,
many verifiers have been introduced to reason about the local robustness of a
given input to a given perturbation. While successful, local robustness cannot
generalize to unseen inputs. Several works analyze global robustness
properties, however, neither can provide a precise guarantee about the cases
where a network classifier does not change its classification. In this work, we
propose a new global robustness property for classifiers aiming at finding the
minimal globally robust bound, which naturally extends the popular local
robustness property for classifiers. We introduce VHAGaR, an anytime verifier
for computing this bound. VHAGaR relies on three main ideas: encoding the
problem as a mixed-integer programming and pruning the search space by
identifying dependencies stemming from the perturbation or the network's
computation and generalizing adversarial attacks to unknown inputs. We evaluate
VHAGaR on several datasets and classifiers and show that, given a three hour
timeout, the average gap between the lower and upper bound on the minimal
globally robust bound computed by VHAGaR is 1.9, while the gap of an existing
global robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than
this verifier. Our results further indicate that leveraging dependencies and
adversarial attacks makes VHAGaR 78.6x faster.</div><div><a href='http://arxiv.org/abs/2402.19322v2'>2402.19322v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13441v1")'>Robustness Verifcation in Neural Networks</div>
<div id='2403.13441v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T09:34:38Z</div><div>Authors: Adrian Wurm</div><div style='padding-top: 10px; width: 80ex'>In this paper we investigate formal verification problems for Neural Network
computations. Of central importance will be various robustness and minimization
problems such as: Given symbolic specifications of allowed inputs and outputs
in form of Linear Programming instances, one question is whether there do exist
valid inputs such that the network computes a valid output? And does this
property hold for all valid inputs? Do two given networks compute the same
function? Is there a smaller network computing the same function?
  The complexity of these questions have been investigated recently from a
practical point of view and approximated by heuristic algorithms. We complement
these achievements by giving a theoretical framework that enables us to
interchange security and efficiency questions in neural networks and analyze
their computational complexities. We show that the problems are conquerable in
a semi-linear setting, meaning that for piecewise linear activation functions
and when the sum- or maximum metric is used, most of them are in P or in NP at
most.</div><div><a href='http://arxiv.org/abs/2403.13441v1'>2403.13441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09963v3")'>Why are Sensitive Functions Hard for Transformers?</div>
<div id='2402.09963v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:17:51Z</div><div>Authors: Michael Hahn, Mark Rofin</div><div style='padding-top: 10px; width: 80ex'>Empirical studies have identified a range of learnability biases and
limitations of transformers, such as a persistent difficulty in learning to
compute simple formal languages such as PARITY, and a bias towards low-degree
functions. However, theoretical understanding remains limited, with existing
expressiveness theory either overpredicting or underpredicting realistic
learning abilities. We prove that, under the transformer architecture, the loss
landscape is constrained by the input-space sensitivity: Transformers whose
output is sensitive to many parts of the input string inhabit isolated points
in parameter space, leading to a low-sensitivity bias in generalization. We
show theoretically and empirically that this theory unifies a broad array of
empirical observations about the learning abilities and biases of transformers,
such as their generalization bias towards low sensitivity and low degree, and
difficulty in length generalization for PARITY. This shows that understanding
transformers' inductive biases requires studying not just their in-principle
expressivity, but also their loss landscape.</div><div><a href='http://arxiv.org/abs/2402.09963v3'>2402.09963v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14961v1")'>End-To-End Set-Based Training for Neural Network Verification</div>
<div id='2401.14961v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T15:52:41Z</div><div>Authors: Lukas Koller, Tobias Ladner, Matthias Althoff</div><div style='padding-top: 10px; width: 80ex'>Neural networks are vulnerable to adversarial attacks, i.e., small input
perturbations can result in substantially different outputs of a neural
network. Safety-critical environments require neural networks that are robust
against input perturbations. However, training and formally verifying robust
neural networks is challenging. We address this challenge by employing, for the
first time, a end-to-end set-based training procedure that trains robust neural
networks for formal verification. Our training procedure drastically simplifies
the subsequent formal robustness verification of the trained neural network.
While previous research has predominantly focused on augmenting neural network
training with adversarial attacks, our approach leverages set-based computing
to train neural networks with entire sets of perturbed inputs. Moreover, we
demonstrate that our set-based training procedure effectively trains robust
neural networks, which are easier to verify. In many cases, set-based trained
neural networks outperform neural networks trained with state-of-the-art
adversarial attacks.</div><div><a href='http://arxiv.org/abs/2401.14961v1'>2401.14961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14678v1")'>Towards a Framework for Deep Learning Certification in Safety-Critical
  Applications Using Inherently Safe Design and Run-Time Error Detection</div>
<div id='2403.14678v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:38:45Z</div><div>Authors: Romeo Valentin</div><div style='padding-top: 10px; width: 80ex'>Although an ever-growing number of applications employ deep learning based
systems for prediction, decision-making, or state estimation, almost no
certification processes have been established that would allow such systems to
be deployed in safety-critical applications. In this work we consider
real-world problems arising in aviation and other safety-critical areas, and
investigate their requirements for a certified model. To this end, we
investigate methodologies from the machine learning research community aimed
towards verifying robustness and reliability of deep learning systems, and
evaluate these methodologies with regard to their applicability to real-world
problems. Then, we establish a new framework towards deep learning
certification based on (i) inherently safe design, and (ii) run-time error
detection. Using a concrete use case from aviation, we show how deep learning
models can recover disentangled variables through the use of weakly-supervised
representation learning. We argue that such a system design is inherently less
prone to common model failures, and can be verified to encode underlying
mechanisms governing the data. Then, we investigate four techniques related to
the run-time safety of a model, namely (i) uncertainty quantification, (ii)
out-of-distribution detection, (iii) feature collapse, and (iv) adversarial
attacks. We evaluate each for their applicability and formulate a set of
desiderata that a certified model should fulfill. Finally, we propose a novel
model structure that exhibits all desired properties discussed in this work,
and is able to make regression and uncertainty predictions, as well as detect
out-of-distribution inputs, while requiring no regression labels to train. We
conclude with a discussion of the current state and expected future progress of
deep learning certification, and its industrial and social implications.</div><div><a href='http://arxiv.org/abs/2403.14678v1'>2403.14678v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02283v1")'>DEM: A Method for Certifying Deep Neural Network Classifier Outputs in
  Aerospace</div>
<div id='2401.02283v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T14:01:24Z</div><div>Authors: Guy Katz, Natan Levy, Idan Refaeli, Raz Yerushalmi</div><div style='padding-top: 10px; width: 80ex'>Software development in the aerospace domain requires adhering to strict,
high-quality standards. While there exist regulatory guidelines for commercial
software in this domain (e.g., ARP-4754 and DO-178), these do not apply to
software with deep neural network (DNN) components. Consequently, it is unclear
how to allow aerospace systems to benefit from the deep learning revolution.
Our work here seeks to address this challenge with a novel, output-centric
approach for DNN certification. Our method employs statistical verification
techniques, and has the key advantage of being able to flag specific inputs for
which the DNN's output may be unreliable - so that they may be later inspected
by a human expert. To achieve this, our method conducts a statistical analysis
of the DNN's predictions for other, nearby inputs, in order to detect
inconsistencies. This is in contrast to existing techniques, which typically
attempt to certify the entire DNN, as opposed to individual outputs. Our method
uses the DNN as a black-box, and makes no assumptions about its topology. We
hope that this work constitutes another step towards integrating DNNs in
safety-critical applications - especially in the aerospace domain, where high
standards of quality and reliability are crucial.</div><div><a href='http://arxiv.org/abs/2401.02283v1'>2401.02283v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13502v2")'>Adversarial Attacks and Defenses in Automated Control Systems: A
  Comprehensive Benchmark</div>
<div id='2403.13502v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T10:59:06Z</div><div>Authors: Vitaliy Pozdnyakov, Aleksandr Kovalenko, Ilya Makarov, Mikhail Drobyshevskiy, Kirill Lukyanov</div><div style='padding-top: 10px; width: 80ex'>Integrating machine learning into Automated Control Systems (ACS) enhances
decision-making in industrial process management. One of the limitations to the
widespread adoption of these technologies in industry is the vulnerability of
neural networks to adversarial attacks. This study explores the threats in
deploying deep learning models for fault diagnosis in ACS using the Tennessee
Eastman Process dataset. By evaluating three neural networks with different
architectures, we subject them to six types of adversarial attacks and explore
five different defense methods. Our results highlight the strong vulnerability
of models to adversarial samples and the varying effectiveness of defense
strategies. We also propose a novel protection approach by combining multiple
defense methods and demonstrate it's efficacy. This research contributes
several insights into securing machine learning within ACS, ensuring robust
fault diagnosis in industrial processes.</div><div><a href='http://arxiv.org/abs/2403.13502v2'>2403.13502v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14628v1")'>Inferring Data Preconditions from Deep Learning Models for Trustworthy
  Prediction in Deployment</div>
<div id='2401.14628v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T03:47:18Z</div><div>Authors: Shibbir Ahmed, Hongyang Gao, Hridesh Rajan</div><div style='padding-top: 10px; width: 80ex'>Deep learning models are trained with certain assumptions about the data
during the development stage and then used for prediction in the deployment
stage. It is important to reason about the trustworthiness of the model's
predictions with unseen data during deployment. Existing methods for specifying
and verifying traditional software are insufficient for this task, as they
cannot handle the complexity of DNN model architecture and expected outcomes.
In this work, we propose a novel technique that uses rules derived from neural
network computations to infer data preconditions for a DNN model to determine
the trustworthiness of its predictions. Our approach, DeepInfer involves
introducing a novel abstraction for a trained DNN model that enables weakest
precondition reasoning using Dijkstra's Predicate Transformer Semantics. By
deriving rules over the inductive type of neural network abstract
representation, we can overcome the matrix dimensionality issues that arise
from the backward non-linear computation from the output layer to the input
layer. We utilize the weakest precondition computation using rules of each kind
of activation function to compute layer-wise precondition from the given
postcondition on the final output of a deep neural network. We extensively
evaluated DeepInfer on 29 real-world DNN models using four different datasets
collected from five different sources and demonstrated the utility,
effectiveness, and performance improvement over closely related work. DeepInfer
efficiently detects correct and incorrect predictions of high-accuracy models
with high recall (0.98) and high F-1 score (0.84) and has significantly
improved over prior technique, SelfChecker. The average runtime overhead of
DeepInfer is low, 0.22 sec for all unseen datasets. We also compared runtime
overhead using the same hardware settings and found that DeepInfer is 3.27
times faster than SelfChecker.</div><div><a href='http://arxiv.org/abs/2401.14628v1'>2401.14628v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07095v1")'>Overcoming the Paradox of Certified Training with Gaussian Smoothing</div>
<div id='2403.07095v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T18:44:36Z</div><div>Authors: Stefan Balauca, Mark Niklas Müller, Yuhao Mao, Maximilian Baader, Marc Fischer, Martin Vechev</div><div style='padding-top: 10px; width: 80ex'>Training neural networks with high certified accuracy against adversarial
examples remains an open problem despite significant efforts. While
certification methods can effectively leverage tight convex relaxations for
bound computation, in training, these methods perform worse than looser
relaxations. Prior work hypothesized that this is caused by the discontinuity
and perturbation sensitivity of the loss surface induced by these tighter
relaxations. In this work, we show theoretically that Gaussian Loss Smoothing
can alleviate both of these issues. We confirm this empirically by proposing a
certified training method combining PGPE, an algorithm computing gradients of a
smoothed loss, with different convex relaxations. When using this training
method, we observe that tighter bounds indeed lead to strictly better networks
that can outperform state-of-the-art methods on the same network. While scaling
PGPE-based training remains challenging due to high computational cost, our
results clearly demonstrate the promise of Gaussian Loss Smoothing for training
certifiably robust neural networks.</div><div><a href='http://arxiv.org/abs/2403.07095v1'>2403.07095v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12550v1")'>UR4NNV: Neural Network Verification, Under-approximation Reachability
  Works!</div>
<div id='2401.12550v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T08:19:00Z</div><div>Authors: Zhen Liang, Taoran Wu, Ran Zhao, Bai Xue, Ji Wang, Wenjing Yang, Shaojun Deng, Wanwei Liu</div><div style='padding-top: 10px; width: 80ex'>Recently, formal verification of deep neural networks (DNNs) has garnered
considerable attention, and over-approximation based methods have become
popular due to their effectiveness and efficiency. However, these strategies
face challenges in addressing the "unknown dilemma" concerning whether the
exact output region or the introduced approximation error violates the property
in question. To address this, this paper introduces the UR4NNV verification
framework, which utilizes under-approximation reachability analysis for DNN
verification for the first time. UR4NNV focuses on DNNs with Rectified Linear
Unit (ReLU) activations and employs a binary tree branch-based
under-approximation algorithm. In each epoch, UR4NNV under-approximates a
sub-polytope of the reachable set and verifies this polytope against the given
property. Through a trial-and-error approach, UR4NNV effectively falsifies DNN
properties while providing confidence levels when reaching verification epoch
bounds and failing falsifying properties. Experimental comparisons with
existing verification methods demonstrate the effectiveness and efficiency of
UR4NNV, significantly reducing the impact of the "unknown dilemma".</div><div><a href='http://arxiv.org/abs/2401.12550v1'>2401.12550v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07308v1")'>Verification-Aided Learning of Neural Network Barrier Functions with
  Termination Guarantees</div>
<div id='2403.07308v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T04:29:43Z</div><div>Authors: Shaoru Chen, Lekan Molu, Mahyar Fazlyab</div><div style='padding-top: 10px; width: 80ex'>Barrier functions are a general framework for establishing a safety guarantee
for a system. However, there is no general method for finding these functions.
To address this shortcoming, recent approaches use self-supervised learning
techniques to learn these functions using training data that are periodically
generated by a verification procedure, leading to a verification-aided learning
framework. Despite its immense potential in automating barrier function
synthesis, the verification-aided learning framework does not have termination
guarantees and may suffer from a low success rate of finding a valid barrier
function in practice. In this paper, we propose a holistic approach to address
these drawbacks. With a convex formulation of the barrier function synthesis,
we propose to first learn an empirically well-behaved NN basis function and
then apply a fine-tuning algorithm that exploits the convexity and
counterexamples from the verification failure to find a valid barrier function
with finite-step termination guarantees: if there exist valid barrier
functions, the fine-tuning algorithm is guaranteed to find one in a finite
number of iterations. We demonstrate that our fine-tuning method can
significantly boost the performance of the verification-aided learning
framework on examples of different scales and using various neural network
verifiers.</div><div><a href='http://arxiv.org/abs/2403.07308v1'>2403.07308v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19102v1")'>FlatNAS: optimizing Flatness in Neural Architecture Search for
  Out-of-Distribution Robustness</div>
<div id='2402.19102v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T12:33:14Z</div><div>Authors: Matteo Gambella, Fabrizio Pittorino, Manuel Roveri</div><div style='padding-top: 10px; width: 80ex'>Neural Architecture Search (NAS) paves the way for the automatic definition
of Neural Network (NN) architectures, attracting increasing research attention
and offering solutions in various scenarios. This study introduces a novel NAS
solution, called Flat Neural Architecture Search (FlatNAS), which explores the
interplay between a novel figure of merit based on robustness to weight
perturbations and single NN optimization with Sharpness-Aware Minimization
(SAM). FlatNAS is the first work in the literature to systematically explore
flat regions in the loss landscape of NNs in a NAS procedure, while jointly
optimizing their performance on in-distribution data, their out-of-distribution
(OOD) robustness, and constraining the number of parameters in their
architecture. Differently from current studies primarily concentrating on OOD
algorithms, FlatNAS successfully evaluates the impact of NN architectures on
OOD robustness, a crucial aspect in real-world applications of machine and deep
learning. FlatNAS achieves a good trade-off between performance, OOD
generalization, and the number of parameters, by using only in-distribution
data in the NAS exploration. The OOD robustness of the NAS-designed models is
evaluated by focusing on robustness to input data corruptions, using popular
benchmark datasets in the literature.</div><div><a href='http://arxiv.org/abs/2402.19102v1'>2402.19102v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15210v1")'>Early Period of Training Impacts Out-of-Distribution Generalization</div>
<div id='2403.15210v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T13:52:53Z</div><div>Authors: Chen Cecilia Liu, Iryna Gurevych</div><div style='padding-top: 10px; width: 80ex'>Prior research has found that differences in the early period of neural
network training significantly impact the performance of in-distribution (ID)
tasks. However, neural networks are often sensitive to out-of-distribution
(OOD) data, making them less reliable in downstream applications. Yet, the
impact of the early training period on OOD generalization remains understudied
due to its complexity and lack of effective analytical methodologies. In this
work, we investigate the relationship between learning dynamics and OOD
generalization during the early period of neural network training. We utilize
the trace of Fisher Information and sharpness, with a focus on gradual
unfreezing (i.e. progressively unfreezing parameters during training) as the
methodology for investigation. Through a series of empirical experiments, we
show that 1) selecting the number of trainable parameters at different times
during training, i.e. realized by gradual unfreezing -- has a minuscule impact
on ID results, but greatly affects the generalization to OOD data; 2) the
absolute values of sharpness and trace of Fisher Information at the initial
period of training are not indicative for OOD generalization, but the relative
values could be; 3) the trace of Fisher Information and sharpness may be used
as indicators for the removal of interventions during early period of training
for better OOD generalization.</div><div><a href='http://arxiv.org/abs/2403.15210v1'>2403.15210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12532v1")'>DAFA: Distance-Aware Fair Adversarial Training</div>
<div id='2401.12532v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T07:15:47Z</div><div>Authors: Hyungyu Lee, Saehyung Lee, Hyemi Jang, Junsung Park, Ho Bae, Sungroh Yoon</div><div style='padding-top: 10px; width: 80ex'>The disparity in accuracy between classes in standard training is amplified
during adversarial training, a phenomenon termed the robust fairness problem.
Existing methodologies aimed to enhance robust fairness by sacrificing the
model's performance on easier classes in order to improve its performance on
harder ones. However, we observe that under adversarial attacks, the majority
of the model's predictions for samples from the worst class are biased towards
classes similar to the worst class, rather than towards the easy classes.
Through theoretical and empirical analysis, we demonstrate that robust fairness
deteriorates as the distance between classes decreases. Motivated by these
insights, we introduce the Distance-Aware Fair Adversarial training (DAFA)
methodology, which addresses robust fairness by taking into account the
similarities between classes. Specifically, our method assigns distinct loss
weights and adversarial margins to each class and adjusts them to encourage a
trade-off in robustness among similar classes. Experimental results across
various datasets demonstrate that our method not only maintains average robust
accuracy but also significantly improves the worst robust accuracy, indicating
a marked improvement in robust fairness compared to existing methods.</div><div><a href='http://arxiv.org/abs/2401.12532v1'>2401.12532v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16760v1")'>One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware
  Quantization Training</div>
<div id='2401.16760v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T05:42:54Z</div><div>Authors: Lianbo Ma, Yuee Zhou, Jianlun Ma, Guo Yu, Qing Li</div><div style='padding-top: 10px; width: 80ex'>Weight quantization is an effective technique to compress deep neural
networks for their deployment on edge devices with limited resources.
Traditional loss-aware quantization methods commonly use the quantized gradient
to replace the full-precision gradient. However, we discover that the gradient
error will lead to an unexpected zig-zagging-like issue in the gradient descent
learning procedures, where the gradient directions rapidly oscillate or
zig-zag, and such issue seriously slows down the model convergence.
Accordingly, this paper proposes a one-step forward and backtrack way for
loss-aware quantization to get more accurate and stable gradient direction to
defy this issue. During the gradient descent learning, a one-step forward
search is designed to find the trial gradient of the next-step, which is
adopted to adjust the gradient of current step towards the direction of fast
convergence. After that, we backtrack the current step to update the
full-precision and quantized weights through the current-step gradient and the
trial gradient. A series of theoretical analysis and experiments on benchmark
deep models have demonstrated the effectiveness and competitiveness of the
proposed method, and our method especially outperforms others on the
convergence performance.</div><div><a href='http://arxiv.org/abs/2401.16760v1'>2401.16760v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08821v1")'>Effective Gradient Sample Size via Variation Estimation for Accelerating
  Sharpness aware Minimization</div>
<div id='2403.08821v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T05:48:05Z</div><div>Authors: Jiaxin Deng, Junbiao Pang, Baochang Zhang, Tian Wang</div><div style='padding-top: 10px; width: 80ex'>Sharpness-aware Minimization (SAM) has been proposed recently to improve
model generalization ability. However, SAM calculates the gradient twice in
each optimization step, thereby doubling the computation costs compared to
stochastic gradient descent (SGD). In this paper, we propose a simple yet
efficient sampling method to significantly accelerate SAM. Concretely, we
discover that the gradient of SAM is a combination of the gradient of SGD and
the Projection of the Second-order gradient matrix onto the First-order
gradient (PSF). PSF exhibits a gradually increasing frequency of change during
the training process. To leverage this observation, we propose an adaptive
sampling method based on the variation of PSF, and we reuse the sampled PSF for
non-sampling iterations. Extensive empirical results illustrate that the
proposed method achieved state-of-the-art accuracies comparable to SAM on
diverse network architectures.</div><div><a href='http://arxiv.org/abs/2403.08821v1'>2403.08821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19442v1")'>Training Dynamics of Multi-Head Softmax Attention for In-Context
  Learning: Emergence, Convergence, and Optimality</div>
<div id='2402.19442v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:43:52Z</div><div>Authors: Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang</div><div style='padding-top: 10px; width: 80ex'>We study the dynamics of gradient flow for training a multi-head softmax
attention model for in-context learning of multi-task linear regression. We
establish the global convergence of gradient flow under suitable choices of
initialization. In addition, we prove that an interesting "task allocation"
phenomenon emerges during the gradient flow dynamics, where each attention head
focuses on solving a single task of the multi-task model. Specifically, we
prove that the gradient flow dynamics can be split into three phases -- a
warm-up phase where the loss decreases rather slowly and the attention heads
gradually build up their inclination towards individual tasks, an emergence
phase where each head selects a single task and the loss rapidly decreases, and
a convergence phase where the attention parameters converge to a limit.
Furthermore, we prove the optimality of gradient flow in the sense that the
limiting model learned by gradient flow is on par with the best possible
multi-head softmax attention model up to a constant factor. Our analysis also
delineates a strict separation in terms of the prediction accuracy of ICL
between single-head and multi-head attention models. The key technique for our
convergence analysis is to map the gradient flow dynamics in the parameter
space to a set of ordinary differential equations in the spectral domain, where
the relative magnitudes of the semi-singular values of the attention weights
determines task allocation. To our best knowledge, our work provides the first
convergence result for the multi-head softmax attention model.</div><div><a href='http://arxiv.org/abs/2402.19442v1'>2402.19442v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01879v1")'>$σ$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial
  Examples</div>
<div id='2402.01879v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:08:11Z</div><div>Authors: Antonio Emanuele Cinà, Francesco Villani, Maura Pintor, Lea Schönherr, Battista Biggio, Marcello Pelillo</div><div style='padding-top: 10px; width: 80ex'>Evaluating the adversarial robustness of deep networks to gradient-based
attacks is challenging. While most attacks consider $\ell_2$- and
$\ell_\infty$-norm constraints to craft input perturbations, only a few
investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular,
$\ell_0$-norm attacks remain the least studied due to the inherent complexity
of optimizing over a non-convex and non-differentiable constraint. However,
evaluating adversarial robustness under these attacks could reveal weaknesses
otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm
attacks. In this work, we propose a novel $\ell_0$-norm attack, called
$\sigma$-zero, which leverages an ad hoc differentiable approximation of the
$\ell_0$ norm to facilitate gradient-based optimization, and an adaptive
projection operator to dynamically adjust the trade-off between loss
minimization and perturbation sparsity. Extensive evaluations using MNIST,
CIFAR10, and ImageNet datasets, involving robust and non-robust models, show
that $\sigma$-zero finds minimum $\ell_0$-norm adversarial examples without
requiring any time-consuming hyperparameter tuning, and that it outperforms all
competing sparse attacks in terms of success rate, perturbation size, and
scalability.</div><div><a href='http://arxiv.org/abs/2402.01879v1'>2402.01879v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14031v1")'>Sparse and Transferable Universal Singular Vectors Attack</div>
<div id='2401.14031v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T09:21:29Z</div><div>Authors: Kseniia Kuvshinova, Olga Tsymboi, Ivan Oseledets</div><div style='padding-top: 10px; width: 80ex'>The research in the field of adversarial attacks and models' vulnerability is
one of the fundamental directions in modern machine learning. Recent studies
reveal the vulnerability phenomenon, and understanding the mechanisms behind
this is essential for improving neural network characteristics and
interpretability. In this paper, we propose a novel sparse universal white-box
adversarial attack. Our approach is based on truncated power iteration
providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian
matrices. Using the ImageNet benchmark validation subset, we analyze the
proposed method in various settings, achieving results comparable to dense
baselines with more than a 50% fooling rate while damaging only 5% of pixels
and utilizing 256 samples for perturbation fitting. We also show that our
algorithm admits higher attack magnitude without affecting the human ability to
solve the task. Furthermore, we investigate that the constructed perturbations
are highly transferable among different models without significantly decreasing
the fooling rate. Our findings demonstrate the vulnerability of
state-of-the-art models to sparse attacks and highlight the importance of
developing robust machine learning systems.</div><div><a href='http://arxiv.org/abs/2401.14031v1'>2401.14031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14772v1")'>Improving Robustness to Model Inversion Attacks via Sparse Coding
  Architectures</div>
<div id='2403.14772v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T18:26:23Z</div><div>Authors: Sayanton V. Dibbo, Adam Breuer, Juston Moore, Michael Teti</div><div style='padding-top: 10px; width: 80ex'>Recent model inversion attack algorithms permit adversaries to reconstruct a
neural network's private training data just by repeatedly querying the network
and inspecting its outputs. In this work, we develop a novel network
architecture that leverages sparse-coding layers to obtain superior robustness
to this class of attacks. Three decades of computer science research has
studied sparse coding in the context of image denoising, object recognition,
and adversarial misclassification settings, but to the best of our knowledge,
its connection to state-of-the-art privacy vulnerabilities remains unstudied.
However, sparse coding architectures suggest an advantageous means to defend
against model inversion attacks because they allow us to control the amount of
irrelevant private information encoded in a network's intermediate
representations in a manner that can be computed efficiently during training
and that is known to have little effect on classification accuracy.
Specifically, compared to networks trained with a variety of state-of-the-art
defenses, our sparse-coding architectures maintain comparable or higher
classification accuracy while degrading state-of-the-art training data
reconstructions by factors of 1.1 to 18.3 across a variety of reconstruction
quality metrics (PSNR, SSIM, FID). This performance advantage holds across 5
datasets ranging from CelebA faces to medical images and CIFAR-10, and across
various state-of-the-art SGD-based and GAN-based inversion attacks, including
Plug-&amp;-Play attacks. We provide a cluster-ready PyTorch codebase to promote
research and standardize defense evaluations.</div><div><a href='http://arxiv.org/abs/2403.14772v1'>2403.14772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14792v1")'>Deep Variational Privacy Funnel: General Modeling with Applications in
  Face Recognition</div>
<div id='2401.14792v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T11:32:53Z</div><div>Authors: Behrooz Razeghi, Parsa Rahimi, Sébastien Marcel</div><div style='padding-top: 10px; width: 80ex'>In this study, we harness the information-theoretic Privacy Funnel (PF) model
to develop a method for privacy-preserving representation learning using an
end-to-end training framework. We rigorously address the trade-off between
obfuscation and utility. Both are quantified through the logarithmic loss, a
measure also recognized as self-information loss. This exploration deepens the
interplay between information-theoretic privacy and representation learning,
offering substantive insights into data protection mechanisms for both
discriminative and generative models. Importantly, we apply our model to
state-of-the-art face recognition systems. The model demonstrates adaptability
across diverse inputs, from raw facial images to both derived or refined
embeddings, and is competent in tasks such as classification, reconstruction,
and generation.</div><div><a href='http://arxiv.org/abs/2401.14792v1'>2401.14792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09316v1")'>Only My Model On My Data: A Privacy Preserving Approach Protecting one
  Model and Deceiving Unauthorized Black-Box Models</div>
<div id='2402.09316v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T17:11:52Z</div><div>Authors: Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks are extensively applied to real-world tasks, such as
face recognition and medical image classification, where privacy and data
protection are critical. Image data, if not protected, can be exploited to
infer personal or contextual information. Existing privacy preservation
methods, like encryption, generate perturbed images that are unrecognizable to
even humans. Adversarial attack approaches prohibit automated inference even
for authorized stakeholders, limiting practical incentives for commercial and
widespread adaptation. This pioneering study tackles an unexplored practical
privacy preservation use case by generating human-perceivable images that
maintain accurate inference by an authorized model while evading other
unauthorized black-box models of similar or dissimilar objectives, and
addresses the previous research gaps. The datasets employed are ImageNet, for
image classification, Celeba-HQ dataset, for identity classification, and
AffectNet, for emotion classification. Our results show that the generated
images can successfully maintain the accuracy of a protected model and degrade
the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and
55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.</div><div><a href='http://arxiv.org/abs/2402.09316v1'>2402.09316v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10859v1")'>Ensembler: Combating model inversion attacks using model ensemble during
  collaborative inference</div>
<div id='2401.10859v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T18:03:21Z</div><div>Authors: Dancheng Liu, Jinjun Xiong</div><div style='padding-top: 10px; width: 80ex'>Deep learning models have exhibited remarkable performance across various
domains. Nevertheless, the burgeoning model sizes compel edge devices to
offload a significant portion of the inference process to the cloud. While this
practice offers numerous advantages, it also raises critical concerns regarding
user data privacy. In scenarios where the cloud server's trustworthiness is in
question, the need for a practical and adaptable method to safeguard data
privacy becomes imperative. In this paper, we introduce Ensembler, an
extensible framework designed to substantially increase the difficulty of
conducting model inversion attacks for adversarial parties. Ensembler leverages
model ensembling on the adversarial server, running in parallel with existing
approaches that introduce perturbations to sensitive data during colloborative
inference. Our experiments demonstrate that when combined with even basic
Gaussian noise, Ensembler can effectively shield images from reconstruction
attacks, achieving recognition levels that fall below human performance in some
strict settings, significantly outperforming baseline methods lacking the
Ensembler framework.</div><div><a href='http://arxiv.org/abs/2401.10859v1'>2401.10859v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02263v1")'>MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly
  Mixed Classifiers</div>
<div id='2402.02263v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T21:12:36Z</div><div>Authors: Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi</div><div style='padding-top: 10px; width: 80ex'>Adversarial robustness often comes at the cost of degraded accuracy, impeding
the real-life application of robust classification models. Training-based
solutions for better trade-offs are limited by incompatibilities with
already-trained high-performance large models, necessitating the exploration of
training-free ensemble approaches. Observing that robust models are more
confident in correct predictions than in incorrect ones on clean and
adversarial data alike, we speculate amplifying this "benign confidence
property" can reconcile accuracy and robustness in an ensemble setting. To
achieve so, we propose "MixedNUTS", a training-free method where the output
logits of a robust classifier and a standard non-robust classifier are
processed by nonlinear transformations with only three parameters, which are
optimized through an efficient algorithm. MixedNUTS then converts the
transformed logits into probabilities and mixes them as the overall output. On
CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom
strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and
near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points,
sacrificing merely 0.87 points in robust accuracy.</div><div><a href='http://arxiv.org/abs/2402.02263v1'>2402.02263v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10562v1")'>Counter-Samples: A Stateless Strategy to Neutralize Black Box
  Adversarial Attacks</div>
<div id='2403.10562v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T10:59:54Z</div><div>Authors: Roey Bokobza, Yisroel Mirsky</div><div style='padding-top: 10px; width: 80ex'>Our paper presents a novel defence against black box attacks, where attackers
use the victim model as an oracle to craft their adversarial examples. Unlike
traditional preprocessing defences that rely on sanitizing input samples, our
stateless strategy counters the attack process itself. For every query we
evaluate a counter-sample instead, where the counter-sample is the original
sample optimized against the attacker's objective. By countering every black
box query with a targeted white box optimization, our strategy effectively
introduces an asymmetry to the game to the defender's advantage. This defence
not only effectively misleads the attacker's search for an adversarial example,
it also preserves the model's accuracy on legitimate inputs and is generic to
multiple types of attacks.
  We demonstrate that our approach is remarkably effective against
state-of-the-art black box attacks and outperforms existing defences for both
the CIFAR-10 and ImageNet datasets. Additionally, we also show that the
proposed defence is robust against strong adversaries as well.</div><div><a href='http://arxiv.org/abs/2403.10562v1'>2403.10562v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10586v1")'>PuriDefense: Randomized Local Implicit Adversarial Purification for
  Defending Black-box Query-based Attacks</div>
<div id='2401.10586v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T09:54:23Z</div><div>Authors: Ping Guo, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang</div><div style='padding-top: 10px; width: 80ex'>Black-box query-based attacks constitute significant threats to Machine
Learning as a Service (MLaaS) systems since they can generate adversarial
examples without accessing the target model's architecture and parameters.
Traditional defense mechanisms, such as adversarial training, gradient masking,
and input transformations, either impose substantial computational costs or
compromise the test accuracy of non-adversarial inputs. To address these
challenges, we propose an efficient defense mechanism, PuriDefense, that
employs random patch-wise purifications with an ensemble of lightweight
purification models at a low level of inference cost. These models leverage the
local implicit function and rebuild the natural image manifold. Our theoretical
analysis suggests that this approach slows down the convergence of query-based
attacks by incorporating randomness into purifications. Extensive experiments
on CIFAR-10 and ImageNet validate the effectiveness of our proposed
purifier-based defense mechanism, demonstrating significant improvements in
robustness against query-based attacks.</div><div><a href='http://arxiv.org/abs/2401.10586v1'>2401.10586v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03124v1")'>Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks</div>
<div id='2402.03124v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:51:34Z</div><div>Authors: Yanbo Wang, Jian Liang, Ran He</div><div style='padding-top: 10px; width: 80ex'>Gradient inversion attacks aim to reconstruct local training data from
intermediate gradients exposed in the federated learning framework. Despite
successful attacks, all previous methods, starting from reconstructing a single
data point and then relaxing the single-image limit to batch level, are only
tested under hard label constraints. Even for single-image reconstruction, we
still lack an analysis-based algorithm to recover augmented soft labels. In
this work, we change the focus from enlarging batchsize to investigating the
hard label constraints, considering a more realistic circumstance where label
smoothing and mixup techniques are used in the training process. In particular,
we are the first to initiate a novel algorithm to simultaneously recover the
ground-truth augmented label and the input feature of the last fully-connected
layer from single-input gradients, and provide a necessary condition for any
analytical-based label recovery methods. Extensive experiments testify to the
label recovery accuracy, as well as the benefits to the following image
reconstruction. We believe soft labels in classification tasks are worth
further attention in gradient inversion attacks.</div><div><a href='http://arxiv.org/abs/2402.03124v1'>2402.03124v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07901v1")'>MIP: CLIP-based Image Reconstruction from PEFT Gradients</div>
<div id='2403.07901v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T02:19:01Z</div><div>Authors: Peiheng Zhou, Ming Hu, Xiaofei Xie, Yihao Huang, Kangjie Chen, Mingsong Chen</div><div style='padding-top: 10px; width: 80ex'>Contrastive Language-Image Pre-training (CLIP) model, as an effective
pre-trained multimodal neural network, has been widely used in distributed
machine learning tasks, especially Federated Learning (FL). Typically,
CLIP-based FL adopts Parameter-Efficient Fine-Tuning (PEFT) for model training,
which only fine-tunes adapter parameters or soft prompts rather than the full
parameters. Although PEFT is different from the traditional training mode, in
this paper, we theoretically analyze that the gradients of adapters or soft
prompts can still be used to perform image reconstruction attacks. Based on our
theoretical analysis, we propose Multm-In-Parvo (MIP), a proprietary
reconstruction attack method targeting CLIP-based distributed machine learning
architecture. Specifically, MIP can reconstruct CLIP training images according
to the gradients of soft prompts or an adapter. In addition, MIP includes a
label prediction strategy to accelerate convergence and an inverse gradient
estimation mechanism to avoid the vanishing gradient problem on the text
encoder. Experimental results show that MIP can effectively reconstruct
training images according to the gradients of soft prompts or adapters of CLIP
models.</div><div><a href='http://arxiv.org/abs/2403.07901v1'>2403.07901v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17888v1")'>ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection</div>
<div id='2402.17888v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:02:47Z</div><div>Authors: Bo Peng, Yadan Luo, Yonggang Zhang, Yixuan Li, Zhen Fang</div><div style='padding-top: 10px; width: 80ex'>Post-hoc out-of-distribution (OOD) detection has garnered intensive attention
in reliable machine learning. Many efforts have been dedicated to deriving
score functions based on logits, distances, or rigorous data distribution
assumptions to identify low-scoring OOD samples. Nevertheless, these estimate
scores may fail to accurately reflect the true data density or impose
impractical constraints. To provide a unified perspective on density-based
score design, we propose a novel theoretical framework grounded in Bregman
divergence, which extends distribution considerations to encompass an
exponential family of distributions. Leveraging the conjugation constraint
revealed in our theorem, we introduce a \textsc{ConjNorm} method, reframing
density function design as a search for the optimal norm coefficient $p$
against the given dataset. In light of the computational challenges of
normalization, we devise an unbiased and analytically tractable estimator of
the partition function using the Monte Carlo-based importance sampling
technique. Extensive experiments across OOD detection benchmarks empirically
demonstrate that our proposed \textsc{ConjNorm} has established a new
state-of-the-art in a variety of OOD detection setups, outperforming the
current best method by up to 13.25$\%$ and 28.19$\%$ (FPR95) on CIFAR-100 and
ImageNet-1K, respectively.</div><div><a href='http://arxiv.org/abs/2402.17888v1'>2402.17888v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00865v1")'>Towards Optimal Feature-Shaping Methods for Out-of-Distribution
  Detection</div>
<div id='2402.00865v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:59:22Z</div><div>Authors: Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould</div><div style='padding-top: 10px; width: 80ex'>Feature shaping refers to a family of methods that exhibit state-of-the-art
performance for out-of-distribution (OOD) detection. These approaches
manipulate the feature representation, typically from the penultimate layer of
a pre-trained deep learning model, so as to better differentiate between
in-distribution (ID) and OOD samples. However, existing feature-shaping methods
usually employ rules manually designed for specific model architectures and OOD
datasets, which consequently limit their generalization ability. To address
this gap, we first formulate an abstract optimization framework for studying
feature-shaping methods. We then propose a concrete reduction of the framework
with a simple piecewise constant shaping function and show that existing
feature-shaping methods approximate the optimal solution to the concrete
optimization problem. Further, assuming that OOD data is inaccessible, we
propose a formulation that yields a closed-form solution for the piecewise
constant shaping function, utilizing solely the ID data. Through extensive
experiments, we show that the feature-shaping function optimized by our method
improves the generalization ability of OOD detection across a large variety of
datasets and model architectures.</div><div><a href='http://arxiv.org/abs/2402.00865v1'>2402.00865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10403v1")'>Energy Correction Model in the Feature Space for Out-of-Distribution
  Detection</div>
<div id='2403.10403v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:37:04Z</div><div>Authors: Marc Lafon, Clément Rambour, Nicolas Thome</div><div style='padding-top: 10px; width: 80ex'>In this work, we study the out-of-distribution (OOD) detection problem
through the use of the feature space of a pre-trained deep classifier. We show
that learning the density of in-distribution (ID) features with an energy-based
models (EBM) leads to competitive detection results. However, we found that the
non-mixing of MCMC sampling during the EBM's training undermines its detection
performance. To overcome this an energy-based correction of a mixture of
class-conditional Gaussian distributions. We obtains favorable results when
compared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100
OOD detection benchmarks.</div><div><a href='http://arxiv.org/abs/2403.10403v1'>2403.10403v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01710v1")'>EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector</div>
<div id='2401.01710v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T12:25:18Z</div><div>Authors: Jiawei Zhang, Yufan Chen, Cheng Jin, Lei Zhu, Yuantao Gu</div><div style='padding-top: 10px; width: 80ex'>Out-of-distribution (OOD) detection plays a crucial role in ensuring the
security of neural networks. Existing works have leveraged the fact that
In-distribution (ID) samples form a subspace in the feature space, achieving
state-of-the-art (SOTA) performance. However, the comprehensive characteristics
of the ID subspace still leave under-explored. Recently, the discovery of
Neural Collapse ($\mathcal{NC}$) sheds light on novel properties of the ID
subspace. Leveraging insight from $\mathcal{NC}$, we observe that the Principal
Angle between the features and the ID feature subspace forms a superior
representation for measuring the likelihood of OOD. Building upon this
observation, we propose a novel $\mathcal{NC}$-inspired OOD scoring function,
named Entropy-enhanced Principal Angle (EPA), which integrates both the global
characteristic of the ID subspace and its inner property. We experimentally
compare EPA with various SOTA approaches, validating its superior performance
and robustness across different network architectures and OOD datasets.</div><div><a href='http://arxiv.org/abs/2401.01710v1'>2401.01710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02949v1")'>Kernel PCA for Out-of-Distribution Detection</div>
<div id='2402.02949v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:21:16Z</div><div>Authors: Kun Fang, Qinghua Tao, Kexin Lv, Mingzhen He, Xiaolin Huang, Jie Yang</div><div style='padding-top: 10px; width: 80ex'>Out-of-Distribution (OoD) detection is vital for the reliability of Deep
Neural Networks (DNNs). Existing works have shown the insufficiency of
Principal Component Analysis (PCA) straightforwardly applied on the features of
DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA
suggests that the network features residing in OoD and InD are not well
separated by simply proceeding in a linear subspace, which instead can be
resolved through proper nonlinear mappings. In this work, we leverage the
framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD
and InD features are allocated with significantly different patterns. We devise
two feature mappings that induce non-linear kernels in KPCA to advocate the
separability between InD and OoD data in the subspace spanned by the principal
components. Given any test sample, the reconstruction error in such subspace is
then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time
complexity in inference. Extensive empirical results on multiple OoD data sets
and network structures verify the superiority of our KPCA-based detector in
efficiency and efficacy with state-of-the-art OoD detection performances.</div><div><a href='http://arxiv.org/abs/2402.02949v1'>2402.02949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00777v1")'>Combating Financial Crimes with Unsupervised Learning Techniques:
  Clustering and Dimensionality Reduction for Anti-Money Laundering</div>
<div id='2403.00777v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T17:31:29Z</div><div>Authors: Ahmed N. Bakry, Almohammady S. Alsharkawy, Mohamed S. Farag, Kamal R. Raslan</div><div style='padding-top: 10px; width: 80ex'>Anti-Money Laundering (AML) is a crucial task in ensuring the integrity of
financial systems. One keychallenge in AML is identifying high-risk groups
based on their behavior. Unsupervised learning, particularly clustering, is a
promising solution for this task. However, the use of hundreds of features
todescribe behavior results in a highdimensional dataset that negatively
impacts clustering performance.In this paper, we investigate the effectiveness
of combining clustering method agglomerative hierarchicalclustering with four
dimensionality reduction techniques -Independent Component Analysis (ICA),
andKernel Principal Component Analysis (KPCA), Singular Value Decomposition
(SVD), Locality Preserving Projections (LPP)- to overcome the issue of
high-dimensionality in AML data and improve clusteringresults. This study aims
to provide insights into the most effective way of reducing the dimensionality
ofAML data and enhance the accuracy of clustering-based AML systems. The
experimental results demonstrate that KPCA outperforms other dimension
reduction techniques when combined with agglomerativehierarchical clustering.
This superiority is observed in the majority of situations, as confirmed by
threedistinct validation indices.</div><div><a href='http://arxiv.org/abs/2403.00777v1'>2403.00777v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10803v1")'>Enhancing Out-of-Distribution Detection with Multitesting-based
  Layer-wise Feature Fusion</div>
<div id='2403.10803v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T04:35:04Z</div><div>Authors: Jiawei Li, Sitong Li, Shanshan Wang, Yicheng Zeng, Falong Tan, Chuanlong Xie</div><div style='padding-top: 10px; width: 80ex'>Deploying machine learning in open environments presents the challenge of
encountering diverse test inputs that differ significantly from the training
data. These out-of-distribution samples may exhibit shifts in local or global
features compared to the training distribution. The machine learning (ML)
community has responded with a number of methods aimed at distinguishing
anomalous inputs from original training data. However, the majority of previous
studies have primarily focused on the output layer or penultimate layer of
pre-trained deep neural networks. In this paper, we propose a novel framework,
Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to
identify distributional shifts in test samples at different levels of features
through rigorous multiple testing procedure. Our approach distinguishes itself
from existing methods as it does not require modifying the structure or
fine-tuning of the pre-trained classifier. Through extensive experiments, we
demonstrate that our proposed framework can seamlessly integrate with any
existing distance-based inspection method while efficiently utilizing feature
extractors of varying depths. Our scheme effectively enhances the performance
of out-of-distribution detection when compared to baseline methods. In
particular, MLOD-Fisher achieves superior performance in general. When trained
using KNN on CIFAR10, MLOD-Fisher significantly lowers the false positive rate
(FPR) from 24.09% to 7.47% on average compared to merely utilizing the features
of the last layer.</div><div><a href='http://arxiv.org/abs/2403.10803v1'>2403.10803v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03412v1")'>Advancing Out-of-Distribution Detection through Data Purification and
  Dynamic Activation Function Design</div>
<div id='2403.03412v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T02:39:22Z</div><div>Authors: Yingrui Ji, Yao Zhu, Zhigang Li, Jiansheng Chen, Yunlong Kong, Jingbo Chen</div><div style='padding-top: 10px; width: 80ex'>In the dynamic realms of machine learning and deep learning, the robustness
and reliability of models are paramount, especially in critical real-world
applications. A fundamental challenge in this sphere is managing
Out-of-Distribution (OOD) samples, significantly increasing the risks of model
misclassification and uncertainty. Our work addresses this challenge by
enhancing the detection and management of OOD samples in neural networks. We
introduce OOD-R (Out-of-Distribution-Rectified), a meticulously curated
collection of open-source datasets with enhanced noise reduction properties.
In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate
evaluation of detection algorithms. Recognizing this, OOD-R incorporates noise
filtering technologies to refine the datasets, ensuring a more accurate and
reliable evaluation of OOD detection algorithms. This approach not only
improves the overall quality of data but also aids in better distinguishing
between OOD and ID samples, resulting in up to a 2.5\% improvement in model
accuracy and a minimum 3.2\% reduction in false positives. Furthermore, we
present ActFun, an innovative method that fine-tunes the model's response to
diverse inputs, thereby improving the stability of feature extraction and
minimizing specificity issues. ActFun addresses the common problem of model
overconfidence in OOD detection by strategically reducing the influence of
hidden units, which enhances the model's capability to estimate OOD uncertainty
more accurately. Implementing ActFun in the OOD-R dataset has led to
significant performance enhancements, including an 18.42\% increase in AUROC of
the GradNorm method and a 16.93\% decrease in FPR95 of the Energy method.
Overall, our research not only advances the methodologies in OOD detection but
also emphasizes the importance of dataset integrity for accurate algorithm
evaluation.</div><div><a href='http://arxiv.org/abs/2403.03412v1'>2403.03412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05043v2")'>CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation
  in Classification Tasks</div>
<div id='2401.05043v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T10:04:49Z</div><div>Authors: Kaizheng Wang, Keivan Shariatmadar, Shireen Kudukkil Manchingal, Fabio Cuzzolin, David Moens, Hans Hallez</div><div style='padding-top: 10px; width: 80ex'>Uncertainty estimation is increasingly attractive for improving the
reliability of neural networks. In this work, we present novel credal-set
interval neural networks (CreINNs) designed for classification tasks. CreINNs
preserve the traditional interval neural network structure, capturing weight
uncertainty through deterministic intervals, while forecasting credal sets
using the mathematical framework of probability intervals. Experimental
validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN)
showcase that CreINNs outperform epistemic uncertainty estimation when compared
to variational Bayesian neural networks (BNNs) and deep ensembles (DEs).
Furthermore, CreINNs exhibit a notable reduction in computational complexity
compared to variational BNNs and demonstrate smaller model sizes than DEs.</div><div><a href='http://arxiv.org/abs/2401.05043v2'>2401.05043v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01484v1")'>Uncertainty Regularized Evidential Regression</div>
<div id='2401.01484v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T01:18:18Z</div><div>Authors: Kai Ye, Tiejin Chen, Hua Wei, Liang Zhan</div><div style='padding-top: 10px; width: 80ex'>The Evidential Regression Network (ERN) represents a novel approach that
integrates deep learning with Dempster-Shafer's theory to predict a target and
quantify the associated uncertainty. Guided by the underlying theory, specific
activation functions must be employed to enforce non-negative values, which is
a constraint that compromises model performance by limiting its ability to
learn from all samples. This paper provides a theoretical analysis of this
limitation and introduces an improvement to overcome it. Initially, we define
the region where the models can't effectively learn from the samples. Following
this, we thoroughly analyze the ERN and investigate this constraint. Leveraging
the insights from our analysis, we address the limitation by introducing a
novel regularization term that empowers the ERN to learn from the whole
training set. Our extensive experiments substantiate our theoretical findings
and demonstrate the effectiveness of the proposed solution.</div><div><a href='http://arxiv.org/abs/2401.01484v1'>2401.01484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09056v2")'>Is Epistemic Uncertainty Faithfully Represented by Evidential Deep
  Learning Methods?</div>
<div id='2402.09056v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:07:05Z</div><div>Authors: Mira Jürgens, Nis Meinert, Viktor Bengs, Eyke Hüllermeier, Willem Waegeman</div><div style='padding-top: 10px; width: 80ex'>Trustworthy ML systems should not only return accurate predictions, but also
a reliable representation of their uncertainty. Bayesian methods are commonly
used to quantify both aleatoric and epistemic uncertainty, but alternative
approaches, such as evidential deep learning methods, have become popular in
recent years. The latter group of methods in essence extends empirical risk
minimization (ERM) for predicting second-order probability distributions over
outcomes, from which measures of epistemic (and aleatoric) uncertainty can be
extracted. This paper presents novel theoretical insights of evidential deep
learning, highlighting the difficulties in optimizing second-order loss
functions and interpreting the resulting epistemic uncertainty measures. With a
systematic setup that covers a wide range of approaches for classification,
regression and counts, it provides novel insights into issues of
identifiability and convergence in second-order loss minimization, and the
relative (rather than absolute) nature of epistemic uncertainty measures.</div><div><a href='http://arxiv.org/abs/2402.09056v2'>2402.09056v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14676v1")'>Unified Uncertainty Estimation for Cognitive Diagnosis Models</div>
<div id='2403.14676v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T13:48:20Z</div><div>Authors: Fei Wang, Qi Liu, Enhong Chen, Chuanren Liu, Zhenya Huang, Jinze Wu, Shijin Wang</div><div style='padding-top: 10px; width: 80ex'>Cognitive diagnosis models have been widely used in different areas,
especially intelligent education, to measure users' proficiency levels on
knowledge concepts, based on which users can get personalized instructions. As
the measurement is not always reliable due to the weak links of the models and
data, the uncertainty of measurement also offers important information for
decisions. However, the research on the uncertainty estimation lags behind that
on advanced model structures for cognitive diagnosis. Existing approaches have
limited efficiency and leave an academic blank for sophisticated models which
have interaction function parameters (e.g., deep learning-based models). To
address these problems, we propose a unified uncertainty estimation approach
for a wide range of cognitive diagnosis models. Specifically, based on the idea
of estimating the posterior distributions of cognitive diagnosis model
parameters, we first provide a unified objective function for mini-batch based
optimization that can be more efficiently applied to a wide range of models and
large datasets. Then, we modify the reparameterization approach in order to
adapt to parameters defined on different domains. Furthermore, we decompose the
uncertainty of diagnostic parameters into data aspect and model aspect, which
better explains the source of uncertainty. Extensive experiments demonstrate
that our method is effective and can provide useful insights into the
uncertainty of cognitive diagnosis.</div><div><a href='http://arxiv.org/abs/2403.14676v1'>2403.14676v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12790v1")'>MORPH: Towards Automated Concept Drift Adaptation for Malware Detection</div>
<div id='2401.12790v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:25:43Z</div><div>Authors: Md Tanvirul Alam, Romy Fieblinger, Ashim Mahara, Nidhi Rastogi</div><div style='padding-top: 10px; width: 80ex'>Concept drift is a significant challenge for malware detection, as the
performance of trained machine learning models degrades over time, rendering
them impractical. While prior research in malware concept drift adaptation has
primarily focused on active learning, which involves selecting representative
samples to update the model, self-training has emerged as a promising approach
to mitigate concept drift. Self-training involves retraining the model using
pseudo labels to adapt to shifting data distributions. In this research, we
propose MORPH -- an effective pseudo-label-based concept drift adaptation
method specifically designed for neural networks. Through extensive
experimental analysis of Android and Windows malware datasets, we demonstrate
the efficacy of our approach in mitigating the impact of concept drift. Our
method offers the advantage of reducing annotation efforts when combined with
active learning. Furthermore, our method significantly improves over existing
works in automated concept drift adaptation for malware detection.</div><div><a href='http://arxiv.org/abs/2401.12790v1'>2401.12790v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10062v1")'>Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection</div>
<div id='2402.10062v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T07:31:06Z</div><div>Authors: Chao Chen, Zhihang Fu, Kai Liu, Ze Chen, Mingyuan Tao, Jieping Ye</div><div style='padding-top: 10px; width: 80ex'>For a machine learning model deployed in real world scenarios, the ability of
detecting out-of-distribution (OOD) samples is indispensable and challenging.
Most existing OOD detection methods focused on exploring advanced training
skills or training-free tricks to prevent the model from yielding overconfident
confidence score for unknown samples. The training-based methods require
expensive training cost and rely on OOD samples which are not always available,
while most training-free methods can not efficiently utilize the prior
information from the training data. In this work, we propose an
\textbf{O}ptimal \textbf{P}arameter and \textbf{N}euron \textbf{P}runing
(\textbf{OPNP}) approach, which aims to identify and remove those parameters
and neurons that lead to over-fitting. The main method is divided into two
steps. In the first step, we evaluate the sensitivity of the model parameters
and neurons by averaging gradients over all training samples. In the second
step, the parameters and neurons with exceptionally large or close to zero
sensitivities are removed for prediction. Our proposal is training-free,
compatible with other post-hoc methods, and exploring the information from all
training data. Extensive experiments are performed on multiple OOD detection
tasks and model architectures, showing that our proposed OPNP consistently
outperforms the existing methods by a large margin.</div><div><a href='http://arxiv.org/abs/2402.10062v1'>2402.10062v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12129v1")'>Out-of-Distribution Detection &amp; Applications With Ablated Learned
  Temperature Energy</div>
<div id='2401.12129v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T17:11:01Z</div><div>Authors: Will LeVine, Benjamin Pikus, Jacob Phillips, Berk Norman, Fernando Amat Gil, Sean Hendryx</div><div style='padding-top: 10px; width: 80ex'>As deep neural networks become adopted in high-stakes domains, it is crucial
to be able to identify when inference inputs are Out-of-Distribution (OOD) so
that users can be alerted of likely drops in performance and calibration
despite high confidence. Among many others, existing methods use the following
two scores to do so without training on any apriori OOD examples: a learned
temperature and an energy score. In this paper we introduce Ablated Learned
Temperature Energy (or "AbeT" for short), a method which combines these prior
methods in novel ways with effective modifications. Due to these contributions,
AbeT lowers the False Positive Rate at $95\%$ True Positive Rate (FPR@95) by
$35.39\%$ in classification (averaged across all ID and OOD datasets measured)
compared to state of the art without training networks in multiple stages or
requiring hyperparameters or test-time backward passes. We additionally provide
empirical insights as to how our model learns to distinguish between
In-Distribution (ID) and OOD samples while only being explicitly trained on ID
samples via exposure to misclassified ID examples at training time. Lastly, we
show the efficacy of our method in identifying predicted bounding boxes and
pixels corresponding to OOD objects in object detection and semantic
segmentation, respectively - with an AUROC increase of $5.15\%$ in object
detection and both a decrease in FPR@95 of $41.48\%$ and an increase in AUPRC
of $34.20\%$ on average in semantic segmentation compared to previous state of
the art.</div><div><a href='http://arxiv.org/abs/2401.12129v1'>2401.12129v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14002v1")'>Uncertainty Driven Active Learning for Image Segmentation in Underwater
  Inspection</div>
<div id='2403.14002v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T22:03:40Z</div><div>Authors: Luiza Ribeiro Marnet, Yury Brodskiy, Stella Grasshof, Andrzej Wasowski</div><div style='padding-top: 10px; width: 80ex'>Active learning aims to select the minimum amount of data to train a model
that performs similarly to a model trained with the entire dataset. We study
the potential of active learning for image segmentation in underwater
infrastructure inspection tasks, where large amounts of data are typically
collected. The pipeline inspection images are usually semantically repetitive
but with great variations in quality. We use mutual information as the
acquisition function, calculated using Monte Carlo dropout. To assess the
effectiveness of the framework, DenseNet and HyperSeg are trained with the
CamVid dataset using active learning. In addition, HyperSeg is trained with a
pipeline inspection dataset of over 50,000 images. For the pipeline dataset,
HyperSeg with active learning achieved 67.5% meanIoU using 12.5% of the data,
and 61.4% with the same amount of randomly selected images. This shows that
using active learning for segmentation models in underwater inspection tasks
can lower the cost significantly.</div><div><a href='http://arxiv.org/abs/2403.14002v1'>2403.14002v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08689v2")'>NODI: Out-Of-Distribution Detection with Noise from Diffusion</div>
<div id='2401.08689v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T08:30:13Z</div><div>Authors: Jingqiu Zhou, Aojun Zhou, Hongsheng Li</div><div style='padding-top: 10px; width: 80ex'>Out-of-distribution (OOD) detection is a crucial part of deploying machine
learning models safely. It has been extensively studied with a plethora of
methods developed in the literature. This problem is tackled with an OOD score
computation, however, previous methods compute the OOD scores with limited
usage of the in-distribution dataset. For instance, the OOD scores are computed
with information from a small portion of the in-distribution data. Furthermore,
these methods encode images with a neural image encoder. The robustness of
these methods is rarely checked with respect to image encoders of different
training methods and architectures. In this work, we introduce the diffusion
process into the OOD task. The diffusion model integrates information on the
whole training set into the predicted noise vectors. What's more, we deduce a
closed-form solution for the noise vector (stable point). Then the noise vector
is converted into our OOD score, we test both the deep model predicted noise
vector and the closed-form noise vector on the OOD benchmarks \cite{openood}.
Our method outperforms previous OOD methods across all types of image encoders
(Table. \ref{main}). A $3.5\%$ performance gain is achieved with the MAE-based
image encoder. Moreover, we studied the robustness of OOD methods by applying
different types of image encoders. Some OOD methods failed to generalize well
when switching image encoders from ResNet to Vision Transformers, our method
performs exhibits good robustness with all the image encoders.</div><div><a href='http://arxiv.org/abs/2401.08689v2'>2401.08689v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12225v1")'>Multimodal Data Curation via Object Detection and Filter Ensembles</div>
<div id='2401.12225v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T08:40:06Z</div><div>Authors: Tzu-Heng Huang, Changho Shin, Sui Jiet Tay, Dyah Adila, Frederic Sala</div><div style='padding-top: 10px; width: 80ex'>We propose an approach for curating multimodal data that we used for our
entry in the 2023 DataComp competition filtering track. Our technique combines
object detection and weak supervision-based ensembling. In the first of two
steps in our approach, we employ an out-of-the-box zero-shot object detection
model to extract granular information and produce a variety of filter designs.
In the second step, we employ weak supervision to ensemble filtering rules.
This approach results in a 4% performance improvement when compared to the
best-performing baseline, producing the top-ranking position in the small scale
track at the time of writing. Furthermore, in the medium scale track, we
achieve a noteworthy 4.2% improvement over the baseline by simply ensembling
existing baselines with weak supervision.</div><div><a href='http://arxiv.org/abs/2401.12225v1'>2401.12225v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02316v2")'>Your Diffusion Model is Secretly a Certifiably Robust Classifier</div>
<div id='2402.02316v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T02:09:18Z</div><div>Authors: Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang, Hang Su, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>Diffusion models are recently employed as generative classifiers for robust
classification. However, a comprehensive theoretical understanding of the
robustness of diffusion classifiers is still lacking, leading us to question
whether they will be vulnerable to future stronger attacks. In this study, we
propose a new family of diffusion classifiers, named Noised Diffusion
Classifiers~(NDCs), that possess state-of-the-art certified robustness.
Specifically, we generalize the diffusion classifiers to classify
Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these
distributions, approximating the likelihood using the ELBO, and calculating
classification probabilities via Bayes' theorem. We integrate these generalized
diffusion classifiers with randomized smoothing to construct smoothed
classifiers possessing non-constant Lipschitzness. Experimental results
demonstrate the superior certified robustness of our proposed NDCs. Notably, we
are the first to achieve 80\%+ and 70\%+ certified robustness on CIFAR-10 under
adversarial perturbations with $\ell_2$ norm less than 0.25 and 0.5,
respectively, using a single off-the-shelf diffusion model without any
additional data.</div><div><a href='http://arxiv.org/abs/2402.02316v2'>2402.02316v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11981v1")'>Diffusion Denoising as a Certified Defense against Clean-label Poisoning</div>
<div id='2403.11981v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:17:07Z</div><div>Authors: Sanghyun Hong, Nicholas Carlini, Alexey Kurakin</div><div style='padding-top: 10px; width: 80ex'>We present a certified defense to clean-label poisoning attacks. These
attacks work by injecting a small number of poisoning samples (e.g., 1%) that
contain $p$-norm bounded adversarial perturbations into the training data to
induce a targeted misclassification of a test-time input. Inspired by the
adversarial robustness achieved by $denoised$ $smoothing$, we show how an
off-the-shelf diffusion model can sanitize the tampered training data. We
extensively test our defense against seven clean-label poisoning attacks and
reduce their attack success to 0-16% with only a negligible drop in the test
time accuracy. We compare our defense with existing countermeasures against
clean-label poisoning, showing that the defense reduces the attack success the
most and offers the best model utility. Our results highlight the need for
future work on developing stronger clean-label attacks and using our certified
yet practical defense as a strong baseline to evaluate these attacks.</div><div><a href='http://arxiv.org/abs/2403.11981v1'>2403.11981v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13719v1")'>Inference Attacks Against Face Recognition Model without Classification
  Layers</div>
<div id='2401.13719v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T09:51:03Z</div><div>Authors: Yuanqing Huang, Huilong Chen, Yinggui Wang, Lei Wang</div><div style='padding-top: 10px; width: 80ex'>Face recognition (FR) has been applied to nearly every aspect of daily life,
but it is always accompanied by the underlying risk of leaking private
information. At present, almost all attack models against FR rely heavily on
the presence of a classification layer. However, in practice, the FR model can
obtain complex features of the input via the model backbone, and then compare
it with the target for inference, which does not explicitly involve the outputs
of the classification layer adopting logit or other losses. In this work, we
advocate a novel inference attack composed of two stages for practical FR
models without a classification layer. The first stage is the membership
inference attack. Specifically, We analyze the distances between the
intermediate features and batch normalization (BN) parameters. The results
indicate that this distance is a critical metric for membership inference. We
thus design a simple but effective attack model that can determine whether a
face image is from the training dataset or not. The second stage is the model
inversion attack, where sensitive private data is reconstructed using a
pre-trained generative adversarial network (GAN) guided by the attack model in
the first stage. To the best of our knowledge, the proposed attack model is the
very first in the literature developed for FR models without a classification
layer. We illustrate the application of the proposed attack model in the
establishment of privacy-preserving FR techniques.</div><div><a href='http://arxiv.org/abs/2401.13719v1'>2401.13719v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08903v2")'>PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks
  on Face Recognition Systems</div>
<div id='2401.08903v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T01:10:17Z</div><div>Authors: Fengfan Zhou, Heifei Ling, Bangjie Yin, Hui Zheng</div><div style='padding-top: 10px; width: 80ex'>Adversarial Attacks on Face Recognition (FR) encompass two types:
impersonation attacks and evasion attacks. We observe that achieving a
successful impersonation attack on FR does not necessarily ensure a successful
dodging attack on FR in the black-box setting. Introducing a novel attack
method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance
the performance of dodging attacks whilst avoiding the degradation of
impersonation attacks. Our method employs adversarial example pruning, enabling
a portion of adversarial perturbations to be set to zero, while tending to
maintain the attack performance. By utilizing adversarial example pruning, we
can prune the pre-trained adversarial examples and selectively free up certain
adversarial perturbations. Thereafter, we embed adversarial perturbations in
the pruned area, which enhances the dodging performance of the adversarial face
examples. The effectiveness of our proposed attack method is demonstrated
through our experimental results, showcasing its superior performance.</div><div><a href='http://arxiv.org/abs/2401.08903v2'>2401.08903v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08998v1")'>Attack and Reset for Unlearning: Exploiting Adversarial Noise toward
  Machine Unlearning through Parameter Re-initialization</div>
<div id='2401.08998v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T06:22:47Z</div><div>Authors: Yoonhwa Jung, Ikhyun Cho, Shun-Hsiang Hsu, Julia Hockenmaier</div><div style='padding-top: 10px; width: 80ex'>With growing concerns surrounding privacy and regulatory compliance, the
concept of machine unlearning has gained prominence, aiming to selectively
forget or erase specific learned information from a trained model. In response
to this critical need, we introduce a novel approach called Attack-and-Reset
for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial
noise to generate a parameter mask, effectively resetting certain parameters
and rendering them unlearnable. ARU outperforms current state-of-the-art
results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC.
In particular, we present the steps involved in attacking and masking that
strategically filter and re-initialize network parameters biased towards the
forget set. Our work represents a significant advancement in rendering data
unexploitable to deep learning models through parameter re-initialization,
achieved by harnessing adversarial noise to craft a mask.</div><div><a href='http://arxiv.org/abs/2401.08998v1'>2401.08998v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01114v1")'>Double-Dip: Thwarting Label-Only Membership Inference Attacks with
  Transfer Learning and Randomization</div>
<div id='2402.01114v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T03:14:37Z</div><div>Authors: Arezoo Rajabi, Reeya Pimple, Aiswarya Janardhanan, Surudhi Asokraj, Bhaskar Ramasubramanian, Radha Poovendran</div><div style='padding-top: 10px; width: 80ex'>Transfer learning (TL) has been demonstrated to improve DNN model performance
when faced with a scarcity of training samples. However, the suitability of TL
as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is
unexplored. A class of privacy attacks called membership inference attacks
(MIAs) aim to determine whether a given sample belongs to the training dataset
(member) or not (nonmember). We introduce Double-Dip, a systematic empirical
study investigating the use of TL (Stage-1) combined with randomization
(Stage-2) to thwart MIAs on overfitted DNNs without degrading classification
accuracy. Our study examines the roles of shared feature space and parameter
values between source and target models, number of frozen layers, and
complexity of pretrained models. We evaluate Double-Dip on three (Target,
Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii)
(CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a)
VGG-19, (b) ResNet-18, (c) Swin-T, and (d) FaceNet. Our experiments demonstrate
that Stage-1 reduces adversary success while also significantly increasing
classification accuracy of nonmembers against an adversary with either
white-box or black-box DNN model access, attempting to carry out SOTA
label-only MIAs. After Stage-2, success of an adversary carrying out a
label-only MIA is further reduced to near 50%, bringing it closer to a random
guess and showing the effectiveness of Double-Dip. Stage-2 of Double-Dip also
achieves lower ASR and higher classification accuracy than regularization and
differential privacy-based methods.</div><div><a href='http://arxiv.org/abs/2402.01114v1'>2402.01114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10558v1")'>Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition
  Against Model Inversion Attack</div>
<div id='2403.10558v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T02:17:57Z</div><div>Authors: Yuanqing Huang, Yinggui Wang, Jianshu Li, Le Yang, Kai Song, Lei Wang</div><div style='padding-top: 10px; width: 80ex'>The utilization of personal sensitive data in training face recognition (FR)
models poses significant privacy concerns, as adversaries can employ model
inversion attacks (MIA) to infer the original training data. Existing defense
methods, such as data augmentation and differential privacy, have been employed
to mitigate this issue. However, these methods often fail to strike an optimal
balance between privacy and accuracy. To address this limitation, this paper
introduces an adaptive hybrid masking algorithm against MIA. Specifically, face
images are masked in the frequency domain using an adaptive MixUp strategy.
Unlike the traditional MixUp algorithm, which is predominantly used for data
augmentation, our modified approach incorporates frequency domain mixing.
Previous studies have shown that increasing the number of images mixed in MixUp
can enhance privacy preservation but at the expense of reduced face recognition
accuracy. To overcome this trade-off, we develop an enhanced adaptive MixUp
strategy based on reinforcement learning, which enables us to mix a larger
number of images while maintaining satisfactory recognition accuracy. To
optimize privacy protection, we propose maximizing the reward function (i.e.,
the loss function of the FR system) during the training of the strategy
network. While the loss function of the FR network is minimized in the phase of
training the FR network. The strategy network and the face recognition network
can be viewed as antagonistic entities in the training process, ultimately
reaching a more balanced trade-off. Experimental results demonstrate that our
proposed hybrid masking scheme outperforms existing defense algorithms in terms
of privacy preservation and recognition accuracy against MIA.</div><div><a href='http://arxiv.org/abs/2403.10558v1'>2403.10558v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07205v1")'>Crafter: Facial Feature Crafting against Inversion-based Identity Theft
  on Deep Models</div>
<div id='2401.07205v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T05:06:42Z</div><div>Authors: Shiming Wang, Zhe Ji, Liyao Xiang, Hao Zhang, Xinbing Wang, Chenghu Zhou, Bo Li</div><div style='padding-top: 10px; width: 80ex'>With the increased capabilities at the edge (e.g., mobile device) and more
stringent privacy requirement, it becomes a recent trend for deep
learning-enabled applications to pre-process sensitive raw data at the edge and
transmit the features to the backend cloud for further processing. A typical
application is to run machine learning (ML) services on facial images collected
from different individuals. To prevent identity theft, conventional methods
commonly rely on an adversarial game-based approach to shed the identity
information from the feature. However, such methods can not defend against
adaptive attacks, in which an attacker takes a countermove against a known
defence strategy. We propose Crafter, a feature crafting mechanism deployed at
the edge, to protect the identity information from adaptive model inversion
attacks while ensuring the ML tasks are properly carried out in the cloud. The
key defence strategy is to mislead the attacker to a non-private prior from
which the attacker gains little about the private identity. In this case, the
crafted features act like poison training samples for attackers with adaptive
model updates. Experimental results indicate that Crafter successfully defends
both basic and possible adaptive attacks, which can not be achieved by
state-of-the-art adversarial game-based methods.</div><div><a href='http://arxiv.org/abs/2401.07205v1'>2401.07205v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02095v1")'>Seeing is not always believing: The Space of Harmless Perturbations</div>
<div id='2402.02095v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T09:22:07Z</div><div>Authors: Lu Chen, Shaofeng Li, Benhao Huang, Fan Yang, Zheng Li, Jie Li, Yuan Luo</div><div style='padding-top: 10px; width: 80ex'>In the context of deep neural networks, we expose the existence of a harmless
perturbation space, where perturbations leave the network output entirely
unaltered. Perturbations within this harmless perturbation space, regardless of
their magnitude when applied to images, exhibit no impact on the network's
outputs of the original images. Specifically, given any linear layer within the
network, where the input dimension $n$ exceeds the output dimension $m$, we
demonstrate the existence of a continuous harmless perturbation subspace with a
dimension of $(n-m)$. Inspired by this, we solve for a family of general
perturbations that consistently influence the network output, irrespective of
their magnitudes. With these theoretical findings, we explore the application
of harmless perturbations for privacy-preserving data usage. Our work reveals
the difference between DNNs and human perception that the significant
perturbations captured by humans may not affect the recognition of DNNs. As a
result, we utilize this gap to design a type of harmless perturbation that is
meaningless for humans while maintaining its recognizable features for DNNs.</div><div><a href='http://arxiv.org/abs/2402.02095v1'>2402.02095v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12710v1")'>Selective, Interpretable, and Motion Consistent Privacy Attribute
  Obfuscation for Action Recognition</div>
<div id='2403.12710v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T13:17:26Z</div><div>Authors: Filip Ilic, He Zhao, Thomas Pock, Richard P. Wildes</div><div style='padding-top: 10px; width: 80ex'>Concerns for the privacy of individuals captured in public imagery have led
to privacy-preserving action recognition. Existing approaches often suffer from
issues arising through obfuscation being applied globally and a lack of
interpretability. Global obfuscation hides privacy sensitive regions, but also
contextual regions important for action recognition. Lack of interpretability
erodes trust in these new technologies. We highlight the limitations of current
paradigms and propose a solution: Human selected privacy templates that yield
interpretability by design, an obfuscation scheme that selectively hides
attributes and also induces temporal consistency, which is important in action
recognition. Our approach is architecture agnostic and directly modifies input
imagery, while existing approaches generally require architecture training. Our
approach offers more flexibility, as no retraining is required, and outperforms
alternatives on three widely used datasets.</div><div><a href='http://arxiv.org/abs/2403.12710v1'>2403.12710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.06661v1")'>Authentication and integrity of smartphone videos through multimedia
  container structure analysis</div>
<div id='2402.06661v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:34:24Z</div><div>Authors: Carlos Quinto Huamán, Ana Lucila Sandoval Orozco, Luis Javier García Villalba</div><div style='padding-top: 10px; width: 80ex'>Nowadays, mobile devices have become the natural substitute for the digital
camera, as they capture everyday situations easily and quickly, encouraging
users to express themselves through images and videos. These videos can be
shared across different platforms exposing them to any kind of intentional
manipulation by criminals who are aware of the weaknesses of forensic
techniques to accuse an innocent person or exonerate a guilty person in a
judicial process. Commonly, manufacturers do not comply 100% with the
specifications of the standards for the creation of videos. Also, videos shared
on social networks, and instant messaging applications go through filtering and
compression processes to reduce their size, facilitate their transfer, and
optimize storage on their platforms. The omission of specifications and results
of transformations carried out by the platforms embed a features pattern in the
multimedia container of the videos. These patterns make it possible to
distinguish the brand of the device that generated the video, social network,
and instant messaging application that was used for the transfer. Research in
recent years has focused on the analysis of AVI containers and tiny video
datasets. This work presents a novel technique to detect possible attacks
against MP4, MOV, and 3GP format videos that affect their integrity and
authenticity. The method is based on the analysis of the structure of video
containers generated by mobile devices and their behavior when shared through
social networks, instant messaging applications, or manipulated by editing
programs. The objectives of the proposal are to verify the integrity of videos,
identify the source of acquisition and distinguish between original and
manipulated videos.</div><div><a href='http://arxiv.org/abs/2402.06661v1'>2402.06661v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10573v1")'>Medical Unlearnable Examples: Securing Medical Data from Unauthorized
  Traning via Sparsity-Aware Local Masking</div>
<div id='2403.10573v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T02:35:36Z</div><div>Authors: Weixiang Sun, Yixin Liu, Zhiling Yan, Kaidi Xu, Lichao Sun</div><div style='padding-top: 10px; width: 80ex'>With the rapid growth of artificial intelligence (AI) in healthcare, there
has been a significant increase in the generation and storage of sensitive
medical data. This abundance of data, in turn, has propelled the advancement of
medical AI technologies. However, concerns about unauthorized data
exploitation, such as training commercial AI models, often deter researchers
from making their invaluable datasets publicly available. In response to the
need to protect this hard-to-collect data while still encouraging medical
institutions to share it, one promising solution is to introduce imperceptible
noise into the data. This method aims to safeguard the data against
unauthorized training by inducing degradation in model generalization. Although
existing methods have shown commendable data protection capabilities in general
domains, they tend to fall short when applied to biomedical data, mainly due to
their failure to account for the sparse nature of medical images. To address
this problem, we propose the Sparsity-Aware Local Masking (SALM) method, a
novel approach that selectively perturbs significant pixel regions rather than
the entire image as previous strategies have done. This simple-yet-effective
approach significantly reduces the perturbation search space by concentrating
on local regions, thereby improving both the efficiency and effectiveness of
data protection for biomedical datasets characterized by sparse features.
Besides, we have demonstrated that SALM maintains the essential characteristics
of the data, ensuring its clinical utility remains uncompromised. Our extensive
experiments across various datasets and model architectures demonstrate that
SALM effectively prevents unauthorized training of deep-learning models and
outperforms previous state-of-the-art data protection methods.</div><div><a href='http://arxiv.org/abs/2403.10573v1'>2403.10573v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04929v1")'>Learning-Based Difficulty Calibration for Enhanced Membership Inference
  Attacks</div>
<div id='2401.04929v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T04:58:17Z</div><div>Authors: Haonan Shi, Tu Ouyang, An Wang</div><div style='padding-top: 10px; width: 80ex'>Machine learning models, in particular deep neural networks, are currently an
integral part of various applications, from healthcare to finance. However,
using sensitive data to train these models raises concerns about privacy and
security. One method that has emerged to verify if the trained models are
privacy-preserving is Membership Inference Attacks (MIA), which allows
adversaries to determine whether a specific data point was part of a model's
training dataset. While a series of MIAs have been proposed in the literature,
only a few can achieve high True Positive Rates (TPR) in the low False Positive
Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA
to be practically useful in real-world settings. In this paper, we present a
novel approach to MIA that is aimed at significantly improving TPR at low FPRs.
Our method, named learning-based difficulty calibration for MIA(LDC-MIA),
characterizes data records by their hardness levels using a neural network
classifier to determine membership. The experiment results show that LDC-MIA
can improve TPR at low FPR by up to 4x compared to the other difficulty
calibration based MIAs. It also has the highest Area Under ROC curve (AUC)
across all datasets. Our method's cost is comparable with most of the existing
MIAs, but is orders of magnitude more efficient than one of the
state-of-the-art methods, LiRA, while achieving similar performance.</div><div><a href='http://arxiv.org/abs/2401.04929v1'>2401.04929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04781v1")'>Selective Encryption using Segmentation Mask with Chaotic Henon Map for
  Multidimensional Medical Images</div>
<div id='2403.04781v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T11:20:24Z</div><div>Authors: S Arut Prakash, Aditya Ganesh Kumar, Prabhu Shankar K. C., Lithicka Anandavel, Aditya Lakshmi Narayanan</div><div style='padding-top: 10px; width: 80ex'>A user-centric design and resource optimization should be at the center of
any technology or innovation. The user-centric perspective gives the developer
the opportunity to develop with task-based optimization. The user in the
medical image field is a medical professional who analyzes the medical images
and gives their diagnosis results to the patient. This scheme, having the
medical professional user's perspective, innovates in the area of Medical Image
storage and security. The architecture is designed with three main segments,
namely: Segmentation, Storage, and Retrieval. This architecture was designed
owing to the fact that the number of retrieval operations done by medical
professionals was toweringly higher when compared to the storage operations
done for some handful number of times for a particular medical image. This
gives room for our innovation to segment out the medically indispensable part
of the medical image, encrypt it, and store it. By encrypting the vital parts
of the image using a strong encryption algorithm like the chaotic Henon map, we
are able to keep the security intact. Now retrieving the medical image demands
only the computationally less stressing decryption of the segmented region of
interest. The decryption of the segmented region of interest results in the
full recovery of the medical image which can be viewed on demand by the medical
professionals for various diagnosis purposes. In this scheme, we were able to
achieve a retrieval speed improvement of around 47% when compared to a full
image encryption of brain medical CT images.</div><div><a href='http://arxiv.org/abs/2403.04781v1'>2403.04781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01082v1")'>Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on
  Learning With Errors</div>
<div id='2402.01082v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T00:48:27Z</div><div>Authors: Samuel Stevens, Emily Wenger, Cathy Li, Niklas Nolte, Eshika Saxena, François Charton, Kristin Lauter</div><div style='padding-top: 10px; width: 80ex'>Learning with Errors (LWE) is a hard math problem underlying recently
standardized post-quantum cryptography (PQC) systems for key exchange and
digital signatures. Prior work proposed new machine learning (ML)-based attacks
on LWE problems with small, sparse secrets, but these attacks require millions
of LWE samples to train on and take days to recover secrets. We propose three
key methods -- better preprocessing, angular embeddings and model pre-training
-- to improve these attacks, speeding up preprocessing by $25\times$ and
improving model sample efficiency by $10\times$. We demonstrate for the first
time that pre-training improves and reduces the cost of ML attacks on LWE. Our
architecture improvements enable scaling to larger-dimension LWE problems: this
work is the first instance of ML attacks recovering sparse binary secrets in
dimension $n=1024$, the smallest dimension used in practice for homomorphic
encryption applications of LWE where sparse binary secrets are proposed.</div><div><a href='http://arxiv.org/abs/2402.01082v1'>2402.01082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11224v1")'>Neural Networks with (Low-Precision) Polynomial Approximations: New
  Insights and Techniques for Accuracy Improvement</div>
<div id='2402.11224v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T08:54:25Z</div><div>Authors: Chi Zhang, Man Ho Au, Siu Ming Yiu</div><div style='padding-top: 10px; width: 80ex'>Replacing non-polynomial functions (e.g., non-linear activation functions
such as ReLU) in a neural network with their polynomial approximations is a
standard practice in privacy-preserving machine learning. The resulting neural
network, called polynomial approximation of neural network (PANN) in this
paper, is compatible with advanced cryptosystems to enable privacy-preserving
model inference. Using ``highly precise'' approximation, state-of-the-art PANN
offers similar inference accuracy as the underlying backbone model. However,
little is known about the effect of approximation, and existing literature
often determined the required approximation precision empirically. In this
paper, we initiate the investigation of PANN as a standalone object.
Specifically, our contribution is two-fold. Firstly, we provide an explanation
on the effect of approximate error in PANN. In particular, we discovered that
(1) PANN is susceptible to some type of perturbations; and (2) weight
regularisation significantly reduces PANN's accuracy. We support our
explanation with experiments. Secondly, based on the insights from our
investigations, we propose solutions to increase inference accuracy for PANN.
Experiments showed that combination of our solutions is very effective: at the
same precision, our PANN is 10% to 50% more accurate than state-of-the-arts;
and at the same accuracy, our PANN only requires a precision of $2^{-9}$ while
state-of-the-art solution requires a precision of $2^{-12}$ using the ResNet-20
model on CIFAR-10 dataset.</div><div><a href='http://arxiv.org/abs/2402.11224v1'>2402.11224v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19254v1")'>Machine learning for modular multiplication</div>
<div id='2402.19254v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:26:03Z</div><div>Authors: Kristin Lauter, Cathy Yuanchen Li, Krystal Maughan, Rachel Newton, Megha Srivastava</div><div style='padding-top: 10px; width: 80ex'>Motivated by cryptographic applications, we investigate two machine learning
approaches to modular multiplication: namely circular regression and a
sequence-to-sequence transformer model. The limited success of both methods
demonstrated in our results gives evidence for the hardness of tasks involving
modular multiplication upon which cryptosystems are based.</div><div><a href='http://arxiv.org/abs/2402.19254v1'>2402.19254v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05432v1")'>TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep
  Neural Networks</div>
<div id='2401.05432v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T03:08:28Z</div><div>Authors: Khondoker Murad Hossain, Tim Oates</div><div style='padding-top: 10px; width: 80ex'>As deep neural networks and the datasets used to train them get larger, the
default approach to integrating them into research and commercial projects is
to download a pre-trained model and fine tune it. But these models can have
uncertain provenance, opening up the possibility that they embed hidden
malicious behavior such as trojans or backdoors, where small changes to an
input (triggers) can cause the model to produce incorrect outputs (e.g., to
misclassify). This paper introduces a novel approach to backdoor detection that
uses two tensor decomposition methods applied to network activations. This has
a number of advantages relative to existing detection methods, including the
ability to analyze multiple models at the same time, working across a wide
variety of network architectures, making no assumptions about the nature of
triggers used to alter network behavior, and being computationally efficient.
We provide a detailed description of the detection pipeline along with results
on models trained on the MNIST digit dataset, CIFAR-10 dataset, and two
difficult datasets from NIST's TrojAI competition. These results show that our
method detects backdoored networks more accurately and efficiently than current
state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2401.05432v1'>2401.05432v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15295v1")'>Multi-Trigger Backdoor Attacks: More Triggers, More Threats</div>
<div id='2401.15295v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T04:49:37Z</div><div>Authors: Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang</div><div style='padding-top: 10px; width: 80ex'>Backdoor attacks have emerged as a primary threat to (pre-)training and
deployment of deep neural networks (DNNs). While backdoor attacks have been
extensively studied in a body of works, most of them were focused on
single-trigger attacks that poison a dataset using a single type of trigger.
Arguably, real-world backdoor attacks can be much more complex, e.g., the
existence of multiple adversaries for the same dataset if it is of high value.
In this work, we investigate the practical threat of backdoor attacks under the
setting of \textbf{multi-trigger attacks} where multiple adversaries leverage
different types of triggers to poison the same dataset. By proposing and
investigating three types of multi-trigger attacks, including parallel,
sequential, and hybrid attacks, we provide a set of important understandings of
the coexisting, overwriting, and cross-activating effects between different
triggers on the same dataset. Moreover, we show that single-trigger attacks
tend to cause overly optimistic views of the security of current defense
techniques, as all examined defense methods struggle to defend against
multi-trigger attacks. Finally, we create a multi-trigger backdoor poisoning
dataset to help future evaluation of backdoor attacks and defenses. Although
our work is purely empirical, we hope it can help steer backdoor research
toward more realistic settings.</div><div><a href='http://arxiv.org/abs/2401.15295v1'>2401.15295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03215v1")'>End-to-End Anti-Backdoor Learning on Images and Time Series</div>
<div id='2401.03215v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T13:34:07Z</div><div>Authors: Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, Yige Li, James Bailey</div><div style='padding-top: 10px; width: 80ex'>Backdoor attacks present a substantial security concern for deep learning
models, especially those utilized in applications critical to safety and
security. These attacks manipulate model behavior by embedding a hidden trigger
during the training phase, allowing unauthorized control over the model's
output during inference time. Although numerous defenses exist for image
classification models, there is a conspicuous absence of defenses tailored for
time series data, as well as an end-to-end solution capable of training clean
models on poisoned data. To address this gap, this paper builds upon
Anti-Backdoor Learning (ABL) and introduces an innovative method, End-to-End
Anti-Backdoor Learning (E2ABL), for robust training against backdoor attacks.
Unlike the original ABL, which employs a two-stage training procedure, E2ABL
accomplishes end-to-end training through an additional classification head
linked to the shallow layers of a Deep Neural Network (DNN). This secondary
head actively identifies potential backdoor triggers, allowing the model to
dynamically cleanse these samples and their corresponding labels during
training. Our experiments reveal that E2ABL significantly improves on existing
defenses and is effective against a broad range of backdoor attacks in both
image and time series domains.</div><div><a href='http://arxiv.org/abs/2401.03215v1'>2401.03215v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02034v1")'>Universal Post-Training Reverse-Engineering Defense Against Backdoors in
  Deep Neural Networks</div>
<div id='2402.02034v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:15:19Z</div><div>Authors: Xi Li, Hang Wang, David J. Miller, George Kesidis</div><div style='padding-top: 10px; width: 80ex'>A variety of defenses have been proposed against backdoors attacks on deep
neural network (DNN) classifiers. Universal methods seek to reliably detect
and/or mitigate backdoors irrespective of the incorporation mechanism used by
the attacker, while reverse-engineering methods often explicitly assume one. In
this paper, we describe a new detector that: relies on internal feature map of
the defended DNN to detect and reverse-engineer the backdoor and identify its
target class; can operate post-training (without access to the training
dataset); is highly effective for various incorporation mechanisms (i.e., is
universal); and which has low computational overhead and so is scalable. Our
detection approach is evaluated for different attacks on a benchmark CIFAR-10
image classifier.</div><div><a href='http://arxiv.org/abs/2402.02034v1'>2402.02034v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13018v1")'>Invisible Backdoor Attack Through Singular Value Decomposition</div>
<div id='2403.13018v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:25:12Z</div><div>Authors: Wenmin Chen, Xiaowei Xu</div><div style='padding-top: 10px; width: 80ex'>With the widespread application of deep learning across various domains,
concerns about its security have grown significantly. Among these, backdoor
attacks pose a serious security threat to deep neural networks (DNNs). In
recent years, backdoor attacks on neural networks have become increasingly
sophisticated, aiming to compromise the security and trustworthiness of models
by implanting hidden, unauthorized functionalities or triggers, leading to
misleading predictions or behaviors. To make triggers less perceptible and
imperceptible, various invisible backdoor attacks have been proposed. However,
most of them only consider invisibility in the spatial domain, making it easy
for recent defense methods to detect the generated toxic images.To address
these challenges, this paper proposes an invisible backdoor attack called DEBA.
DEBA leverages the mathematical properties of Singular Value Decomposition
(SVD) to embed imperceptible backdoors into models during the training phase,
thereby causing them to exhibit predefined malicious behavior under specific
trigger conditions. Specifically, we first perform SVD on images, and then
replace the minor features of trigger images with those of clean images, using
them as triggers to ensure the effectiveness of the attack. As minor features
are scattered throughout the entire image, the major features of clean images
are preserved, making poisoned images visually indistinguishable from clean
ones. Extensive experimental evaluations demonstrate that DEBA is highly
effective, maintaining high perceptual quality and a high attack success rate
for poisoned images. Furthermore, we assess the performance of DEBA under
existing defense measures, showing that it is robust and capable of
significantly evading and resisting the effects of these defense measures.</div><div><a href='http://arxiv.org/abs/2403.13018v1'>2403.13018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03590v1")'>DeepEclipse: How to Break White-Box DNN-Watermarking Schemes</div>
<div id='2403.03590v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T10:24:47Z</div><div>Authors: Alessandro Pegoraro, Carlotta Segna, Kavita Kumari, Ahmad-Reza Sadeghi</div><div style='padding-top: 10px; width: 80ex'>Deep Learning (DL) models have become crucial in digital transformation, thus
raising concerns about their intellectual property rights. Different
watermarking techniques have been developed to protect Deep Neural Networks
(DNNs) from IP infringement, creating a competitive field for DNN watermarking
and removal methods. The predominant watermarking schemes use white-box
techniques, which involve modifying weights by adding a unique signature to
specific DNN layers. On the other hand, existing attacks on white-box
watermarking usually require knowledge of the specific deployed watermarking
scheme or access to the underlying data for further training and fine-tuning.
We propose DeepEclipse, a novel and unified framework designed to remove
white-box watermarks. We present obfuscation techniques that significantly
differ from the existing white-box watermarking removal schemes. DeepEclipse
can evade watermark detection without prior knowledge of the underlying
watermarking scheme, additional data, or training and fine-tuning. Our
evaluation reveals that DeepEclipse excels in breaking multiple white-box
watermarking schemes, reducing watermark detection to random guessing while
maintaining a similar model accuracy as the original one. Our framework
showcases a promising solution to address the ongoing DNN watermark protection
and removal challenges.</div><div><a href='http://arxiv.org/abs/2403.03590v1'>2403.03590v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15239v1")'>MEA-Defender: A Robust Watermark against Model Extraction Attack</div>
<div id='2401.15239v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T23:12:53Z</div><div>Authors: Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang</div><div style='padding-top: 10px; width: 80ex'>Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been
trained using deep learning algorithms. To protect the Intellectual Property
(IP) of the original owners over such DNN models, backdoor-based watermarks
have been extensively studied. However, most of such watermarks fail upon model
extraction attack, which utilizes input samples to query the target model and
obtains the corresponding outputs, thus training a substitute model using such
input-output pairs. In this paper, we propose a novel watermark to protect IP
of DNN models against model extraction, named MEA-Defender. In particular, we
obtain the watermark by combining two samples from two source classes in the
input domain and design a watermark loss function that makes the output domain
of the watermark within that of the main task samples. Since both the input
domain and the output domain of our watermark are indispensable parts of those
of the main task samples, the watermark will be extracted into the stolen model
along with the main task during model extraction. We conduct extensive
experiments on four model extraction attacks, using five datasets and six
models trained based on supervised learning and self-supervised learning
algorithms. The experimental results demonstrate that MEA-Defender is highly
robust against different model extraction attacks, and various watermark
removal/detection approaches.</div><div><a href='http://arxiv.org/abs/2401.15239v1'>2401.15239v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10663v1")'>Not Just Change the Labels, Learn the Features: Watermarking Deep Neural
  Networks with Multi-View Data</div>
<div id='2403.10663v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T20:12:41Z</div><div>Authors: Yuxuan Li, Sarthak Kumar Maharana, Yunhui Guo</div><div style='padding-top: 10px; width: 80ex'>With the increasing prevalence of Machine Learning as a Service (MLaaS)
platforms, there is a growing focus on deep neural network (DNN) watermarking
techniques. These methods are used to facilitate the verification of ownership
for a target DNN model to protect intellectual property. One of the most widely
employed watermarking techniques involves embedding a trigger set into the
source model. Unfortunately, existing methodologies based on trigger sets are
still susceptible to functionality-stealing attacks, potentially enabling
adversaries to steal the functionality of the source model without a reliable
means of verifying ownership. In this paper, we first introduce a novel
perspective on trigger set-based watermarking methods from a feature learning
perspective. Specifically, we demonstrate that by selecting data exhibiting
multiple features, also referred to as $\textit{multi-view data}$, it becomes
feasible to effectively defend functionality stealing attacks. Based on this
perspective, we introduce a novel watermarking technique based on Multi-view
dATa, called MAT, for efficiently embedding watermarks within DNNs. This
approach involves constructing a trigger set with multi-view data and
incorporating a simple feature-based regularization method for training the
source model. We validate our method across various benchmarks and demonstrate
its efficacy in defending against model extraction attacks, surpassing relevant
baselines by a significant margin.</div><div><a href='http://arxiv.org/abs/2403.10663v1'>2403.10663v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14883v1")'>Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning</div>
<div id='2402.14883v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T04:55:14Z</div><div>Authors: Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li</div><div style='padding-top: 10px; width: 80ex'>To support various applications, business owners often seek the customized
models that are obtained by fine-tuning a pre-trained LLM through the API
provided by LLM owners or cloud servers. However, this process carries a
substantial risk of model misuse, potentially resulting in severe economic
consequences for business owners. Thus, safeguarding the copyright of these
customized models during LLM fine-tuning has become an urgent practical
requirement, but there are limited existing solutions to provide such
protection. To tackle this pressing issue, we propose a novel watermarking
approach named "Double-I watermark". Specifically, based on the instruct-tuning
data, two types of backdoor data paradigms are introduced with trigger in the
instruction and the input, respectively. By leveraging LLM's learning
capability to incorporate customized backdoor samples into the dataset, the
proposed approach effectively injects specific watermarking information into
the customized model during fine-tuning, which makes it easy to inject and
verify watermarks in commercial scenarios. We evaluate the proposed "Double-I
watermark" under various fine-tuning methods, demonstrating its harmlessness,
robustness, uniqueness, imperceptibility, and validity through both theoretical
analysis and experimental verification.</div><div><a href='http://arxiv.org/abs/2402.14883v1'>2402.14883v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02870v1")'>Precise Extraction of Deep Learning Models via Side-Channel Attacks on
  Edge/Endpoint Devices</div>
<div id='2403.02870v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:26:22Z</div><div>Authors: Younghan Lee, Sohee Jun, Yungi Cho, Woorim Han, Hyungon Moon, Yunheung Paek</div><div style='padding-top: 10px; width: 80ex'>With growing popularity, deep learning (DL) models are becoming larger-scale,
and only the companies with vast training datasets and immense computing power
can manage their business serving such large models. Most of those DL models
are proprietary to the companies who thus strive to keep their private models
safe from the model extraction attack (MEA), whose aim is to steal the model by
training surrogate models. Nowadays, companies are inclined to offload the
models from central servers to edge/endpoint devices. As revealed in the latest
studies, adversaries exploit this opportunity as new attack vectors to launch
side-channel attack (SCA) on the device running victim model and obtain various
pieces of the model information, such as the model architecture (MA) and image
dimension (ID). Our work provides a comprehensive understanding of such a
relationship for the first time and would benefit future MEA studies in both
offensive and defensive sides in that they may learn which pieces of
information exposed by SCA are more important than the others. Our analysis
additionally reveals that by grasping the victim model information from SCA,
MEA can get highly effective and successful even without any prior knowledge of
the model. Finally, to evince the practicality of our analysis results, we
empirically apply SCA, and subsequently, carry out MEA under realistic threat
assumptions. The results show up to 5.8 times better performance than when the
adversary has no model information about the victim model.</div><div><a href='http://arxiv.org/abs/2403.02870v1'>2403.02870v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03846v1")'>On the Effectiveness of Distillation in Mitigating Backdoors in
  Pre-trained Encoder</div>
<div id='2403.03846v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:42:10Z</div><div>Authors: Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng, Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu, Zhenyu Chen</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study a defense against poisoned encoders in SSL called
distillation, which is a defense used in supervised learning originally.
Distillation aims to distill knowledge from a given model (a.k.a the teacher
net) and transfer it to another (a.k.a the student net). Now, we use it to
distill benign knowledge from poisoned pre-trained encoders and transfer it to
a new encoder, resulting in a clean pre-trained encoder. In particular, we
conduct an empirical study on the effectiveness and performance of distillation
against poisoned encoders. Using two state-of-the-art backdoor attacks against
pre-trained image encoders and four commonly used image classification
datasets, our experimental results show that distillation can reduce attack
success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy.
Moreover, we investigate the impact of three core components of distillation on
performance: teacher net, student net, and distillation loss. By comparing 4
different teacher nets, 3 student nets, and 6 distillation losses, we find that
fine-tuned teacher nets, warm-up-training-based student nets, and
attention-based distillation loss perform best, respectively.</div><div><a href='http://arxiv.org/abs/2403.03846v1'>2403.03846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14977v1")'>Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models</div>
<div id='2402.14977v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T21:31:43Z</div><div>Authors: Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong</div><div style='padding-top: 10px; width: 80ex'>Foundation model has become the backbone of the AI ecosystem. In particular,
a foundation model can be used as a general-purpose feature extractor to build
various downstream classifiers. However, foundation models are vulnerable to
backdoor attacks and a backdoored foundation model is a single-point-of-failure
of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor
vulnerabilities simultaneously. In this work, we propose Mudjacking, the first
method to patch foundation models to remove backdoors. Specifically, given a
misclassified trigger-embedded input detected after a backdoored foundation
model is deployed, Mudjacking adjusts the parameters of the foundation model to
remove the backdoor. We formulate patching a foundation model as an
optimization problem and propose a gradient descent based method to solve it.
We evaluate Mudjacking on both vision and language foundation models, eleven
benchmark datasets, five existing backdoor attacks, and thirteen adaptive
backdoor attacks. Our results show that Mudjacking can remove backdoor from a
foundation model while maintaining its utility.</div><div><a href='http://arxiv.org/abs/2402.14977v1'>2402.14977v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10717v1")'>Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized
  Scaled Prediction Consistency</div>
<div id='2403.10717v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T22:35:07Z</div><div>Authors: Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu</div><div style='padding-top: 10px; width: 80ex'>Modern machine learning (ML) systems demand substantial training data, often
resorting to external sources. Nevertheless, this practice renders them
vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies
have primarily focused on the identification of backdoored models or poisoned
data characteristics, typically operating under the assumption of access to
clean data. In this work, we delve into a relatively underexplored challenge:
the automatic identification of backdoor data within a poisoned dataset, all
under realistic conditions, i.e., without the need for additional clean data or
without manually defining a threshold for backdoor detection. We draw an
inspiration from the scaled prediction consistency (SPC) technique, which
exploits the prediction invariance of poisoned data to an input scaling factor.
Based on this, we pose the backdoor data identification problem as a
hierarchical data splitting optimization problem, leveraging a novel SPC-based
loss function as the primary optimization objective. Our innovation unfolds in
several key aspects. First, we revisit the vanilla SPC method, unveiling its
limitations in addressing the proposed backdoor identification problem.
Subsequently, we develop a bi-level optimization-based approach to precisely
identify backdoor data by minimizing the advanced SPC loss. Finally, we
demonstrate the efficacy of our proposal against a spectrum of backdoor
attacks, encompassing basic label-corrupted attacks as well as more
sophisticated clean-label attacks, evaluated across various benchmark datasets.
Experiment results show that our approach often surpasses the performance of
current baselines in identifying backdoor data points, resulting in about
4%-36% improvement in average AUROC. Codes are available at
https://github.com/OPTML-Group/BackdoorMSPC.</div><div><a href='http://arxiv.org/abs/2403.10717v1'>2403.10717v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01537v1")'>The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of
  Triggers</div>
<div id='2401.01537v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T04:31:59Z</div><div>Authors: Orson Mengara</div><div style='padding-top: 10px; width: 80ex'>The area of Machine Learning as a Service (MLaaS) is experiencing increased
implementation due to recent advancements in the AI (Artificial Intelligence)
industry. However, this spike has prompted concerns regarding AI defense
mechanisms, specifically regarding potential covert attacks from third-party
providers that cannot be entirely trusted. Recent research has uncovered that
auditory backdoors may use certain modifications as their initiating mechanism.
DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor
attacks that use cleverly designed tweaks to ensure that corrupted samples are
indistinguishable from clean. By utilizing fluctuating signal sampling rates
and masking speaker identities through dynamic sound triggers (such as the
clapping of hands), it is possible to deceive speech recognition systems (ASR).
Our empirical testing demonstrates that DynamicTrigger is both potent and
stealthy, achieving impressive success rates during covert attacks while
maintaining exceptional accuracy with non-poisoned datasets.</div><div><a href='http://arxiv.org/abs/2401.01537v1'>2401.01537v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05967v2")'>The last Dance : Robust backdoor attack via diffusion models and
  bayesian approach</div>
<div id='2402.05967v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:00:07Z</div><div>Authors: Orson Mengara</div><div style='padding-top: 10px; width: 80ex'>Diffusion models are state-of-the-art deep learning generative models that
are trained on the principle of learning forward and backward diffusion
processes via the progressive addition of noise and denoising. In this paper,
we aim to fool audio-based DNN models, such as those from the Hugging Face
framework, primarily those that focus on audio, in particular transformer-based
artificial intelligence models, which are powerful machine learning models that
save time and achieve results faster and more efficiently. We demonstrate the
feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers
derived from Hugging Face, a popular framework in the world of artificial
intelligence research. The backdoor attack developed in this paper is based on
poisoning model training data uniquely by incorporating backdoor diffusion
sampling and a Bayesian approach to the distribution of poisoned data.</div><div><a href='http://arxiv.org/abs/2402.05967v2'>2402.05967v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00695v1")'>Approximating Optimal Morphing Attacks using Template Inversion</div>
<div id='2402.00695v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:51:46Z</div><div>Authors: Laurent Colbois, Hatef Otroshi Shahreza, Sébastien Marcel</div><div style='padding-top: 10px; width: 80ex'>Recent works have demonstrated the feasibility of inverting face recognition
systems, enabling to recover convincing face images using only their
embeddings. We leverage such template inversion models to develop a novel type
ofdeep morphing attack based on inverting a theoretical optimal morph
embedding, which is obtained as an average of the face embeddings of source
images. We experiment with two variants of this approach: the first one
exploits a fully self-contained embedding-to-image inversion model, while the
second leverages the synthesis network of a pretrained StyleGAN network for
increased morph realism. We generate morphing attacks from several source
datasets and study the effectiveness of those attacks against several face
recognition networks. We showcase that our method can compete with and
regularly beat the previous state of the art for deep-learning based morph
generation in terms of effectiveness, both in white-box and black-box attack
scenarios, and is additionally much faster to run. We hope this might
facilitate the development of large scale deep morph datasets for training
detection models.</div><div><a href='http://arxiv.org/abs/2402.00695v1'>2402.00695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12055v1")'>NEUROSEC: FPGA-Based Neuromorphic Audio Security</div>
<div id='2401.12055v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T15:47:05Z</div><div>Authors: Murat Isik, Hiruna Vishwamith, Yusuf Sur, Kayode Inadagbo, I. Can Dikmen</div><div style='padding-top: 10px; width: 80ex'>Neuromorphic systems, inspired by the complexity and functionality of the
human brain, have gained interest in academic and industrial attention due to
their unparalleled potential across a wide range of applications. While their
capabilities herald innovation, it is imperative to underscore that these
computational paradigms, analogous to their traditional counterparts, are not
impervious to security threats. Although the exploration of neuromorphic
methodologies for image and video processing has been rigorously pursued, the
realm of neuromorphic audio processing remains in its early stages. Our results
highlight the robustness and precision of our FPGA-based neuromorphic system.
Specifically, our system showcases a commendable balance between desired signal
and background noise, efficient spike rate encoding, and unparalleled
resilience against adversarial attacks such as FGSM and PGD. A standout feature
of our framework is its detection rate of 94%, which, when compared to other
methodologies, underscores its greater capability in identifying and mitigating
threats within 5.39 dB, a commendable SNR ratio. Furthermore, neuromorphic
computing and hardware security serve many sensor domains in mission-critical
and privacy-preserving applications.</div><div><a href='http://arxiv.org/abs/2401.12055v1'>2401.12055v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11563v1")'>Advancing Neuromorphic Computing: Mixed-Signal Design Techniques
  Leveraging Brain Code Units and Fundamental Code Units</div>
<div id='2403.11563v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T08:33:56Z</div><div>Authors: Murat Isik, Sols Miziev, Wiktoria Pawlak, Newton Howard</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a groundbreaking digital neuromorphic architecture that
innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU)
using mixedsignal design methodologies. Leveraging open-source datasets and the
latest advances in materials science, our research focuses on enhancing the
computational efficiency, accuracy, and adaptability of neuromorphic systems.
The core of our approach lies in harmonizing the precision and scalability of
digital systems with the robustness and energy efficiency of analog processing.
Through experimentation, we demonstrate the effectiveness of our system across
various metrics. The BCU achieved an accuracy of 88.0% and a power efficiency
of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power
efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly
improved latency and throughput, achieving a latency as low as 0.75 ms and
throughput up to 213 TOP/s. These results firmly establish the potential of our
architecture in neuromorphic computing, providing a solid foundation for future
developments in this domain. Our study underscores the feasibility of
mixedsignal neuromorphic systems and their promise in advancing the field,
particularly in applications requiring high efficiency and adaptability</div><div><a href='http://arxiv.org/abs/2403.11563v1'>2403.11563v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04274v1")'>FPGA Deployment of LFADS for Real-time Neuroscience Experiments</div>
<div id='2402.04274v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T07:52:20Z</div><div>Authors: Xiaohan Liu, ChiJui Chen, YanLun Huang, LingChi Yang, Elham E Khoda, Yihui Chen, Scott Hauck, Shih-Chieh Hsu, Bo-Cheng Lai</div><div style='padding-top: 10px; width: 80ex'>Large-scale recordings of neural activity are providing new opportunities to
study neural population dynamics. A powerful method for analyzing such
high-dimensional measurements is to deploy an algorithm to learn the
low-dimensional latent dynamics. LFADS (Latent Factor Analysis via Dynamical
Systems) is a deep learning method for inferring latent dynamics from
high-dimensional neural spiking data recorded simultaneously in single trials.
This method has shown a remarkable performance in modeling complex brain
signals with an average inference latency in milliseconds. As our capacity of
simultaneously recording many neurons is increasing exponentially, it is
becoming crucial to build capacity for deploying low-latency inference of the
computing algorithms. To improve the real-time processing ability of LFADS, we
introduce an efficient implementation of the LFADS models onto Field
Programmable Gate Arrays (FPGA). Our implementation shows an inference latency
of 41.97 $\mu$s for processing the data in a single trial on a Xilinx U55C.</div><div><a href='http://arxiv.org/abs/2402.04274v1'>2402.04274v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.19355v1")'>Unraveling Adversarial Examples against Speaker Identification --
  Techniques for Attack Detection and Victim Model Classification</div>
<div id='2402.19355v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T17:06:52Z</div><div>Authors: Sonal Joshi, Thomas Thebaud, Jesús Villalba, Najim Dehak</div><div style='padding-top: 10px; width: 80ex'>Adversarial examples have proven to threaten speaker identification systems,
and several countermeasures against them have been proposed. In this paper, we
propose a method to detect the presence of adversarial examples, i.e., a binary
classifier distinguishing between benign and adversarial examples. We build
upon and extend previous work on attack type classification by exploring new
architectures. Additionally, we introduce a method for identifying the victim
model on which the adversarial attack is carried out. To achieve this, we
generate a new dataset containing multiple attacks performed against various
victim models. We achieve an AUC of 0.982 for attack detection, with no more
than a 0.03 drop in performance for unknown attacks. Our attack classification
accuracy (excluding benign) reaches 86.48% across eight attack types using our
LightResNet34 architecture, while our victim model classification accuracy
reaches 72.28% across four victim models.</div><div><a href='http://arxiv.org/abs/2402.19355v1'>2402.19355v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02028v1")'>Unlearnable Examples For Time Series</div>
<div id='2402.02028v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T04:48:47Z</div><div>Authors: Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey</div><div style='padding-top: 10px; width: 80ex'>Unlearnable examples (UEs) refer to training samples modified to be
unlearnable to Deep Neural Networks (DNNs). These examples are usually
generated by adding error-minimizing noises that can fool a DNN model into
believing that there is nothing (no error) to learn from the data. The concept
of UE has been proposed as a countermeasure against unauthorized data
exploitation on personal data. While UE has been extensively studied on images,
it is unclear how to craft effective UEs for time series data. In this work, we
introduce the first UE generation method to protect time series data from
unauthorized training by deep learning models. To this end, we propose a new
form of error-minimizing noise that can be \emph{selectively} applied to
specific segments of time series, rendering them unlearnable to DNN models
while remaining imperceptible to human observers. Through extensive experiments
on a wide range of time series datasets, we demonstrate that the proposed UE
generation method is effective in both classification and generation tasks. It
can protect time series data against unauthorized exploitation, while
preserving their utility for legitimate usage, thereby contributing to the
development of secure and trustworthy machine learning systems.</div><div><a href='http://arxiv.org/abs/2402.02028v1'>2402.02028v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15883v1")'>TransTroj: Transferable Backdoor Attacks to Pre-trained Models via
  Embedding Indistinguishability</div>
<div id='2401.15883v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T04:35:48Z</div><div>Authors: Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei Zhang</div><div style='padding-top: 10px; width: 80ex'>Pre-trained models (PTMs) are extensively utilized in various downstream
tasks. Adopting untrusted PTMs may suffer from backdoor attacks, where the
adversary can compromise the downstream models by injecting backdoors into the
PTM. However, existing backdoor attacks to PTMs can only achieve partially
task-agnostic and the embedded backdoors are easily erased during the
fine-tuning process. In this paper, we propose a novel transferable backdoor
attack, TransTroj, to simultaneously meet functionality-preserving, durable,
and task-agnostic. In particular, we first formalize transferable backdoor
attacks as the indistinguishability problem between poisoned and clean samples
in the embedding space. We decompose the embedding indistinguishability into
pre- and post-indistinguishability, representing the similarity of the poisoned
and reference embeddings before and after the attack. Then, we propose a
two-stage optimization that separately optimizes triggers and victim PTMs to
achieve embedding indistinguishability. We evaluate TransTroj on four PTMs and
six downstream tasks. Experimental results show that TransTroj significantly
outperforms SOTA task-agnostic backdoor attacks (18%$\sim$99%, 68% on average)
and exhibits superior performance under various system settings. The code is
available at https://github.com/haowang-cqu/TransTroj .</div><div><a href='http://arxiv.org/abs/2401.15883v1'>2401.15883v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00576v1")'>Tropical Decision Boundaries for Neural Networks Are Robust Against
  Adversarial Attacks</div>
<div id='2402.00576v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T13:14:38Z</div><div>Authors: Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, Jefferson Huang</div><div style='padding-top: 10px; width: 80ex'>We introduce a simple, easy to implement, and computationally efficient
tropical convolutional neural network architecture that is robust against
adversarial attacks. We exploit the tropical nature of piece-wise linear neural
networks by embedding the data in the tropical projective torus in a single
hidden layer which can be added to any model. We study the geometry of its
decision boundary theoretically and show its robustness against adversarial
attacks on image datasets using computational experiments.</div><div><a href='http://arxiv.org/abs/2402.00576v1'>2402.00576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08695v1")'>Game of Trojans: Adaptive Adversaries Against Output-based
  Trojaned-Model Detectors</div>
<div id='2402.08695v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T20:14:46Z</div><div>Authors: Dinuka Sahabandu, Xiaojun Xu, Arezoo Rajabi, Luyao Niu, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran</div><div style='padding-top: 10px; width: 80ex'>We propose and analyze an adaptive adversary that can retrain a Trojaned DNN
and is also aware of SOTA output-based Trojaned model detectors. We show that
such an adversary can ensure (1) high accuracy on both trigger-embedded and
clean samples and (2) bypass detection. Our approach is based on an observation
that the high dimensionality of the DNN parameters provides sufficient degrees
of freedom to simultaneously achieve these objectives. We also enable SOTA
detectors to be adaptive by allowing retraining to recalibrate their
parameters, thus modeling a co-evolution of parameters of a Trojaned model and
detectors. We then show that this co-evolution can be modeled as an iterative
game, and prove that the resulting (optimal) solution of this interactive game
leads to the adversary successfully achieving the above objectives. In
addition, we provide a greedy algorithm for the adversary to select a minimum
number of input samples for embedding triggers. We show that for cross-entropy
or log-likelihood loss functions used by the DNNs, the greedy algorithm
provides provable guarantees on the needed number of trigger-embedded input
samples. Extensive experiments on four diverse datasets -- MNIST, CIFAR-10,
CIFAR-100, and SpeechCommand -- reveal that the adversary effectively evades
four SOTA output-based Trojaned model detectors: MNTD, NeuralCleanse, STRIP,
and TABOR.</div><div><a href='http://arxiv.org/abs/2402.08695v1'>2402.08695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09226v1")'>Directional Convergence Near Small Initializations and Saddles in
  Two-Homogeneous Neural Networks</div>
<div id='2402.09226v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:10:37Z</div><div>Authors: Akshay Kumar, Jarvis Haupt</div><div style='padding-top: 10px; width: 80ex'>This paper examines gradient flow dynamics of two-homogeneous neural networks
for small initializations, where all weights are initialized near the origin.
For both square and logistic losses, it is shown that for sufficiently small
initializations, the gradient flow dynamics spend sufficient time in the
neighborhood of the origin to allow the weights of the neural network to
approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a
neural correlation function that quantifies the correlation between the output
of the neural network and corresponding labels in the training data set. For
square loss, it has been observed that neural networks undergo saddle-to-saddle
dynamics when initialized close to the origin. Motivated by this, this paper
also shows a similar directional convergence among weights of small magnitude
in the neighborhood of certain saddle points.</div><div><a href='http://arxiv.org/abs/2402.09226v1'>2402.09226v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11427v1")'>OptEx: Expediting First-Order Optimization with Approximately
  Parallelized Iterations</div>
<div id='2402.11427v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T02:19:02Z</div><div>Authors: Yao Shu, Jiongfeng Fang, Ying Tiffany He, Fei Richard Yu</div><div style='padding-top: 10px; width: 80ex'>First-order optimization (FOO) algorithms are pivotal in numerous
computational domains such as machine learning and signal denoising. However,
their application to complex tasks like neural network training often entails
significant inefficiencies due to the need for many sequential iterations for
convergence. In response, we introduce first-order optimization expedited with
approximately parallelized iterations (OptEx), the first framework that
enhances the efficiency of FOO by leveraging parallel computing to mitigate its
iterative bottleneck. OptEx employs kernelized gradient estimation to make use
of gradient history for future gradient prediction, enabling parallelization of
iterations -- a strategy once considered impractical because of the inherent
iterative dependency in FOO. We provide theoretical guarantees for the
reliability of our kernelized gradient estimation and the iteration complexity
of SGD-based OptEx, confirming that estimation errors diminish to zero as
historical gradients accumulate and that SGD-based OptEx enjoys an effective
acceleration rate of $\Omega(\sqrt{N})$ over standard SGD given parallelism of
N. We also use extensive empirical studies, including synthetic functions,
reinforcement learning tasks, and neural network training across various
datasets, to underscore the substantial efficiency improvements achieved by
OptEx.</div><div><a href='http://arxiv.org/abs/2402.11427v1'>2402.11427v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02359v1")'>Incremental Quasi-Newton Methods with Faster Superlinear Convergence
  Rates</div>
<div id='2402.02359v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:54:51Z</div><div>Authors: Zhuanghua Liu, Luo Luo, Bryan Kian Hsiang Low</div><div style='padding-top: 10px; width: 80ex'>We consider the finite-sum optimization problem, where each component
function is strongly convex and has Lipschitz continuous gradient and Hessian.
The recently proposed incremental quasi-Newton method is based on BFGS update
and achieves a local superlinear convergence rate that is dependent on the
condition number of the problem. This paper proposes a more efficient
quasi-Newton method by incorporating the symmetric rank-1 update into the
incremental framework, which results in the condition-number-free local
superlinear convergence rate. Furthermore, we can boost our method by applying
the block update on the Hessian approximation, which leads to an even faster
local convergence rate. The numerical experiments show the proposed methods
significantly outperform the baseline methods.</div><div><a href='http://arxiv.org/abs/2402.02359v1'>2402.02359v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07271v1")'>Anderson acceleration for iteratively reweighted $\ell_1$ algorithm</div>
<div id='2403.07271v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T03:00:15Z</div><div>Authors: Kexin Li</div><div style='padding-top: 10px; width: 80ex'>Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving
sparse optimization problems with nonconvex and nonsmooth regularization. The
development of its acceleration algorithm, often employing Nesterov
acceleration, has sparked significant interest. Nevertheless, the convergence
and complexity analysis of these acceleration algorithms consistently poses
substantial challenges. Recently, Anderson acceleration has gained prominence
owing to its exceptional performance for speeding up fixed-point iteration,
with numerous recent studies applying it to gradient-based algorithms.
Motivated by the powerful impact of Anderson acceleration, we propose an
Anderson-accelerated IRL1 algorithm and establish its local linear convergence
rate. We extend this convergence result, typically observed in smooth settings,
to a nonsmooth scenario. Importantly, our theoretical results do not depend on
the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov
acceleration-based algorithms. Furthermore, to ensure global convergence, we
introduce a globally convergent Anderson accelerated IRL1 algorithm by
incorporating a classical nonmonotone line search condition. Experimental
results indicate that our algorithm outperforms existing Nesterov
acceleration-based algorithms.</div><div><a href='http://arxiv.org/abs/2403.07271v1'>2403.07271v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09274v1")'>Avoiding strict saddle points of nonconvex regularized problems</div>
<div id='2401.09274v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T15:25:50Z</div><div>Authors: Luwei Bai</div><div style='padding-top: 10px; width: 80ex'>We introduce a strict saddle property for $\ell_p$ regularized functions, and
propose an iterative reweighted $\ell_1$ algorithm to solve the $\ell_p$
regularized problems. The algorithm is guaranteed to converge only to local
minimizers when randomly initialized. The strict saddle property is shown
generic on these sparse optimization problems. Those analyses as well as the
proposed algorithm can be easily extended to general nonconvex regularized
problems.</div><div><a href='http://arxiv.org/abs/2401.09274v1'>2401.09274v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02544v1")'>Hyperparameter Estimation for Sparse Bayesian Learning Models</div>
<div id='2401.02544v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T21:24:01Z</div><div>Authors: Feng Yu, Lixin Shen, Guohui Song</div><div style='padding-top: 10px; width: 80ex'>Sparse Bayesian Learning (SBL) models are extensively used in signal
processing and machine learning for promoting sparsity through hierarchical
priors. The hyperparameters in SBL models are crucial for the model's
performance, but they are often difficult to estimate due to the non-convexity
and the high-dimensionality of the associated objective function. This paper
presents a comprehensive framework for hyperparameter estimation in SBL models,
encompassing well-known algorithms such as the expectation-maximization (EM),
MacKay, and convex bounding (CB) algorithms. These algorithms are cohesively
interpreted within an alternating minimization and linearization (AML)
paradigm, distinguished by their unique linearized surrogate functions.
Additionally, a novel algorithm within the AML framework is introduced, showing
enhanced efficiency, especially under low signal noise ratios. This is further
improved by a new alternating minimization and quadratic approximation (AMQ)
paradigm, which includes a proximal regularization term. The paper
substantiates these advancements with thorough convergence analysis and
numerical experiments, demonstrating the algorithm's effectiveness in various
noise conditions and signal-to-noise ratios.</div><div><a href='http://arxiv.org/abs/2401.02544v1'>2401.02544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15771v3")'>Bayesian Nonparametrics Meets Data-Driven Robust Optimization</div>
<div id='2401.15771v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T21:19:15Z</div><div>Authors: Nicola Bariletto, Nhat Ho</div><div style='padding-top: 10px; width: 80ex'>Training machine learning and statistical models often involves optimizing a
data-driven risk criterion. The risk is usually computed with respect to the
empirical data distribution, but this may result in poor and unstable
out-of-sample performance due to distributional uncertainty. In the spirit of
distributionally robust optimization, we propose a novel robust criterion by
combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory
and recent decision-theoretic models of smooth ambiguity-averse preferences.
First, we highlight novel connections with standard regularized empirical risk
minimization techniques, among which Ridge and LASSO regressions. Then, we
theoretically demonstrate the existence of favorable finite-sample and
asymptotic statistical guarantees on the performance of the robust optimization
procedure. For practical implementation, we propose and study tractable
approximations of the criterion based on well-known Dirichlet Process
representations. We also show that the smoothness of the criterion naturally
leads to standard gradient-based numerical optimization. Finally, we provide
insights into the workings of our method by applying it to high-dimensional
sparse linear regression, binary classification, and robust location parameter
estimation tasks.</div><div><a href='http://arxiv.org/abs/2401.15771v3'>2401.15771v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13622v1")'>Analysis of Bootstrap and Subsampling in High-dimensional Regularized
  Regression</div>
<div id='2402.13622v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T08:50:33Z</div><div>Authors: Lucas Clarté, Adrien Vandenbroucque, Guillaume Dalle, Bruno Loureiro, Florent Krzakala, Lenka Zdeborová</div><div style='padding-top: 10px; width: 80ex'>We investigate popular resampling methods for estimating the uncertainty of
statistical models, such as subsampling, bootstrap and the jackknife, and their
performance in high-dimensional supervised regression tasks. We provide a tight
asymptotic description of the biases and variances estimated by these methods
in the context of generalized linear models, such as ridge and logistic
regression, taking the limit where the number of samples $n$ and dimension $d$
of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Our
findings are three-fold: i) resampling methods are fraught with problems in
high dimensions and exhibit the double-descent-like behavior typical of these
situations; ii) only when $\alpha$ is large enough do they provide consistent
and reliable error estimations (we give convergence rates); iii) in the
over-parametrized regime $\alpha\!&lt;\!1$ relevant to modern machine learning
practice, their predictions are not consistent, even with optimal
regularization.</div><div><a href='http://arxiv.org/abs/2402.13622v1'>2402.13622v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13196v1")'>Practical Kernel Tests of Conditional Independence</div>
<div id='2402.13196v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:07:59Z</div><div>Authors: Roman Pogodin, Antonin Schrab, Yazhe Li, Danica J. Sutherland, Arthur Gretton</div><div style='padding-top: 10px; width: 80ex'>We describe a data-efficient, kernel-based approach to statistical testing of
conditional independence. A major challenge of conditional independence
testing, absent in tests of unconditional independence, is to obtain the
correct test level (the specified upper bound on the rate of false positives),
while still attaining competitive test power. Excess false positives arise due
to bias in the test statistic, which is obtained using nonparametric kernel
ridge regression. We propose three methods for bias control to correct the test
level, based on data splitting, auxiliary data, and (where possible) simpler
function classes. We show these combined strategies are effective both for
synthetic and real-world data.</div><div><a href='http://arxiv.org/abs/2402.13196v1'>2402.13196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01297v2")'>Characterizing Overfitting in Kernel Ridgeless Regression Through the
  Eigenspectrum</div>
<div id='2402.01297v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:36:53Z</div><div>Authors: Tin Sum Cheng, Aurelien Lucchi, Anastasis Kratsios, David Belius</div><div style='padding-top: 10px; width: 80ex'>We derive new bounds for the condition number of kernel matrices, which we
then use to enhance existing non-asymptotic test error bounds for kernel
ridgeless regression in the over-parameterized regime for a fixed input
dimension. For kernels with polynomial spectral decay, we recover the bound
from previous work; for exponential decay, our bound is non-trivial and novel.
  Our conclusion on overfitting is two-fold: (i) kernel regressors whose
eigenspectrum decays polynomially must generalize well, even in the presence of
noisy labeled training data; these models exhibit so-called tempered
overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays
exponentially, then it generalizes poorly, i.e., it exhibits catastrophic
overfitting. This adds to the available characterization of kernel ridge
regressors exhibiting benign overfitting as the extremal case where the
eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new
random matrix theory (RMT) techniques with recent tools in the kernel ridge
regression (KRR) literature.</div><div><a href='http://arxiv.org/abs/2402.01297v2'>2402.01297v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12885v1")'>A Bound on the Maximal Marginal Degrees of Freedom</div>
<div id='2402.12885v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T10:25:44Z</div><div>Authors: Paul Dommel</div><div style='padding-top: 10px; width: 80ex'>Common kernel ridge regression is expensive in memory allocation and
computation time. This paper addresses low rank approximations and surrogates
for kernel ridge regression, which bridge these difficulties. The fundamental
contribution of the paper is a lower bound on the rank of the low dimensional
approximation, which is required such that the prediction power remains
reliable. The bound relates the effective dimension with the largest
statistical leverage score. We characterize the effective dimension and its
growth behavior with respect to the regularization parameter by involving the
regularity of the kernel. This growth is demonstrated to be asymptotically
logarithmic for suitably chosen kernels, justifying low-rank approximations as
the Nystr\"om method.</div><div><a href='http://arxiv.org/abs/2402.12885v1'>2402.12885v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15718v1")'>A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime</div>
<div id='2402.15718v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T04:57:59Z</div><div>Authors: Jihao Long, Xiaojun Peng, Lei Wu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we conduct a comprehensive analysis of generalization
properties of Kernel Ridge Regression (KRR) in the noiseless regime, a scenario
crucial to scientific computing, where data are often generated via computer
simulations. We prove that KRR can attain the minimax optimal rate, which
depends on both the eigenvalue decay of the associated kernel and the relative
smoothness of target functions. Particularly, when the eigenvalue decays
exponentially fast, KRR achieves the spectral accuracy, i.e., a convergence
rate faster than any polynomial. Moreover, the numerical experiments well
corroborate our theoretical findings. Our proof leverages a novel extension of
the duality framework introduced by Chen et al. (2023), which could be useful
in analyzing kernel-based methods beyond the scope of this work.</div><div><a href='http://arxiv.org/abs/2402.15718v1'>2402.15718v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06731v1")'>On the Approximation of Kernel functions</div>
<div id='2403.06731v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T13:50:07Z</div><div>Authors: Paul Dommel, Alois Pichler</div><div style='padding-top: 10px; width: 80ex'>Various methods in statistical learning build on kernels considered in
reproducing kernel Hilbert spaces. In applications, the kernel is often
selected based on characteristics of the problem and the data. This kernel is
then employed to infer response variables at points, where no explanatory data
were observed. The data considered here are located in compact sets in higher
dimensions and the paper addresses approximations of the kernel itself. The new
approach considers Taylor series approximations of radial kernel functions. For
the Gauss kernel on the unit cube, the paper establishes an upper bound of the
associated eigenfunctions, which grows only polynomially with respect to the
index. The novel approach substantiates smaller regularization parameters than
considered in the literature, overall leading to better approximations. This
improvement confirms low rank approximation methods such as the Nystr\"om
method.</div><div><a href='http://arxiv.org/abs/2403.06731v1'>2403.06731v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10070v1")'>A Structure-Preserving Kernel Method for Learning Hamiltonian Systems</div>
<div id='2403.10070v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T07:20:21Z</div><div>Authors: Jianyu Hu, Juan-Pablo Ortega, Daiying Yin</div><div style='padding-top: 10px; width: 80ex'>A structure-preserving kernel ridge regression method is presented that
allows the recovery of potentially high-dimensional and nonlinear Hamiltonian
functions out of datasets made of noisy observations of Hamiltonian vector
fields. The method proposes a closed-form solution that yields excellent
numerical performances that surpass other techniques proposed in the literature
in this setup. From the methodological point of view, the paper extends kernel
regression methods to problems in which loss functions involving linear
functions of gradients are required and, in particular, a differential
reproducing property and a Representer Theorem are proved in this context. The
relation between the structure-preserving kernel estimator and the Gaussian
posterior mean estimator is analyzed. A full error analysis is conducted that
provides convergence rates using fixed and adaptive regularization parameters.
The good performance of the proposed estimator is illustrated with various
numerical experiments.</div><div><a href='http://arxiv.org/abs/2403.10070v1'>2403.10070v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11696v1")'>Generalization error of spectral algorithms</div>
<div id='2403.11696v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T11:52:33Z</div><div>Authors: Maksim Velikanov, Maxim Panov, Dmitry Yarotsky</div><div style='padding-top: 10px; width: 80ex'>The asymptotically precise estimation of the generalization of kernel methods
has recently received attention due to the parallels between neural networks
and their associated kernels. However, prior works derive such estimates for
training by kernel ridge regression (KRR), whereas neural networks are
typically trained with gradient descent (GD). In the present work, we consider
the training of kernels with a family of $\textit{spectral algorithms}$
specified by profile $h(\lambda)$, and including KRR and GD as special cases.
Then, we derive the generalization error as a functional of learning profile
$h(\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional
translation-invariant model. Under power-law assumptions on the spectrum of the
kernel and target, we use our framework to (i) give full loss asymptotics for
both noisy and noiseless observations (ii) show that the loss localizes on
certain spectral scales, giving a new perspective on the KRR saturation
phenomenon (iii) conjecture, and demonstrate for the considered data models,
the universality of the loss w.r.t. non-spectral details of the problem, but
only in case of noisy observation.</div><div><a href='http://arxiv.org/abs/2403.11696v1'>2403.11696v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01599v1")'>Generalization Error Curves for Analytic Spectral Algorithms under
  Power-law Decay</div>
<div id='2401.01599v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T08:00:50Z</div><div>Authors: Yicheng Li, Weiye Gan, Zuoqiang Shi, Qian Lin</div><div style='padding-top: 10px; width: 80ex'>The generalization error curve of certain kernel regression method aims at
determining the exact order of generalization error with various source
condition, noise level and choice of the regularization parameter rather than
the minimax rate. In this work, under mild assumptions, we rigorously provide a
full characterization of the generalization error curves of the kernel gradient
descent method (and a large class of analytic spectral algorithms) in kernel
regression. Consequently, we could sharpen the near inconsistency of kernel
interpolation and clarify the saturation effects of kernel regression
algorithms with higher qualification, etc. Thanks to the neural tangent kernel
theory, these results greatly improve our understanding of the generalization
behavior of training the wide neural networks. A novel technical contribution,
the analytic functional argument, might be of independent interest.</div><div><a href='http://arxiv.org/abs/2401.01599v1'>2401.01599v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01270v1")'>Optimal Rates of Kernel Ridge Regression under Source Condition in Large
  Dimensions</div>
<div id='2401.01270v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T16:14:35Z</div><div>Authors: Haobo Zhang, Yicheng Li, Weihao Lu, Qian Lin</div><div style='padding-top: 10px; width: 80ex'>Motivated by the studies of neural networks (e.g.,the neural tangent kernel
theory), we perform a study on the large-dimensional behavior of kernel ridge
regression (KRR) where the sample size $n \asymp d^{\gamma}$ for some $\gamma &gt;
0$. Given an RKHS $\mathcal{H}$ associated with an inner product kernel defined
on the sphere $\mathbb{S}^{d}$, we suppose that the true function $f_{\rho}^{*}
\in [\mathcal{H}]^{s}$, the interpolation space of $\mathcal{H}$ with source
condition $s&gt;0$. We first determined the exact order (both upper and lower
bound) of the generalization error of kernel ridge regression for the optimally
chosen regularization parameter $\lambda$. We then further showed that when
$0&lt;s\le1$, KRR is minimax optimal; and when $s&gt;1$, KRR is not minimax optimal
(a.k.a. he saturation effect). Our results illustrate that the curves of rate
varying along $\gamma$ exhibit the periodic plateau behavior and the multiple
descent behavior and show how the curves evolve with $s&gt;0$. Interestingly, our
work provides a unified viewpoint of several recent works on kernel regression
in the large-dimensional setting, which correspond to $s=0$ and $s=1$
respectively.</div><div><a href='http://arxiv.org/abs/2401.01270v1'>2401.01270v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01148v1")'>The Optimality of Kernel Classifiers in Sobolev Space</div>
<div id='2402.01148v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T05:23:34Z</div><div>Authors: Jianfa Lai, Zhifan Li, Dongming Huang, Qian Lin</div><div style='padding-top: 10px; width: 80ex'>Kernel methods are widely used in machine learning, especially for
classification problems. However, the theoretical analysis of kernel
classification is still limited. This paper investigates the statistical
performances of kernel classifiers. With some mild assumptions on the
conditional probability $\eta(x)=\mathbb{P}(Y=1\mid X=x)$, we derive an upper
bound on the classification excess risk of a kernel classifier using recent
advances in the theory of kernel regression. We also obtain a minimax lower
bound for Sobolev spaces, which shows the optimality of the proposed
classifier. Our theoretical results can be extended to the generalization error
of overparameterized neural network classifiers. To make our theoretical
results more applicable in realistic settings, we also propose a simple method
to estimate the interpolation smoothness of $2\eta(x)-1$ and apply the method
to real datasets.</div><div><a href='http://arxiv.org/abs/2402.01148v1'>2402.01148v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08260v1")'>Fast Kernel Summation in High Dimensions via Slicing and Fourier
  Transforms</div>
<div id='2401.08260v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T10:31:27Z</div><div>Authors: Johannes Hertrich</div><div style='padding-top: 10px; width: 80ex'>Kernel-based methods are heavily used in machine learning. However, they
suffer from $O(N^2)$ complexity in the number $N$ of considered data points. In
this paper, we propose an approximation procedure, which reduces this
complexity to $O(N)$. Our approach is based on two ideas. First, we prove that
any radial kernel with analytic basis function can be represented as sliced
version of some one-dimensional kernel and derive an analytic formula for the
one-dimensional counterpart. It turns out that the relation between one- and
$d$-dimensional kernels is given by a generalized Riemann-Liouville fractional
integral. Hence, we can reduce the $d$-dimensional kernel summation to a
one-dimensional setting. Second, for solving these one-dimensional problems
efficiently, we apply fast Fourier summations on non-equispaced data, a sorting
algorithm or a combination of both. Due to its practical importance we pay
special attention to the Gaussian kernel, where we show a dimension-independent
error bound and represent its one-dimensional counterpart via a closed-form
Fourier transform. We provide a run time comparison and error estimate of our
fast kernel summations.</div><div><a href='http://arxiv.org/abs/2401.08260v1'>2401.08260v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12187v1")'>Approximation of RKHS Functionals by Neural Networks</div>
<div id='2403.12187v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T18:58:23Z</div><div>Authors: Tian-Yi Zhou, Namjoon Suh, Guang Cheng, Xiaoming Huo</div><div style='padding-top: 10px; width: 80ex'>Motivated by the abundance of functional data such as time series and images,
there has been a growing interest in integrating such data into neural networks
and learning maps from function spaces to R (i.e., functionals). In this paper,
we study the approximation of functionals on reproducing kernel Hilbert spaces
(RKHS's) using neural networks. We establish the universality of the
approximation of functionals on the RKHS's. Specifically, we derive explicit
error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev
kernels. Moreover, we apply our findings to functional regression, proving that
neural networks can accurately approximate the regression maps in generalized
functional linear models. Existing works on functional learning require
integration-type basis function expansions with a set of pre-specified basis
functions. By leveraging the interpolating orthogonal projections in RKHS's,
our proposed network is much simpler in that we use point evaluations to
replace basis function expansions.</div><div><a href='http://arxiv.org/abs/2403.12187v1'>2403.12187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14227v1")'>Quaternion recurrent neural network with real-time recurrent learning
  and maximum correntropy criterion</div>
<div id='2402.14227v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T02:17:50Z</div><div>Authors: Pauline Bourigault, Dongpo Xu, Danilo P. Mandic</div><div style='padding-top: 10px; width: 80ex'>We develop a robust quaternion recurrent neural network (QRNN) for real-time
processing of 3D and 4D data with outliers. This is achieved by combining the
real-time recurrent learning (RTRL) algorithm and the maximum correntropy
criterion (MCC) as a loss function. While both the mean square error and
maximum correntropy criterion are viable cost functions, it is shown that the
non-quadratic maximum correntropy loss function is less sensitive to outliers,
making it suitable for applications with multidimensional noisy or uncertain
data. Both algorithms are derived based on the novel generalised HR (GHR)
calculus, which allows for the differentiation of real functions of quaternion
variables and offers the product and chain rules, thus enabling elegant and
compact derivations. Simulation results in the context of motion prediction of
chest internal markers for lung cancer radiotherapy, which includes regular and
irregular breathing sequences, support the analysis.</div><div><a href='http://arxiv.org/abs/2402.14227v1'>2402.14227v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05368v1")'>Exploring the Links between the Fundamental Lemma and Kernel Regression</div>
<div id='2403.05368v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T14:59:15Z</div><div>Authors: Oleksii Molodchyk, Timm Faulwasser</div><div style='padding-top: 10px; width: 80ex'>Generalizations and variations of the fundamental lemma by Willems et al. are
an active topic of recent research. In this note, we explore and formalize the
links between kernel regression and known nonlinear extensions of the
fundamental lemma. Applying a transformation to the usual linear equation in
Hankel matrices, we arrive at an alternative implicit kernel representation of
the system trajectories while keeping the requirements on persistency of
excitation. We show that this representation is equivalent to the solution of a
specific kernel regression problem. We explore the possible structures of the
underlying kernel as well as the system classes to which they correspond.</div><div><a href='http://arxiv.org/abs/2403.05368v1'>2403.05368v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00501v1")'>Equivalence of the Empirical Risk Minimization to Regularization on the
  Family of f-Divergences</div>
<div id='2402.00501v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T11:12:00Z</div><div>Authors: Francisco Daunas, Iñaki Esnaola, Samir M. Perlaza, H. Vincent Poor</div><div style='padding-top: 10px; width: 80ex'>The solution to empirical risk minimization with $f$-divergence
regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under
such conditions, the optimal measure is shown to be unique. Examples of the
solution for particular choices of the function $f$ are presented. Previously
known solutions to common regularization choices are obtained by leveraging the
flexibility of the family of $f$-divergences. These include the unique
solutions to empirical risk minimization with relative entropy regularization
(Type-I and Type-II). The analysis of the solution unveils the following
properties of $f$-divergences when used in the ERM-$f$DR problem: $i\bigl)$
$f$-divergence regularization forces the support of the solution to coincide
with the support of the reference measure, which introduces a strong inductive
bias that dominates the evidence provided by the training data; and $ii\bigl)$
any $f$-divergence regularization is equivalent to a different $f$-divergence
regularization with an appropriate transformation of the empirical risk
function.</div><div><a href='http://arxiv.org/abs/2402.00501v1'>2402.00501v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15254v1")'>Finite Sample Confidence Regions for Linear Regression Parameters Using
  Arbitrary Predictors</div>
<div id='2401.15254v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T00:15:48Z</div><div>Authors: Charles Guille-Escuret, Eugene Ndiaye</div><div style='padding-top: 10px; width: 80ex'>We explore a novel methodology for constructing confidence regions for
parameters of linear models, using predictions from any arbitrary predictor.
Our framework requires minimal assumptions on the noise and can be extended to
functions deviating from strict linearity up to some adjustable threshold,
thereby accommodating a comprehensive and pragmatically relevant set of
functions. The derived confidence regions can be cast as constraints within a
Mixed Integer Linear Programming framework, enabling optimisation of linear
objectives. This representation enables robust optimization and the extraction
of confidence intervals for specific parameter coordinates. Unlike previous
methods, the confidence region can be empty, which can be used for hypothesis
testing. Finally, we validate the empirical applicability of our method on
synthetic data.</div><div><a href='http://arxiv.org/abs/2401.15254v1'>2401.15254v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15213v2")'>Statistical Agnostic Regression: a machine learning method to validate
  regression models</div>
<div id='2402.15213v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T09:19:26Z</div><div>Authors: Juan M Gorriz, J. Ramirez, F. Segovia, F. J. Martinez-Murcia, C. Jiménez-Mesa, J. Suckling</div><div style='padding-top: 10px; width: 80ex'>Regression analysis is a central topic in statistical modeling, aiming to
estimate the relationships between a dependent variable, commonly referred to
as the response variable, and one or more independent variables, i.e.,
explanatory variables. Linear regression is by far the most popular method for
performing this task in several fields of research, such as prediction,
forecasting, or causal inference. Beyond various classical methods to solve
linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso
regressions - which are often the foundation for more advanced machine learning
(ML) techniques - the latter have been successfully applied in this scenario
without a formal definition of statistical significance. At most, permutation
or classical analyses based on empirical measures (e.g., residuals or accuracy)
have been conducted to reflect the greater ability of ML estimations for
detection. In this paper, we introduce a method, named Statistical Agnostic
Regression (SAR), for evaluating the statistical significance of an ML-based
linear regression based on concentration inequalities of the actual risk using
the analysis of the worst case. To achieve this goal, similar to the
classification problem, we define a threshold to establish that there is
sufficient evidence with a probability of at least 1-eta to conclude that there
is a linear relationship in the population between the explanatory (feature)
and the response (label) variables. Simulations in only two dimensions
demonstrate the ability of the proposed agnostic test to provide a similar
analysis of variance given by the classical $F$ test for the slope parameter.</div><div><a href='http://arxiv.org/abs/2402.15213v2'>2402.15213v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08702v2")'>Do We Really Even Need Data?</div>
<div id='2401.08702v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T23:19:21Z</div><div>Authors: Kentaro Hoffman, Stephen Salerno, Awan Afiaz, Jeffrey T. Leek, Tyler H. McCormick</div><div style='padding-top: 10px; width: 80ex'>As artificial intelligence and machine learning tools become more accessible,
and scientists face new obstacles to data collection (e.g. rising costs,
declining survey response rates), researchers increasingly use predictions from
pre-trained algorithms as outcome variables. Though appealing for financial and
logistical reasons, using standard tools for inference can misrepresent the
association between independent variables and the outcome of interest when the
true, unobserved outcome is replaced by a predicted value. In this paper, we
characterize the statistical challenges inherent to this so-called ``inference
with predicted data'' problem and elucidate three potential sources of error:
(i) the relationship between predicted outcomes and their true, unobserved
counterparts, (ii) robustness of the machine learning model to resampling or
uncertainty about the training data, and (iii) appropriately propagating not
just bias but also uncertainty from predictions into the ultimate inference
procedure.</div><div><a href='http://arxiv.org/abs/2401.08702v2'>2401.08702v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10727v1")'>Predictive Uncertainty Quantification via Risk Decompositions for
  Strictly Proper Scoring Rules</div>
<div id='2402.10727v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T14:40:22Z</div><div>Authors: Nikita Kotelevskii, Maxim Panov</div><div style='padding-top: 10px; width: 80ex'>Distinguishing sources of predictive uncertainty is of crucial importance in
the application of forecasting models across various domains. Despite the
presence of a great variety of proposed uncertainty measures, there are no
strict definitions to disentangle them. Furthermore, the relationship between
different measures of uncertainty quantification remains somewhat unclear. In
this work, we introduce a general framework, rooted in statistical reasoning,
which not only allows the creation of new uncertainty measures but also
clarifies their interrelations. Our approach leverages statistical risk to
distinguish aleatoric and epistemic uncertainty components and utilizes proper
scoring rules to quantify them. To make it practically tractable, we propose an
idea to incorporate Bayesian reasoning into this framework and discuss the
properties of the proposed approximation.</div><div><a href='http://arxiv.org/abs/2402.10727v1'>2402.10727v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16326v2")'>A Provably Accurate Randomized Sampling Algorithm for Logistic
  Regression</div>
<div id='2402.16326v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T06:20:28Z</div><div>Authors: Agniva Chowdhury, Pradeep Ramuhalli</div><div style='padding-top: 10px; width: 80ex'>In statistics and machine learning, logistic regression is a widely-used
supervised learning technique primarily employed for binary classification
tasks. When the number of observations greatly exceeds the number of predictor
variables, we present a simple, randomized sampling-based algorithm for
logistic regression problem that guarantees high-quality approximations to both
the estimated probabilities and the overall discrepancy of the model. Our
analysis builds upon two simple structural conditions that boil down to
randomized matrix multiplication, a fundamental and well-understood primitive
of randomized numerical linear algebra. We analyze the properties of estimated
probabilities of logistic regression when leverage scores are used to sample
observations, and prove that accurate approximations can be achieved with a
sample whose size is much smaller than the total number of observations. To
further validate our theoretical findings, we conduct comprehensive empirical
evaluations. Overall, our work sheds light on the potential of using randomized
sampling approaches to efficiently approximate the estimated probabilities in
logistic regression, offering a practical and computationally efficient
solution for large-scale datasets.</div><div><a href='http://arxiv.org/abs/2402.16326v2'>2402.16326v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14103v1")'>Computational-Statistical Gaps for Improper Learning in Sparse Linear
  Regression</div>
<div id='2402.14103v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:55:01Z</div><div>Authors: Rares-Darius Buhai, Jingqiu Ding, Stefan Tiegel</div><div style='padding-top: 10px; width: 80ex'>We study computational-statistical gaps for improper learning in sparse
linear regression. More specifically, given $n$ samples from a $k$-sparse
linear model in dimension $d$, we ask what is the minimum sample complexity to
efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense
estimate for the regression vector that achieves non-trivial prediction error
on the $n$ samples. Information-theoretically this can be achieved using
$\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature,
there is no polynomial-time algorithm known to achieve the same guarantees
using less than $\Theta(d)$ samples without additional restrictions on the
model. Similarly, existing hardness results are either restricted to the proper
setting, in which the estimate must be sparse as well, or only apply to
specific algorithms.
  We give evidence that efficient algorithms for this task require at least
(roughly) $\Omega(k^2)$ samples. In particular, we show that an improper
learning algorithm for sparse linear regression can be used to solve sparse PCA
problems (with a negative spike) in their Wishart form, in regimes in which
efficient algorithms are widely believed to require at least $\Omega(k^2)$
samples. We complement our reduction with low-degree and statistical query
lower bounds for the sparse PCA problems from which we reduce.
  Our hardness results apply to the (correlated) random design setting in which
the covariates are drawn i.i.d. from a mean-zero Gaussian distribution with
unknown covariance.</div><div><a href='http://arxiv.org/abs/2402.14103v1'>2402.14103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14645v1")'>Sparse Linear Regression and Lattice Problems</div>
<div id='2402.14645v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T15:45:27Z</div><div>Authors: Aparna Gupte, Neekon Vafa, Vinod Vaikuntanathan</div><div style='padding-top: 10px; width: 80ex'>Sparse linear regression (SLR) is a well-studied problem in statistics where
one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector
$y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is,
$\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find
a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean
squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$.
While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig
selector solve SLR when the design matrix is well-conditioned, no general
algorithm is known, nor is there any formal evidence of hardness in an
average-case setting with respect to all efficient algorithms.
  We give evidence of average-case hardness of SLR w.r.t. all efficient
algorithms assuming the worst-case hardness of lattice problems. Specifically,
we give an instance-by-instance reduction from a variant of the bounded
distance decoding (BDD) problem on lattices to SLR, where the condition number
of the lattice basis that defines the BDD instance is directly related to the
restricted eigenvalue condition of the design matrix, which characterizes some
of the classical statistical-computational gaps for sparse linear regression.
Also, by appealing to worst-case to average-case reductions from the world of
lattices, this shows hardness for a distribution of SLR instances; while the
design matrices are ill-conditioned, the resulting SLR instances are in the
identifiable regime.
  Furthermore, for well-conditioned (essentially) isotropic Gaussian design
matrices, where Lasso is known to behave well in the identifiable regime, we
show hardness of outputting any good solution in the unidentifiable regime
where there are many solutions, assuming the worst-case hardness of standard
and well-studied lattice problems.</div><div><a href='http://arxiv.org/abs/2402.14645v1'>2402.14645v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15409v1")'>Lasso with Latents: Efficient Estimation, Covariate Rescaling, and
  Computational-Statistical Gaps</div>
<div id='2402.15409v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:16:38Z</div><div>Authors: Jonathan Kelner, Frederic Koehler, Raghu Meka, Dhruv Rohatgi</div><div style='padding-top: 10px; width: 80ex'>It is well-known that the statistical performance of Lasso can suffer
significantly when the covariates of interest have strong correlations. In
particular, the prediction error of Lasso becomes much worse than
computationally inefficient alternatives like Best Subset Selection. Due to a
large conjectured computational-statistical tradeoff in the problem of sparse
linear regression, it may be impossible to close this gap in general.
  In this work, we propose a natural sparse linear regression setting where
strong correlations between covariates arise from unobserved latent variables.
In this setting, we analyze the problem caused by strong correlations and
design a surprisingly simple fix. While Lasso with standard normalization of
covariates fails, there exists a heterogeneous scaling of the covariates with
which Lasso will suddenly obtain strong provable guarantees for estimation.
Moreover, we design a simple, efficient procedure for computing such a "smart
scaling."
  The sample complexity of the resulting "rescaled Lasso" algorithm incurs (in
the worst case) quadratic dependence on the sparsity of the underlying signal.
While this dependence is not information-theoretically necessary, we give
evidence that it is optimal among the class of polynomial-time algorithms, via
the method of low-degree polynomials. This argument reveals a new connection
between sparse linear regression and a special version of sparse PCA with a
near-critical negative spike. The latter problem can be thought of as a
real-valued analogue of learning a sparse parity. Using it, we also establish
the first computational-statistical gap for the closely related problem of
learning a Gaussian Graphical Model.</div><div><a href='http://arxiv.org/abs/2402.15409v1'>2402.15409v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05529v2")'>Computational-Statistical Gaps in Gaussian Single-Index Models</div>
<div id='2403.05529v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T18:50:19Z</div><div>Authors: Alex Damian, Loucas Pillaud-Vivien, Jason D. Lee, Joan Bruna</div><div style='padding-top: 10px; width: 80ex'>Single-Index Models are high-dimensional regression problems with planted
structure, whereby labels depend on an unknown one-dimensional projection of
the input via a generic, non-linear, and potentially non-deterministic
transformation. As such, they encompass a broad class of statistical inference
tasks, and provide a rich template to study statistical and computational
trade-offs in the high-dimensional regime.
  While the information-theoretic sample complexity to recover the hidden
direction is linear in the dimension $d$, we show that computationally
efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree
Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$
samples, where $k^\star$ is a "generative" exponent associated with the model
that we explicitly characterize. Moreover, we show that this sample complexity
is also sufficient, by establishing matching upper bounds using a partial-trace
algorithm. Therefore, our results provide evidence of a sharp
computational-to-statistical gap (under both the SQ and LDP class) whenever
$k^\star&gt;2$. To complete the study, we provide examples of smooth and Lipschitz
deterministic target functions with arbitrarily large generative exponents
$k^\star$.</div><div><a href='http://arxiv.org/abs/2403.05529v2'>2403.05529v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04744v1")'>SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker
  Assumptions</div>
<div id='2403.04744v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:49:32Z</div><div>Authors: Ilias Diakonikolas, Daniel Kane, Lisheng Ren, Yuxin Sun</div><div style='padding-top: 10px; width: 80ex'>We study the complexity of Non-Gaussian Component Analysis (NGCA) in the
Statistical Query (SQ) model. Prior work developed a general methodology to
prove SQ lower bounds for this task that have been applicable to a wide range
of contexts. In particular, it was known that for any univariate distribution
$A$ satisfying certain conditions, distinguishing between a standard
multivariate Gaussian and a distribution that behaves like $A$ in a random
hidden direction and like a standard Gaussian in the orthogonal complement, is
SQ-hard. The required conditions were that (1) $A$ matches many low-order
moments with the standard univariate Gaussian, and (2) the chi-squared norm of
$A$ with respect to the standard Gaussian is finite. While the moment-matching
condition is necessary for hardness, the chi-squared condition was only
required for technical reasons. In this work, we establish that the latter
condition is indeed not necessary. In particular, we prove near-optimal SQ
lower bounds for NGCA under the moment-matching condition only. Our result
naturally generalizes to the setting of a hidden subspace. Leveraging our
general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of
concrete estimation tasks where existing techniques provide sub-optimal or even
vacuous guarantees.</div><div><a href='http://arxiv.org/abs/2403.04744v1'>2403.04744v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02300v1")'>Statistical Query Lower Bounds for Learning Truncated Gaussians</div>
<div id='2403.02300v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T18:30:33Z</div><div>Authors: Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis</div><div style='padding-top: 10px; width: 80ex'>We study the problem of estimating the mean of an identity covariance
Gaussian in the truncated setting, in the regime when the truncation set comes
from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed
but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to
samples from the distribution $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$
truncated to the set $S$. The goal is to estimate $\boldsymbol\mu$ within
accuracy $\epsilon&gt;0$ in $\ell_2$-norm. Our main result is a Statistical Query
(SQ) lower bound suggesting a super-polynomial information-computation gap for
this task. In more detail, we show that the complexity of any SQ algorithm for
this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class
$\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples
information-theoretically suffice. Concretely, our SQ lower bound applies when
$\mathcal{C}$ is a union of a bounded number of rectangles whose VC dimension
and Gaussian surface are small. As a corollary of our construction, it also
follows that the complexity of the previously known algorithm for this task is
qualitatively best possible.</div><div><a href='http://arxiv.org/abs/2403.02300v1'>2403.02300v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00544v1")'>Quantum-Assisted Hilbert-Space Gaussian Process Regression</div>
<div id='2402.00544v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T12:13:35Z</div><div>Authors: Ahmad Farooq, Cristian A. Galvis-Florez, Simo Särkkä</div><div style='padding-top: 10px; width: 80ex'>Gaussian processes are probabilistic models that are commonly used as
functional priors in machine learning. Due to their probabilistic nature, they
can be used to capture the prior information on the statistics of noise,
smoothness of the functions, and training data uncertainty. However, their
computational complexity quickly becomes intractable as the size of the data
set grows. We propose a Hilbert space approximation-based quantum algorithm for
Gaussian process regression to overcome this limitation. Our method consists of
a combination of classical basis function expansion with quantum computing
techniques of quantum principal component analysis, conditional rotations, and
Hadamard and Swap tests. The quantum principal component analysis is used to
estimate the eigenvalues while the conditional rotations and the Hadamard and
Swap tests are employed to evaluate the posterior mean and variance of the
Gaussian process. Our method provides polynomial computational complexity
reduction over the classical method.</div><div><a href='http://arxiv.org/abs/2402.00544v1'>2402.00544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10416v1")'>Robust Sparse Estimation for Gaussians with Optimal Error under Huber
  Contamination</div>
<div id='2403.10416v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:51:27Z</div><div>Authors: Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia, Thanasis Pittas</div><div style='padding-top: 10px; width: 80ex'>We study Gaussian sparse estimation tasks in Huber's contamination model with
a focus on mean estimation, PCA, and linear regression. For each of these
tasks, we give the first sample and computationally efficient robust estimators
with optimal error guarantees, within constant factors. All prior efficient
algorithms for these tasks incur quantitatively suboptimal error. Concretely,
for Gaussian robust $k$-sparse mean estimation on $\mathbb{R}^d$ with
corruption rate $\epsilon&gt;0$, our algorithm has sample complexity
$(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$, runs in sample polynomial time,
and approximates the target mean within $\ell_2$-error $O(\epsilon)$. Previous
efficient algorithms inherently incur error $\Omega(\epsilon
\sqrt{\log(1/\epsilon)})$. At the technical level, we develop a novel
multidimensional filtering method in the sparse regime that may find other
applications.</div><div><a href='http://arxiv.org/abs/2403.10416v1'>2403.10416v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17756v1")'>Robustly Learning Single-Index Models via Alignment Sharpness</div>
<div id='2402.17756v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:48:07Z</div><div>Authors: Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas</div><div style='padding-top: 10px; width: 80ex'>We study the problem of learning Single-Index Models under the $L_2^2$ loss
in the agnostic model. We give an efficient learning algorithm, achieving a
constant factor approximation to the optimal loss, that succeeds under a range
of distributions (including log-concave distributions) and a broad class of
monotone and Lipschitz link functions. This is the first efficient constant
factor approximate agnostic learner, even for Gaussian data and for any
nontrivial class of link functions. Prior work for the case of unknown link
function either works in the realizable setting or does not attain constant
factor approximation. The main technical ingredient enabling our algorithm and
analysis is a novel notion of a local error bound in optimization that we term
alignment sharpness and that may be of broader interest.</div><div><a href='http://arxiv.org/abs/2402.17756v1'>2402.17756v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01400v1")'>Query-Efficient Correlation Clustering with Noisy Oracle</div>
<div id='2402.01400v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:31:24Z</div><div>Authors: Yuko Kuroki, Atsushi Miyauchi, Francesco Bonchi, Wei Chen</div><div style='padding-top: 10px; width: 80ex'>We study a general clustering setting in which we have $n$ elements to be
clustered, and we aim to perform as few queries as possible to an oracle that
returns a noisy sample of the similarity between two elements. Our setting
encompasses many application domains in which the similarity function is costly
to compute and inherently noisy. We propose two novel formulations of online
learning problems rooted in the paradigm of Pure Exploration in Combinatorial
Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For
both settings, we design algorithms that combine a sampling strategy with a
classic approximation algorithm for correlation clustering and study their
theoretical guarantees. Our results are the first examples of polynomial-time
algorithms that work for the case of PE-CMAB in which the underlying offline
optimization problem is NP-hard.</div><div><a href='http://arxiv.org/abs/2402.01400v1'>2402.01400v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03587v1")'>Effective Acquisition Functions for Active Correlation Clustering</div>
<div id='2402.03587v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T23:33:57Z</div><div>Authors: Linus Aronsson, Morteza Haghir Chehreghani</div><div style='padding-top: 10px; width: 80ex'>Correlation clustering is a powerful unsupervised learning paradigm that
supports positive and negative similarities. In this paper, we assume the
similarities are not known in advance. Instead, we employ active learning to
iteratively query similarities in a cost-efficient way. In particular, we
develop three effective acquisition functions to be used in this setting. One
is based on the notion of inconsistency (i.e., when similarities violate the
transitive property). The remaining two are based on information-theoretic
quantities, i.e., entropy and information gain.</div><div><a href='http://arxiv.org/abs/2402.03587v1'>2402.03587v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15566v1")'>On the Robustness of Cross-Concentrated Sampling for Matrix Completion</div>
<div id='2401.15566v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T04:18:44Z</div><div>Authors: HanQin Cai, Longxiu Huang, Chandra Kundu, Bowen Su</div><div style='padding-top: 10px; width: 80ex'>Matrix completion is one of the crucial tools in modern data science
research. Recently, a novel sampling model for matrix completion coined
cross-concentrated sampling (CCS) has caught much attention. However, the
robustness of the CCS model against sparse outliers remains unclear in the
existing studies. In this paper, we aim to answer this question by exploring a
novel Robust CCS Completion problem. A highly efficient non-convex iterative
algorithm, dubbed Robust CUR Completion (RCURC), is proposed. The empirical
performance of the proposed algorithm, in terms of both efficiency and
robustness, is verified in synthetic and real datasets.</div><div><a href='http://arxiv.org/abs/2401.15566v1'>2401.15566v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01810v1")'>Misspecification uncertainties in near-deterministic regression</div>
<div id='2402.01810v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:41:21Z</div><div>Authors: Thomas D Swinburne, Danny Perez</div><div style='padding-top: 10px; width: 80ex'>The expected loss is an upper bound to the model generalization error which
admits robust PAC-Bayes bounds for learning. However, loss minimization is
known to ignore misspecification, where models cannot exactly reproduce
observations. This leads to significant underestimates of parameter
uncertainties in the large data, or underparameterized, limit. We analyze the
generalization error of near-deterministic, misspecified and underparametrized
surrogate models, a regime of broad relevance in science and engineering. We
show posterior distributions must cover every training point to avoid a
divergent generalization error and derive an ensemble {ansatz} that respects
this constraint, which for linear models incurs minimal overhead. The efficient
approach is demonstrated on model problems before application to high
dimensional datasets in atomistic machine learning. Parameter uncertainties
from misspecification survive in the underparametrized limit, giving accurate
prediction and bounding of test errors.</div><div><a href='http://arxiv.org/abs/2402.01810v1'>2402.01810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15995v1")'>Improved Hardness Results for Learning Intersections of Halfspaces</div>
<div id='2402.15995v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T05:26:35Z</div><div>Authors: Stefan Tiegel</div><div style='padding-top: 10px; width: 80ex'>We show strong (and surprisingly simple) lower bounds for weakly learning
intersections of halfspaces in the improper setting. Strikingly little is known
about this problem. For instance, it is not even known if there is a
polynomial-time algorithm for learning the intersection of only two halfspaces.
On the other hand, lower bounds based on well-established assumptions (such as
approximating worst-case lattice problems or variants of Feige's 3SAT
hypothesis) are only known (or are implied by existing results) for the
intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With
intersections of fewer halfspaces being only ruled out under less standard
assumptions [DV21] (such as the existence of local pseudo-random generators
with large stretch). We significantly narrow this gap by showing that even
learning $\omega(\log \log N)$ halfspaces in dimension $N$ takes
super-polynomial time under standard assumptions on worst-case lattice problems
(namely that SVP and SIVP are hard to approximate within polynomial factors).
Further, we give unconditional hardness results in the statistical query
framework. Specifically, we show that for any $k$ (even constant), learning $k$
halfspaces in dimension $N$ requires accuracy $N^{-\Omega(k)}$, or
exponentially many queries -- in particular ruling out SQ algorithms with
polynomial accuracy for $\omega(1)$ halfspaces. To the best of our knowledge
this is the first unconditional hardness result for learning a super-constant
number of halfspaces.
  Our lower bounds are obtained in a unified way via a novel connection we make
between intersections of halfspaces and the so-called parallel pancakes
distribution [DKS17,BLPR19,BRST21] that has been at the heart of many lower
bound constructions in (robust) high-dimensional statistics in the past few
years.</div><div><a href='http://arxiv.org/abs/2402.15995v1'>2402.15995v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02873v1")'>A Note on High-Probability Analysis of Algorithms with Exponential,
  Sub-Gaussian, and General Light Tails</div>
<div id='2403.02873v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:38:20Z</div><div>Authors: Amit Attia, Tomer Koren</div><div style='padding-top: 10px; width: 80ex'>This short note describes a simple technique for analyzing probabilistic
algorithms that rely on a light-tailed (but not necessarily bounded) source of
randomization. We show that the analysis of such an algorithm can be reduced,
in a black-box manner and with only a small loss in logarithmic factors, to an
analysis of a simpler variant of the same algorithm that uses bounded random
variables and often easier to analyze. This approach simultaneously applies to
any light-tailed randomization, including exponential, sub-Gaussian, and more
general fast-decaying distributions, without needing to appeal to specialized
concentration inequalities. Analyses of a generalized Azuma inequality and
stochastic optimization with general light-tailed noise are provided to
illustrate the technique.</div><div><a href='http://arxiv.org/abs/2403.02873v1'>2403.02873v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15053v1")'>Nonlinear Bayesian optimal experimental design using logarithmic Sobolev
  inequalities</div>
<div id='2402.15053v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T02:14:44Z</div><div>Authors: Fengyi Li, Ayoub Belhadji, Youssef Marzouk</div><div style='padding-top: 10px; width: 80ex'>We study the problem of selecting $k$ experiments from a larger candidate
pool, where the goal is to maximize mutual information (MI) between the
selected subset and the underlying parameters. Finding the exact solution is to
this combinatorial optimization problem is computationally costly, not only due
to the complexity of the combinatorial search but also the difficulty of
evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches
based on new computationally inexpensive lower bounds for MI, constructed via
log-Sobolev inequalities. We demonstrate that our method outperforms random
selection strategies, Gaussian approximations, and nested Monte Carlo (NMC)
estimators of MI in various settings, including optimal design for nonlinear
models with non-additive noise.</div><div><a href='http://arxiv.org/abs/2402.15053v1'>2402.15053v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18337v1")'>Probabilistic Bayesian optimal experimental design using conditional
  normalizing flows</div>
<div id='2402.18337v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T13:59:20Z</div><div>Authors: Rafael Orozco, Felix J. Herrmann, Peng Chen</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimal experimental design (OED) seeks to conduct the most
informative experiment under budget constraints to update the prior knowledge
of a system to its posterior from the experimental data in a Bayesian
framework. Such problems are computationally challenging because of (1)
expensive and repeated evaluation of some optimality criterion that typically
involves a double integration with respect to both the system parameters and
the experimental data, (2) suffering from the curse-of-dimensionality when the
system parameters and design variables are high-dimensional, (3) the
optimization is combinatorial and highly non-convex if the design variables are
binary, often leading to non-robust designs. To make the solution of the
Bayesian OED problem efficient, scalable, and robust for practical
applications, we propose a novel joint optimization approach. This approach
performs simultaneous (1) training of a scalable conditional normalizing flow
(CNF) to efficiently maximize the expected information gain (EIG) of a jointly
learned experimental design (2) optimization of a probabilistic formulation of
the binary experimental design with a Bernoulli distribution. We demonstrate
the performance of our proposed method for a practical MRI data acquisition
problem, one of the most challenging Bayesian OED problems that has
high-dimensional (320 $\times$ 320) parameters at high image resolution,
high-dimensional (640 $\times$ 386) observations, and binary mask designs to
select the most informative observations.</div><div><a href='http://arxiv.org/abs/2402.18337v1'>2402.18337v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11782v1")'>A tutorial on learning from preferences and choices with Gaussian
  Processes</div>
<div id='2403.11782v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:40:48Z</div><div>Authors: Alessio Benavoli, Dario Azzimonti</div><div style='padding-top: 10px; width: 80ex'>Preference modelling lies at the intersection of economics, decision theory,
machine learning and statistics. By understanding individuals' preferences and
how they make choices, we can build products that closely match their
expectations, paving the way for more efficient and personalised applications
across a wide range of domains. The objective of this tutorial is to present a
cohesive and comprehensive framework for preference learning with Gaussian
Processes (GPs), demonstrating how to seamlessly incorporate rationality
principles (from economics and decision theory) into the learning process. By
suitably tailoring the likelihood function, this framework enables the
construction of preference learning models that encompass random utility
models, limits of discernment, and scenarios with multiple conflicting
utilities for both object- and label-preference. This tutorial builds upon
established research while simultaneously introducing some novel GP-based
models to address specific gaps in the existing literature.</div><div><a href='http://arxiv.org/abs/2403.11782v1'>2403.11782v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01306v1")'>KTO: Model Alignment as Prospect Theoretic Optimization</div>
<div id='2402.01306v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:53:36Z</div><div>Authors: Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela</div><div style='padding-top: 10px; width: 80ex'>Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive
random variables in a biased but well-defined manner; for example, humans are
famously loss-averse. We show that objectives for aligning LLMs with human
feedback implicitly incorporate many of these biases -- the success of these
objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed
to them being $\textit{human-aware loss functions}$ (HALOs). However, the
utility functions these methods attribute to humans still differ from those in
the prospect theory literature. Using a Kahneman-Tversky model of human
utility, we propose a HALO that directly maximizes the utility of generations
instead of maximizing the log-likelihood of preferences, as current methods do.
We call this approach Kahneman-Tversky Optimization (KTO), and it matches or
exceeds the performance of preference-based methods at scales from 1B to 30B.
Crucially, KTO does not need preferences -- only a binary signal of whether an
output is desirable or undesirable for a given input. This makes it far easier
to use in the real world, where preference data is scarce and expensive.</div><div><a href='http://arxiv.org/abs/2402.01306v1'>2402.01306v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11016v1")'>Bounding Consideration Probabilities in Consider-Then-Choose Ranking
  Models</div>
<div id='2401.11016v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T20:27:29Z</div><div>Authors: Ben Aoki-Sherwood, Catherine Bregou, David Liben-Nowell, Kiran Tomlinson, Thomas Zeng</div><div style='padding-top: 10px; width: 80ex'>A common theory of choice posits that individuals make choices in a two-step
process, first selecting some subset of the alternatives to consider before
making a selection from the resulting consideration set. However, inferring
unobserved consideration sets (or item consideration probabilities) in this
"consider then choose" setting poses significant challenges, because even
simple models of consideration with strong independence assumptions are not
identifiable, even if item utilities are known. We consider a natural extension
of consider-then-choose models to a top-$k$ ranking setting, where we assume
rankings are constructed according to a Plackett-Luce model after sampling a
consideration set. While item consideration probabilities remain non-identified
in this setting, we prove that knowledge of item utilities allows us to infer
bounds on the relative sizes of consideration probabilities. Additionally,
given a condition on the expected consideration set size, we derive absolute
upper and lower bounds on item consideration probabilities. We also provide
algorithms to tighten those bounds on consideration probabilities by
propagating inferred constraints. Thus, we show that we can learn useful
information about consideration probabilities despite not being able to
identify them precisely. We demonstrate our methods on a ranking dataset from a
psychology experiment with two different ranking tasks (one with fixed
consideration sets and one with unknown consideration sets). This combination
of data allows us to estimate utilities and then learn about unknown
consideration probabilities using our bounds.</div><div><a href='http://arxiv.org/abs/2401.11016v1'>2401.11016v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02290v1")'>Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package
  in R and Python</div>
<div id='2402.02290v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T23:04:32Z</div><div>Authors: Giovanni Saraceno, Marianthi Markatou, Raktim Mukhopadhyay, Mojgan Golzy</div><div style='padding-top: 10px; width: 80ex'>We introduce the QuadratiK package that incorporates innovative data analysis
methodologies. The presented software, implemented in both R and Python, offers
a comprehensive set of goodness-of-fit tests and clustering techniques using
kernel-based quadratic distances, thereby bridging the gap between the
statistical and machine learning literatures. Our software implements one, two
and k-sample tests for goodness of fit, providing an efficient and
mathematically sound way to assess the fit of probability distributions.
Expanded capabilities of our software include supporting tests for uniformity
on the $d$-dimensional Sphere based on Poisson kernel densities, and algorithms
for generating random samples from Poisson kernel densities. Particularly
noteworthy is the incorporation of a unique clustering algorithm specifically
tailored for spherical data that leverages a mixture of Poisson-kernel-based
densities on the sphere. Alongside this, our software includes additional
graphical functions, aiding the users in validating, as well as visualizing and
representing clustering results. This enhances interpretability and usability
of the analysis. In summary, our R and Python packages serve as a powerful
suite of tools, offering researchers and practitioners the means to delve
deeper into their data, draw robust inference, and conduct potentially
impactful analyses and inference across a wide array of disciplines.</div><div><a href='http://arxiv.org/abs/2402.02290v1'>2402.02290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16708v2")'>Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible
  Cluster Shapes</div>
<div id='2401.16708v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T03:12:19Z</div><div>Authors: Yung-Peng Hsu, Hung-Hsuan Chen</div><div style='padding-top: 10px; width: 80ex'>This paper introduces the multivariate beta mixture model (MBMM), a new
probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes
because of the flexible probability density function of the multivariate beta
distribution. We introduce the properties of MBMM, describe the parameter
learning procedure, and present the experimental results, showing that MBMM
fits diverse cluster shapes on synthetic and real datasets. The code is
released anonymously at https://github.com/hhchen1105/mbmm/.</div><div><a href='http://arxiv.org/abs/2401.16708v2'>2401.16708v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10566v1")'>Robust Multi-Modal Density Estimation</div>
<div id='2401.10566v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T09:10:58Z</div><div>Authors: Anna Mészáros, Julian F. Schumann, Javier Alonso-Mora, Arkady Zgonnikov, Jens Kober</div><div style='padding-top: 10px; width: 80ex'>Development of multi-modal, probabilistic prediction models has lead to a
need for comprehensive evaluation metrics. While several metrics can
characterize the accuracy of machine-learned models (e.g., negative
log-likelihood, Jensen-Shannon divergence), these metrics typically operate on
probability densities. Applying them to purely sample-based prediction models
thus requires that the underlying density function is estimated. However,
common methods such as kernel density estimation (KDE) have been demonstrated
to lack robustness, while more complex methods have not been evaluated in
multi-modal estimation problems. In this paper, we present ROME (RObust
Multi-modal density Estimator), a non-parametric approach for density
estimation which addresses the challenge of estimating multi-modal, non-normal,
and highly correlated distributions. ROME utilizes clustering to segment a
multi-modal set of samples into multiple uni-modal ones and then combines
simple KDE estimates obtained for individual clusters in a single multi-modal
estimate. We compared our approach to state-of-the-art methods for density
estimation as well as ablations of ROME, showing that it not only outperforms
established methods but is also more robust to a variety of distributions. Our
results demonstrate that ROME can overcome the issues of over-fitting and
over-smoothing exhibited by other estimators, promising a more robust
evaluation of probabilistic machine learning models.</div><div><a href='http://arxiv.org/abs/2401.10566v1'>2401.10566v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00773v2")'>Unsupervised Outlier Detection using Random Subspace and Subsampling
  Ensembles of Dirichlet Process Mixtures</div>
<div id='2401.00773v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T14:34:11Z</div><div>Authors: Dongwook Kim, Juyeon Park, Hee Cheol Chung, Seonghyun Jeong</div><div style='padding-top: 10px; width: 80ex'>Probabilistic mixture models are acknowledged as a valuable tool for
unsupervised outlier detection owing to their interpretability and intuitive
grounding in statistical principles. Within this framework, Dirichlet process
mixture models emerge as a compelling alternative to conventional finite
mixture models for both clustering and outlier detection tasks. However,
despite their evident advantages, the widespread adoption of Dirichlet process
mixture models in unsupervised outlier detection has been hampered by
challenges related to computational inefficiency and sensitivity to outliers
during the construction of detectors. To tackle these challenges, we propose a
novel outlier detection method based on ensembles of Dirichlet process Gaussian
mixtures. The proposed method is a fully unsupervised algorithm that
capitalizes on random subspace and subsampling ensembles, not only ensuring
efficient computation but also enhancing the robustness of the resulting
outlier detector. Moreover, the proposed method leverages variational inference
for Dirichlet process mixtures to ensure efficient and fast computation.
Empirical studies with benchmark datasets demonstrate that our method
outperforms existing approaches for unsupervised outlier detection.</div><div><a href='http://arxiv.org/abs/2401.00773v2'>2401.00773v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08777v1")'>Robust Anomaly Detection for Particle Physics Using Multi-Background
  Representation Learning</div>
<div id='2401.08777v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T19:00:20Z</div><div>Authors: Abhijith Gandrakota, Lily Zhang, Aahlad Puli, Kyle Cranmer, Jennifer Ngadiuba, Rajesh Ranganath, Nhan Tran</div><div style='padding-top: 10px; width: 80ex'>Anomaly, or out-of-distribution, detection is a promising tool for aiding
discoveries of new particles or processes in particle physics. In this work, we
identify and address two overlooked opportunities to improve anomaly detection
for high-energy physics. First, rather than train a generative model on the
single most dominant background process, we build detection algorithms using
representation learning from multiple background types, thus taking advantage
of more information to improve estimation of what is relevant for detection.
Second, we generalize decorrelation to the multi-background setting, thus
directly enforcing a more complete definition of robustness for anomaly
detection. We demonstrate the benefit of the proposed robust multi-background
anomaly detection algorithms on a high-dimensional dataset of particle decays
at the Large Hadron Collider.</div><div><a href='http://arxiv.org/abs/2401.08777v1'>2401.08777v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12808v1")'>Learning Generalization and Regularization of Nonhomogeneous Temporal
  Poisson Processes</div>
<div id='2402.12808v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T08:27:50Z</div><div>Authors: Son Nguyen Van, Hoai Nguyen Xuan</div><div style='padding-top: 10px; width: 80ex'>The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is
an essentially important counting process with numerous real-world
applications. Up to date, almost all works in the literature have been on the
estimation of NHPPs with infinite data using non-data driven binning methods.
In this paper, we formulate the problem of estimation of NHPPs from finite and
limited data as a learning generalization problem. We mathematically show that
while binning methods are essential for the estimation of NHPPs, they pose a
threat of overfitting when the amount of data is limited. We propose a
framework for regularized learning of NHPPs with two new adaptive and
data-driven binning methods that help to remove the ad-hoc tuning of binning
parameters. Our methods are experimentally tested on synthetic and real-world
datasets and the results show their effectiveness.</div><div><a href='http://arxiv.org/abs/2402.12808v1'>2402.12808v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02162v2")'>A Bayesian cluster validity index</div>
<div id='2402.02162v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T14:23:36Z</div><div>Authors: Nathakhun Wiroonsri, Onthada Preedasawakul</div><div style='padding-top: 10px; width: 80ex'>Selecting the appropriate number of clusters is a critical step in applying
clustering algorithms. To assist in this process, various cluster validity
indices (CVIs) have been developed. These indices are designed to identify the
optimal number of clusters within a dataset. However, users may not always seek
the absolute optimal number of clusters but rather a secondary option that
better aligns with their specific applications. This realization has led us to
introduce a Bayesian cluster validity index (BCVI), which builds upon existing
indices. The BCVI utilizes either Dirichlet or generalized Dirichlet priors,
resulting in the same posterior distribution. We evaluate our BCVI using the
Wiroonsri index for hard clustering and the Wiroonsri-Preedasawakul index for
soft clustering as underlying indices. We compare the performance of our
proposed BCVI with that of the original underlying indices and several other
existing CVIs, including Davies-Bouldin, Starczewski, Xie-Beni, and KWON2
indices. Our BCVI offers clear advantages in situations where user expertise is
valuable, allowing users to specify their desired range for the final number of
clusters. To illustrate this, we conduct experiments classified into three
different scenarios. Additionally, we showcase the practical applicability of
our approach through real-world datasets, such as MRI brain tumor images. These
tools will be published as a new R package 'BayesCVI'.</div><div><a href='http://arxiv.org/abs/2402.02162v2'>2402.02162v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07960v1")'>Unsupervised self-organising map of prostate cell Raman spectra shows
  disease-state subclustering</div>
<div id='2403.07960v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T09:37:20Z</div><div>Authors: Daniel West, Susan Stepney, Y. Hancock</div><div style='padding-top: 10px; width: 80ex'>Prostate cancer is a disease which poses an interesting clinical question:
should it be treated? A small subset of prostate cancers are aggressive and
require removal and treatment to prevent metastatic spread. However,
conventional diagnostics remain challenged to risk-stratify such patients,
hence, new methods of approach to biomolecularly subclassify the disease are
needed. Here we use an unsupervised, self-organising map approach to analyse
live-cell Raman spectroscopy data obtained from prostate cell-lines; our aim is
to test the feasibility of this method to differentiate, at the
single-cell-level, cancer from normal using high-dimensional datasets with
minimal preprocessing. The results demonstrate not only successful separation
of normal prostate and cancer cells, but also a new subclustering of the
prostate cancer cell-line into two groups. Initial analysis of the spectra from
each of the cancer subclusters demonstrates a differential expression of
lipids, which, against the normal control, may be linked to disease-related
changes in cellular signalling.</div><div><a href='http://arxiv.org/abs/2403.07960v1'>2403.07960v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12158v1")'>Variational Approach for Efficient KL Divergence Estimation in Dirichlet
  Mixture Models</div>
<div id='2403.12158v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T18:14:54Z</div><div>Authors: Samyajoy Pal, Christian Heumann</div><div style='padding-top: 10px; width: 80ex'>This study tackles the efficient estimation of Kullback-Leibler (KL)
Divergence in Dirichlet Mixture Models (DMM), crucial for clustering
compositional data. Despite the significance of DMMs, obtaining an analytically
tractable solution for KL Divergence has proven elusive. Past approaches relied
on computationally demanding Monte Carlo methods, motivating our introduction
of a novel variational approach. Our method offers a closed-form solution,
significantly enhancing computational efficiency for swift model comparisons
and robust estimation evaluations. Validation using real and simulated data
showcases its superior efficiency and accuracy over traditional Monte
Carlo-based methods, opening new avenues for rapid exploration of diverse DMM
models and advancing statistical analyses of compositional data.</div><div><a href='http://arxiv.org/abs/2403.12158v1'>2403.12158v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09215v1")'>On the Laplace Approximation as Model Selection Criterion for Gaussian
  Processes</div>
<div id='2403.09215v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T09:28:28Z</div><div>Authors: Andreas Besginow, Jan David Hüwel, Thomas Pawellek, Christian Beecks, Markus Lange-Hegermann</div><div style='padding-top: 10px; width: 80ex'>Model selection aims to find the best model in terms of accuracy,
interpretability or simplicity, preferably all at once. In this work, we focus
on evaluating model performance of Gaussian process models, i.e. finding a
metric that provides the best trade-off between all those criteria. While
previous work considers metrics like the likelihood, AIC or dynamic nested
sampling, they either lack performance or have significant runtime issues,
which severely limits applicability. We address these challenges by introducing
multiple metrics based on the Laplace approximation, where we overcome a severe
inconsistency occuring during naive application of the Laplace approximation.
Experiments show that our metrics are comparable in quality to the gold
standard dynamic nested sampling without compromising for computational speed.
Our model selection criteria allow significantly faster and high quality model
selection of Gaussian process models.</div><div><a href='http://arxiv.org/abs/2403.09215v1'>2403.09215v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11552v1")'>Empirical Density Estimation based on Spline Quasi-Interpolation with
  applications to Copulas clustering modeling</div>
<div id='2402.11552v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T11:49:38Z</div><div>Authors: Cristiano Tamborrino, Antonella Falini, Francesca Mazzia</div><div style='padding-top: 10px; width: 80ex'>Density estimation is a fundamental technique employed in various fields to
model and to understand the underlying distribution of data. The primary
objective of density estimation is to estimate the probability density function
of a random variable. This process is particularly valuable when dealing with
univariate or multivariate data and is essential for tasks such as clustering,
anomaly detection, and generative modeling. In this paper we propose the
mono-variate approximation of the density using spline quasi interpolation and
we applied it in the context of clustering modeling. The clustering technique
used is based on the construction of suitable multivariate distributions which
rely on the estimation of the monovariate empirical densities (marginals). Such
an approximation is achieved by using the proposed spline quasi-interpolation,
while the joint distributions to model the sought clustering partition is
constructed with the use of copulas functions. In particular, since copulas can
capture the dependence between the features of the data independently from the
marginal distributions, a finite mixture copula model is proposed. The
presented algorithm is validated on artificial and real datasets.</div><div><a href='http://arxiv.org/abs/2402.11552v1'>2402.11552v1</a></div>
</div></div>
    <div><a href="arxiv_2.html">Prev (2)</a></div>
    <div><a href="arxiv_4.html">Next (4)</a></div>
    