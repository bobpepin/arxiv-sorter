
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08712v1")'>Survival Analysis of Young Triple-Negative Breast Cancer Patients</div>
<div id='2401.08712v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T17:51:14Z</div><div>Authors: M. Mehdi Owrang O, Fariba Jafari Horestani, Ginger Schwarz</div><div style='padding-top: 10px; width: 80ex'>Breast cancer prognosis is crucial for effective treatment, with the disease
more common in women over 40 years old but rare under 40 years old, where less
than 5 percent of cases occur in the U.S. Studies indicate a worse prognosis in
younger women, which varies by ethnicity. Breast cancers are classified based
on receptors like estrogen, progesterone, and HER2. Triple-negative breast
cancer (TNBC), lacking these receptors, accounts for about 15 percent of cases
and is more prevalent in younger patients, often resulting in poorer outcomes.
Nevertheless, the impact of age on TNBC prognosis remains unclear. Factors like
age, race, tumor grade, size, and lymph node status are studied for their role
in TNBC's clinical outcomes, but current research is inconclusive about
age-related differences. This study uses SEER data set to examine the influence
of younger age on survivability in TNBC patients, aiming to determine if age is
a significant prognostic factor. Our experimental results on SEER dataset
confirm the existing research reports that TNBC patients have worse prognosis
compared to non-TNBC based on age. Our main goal was to investigate whether
younger age has any significance on the survivability of TNBC patients.
Experimental results do not show that younger age has any significance on the
prognosis and survival rate of the TNBC patients</div><div><a href='http://arxiv.org/abs/2401.08712v1'>2401.08712v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06406v1")'>Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis:
  A review</div>
<div id='2401.06406v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T07:01:36Z</div><div>Authors: Lingchao Mao, Hairong Wang, Leland S. Hu, Nhan L Tran, Peter D Canoll, Kristin R Swanson, Jing Li</div><div style='padding-top: 10px; width: 80ex'>Cancer remains one of the most challenging diseases to treat in the medical
field. Machine learning has enabled in-depth analysis of rich multi-omics
profiles and medical imaging for cancer diagnosis and prognosis. Despite these
advancements, machine learning models face challenges stemming from limited
labeled sample sizes, the intricate interplay of high-dimensionality data
types, the inherent heterogeneity observed among patients and within tumors,
and concerns about interpretability and consistency with existing biomedical
knowledge. One approach to surmount these challenges is to integrate biomedical
knowledge into data-driven models, which has proven potential to improve the
accuracy, robustness, and interpretability of model results. Here, we review
the state-of-the-art machine learning studies that adopted the fusion of
biomedical knowledge and data, termed knowledge-informed machine learning, for
cancer diagnosis and prognosis. Emphasizing the properties inherent in four
primary data types including clinical, imaging, molecular, and treatment data,
we highlight modeling considerations relevant to these contexts. We provide an
overview of diverse forms of knowledge representation and current strategies of
knowledge integration into machine learning pipelines with concrete examples.
We conclude the review article by discussing future directions to advance
cancer research through knowledge-informed machine learning.</div><div><a href='http://arxiv.org/abs/2401.06406v1'>2401.06406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07937v1")'>Integrate Any Omics: Towards genome-wide data integration for patient
  stratification</div>
<div id='2401.07937v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T19:57:07Z</div><div>Authors: Shihao Ma, Andy G. X. Zeng, Benjamin Haibe-Kains, Anna Goldenberg, John E Dick, Bo Wang</div><div style='padding-top: 10px; width: 80ex'>High-throughput omics profiling advancements have greatly enhanced cancer
patient stratification. However, incomplete data in multi-omics integration
presents a significant challenge, as traditional methods like sample exclusion
or imputation often compromise biological diversity and dependencies.
Furthermore, the critical task of accurately classifying new patients with
partial omics data into existing subtypes is commonly overlooked. To address
these issues, we introduce IntegrAO (Integrate Any Omics), an unsupervised
framework for integrating incomplete multi-omics data and classifying new
samples. IntegrAO first combines partially overlapping patient graphs from
diverse omics sources and utilizes graph neural networks to produce unified
patient embeddings. Our systematic evaluation across five cancer cohorts
involving six omics modalities demonstrates IntegrAO's robustness to missing
data and its accuracy in classifying new samples with partial profiles. An
acute myeloid leukemia case study further validates its capability to uncover
biological and clinical heterogeneity in incomplete datasets. IntegrAO's
ability to handle heterogeneous and incomplete data makes it an essential tool
for precision oncology, offering a holistic approach to patient
characterization.</div><div><a href='http://arxiv.org/abs/2401.07937v1'>2401.07937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10844v1")'>Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer
  Subtype Diagnosis</div>
<div id='2401.10844v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:54:58Z</div><div>Authors: Charles Theodore Kent, Leila Bagheriye, Johan Kwisthout</div><div style='padding-top: 10px; width: 80ex'>Recent strides in the field of neural computation has seen the adoption of
Winner Take All (WTA) circuits to facilitate the unification of hierarchical
Bayesian inference and spiking neural networks as a neurobiologically plausible
model of information processing. Current research commonly validates the
performance of these networks via classification tasks, particularly of the
MNIST dataset. However, researchers have not yet reached consensus about how
best to translate the stochastic responses from these networks into discrete
decisions, a process known as population decoding. Despite being an often
underexamined part of SNNs, in this work we show that population decoding has a
significanct impact on the classification performance of WTA networks. For this
purpose, we apply a WTA network to the problem of cancer subtype diagnosis from
multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing
so we utilise a novel implementation of gene similarity networks, a feature
encoding technique based on Kohoens self organising map algorithm. We further
show that the impact of selecting certain population decoding methods is
amplified when facing imbalanced datasets.</div><div><a href='http://arxiv.org/abs/2401.10844v1'>2401.10844v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06823v1")'>Interpretable deep learning in single-cell omics</div>
<div id='2401.06823v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T23:59:37Z</div><div>Authors: Manoj M Wagle, Siqu Long, Carissa Chen, Chunlei Liu, Pengyi Yang</div><div style='padding-top: 10px; width: 80ex'>Recent developments in single-cell omics technologies have enabled the
quantification of molecular profiles in individual cells at an unparalleled
resolution. Deep learning, a rapidly evolving sub-field of machine learning,
has instilled a significant interest in single-cell omics research due to its
remarkable success in analysing heterogeneous high-dimensional single-cell
omics data. Nevertheless, the inherent multi-layer nonlinear architecture of
deep learning models often makes them `black boxes' as the reasoning behind
predictions is often unknown and not transparent to the user. This has
stimulated an increasing body of research for addressing the lack of
interpretability in deep learning models, especially in single-cell omics data
analyses, where the identification and understanding of molecular regulators
are crucial for interpreting model predictions and directing downstream
experimental validations. In this work, we introduce the basics of single-cell
omics technologies and the concept of interpretable deep learning. This is
followed by a review of the recent interpretable deep learning models applied
to various single-cell omics research. Lastly, we highlight the current
limitations and discuss potential future directions. We anticipate this review
to bring together the single-cell and machine learning research communities to
foster future development and application of interpretable deep learning in
single-cell omics research.</div><div><a href='http://arxiv.org/abs/2401.06823v1'>2401.06823v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03231v1")'>Machine and deep learning methods for predicting 3D genome organization</div>
<div id='2403.03231v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:04:41Z</div><div>Authors: Brydon P. G. Wall, My Nguyen, J. Chuck Harrell, Mikhail G. Dozmorov</div><div style='padding-top: 10px; width: 80ex'>Three-Dimensional (3D) chromatin interactions, such as enhancer-promoter
interactions (EPIs), loops, Topologically Associating Domains (TADs), and A/B
compartments play critical roles in a wide range of cellular processes by
regulating gene expression. Recent development of chromatin conformation
capture technologies has enabled genome-wide profiling of various 3D
structures, even with single cells. However, current catalogs of 3D structures
remain incomplete and unreliable due to differences in technology, tools, and
low data resolution. Machine learning methods have emerged as an alternative to
obtain missing 3D interactions and/or improve resolution. Such methods
frequently use genome annotation data (ChIP-seq, DNAse-seq, etc.), DNA
sequencing information (k-mers, Transcription Factor Binding Site (TFBS)
motifs), and other genomic properties to learn the associations between genomic
features and chromatin interactions. In this review, we discuss computational
tools for predicting three types of 3D interactions (EPIs, chromatin
interactions, TAD boundaries) and analyze their pros and cons. We also point
out obstacles of computational prediction of 3D interactions and suggest future
research directions.</div><div><a href='http://arxiv.org/abs/2403.03231v1'>2403.03231v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02460v2")'>Review of multimodal machine learning approaches in healthcare</div>
<div id='2402.02460v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T12:21:38Z</div><div>Authors: Felix Krones, Umar Marikkar, Guy Parsons, Adam Szmul, Adam Mahdi</div><div style='padding-top: 10px; width: 80ex'>Machine learning methods in healthcare have traditionally focused on using
data from a single modality, limiting their ability to effectively replicate
the clinical practice of integrating multiple sources of information for
improved decision making. Clinicians typically rely on a variety of data
sources including patients' demographic information, laboratory data, vital
signs and various imaging data modalities to make informed decisions and
contextualise their findings. Recent advances in machine learning have
facilitated the more efficient incorporation of multimodal data, resulting in
applications that better represent the clinician's approach. Here, we provide a
review of multimodal machine learning approaches in healthcare, offering a
comprehensive overview of recent literature. We discuss the various data
modalities used in clinical diagnosis, with a particular emphasis on imaging
data. We evaluate fusion techniques, explore existing multimodal datasets and
examine common training strategies.</div><div><a href='http://arxiv.org/abs/2402.02460v2'>2402.02460v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13843v1")'>Machine Learning and Vision Transformers for Thyroid Carcinoma
  Diagnosis: A review</div>
<div id='2403.13843v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T17:45:04Z</div><div>Authors: Yassine Habchi, Hamza Kheddar, Yassine Himeur, Abdelkrim Boukabou, Ammar Chouchane, Abdelmalik Ouamane, Shadi Atalla, Wathiq Mansoor</div><div style='padding-top: 10px; width: 80ex'>The growing interest in developing smart diagnostic systems to help medical
experts process extensive data for treating incurable diseases has been
notable. In particular, the challenge of identifying thyroid cancer (TC) has
seen progress with the use of machine learning (ML) and big data analysis,
incorporating transformers to evaluate TC prognosis and determine the risk of
malignancy in individuals. This review article presents a summary of various
studies on AIbased approaches, especially those employing transformers, for
diagnosing TC. It introduces a new categorization system for these methods
based on artifcial intelligence (AI) algorithms, the goals of the framework,
and the computing environments used. Additionally, it scrutinizes and contrasts
the available TC datasets by their features. The paper highlights the
importance of AI instruments in aiding the diagnosis and treatment of TC
through supervised, unsupervised, or mixed approaches, with a special focus on
the ongoing importance of transformers in medical diagnostics and disease
management. It further discusses the progress made and the continuing obstacles
in this area. Lastly, it explores future directions and focuses within this
research feld.</div><div><a href='http://arxiv.org/abs/2403.13843v1'>2403.13843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10586v1")'>From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive
  Bladder Cancer Recurrence Prediction</div>
<div id='2403.10586v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:03:45Z</div><div>Authors: Saram Abbas, Dr Rishad Shafik, Prof Naeem Soomro, Prof Rakesh Heer, Dr Kabita Adhikari</div><div style='padding-top: 10px; width: 80ex'>Bladder cancer, the leading urinary tract cancer, is responsible for 15
deaths daily in the UK. This cancer predominantly manifests as
non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet
penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very
high recurrence rate of 70-80% and hence the costliest treatments. Current
tools for predicting recurrence use scoring systems that overestimate risk and
have poor accuracy. Inaccurate and delayed prediction of recurrence
significantly elevates the likelihood of mortality. Accurate prediction of
recurrence is hence vital for cost-effective management and treatment planning.
This is where Machine learning (ML) techniques have emerged as a promising
approach for predicting NMIBC recurrence by leveraging molecular and clinical
data. This review provides a comprehensive analysis of ML approaches for
predicting NMIBC recurrence. Our systematic evaluation demonstrates the
potential of diverse ML algorithms and markers, including radiomic, clinical,
histopathological, genomic, and biochemical data in enhancing recurrence
prediction and personalised patient management. We summarise various prediction
tasks, data modalities, and ML models used, highlighting their performance,
limitations, and future directions of incorporating cost-effectiveness.
Challenges related to generalisability and interpretability of artificial
intelligent models are discussed, emphasising the need for collaborative
efforts and robust datasets.</div><div><a href='http://arxiv.org/abs/2403.10586v1'>2403.10586v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04558v2")'>Reducing self-supervised learning complexity improves weakly-supervised
  classification performance in computational pathology</div>
<div id='2403.04558v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:56:06Z</div><div>Authors: Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather</div><div style='padding-top: 10px; width: 80ex'>Deep Learning models have been successfully utilized to extract clinically
actionable insights from routinely available histology data. Generally, these
models require annotations performed by clinicians, which are scarce and costly
to generate. The emergence of self-supervised learning (SSL) methods remove
this barrier, allowing for large-scale analyses on non-annotated data. However,
recent SSL approaches apply increasingly expansive model architectures and
larger datasets, causing the rapid escalation of data volumes, hardware
prerequisites, and overall expenses, limiting access to these resources to few
institutions. Therefore, we investigated the complexity of contrastive SSL in
computational pathology in relation to classification performance with the
utilization of consumer-grade hardware. Specifically, we analyzed the effects
of adaptations in data volume, architecture, and algorithms on downstream
classification tasks, emphasizing their impact on computational resources. We
trained breast cancer foundation models on a large public patient cohort and
validated them on various downstream classification tasks in a weakly
supervised manner on two external public patient cohorts. Our experiments
demonstrate that we can improve downstream classification performance whilst
reducing SSL training duration by 90%. In summary, we propose a set of
adaptations which enable the utilization of SSL in computational pathology in
non-resource abundant environments.</div><div><a href='http://arxiv.org/abs/2403.04558v2'>2403.04558v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04962v1")'>C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer
  Grading</div>
<div id='2403.04962v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T00:15:43Z</div><div>Authors: Sudipta Paul, Bulent Yener, Amanda W. Lund</div><div style='padding-top: 10px; width: 80ex'>Graph-based learning approaches, due to their ability to encode tissue/organ
structure information, are increasingly favored for grading colorectal cancer
histology images. Recent graph-based techniques involve dividing whole slide
images (WSIs) into smaller or medium-sized patches, and then building graphs on
each patch for direct use in training. This method, however, fails to capture
the tissue structure information present in an entire WSI and relies on
training from a significantly large dataset of image patches. In this paper, we
propose a novel cell-to-patch graph convolutional network (C2P-GCN), which is a
two-stage graph formation-based approach. In the first stage, it forms a
patch-level graph based on the cell organization on each patch of a WSI. In the
second stage, it forms an image-level graph based on a similarity measure
between patches of a WSI considering each patch as a node of a graph. This
graph representation is then fed into a multi-layer GCN-based classification
network. Our approach, through its dual-phase graph construction, effectively
gathers local structural details from individual patches and establishes a
meaningful connection among all patches across a WSI. As C2P-GCN integrates the
structural data of an entire WSI into a single graph, it allows our model to
work with significantly fewer training data compared to the latest models for
colorectal cancer. Experimental validation of C2P-GCN on two distinct
colorectal cancer datasets demonstrates the effectiveness of our method.</div><div><a href='http://arxiv.org/abs/2403.04962v1'>2403.04962v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06169v1")'>Deep Learning model predicts the c-Kit-11 mutational status of canine
  cutaneous mast cell tumors by HE stained histological slides</div>
<div id='2401.06169v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T12:00:59Z</div><div>Authors: Chloé Puget, Jonathan Ganz, Julian Ostermaier, Thomas Konrad, Eda Parlak, Christof Albert Bertram, Matti Kiupel, Katharina Breininger, Marc Aubreville, Robert Klopfleisch</div><div style='padding-top: 10px; width: 80ex'>Numerous prognostic factors are currently assessed histopathologically in
biopsies of canine mast cell tumors to evaluate clinical behavior. In addition,
PCR analysis of the c-Kit exon 11 mutational status is often performed to
evaluate the potential success of a tyrosine kinase inhibitor therapy. This
project aimed at training deep learning models (DLMs) to identify the c-Kit-11
mutational status of MCTs solely based on morphology without additional
molecular analysis. HE slides of 195 mutated and 173 non-mutated tumors were
stained consecutively in two different laboratories and scanned with three
different slide scanners. This resulted in six different datasets
(stain-scanner variations) of whole slide images. DLMs were trained with single
and mixed datasets and their performances was assessed under scanner and
staining domain shifts. The DLMs correctly classified HE slides according to
their c-Kit 11 mutation status in, on average, 87% of cases for the best-suited
stain-scanner variant. A relevant performance drop could be observed when the
stain-scanner combination of the training and test dataset differed.
Multi-variant datasets improved the average accuracy but did not reach the
maximum accuracy of algorithms trained and tested on the same stain-scanner
variant. In summary, DLM-assisted morphological examination of MCTs can predict
c-Kit-exon 11 mutational status of MCTs with high accuracy. However, the
recognition performance is impeded by a change of scanner or staining protocol.
Larger data sets with higher numbers of scans originating from different
laboratories and scanners may lead to more robust DLMs to identify c-Kit
mutations in HE slides.</div><div><a href='http://arxiv.org/abs/2401.06169v1'>2401.06169v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17317v1")'>How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced
  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation</div>
<div id='2402.17317v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T08:49:30Z</div><div>Authors: André Ferreira, Naida Solak, Jianning Li, Philipp Dammann, Jens Kleesiek, Victor Alves, Jan Egger</div><div style='padding-top: 10px; width: 80ex'>Deep Learning is the state-of-the-art technology for segmenting brain
tumours. However, this requires a lot of high-quality data, which is difficult
to obtain, especially in the medical field. Therefore, our solutions address
this problem by using unconventional mechanisms for data augmentation.
Generative adversarial networks and registration are used to massively increase
the amount of available samples for training three different deep learning
models for brain tumour segmentation, the first task of the BraTS2023
challenge. The first model is the standard nnU-Net, the second is the Swin
UNETR and the third is the winning solution of the BraTS 2021 Challenge. The
entire pipeline is built on the nnU-Net implementation, except for the
generation of the synthetic data. The use of convolutional algorithms and
transformers is able to fill each other's knowledge gaps. Using the new metric,
our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95
14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the
validation set.</div><div><a href='http://arxiv.org/abs/2402.17317v1'>2402.17317v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03173v1")'>UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis</div>
<div id='2401.03173v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T09:28:49Z</div><div>Authors: Tran Cao Minh, Nguyen Kim Quoc, Phan Cong Vinh, Dang Nhu Phu, Vuong Xuan Chi, Ha Minh Tan</div><div style='padding-top: 10px; width: 80ex'>In the field of medical imaging, breast ultrasound has emerged as a crucial
diagnostic tool for early detection of breast cancer. However, the accuracy of
diagnosing the location of the affected area and the extent of the disease
depends on the experience of the physician. In this paper, we propose a novel
model called UGGNet, combining the power of the U-Net and VGG architectures to
enhance the performance of breast ultrasound image analysis. The U-Net
component of the model helps accurately segment the lesions, while the VGG
component utilizes deep convolutional layers to extract features. The fusion of
these two architectures in UGGNet aims to optimize both segmentation and
feature representation, providing a comprehensive solution for accurate
diagnosis in breast ultrasound images. Experimental results have demonstrated
that the UGGNet model achieves a notable accuracy of 78.2% on the "Breast
Ultrasound Images Dataset."</div><div><a href='http://arxiv.org/abs/2401.03173v1'>2401.03173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09672v1")'>COMPRER: A Multimodal Multi-Objective Pretraining Framework for Enhanced
  Medical Image Representation</div>
<div id='2403.09672v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T08:05:58Z</div><div>Authors: Guy Lutsker, Hagai Rossman, Nastya Godiva, Eran Segal</div><div style='padding-top: 10px; width: 80ex'>Substantial advances in multi-modal Artificial Intelligence (AI) facilitate
the combination of diverse medical modalities to achieve holistic health
assessments. We present COMPRER , a novel multi-modal, multi-objective
pretraining framework which enhances medical-image representation, diagnostic
inferences, and prognosis of diseases. COMPRER employs a multi-objective
training framework, where each objective introduces distinct knowledge to the
model. This includes a multimodal loss that consolidates information across
different imaging modalities; A temporal loss that imparts the ability to
discern patterns over time; Medical-measure prediction adds appropriate medical
insights; Lastly, reconstruction loss ensures the integrity of image structure
within the latent space. Despite the concern that multiple objectives could
weaken task performance, our findings show that this combination actually
boosts outcomes on certain tasks. Here, we apply this framework to both fundus
images and carotid ultrasound, and validate our downstream tasks capabilities
by predicting both current and future cardiovascular conditions. COMPRER
achieved higher Area Under the Curve (AUC) scores in evaluating medical
conditions compared to existing models on held-out data. On the
Out-of-distribution (OOD) UK-Biobank dataset COMPRER maintains favorable
performance over well-established models with more parameters, even though
these models were trained on $75\times$ more data than COMPRER. In addition, to
better assess our model's performance in contrastive learning, we introduce a
novel evaluation metric, providing deeper understanding of the effectiveness of
the latent space pairing.</div><div><a href='http://arxiv.org/abs/2403.09672v1'>2403.09672v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06197v1")'>DrFuse: Learning Disentangled Representation for Clinical Multi-Modal
  Fusion with Missing Modality and Modal Inconsistency</div>
<div id='2403.06197v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T12:41:34Z</div><div>Authors: Wenfang Yao, Kejing Yin, William K. Cheung, Jia Liu, Jing Qin</div><div style='padding-top: 10px; width: 80ex'>The combination of electronic health records (EHR) and medical images is
crucial for clinicians in making diagnoses and forecasting prognosis.
Strategically fusing these two data modalities has great potential to improve
the accuracy of machine learning models in clinical prediction tasks. However,
the asynchronous and complementary nature of EHR and medical images presents
unique challenges. Missing modalities due to clinical and administrative
factors are inevitable in practice, and the significance of each data modality
varies depending on the patient and the prediction target, resulting in
inconsistent predictions and suboptimal model performance. To address these
challenges, we propose DrFuse to achieve effective clinical multi-modal fusion.
It tackles the missing modality issue by disentangling the features shared
across modalities and those unique within each modality. Furthermore, we
address the modal inconsistency issue via a disease-wise attention layer that
produces the patient- and disease-wise weighting for each modality to make the
final prediction. We validate the proposed method using real-world large-scale
datasets, MIMIC-IV and MIMIC-CXR. Experimental results show that the proposed
method significantly outperforms the state-of-the-art models. Our
implementation is publicly available at https://github.com/dorothy-yao/drfuse.</div><div><a href='http://arxiv.org/abs/2403.06197v1'>2403.06197v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11252v1")'>Automated Fusion of Multimodal Electronic Health Records for Better
  Medical Predictions</div>
<div id='2401.11252v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T15:14:14Z</div><div>Authors: Suhan Cui, Jiaqi Wang, Yuan Zhong, Han Liu, Ting Wang, Fenglong Ma</div><div style='padding-top: 10px; width: 80ex'>The widespread adoption of Electronic Health Record (EHR) systems in
healthcare institutes has generated vast amounts of medical data, offering
significant opportunities for improving healthcare services through deep
learning techniques. However, the complex and diverse modalities and feature
structures in real-world EHR data pose great challenges for deep learning model
design. To address the multi-modality challenge in EHR data, current approaches
primarily rely on hand-crafted model architectures based on intuition and
empirical experiences, leading to sub-optimal model architectures and limited
performance. Therefore, to automate the process of model design for mining EHR
data, we propose a novel neural architecture search (NAS) framework named
AutoFM, which can automatically search for the optimal model architectures for
encoding diverse input modalities and fusion strategies. We conduct thorough
experiments on real-world multi-modal EHR data and prediction tasks, and the
results demonstrate that our framework not only achieves significant
performance improvement over existing state-of-the-art methods but also
discovers meaningful network architectures effectively.</div><div><a href='http://arxiv.org/abs/2401.11252v1'>2401.11252v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01077v1")'>Recent Advances in Predictive Modeling with Electronic Health Records</div>
<div id='2402.01077v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T00:31:01Z</div><div>Authors: Jiaqi Wang, Junyu Luo, Muchao Ye, Xiaochen Wang, Yuan Zhong, Aofei Chang, Guanjie Huang, Ziyi Yin, Cao Xiao, Jimeng Sun, Fenglong Ma</div><div style='padding-top: 10px; width: 80ex'>The development of electronic health records (EHR) systems has enabled the
collection of a vast amount of digitized patient data. However, utilizing EHR
data for predictive modeling presents several challenges due to its unique
characteristics. With the advancements in machine learning techniques, deep
learning has demonstrated its superiority in various applications, including
healthcare. This survey systematically reviews recent advances in deep
learning-based predictive models using EHR data. Specifically, we begin by
introducing the background of EHR data and providing a mathematical definition
of the predictive modeling task. We then categorize and summarize predictive
deep models from multiple perspectives. Furthermore, we present benchmarks and
toolkits relevant to predictive modeling in healthcare. Finally, we conclude
this survey by discussing open challenges and suggesting promising directions
for future research.</div><div><a href='http://arxiv.org/abs/2402.01077v1'>2402.01077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11425v1")'>Narrative Feature or Structured Feature? A Study of Large Language
  Models to Identify Cancer Patients at Risk of Heart Failure</div>
<div id='2403.11425v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T02:42:01Z</div><div>Authors: Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas J. George, Jiang Bian, Yonghui Wu</div><div style='padding-top: 10px; width: 80ex'>Cancer treatments are known to introduce cardiotoxicity, negatively impacting
outcomes and survivorship. Identifying cancer patients at risk of heart failure
(HF) is critical to improving cancer treatment outcomes and safety. This study
examined machine learning (ML) models to identify cancer patients at risk of HF
using electronic health records (EHRs), including traditional ML, Time-Aware
long short-term memory (T-LSTM), and large language models (LLMs) using novel
narrative features derived from the structured medical codes. We identified a
cancer cohort of 12,806 patients from the University of Florida Health,
diagnosed with lung, breast, and colorectal cancers, among which 1,602
individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the
best F1 scores, outperforming the traditional support vector machines by 39%,
the T-LSTM deep learning model by 7%, and a widely used transformer model,
BERT, by 5.6%. The analysis shows that the proposed narrative features
remarkably increased feature density and improved performance.</div><div><a href='http://arxiv.org/abs/2403.11425v1'>2403.11425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10581v2")'>Large Language Model-informed ECG Dual Attention Network for Heart
  Failure Risk Prediction</div>
<div id='2403.10581v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T13:25:09Z</div><div>Authors: Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta, Vicente Grau</div><div style='padding-top: 10px; width: 80ex'>Heart failure (HF) poses a significant public health challenge, with a rising
global mortality rate. Early detection and prevention of HF could significantly
reduce its impact. We introduce a novel methodology for predicting HF risk
using 12-lead electrocardiograms (ECGs). We present a novel, lightweight
dual-attention ECG network designed to capture complex ECG features essential
for early HF risk prediction, despite the notable imbalance between low and
high-risk groups. This network incorporates a cross-lead attention module and
twelve lead-specific temporal attention modules, focusing on cross-lead
interactions and each lead's local dynamics. To further alleviate model
overfitting, we leverage a large language model (LLM) with a public ECG-Report
dataset for pretraining on an ECG-report alignment task. The network is then
fine-tuned for HF risk prediction using two specific cohorts from the UK
Biobank study, focusing on patients with hypertension (UKB-HYP) and those who
have had a myocardial infarction (UKB-MI).The results reveal that LLM-informed
pre-training substantially enhances HF risk prediction in these cohorts. The
dual-attention design not only improves interpretability but also predictive
accuracy, outperforming existing competitive methods with C-index scores of
0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's
potential in advancing HF risk assessment with clinical complex ECG data.</div><div><a href='http://arxiv.org/abs/2403.10581v2'>2403.10581v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07796v1")'>Fusing Echocardiography Images and Medical Records for Continuous
  Patient Stratification</div>
<div id='2401.07796v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T16:04:46Z</div><div>Authors: Nathan Painchaud, Pierre-Yves Courand, Pierre-Marc Jodoin, Nicolas Duchateau, Olivier Bernard</div><div style='padding-top: 10px; width: 80ex'>Deep learning now enables automatic and robust extraction of cardiac function
descriptors from echocardiographic sequences, such as ejection fraction or
strain. These descriptors provide fine-grained information that physicians
consider, in conjunction with more global variables from the clinical record,
to assess patients' condition. Drawing on novel transformer models applied to
tabular data (e.g., variables from electronic health records), we propose a
method that considers all descriptors extracted from medical records and
echocardiograms to learn the representation of a difficult-to-characterize
cardiovascular pathology, namely hypertension. Our method first projects each
variable into its own representation space using modality-specific approaches.
These standardized representations of multimodal data are then fed to a
transformer encoder, which learns to merge them into a comprehensive
representation of the patient through a pretext task of predicting a clinical
rating. This pretext task is formulated as an ordinal classification to enforce
a pathological continuum in the representation space. We observe the major
trends along this continuum for a cohort of 239 hypertensive patients to
describe, with unprecedented gradation, the effect of hypertension on a number
of cardiac function descriptors. Our analysis shows that i) pretrained weights
from a foundation model allow to reach good performance (83% accuracy) even
with limited data (less than 200 training samples), ii) trends across the
population are reproducible between trainings, and iii) for descriptors whose
interactions with hypertension are well documented, patterns are consistent
with prior physiological knowledge.</div><div><a href='http://arxiv.org/abs/2401.07796v1'>2401.07796v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10164v1")'>CoReEcho: Continuous Representation Learning for 2D+time
  Echocardiography Analysis</div>
<div id='2403.10164v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T10:18:06Z</div><div>Authors: Fadillah Adamsyah Maani, Numan Saeed, Aleksandr Matsun, Mohammad Yaqub</div><div style='padding-top: 10px; width: 80ex'>Deep learning (DL) models have been advancing automatic medical image
analysis on various modalities, including echocardiography, by offering a
comprehensive end-to-end training pipeline. This approach enables DL models to
regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting
in superior performance. However, the end-to-end training pipeline makes the
learned representations less explainable. The representations may also fail to
capture the continuous relation among echocardiogram clips, indicating the
existence of spurious correlations, which can negatively affect the
generalization. To mitigate this issue, we propose CoReEcho, a novel training
framework emphasizing continuous representations tailored for direct EF
regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms
the current state-of-the-art (SOTA) on the largest echocardiography dataset
(EchoNet-Dynamic) with MAE of 3.90 &amp; R2 of 82.44, and 2) provides robust and
generalizable features that transfer more effectively in related downstream
tasks. The code is publicly available at https://github.com/fadamsyah/CoReEcho.</div><div><a href='http://arxiv.org/abs/2403.10164v1'>2403.10164v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.09708v1")'>Institutional-Level Monitoring of Immune Checkpoint Inhibitor IrAEs
  Using a Novel Natural Language Processing Algorithmic Pipeline</div>
<div id='2403.09708v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T19:18:27Z</div><div>Authors: Michael Shapiro, Herut Dor, Anna Gurevich-Shapiro, Tal Etan, Ido Wolf</div><div style='padding-top: 10px; width: 80ex'>Background: Immune checkpoint inhibitors (ICIs) have revolutionized cancer
treatment but can result in severe immune-related adverse events (IrAEs).
Monitoring IrAEs on a large scale is essential for personalized risk profiling
and assisting in treatment decisions.
  Methods: In this study, we conducted an analysis of clinical notes from
patients who received ICIs at the Tel Aviv Sourasky Medical Center. By
employing a Natural Language Processing algorithmic pipeline, we systematically
identified seven common or severe IrAEs. We examined the utilization of
corticosteroids, treatment discontinuation rates following IrAEs, and
constructed survival curves to visualize the occurrence of adverse events
during treatment.
  Results: Our analysis encompassed 108,280 clinical notes associated with
1,635 patients who had undergone ICI therapy. The detected incidence of IrAEs
was consistent with previous reports, exhibiting substantial variation across
different ICIs. Treatment with corticosteroids varied depending on the specific
IrAE, ranging from 17.3% for thyroiditis to 57.4% for myocarditis. Our
algorithm demonstrated high accuracy in identifying IrAEs, as indicated by an
area under the curve (AUC) of 0.89 for each suspected note and F1 scores of
0.87 or higher for five out of the seven IrAEs examined at the patient level.
  Conclusions: This study presents a novel, large-scale monitoring approach
utilizing deep neural networks for IrAEs. Our method provides accurate results,
enhancing understanding of detrimental consequences experienced by ICI-treated
patients. Moreover, it holds potential for monitoring other medications,
enabling comprehensive post-marketing surveillance to identify susceptible
populations and establish personalized drug safety profiles.</div><div><a href='http://arxiv.org/abs/2403.09708v1'>2403.09708v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02996v1")'>An AI-enabled Bias-Free Respiratory Disease Diagnosis Model using Cough
  Audio: A Case Study for COVID-19</div>
<div id='2401.02996v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T13:09:45Z</div><div>Authors: Tabish Saeed, Aneeqa Ijaz, Ismail Sadiq, Haneya N. Qureshi, Ali Rizwan, Ali Imran</div><div style='padding-top: 10px; width: 80ex'>Cough-based diagnosis for Respiratory Diseases (RDs) using Artificial
Intelligence (AI) has attracted considerable attention, yet many existing
studies overlook confounding variables in their predictive models. These
variables can distort the relationship between cough recordings (input data)
and RD status (output variable), leading to biased associations and unrealistic
model performance. To address this gap, we propose the Bias Free Network
(RBFNet), an end to end solution that effectively mitigates the impact of
confounders in the training data distribution. RBFNet ensures accurate and
unbiased RD diagnosis features, emphasizing its relevance by incorporating a
COVID19 dataset in this study. This approach aims to enhance the reliability of
AI based RD diagnosis models by navigating the challenges posed by confounding
variables. A hybrid of a Convolutional Neural Networks (CNN) and Long-Short
Term Memory (LSTM) networks is proposed for the feature encoder module of
RBFNet. An additional bias predictor is incorporated in the classification
scheme to formulate a conditional Generative Adversarial Network (cGAN) which
helps in decorrelating the impact of confounding variables from RD prediction.
The merit of RBFNet is demonstrated by comparing classification performance
with State of The Art (SoTA) Deep Learning (DL) model (CNN LSTM) after training
on different unbalanced COVID-19 data sets, created by using a large scale
proprietary cough data set. RBF-Net proved its robustness against extremely
biased training scenarios by achieving test set accuracies of 84.1%, 84.6%, and
80.5% for the following confounding variables gender, age, and smoking status,
respectively. RBF-Net outperforms the CNN-LSTM model test set accuracies by
5.5%, 7.7%, and 8.2%, respectively</div><div><a href='http://arxiv.org/abs/2401.02996v1'>2401.02996v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02630v1")'>Model-Agnostic Interpretation Framework in Machine Learning: A
  Comparative Study in NBA Sports</div>
<div id='2401.02630v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T04:25:21Z</div><div>Authors: Shun Liu</div><div style='padding-top: 10px; width: 80ex'>The field of machine learning has seen tremendous progress in recent years,
with deep learning models delivering exceptional performance across a range of
tasks. However, these models often come at the cost of interpretability, as
they operate as opaque "black boxes" that obscure the rationale behind their
decisions. This lack of transparency can limit understanding of the models'
underlying principles and impede their deployment in sensitive domains, such as
healthcare or finance. To address this challenge, our research team has
proposed an innovative framework designed to reconcile the trade-off between
model performance and interpretability. Our approach is centered around modular
operations on high-dimensional data, which enable end-to-end processing while
preserving interpretability. By fusing diverse interpretability techniques and
modularized data processing, our framework sheds light on the decision-making
processes of complex models without compromising their performance. We have
extensively tested our framework and validated its superior efficacy in
achieving a harmonious balance between computational efficiency and
interpretability. Our approach addresses a critical need in contemporary
machine learning applications by providing unprecedented insights into the
inner workings of complex models, fostering trust, transparency, and
accountability in their deployment across diverse domains.</div><div><a href='http://arxiv.org/abs/2401.02630v1'>2401.02630v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12005v1")'>Visualization for Trust in Machine Learning Revisited: The State of the
  Field in 2023</div>
<div id='2403.12005v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:42:27Z</div><div>Authors: Angelos Chatzimparmpas, Kostiantyn Kucher, Andreas Kerren</div><div style='padding-top: 10px; width: 80ex'>Visualization for explainable and trustworthy machine learning remains one of
the most important and heavily researched fields within information
visualization and visual analytics with various application domains, such as
medicine, finance, and bioinformatics. After our 2020 state-of-the-art report
comprising 200 techniques, we have persistently collected peer-reviewed
articles describing visualization techniques, categorized them based on the
previously established categorization schema consisting of 119 categories, and
provided the resulting collection of 542 techniques in an online survey
browser. In this survey article, we present the updated findings of new
analyses of this dataset as of fall 2023 and discuss trends, insights, and
eight open challenges for using visualizations in machine learning. Our results
corroborate the rapidly growing trend of visualization techniques for
increasing trust in machine learning models in the past three years, with
visualization found to help improve popular model explainability methods and
check new deep learning architectures, for instance.</div><div><a href='http://arxiv.org/abs/2403.12005v1'>2403.12005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12518v2")'>Gaussian Process Neural Additive Models</div>
<div id='2402.12518v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:29:34Z</div><div>Authors: Wei Zhang, Brian Barr, John Paisley</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks have revolutionized many fields, but their black-box
nature also occasionally prevents their wider adoption in fields such as
healthcare and finance, where interpretable and explainable models are
required. The recent development of Neural Additive Models (NAMs) is a
significant step in the direction of interpretable deep learning for tabular
datasets. In this paper, we propose a new subclass of NAMs that use a
single-layer neural network construction of the Gaussian process via random
Fourier features, which we call Gaussian Process Neural Additive Models
(GP-NAM). GP-NAMs have the advantage of a convex objective function and number
of trainable parameters that grows linearly with feature dimensionality. It
suffers no loss in performance compared to deeper NAM approaches because GPs
are well-suited for learning complex non-parametric univariate functions. We
demonstrate the performance of GP-NAM on several tabular datasets, showing that
it achieves comparable or better performance in both classification and
regression tasks with a large reduction in the number of parameters.</div><div><a href='http://arxiv.org/abs/2402.12518v2'>2402.12518v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00016v1")'>Deep Sensitivity Analysis for Objective-Oriented Combinatorial
  Optimization</div>
<div id='2403.00016v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T02:15:47Z</div><div>Authors: Ganga Gireesan, Nisha Pillai, Michael J Rothrock, Bindu Nanduri, Zhiqian Chen, Mahalingam Ramkumar</div><div style='padding-top: 10px; width: 80ex'>Pathogen control is a critical aspect of modern poultry farming, providing
important benefits for both public health and productivity. Effective poultry
management measures to reduce pathogen levels in poultry flocks promote food
safety by lowering risks of food-borne illnesses. They also support animal
health and welfare by preventing infectious diseases that can rapidly spread
and impact flock growth, egg production, and overall health. This study frames
the search for optimal management practices that minimize the presence of
multiple pathogens as a combinatorial optimization problem. Specifically, we
model the various possible combinations of management settings as a solution
space that can be efficiently explored to identify configurations that
optimally reduce pathogen levels. This design incorporates a neural network
feedback-based method that combines feature explanations with global
sensitivity analysis to ensure combinatorial optimization in multiobjective
settings. Our preliminary experiments have promising results when applied to
two real-world agricultural datasets. While further validation is still needed,
these early experimental findings demonstrate the potential of the model to
derive targeted feature interactions that adaptively optimize pathogen control
under varying real-world constraints.</div><div><a href='http://arxiv.org/abs/2403.00016v1'>2403.00016v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15290v1")'>Benchmarking with MIMIC-IV, an irregular, spare clinical time series
  dataset</div>
<div id='2401.15290v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T04:09:41Z</div><div>Authors: Hung Bui, Harikrishna Warrier, Yogesh Gupta</div><div style='padding-top: 10px; width: 80ex'>Electronic health record (EHR) is more and more popular, and it comes with
applying machine learning solutions to resolve various problems in the domain.
This growing research area also raises the need for EHRs accessibility. Medical
Information Mart for Intensive Care (MIMIC) dataset is a popular, public, and
free EHR dataset in a raw format that has been used in numerous studies.
However, despite of its popularity, it is lacking benchmarking work, especially
with recent state of the art works in the field of deep learning with
time-series tabular data. The aim of this work is to fill this lack by
providing a benchmark for latest version of MIMIC dataset, MIMIC-IV. We also
give a detailed literature survey about studies that has been already done for
MIIMIC-III.</div><div><a href='http://arxiv.org/abs/2401.15290v1'>2401.15290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17917v1")'>Collaborative learning of common latent representations in routinely
  collected multivariate ICU physiological signals</div>
<div id='2402.17917v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T22:10:51Z</div><div>Authors: Hollan Haule, Ian Piper, Patricia Jones, Tsz-Yan Milly Lo, Javier Escudero</div><div style='padding-top: 10px; width: 80ex'>In Intensive Care Units (ICU), the abundance of multivariate time series
presents an opportunity for machine learning (ML) to enhance patient
phenotyping. In contrast to previous research focused on electronic health
records (EHR), here we propose an ML approach for phenotyping using routinely
collected physiological time series data. Our new algorithm integrates Long
Short-Term Memory (LSTM) networks with collaborative filtering concepts to
identify common physiological states across patients. Tested on real-world ICU
clinical data for intracranial hypertension (IH) detection in patients with
brain injury, our method achieved an area under the curve (AUC) of 0.889 and
average precision (AP) of 0.725. Moreover, our algorithm outperforms
autoencoders in learning more structured latent representations of the
physiological signals. These findings highlight the promise of our methodology
for patient phenotyping, leveraging routinely collected multivariate time
series to improve clinical care practices.</div><div><a href='http://arxiv.org/abs/2402.17917v1'>2402.17917v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10283v1")'>Window Stacking Meta-Models for Clinical EEG Classification</div>
<div id='2401.10283v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T17:45:52Z</div><div>Authors: Yixuan Zhu, Rohan Kandasamy, Luke J. W. Canham, David Western</div><div style='padding-top: 10px; width: 80ex'>Windowing is a common technique in EEG machine learning classification and
other time series tasks. However, a challenge arises when employing this
technique: computational expense inhibits learning global relationships across
an entire recording or set of recordings. Furthermore, the labels inherited by
windows from their parent recordings may not accurately reflect the content of
that window in isolation. To resolve these issues, we introduce a multi-stage
model architecture, incorporating meta-learning principles tailored to
time-windowed data aggregation. We further tested two distinct strategies to
alleviate these issues: lengthening the window and utilizing overlapping to
augment data. Our methods, when tested on the Temple University Hospital
Abnormal EEG Corpus (TUAB), dramatically boosted the benchmark accuracy from
89.8 percent to 99.0 percent. This breakthrough performance surpasses prior
performance projections for this dataset and paves the way for clinical
applications of machine learning solutions to EEG interpretation challenges. On
a broader and more varied dataset from the Temple University Hospital EEG
Corpus (TUEG), we attained an accuracy of 86.7%, nearing the assumed
performance ceiling set by variable inter-rater agreement on such datasets.</div><div><a href='http://arxiv.org/abs/2401.10283v1'>2401.10283v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06970v1")'>TemporalAugmenter: An Ensemble Recurrent Based Deep Learning Approach
  for Signal Classification</div>
<div id='2401.06970v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T03:53:47Z</div><div>Authors: Nelly Elsayed, Constantinos L. Zekios, Navid Asadizanjani, Zag ElSayed</div><div style='padding-top: 10px; width: 80ex'>Ensemble modeling has been widely used to solve complex problems as it helps
to improve overall performance and generalization. In this paper, we propose a
novel TemporalAugmenter approach based on ensemble modeling for augmenting the
temporal information capturing for long-term and short-term dependencies in
data integration of two variations of recurrent neural networks in two learning
streams to obtain the maximum possible temporal extraction. Thus, the proposed
model augments the extraction of temporal dependencies. In addition, the
proposed approach reduces the preprocessing and prior stages of feature
extraction, which reduces the required energy to process the models built upon
the proposed TemporalAugmenter approach, contributing towards green AI.
Moreover, the proposed model can be simply integrated into various domains
including industrial, medical, and human-computer interaction applications. Our
proposed approach empirically evaluated the speech emotion recognition,
electrocardiogram signal, and signal quality examination tasks as three
different signals with varying complexity and different temporal dependency
features.</div><div><a href='http://arxiv.org/abs/2401.06970v1'>2401.06970v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17779v1")'>Assessing the importance of long-range correlations for
  deep-learning-based sleep staging</div>
<div id='2402.17779v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:39:10Z</div><div>Authors: Tiezhi Wang, Nils Strodthoff</div><div style='padding-top: 10px; width: 80ex'>This study aims to elucidate the significance of long-range correlations for
deep-learning-based sleep staging. It is centered around S4Sleep(TS), a
recently proposed model for automated sleep staging. This model utilizes
electroencephalography (EEG) as raw time series input and relies on structured
state space sequence (S4) models as essential model component. Although the
model already surpasses state-of-the-art methods for a moderate number of 15
input epochs, recent literature results suggest potential benefits from
incorporating very long correlations spanning hundreds of input epochs. In this
submission, we explore the possibility of achieving further enhancements by
systematically scaling up the model's input size, anticipating potential
improvements in prediction accuracy. In contrast to findings in literature, our
results demonstrate that augmenting the input size does not yield a significant
enhancement in the performance of S4Sleep(TS). These findings, coupled with the
distinctive ability of S4 models to capture long-range dependencies in time
series data, cast doubt on the diagnostic relevance of very long-range
interactions for sleep staging.</div><div><a href='http://arxiv.org/abs/2402.17779v1'>2402.17779v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06966v1")'>DeepCover: Advancing RNN Test Coverage and Online Error Prediction using
  State Machine Extraction</div>
<div id='2402.06966v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T14:45:23Z</div><div>Authors: Pouria Golshanrad, Fathiyeh Faghih</div><div style='padding-top: 10px; width: 80ex'>Recurrent neural networks (RNNs) have emerged as powerful tools for
processing sequential data in various fields, including natural language
processing and speech recognition. However, the lack of explainability in RNN
models has limited their interpretability, posing challenges in understanding
their internal workings. To address this issue, this paper proposes a
methodology for extracting a state machine (SM) from an RNN-based model to
provide insights into its internal function. The proposed SM extraction
algorithm was assessed using four newly proposed metrics: Purity, Richness,
Goodness, and Scale. The proposed methodology along with its assessment metrics
contribute to increasing explainability in RNN models by providing a clear
representation of their internal decision making process through the extracted
SM. In addition to improving the explainability of RNNs, the extracted SM can
be used to advance testing and and monitoring of the primary RNN-based model.
To enhance RNN testing, we introduce six model coverage criteria based on the
extracted SM, serving as metrics for evaluating the effectiveness of test
suites designed to analyze the primary model. We also propose a tree-based
model to predict the error probability of the primary model for each input
based on the extracted SM. We evaluated our proposed online error prediction
approach using the MNIST dataset and Mini Speech Commands dataset, achieving an
area under the curve (AUC) exceeding 80\% for the receiver operating
characteristic (ROC) chart.</div><div><a href='http://arxiv.org/abs/2402.06966v1'>2402.06966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15377v1")'>Validation of artificial neural networks to model the acoustic behaviour
  of induction motors</div>
<div id='2401.15377v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T10:49:33Z</div><div>Authors: F. J. Jimenez-Romero, D. Guijo-Rubio, F. R. Lara-Raya, A. Ruiz-Gonzalez, C. Hervas-Martinez</div><div style='padding-top: 10px; width: 80ex'>In the last decade, the sound quality of electric induction motors is a hot
topic in the research field. Specially, due to its high number of applications,
the population is exposed to physical and psychological discomfort caused by
the noise emission. Therefore, it is necessary to minimise its psychological
impact on the population. In this way, the main goal of this work is to
evaluate the use of multitask artificial neural networks as a modelling
technique for simultaneously predicting psychoacoustic parameters of induction
motors. Several inputs are used, such as, the electrical magnitudes of the
motor power signal and the number of poles, instead of separating the noise of
the electric motor from the environmental noise. Two different kind of
artificial neural networks are proposed to evaluate the acoustic quality of
induction motors, by using the equivalent sound pressure, the loudness, the
roughness and the sharpness as outputs. Concretely, two different topologies
have been considered: simple models and more complex models. The former are
more interpretable, while the later lead to higher accuracy at the cost of
hiding the cause-effect relationship. Focusing on the simple interpretable
models, product unit neural networks achieved the best results: for MSE and for
SEP. The main benefit of this product unit model is its simplicity, since only
10 inputs variables are used, outlining the effective transfer mechanism of
multitask artificial neural networks to extract common features of multiple
tasks. Finally, a deep analysis of the acoustic quality of induction motors in
done using the best product unit neural networks.</div><div><a href='http://arxiv.org/abs/2401.15377v1'>2401.15377v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12733v1")'>TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation
  Prediction with Noisy Physiological Data</div>
<div id='2401.12733v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T13:11:05Z</div><div>Authors: Niqi Liu, Fang Liu, Wenqi Ji, Xinxin Du, Xu Liu, Guozhen Zhao, Wenting Mu, Yong-Jin Liu</div><div style='padding-top: 10px; width: 80ex'>The robust generalization of deep learning models in the presence of inherent
noise remains a significant challenge, especially when labels are subjective
and noise is indiscernible in natural settings. This problem is particularly
pronounced in many practical applications. In this paper, we address a special
and important scenario of monitoring suicidal ideation, where time-series data,
such as photoplethysmography (PPG), is susceptible to such noise. Current
methods predominantly focus on image and text data or address artificially
introduced noise, neglecting the complexities of natural noise in time-series
analysis. To tackle this, we introduce a novel neural network model tailored
for analyzing noisy physiological time-series data, named TNANet, which merges
advanced encoding techniques with confidence learning, enhancing prediction
accuracy. Another contribution of our work is the collection of a specialized
dataset of PPG signals derived from real-world environments for suicidal
ideation prediction. Employing this dataset, our TNANet achieves the prediction
accuracy of 63.33% in a binary classification task, outperforming
state-of-the-art models. Furthermore, comprehensive evaluations were conducted
on three other well-known public datasets with artificially introduced noise to
rigorously test the TNANet's capabilities. These tests consistently
demonstrated TNANet's superior performance by achieving an accuracy improvement
of more than 10% compared to baseline methods.</div><div><a href='http://arxiv.org/abs/2401.12733v1'>2401.12733v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13005v3")'>SzCORE: A Seizure Community Open-source Research Evaluation framework
  for the validation of EEG-based automated seizure detection algorithms</div>
<div id='2402.13005v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T13:38:04Z</div><div>Authors: Jonathan Dan, Una Pale, Alireza Amirshahi, William Cappelletti, Thorir Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Adriano Bernini, Luca Benini, Sándor Beniczky, David Atienza, Philippe Ryvlin</div><div style='padding-top: 10px; width: 80ex'>The need for high-quality automated seizure detection algorithms based on
electroencephalography (EEG) becomes ever more pressing with the increasing use
of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods
of these algorithms influences the reported results and makes comprehensive
evaluation and comparison challenging. This heterogeneity concerns in
particular the choice of datasets, evaluation methodologies, and performance
metrics. In this paper, we propose a unified framework designed to establish
standardization in the validation of EEG-based seizure detection algorithms.
Based on existing guidelines and recommendations, the framework introduces a
set of recommendations and standards related to datasets, file formats, EEG
data input content, seizure annotation input and output, cross-validation
strategies, and performance metrics. We also propose the 10-20 seizure
detection benchmark, a machine-learning benchmark based on public datasets
converted to a standardized format. This benchmark defines the machine-learning
task as well as reporting metrics. We illustrate the use of the benchmark by
evaluating a set of existing seizure detection algorithms. The SzCORE (Seizure
Community Open-source Research Evaluation) framework and benchmark are made
publicly available along with an open-source software library to facilitate
research use, while enabling rigorous evaluation of the clinical significance
of the algorithms, fostering a collective effort to more optimally detect
seizures to improve the lives of people with epilepsy.</div><div><a href='http://arxiv.org/abs/2402.13005v3'>2402.13005v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07201v1")'>A multi-cohort study on prediction of acute brain dysfunction states
  using selective state space models</div>
<div id='2403.07201v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T22:58:11Z</div><div>Authors: Brandon Silva, Miguel Contreras, Sabyasachi Bandyopadhyay, Yuanfang Ren, Ziyuan Guan, Jeremy Balch, Kia Khezeli, Tezcan Ozrazgat Baslanti, Ben Shickel, Azra Bihorac, Parisa Rashidi</div><div style='padding-top: 10px; width: 80ex'>Assessing acute brain dysfunction (ABD), including delirium and coma in the
intensive care unit (ICU), is a critical challenge due to its prevalence and
severe implications for patient outcomes. Current diagnostic methods rely on
infrequent clinical observations, which can only determine a patient's ABD
status after onset. Our research attempts to solve these problems by harnessing
Electronic Health Records (EHR) data to develop automated methods for ABD
prediction for patients in the ICU. Existing models solely predict a single
state (e.g., either delirium or coma), require at least 24 hours of observation
data to make predictions, do not dynamically predict fluctuating ABD conditions
during ICU stay (typically a one-time prediction), and use small sample size,
proprietary single-hospital datasets. Our research fills these gaps in the
existing literature by dynamically predicting delirium, coma, and mortality for
12-hour intervals throughout an ICU stay and validating on two public datasets.
Our research also introduces the concept of dynamically predicting critical
transitions from non-ABD to ABD and between different ABD states in real time,
which could be clinically more informative for the hospital staff. We compared
the predictive performance of two state-of-the-art neural network models, the
MAMBA selective state space model and the Longformer Transformer model. Using
the MAMBA model, we achieved a mean area under the receiving operator
characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour
intervals. The model achieves a mean AUROC of 0.79 when predicting transitions
between ABD states. Our study uses a curated dataset from the University of
Florida Health Shands Hospital for internal validation and two publicly
available datasets, MIMIC-IV and eICU, for external validation, demonstrating
robustness across ICU stays from 203 hospitals and 140,945 patients.</div><div><a href='http://arxiv.org/abs/2403.07201v1'>2403.07201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13657v2")'>Inadequacy of common stochastic neural networks for reliable clinical
  decision support</div>
<div id='2401.13657v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T18:49:30Z</div><div>Authors: Adrian Lindenmeyer, Malte Blattmann, Stefan Franke, Thomas Neumuth, Daniel Schneider</div><div style='padding-top: 10px; width: 80ex'>Widespread adoption of AI for medical decision making is still hindered due
to ethical and safety-related concerns. For AI-based decision support systems
in healthcare settings it is paramount to be reliable and trustworthy. Common
deep learning approaches, however, have the tendency towards overconfidence
under data shift. Such inappropriate extrapolation beyond evidence-based
scenarios may have dire consequences. This highlights the importance of
reliable estimation of local uncertainty and its communication to the end user.
While stochastic neural networks have been heralded as a potential solution to
these issues, this study investigates their actual reliability in clinical
applications. We centered our analysis on the exemplary use case of mortality
prediction for ICU hospitalizations using EHR from MIMIC3 study. For
predictions on the EHR time series, Encoder-Only Transformer models were
employed. Stochasticity of model functions was achieved by incorporating common
methods such as Bayesian neural network layers and model ensembles. Our models
achieve state of the art performance in terms of discrimination performance
(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality
prediction benchmark. However, epistemic uncertainty is critically
underestimated by the selected stochastic deep learning methods. A heuristic
proof for the responsible collapse of the posterior distribution is provided.
Our findings reveal the inadequacy of commonly used stochastic deep learning
approaches to reliably recognize OoD samples. In both methods, unsubstantiated
model confidence is not prevented due to strongly biased functional posteriors,
rendering them inappropriate for reliable clinical decision support. This
highlights the need for approaches with more strictly enforced or inherent
distance-awareness to known data points, e.g., using kernel-based techniques.</div><div><a href='http://arxiv.org/abs/2401.13657v2'>2401.13657v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13842v1")'>Analyzing the Variations in Emergency Department Boarding and Testing
  the Transferability of Forecasting Models across COVID-19 Pandemic Waves in
  Hong Kong: Hybrid CNN-LSTM approach to quantifying building-level
  socioecological risk</div>
<div id='2403.13842v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T11:48:40Z</div><div>Authors: Eman Leung, Jingjing Guan, Kin On Kwok, CT Hung, CC. Ching, CK. Chung, Hector Tsang, EK Yeoh, Albert Lee</div><div style='padding-top: 10px; width: 80ex'>Emergency department's (ED) boarding (defined as ED waiting time greater than
four hours) has been linked to poor patient outcomes and health system
performance. Yet, effective forecasting models is rare before COVID-19, lacking
during the peri-COVID era. Here, a hybrid convolutional neural network
(CNN)-Long short-term memory (LSTM) model was applied to public-domain data
sourced from Hong Kong's Hospital Authority, Department of Health, and Housing
Authority. In addition, we sought to identify the phase of the COVID-19
pandemic that most significantly perturbed our complex adaptive healthcare
system, thereby revealing a stable pattern of interconnectedness among its
components, using deep transfer learning methodology.
  Our result shows that 1) the greatest proportion of days with ED boarding was
found between waves four and five; 2) the best-performing model for forecasting
ED boarding was observed between waves four and five, which was based on
features representing time-invariant residential buildings' built environment
and sociodemographic profiles and the historical time series of ED boarding and
case counts, compared to during the waves when best-performing forecasting is
based on time-series features alone; and 3) when the model built from the
period between waves four and five was applied to data from other waves via
deep transfer learning, the transferred model enhanced the performance of
indigenous models.</div><div><a href='http://arxiv.org/abs/2403.13842v1'>2403.13842v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07038v1")'>Leveraging graph neural networks for supporting Automatic Triage of
  Patients</div>
<div id='2403.07038v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T09:54:35Z</div><div>Authors: Annamaria Defilippo, Pierangelo Veltri, Pietro Lio', Pietro Hiram Guzzi</div><div style='padding-top: 10px; width: 80ex'>Patient triage plays a crucial role in emergency departments, ensuring timely
and appropriate care based on correctly evaluating the emergency grade of
patient conditions.
  Triage methods are generally performed by human operator based on her own
experience and information that are gathered from the patient management
process.
  Thus, it is a process that can generate errors in emergency level
associations. Recently, Traditional triage methods heavily rely on human
decisions, which can be subjective and prone to errors.
  Recently, a growing interest has been focused on leveraging artificial
intelligence (AI) to develop algorithms able to maximize information gathering
and minimize errors in patient triage processing.
  We define and implement an AI based module to manage patients emergency code
assignments in emergency departments. It uses emergency department historical
data to train the medical decision process. Data containing relevant patient
information, such as vital signs, symptoms, and medical history, are used to
accurately classify patients into triage categories. Experimental results
demonstrate that the proposed algorithm achieved high accuracy outperforming
traditional triage methods. By using the proposed method we claim that
healthcare professionals can predict severity index to guide patient management
processing and resource allocation.</div><div><a href='http://arxiv.org/abs/2403.07038v1'>2403.07038v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08002v1")'>Discovery of Generalizable TBI Phenotypes Using Multivariate Time-Series
  Clustering</div>
<div id='2401.08002v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T23:10:22Z</div><div>Authors: Hamid Ghaderi, Brandon Foreman, Chandan K. Reddy, Vignesh Subbian</div><div style='padding-top: 10px; width: 80ex'>Traumatic Brain Injury (TBI) presents a broad spectrum of clinical
presentations and outcomes due to its inherent heterogeneity, leading to
diverse recovery trajectories and varied therapeutic responses. While many
studies have delved into TBI phenotyping for distinct patient populations,
identifying TBI phenotypes that consistently generalize across various settings
and populations remains a critical research gap. Our research addresses this by
employing multivariate time-series clustering to unveil TBI's dynamic
intricates. Utilizing a self-supervised learning-based approach to clustering
multivariate time-Series data with missing values (SLAC-Time), we analyzed both
the research-centric TRACK-TBI and the real-world MIMIC-IV datasets.
Remarkably, the optimal hyperparameters of SLAC-Time and the ideal number of
clusters remained consistent across these datasets, underscoring SLAC-Time's
stability across heterogeneous datasets. Our analysis revealed three
generalizable TBI phenotypes ({\alpha}, \b{eta}, and {\gamma}), each exhibiting
distinct non-temporal features during emergency department visits, and temporal
feature profiles throughout ICU stays. Specifically, phenotype {\alpha}
represents mild TBI with a remarkably consistent clinical presentation. In
contrast, phenotype \b{eta} signifies severe TBI with diverse clinical
manifestations, and phenotype {\gamma} represents a moderate TBI profile in
terms of severity and clinical diversity. Age is a significant determinant of
TBI outcomes, with older cohorts recording higher mortality rates. Importantly,
while certain features varied by age, the core characteristics of TBI
manifestations tied to each phenotype remain consistent across diverse
populations.</div><div><a href='http://arxiv.org/abs/2401.08002v1'>2401.08002v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12930v1")'>pyAKI -- An Open Source Solution to Automated KDIGO classification</div>
<div id='2401.12930v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T17:33:41Z</div><div>Authors: Christian Porschen, Jan Ernsting, Paul Brauckmann, Raphael Weiss, Till Würdemann, Hendrik Booke, Wida Amini, Ludwig Maidowski, Benjamin Risse, Tim Hahn, Thilo von Groote</div><div style='padding-top: 10px; width: 80ex'>Acute Kidney Injury (AKI) is a frequent complication in critically ill
patients, affecting up to 50% of patients in the intensive care units. The lack
of standardized and open-source tools for applying the Kidney Disease Improving
Global Outcomes (KDIGO) criteria to time series data has a negative impact on
workload and study quality. This project introduces pyAKI, an open-source
pipeline addressing this gap by providing a comprehensive solution for
consistent KDIGO criteria implementation. The pyAKI pipeline was developed and
validated using a subset of the Medical Information Mart for Intensive Care
(MIMIC)-IV database, a commonly used database in critical care research. We
defined a standardized data model in order to ensure reproducibility.
Validation against expert annotations demonstrated pyAKI's robust performance
in implementing KDIGO criteria. Comparative analysis revealed its ability to
surpass the quality of human labels. This work introduces pyAKI as an
open-source solution for implementing the KDIGO criteria for AKI diagnosis
using time series data with high accuracy and performance.</div><div><a href='http://arxiv.org/abs/2401.12930v1'>2401.12930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12818v1")'>Dynamic Survival Analysis for Early Event Prediction</div>
<div id='2403.12818v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T15:17:23Z</div><div>Authors: Hugo Yèche, Manuel Burger, Dinara Veshchezerova, Gunnar Rätsch</div><div style='padding-top: 10px; width: 80ex'>This study advances Early Event Prediction (EEP) in healthcare through
Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk
localization into alarm policies to enhance clinical event metrics. By adapting
and evaluating DSA models against traditional EEP benchmarks, our research
demonstrates their ability to match EEP models on a time-step level and
significantly improve event-level metrics through a new alarm prioritization
scheme (up to 11% AuPRC difference). This approach represents a significant
step forward in predictive healthcare, providing a more nuanced and actionable
framework for early event prediction and management.</div><div><a href='http://arxiv.org/abs/2403.12818v1'>2403.12818v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10972v1")'>Modeling methodology for the accurate and prompt prediction of
  symptomatic events in chronic diseases</div>
<div id='2402.10972v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T08:30:50Z</div><div>Authors: Josué Pagán, José L. Risco-Martín, José M. Moya, José L. Ayala</div><div style='padding-top: 10px; width: 80ex'>Prediction of symptomatic crises in chronic diseases allows to take decisions
before the symptoms occur, such as the intake of drugs to avoid the symptoms or
the activation of medical alarms. The prediction horizon is in this case an
important parameter in order to fulfill the pharmacokinetics of medications, or
the time response of medical services. This paper presents a study about the
prediction limits of a chronic disease with symptomatic crises: the migraine.
For that purpose, this work develops a methodology to build predictive migraine
models and to improve these predictions beyond the limits of the initial
models. The maximum prediction horizon is analyzed, and its dependency on the
selected features is studied. A strategy for model selection is proposed to
tackle the trade off between conservative but robust predictive models, with
respect to less accurate predictions with higher horizons. The obtained results
show a prediction horizon close to 40 minutes, which is in the time range of
the drug pharmacokinetics. Experiments have been performed in a realistic
scenario where input data have been acquired in an ambulatory clinical study by
the deployment of a non-intrusive Wireless Body Sensor Network. Our results
provide an effective methodology for the selection of the future horizon in the
development of prediction algorithms for diseases experiencing symptomatic
crises.</div><div><a href='http://arxiv.org/abs/2402.10972v1'>2402.10972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.05940v1")'>Causal Relationship Network of Risk Factors Impacting Workday Loss in
  Underground Coal Mines</div>
<div id='2402.05940v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T22:45:34Z</div><div>Authors: Shangsi Ren, Cameron A. Beeche, Zhiyi Shi, Maria Acevedo Garcia, Katherine Zychowski, Shuguang Leng, Pedram Roghanchi, Jiantao Pu</div><div style='padding-top: 10px; width: 80ex'>This study aims to establish the causal relationship network between various
factors leading to workday loss in underground coal mines using a novel causal
artificial intelligence (AI) method. The analysis utilizes data obtained from
the National Institute for Occupational Safety and Health (NIOSH). A total of
101,010 injury records from 3,982 unique underground coal mines spanning the
years from 1990 to 2020 were extracted from the NIOSH database. Causal
relationships were analyzed and visualized using a novel causal AI method
called Grouped Greedy Equivalence Search (GGES). The impact of each variable on
workday loss was assessed through intervention do-calculus adjustment (IDA)
scores. Model training and validation were performed using the 10-fold
cross-validation technique. Performance metrics, including adjacency precision
(AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall
(AHR), were utilized to evaluate the models. Findings revealed that after 2006,
key direct causes of workday loss among mining employees included total mining
experience, mean office employees, mean underground employees, county, and
total mining experience (years). Total mining experience emerged as the most
influential factor, whereas mean employees per mine exhibited the least
influence. The analyses emphasized the significant role of total mining
experience in determining workday loss. The models achieved optimal
performance, with AP, AR, AHP, and AHR values measuring 0.694, 0.653, 0.386,
and 0.345, respectively. This study demonstrates the feasibility of utilizing
the new GGES method to clarify the causal factors behind the workday loss by
analyzing employment demographics and injury records and establish their causal
relationship network.</div><div><a href='http://arxiv.org/abs/2402.05940v1'>2402.05940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04012v1")'>Temporal Cross-Attention for Dynamic Embedding and Tokenization of
  Multimodal Electronic Health Records</div>
<div id='2403.04012v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T19:46:44Z</div><div>Authors: Yingbo Ma, Suraj Kolla, Dhruv Kaliraman, Victoria Nolan, Zhenhong Hu, Ziyuan Guan, Yuanfang Ren, Brooke Armfield, Tezcan Ozrazgat-Baslanti, Tyler J. Loftus, Parisa Rashidi, Azra Bihorac, Benjamin Shickel</div><div style='padding-top: 10px; width: 80ex'>The breadth, scale, and temporal granularity of modern electronic health
records (EHR) systems offers great potential for estimating personalized and
contextual patient health trajectories using sequential deep learning. However,
learning useful representations of EHR data is challenging due to its high
dimensionality, sparsity, multimodality, irregular and variable-specific
recording frequency, and timestamp duplication when multiple measurements are
recorded simultaneously. Although recent efforts to fuse structured EHR and
unstructured clinical notes suggest the potential for more accurate prediction
of clinical outcomes, less focus has been placed on EHR embedding approaches
that directly address temporal EHR challenges by learning time-aware
representations from multimodal patient time series. In this paper, we
introduce a dynamic embedding and tokenization framework for precise
representation of multimodal clinical time series that combines novel methods
for encoding time and sequential position with temporal cross-attention. Our
embedding and tokenization framework, when integrated into a multitask
transformer classifier with sliding window attention, outperformed baseline
approaches on the exemplar task of predicting the occurrence of nine
postoperative complications of more than 120,000 major inpatient surgeries
using multimodal data from three hospitals and two academic health centers in
the United States.</div><div><a href='http://arxiv.org/abs/2403.04012v1'>2403.04012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14694v2")'>TA-RNN: an Attention-based Time-aware Recurrent Neural Network
  Architecture for Electronic Health Records</div>
<div id='2401.14694v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T07:34:53Z</div><div>Authors: Mohammad Al Olaimat, Serdar Bozdag</div><div style='padding-top: 10px; width: 80ex'>Motivation: Electronic Health Records (EHR) represent a comprehensive
resource of a patient's medical history. EHR are essential for utilizing
advanced technologies such as deep learning (DL), enabling healthcare providers
to analyze extensive data, extract valuable insights, and make precise and
data-driven clinical decisions. DL methods such as Recurrent Neural Networks
(RNN) have been utilized to analyze EHR to model disease progression and
predict diagnosis. However, these methods do not address some inherent
irregularities in EHR data such as irregular time intervals between clinical
visits. Furthermore, most DL models are not interpretable. In this study, we
propose two interpretable DL architectures based on RNN, namely Time-Aware RNN
(TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical
outcome in EHR at next visit and multiple visits ahead, respectively. To
mitigate the impact of irregular time intervals, we propose incorporating time
embedding of the elapsed times between visits. For interpretability, we propose
employing a dual-level attention mechanism that operates between visits and
features within each visit.
  Results: The results of the experiments conducted on Alzheimer's Disease
Neuroimaging Initiative (ADNI) and National Alzheimer's Coordinating Center
(NACC) datasets indicated superior performance of proposed models for
predicting Alzheimer's Disease (AD) compared to state-of-the-art and baseline
approaches based on F2 and sensitivity. Additionally, TA-RNN showed superior
performance on Medical Information Mart for Intensive Care (MIMIC-III) dataset
for mortality prediction. In our ablation study, we observed enhanced
predictive performance by incorporating time embedding and attention
mechanisms. Finally, investigating attention weights helped identify
influential visits and features in predictions.</div><div><a href='http://arxiv.org/abs/2401.14694v2'>2401.14694v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00756v1")'>MPRE: Multi-perspective Patient Representation Extractor for Disease
  Prediction</div>
<div id='2401.00756v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T13:52:05Z</div><div>Authors: Ziyue Yu, Jiayi Wang, Wuman Luo, Rita Tse, Giovanni Pau</div><div style='padding-top: 10px; width: 80ex'>Patient representation learning based on electronic health records (EHR) is a
critical task for disease prediction. This task aims to effectively extract
useful information on dynamic features. Although various existing works have
achieved remarkable progress, the model performance can be further improved by
fully extracting the trends, variations, and the correlation between the trends
and variations in dynamic features. In addition, sparse visit records limit the
performance of deep learning models. To address these issues, we propose the
Multi-perspective Patient Representation Extractor (MPRE) for disease
prediction. Specifically, we propose Frequency Transformation Module (FTM) to
extract the trend and variation information of dynamic features in the
time-frequency domain, which can enhance the feature representation. In the 2D
Multi-Extraction Network (2D MEN), we form the 2D temporal tensor based on
trend and variation. Then, the correlations between trend and variation are
captured by the proposed dilated operation. Moreover, we propose the
First-Order Difference Attention Mechanism (FODAM) to calculate the
contributions of differences in adjacent variations to the disease diagnosis
adaptively. To evaluate the performance of MPRE and baseline methods, we
conduct extensive experiments on two real-world public datasets. The experiment
results show that MPRE outperforms state-of-the-art baseline methods in terms
of AUROC and AUPRC.</div><div><a href='http://arxiv.org/abs/2401.00756v1'>2401.00756v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16230v1")'>GARNN: An Interpretable Graph Attentive Recurrent Neural Network for
  Predicting Blood Glucose Levels via Multivariate Time Series</div>
<div id='2402.16230v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T01:18:53Z</div><div>Authors: Chengzhe Piao, Taiyu Zhu, Stephanie E Baldeweg, Paul Taylor, Pantelis Georgiou, Jiahao Sun, Jun Wang, Kezhi Li</div><div style='padding-top: 10px; width: 80ex'>Accurate prediction of future blood glucose (BG) levels can effectively
improve BG management for people living with diabetes, thereby reducing
complications and improving quality of life. The state of the art of BG
prediction has been achieved by leveraging advanced deep learning methods to
model multi-modal data, i.e., sensor data and self-reported event data,
organised as multi-variate time series (MTS). However, these methods are mostly
regarded as ``black boxes'' and not entirely trusted by clinicians and
patients. In this paper, we propose interpretable graph attentive recurrent
neural networks (GARNNs) to model MTS, explaining variable contributions via
summarizing variable importance and generating feature maps by graph attention
mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets,
representing diverse clinical scenarios. Upon comparison with twelve
well-established baseline methods, GARNNs not only achieve the best prediction
accuracy but also provide high-quality temporal interpretability, in particular
for postprandial glucose levels as a result of corresponding meal intake and
insulin injection. These findings underline the potential of GARNN as a robust
tool for improving diabetes care, bridging the gap between deep learning
technology and real-world healthcare solutions.</div><div><a href='http://arxiv.org/abs/2402.16230v1'>2402.16230v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15337v1")'>Deep Learning with Information Fusion and Model Interpretation for
  Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart
  Rate Monitoring Data</div>
<div id='2401.15337v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T07:59:54Z</div><div>Authors: Zenghui Lin, Xintong Liu, Nan Wang, Ruichen Li, Qingao Liu, Jingying Ma, Liwei Wang, Yan Wang, Shenda Hong</div><div style='padding-top: 10px; width: 80ex'>Long-term fetal heart rate (FHR) monitoring during the antepartum period,
increasingly popularized by electronic FHR monitoring, represents a growing
approach in FHR monitoring. This kind of continuous monitoring, in contrast to
the short-term one, collects an extended period of fetal heart data. This
offers a more comprehensive understanding of fetus's conditions. However, the
interpretation of long-term antenatal fetal heart monitoring is still in its
early stages, lacking corresponding clinical standards. Furthermore, the
substantial amount of data generated by continuous monitoring imposes a
significant burden on clinical work when analyzed manually. To address above
challenges, this study develops an automatic analysis system named LARA
(Long-term Antepartum Risk Analysis system) for continuous FHR monitoring,
combining deep learning and information fusion methods. LARA's core is a
well-established convolutional neural network (CNN) model. It processes
long-term FHR data as input and generates a Risk Distribution Map (RDM) and
Risk Index (RI) as the analysis results. We evaluate LARA on inner test
dataset, the performance metrics are as follows: AUC 0.872, accuracy 0.816,
specificity 0.811, sensitivity 0.806, precision 0.271, and F1 score 0.415. In
our study, we observe that long-term FHR monitoring data with higher RI is more
likely to result in adverse outcomes (p=0.0021). In conclusion, this study
introduces LARA, the first automated analysis system for long-term FHR
monitoring, initiating the further explorations into its clinical value in the
future.</div><div><a href='http://arxiv.org/abs/2401.15337v1'>2401.15337v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14926v1")'>Contrastive Learning on Multimodal Analysis of Electronic Health Records</div>
<div id='2403.14926v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T03:01:42Z</div><div>Authors: Tianxi Cai, Feiqing Huang, Ryumei Nakada, Linjun Zhang, Doudou Zhou</div><div style='padding-top: 10px; width: 80ex'>Electronic health record (EHR) systems contain a wealth of multimodal
clinical data including structured data like clinical codes and unstructured
data such as clinical notes. However, many existing EHR-focused studies has
traditionally either concentrated on an individual modality or merged different
modalities in a rather rudimentary fashion. This approach often results in the
perception of structured and unstructured data as separate entities, neglecting
the inherent synergy between them. Specifically, the two important modalities
contain clinically relevant, inextricably linked and complementary health
information. A more complete picture of a patient's medical history is captured
by the joint analysis of the two modalities of data. Despite the great success
of multimodal contrastive learning on vision-language, its potential remains
under-explored in the realm of multimodal EHR, particularly in terms of its
theoretical understanding. To accommodate the statistical analysis of
multimodal EHR data, in this paper, we propose a novel multimodal feature
embedding generative model and design a multimodal contrastive loss to obtain
the multimodal EHR feature representation. Our theoretical analysis
demonstrates the effectiveness of multimodal learning compared to
single-modality learning and connects the solution of the loss function to the
singular value decomposition of a pointwise mutual information matrix. This
connection paves the way for a privacy-preserving algorithm tailored for
multimodal EHR feature representation learning. Simulation studies show that
the proposed algorithm performs well under a variety of configurations. We
further validate the clinical utility of the proposed algorithm in real-world
EHR data.</div><div><a href='http://arxiv.org/abs/2403.14926v1'>2403.14926v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04086v1")'>Automated Multi-Task Learning for Joint Disease Prediction on Electronic
  Health Records</div>
<div id='2403.04086v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T22:32:48Z</div><div>Authors: Suhan Cui, Prasenjit Mitra</div><div style='padding-top: 10px; width: 80ex'>In the realm of big data and digital healthcare, Electronic Health Records
(EHR) have become a rich source of information with the potential to improve
patient care and medical research. In recent years, machine learning models
have proliferated for analyzing EHR data to predict patients future health
conditions. Among them, some studies advocate for multi-task learning (MTL) to
jointly predict multiple target diseases for improving the prediction
performance over single task learning. Nevertheless, current MTL frameworks for
EHR data have significant limitations due to their heavy reliance on human
experts to identify task groups for joint training and design model
architectures. To reduce human intervention and improve the framework design,
we propose an automated approach named AutoDP, which can search for the optimal
configuration of task grouping and architectures simultaneously. To tackle the
vast joint search space encompassing task combinations and architectures, we
employ surrogate model-based optimization, enabling us to efficiently discover
the optimal solution. Experimental results on real-world EHR data demonstrate
the efficacy of the proposed AutoDP framework. It achieves significant
performance improvements over both hand-crafted and automated state-of-the-art
methods, also maintains a feasible search cost at the same time.</div><div><a href='http://arxiv.org/abs/2403.04086v1'>2403.04086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15814v2")'>OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for
  Medication Recommendation</div>
<div id='2401.15814v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T00:29:39Z</div><div>Authors: Weicong Tan, Weiqing Wang, Xin Zhou, Wray Buntine, Gordon Bingham, Hongzhi Yin</div><div style='padding-top: 10px; width: 80ex'>Most existing medication recommendation models learn representations for
medical concepts based on electronic health records (EHRs) and make
recommendations with learnt representations. However, most medications appear
in the dataset for limited times, resulting in insufficient learning of their
representations. Medical ontologies are the hierarchical classification systems
for medical terms where similar terms are in the same class on a certain level.
In this paper, we propose OntoMedRec, the logically-pretrained and
model-agnostic medical Ontology Encoders for Medication Recommendation that
addresses data sparsity problem with medical ontologies. We conduct
comprehensive experiments on benchmark datasets to evaluate the effectiveness
of OntoMedRec, and the result shows the integration of OntoMedRec improves the
performance of various models in both the entire EHR datasets and the
admissions with few-shot medications. We provide the GitHub repository for the
source code on https://anonymous.4open.science/r/OntoMedRec-D123</div><div><a href='http://arxiv.org/abs/2401.15814v2'>2401.15814v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08820v1")'>Diet-ODIN: A Novel Framework for Opioid Misuse Detection with
  Interpretable Dietary Patterns</div>
<div id='2403.08820v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:36:24Z</div><div>Authors: Zheyuan Zhang, Zehong Wang, Shifu Hou, Evan Hall, Landon Bachman, Vincent Galassi, Jasmine White, Nitesh V. Chawla, Chuxu Zhang, Yanfang Ye</div><div style='padding-top: 10px; width: 80ex'>The opioid crisis has been one of the most critical society concerns in the
United States. Although the medication assisted treatment (MAT) is recognized
as the most effective treatment for opioid misuse and addiction, the various
side effects can trigger opioid relapse. In addition to MAT, the dietary
nutrition intervention has been demonstrated its importance in opioid misuse
prevention and recovery. However, research on the alarming connections between
dietary patterns and opioid misuse remain under-explored. In response to this
gap, in this paper, we first establish a large-scale multifaceted dietary
benchmark dataset related to opioid users at the first attempt and then develop
a novel framework - i.e., namely Opioid Misuse Detection with Interpretable
Dietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large
language model (LLM) for the identification of users with opioid misuse and the
interpretation of their associated dietary patterns. Specifically, in
Diet-ODIN, we first construct an HG to comprehensively incorporate both dietary
and health-related information, and then we devise a holistic graph learning
framework with noise reduction to fully capitalize both users' individual
dietary habits and shared dietary patterns for the detection of users with
opioid misuse. To further delve into the intricate correlations between dietary
patterns and opioid misuse, we exploit an LLM by utilizing the knowledge
obtained from the graph learning model for interpretation. The extensive
experimental results based on our established benchmark with quantitative and
qualitative measures demonstrate the outstanding performance of Diet-ODIN in
exploring the complex interplay between opioid misuse and dietary patterns, by
comparison with state-of-the-art baseline methods.</div><div><a href='http://arxiv.org/abs/2403.08820v1'>2403.08820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.08818v1")'>Multimodal Fusion of EHR in Structures and Semantics: Integrating
  Clinical Records and Notes with Hypergraph and LLM</div>
<div id='2403.08818v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T23:48:40Z</div><div>Authors: Hejie Cui, Xinyu Fang, Ran Xu, Xuan Kan, Joyce C. Ho, Carl Yang</div><div style='padding-top: 10px; width: 80ex'>Electronic Health Records (EHRs) have become increasingly popular to support
clinical decision-making and healthcare in recent decades. EHRs usually contain
heterogeneous information, such as structural data in tabular form and
unstructured data in textual notes. Different types of information in EHRs can
complement each other and provide a more complete picture of the health status
of a patient. While there has been a lot of research on representation learning
of structured EHR data, the fusion of different types of EHR data (multimodal
fusion) is not well studied. This is mostly because of the complex medical
coding systems used and the noise and redundancy present in the written notes.
In this work, we propose a new framework called MINGLE, which integrates both
structures and semantics in EHR effectively. Our framework uses a two-level
infusion strategy to combine medical concept semantics and clinical note
semantics into hypergraph neural networks, which learn the complex interactions
between different types of data to generate visit representations for
downstream prediction. Experiment results on two EHR datasets, the public
MIMIC-III and private CRADLE, show that MINGLE can effectively improve
predictive performance by 11.83% relatively, enhancing semantic integration as
well as multimodal fusion for structural and textual EHR data.</div><div><a href='http://arxiv.org/abs/2403.08818v1'>2403.08818v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05220v1")'>Synthetic Privileged Information Enhances Medical Image Representation
  Learning</div>
<div id='2403.05220v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T11:18:26Z</div><div>Authors: Lucas Farndale, Chris Walsh, Robert Insall, Ke Yuan</div><div style='padding-top: 10px; width: 80ex'>Multimodal self-supervised representation learning has consistently proven to
be a highly effective method in medical image analysis, offering strong task
performance and producing biologically informed insights. However, these
methods heavily rely on large, paired datasets, which is prohibitive for their
use in scenarios where paired data does not exist, or there is only a small
amount available. In contrast, image generation methods can work well on very
small datasets, and can find mappings between unpaired datasets, meaning an
effectively unlimited amount of paired synthetic data can be generated. In this
work, we demonstrate that representation learning can be significantly improved
by synthetically generating paired information, both compared to training on
either single-modality (up to 4.4x error reduction) or authentic multi-modal
paired datasets (up to 5.6x error reduction).</div><div><a href='http://arxiv.org/abs/2403.05220v1'>2403.05220v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01054v1")'>Unconditional Latent Diffusion Models Memorize Patient Imaging Data</div>
<div id='2402.01054v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T22:58:21Z</div><div>Authors: Salman Ul Hassan Dar, Marvin Seyfarth, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Sandy Engelhardt</div><div style='padding-top: 10px; width: 80ex'>Generative latent diffusion models hold a wide range of applications in the
medical imaging domain. A noteworthy application is privacy-preserved open-data
sharing by proposing synthetic data as surrogates of real patient data. Despite
the promise, these models are susceptible to patient data memorization, where
models generate patient data copies instead of novel synthetic samples. This
undermines the whole purpose of preserving patient data and may even result in
patient re-identification. Considering the importance of the problem,
surprisingly it has received relatively little attention in the medical imaging
community. To this end, we assess memorization in latent diffusion models for
medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR,
and X-ray datasets for synthetic data generation. Afterwards, we examine the
amount of training data memorized utilizing self-supervised models and further
investigate various factors that can possibly lead to memorization by training
models in different settings. We observe a surprisingly large amount of data
memorization among all datasets, with up to 41.7%, 19.6%, and 32.6% of the
training data memorized in CT, MRI, and X-ray datasets respectively. Further
analyses reveal that increasing training data size and using data augmentation
reduce memorization, while over-training enhances it. Overall, our results
suggest a call for memorization-informed evaluation of synthetic data prior to
open-data sharing.</div><div><a href='http://arxiv.org/abs/2402.01054v1'>2402.01054v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04290v1")'>MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided
  Diffusion with Visual Invariant</div>
<div id='2403.04290v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T07:39:00Z</div><div>Authors: Chenlu Zhan, Yu Lin, Gaoang Wang, Hongwei Wang, Jian Wu</div><div style='padding-top: 10px; width: 80ex'>Medical generative models, acknowledged for their high-quality sample
generation ability, have accelerated the fast growth of medical applications.
However, recent works concentrate on separate medical generation models for
distinct medical tasks and are restricted to inadequate medical multi-modal
knowledge, constraining medical comprehensive diagnosis. In this paper, we
propose MedM2G, a Medical Multi-Modal Generative framework, with the key
innovation to align, extract, and generate medical multi-modal within a unified
model. Extending beyond single or two medical modalities, we efficiently align
medical multi-modal through the central alignment approach in the unified
space. Significantly, our framework extracts valuable clinical knowledge by
preserving the medical visual invariant of each imaging modal, thereby
enhancing specific medical information for multi-modal generation. By
conditioning the adaptive cross-guided parameters into the multi-flow diffusion
framework, our model promotes flexible interactions among medical multi-modal
for generation. MedM2G is the first medical generative model that unifies
medical generation tasks of text-to-image, image-to-text, and unified
generation of medical modalities (CT, MRI, X-ray). It performs 5 medical
generation tasks across 10 datasets, consistently outperforming various
state-of-the-art works.</div><div><a href='http://arxiv.org/abs/2403.04290v1'>2403.04290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01509v1")'>Advancing Brain Tumor Inpainting with Generative Models</div>
<div id='2402.01509v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T15:43:51Z</div><div>Authors: Ruizhi Zhu, Xinru Zhang, Haowen Pang, Chundan Xu, Chuyang Ye</div><div style='padding-top: 10px; width: 80ex'>Synthesizing healthy brain scans from diseased brain scans offers a potential
solution to address the limitations of general-purpose algorithms, such as
tissue segmentation and brain extraction algorithms, which may not effectively
handle diseased images. We consider this a 3D inpainting task and investigate
the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic
resonance imaging(MRI) data. Our contributions encompass potential
modifications tailored to MRI-specific needs, and we conducted evaluations of
multiple inpainting techniques using the BraTS2023 Inpainting datasets to
assess their efficacy and limitations.</div><div><a href='http://arxiv.org/abs/2402.01509v1'>2402.01509v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00771v1")'>XProspeCT: CT Volume Generation from Paired X-Rays</div>
<div id='2403.00771v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T21:57:49Z</div><div>Authors: Benjamin Paulson, Joshua Goldshteyn, Sydney Balboni, John Cisler, Andrew Crisler, Natalia Bukowski, Julia Kalish, Theodore Colwell</div><div style='padding-top: 10px; width: 80ex'>Computed tomography (CT) is a beneficial imaging tool for diagnostic
purposes. CT scans provide detailed information concerning the internal
anatomic structures of a patient, but present higher radiation dose and costs
compared to X-ray imaging. In this paper, we build on previous research to
convert orthogonal X-ray images into simulated CT volumes by exploring larger
datasets and various model structures. Significant model variations include
UNet architectures, custom connections, activation functions, loss functions,
optimizers, and a novel back projection approach.</div><div><a href='http://arxiv.org/abs/2403.00771v1'>2403.00771v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00376v1")'>Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET
  Image Reconstruction</div>
<div id='2402.00376v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T06:47:56Z</div><div>Authors: Jiaqi Cui, Yan Wang, Lu Wen, Pinxian Zeng, Xi Wu, Jiliu Zhou, Dinggang Shen</div><div style='padding-top: 10px; width: 80ex'>To obtain high-quality Positron emission tomography (PET) images while
minimizing radiation exposure, numerous methods have been proposed to
reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET
(LPET) images. However, these methods heavily rely on voxel-based
representations, which fall short of adequately accounting for the precise
structure and fine-grained context, leading to compromised reconstruction. In
this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN,
to reconstruct high-quality SPET images from LPET. Specifically, inspired by
the geometric representation power of points, we resort to a point-based
representation to enhance the explicit expression of the image structure, thus
facilitating the reconstruction with finer details. Moreover, a context
clustering strategy is applied to explore the contextual relationships among
points, which mitigates the ambiguities of small structures in the
reconstructed images. Experiments on both clinical and phantom datasets
demonstrate that our PCC-GAN outperforms the state-of-the-art reconstruction
methods qualitatively and quantitatively. Code is available at
https://github.com/gluucose/PCCGAN.</div><div><a href='http://arxiv.org/abs/2402.00376v1'>2402.00376v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10831v1")'>GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers</div>
<div id='2402.10831v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T17:03:08Z</div><div>Authors: Ehtasham Naseer, Ali Imran Sandhu, Muhammad Adnan Siddique, Waqas W. Ahmed, Mohamed Farhat, Ying Wu</div><div style='padding-top: 10px; width: 80ex'>Inverse scattering problems are inherently challenging, given the fact they
are ill-posed and nonlinear. This paper presents a powerful deep learning-based
approach that relies on generative adversarial networks to accurately and
efficiently reconstruct randomly-shaped two-dimensional dielectric objects from
amplitudes of multi-frequency scattered electric fields. An adversarial
autoencoder (AAE) is trained to learn to generate the scatterer's geometry from
a lower-dimensional latent representation constrained to adhere to the Gaussian
distribution. A cohesive inverse neural network (INN) framework is set up
comprising a sequence of appropriately designed dense layers, the
already-trained generator as well as a separately trained forward neural
network. The images reconstructed at the output of the inverse network are
validated through comparison with outputs from the forward neural network,
addressing the non-uniqueness challenge inherent to electromagnetic (EM)
imaging problems. The trained INN demonstrates an enhanced robustness,
evidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure
similarity index (SSI) of $0.90$. The study not only demonstrates a significant
reduction in computational load, but also marks a substantial improvement over
traditional objective-function-based methods. It contributes both to the fields
of machine learning and EM imaging by offering a real-time quantitative imaging
approach. The results obtained with the simulated data, for both training and
testing, yield promising results and may open new avenues for radio-frequency
inverse imaging.</div><div><a href='http://arxiv.org/abs/2402.10831v1'>2402.10831v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01414v1")'>VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics</div>
<div id='2401.01414v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T19:51:49Z</div><div>Authors: Ammar A. Siddiqui, Santosh Tirunagari, Tehseen Zia, David Windridge</div><div style='padding-top: 10px; width: 80ex'>Visual attribution in medical imaging seeks to make evident the
diagnostically-relevant components of a medical image, in contrast to the more
common detection of diseased tissue deployed in standard machine vision
pipelines (which are less straightforwardly interpretable/explainable to
clinicians). We here present a novel generative visual attribution technique,
one that leverages latent diffusion models in combination with domain-specific
large language models, in order to generate normal counterparts of abnormal
images. The discrepancy between the two hence gives rise to a mapping
indicating the diagnostically-relevant image components. To achieve this, we
deploy image priors in conjunction with appropriate conditioning mechanisms in
order to control the image generative process, including natural language text
prompts acquired from medical science and applied radiology. We perform
experiments and quantitatively evaluate our results on the COVID-19 Radiography
Database containing labelled chest X-rays with differing pathologies via the
Frechet Inception Distance (FID), Structural Similarity (SSIM) and Multi Scale
Structural Similarity Metric (MS-SSIM) metrics obtained between real and
generated images. The resulting system also exhibits a range of latent
capabilities including zero-shot localized disease induction, which are
evaluated with real examples from the cheXpert dataset.</div><div><a href='http://arxiv.org/abs/2401.01414v1'>2401.01414v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02598v2")'>Pooling Image Datasets With Multiple Covariate Shift and Imbalance</div>
<div id='2403.02598v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T02:20:33Z</div><div>Authors: Sotirios Panagiotis Chytas, Vishnu Suresh Lokhande, Peiran Li, Vikas Singh</div><div style='padding-top: 10px; width: 80ex'>Small sample sizes are common in many disciplines, which necessitates pooling
roughly similar datasets across multiple institutions to study weak but
relevant associations between images and disease outcomes. Such data often
manifest shift/imbalance in covariates (i.e., secondary non-imaging data).
Controlling for such nuisance variables is common within standard statistical
analysis, but the ideas do not directly apply to overparameterized models.
Consequently, recent work has shown how strategies from invariant
representation learning provides a meaningful starting point, but the current
repertoire of methods is limited to accounting for shifts/imbalances in just a
couple of covariates at a time. In this paper, we show how viewing this problem
from the perspective of Category theory provides a simple and effective
solution that completely avoids elaborate multi-stage training pipelines that
would otherwise be needed. We show the effectiveness of this approach via
extensive experiments on real datasets. Further, we discuss how this style of
formulation offers a unified perspective on at least 5+ distinct problem
settings, from self-supervised learning to matching problems in 3D
reconstruction.</div><div><a href='http://arxiv.org/abs/2403.02598v2'>2403.02598v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03227v3")'>IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of
  brain MR images</div>
<div id='2402.03227v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:38:49Z</div><div>Authors: Vincent Roca, Grégory Kuchcinski, Jean-Pierre Pruvo, Dorian Manouvriez, Renaud Lopes</div><div style='padding-top: 10px; width: 80ex'>In MRI studies, the aggregation of imaging data from multiple acquisition
sites enhances sample size but may introduce site-related variabilities that
hinder consistency in subsequent analyses. Deep learning methods for image
translation have emerged as a solution for harmonizing MR images across sites.
In this study, we introduce IGUANe (Image Generation with Unified Adversarial
Networks), an original 3D model that leverages the strengths of domain
translation and straightforward application of style transfer methods for
multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture
by integrating an arbitrary number of domains for training through a
many-to-one strategy. During inference, the model can be applied to any image,
even from an unknown acquisition site, making it a universal generator for
harmonization. Trained on a dataset comprising T1-weighted images from 11
different scanners, IGUANe was evaluated on data from unseen sites. The
assessments included the transformation of MR images with traveling subjects,
the preservation of pairwise distances between MR images within domains, the
evolution of volumetric patterns related to age and Alzheimer$^\prime$s disease
(AD), and the performance in age regression and patient classification tasks.
Comparisons with other harmonization and normalization methods suggest that
IGUANe better preserves individual information in MR images and is more
suitable for maintaining and reinforcing variabilities related to age and AD.
Future studies may further assess IGUANe in other multicenter contexts, either
using the same model or retraining it for applications to different image
modalities.</div><div><a href='http://arxiv.org/abs/2402.03227v3'>2402.03227v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16008v1")'>Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach
  to Model Interpretability and Precision</div>
<div id='2402.16008v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T06:53:35Z</div><div>Authors: Yasmine Mustafa, Tie Luo</div><div style='padding-top: 10px; width: 80ex'>The evolution of deep learning and artificial intelligence has significantly
reshaped technological landscapes. However, their effective application in
crucial sectors such as medicine demands more than just superior performance,
but trustworthiness as well. While interpretability plays a pivotal role,
existing explainable AI (XAI) approaches often do not reveal {\em Clever Hans}
behavior where a model makes (ungeneralizable) correct predictions using
spurious correlations or biases in data. Likewise, current post-hoc XAI methods
are susceptible to generating unjustified counterfactual examples. In this
paper, we approach XAI with an innovative {\em model debugging} methodology
realized through Jacobian Saliency Map (JSM). To cast the problem into a
concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case,
motivated by its significant impact on human lives and the formidable challenge
in its early detection, stemming from the intricate nature of its progression.
We introduce an interpretable, multimodal model for AD classification over its
multi-stage progression, incorporating JSM as a modality-agnostic tool that
provides insights into volumetric changes indicative of brain abnormalities.
Our extensive evaluation including ablation study manifests the efficacy of
using JSM for model debugging and interpretation, while significantly enhancing
model accuracy as well.</div><div><a href='http://arxiv.org/abs/2402.16008v1'>2402.16008v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03132v1")'>Vision Transformers and Bi-LSTM for Alzheimer's Disease Diagnosis from
  3D MRI</div>
<div id='2401.03132v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:11:03Z</div><div>Authors: Taymaz Akan, Sait Alp, Mohammad A. N Bhuiyanb</div><div style='padding-top: 10px; width: 80ex'>Alzheimer's is a brain disease that gets worse over time and affects memory,
thinking, and behavior. Alzheimer's disease (AD) can be treated and managed if
it is diagnosed early, which can slow the progression of symptoms and improve
quality of life. In this study, we suggested using the Visual Transformer (ViT)
and bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used
ViT to extract features from the MRI and then map them to a feature sequence.
Then, we used Bi-LSTM sequence modeling to keep the interdependencies between
related features. In addition, we evaluated the performance of the proposed
model for the binary classification of AD patients using data from the
Alzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our
method against other deep learning models in the literature. The proposed
method performs well in terms of accuracy, precision, F-score, and recall for
the diagnosis of AD.</div><div><a href='http://arxiv.org/abs/2401.03132v1'>2401.03132v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05353v1")'>Hybridized Convolutional Neural Networks and Long Short-Term Memory for
  Improved Alzheimer's Disease Diagnosis from MRI Scans</div>
<div id='2403.05353v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T14:34:32Z</div><div>Authors: Maleka Khatun, Md Manowarul Islam, Habibur Rahman Rifat, Md. Shamim Bin Shahid, Md. Alamin Talukder, Md Ashraf Uddin</div><div style='padding-top: 10px; width: 80ex'>Brain-related diseases are more sensitive than other diseases due to several
factors, including the complexity of surgical procedures, high costs, and other
challenges. Alzheimer's disease is a common brain disorder that causes memory
loss and the shrinking of brain cells. Early detection is critical for
providing proper treatment to patients. However, identifying Alzheimer's at an
early stage using manual scanning of CT or MRI scans is challenging. Therefore,
researchers have delved into the exploration of computer-aided systems,
employing Machine Learning and Deep Learning methodologies, which entail the
training of datasets to detect Alzheimer's disease. This study aims to present
a hybrid model that combines a CNN model's feature extraction capabilities with
an LSTM model's detection capabilities. This study has applied the transfer
learning called VGG16 in the hybrid model to extract features from MRI images.
The LSTM detects features between the convolution layer and the fully connected
layer. The output layer of the fully connected layer uses the softmax function.
The training of the hybrid model involved utilizing the ADNI dataset. The trial
findings revealed that the model achieved a level of accuracy of 98.8%, a
sensitivity rate of 100%, and a specificity rate of 76%. The proposed hybrid
model outperforms its contemporary CNN counterparts, showcasing a superior
performance.</div><div><a href='http://arxiv.org/abs/2403.05353v1'>2403.05353v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14130v1")'>Attention-based Efficient Classification for 3D MRI Image of Alzheimer's
  Disease</div>
<div id='2401.14130v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T12:18:46Z</div><div>Authors: Yihao Lin, Ximeng Li, Yan Zhang, Jinshan Tang</div><div style='padding-top: 10px; width: 80ex'>Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to
its subtle and complex clinical symptoms. Deep learning-assisted medical
diagnosis using image recognition techniques has become an important research
topic in this field. The features have to accurately capture main variations of
anatomical brain structures. However, time-consuming is expensive for feature
extraction by deep learning training. This study proposes a novel Alzheimer's
disease detection model based on Convolutional Neural Networks. The model
utilizes a pre-trained ResNet network as the backbone, incorporating
post-fusion algorithm for 3D medical images and attention mechanisms. The
experimental results indicate that the employed 2D fusion algorithm effectively
improves the model's training expense. And the introduced attention mechanism
accurately weights important regions in images, further enhancing the model's
diagnostic accuracy.</div><div><a href='http://arxiv.org/abs/2401.14130v1'>2401.14130v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09475v1")'>Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from
  MRIs</div>
<div id='2401.09475v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T03:29:56Z</div><div>Authors: Zhaonian Zhang, Richard Jiang</div><div style='padding-top: 10px; width: 80ex'>The integration of machine learning in medicine has significantly improved
diagnostic precision, particularly in the interpretation of complex structures
like the human brain. Diagnosing challenging conditions such as Alzheimer's
disease has prompted the development of brain age estimation techniques. These
methods often leverage three-dimensional Magnetic Resonance Imaging (MRI)
scans, with recent studies emphasizing the efficacy of 3D convolutional neural
networks (CNNs) like 3D ResNet. However, the untapped potential of Vision
Transformers (ViTs), known for their accuracy and interpretability, persists in
this domain due to limitations in their 3D versions. This paper introduces
Triamese-ViT, an innovative adaptation of the ViT model for brain age
estimation. Our model uniquely combines ViTs from three different orientations
to capture 3D information, significantly enhancing accuracy and
interpretability. Tested on a dataset of 1351 MRI scans, Triamese-ViT achieves
a Mean Absolute Error (MAE) of 3.84, a 0.9 Spearman correlation coefficient
with chronological age, and a -0.29 Spearman correlation coefficient between
the brain age gap (BAG) and chronological age, significantly better than
previous methods for brian age estimation. A key innovation of Triamese-ViT is
its capacity to generate a comprehensive 3D-like attention map, synthesized
from 2D attention maps of each orientation-specific ViT. This feature is
particularly beneficial for in-depth brain age analysis and disease diagnosis,
offering deeper insights into brain health and the mechanisms of age-related
neural changes.</div><div><a href='http://arxiv.org/abs/2401.09475v1'>2401.09475v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13319v1")'>HyperFusion: A Hypernetwork Approach to Multimodal Integration of
  Tabular and Medical Imaging Data for Predictive Modeling</div>
<div id='2403.13319v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T05:50:04Z</div><div>Authors: Daniel Duenias, Brennan Nichyporuk, Tal Arbel, Tammy Riklin Raviv</div><div style='padding-top: 10px; width: 80ex'>The integration of diverse clinical modalities such as medical imaging and
the tabular data obtained by the patients' Electronic Health Records (EHRs) is
a crucial aspect of modern healthcare. The integrative analysis of multiple
sources can provide a comprehensive understanding of a patient's condition and
can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs)
consistently showcase outstanding performance in a wide range of multimodal
tasks in the medical domain. However, the complex endeavor of effectively
merging medical imaging with clinical, demographic and genetic information
represented as numerical tabular data remains a highly active and ongoing
research pursuit.
  We present a novel framework based on hypernetworks to fuse clinical imaging
and tabular data by conditioning the image processing on the EHR's values and
measurements. This approach aims to leverage the complementary information
present in these modalities to enhance the accuracy of various medical
applications. We demonstrate the strength and the generality of our method on
two different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely,
brain age prediction conditioned by subject's sex, and multiclass Alzheimer's
Disease (AD) classification conditioned by tabular data. We show that our
framework outperforms both single-modality models and state-of-the-art
MRI-tabular data fusion methods. The code, enclosed to this manuscript will be
made publicly available.</div><div><a href='http://arxiv.org/abs/2403.13319v1'>2403.13319v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10894v1")'>Fusion of Diffusion Weighted MRI and Clinical Data for Predicting
  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning</div>
<div id='2402.10894v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:51:42Z</div><div>Authors: Chia-Ling Tsai, Hui-Yun Su, Shen-Feng Sung, Wei-Yang Lin, Ying-Ying Su, Tzu-Hsien Yang, Man-Lin Mai</div><div style='padding-top: 10px; width: 80ex'>Stroke is a common disabling neurological condition that affects about
one-quarter of the adult population over age 25; more than half of patients
still have poor outcomes, such as permanent functional dependence or even
death, after the onset of acute stroke. The aim of this study is to investigate
the efficacy of diffusion-weighted MRI modalities combining with structured
health profile on predicting the functional outcome to facilitate early
intervention. A deep fusion learning network is proposed with two-stage
training: the first stage focuses on cross-modality representation learning and
the second stage on classification. Supervised contrastive learning is
exploited to learn discriminative features that separate the two classes of
patients from embeddings of individual modalities and from the fused multimodal
embedding. The network takes as the input DWI and ADC images, and structured
health profile data. The outcome is the prediction of the patient needing
long-term care at 3 months after the onset of stroke. Trained and evaluated
with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80
and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing
models that consolidate both imaging and structured data in the medical domain.
If trained with comprehensive clinical variables, including NIHSS and
comorbidities, the gain from images on making accurate prediction is not
considered substantial, but significant. However, diffusion-weighted MRI can
replace NIHSS to achieve comparable level of accuracy combining with other
readily available clinical variables for better generalization.</div><div><a href='http://arxiv.org/abs/2402.10894v1'>2402.10894v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11032v1")'>FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a
  Multi-Stage Tabular Deep Learning</div>
<div id='2403.11032v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T22:35:21Z</div><div>Authors: Sadaf Khademi, Zohreh Hajiakhondi, Golnaz Vaseghi, Nizal Sarrafzadegan, Arash Mohammadi</div><div style='padding-top: 10px; width: 80ex'>Familial Hypercholesterolemia (FH) is a genetic disorder characterized by
elevated levels of Low-Density Lipoprotein (LDL) cholesterol or its associated
genes. Early-stage and accurate categorization of FH is of significance
allowing for timely interventions to mitigate the risk of life-threatening
conditions. Conventional diagnosis approach, however, is complex, costly, and a
challenging interpretation task even for experienced clinicians resulting in
high underdiagnosis rates. Although there has been a recent surge of interest
in using Machine Learning (ML) models for early FH detection, existing
solutions only consider a binary classification task solely using classical ML
models. Despite its significance, application of Deep Learning (DL) for FH
detection is in its infancy, possibly, due to categorical nature of the
underlying clinical data. The paper addresses this gap by introducing the
FH-TabNet, which is a multi-stage tabular DL network for multi-class (Definite,
Probable, Possible, and Unlikely) FH detection. The FH-TabNet initially
involves applying a deep tabular data learning architecture (TabNet) for
primary categorization into healthy (Possible/Unlikely) and patient
(Probable/Definite) classes. Subsequently, independent TabNet classifiers are
applied to each subgroup, enabling refined classification. The model's
performance is evaluated through 5-fold cross-validation illustrating superior
performance in categorizing FH patients, particularly in the challenging
low-prevalence subcategories.</div><div><a href='http://arxiv.org/abs/2403.11032v1'>2403.11032v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.18035v1")'>Optimizing contrastive learning for cortical folding pattern detection</div>
<div id='2401.18035v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T17:59:57Z</div><div>Authors: Aymeric Gaudin, Louise Guillon, Clara Fischer, Arnaud Cachia, Denis Rivière, Jean-François Mangin, Joël Chavas</div><div style='padding-top: 10px; width: 80ex'>The human cerebral cortex has many bumps and grooves called gyri and sulci.
Even though there is a high inter-individual consistency for the main cortical
folds, this is not the case when we examine the exact shapes and details of the
folding patterns. Because of this complexity, characterizing the cortical
folding variability and relating them to subjects' behavioral characteristics
or pathologies is still an open scientific problem. Classical approaches
include labeling a few specific patterns, either manually or
semi-automatically, based on geometric distances, but the recent availability
of MRI image datasets of tens of thousands of subjects makes modern
deep-learning techniques particularly attractive. Here, we build a
self-supervised deep-learning model to detect folding patterns in the cingulate
region. We train a contrastive self-supervised model (SimCLR) on both Human
Connectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets with
topological-based augmentations on the cortical skeletons, which are
topological objects that capture the shape of the folds. We explore several
backbone architectures (convolutional network, DenseNet, and PointNet) for the
SimCLR. For evaluation and testing, we perform a linear classification task on
a database manually labeled for the presence of the "double-parallel" folding
pattern in the cingulate region, which is related to schizophrenia
characteristics. The best model, giving a test AUC of 0.76, is a convolutional
network with 6 layers, a 10-dimensional latent space, a linear projection head,
and using the branch-clipping augmentation. This is the first time that a
self-supervised deep learning model has been applied to cortical skeletons on
such a large dataset and quantitatively evaluated. We can now envisage the next
step: applying it to other brain regions to detect other biomarkers.</div><div><a href='http://arxiv.org/abs/2401.18035v1'>2401.18035v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11775v1")'>FOD-Swin-Net: angular super resolution of fiber orientation distribution
  using a transformer-based deep model</div>
<div id='2402.11775v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:07:15Z</div><div>Authors: Mateus Oliveira da Silva, Caio Pinheiro Santana, Diedre Santos do Carmo, Letícia Rittner</div><div style='padding-top: 10px; width: 80ex'>Identifying and characterizing brain fiber bundles can help to understand
many diseases and conditions. An important step in this process is the
estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance
Imaging (DW-MRI). However, obtaining robust orientation estimates demands
high-resolution data, leading to lengthy acquisitions that are not always
clinically available. In this work, we explore the use of automated angular
super resolution from faster acquisitions to overcome this challenge. Using the
publicly available Human Connectome Project (HCP) DW-MRI data, we trained a
transformer-based deep learning architecture to achieve angular super
resolution in fiber orientation distribution (FOD). Our patch-based
methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction
driven from 32 directions to be comparable to a multi-shell 288 direction FOD
reconstruction, greatly reducing the number of required directions on initial
acquisition. Evaluations of the reconstructed FOD with Angular Correlation
Coefficient and qualitative visualizations reveal superior performance than the
state-of-the-art in HCP testing data. Open source code for reproducibility is
available at https://github.com/MICLab-Unicamp/FOD-Swin-Net.</div><div><a href='http://arxiv.org/abs/2402.11775v1'>2402.11775v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00019v1")'>Diffusion MRI with Machine Learning</div>
<div id='2402.00019v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T13:03:35Z</div><div>Authors: Davood Karimi</div><div style='padding-top: 10px; width: 80ex'>Diffusion-weighted magnetic resonance imaging (dMRI) offers unique
capabilities such as noninvasive assessment of brain's micro-structure and
structural connectivity. However, analyzing the dMRI data to extract useful
information for clinical and scientific purposes is challenging. The dMRI
measurements often suffer from strong noise and artifacts, there is usually
high inter-session and inter-scanner heterogeneity in the data and considerable
inter-subject variability in brain structure, and the relationship between
measurements and the phenomena of interest can be highly complex. Recent years
have witnessed increasing use of machine learning methods for dMRI analysis.
This manuscript aims to assess these efforts, with a focus on methods that have
addressed micro-structure mapping, tractography, white matter tract analysis,
as well as data preprocessing and harmonization. We summarize the main
findings, strengths, and weaknesses of the existing methods and suggest topics
for future research. We find that machine learning may be exceptionally suited
to tackle some of the difficult tasks in dMRI analysis. However, for this to
happen, several shortcomings of existing methods and critical unresolved issues
need to be addressed. These include deficient evaluation practices, lack of
rich training datasets and validation benchmarks, as well as model
generalizability, reliability, and explainability concerns.</div><div><a href='http://arxiv.org/abs/2402.00019v1'>2402.00019v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05578v3")'>Fast Cerebral Blood Flow Analysis via Extreme Learning Machine</div>
<div id='2401.05578v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T23:01:35Z</div><div>Authors: Xi Chen, Zhenya Zang, Xingda Li</div><div style='padding-top: 10px; width: 80ex'>We introduce a rapid and precise analytical approach for analyzing cerebral
blood flow (CBF) using Diffuse Correlation Spectroscopy (DCS) with the
application of the Extreme Learning Machine (ELM). Our evaluation of ELM and
existing algorithms involves a comprehensive set of metrics. We assess these
algorithms using synthetic datasets for both semi-infinite and multi-layer
models. The results demonstrate that ELM consistently achieves higher fidelity
across various noise levels and optical parameters, showcasing robust
generalization ability and outperforming iterative fitting algorithms. Through
a comparison with a computationally efficient neural network, ELM attains
comparable accuracy with reduced training and inference times. Notably, the
absence of a back-propagation process in ELM during training results in
significantly faster training speeds compared to existing neural network
approaches. This proposed strategy holds promise for edge computing
applications with online training capabilities.</div><div><a href='http://arxiv.org/abs/2401.05578v3'>2401.05578v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05580v3")'>Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A
  Transfer Learning Approach with Noise Robustness Analysis</div>
<div id='2401.05580v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T23:10:51Z</div><div>Authors: Xi Chen, Xingda Li</div><div style='padding-top: 10px; width: 80ex'>Diffuse correlation spectroscopy (DCS) is an emerging noninvasive technique
that measures the tissue blood flow, by using near-infrared coherent
point-source illumination to detect spectral changes. While machine learning
has demonstrated significant potential for measuring blood flow index (BFi), an
open question concerning the success of this approach pertains to its
robustness in scenarios involving deviations between datasets with varying
Signal-to-Noise Ratios (SNRs) originating from diverse clinical applications
and various setups. This study proposes a transfer learning approach, aims to
assess the influence of SNRs on the generalization ability of learned features,
and demonstrate the robustness for transfer learning. A synthetic dataset with
varying levels of added noise is utilized to simulate different SNRs. The
proposed network takes a 1x64 autocorrelation curve as input and generates BFi
and the correlation parameter beta. The proposed model demonstrates excellent
performance across different SNRs, exhibiting enhanced fitting accuracy,
particularly for low SNR datasets when compared with other fitting methods.
This highlights its potential for clinical diagnosis and treatment across
various scenarios under different clinical setups.</div><div><a href='http://arxiv.org/abs/2401.05580v3'>2401.05580v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09758v1")'>Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network
  Kernel for Gaussian Process Regression</div>
<div id='2403.09758v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:41:15Z</div><div>Authors: Shaghayegh Z. Ashtiani, Mohammad Sarabian, Kaveh Laksari, Hessam Babaee</div><div style='padding-top: 10px; width: 80ex'>Blood flow reconstruction in the vasculature is important for many clinical
applications. However, in clinical settings, the available data are often quite
limited. For instance, Transcranial Doppler ultrasound (TCD) is a noninvasive
clinical tool that is commonly used in the clinical settings to measure blood
velocity waveform at several locations on brain's vasculature. This amount of
data is grossly insufficient for training machine learning surrogate models,
such as deep neural networks or Gaussian process regression. In this work, we
propose a Gaussian process regression approach based on physics-informed
kernels, enabling near-real-time reconstruction of blood flow in data-poor
regimes. We introduce a novel methodology to reconstruct the kernel within the
vascular network, which is a non-Euclidean space. The proposed kernel encodes
both spatiotemporal and vessel-to-vessel correlations, thus enabling blood flow
reconstruction in vessels that lack direct measurements. We demonstrate that
any prediction made with the proposed kernel satisfies the conservation of mass
principle. The kernel is constructed by running stochastic one-dimensional
blood flow simulations, where the stochasticity captures the epistemic
uncertainties, such as lack of knowledge about boundary conditions and
uncertainties in vasculature geometries. We demonstrate the performance of the
model on three test cases, namely, a simple Y-shaped bifurcation, abdominal
aorta, and the Circle of Willis in the brain.</div><div><a href='http://arxiv.org/abs/2403.09758v1'>2403.09758v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13040v1")'>Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping</div>
<div id='2403.13040v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:35:17Z</div><div>Authors: Hang Jung Ling, Salomé Bru, Julia Puig, Florian Vixège, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia</div><div style='padding-top: 10px; width: 80ex'>Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify
color Doppler in cardiac imaging. In this study, we propose novel alternatives
to the traditional iVFM optimization scheme by utilizing physics-informed
neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.
Through rigorous evaluation on simulated color Doppler images derived from a
patient-specific computational fluid dynamics model and in vivo Doppler
acquisitions, both approaches demonstrate comparable reconstruction performance
to the original iVFM algorithm. The efficiency of PINNs is boosted through
dual-stage optimization and pre-optimized weights. On the other hand, the
nnU-Net method excels in generalizability and real time capabilities. Notably,
nnU-Net shows superior robustness on sparse and truncated Doppler data while
maintaining independence from explicit boundary conditions. Overall, our
results highlight the effectiveness of these methods in reconstructing
intraventricular vector blood flow. The study also suggests potential
applications of PINNs in ultrafast color Doppler imaging and the incorporation
of fluid dynamics equations to derive biomarkers for cardiovascular diseases
based on blood flow.</div><div><a href='http://arxiv.org/abs/2403.13040v1'>2403.13040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12435v2")'>Quantitative Analysis of Molecular Transport in the Extracellular Space
  Using Physics-Informed Neural Network</div>
<div id='2401.12435v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T02:04:15Z</div><div>Authors: Jiayi Xie, Hongfeng Li, Jin Cheng, Qingrui Cai, Hanbo Tan, Lingyun Zu, Xiaobo Qu, Hongbin Han</div><div style='padding-top: 10px; width: 80ex'>The brain extracellular space (ECS), an irregular, extremely tortuous
nanoscale space located between cells or between cells and blood vessels, is
crucial for nerve cell survival. It plays a pivotal role in high-level brain
functions such as memory, emotion, and sensation. However, the specific form of
molecular transport within the ECS remain elusive. To address this challenge,
this paper proposes a novel approach to quantitatively analyze the molecular
transport within the ECS by solving an inverse problem derived from the
advection-diffusion equation (ADE) using a physics-informed neural network
(PINN). PINN provides a streamlined solution to the ADE without the need for
intricate mathematical formulations or grid settings. Additionally, the
optimization of PINN facilitates the automatic computation of the diffusion
coefficient governing long-term molecule transport and the velocity of
molecules driven by advection. Consequently, the proposed method allows for the
quantitative analysis and identification of the specific pattern of molecular
transport within the ECS through the calculation of the Peclet number.
Experimental validation on two datasets of magnetic resonance images (MRIs)
captured at different time points showcases the effectiveness of the proposed
method. Notably, our simulations reveal identical molecular transport patterns
between datasets representing rats with tracer injected into the same brain
region. These findings highlight the potential of PINN as a promising tool for
comprehensively exploring molecular transport within the ECS.</div><div><a href='http://arxiv.org/abs/2401.12435v2'>2401.12435v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14863v1")'>Distribution-informed and wavelength-flexible data-driven photoacoustic
  oximetry</div>
<div id='2403.14863v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T22:18:25Z</div><div>Authors: Janek Gröhl, Kylie Yeung, Kevin Gu, Thomas R. Else, Monika Golinska, Ellie V. Bunce, Lina Hacker, Sarah E. Bohndiek</div><div style='padding-top: 10px; width: 80ex'>Significance: Photoacoustic imaging (PAI) promises to measure
spatially-resolved blood oxygen saturation, but suffers from a lack of accurate
and robust spectral unmixing methods to deliver on this promise. Accurate blood
oxygenation estimation could have important clinical applications, from cancer
detection to quantifying inflammation.
  Aim: This study addresses the inflexibility of existing data-driven methods
for estimating blood oxygenation in PAI by introducing a recurrent neural
network architecture.
  Approach: We created 25 simulated training dataset variations to assess
neural network performance. We used a long short-term memory network to
implement a wavelength-flexible network architecture and proposed the
Jensen-Shannon divergence to predict the most suitable training dataset.
  Results: The network architecture can handle arbitrary input wavelengths and
outperforms linear unmixing and the previously proposed learned spectral
decolouring method. Small changes in the training data significantly affect the
accuracy of our method, but we find that the Jensen-Shannon divergence
correlates with the estimation error and is thus suitable for predicting the
most appropriate training datasets for any given application.
  Conclusions: A flexible data-driven network architecture combined with the
Jensen-Shannon Divergence to predict the best training data set provides a
promising direction that might enable robust data-driven photoacoustic oximetry
for clinical use cases.</div><div><a href='http://arxiv.org/abs/2403.14863v1'>2403.14863v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01201v1")'>Whole-examination AI estimation of fetal biometrics from 20-week
  ultrasound scans</div>
<div id='2401.01201v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T13:04:41Z</div><div>Authors: Lorenzo Venturini, Samuel Budd, Alfonso Farruggia, Robert Wright, Jacqueline Matthew, Thomas G. Day, Bernhard Kainz, Reza Razavi, Jo V. Hajnal</div><div style='padding-top: 10px; width: 80ex'>The current approach to fetal anomaly screening is based on biometric
measurements derived from individually selected ultrasound images. In this
paper, we introduce a paradigm shift that attains human-level performance in
biometric measurement by aggregating automatically extracted biometrics from
every frame across an entire scan, with no need for operator intervention. We
use a convolutional neural network to classify each frame of an ultrasound
video recording. We then measure fetal biometrics in every frame where
appropriate anatomy is visible. We use a Bayesian method to estimate the true
value of each biometric from a large number of measurements and
probabilistically reject outliers. We performed a retrospective experiment on
1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,
estimated fetal biometrics in those scans and compared our estimates to the
measurements sonographers took during the scan. Our method achieves human-level
performance in estimating fetal biometrics and estimates well-calibrated
credible intervals in which the true biometric value is expected to lie.</div><div><a href='http://arxiv.org/abs/2401.01201v1'>2401.01201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06940v1")'>Conditional Score-Based Diffusion Model for Cortical Thickness
  Trajectory Prediction</div>
<div id='2403.06940v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:26:18Z</div><div>Authors: Qing Xiao, Siyeop Yoon, Hui Ren, Matthew Tivnan, Lichao Sun, Quanzheng Li, Tianming Liu, Yu Zhang, Xiang Li</div><div style='padding-top: 10px; width: 80ex'>Alzheimer's Disease (AD) is a neurodegenerative condition characterized by
diverse progression rates among individuals, with changes in cortical thickness
(CTh) closely linked to its progression. Accurately forecasting CTh
trajectories can significantly enhance early diagnosis and intervention
strategies, providing timely care. However, the longitudinal data essential for
these studies often suffer from temporal sparsity and incompleteness,
presenting substantial challenges in modeling the disease's progression
accurately. Existing methods are limited, focusing primarily on datasets
without missing entries or requiring predefined assumptions about CTh
progression. To overcome these obstacles, we propose a conditional score-based
diffusion model specifically designed to generate CTh trajectories with the
given baseline information, such as age, sex, and initial diagnosis. Our
conditional diffusion model utilizes all available data during the training
phase to make predictions based solely on baseline information during inference
without needing prior history about CTh progression. The prediction accuracy of
the proposed CTh prediction pipeline using a conditional score-based model was
compared for sub-groups consisting of cognitively normal, mild cognitive
impairment, and AD subjects. The Bland-Altman analysis shows our
diffusion-based prediction model has a near-zero bias with narrow 95%
confidential interval compared to the ground-truth CTh in 6-36 months. In
addition, our conditional diffusion model has a stochastic generative nature,
therefore, we demonstrated an uncertainty analysis of patient-specific CTh
prediction through multiple realizations.</div><div><a href='http://arxiv.org/abs/2403.06940v1'>2403.06940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06087v1")'>Learning the irreversible progression trajectory of Alzheimer's disease</div>
<div id='2403.06087v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T04:17:42Z</div><div>Authors: Yipei Wang, Bing He, Shannon Risacher, Andrew Saykin, Jingwen Yan, Xiaoqian Wang</div><div style='padding-top: 10px; width: 80ex'>Alzheimer's disease (AD) is a progressive and irreversible brain disorder
that unfolds over the course of 30 years. Therefore, it is critical to capture
the disease progression in an early stage such that intervention can be applied
before the onset of symptoms. Machine learning (ML) models have been shown
effective in predicting the onset of AD. Yet for subjects with follow-up
visits, existing techniques for AD classification only aim for accurate group
assignment, where the monotonically increasing risk across follow-up visits is
usually ignored. Resulted fluctuating risk scores across visits violate the
irreversibility of AD, hampering the trustworthiness of models and also
providing little value to understanding the disease progression. To address
this issue, we propose a novel regularization approach to predict AD
longitudinally. Our technique aims to maintain the expected monotonicity of
increasing disease risk during progression while preserving expressiveness.
Specifically, we introduce a monotonicity constraint that encourages the model
to predict disease risk in a consistent and ordered manner across follow-up
visits. We evaluate our method using the longitudinal structural MRI and
amyloid-PET imaging data from the Alzheimer's Disease Neuroimaging Initiative
(ADNI). Our model outperforms existing techniques in capturing the
progressiveness of disease risk, and at the same time preserves prediction
accuracy.</div><div><a href='http://arxiv.org/abs/2403.06087v1'>2403.06087v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09517v1")'>Dimensional Neuroimaging Endophenotypes: Neurobiological Representations
  of Disease Heterogeneity Through Machine Learning</div>
<div id='2401.09517v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T16:31:48Z</div><div>Authors: Junhao Wen, Mathilde Antoniades, Zhijian Yang, Gyujoon Hwang, Ioanna Skampardoni, Rongguang Wang, Christos Davatzikos</div><div style='padding-top: 10px; width: 80ex'>Machine learning has been increasingly used to obtain individualized
neuroimaging signatures for disease diagnosis, prognosis, and response to
treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it
has contributed to a better understanding of disease heterogeneity by
identifying disease subtypes that present significant differences in various
brain phenotypic measures. In this review, we first present a systematic
literature overview of studies using machine learning and multimodal MRI to
unravel disease heterogeneity in various neuropsychiatric and neurodegenerative
disorders, including Alzheimer disease, schizophrenia, major depressive
disorder, autism spectrum disorder, multiple sclerosis, as well as their
potential in transdiagnostic settings. Subsequently, we summarize relevant
machine learning methodologies and discuss an emerging paradigm which we call
dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological
heterogeneity of neuropsychiatric and neurodegenerative disorders into a low
dimensional yet informative, quantitative brain phenotypic representation,
serving as a robust intermediate phenotype (i.e., endophenotype) largely
reflecting underlying genetics and etiology. Finally, we discuss the potential
clinical implications of the current findings and envision future research
avenues.</div><div><a href='http://arxiv.org/abs/2401.09517v1'>2401.09517v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13301v1")'>Classification of Radiologically Isolated Syndrome and Clinically
  Isolated Syndrome with Machine-Learning Techniques</div>
<div id='2401.13301v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T08:49:50Z</div><div>Authors: V Mato-Abad, A Labiano-Fontcuberta, S Rodriguez-Yanez, R Garcia-Vazquez, CR Munteanu, J Andrade-Garda, A Domingo-Santos, V Galan Sanchez-Seco, Y Aladro, ML Martinez-Gines, L Ayuso, J Benito-Leon</div><div style='padding-top: 10px; width: 80ex'>Background and purpose: The unanticipated detection by magnetic resonance
imaging (MRI) in the brain of asymptomatic subjects of white matter lesions
suggestive of multiple sclerosis (MS) has been named radiologically isolated
syndrome (RIS). As the difference between early MS [i.e. clinically isolated
syndrome (CIS)] and RIS is the occurrence of a clinical event, it is logical to
improve detection of the subclinical form without interfering with MRI as there
are radiological diagnostic criteria for that. Our objective was to use
machine-learning classification methods to identify morphometric measures that
help to discriminate patients with RIS from those with CIS.
  Methods: We used a multimodal 3-T MRI approach by combining MRI biomarkers
(cortical thickness, cortical and subcortical grey matter volume, and white
matter integrity) of a cohort of 17 patients with RIS and 17 patients with CIS
for single-subject level classification.
  Results: The best proposed models to predict the diagnosis of CIS and RIS
were based on the Naive Bayes, Bagging and Multilayer Perceptron classifiers
using only three features: the left rostral middle frontal gyrus volume and the
fractional anisotropy values in the right amygdala and right lingual gyrus. The
Naive Bayes obtained the highest accuracy [overall classification, 0.765; area
under the receiver operating characteristic (AUROC), 0.782].
  Conclusions: A machine-learning approach applied to multimodal MRI data may
differentiate between the earliest clinical expressions of MS (CIS and RIS)
with an accuracy of 78%.
  Keywords: Bagging; Multilayer Perceptron; Naive Bayes classifier; clinically
isolated syndrome; diffusion tensor imaging; machine-learning; magnetic
resonance imaging; multiple sclerosis; radiologically isolated syndrome.</div><div><a href='http://arxiv.org/abs/2401.13301v1'>2401.13301v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00981v1")'>Machine Learning Classification of Alzheimer's Disease Stages Using
  Cerebrospinal Fluid Biomarkers Alone</div>
<div id='2401.00981v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T00:55:10Z</div><div>Authors: Vivek Kumar Tiwari, Premananda Indic, Shawana Tabassum</div><div style='padding-top: 10px; width: 80ex'>Early diagnosis of Alzheimer's disease is a challenge because the existing
methodologies do not identify the patients in their preclinical stage, which
can last up to a decade prior to the onset of clinical symptoms. Several
research studies demonstrate the potential of cerebrospinal fluid biomarkers,
amyloid beta 1-42, T-tau, and P-tau, in early diagnosis of Alzheimer's disease
stages. In this work, we used machine learning models to classify different
stages of Alzheimer's disease based on the cerebrospinal fluid biomarker levels
alone. An electronic health record of patients from the National Alzheimer's
Coordinating Centre database was analyzed and the patients were subdivided
based on mini-mental state scores and clinical dementia ratings. Statistical
and correlation analyses were performed to identify significant differences
between the Alzheimer's stages. Afterward, machine learning classifiers
including K-Nearest Neighbors, Ensemble Boosted Tree, Ensemble Bagged Tree,
Support Vector Machine, Logistic Regression, and Naive Bayes classifiers were
employed to classify the Alzheimer's disease stages. The results demonstrate
that Ensemble Boosted Tree (84.4%) and Logistic Regression (73.4%) provide the
highest accuracy for binary classification, while Ensemble Bagged Tree (75.4%)
demonstrates better accuracy for multiclassification. The findings from this
research are expected to help clinicians in making an informed decision
regarding the early diagnosis of Alzheimer's from the cerebrospinal fluid
biomarkers alone, monitoring of the disease progression, and implementation of
appropriate intervention measures.</div><div><a href='http://arxiv.org/abs/2401.00981v1'>2401.00981v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08539v1")'>Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning</div>
<div id='2402.08539v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T15:43:30Z</div><div>Authors: Mingyang Li, Hongyu Liu, Yixuan Li, Zejun Wang, Yuan Yuan, Honglin Dai</div><div style='padding-top: 10px; width: 80ex'>This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI)
dataset and aims to explore early detection and disease progression in
Alzheimer's disease (AD). We employ innovative data preprocessing strategies,
including the use of the random forest algorithm to fill missing data and the
handling of outliers and invalid data, thereby fully mining and utilizing these
limited data resources. Through Spearman correlation coefficient analysis, we
identify some features strongly correlated with AD diagnosis. We build and test
three machine learning models using these features: random forest, XGBoost, and
support vector machine (SVM). Among them, the XGBoost model performs the best
in terms of diagnostic performance, achieving an accuracy of 91%. Overall, this
study successfully overcomes the challenge of missing data and provides
valuable insights into early detection of Alzheimer's disease, demonstrating
its unique research value and practical significance.</div><div><a href='http://arxiv.org/abs/2402.08539v1'>2402.08539v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15672v1")'>Evaluating Echo State Network for Parkinson's Disease Prediction using
  Voice Features</div>
<div id='2401.15672v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T14:39:43Z</div><div>Authors: Seyedeh Zahra Seyedi Hosseininian, Ahmadreza Tajari, Mohsen Ghalehnoie, Alireza Alfi</div><div style='padding-top: 10px; width: 80ex'>Parkinson's disease (PD) is a debilitating neurological disorder that
necessitates precise and early diagnosis for effective patient care. This study
aims to develop a diagnostic model capable of achieving both high accuracy and
minimizing false negatives, a critical factor in clinical practice. Given the
limited training data, a feature selection strategy utilizing ANOVA is employed
to identify the most informative features. Subsequently, various machine
learning methods, including Echo State Networks (ESN), Random Forest, k-nearest
Neighbors, Support Vector Classifier, Extreme Gradient Boosting, and Decision
Tree, are employed and thoroughly evaluated. The statistical analyses of the
results highlight ESN's exceptional performance, showcasing not only superior
accuracy but also the lowest false negative rate among all methods.
Consistently, statistical data indicates that the ESN method consistently
maintains a false negative rate of less than 8% in 83% of cases. ESN's capacity
to strike a delicate balance between diagnostic precision and minimizing
misclassifications positions it as an exemplary choice for PD diagnosis,
especially in scenarios characterized by limited data. This research marks a
significant step towards more efficient and reliable PD diagnosis, with
potential implications for enhanced patient outcomes and healthcare dynamics.</div><div><a href='http://arxiv.org/abs/2401.15672v1'>2401.15672v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00638v1")'>Random Forest-Based Prediction of Stroke Outcome</div>
<div id='2402.00638v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T14:54:17Z</div><div>Authors: Carlos Fernandez-Lozano, Pablo Hervella, Virginia Mato-Abad, Manuel Rodriguez-Yanez, Sonia Suarez-Garaboa, Iria Lopez-Dequidt, Ana Estany-Gestal, Tomas Sobrino, Francisco Campos, Jose Castillo, Santiago Rodriguez-Yanez, Ramon Iglesias-Rey</div><div style='padding-top: 10px; width: 80ex'>We research into the clinical, biochemical and neuroimaging factors
associated with the outcome of stroke patients to generate a predictive model
using machine learning techniques for prediction of mortality and morbidity 3
months after admission. The dataset consisted of patients with ischemic stroke
(IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit
of a European Tertiary Hospital prospectively registered. We identified the
main variables for machine learning Random Forest (RF), generating a predictive
model that can estimate patient mortality/morbidity. In conclusion, machine
learning algorithms RF can be effectively used in stroke patients for long-term
outcome prediction of mortality and morbidity.</div><div><a href='http://arxiv.org/abs/2402.00638v1'>2402.00638v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06645v2")'>Ricci flow-based brain surface covariance descriptors for diagnosing
  Alzheimer's disease</div>
<div id='2403.06645v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T12:07:33Z</div><div>Authors: Fatemeh Ahmadi, Mohamad Ebrahim Shiri, Behroz Bidabad, Maral Sedaghat, Pooran Memari</div><div style='padding-top: 10px; width: 80ex'>Automated feature extraction from MRI brain scans and diagnosis of
Alzheimer's disease are ongoing challenges. With advances in 3D imaging
technology, 3D data acquisition is becoming more viable and efficient than its
2D counterpart. Rather than using feature-based vectors, in this paper, for the
first time, we suggest a pipeline to extract novel covariance-based descriptors
from the cortical surface using the Ricci energy optimization. The covariance
descriptors are components of the nonlinear manifold of symmetric
positive-definite matrices, thus we focus on using the Gaussian radial basis
function to apply manifold-based classification to the 3D shape problem.
Applying this novel signature to the analysis of abnormal cortical brain
morphometry allows for diagnosing Alzheimer's disease. Experimental studies
performed on about two hundred 3D MRI brain models, gathered from Alzheimer's
Disease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of
our descriptors in achieving remarkable classification accuracy.</div><div><a href='http://arxiv.org/abs/2403.06645v2'>2403.06645v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13523v1")'>Balancing Spectral, Temporal and Spatial Information for EEG-based
  Alzheimer's Disease Classification</div>
<div id='2402.13523v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T04:36:30Z</div><div>Authors: Stephan Goerttler, Fei He, Min Wu</div><div style='padding-top: 10px; width: 80ex'>The prospect of future treatment warrants the development of cost-effective
screening for Alzheimer's disease (AD). A promising candidate in this regard is
electroencephalography (EEG), as it is one of the most economic imaging
modalities. Recent efforts in EEG analysis have shifted towards leveraging
spatial information, employing novel frameworks such as graph signal processing
or graph neural networks. Here, we systematically investigate the importance of
spatial information relative to spectral or temporal information by varying the
proportion of each dimension for AD classification. To do so, we test various
dimension resolution configurations on two routine EEG datasets. We find that
spatial information is consistently more relevant than temporal information and
equally relevant as spectral information. These results emphasise the necessity
to consider spatial information for EEG-based AD classification. On our second
dataset, we further find that well-balanced feature resolutions boost
classification accuracy by up to 1.6%. Our resolution-based feature extraction
has the potential to improve AD classification specifically, and multivariate
signal classification generally.</div><div><a href='http://arxiv.org/abs/2402.13523v1'>2402.13523v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07858v1")'>Multiscale Neuroimaging Features for the Identification of Medication
  Class and Non-Responders in Mood Disorder Treatment</div>
<div id='2402.07858v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:05:03Z</div><div>Authors: Bradley T. Baker, Mustafa S. Salman, Zening Fu, Armin Iraji, Elizabeth Osuch, Jeremy Bockholt, Vince D. Calhoun</div><div style='padding-top: 10px; width: 80ex'>In the clinical treatment of mood disorders, the complex behavioral symptoms
presented by patients and variability of patient response to particular
medication classes can create difficulties in providing fast and reliable
treatment when standard diagnostic and prescription methods are used.
Increasingly, the incorporation of physiological information such as
neuroimaging scans and derivatives into the clinical process promises to
alleviate some of the uncertainty surrounding this process. Particularly, if
neural features can help to identify patients who may not respond to standard
courses of anti-depressants or mood stabilizers, clinicians may elect to avoid
lengthy and side-effect-laden treatments and seek out a different, more
effective course that might otherwise not have been under consideration.
Previously, approaches for the derivation of relevant neuroimaging features
work at only one scale in the data, potentially limiting the depth of
information available for clinical decision support. In this work, we show that
the utilization of multi spatial scale neuroimaging features - particularly
resting state functional networks and functional network connectivity measures
- provide a rich and robust basis for the identification of relevant medication
class and non-responders in the treatment of mood disorders. We demonstrate
that the generated features, along with a novel approach for fast and automated
feature selection, can support high accuracy rates in the identification of
medication class and non-responders as well as the identification of novel,
multi-scale biomarkers.</div><div><a href='http://arxiv.org/abs/2402.07858v1'>2402.07858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00033v3")'>Identification of Craving Maps among Marijuana Users via the Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks</div>
<div id='2403.00033v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T04:01:38Z</div><div>Authors: Jun-En Ding, Shihao Yang, Anna Zilverstand, Feng Liu</div><div style='padding-top: 10px; width: 80ex'>The consumption of high doses of marijuana can have significant psychological
and social impacts. In this study, we propose an interpretable framework called
the HOGANN (High-Order Graph Attention Neural Networks) for Marijuana addiction
classification and followed by the analysis of the localized brain network
communities that demonstrated abnormal brain activities among chronic marijuana
users. The HOGANN integrates dynamic intrinsic functional brain networks
estimated from the resting-state functional magnetic resonance imaging
(rs-fMRI) using the Long Short-Term Memory (LSTM) to capture temporal network
dynamics. We employed an high-order attention module for information fusion and
message passing among the neighboring nodes, enhancing the network community
level analysis. We validated our model on two data cohorts and the overall
classification for both dataset have achieved a much higher accuracy than the
comparison algorithms. In addition, we identified the most relevant subnetworks
and cognitive regions which are impacted by persistent marijuana consumption,
suggesting that chronic marijuana consumption can adversely affect functional
brain networks, particularly within the Dorsal Attention and Frontoparietal
networks. Most interestingly, we found our model performs better on the cohorts
of subjects with long time dependence, which suggests longer time consumption
of marijuana brings more significant changes of brain networks. The model can
identify craving brain maps, and thus pinpointing brain regions that are
important for analysis.</div><div><a href='http://arxiv.org/abs/2403.00033v3'>2403.00033v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00724v1")'>Automatic Segmentation of the Spinal Cord Nerve Rootlets</div>
<div id='2402.00724v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:14:54Z</div><div>Authors: Jan Valosek, Theo Mathieu, Raphaelle Schlienger, Olivia S. Kowalczyk, Julien Cohen-Adad</div><div style='padding-top: 10px; width: 80ex'>Precise identification of spinal nerve rootlets is relevant to delineate
spinal levels for the study of functional activity in the spinal cord. The goal
of this study was to develop an automatic method for the semantic segmentation
of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI)
scans. Images from two open-access MRI datasets were used to train a 3D
multi-class convolutional neural network using an active learning approach to
segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal
level. The method was tested on 3T T2-weighted images from datasets unseen
during training to assess inter-site, inter-session, and inter-resolution
variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation
across rootlets levels), suggesting a good performance. The method also
demonstrated low inter-vendor and inter-site variability (coefficient of
variation &lt;= 1.41 %), as well as low inter-session variability (coefficient of
variation &lt;= 1.30 %) indicating stable predictions across different MRI
vendors, sites, and sessions. The proposed methodology is open-source and
readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher.</div><div><a href='http://arxiv.org/abs/2402.00724v1'>2402.00724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12115v1")'>Deep learning automates Cobb angle measurement compared with
  multi-expert observers</div>
<div id='2403.12115v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:43:45Z</div><div>Authors: Keyu Li, Hanxue Gu, Roy Colglazier, Robert Lark, Elizabeth Hubbard, Robert French, Denise Smith, Jikai Zhang, Erin McCrum, Anthony Catanzano, Joseph Cao, Leah Waldman, Maciej A. Mazurowski, Benjamin Alman</div><div style='padding-top: 10px; width: 80ex'>Scoliosis, a prevalent condition characterized by abnormal spinal curvature
leading to deformity, requires precise assessment methods for effective
diagnosis and management. The Cobb angle is a widely used scoliosis
quantification method that measures the degree of curvature between the tilted
vertebrae. Yet, manual measuring of Cobb angles is time-consuming and
labor-intensive, fraught with significant interobserver and intraobserver
variability. To address these challenges and the lack of interpretability found
in certain existing automated methods, we have created fully automated software
that not only precisely measures the Cobb angle but also provides clear
visualizations of these measurements. This software integrates deep neural
network-based spine region detection and segmentation, spine centerline
identification, pinpointing the most significantly tilted vertebrae, and direct
visualization of Cobb angles on the original images. Upon comparison with the
assessments of 7 expert readers, our algorithm exhibited a mean deviation in
Cobb angle measurements of 4.17 degrees, notably surpassing the manual
approach's average intra-reader discrepancy of 5.16 degrees. The algorithm also
achieved intra-class correlation coefficients (ICC) exceeding 0.96 and Pearson
correlation coefficients above 0.944, reflecting robust agreement with expert
assessments and superior measurement reliability. Through the comprehensive
reader study and statistical analysis, we believe this algorithm not only
ensures a higher consensus with expert readers but also enhances
interpretability and reproducibility during assessments. It holds significant
promise for clinical application, potentially aiding physicians in more
accurate scoliosis assessment and diagnosis, thereby improving patient care.</div><div><a href='http://arxiv.org/abs/2403.12115v1'>2403.12115v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06174v1")'>Machine Learning Applications in Spine Biomechanics</div>
<div id='2401.06174v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T04:45:12Z</div><div>Authors: Farshid Ghezelbash, Amir Hossein Eskandari, Xavier Robert-Lachaine, Frank Cao, Mehran Pesteie, Zhuohua Qiao, Aboulfazl Shirazi-Adl, Christian Larivière</div><div style='padding-top: 10px; width: 80ex'>Spine biomechanics is at a transformation with the advent and integration of
machine learning and computer vision technologies. These novel techniques
facilitate the estimation of 3D body shapes, anthropometrics, and kinematics
from as simple as a single-camera image, making them more accessible and
practical for a diverse range of applications. This study introduces a
framework that merges these methodologies with traditional musculoskeletal
modeling, enabling comprehensive analysis of spinal biomechanics during complex
activities from a single camera. Additionally, we aim to evaluate their
performance and limitations in spine biomechanics applications. The real-world
applications explored in this study include assessment in workplace lifting,
evaluation of whiplash injuries in car accidents, and biomechanical analysis in
professional sports. Our results demonstrate potential and limitations of
various algorithms in estimating body shape, kinematics, and conducting
in-field biomechanical analyses. In industrial settings, the potential to
utilize these new technologies for biomechanical risk assessments offers a
pathway for preventive measures against back injuries. In sports activities,
the proposed framework provides new opportunities for performance optimization,
injury prevention, and rehabilitation. The application in forensic domain
further underscores the wide-reaching implications of this technology. While
certain limitations were identified, particularly in accuracy of predictions,
complex interactions, and external load estimation, this study demonstrates
their potential for advancement in spine biomechanics, heralding an optimistic
future in both research and practical applications.</div><div><a href='http://arxiv.org/abs/2401.06174v1'>2401.06174v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05591v1")'>Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive
  Manufacturing Processes</div>
<div id='2403.05591v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T23:32:45Z</div><div>Authors: Anand Krishnan, Xingjian Yang, Utsav Seth, Jonathan M. Jeyachandran, Jonathan Y. Ahn, Richard Gardner, Samuel F. Pedigo, Adriana, Blom-Schieber, Ashis G. Banerjee, Krithika Manohar</div><div style='padding-top: 10px; width: 80ex'>Hand-intensive manufacturing processes, such as composite layup and textile
draping, require significant human dexterity to accommodate task complexity.
These strenuous hand motions often lead to musculoskeletal disorders and
rehabilitation surgeries. We develop a data-driven ergonomic risk assessment
system with a special focus on hand and finger activity to better identify and
address ergonomic issues related to hand-intensive manufacturing processes. The
system comprises a multi-modal sensor testbed to collect and synchronize
operator upper body pose, hand pose and applied forces; a Biometric Assessment
of Complete Hand (BACH) formulation to measure high-fidelity hand and finger
risks; and industry-standard risk scores associated with upper body posture,
RULA, and hand activity, HAL. Our findings demonstrate that BACH captures
injurious activity with a higher granularity in comparison to the existing
metrics. Machine learning models are also used to automate RULA and HAL
scoring, and generalize well to unseen participants. Our assessment system,
therefore, provides ergonomic interpretability of the manufacturing processes
studied, and could be used to mitigate risks through minor workplace
optimization and posture corrections.</div><div><a href='http://arxiv.org/abs/2403.05591v1'>2403.05591v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07132v1")'>A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis
  Degree Classification</div>
<div id='2403.07132v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:02:17Z</div><div>Authors: Gabriel Toshio Hirokawa Higa, Joyce Katiuccia Medeiros Ramos Carvalho, Paolo Brito Pascoalini Zanoni, Gisele Braziliano de Andrade, Hemerson Pistori</div><div style='padding-top: 10px; width: 80ex'>Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a
respiratory disorder that affects the health and welfare of the dogs with
various symptoms. In this paper, a new annotated dataset composed of 190 images
of bulldogs' nostrils is presented. Three degrees of stenosis are approximately
equally represented in the dataset: mild, moderate and severe stenosis. The
dataset also comprises a small quantity of non stenotic nostril images. To the
best of our knowledge, this is the first image dataset addressing this problem.
Furthermore, deep learning is investigated as an alternative to automatically
infer stenosis degree using nostril images. In this work, several neural
networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT.
For this evaluation, the problem was modeled in two different ways: first, as a
three-class classification problem (mild or open, moderate, and severe);
second, as a binary classification problem, with severe stenosis as target. For
the multiclass classification, a maximum median f-score of 53.77\% was achieved
by the MobileNetV3. For binary classification, a maximum median f-score of
72.08\% has been reached by ResNet50, indicating that the problem is
challenging but possibly tractable.</div><div><a href='http://arxiv.org/abs/2403.07132v1'>2403.07132v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.13045v2")'>Assessment of Sports Concussion in Female Athletes: A Role for
  Neuroinformatics?</div>
<div id='2401.13045v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T19:02:13Z</div><div>Authors: Rachel Edelstein, Sterling Gutterman, Benjamin Newman, John Darrell Van Horn</div><div style='padding-top: 10px; width: 80ex'>Over the past decade, the intricacies of sports-related concussions among
female athletes have become readily apparent. Traditional clinical methods for
diagnosing concussions suffer limitations when applied to female athletes,
often failing to capture subtle changes in brain structure and function.
Advanced neuroinformatics techniques and machine learning models have become
invaluable assets in this endeavor. While these technologies have been
extensively employed in understanding concussion in male athletes, there
remains a significant gap in our comprehension of their effectiveness for
female athletes. With its remarkable data analysis capacity, machine learning
offers a promising avenue to bridge this deficit. By harnessing the power of
machine learning, researchers can link observed phenotypic neuroimaging data to
sex-specific biological mechanisms, unraveling the mysteries of concussions in
female athletes. Furthermore, embedding methods within machine learning enable
examining brain architecture and its alterations beyond the conventional
anatomical reference frame. In turn, allows researchers to gain deeper insights
into the dynamics of concussions, treatment responses, and recovery processes.
To guarantee that female athletes receive the optimal care they deserve,
researchers must employ advanced neuroimaging techniques and sophisticated
machine-learning models. These tools enable an in-depth investigation of the
underlying mechanisms responsible for concussion symptoms stemming from
neuronal dysfunction in female athletes. This paper endeavors to address the
crucial issue of sex differences in multimodal neuroimaging experimental design
and machine learning approaches within female athlete populations, ultimately
ensuring that they receive the tailored care they require when facing the
challenges of concussions.</div><div><a href='http://arxiv.org/abs/2401.13045v2'>2401.13045v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08749v1")'>Automated detection of motion artifacts in brain MR images using deep
  learning and explainable artificial intelligence</div>
<div id='2402.08749v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:36:23Z</div><div>Authors: Marina Manso Jimeno, Keerthi Sravan Ravi, Maggie Fung, John Thomas Vaughan, Jr., Sairam Geethanath</div><div style='padding-top: 10px; width: 80ex'>Quality assessment, including inspecting the images for artifacts, is a
critical step during MRI data acquisition to ensure data quality and downstream
analysis or interpretation success. This study demonstrates a deep learning
model to detect rigid motion in T1-weighted brain images. We leveraged a 2D CNN
for three-class classification and tested it on publicly available
retrospective and prospective datasets. Grad-CAM heatmaps enabled the
identification of failure modes and provided an interpretation of the model's
results. The model achieved average precision and recall metrics of 85% and 80%
on six motion-simulated retrospective datasets. Additionally, the model's
classifications on the prospective dataset showed a strong inverse correlation
(-0.84) compared to average edge strength, an image quality metric indicative
of motion. This model is part of the ArtifactID tool, aimed at inline automatic
detection of Gibbs ringing, wrap-around, and motion artifacts. This tool
automates part of the time-consuming QA process and augments expertise on-site,
particularly relevant in low-resource settings where local MR knowledge is
scarce.</div><div><a href='http://arxiv.org/abs/2402.08749v1'>2402.08749v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17410v1")'>A novel image space formalism of Fourier domain interpolation neural
  networks for noise propagation analysis</div>
<div id='2402.17410v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T11:01:58Z</div><div>Authors: Peter Dawood, Felix Breuer, Istvan Homolya, Jannik Stebani, Maximilian Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer</div><div style='padding-top: 10px; width: 80ex'>Purpose: To develop an image space formalism of multi-layer convolutional
neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions
and analytically estimate noise propagation during CNN inference. Theory and
Methods: Nonlinear activations in the Fourier domain (also known as k-space)
using complex-valued Rectifier Linear Units are expressed as elementwise
multiplication with activation masks. This operation is transformed into a
convolution in the image space. After network training in k-space, this
approach provides an algebraic expression for the derivative of the
reconstructed image with respect to the aliased coil images, which serve as the
input tensors to the network in the image space. This allows the variance in
the network inference to be estimated analytically and to be used to describe
noise characteristics. Monte-Carlo simulations and numerical approaches based
on auto-differentiation were used for validation. The framework was tested on
retrospectively undersampled invivo brain images. Results: Inferences conducted
in the image domain are quasi-identical to inferences in the k-space,
underlined by corresponding quantitative metrics. Noise variance maps obtained
from the analytical expression correspond with those obtained via Monte-Carlo
simulations, as well as via an auto-differentiation approach. The noise
resilience is well characterized, as in the case of classical Parallel Imaging.
Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes
in variance maps obtained via Monte-Carlo simulations. Conclusion: The
quasi-equivalent image space formalism for neural networks for k-space
interpolation enables fast and accurate description of the noise
characteristics during CNN inference, analogous to geometry-factor maps in
traditional parallel imaging methods.</div><div><a href='http://arxiv.org/abs/2402.17410v1'>2402.17410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14829v1")'>Hyperbolic Secant representation of the logistic function: Application
  to probabilistic Multiple Instance Learning for CT intracranial hemorrhage
  detection</div>
<div id='2403.14829v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T20:43:34Z</div><div>Authors: F. M. Castro-Macías, P. Morales-Álvarez, Y. Wu, R. Molina, A. K. Katsaggelos</div><div style='padding-top: 10px; width: 80ex'>Multiple Instance Learning (MIL) is a weakly supervised paradigm that has
been successfully applied to many different scientific areas and is
particularly well suited to medical imaging. Probabilistic MIL methods, and
more specifically Gaussian Processes (GPs), have achieved excellent results due
to their high expressiveness and uncertainty quantification capabilities. One
of the most successful GP-based MIL methods, VGPMIL, resorts to a variational
bound to handle the intractability of the logistic function. Here, we formulate
VGPMIL using P\'olya-Gamma random variables. This approach yields the same
variational posterior approximations as the original VGPMIL, which is a
consequence of the two representations that the Hyperbolic Secant distribution
admits. This leads us to propose a general GP-based MIL method that takes
different forms by simply leveraging distributions other than the Hyperbolic
Secant one. Using the Gamma distribution we arrive at a new approach that
obtains competitive or superior predictive performance and efficiency. This is
validated in a comprehensive experimental study including one synthetic MIL
dataset, two well-known MIL benchmarks, and a real-world medical problem. We
expect that this work provides useful ideas beyond MIL that can foster further
research in the field.</div><div><a href='http://arxiv.org/abs/2403.14829v1'>2403.14829v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12344v1")'>OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for
  Generalized and Robust Retinal Disease Detection</div>
<div id='2401.12344v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T20:17:14Z</div><div>Authors: Fatema-E Jannat, Sina Gholami, Minhaj Nur Alam, Hamed Tabkhi</div><div style='padding-top: 10px; width: 80ex'>Despite the revolutionary impact of AI and the development of locally trained
algorithms, achieving widespread generalized learning from multi-modal data in
medical AI remains a significant challenge. This gap hinders the practical
deployment of scalable medical AI solutions. Addressing this challenge, our
research contributes a self-supervised robust machine learning framework,
OCT-SelfNet, for detecting eye diseases using optical coherence tomography
(OCT) images. In this work, various data sets from various institutions are
combined enabling a more comprehensive range of representation. Our method
addresses the issue using a two-phase training approach that combines
self-supervised pretraining and supervised fine-tuning with a mask autoencoder
based on the SwinV2 backbone by providing a solution for real-world clinical
deployment. Extensive experiments on three datasets with different encoder
backbones, low data settings, unseen data settings, and the effect of
augmentation show that our method outperforms the baseline model, Resnet-50 by
consistently attaining AUC-ROC performance surpassing 77% across all tests,
whereas the baseline model exceeds 54%. Moreover, in terms of the AUC-PR
metric, our proposed method exceeded 42%, showcasing a substantial increase of
at least 10% in performance compared to the baseline, which exceeded only 33%.
This contributes to our understanding of our approach's potential and
emphasizes its usefulness in clinical settings.</div><div><a href='http://arxiv.org/abs/2401.12344v1'>2401.12344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09747v1")'>Less is more: Ensemble Learning for Retinal Disease Recognition Under
  Limited Resources</div>
<div id='2402.09747v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T06:58:25Z</div><div>Authors: Jiahao Wang, Hong Peng, Shengchao Chen, Sufen Ren</div><div style='padding-top: 10px; width: 80ex'>Retinal optical coherence tomography (OCT) images provide crucial insights
into the health of the posterior ocular segment. Therefore, the advancement of
automated image analysis methods is imperative to equip clinicians and
researchers with quantitative data, thereby facilitating informed
decision-making. The application of deep learning (DL)-based approaches has
gained extensive traction for executing these analysis tasks, demonstrating
remarkable performance compared to labor-intensive manual analyses. However,
the acquisition of Retinal OCT images often presents challenges stemming from
privacy concerns and the resource-intensive labeling procedures, which
contradicts the prevailing notion that DL models necessitate substantial data
volumes for achieving superior performance. Moreover, limitations in available
computational resources constrain the progress of high-performance medical
artificial intelligence, particularly in less developed regions and countries.
This paper introduces a novel ensemble learning mechanism designed for
recognizing retinal diseases under limited resources (e.g., data, computation).
The mechanism leverages insights from multiple pre-trained models, facilitating
the transfer and adaptation of their knowledge to Retinal OCT images. This
approach establishes a robust model even when confronted with limited labeled
data, eliminating the need for an extensive array of parameters, as required in
learning from scratch. Comprehensive experimentation on real-world datasets
demonstrates that the proposed approach can achieve superior performance in
recognizing Retinal OCT images, even when dealing with exceedingly restricted
labeled datasets. Furthermore, this method obviates the necessity of learning
extensive-scale parameters, making it well-suited for deployment in
low-resource scenarios.</div><div><a href='http://arxiv.org/abs/2402.09747v1'>2402.09747v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07818v1")'>Label Dropout: Improved Deep Learning Echocardiography Segmentation
  Using Multiple Datasets With Domain Shift and Partial Labelling</div>
<div id='2403.07818v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:57:56Z</div><div>Authors: Iman Islam, Esther Puyol-Antón, Bram Ruijsink, Andrew J. Reader, Andrew P. King</div><div style='padding-top: 10px; width: 80ex'>Echocardiography (echo) is the first imaging modality used when assessing
cardiac function. The measurement of functional biomarkers from echo relies
upon the segmentation of cardiac structures and deep learning models have been
proposed to automate the segmentation process. However, in order to translate
these tools to widespread clinical use it is important that the segmentation
models are robust to a wide variety of images (e.g. acquired from different
scanners, by operators with different levels of expertise etc.). To achieve
this level of robustness it is necessary that the models are trained with
multiple diverse datasets. A significant challenge faced when training with
multiple diverse datasets is the variation in label presence, i.e. the combined
data are often partially-labelled. Adaptations of the cross entropy loss
function have been proposed to deal with partially labelled data. In this paper
we show that training naively with such a loss function and multiple diverse
datasets can lead to a form of shortcut learning, where the model associates
label presence with domain characteristics, leading to a drop in performance.
To address this problem, we propose a novel label dropout scheme to break the
link between domain characteristics and the presence or absence of labels. We
demonstrate that label dropout improves echo segmentation Dice score by 62% and
25% on two cardiac structures when training using multiple diverse partially
labelled datasets.</div><div><a href='http://arxiv.org/abs/2403.07818v1'>2403.07818v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13658v1")'>Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics
  Instability Detection</div>
<div id='2403.13658v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T15:06:49Z</div><div>Authors: Mohammod N. I. Suvon, Prasun C. Tripathi, Wenrui Fan, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew J. Swift, Chen Chen, Haiping Lu</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in non-invasive detection of cardiac hemodynamic
instability (CHDI) primarily focus on applying machine learning techniques to a
single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite
their potential, these approaches often fall short especially when the size of
labeled patient data is limited, a common challenge in the medical domain.
Furthermore, only a few studies have explored multimodal methods to study CHDI,
which mostly rely on costly modalities such as cardiac MRI and echocardiogram.
In response to these limitations, we propose a novel multimodal variational
autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray
(CXR) and electrocardiogram (ECG) modalities with pre-training on a large
unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a
novel tri-stream pre-training strategy to learn both shared and
modality-specific features, thus enabling fine-tuning with both unimodal and
multimodal datasets. We pre-train $\text{CardioVAE}_\text{X,G}$ on a large,
unlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then
fine-tune the pre-trained model on a labeled dataset of $795$ subjects from the
ASPIRE registry. Comprehensive evaluations against existing methods show that
$\text{CardioVAE}_\text{X,G}$ offers promising performance (AUROC $=0.79$ and
Accuracy $=0.77$), representing a significant step forward in non-invasive
prediction of CHDI. Our model also excels in producing fine interpretations of
predictions directly associated with clinical features, thereby supporting
clinical decision-making.</div><div><a href='http://arxiv.org/abs/2403.13658v1'>2403.13658v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06024v1")'>Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis
  Diagnosis</div>
<div id='2403.06024v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T22:23:45Z</div><div>Authors: Zhe Huang, Xiaowei Yu, Benjamin S. Wessler, Michael C. Hughes</div><div style='padding-top: 10px; width: 80ex'>Automated interpretation of ultrasound imaging of the heart (echocardiograms)
could improve the detection and treatment of aortic stenosis (AS), a deadly
heart disease. However, existing deep learning pipelines for assessing AS from
echocardiograms have two key limitations. First, most methods rely on limited
2D cineloops, thereby ignoring widely available Doppler imaging that contains
important complementary information about pressure gradients and blood flow
abnormalities associated with AS. Second, obtaining labeled data is difficult.
There are often far more unlabeled echocardiogram recordings available, but
these remain underutilized by existing methods. To overcome these limitations,
we introduce Semi-supervised Multimodal Multiple-Instance Learning (SMMIL), a
new deep learning framework for automatic interpretation for structural heart
diseases like AS. When deployed, SMMIL can combine information from two input
modalities, spectral Dopplers and 2D cineloops, to produce a study-level AS
diagnosis. During training, SMMIL can combine a smaller labeled set and an
abundant unlabeled set of both modalities to improve its classifier.
Experiments demonstrate that SMMIL outperforms recent alternatives at 3-level
AS severity classification as well as several clinically relevant AS detection
tasks.</div><div><a href='http://arxiv.org/abs/2403.06024v1'>2403.06024v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01654v1")'>LESEN: Label-Efficient deep learning for Multi-parametric MRI-based
  Visual Pathway Segmentation</div>
<div id='2401.01654v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T10:22:13Z</div><div>Authors: Alou Diakite, Cheng Li, Lei Xie, Yuanjing Feng, Hua Han, Shanshan Wang</div><div style='padding-top: 10px; width: 80ex'>Recent research has shown the potential of deep learning in multi-parametric
MRI-based visual pathway (VP) segmentation. However, obtaining labeled data for
training is laborious and time-consuming. Therefore, it is crucial to develop
effective algorithms in situations with limited labeled samples. In this work,
we propose a label-efficient deep learning method with self-ensembling (LESEN).
LESEN incorporates supervised and unsupervised losses, enabling the student and
teacher models to mutually learn from each other, forming a self-ensembling
mean teacher framework. Additionally, we introduce a reliable unlabeled sample
selection (RUSS) mechanism to further enhance LESEN's effectiveness. Our
experiments on the human connectome project (HCP) dataset demonstrate the
superior performance of our method when compared to state-of-the-art
techniques, advancing multimodal VP segmentation for comprehensive analysis in
clinical and research settings. The implementation code will be available at:
https://github.com/aldiak/Semi-Supervised-Multimodal-Visual-Pathway-
Delineation.</div><div><a href='http://arxiv.org/abs/2401.01654v1'>2401.01654v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00254v1")'>Cloud-based Federated Learning Framework for MRI Segmentation</div>
<div id='2403.00254v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T03:39:17Z</div><div>Authors: Rukesh Prajapati, Amr S. El-Wakeel</div><div style='padding-top: 10px; width: 80ex'>In contemporary rural healthcare settings, the principal challenge in
diagnosing brain images is the scarcity of available data, given that most of
the existing deep learning models demand extensive training data to optimize
their performance, necessitating centralized processing methods that
potentially compromise data privacy. This paper proposes a novel framework
tailored for brain tissue segmentation in rural healthcare facilities. The
framework employs a deep reinforcement learning (DRL) environment in tandem
with a refinement model (RM) deployed locally at rural healthcare sites. The
proposed DRL model has a reduced parameter count and practicality for
implementation across distributed rural sites. To uphold data privacy and
enhance model generalization without transgressing privacy constraints, we
employ federated learning (FL) for cooperative model training. We demonstrate
the efficacy of our approach by training the network with a limited data set
and observing a substantial performance enhancement, mitigating inaccuracies
and irregularities in segmentation across diverse sites. Remarkably, the DRL
model attains an accuracy of up to 80%, surpassing the capabilities of
conventional convolutional neural networks when confronted with data
insufficiency. Incorporating our RM results in an additional accuracy
improvement of at least 10%, while FL contributes to a further accuracy
enhancement of up to 5%. Collectively, the framework achieves an average 92%
accuracy rate within rural healthcare settings characterized by data
constraints.</div><div><a href='http://arxiv.org/abs/2403.00254v1'>2403.00254v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15434v1")'>Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation
  on multi-parametric MRI</div>
<div id='2401.15434v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T15:05:25Z</div><div>Authors: Jingyun Chen, Yading Yuan</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) enables collaborative model training among medical
centers without sharing private data. However, traditional FL risks on server
failures and suboptimal performance on local data due to the nature of
centralized model aggregation. To address these issues, we present Gossip
Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for
direct peer-to-peer communication. In addition, GML encourages each site to
optimize its local model through mutual learning to account for data variations
among different sites. For the task of tumor segmentation using 146 cases from
four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed
local models and achieved similar performance as FedAvg with only 25%
communication overhead.</div><div><a href='http://arxiv.org/abs/2401.15434v1'>2401.15434v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06180v1")'>Decentralized Gossip Mutual Learning (GML) for automatic head and neck
  tumor segmentation</div>
<div id='2401.06180v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T01:04:30Z</div><div>Authors: Jingyun Chen, Yading Yuan</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL) has emerged as a promising strategy for
collaboratively training complicated machine learning models from different
medical centers without the need of data sharing. However, the traditional FL
relies on a central server to orchestrate the global model training among
clients. This makes it vulnerable to the failure of the model server.
Meanwhile, the model trained based on the global data property may not yield
the best performance on the local data of a particular site due to the
variations of data characteristics among them. To address these limitations, we
proposed Gossip Mutual Learning(GML), a decentralized collaborative learning
framework that employs Gossip Protocol for direct peer-to-peer communication
and encourages each site to optimize its local model by leveraging useful
information from peers through mutual learning. On the task of tumor
segmentation on PET/CT images using HECKTOR21 dataset with 223 cases from five
clinical sites, we demonstrated GML could improve tumor segmentation
performance in terms of Dice Similarity Coefficient (DSC) by 3.2%, 4.6% and
10.4% on site-specific testing cases as compared to three baseline methods:
pooled training, FedAvg and individual training, respectively. We also showed
GML has comparable generalization performance as pooled training and FedAvg
when applying them on 78 cases from two out-of-sample sites where no case was
used for model training. In our experimental setup, GML showcased a sixfold
decrease in communication overhead compared to FedAvg, requiring only 16.67% of
the total communication overhead.</div><div><a href='http://arxiv.org/abs/2401.06180v1'>2401.06180v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12500v1")'>Integrating kNN with Foundation Models for Adaptable and Privacy-Aware
  Image Classification</div>
<div id='2402.12500v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:08:13Z</div><div>Authors: Sebastian Doerrich, Tobias Archut, Francesco Di Salvo, Christian Ledig</div><div style='padding-top: 10px; width: 80ex'>Traditional deep learning models implicity encode knowledge limiting their
transparency and ability to adapt to data changes. Yet, this adaptability is
vital for addressing user data privacy concerns. We address this limitation by
storing embeddings of the underlying training data independently of the model
weights, enabling dynamic data modifications without retraining. Specifically,
our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a
vision-based foundation model, pre-trained self-supervised on natural images,
enhancing interpretability and adaptability. We share open-source
implementations of a previously unpublished baseline method as well as our
performance-improving contributions. Quantitative experiments confirm improved
classification across established benchmark datasets and the method's
applicability to distinct medical image classification tasks. Additionally, we
assess the method's robustness in continual learning and data removal
scenarios. The approach exhibits great promise for bridging the gap between
foundation models' performance and challenges tied to data privacy. The source
code is available at
https://github.com/TobArc/privacy-aware-image-classification-with-kNN.</div><div><a href='http://arxiv.org/abs/2402.12500v1'>2402.12500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01911v1")'>Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A
  Pilot Study on MedCLIP</div>
<div id='2401.01911v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T18:42:19Z</div><div>Authors: Ruinan Jin, Chun-Yin Huang, Chenyu You, Xiaoxiao Li</div><div style='padding-top: 10px; width: 80ex'>In recent years, foundation models (FMs) have solidified their role as
cornerstone advancements in the deep learning domain. By extracting intricate
patterns from vast datasets, these models consistently achieve state-of-the-art
results across a spectrum of downstream tasks, all without necessitating
extensive computational resources. Notably, MedCLIP, a vision-language
contrastive learning-based medical FM, has been designed using unpaired
image-text training. While the medical domain has often adopted unpaired
training to amplify data, the exploration of potential security concerns linked
to this approach hasn't kept pace with its practical usage. Notably, the
augmentation capabilities inherent in unpaired training also indicate that
minor label discrepancies can result in significant model deviations. In this
study, we frame this label discrepancy as a backdoor attack problem. We further
analyze its impact on medical FMs throughout the FM supply chain. Our
evaluation primarily revolves around MedCLIP, emblematic of medical FM
employing the unpaired strategy. We begin with an exploration of
vulnerabilities in MedCLIP stemming from unpaired image-text matching, termed
BadMatch. BadMatch is achieved using a modest set of wrongly labeled data.
Subsequently, we disrupt MedCLIP's contrastive learning through
BadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings
of clean and poisoned data. Additionally, combined with BadMatch and BadDist,
the attacking pipeline consistently fends off backdoor assaults across diverse
model designs, datasets, and triggers. Also, our findings reveal that current
defense strategies are insufficient in detecting these latent threats in
medical FMs' supply chains.</div><div><a href='http://arxiv.org/abs/2401.01911v1'>2401.01911v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01183v1")'>Leveraging Self-Supervised Learning for Scene Recognition in Child
  Sexual Abuse Imagery</div>
<div id='2403.01183v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T11:44:14Z</div><div>Authors: Pedro H. V. Valois, João Macedo, Leo S. F. Ribeiro, Jefersson A. dos Santos, Sandra Avila</div><div style='padding-top: 10px; width: 80ex'>Crime in the 21st century is split into a virtual and real world. However,
the former has become a global menace to people's well-being and security in
the latter. The challenges it presents must be faced with unified global
cooperation, and we must rely more than ever on automated yet trustworthy tools
to combat the ever-growing nature of online offenses. Over 10 million child
sexual abuse reports are submitted to the US National Center for Missing &amp;
Exploited Children every year, and over 80% originated from online sources.
Therefore, investigation centers and clearinghouses cannot manually process and
correctly investigate all imagery. In light of that, reliable automated tools
that can securely and efficiently deal with this data are paramount. In this
sense, the scene recognition task looks for contextual cues in the environment,
being able to group and classify child sexual abuse data without requiring to
be trained on sensitive material. The scarcity and limitations of working with
child sexual abuse images lead to self-supervised learning, a machine-learning
methodology that leverages unlabeled data to produce powerful representations
that can be more easily transferred to target tasks. This work shows that
self-supervised deep learning models pre-trained on scene-centric data can
reach 71.6% balanced accuracy on our indoor scene classification task and, on
average, 2.2 percentage points better performance than a fully supervised
version. We cooperate with Brazilian Federal Police experts to evaluate our
indoor classification model on actual child abuse material. The results
demonstrate a notable discrepancy between the features observed in widely used
scene datasets and those depicted on sensitive materials.</div><div><a href='http://arxiv.org/abs/2403.01183v1'>2403.01183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03849v2")'>MedMamba: Vision Mamba for Medical Image Classification</div>
<div id='2403.03849v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:49:33Z</div><div>Authors: Yubiao Yue, Zhenzhang Li</div><div style='padding-top: 10px; width: 80ex'>Medical image classification is a very fundamental and crucial task in the
field of computer vision. These years, CNN-based and Transformer-based models
have been widely used to classify various medical images. Unfortunately, The
limitation of CNNs in long-range modeling capabilities prevents them from
effectively extracting features in medical images, while Transformers are
hampered by their quadratic computational complexity. Recent research has shown
that the state space model (SSM) represented by Mamba can efficiently model
long-range interactions while maintaining linear computational complexity.
Inspired by this, we propose Vision Mamba for medical image classification
(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM
combines the local feature extraction ability of convolutional layers with the
ability of SSM to capture long-range dependency, thereby modeling medical
images with different modalities. To demonstrate the potential of MedMamba, we
conducted extensive experiments using 14 publicly available medical datasets
with different imaging techniques and two private datasets built by ourselves.
Extensive experimental results demonstrate that the proposed MedMamba performs
well in detecting lesions in various medical images. To the best of our
knowledge, this is the first Vision Mamba tailored for medical image
classification. The purpose of this work is to establish a new baseline for
medical image classification tasks and provide valuable insights for the future
development of more efficient and effective SSM-based artificial intelligence
algorithms and application systems in the medical. Source code has been
available at https://github.com/YubiaoYue/MedMamba.</div><div><a href='http://arxiv.org/abs/2403.03849v2'>2403.03849v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02274v1")'>InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention
  for medical image Classification</div>
<div id='2402.02274v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T22:04:22Z</div><div>Authors: Elham Sadeghnezhad, Sajjad Salem</div><div style='padding-top: 10px; width: 80ex'>Initial weighting is significant in deep neural networks because the random
selection of weights produces different outputs and increases the probability
of overfitting and underfitting. On the other hand, vector-based approaches to
extract vector features need rich vectors for more accurate classification. The
InceptionCapsule approach is presented to alleviate these two problems. This
approach uses transfer learning and the Inception-ResNet model to avoid random
selection of weights, which takes initial weights from ImageNet. It also uses
the output of Inception middle layers to generate rich vectors. Extracted
vectors are given to a capsule network for learning, which is equipped with an
attention technique. Kvasir data and BUSI with the GT dataset were used to
evaluate this approach. This model was able to achieve 97.62 accuracies in
5-class classification and also achieved 94.30 accuracies in 8-class
classification on Kvasir. In the BUSI with GT dataset, the proposed approach
achieved accuracy=98.88, Precision=95.34, and F1-score=93.74, which are
acceptable results compared to other approaches in the literature.</div><div><a href='http://arxiv.org/abs/2402.02274v1'>2402.02274v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00692v1")'>Self-supervised learning for skin cancer diagnosis with limited training
  data</div>
<div id='2401.00692v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T08:11:38Z</div><div>Authors: Hamish Haggerty, Rohitash Chandra</div><div style='padding-top: 10px; width: 80ex'>Cancer diagnosis is a well-studied problem in machine learning since early
detection of cancer is often the determining factor in prognosis. Supervised
deep learning achieves excellent results in cancer image classification,
usually through transfer learning. However, these models require large amounts
of labelled data and for several types of cancer, large labelled datasets do
not exist. In this paper, we demonstrate that a model pre-trained using a
self-supervised learning algorithm known as Barlow Twins can outperform the
conventional supervised transfer learning pipeline. We juxtapose two base
models: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a
self-supervised fashion on ImageNet. Both are subsequently fine tuned on a
small labelled skin lesion dataset and evaluated on a large test set. We
achieve a mean test accuracy of 70\% for self-supervised transfer in comparison
to 66\% for supervised transfer. Interestingly, boosting performance further is
possible by self-supervised pretraining a second time (on unlabelled skin
lesion images) before subsequent fine tuning. This hints at an alternative path
to collecting more labelled data in settings where this is challenging - namely
just collecting more unlabelled images. Our framework is applicable to cancer
image classification models in the low-labelled data regime.</div><div><a href='http://arxiv.org/abs/2401.00692v1'>2401.00692v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14497v1")'>Investigating the Quality of DermaMNIST and Fitzpatrick17k
  Dermatological Image Datasets</div>
<div id='2401.14497v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:29:01Z</div><div>Authors: Kumar Abhishek, Aditi Jain, Ghassan Hamarneh</div><div style='padding-top: 10px; width: 80ex'>The remarkable progress of deep learning in dermatological tasks has brought
us closer to achieving diagnostic accuracies comparable to those of human
experts. However, while large datasets play a crucial role in the development
of reliable deep neural network models, the quality of data therein and their
correct usage are of paramount importance. Several factors can impact data
quality, such as the presence of duplicates, data leakage across train-test
partitions, mislabeled images, and the absence of a well-defined test
partition. In this paper, we conduct meticulous analyses of two popular
dermatological image datasets: DermaMNIST and Fitzpatrick17k, uncovering these
data quality issues, measure the effects of these problems on the benchmark
results, and propose corrections to the datasets. Besides ensuring the
reproducibility of our analysis, by making our analysis pipeline and the
accompanying code publicly available, we aim to encourage similar explorations
and to facilitate the identification and addressing of potential data quality
issues in other large datasets.</div><div><a href='http://arxiv.org/abs/2401.14497v1'>2401.14497v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15566v1")'>Closing the AI generalization gap by adjusting for dermatology condition
  distribution differences across clinical settings</div>
<div id='2402.15566v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T19:07:53Z</div><div>Authors: Rajeev V. Rikhye, Aaron Loh, Grace Eunhae Hong, Preeti Singh, Margaret Ann Smith, Vijaytha Muralidharan, Doris Wong, Rory Sayres, Michelle Phung, Nicolas Betancourt, Bradley Fong, Rachna Sahasrabudhe, Khoban Nasim, Alec Eschholz, Basil Mustafa, Jan Freyberg, Terry Spitz, Yossi Matias, Greg S. Corrado, Katherine Chou, Dale R. Webster, Peggy Bui, Yuan Liu, Yun Liu, Justin Ko, Steven Lin</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been great progress in the ability of artificial
intelligence (AI) algorithms to classify dermatological conditions from
clinical photographs. However, little is known about the robustness of these
algorithms in real-world settings where several factors can lead to a loss of
generalizability. Understanding and overcoming these limitations will permit
the development of generalizable AI that can aid in the diagnosis of skin
conditions across a variety of clinical settings. In this retrospective study,
we demonstrate that differences in skin condition distribution, rather than in
demographics or image capture mode are the main source of errors when an AI
algorithm is evaluated on data from a previously unseen source. We demonstrate
a series of steps to close this generalization gap, requiring progressively
more information about the new source, ranging from the condition distribution
to training data enriched for data less frequently seen during training. Our
results also suggest comparable performance from end-to-end fine tuning versus
fine tuning solely the classification layer on top of a frozen embedding model.
Our approach can inform the adaptation of AI algorithms to new settings, based
on the information and resources available.</div><div><a href='http://arxiv.org/abs/2402.15566v1'>2402.15566v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08592v1")'>Convolutional Neural Networks Towards Facial Skin Lesions Detection</div>
<div id='2402.08592v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:52:10Z</div><div>Authors: Reza Sarshar, Mohammad Heydari, Elham Akhondzadeh Noughabi</div><div style='padding-top: 10px; width: 80ex'>Facial analysis has emerged as a prominent area of research with diverse
applications, including cosmetic surgery programs, the beauty industry,
photography, and entertainment. Manipulating patient images often necessitates
professional image processing software. This study contributes by providing a
model that facilitates the detection of blemishes and skin lesions on facial
images through a convolutional neural network and machine learning approach.
The proposed method offers advantages such as simple architecture, speed and
suitability for image processing while avoiding the complexities associated
with traditional methods. The model comprises four main steps: area selection,
scanning the chosen region, lesion diagnosis, and marking the identified
lesion. Raw data for this research were collected from a reputable clinic in
Tehran specializing in skincare and beauty services. The dataset includes
administrative information, clinical data, and facial and profile images. A
total of 2300 patient images were extracted from this raw data. A software tool
was developed to crop and label lesions, with input from two treatment experts.
In the lesion preparation phase, the selected area was standardized to 50 * 50
pixels. Subsequently, a convolutional neural network model was employed for
lesion labeling. The classification model demonstrated high accuracy, with a
measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity.
Internal validation involved performance indicators and cross-validation, while
external validation compared the model's performance indicators with those of
the transfer learning method using the Vgg16 deep network model. Compared to
existing studies, the results of this research showcase the efficacy and
desirability of the proposed model and methodology.</div><div><a href='http://arxiv.org/abs/2402.08592v1'>2402.08592v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04746v1")'>Skin Cancer Segmentation and Classification Using Vision Transformer for
  Automatic Analysis in Dermatoscopy-based Non-invasive Digital System</div>
<div id='2401.04746v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T11:22:54Z</div><div>Authors: Galib Muhammad Shahriar Himel, Md. Masudul Islam, Kh Abdullah Al-Aff, Shams Ibne Karim, Md. Kabir Uddin Sikder</div><div style='padding-top: 10px; width: 80ex'>Skin cancer is a global health concern, necessitating early and accurate
diagnosis for improved patient outcomes. This study introduces a groundbreaking
approach to skin cancer classification, employing the Vision Transformer, a
state-of-the-art deep learning architecture renowned for its success in diverse
image analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulously
annotated skin lesion images, the model undergoes preprocessing for enhanced
robustness. The Vision Transformer, adapted to the skin cancer classification
task, leverages the self-attention mechanism to capture intricate spatial
dependencies, achieving superior performance over traditional deep learning
architectures. Segment Anything Model aids in precise segmentation of cancerous
areas, attaining high IOU and Dice Coefficient. Extensive experiments highlight
the model's supremacy, particularly the Google-based ViT patch-32 variant,
which achieves 96.15% accuracy and showcases potential as an effective tool for
dermatologists in skin cancer diagnosis, contributing to advancements in
dermatological practices.</div><div><a href='http://arxiv.org/abs/2401.04746v1'>2401.04746v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08272v1")'>Siamese Content-based Search Engine for a More Transparent Skin and
  Breast Cancer Diagnosis through Histological Imaging</div>
<div id='2401.08272v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T10:51:55Z</div><div>Authors: Zahra Tabatabaei, Adrián Colomer, JAvier Oliver Moll, Valery Naranjo</div><div style='padding-top: 10px; width: 80ex'>Computer Aid Diagnosis (CAD) has developed digital pathology with Deep
Learning (DL)-based tools to assist pathologists in decision-making.
Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek
highly correlated patches in terms of similarity in histopathological features.
In this work, we proposed two CBHIR approaches on breast (Breast-twins) and
skin cancer (Skin-twins) data sets for robust and accurate patch-level
retrieval, integrating a custom-built Siamese network as a feature extractor.
The proposed Siamese network is able to generalize for unseen images by
focusing on the similar histopathological features of the input pairs. The
proposed CBHIR approaches are evaluated on the Breast (public) and Skin
(private) data sets with top K accuracy. Finding the optimum amount of K is
challenging, but also, as much as K increases, the dissimilarity between the
query and the returned images increases which might mislead the pathologists.
To the best of the author's belief, this paper is tackling this issue for the
first time on histopathological images by evaluating the top first retrieved
images. The Breast-twins model achieves 70% of the F1score at the top first,
which exceeds the other state-of-the-art methods at a higher amount of K such
as 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto
Encoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model
tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential
(STUMP) to assist pathologists with retrieving top K images and their
corresponding labels. So, this approach can offer a more explainable CAD tool
to pathologists in terms of transparency, trustworthiness, or reliability among
other characteristics.</div><div><a href='http://arxiv.org/abs/2401.08272v1'>2401.08272v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01410v1")'>XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision</div>
<div id='2402.01410v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:42:45Z</div><div>Authors: Miguel Correia, Alceu Bissoto, Carlos Santiago, Catarina Barata</div><div style='padding-top: 10px; width: 80ex'>Skin cancer detection through dermoscopy image analysis is a critical task.
However, existing models used for this purpose often lack interpretability and
reliability, raising the concern of physicians due to their black-box nature.
In this paper, we propose a novel approach for the diagnosis of melanoma using
an interpretable prototypical-part model. We introduce a guided supervision
based on non-expert feedback through the incorporation of: 1) binary masks,
obtained automatically using a segmentation network; and 2) user-refined
prototypes. These two distinct information pathways aim to ensure that the
learned prototypes correspond to relevant areas within the skin lesion,
excluding confounding factors beyond its boundaries. Experimental results
demonstrate that, even without expert supervision, our approach achieves
superior performance and generalization compared to non-interpretable models.</div><div><a href='http://arxiv.org/abs/2402.01410v1'>2402.01410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08789v1")'>Bridging Human Concepts and Computer Vision for Explainable Face
  Verification</div>
<div id='2403.08789v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T09:13:49Z</div><div>Authors: Miriam Doh, Caroline Mazini Rodrigues, Nicolas Boutry, Laurent Najman, Matei Mancas, Hugues Bersini</div><div style='padding-top: 10px; width: 80ex'>With Artificial Intelligence (AI) influencing the decision-making process of
sensitive applications such as Face Verification, it is fundamental to ensure
the transparency, fairness, and accountability of decisions. Although
Explainable Artificial Intelligence (XAI) techniques exist to clarify AI
decisions, it is equally important to provide interpretability of these
decisions to humans. In this paper, we present an approach to combine computer
and human vision to increase the explanation's interpretability of a face
verification algorithm. In particular, we are inspired by the human perceptual
process to understand how machines perceive face's human-semantic areas during
face comparison tasks. We use Mediapipe, which provides a segmentation
technique that identifies distinct human-semantic facial regions, enabling the
machine's perception analysis. Additionally, we adapted two model-agnostic
algorithms to provide human-interpretable insights into the decision-making
processes.</div><div><a href='http://arxiv.org/abs/2403.08789v1'>2403.08789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07940v1")'>Hair and scalp disease detection using deep learning</div>
<div id='2403.07940v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T04:49:40Z</div><div>Authors: Kavita Sultanpure, Bhairavi Shirsath, Bhakti Bhande, Harshada Sawai, Srushti Gawade, Suraj Samgir</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a notable advancement in the integration of
healthcare and technology, particularly evident in the field of medical image
analysis. This paper introduces a pioneering approach in dermatology,
presenting a robust method for the detection of hair and scalp diseases using
state-of-the-art deep learning techniques. Our methodology relies on
Convolutional Neural Networks (CNNs), well-known for their efficacy in image
recognition, to meticulously analyze images for various dermatological
conditions affecting the hair and scalp. Our proposed system represents a
significant advancement in dermatological diagnostics, offering a non-invasive
and highly efficient means of early detection and diagnosis. By leveraging the
capabilities of CNNs, our model holds the potential to revolutionize
dermatology, providing accessible and timely healthcare solutions. Furthermore,
the seamless integration of our trained model into a web-based platform
developed with the Django framework ensures broad accessibility and usability,
democratizing advanced medical diagnostics. The integration of machine learning
algorithms into web applications marks a pivotal moment in healthcare delivery,
promising empowerment for both healthcare providers and patients. Through the
synergy between technology and healthcare, our paper outlines the meticulous
methodology, technical intricacies, and promising future prospects of our
system. With a steadfast commitment to advancing healthcare frontiers, our goal
is to significantly contribute to leveraging technology for improved healthcare
outcomes globally. This endeavor underscores the profound impact of
technological innovation in shaping the future of healthcare delivery and
patient care, highlighting the transformative potential of our approach.</div><div><a href='http://arxiv.org/abs/2403.07940v1'>2403.07940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02246v1")'>ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from
  Invoice Images</div>
<div id='2402.02246v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T19:24:45Z</div><div>Authors: Adem Akdoğan, Murat Kurt</div><div style='padding-top: 10px; width: 80ex'>In this work, product tables in invoices are obtained autonomously via a deep
learning model, which is named as ExTTNet. Firstly, text is obtained from
invoice images using Optical Character Recognition (OCR) techniques. Tesseract
OCR engine [37] is used for this process. Afterwards, the number of existing
features is increased by using feature extraction methods to increase the
accuracy. Labeling process is done according to whether each text obtained as a
result of OCR is a table element or not. In this study, a multilayer artificial
neural network model is used. The training has been carried out with an Nvidia
RTX 3090 graphics card and taken $162$ minutes. As a result of the training,
the F1 score is $0.92$.</div><div><a href='http://arxiv.org/abs/2402.02246v1'>2402.02246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14898v1")'>Web-based Melanoma Detection</div>
<div id='2403.14898v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T01:04:51Z</div><div>Authors: SangHyuk Kim, Edward Gaibor, Daniel Haehn</div><div style='padding-top: 10px; width: 80ex'>Melanoma is the most aggressive form of skin cancer, and early detection can
significantly increase survival rates and prevent cancer spread. However,
developing reliable automated detection techniques is difficult due to the lack
of standardized datasets and evaluation methods. This study introduces a
unified melanoma classification approach that supports 54 combinations of 11
datasets and 24 state-of-the-art deep learning architectures. It enables a fair
comparison of 1,296 experiments and results in a lightweight model deployable
to the web-based MeshNet architecture named Mela-D. This approach can run up to
33x faster by reducing parameters 24x to yield an analogous 88.8\% accuracy
comparable with ResNet50 on previously unseen images. This allows efficient and
accurate melanoma detection in real-world settings that can run on
consumer-level hardware.</div><div><a href='http://arxiv.org/abs/2403.14898v1'>2403.14898v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07118v1")'>Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding
  Remote Smartphone-based Consultation</div>
<div id='2402.07118v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T07:27:01Z</div><div>Authors: Dhruv Srikanth, Jayang Gurung, N Satya Deepika, Vineet Joshi, Pravin Vaddavalli, Soumya Jana</div><div style='padding-top: 10px; width: 80ex'>Blindness and other eye diseases are a global health concern, particularly in
low- and middle-income countries like India. In this regard, during the
COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi
attachment for smartphone-based eye imaging gained in use. However, quality of
user-captured image often remained inadequate, requiring clinician vetting and
delays. In this backdrop, we propose an AI-based quality assessment system with
instant feedback mimicking clinicians' judgments and tested on patient-captured
images. Dividing the complex problem hierarchically, here we tackle a
nontrivial part, and demonstrate a proof of the concept.</div><div><a href='http://arxiv.org/abs/2402.07118v1'>2402.07118v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13241v1")'>Tackling Noisy Labels with Network Parameter Additive Decomposition</div>
<div id='2403.13241v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T02:11:28Z</div><div>Authors: Jingyi Wang, Xiaobo Xia, Long Lan, Xinghao Wu, Jun Yu, Wenjing Yang, Bo Han, Tongliang Liu</div><div style='padding-top: 10px; width: 80ex'>Given data with noisy labels, over-parameterized deep networks suffer
overfitting mislabeled data, resulting in poor generalization. The memorization
effect of deep networks shows that although the networks have the ability to
memorize all noisy data, they would first memorize clean training data, and
then gradually memorize mislabeled training data. A simple and effective method
that exploits the memorization effect to combat noisy labels is early stopping.
However, early stopping cannot distinguish the memorization of clean data and
mislabeled data, resulting in the network still inevitably overfitting
mislabeled data in the early training stage.In this paper, to decouple the
memorization of clean data and mislabeled data, and further reduce the side
effect of mislabeled data, we perform additive decomposition on network
parameters. Namely, all parameters are additively decomposed into two groups,
i.e., parameters $\mathbf{w}$ are decomposed as
$\mathbf{w}=\bm{\sigma}+\bm{\gamma}$. Afterward, the parameters $\bm{\sigma}$
are considered to memorize clean data, while the parameters $\bm{\gamma}$ are
considered to memorize mislabeled data. Benefiting from the memorization
effect, the updates of the parameters $\bm{\sigma}$ are encouraged to fully
memorize clean data in early training, and then discouraged with the increase
of training epochs to reduce interference of mislabeled data. The updates of
the parameters $\bm{\gamma}$ are the opposite. In testing, only the parameters
$\bm{\sigma}$ are employed to enhance generalization. Extensive experiments on
both simulated and real-world benchmarks confirm the superior performance of
our method.</div><div><a href='http://arxiv.org/abs/2403.13241v1'>2403.13241v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11242v1")'>Learning with Imbalanced Noisy Data by Preventing Bias in Sample
  Selection</div>
<div id='2402.11242v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T10:34:53Z</div><div>Authors: Huafeng Liu, Mengmeng Sheng, Zeren Sun, Yazhou Yao, Xian-Sheng Hua, Heng-Tao Shen</div><div style='padding-top: 10px; width: 80ex'>Learning with noisy labels has gained increasing attention because the
inevitable imperfect labels in real-world scenarios can substantially hurt the
deep model performance. Recent studies tend to regard low-loss samples as clean
ones and discard high-loss ones to alleviate the negative impact of noisy
labels. However, real-world datasets contain not only noisy labels but also
class imbalance. The imbalance issue is prone to causing failure in the
loss-based sample selection since the under-learning of tail classes also leans
to produce high losses. To this end, we propose a simple yet effective method
to address noisy labels in imbalanced datasets. Specifically, we propose
Class-Balance-based sample Selection (CBS) to prevent the tail class samples
from being neglected during training. We propose Confidence-based Sample
Augmentation (CSA) for the chosen clean samples to enhance their reliability in
the training process. To exploit selected noisy samples, we resort to
prediction history to rectify labels of noisy samples. Moreover, we introduce
the Average Confidence Margin (ACM) metric to measure the quality of corrected
labels by leveraging the model's evolving training dynamics, thereby ensuring
that low-quality corrected noisy samples are appropriately masked out. Lastly,
consistency regularization is imposed on filtered label-corrected noisy samples
to boost model performance. Comprehensive experimental results on synthetic and
real-world datasets demonstrate the effectiveness and superiority of our
proposed method, especially in imbalanced scenarios. Comprehensive experimental
results on synthetic and real-world datasets demonstrate the effectiveness and
superiority of our proposed method, especially in imbalanced scenarios.</div><div><a href='http://arxiv.org/abs/2402.11242v1'>2402.11242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13360v2")'>Debiased Sample Selection for Combating Noisy Labels</div>
<div id='2401.13360v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T10:37:28Z</div><div>Authors: Qi Wei, Lei Feng, Haobo Wang, Bo An</div><div style='padding-top: 10px; width: 80ex'>Learning with noisy labels aims to ensure model generalization given a
label-corrupted training set. The sample selection strategy achieves promising
performance by selecting a label-reliable subset for model training. In this
paper, we empirically reveal that existing sample selection methods suffer from
both data and training bias that are represented as imbalanced selected sets
and accumulation errors in practice, respectively. However, only the training
bias was handled in previous studies. To address this limitation, we propose a
noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection.
Specifically, to mitigate the training bias, we design a robust network
architecture that integrates with multiple experts. Compared with the
prevailing double-branch network, our network exhibits better performance of
selection and prediction by ensembling these experts while training with fewer
parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling
strategy based on two weight-based data samplers. By training on the mixture of
two class-discriminative mini-batches, the model mitigates the effect of the
imbalanced training set while avoiding sparse representations that are easily
caused by sampling strategies. Extensive experiments and analyses demonstrate
the effectiveness of ITEM. Our code is available at this url
\href{https://github.com/1998v7/ITEM}{ITEM}.</div><div><a href='http://arxiv.org/abs/2401.13360v2'>2401.13360v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07062v1")'>Dirichlet-Based Prediction Calibration for Learning with Noisy Labels</div>
<div id='2401.07062v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T12:33:04Z</div><div>Authors: Chen-Chen Zong, Ye-Wen Wang, Ming-Kun Xie, Sheng-Jun Huang</div><div style='padding-top: 10px; width: 80ex'>Learning with noisy labels can significantly hinder the generalization
performance of deep neural networks (DNNs). Existing approaches address this
issue through loss correction or example selection methods. However, these
methods often rely on the model's predictions obtained from the softmax
function, which can be over-confident and unreliable. In this study, we
identify the translation invariance of the softmax function as the underlying
cause of this problem and propose the \textit{Dirichlet-based Prediction
Calibration} (DPC) method as a solution. Our method introduces a calibrated
softmax function that breaks the translation invariance by incorporating a
suitable constant in the exponent term, enabling more reliable model
predictions. To ensure stable model training, we leverage a Dirichlet
distribution to assign probabilities to predicted labels and introduce a novel
evidence deep learning (EDL) loss. The proposed loss function encourages
positive and sufficiently large logits for the given label, while penalizing
negative and small logits for other labels, leading to more distinct logits and
facilitating better example selection based on a large-margin criterion.
Through extensive experiments on diverse benchmark datasets, we demonstrate
that DPC achieves state-of-the-art performance. The code is available at
https://github.com/chenchenzong/DPC.</div><div><a href='http://arxiv.org/abs/2401.07062v1'>2401.07062v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01346v1")'>Improve Cost Efficiency of Active Learning over Noisy Dataset</div>
<div id='2403.01346v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T23:53:24Z</div><div>Authors: Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng</div><div style='padding-top: 10px; width: 80ex'>Active learning is a learning strategy whereby the machine learning algorithm
actively identifies and labels data points to optimize its learning. This
strategy is particularly effective in domains where an abundance of unlabeled
data exists, but the cost of labeling these data points is prohibitively
expensive. In this paper, we consider cases of binary classification, where
acquiring a positive instance incurs a significantly higher cost compared to
that of negative instances. For example, in the financial industry, such as in
money-lending businesses, a defaulted loan constitutes a positive event leading
to substantial financial loss. To address this issue, we propose a shifted
normal distribution sampling function that samples from a wider range than
typical uncertainty sampling. Our simulation underscores that our proposed
sampling function limits both noisy and positive label selection, delivering
between 20% and 32% improved cost efficiency over different test datasets.</div><div><a href='http://arxiv.org/abs/2403.01346v1'>2403.01346v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01352v1")'>Improving Uncertainty Sampling with Bell Curve Weight Function</div>
<div id='2403.01352v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T00:14:12Z</div><div>Authors: Zan-Kai Chong, Hiroyuki Ohsaki, Bok-Min Goi</div><div style='padding-top: 10px; width: 80ex'>Typically, a supervised learning model is trained using passive learning by
randomly selecting unlabelled instances to annotate. This approach is effective
for learning a model, but can be costly in cases where acquiring labelled
instances is expensive. For example, it can be time-consuming to manually
identify spam mails (labelled instances) from thousands of emails (unlabelled
instances) flooding an inbox during initial data collection. Generally, we
answer the above scenario with uncertainty sampling, an active learning method
that improves the efficiency of supervised learning by using fewer labelled
instances than passive learning. Given an unlabelled data pool, uncertainty
sampling queries the labels of instances where the predicted probabilities, p,
fall into the uncertainty region, i.e., $p \approx 0.5$. The newly acquired
labels are then added to the existing labelled data pool to learn a new model.
Nonetheless, the performance of uncertainty sampling is susceptible to the area
of unpredictable responses (AUR) and the nature of the dataset. It is difficult
to determine whether to use passive learning or uncertainty sampling without
prior knowledge of a new dataset. To address this issue, we propose bell curve
sampling, which employs a bell curve weight function to acquire new labels.
With the bell curve centred at p=0.5, bell curve sampling selects instances
whose predicted values are in the uncertainty area most of the time without
neglecting the rest. Simulation results show that, most of the time bell curve
sampling outperforms uncertainty sampling and passive learning in datasets of
different natures and with AUR.</div><div><a href='http://arxiv.org/abs/2403.01352v1'>2403.01352v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02591v1")'>Synthetic Information towards Maximum Posterior Ratio for deep learning
  on Imbalanced Data</div>
<div id='2401.02591v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T01:08:26Z</div><div>Authors: Hung Nguyen, Morris Chang</div><div style='padding-top: 10px; width: 80ex'>This study examines the impact of class-imbalanced data on deep learning
models and proposes a technique for data balancing by generating synthetic data
for the minority class. Unlike random-based oversampling, our method
prioritizes balancing the informative regions by identifying high entropy
samples. Generating well-placed synthetic data can enhance machine learning
algorithms accuracy and efficiency, whereas poorly-placed ones may lead to
higher misclassification rates. We introduce an algorithm that maximizes the
probability of generating a synthetic sample in the correct region of its class
by optimizing the class posterior ratio. Additionally, to maintain data
topology, synthetic data are generated within each minority sample's
neighborhood. Our experimental results on forty-one datasets demonstrate the
superior performance of our technique in enhancing deep-learning models.</div><div><a href='http://arxiv.org/abs/2401.02591v1'>2401.02591v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05918v2")'>SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to
  Imbalanced Data</div>
<div id='2403.05918v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T14:01:04Z</div><div>Authors: Ming Zheng, Yang Yang, Zhi-Hang Zhao, Shan-Chao Gan, Yang Chen, Si-Kai Ni, Yang Lu</div><div style='padding-top: 10px; width: 80ex'>In the field of data mining and machine learning, commonly used
classification models cannot effectively learn in unbalanced data. In order to
balance the data distribution before model training, oversampling methods are
often used to generate data for a small number of classes to solve the problem
of classifying unbalanced data. Most of the classical oversampling methods are
based on the SMOTE technique, which only focuses on the local information of
the data, and therefore the generated data may have the problem of not being
realistic enough. In the current oversampling methods based on generative
networks, the methods based on GANs can capture the true distribution of data,
but there is the problem of pattern collapse and training instability in
training; in the oversampling methods based on denoising diffusion probability
models, the neural network of the inverse diffusion process using the U-Net is
not applicable to tabular data, and although the MLP can be used to replace the
U-Net, the problem exists due to the simplicity of the structure and the poor
effect of removing noise. problem of poor noise removal. In order to overcome
the above problems, we propose a novel oversampling method SEMRes-DDPM.In the
SEMRes-DDPM backward diffusion process, a new neural network structure
SEMST-ResNet is used, which is suitable for tabular data and has good noise
removal effect, and it can generate tabular data with higher quality.
Experiments show that the SEMResNet network removes noise better than MLP;
SEMRes-DDPM generates data distributions that are closer to the real data
distributions than TabDDPM with CWGAN-GP; on 20 real unbalanced tabular
datasets with 9 classification models, SEMRes-DDPM improves the quality of the
generated tabular data in terms of three evaluation metrics (F1, G-mean, AUC)
with better classification performance than other SOTA oversampling methods.</div><div><a href='http://arxiv.org/abs/2403.05918v2'>2403.05918v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10658v1")'>InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance
  Semi-Supervised Learning</div>
<div id='2403.10658v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T19:54:10Z</div><div>Authors: Zhe Huang, Xiaowei Yu, Dajiang Zhu, Michael C. Hughes</div><div style='padding-top: 10px; width: 80ex'>Semi-supervised learning (SSL) seeks to enhance task performance by training
on both labeled and unlabeled data. Mainstream SSL image classification methods
mostly optimize a loss that additively combines a supervised classification
objective with a regularization term derived solely from unlabeled data. This
formulation neglects the potential for interaction between labeled and
unlabeled images. In this paper, we introduce InterLUDE, a new approach to
enhance SSL made of two parts that each benefit from labeled-unlabeled
interaction. The first part, embedding fusion, interpolates between labeled and
unlabeled embeddings to improve representation learning. The second part is a
new loss, grounded in the principle of consistency regularization, that aims to
minimize discrepancies in the model's predictions between labeled versus
unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a
medical SSL task with an uncurated unlabeled set show clear benefits to our
approach. On the STL-10 dataset with only 40 labels, InterLUDE achieves 3.2%
error rate, while the best previous method reports 14.9%.</div><div><a href='http://arxiv.org/abs/2403.10658v1'>2403.10658v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10458v1")'>Contrastive Unlearning: A Contrastive Approach to Machine Unlearning</div>
<div id='2401.10458v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T02:16:30Z</div><div>Authors: Hong kyu Lee, Qiuchen Zhang, Carl Yang, Jian Lou, Li Xiong</div><div style='padding-top: 10px; width: 80ex'>Machine unlearning aims to eliminate the influence of a subset of training
samples (i.e., unlearning samples) from a trained model. Effectively and
efficiently removing the unlearning samples without negatively impacting the
overall model performance is still challenging. In this paper, we propose a
contrastive unlearning framework, leveraging the concept of representation
learning for more effective unlearning. It removes the influence of unlearning
samples by contrasting their embeddings against the remaining samples so that
they are pushed away from their original classes and pulled toward other
classes. By directly optimizing the representation space, it effectively
removes the influence of unlearning samples while maintaining the
representations learned from the remaining samples. Experiments on a variety of
datasets and models on both class unlearning and sample unlearning showed that
contrastive unlearning achieves the best unlearning effects and efficiency with
the lowest performance loss compared with the state-of-the-art algorithms.</div><div><a href='http://arxiv.org/abs/2401.10458v1'>2401.10458v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12459v1")'>Non-negative Contrastive Learning</div>
<div id='2403.12459v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T05:30:50Z</div><div>Authors: Yifei Wang, Qi Zhang, Yaoyu Guo, Yisen Wang</div><div style='padding-top: 10px; width: 80ex'>Deep representations have shown promising performance when transferred to
downstream tasks in a black-box manner. Yet, their inherent lack of
interpretability remains a significant challenge, as these features are often
opaque to human understanding. In this paper, we propose Non-negative
Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization
(NMF) aimed at deriving interpretable features. The power of NCL lies in its
enforcement of non-negativity constraints on features, reminiscent of NMF's
capability to extract features that align closely with sample clusters. NCL not
only aligns mathematically well with an NMF objective but also preserves NMF's
interpretability attributes, resulting in a more sparse and disentangled
representation compared to standard contrastive learning (CL). Theoretically,
we establish guarantees on the identifiability and downstream generalization of
NCL. Empirically, we show that these advantages enable NCL to outperform CL
significantly on feature disentanglement, feature selection, as well as
downstream classification tasks. At last, we show that NCL can be easily
extended to other learning scenarios and benefit supervised learning as well.
Code is available at https://github.com/PKU-ML/non_neg.</div><div><a href='http://arxiv.org/abs/2403.12459v1'>2403.12459v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06726v2")'>Probabilistic Contrastive Learning for Long-Tailed Visual Recognition</div>
<div id='2403.06726v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T13:44:49Z</div><div>Authors: Chaoqun Du, Yulin Wang, Shiji Song, Gao Huang</div><div style='padding-top: 10px; width: 80ex'>Long-tailed distributions frequently emerge in real-world data, where a large
number of minority categories contain a limited number of samples. Such
imbalance issue considerably impairs the performance of standard supervised
learning algorithms, which are mainly designed for balanced training sets.
Recent investigations have revealed that supervised contrastive learning
exhibits promising potential in alleviating the data imbalance. However, the
performance of supervised contrastive learning is plagued by an inherent
challenge: it necessitates sufficiently large batches of training data to
construct contrastive pairs that cover all categories, yet this requirement is
difficult to meet in the context of class-imbalanced data. To overcome this
obstacle, we propose a novel probabilistic contrastive (ProCo) learning
algorithm that estimates the data distribution of the samples from each class
in the feature space, and samples contrastive pairs accordingly. In fact,
estimating the distributions of all classes using features in a small batch,
particularly for imbalanced data, is not feasible. Our key idea is to introduce
a reasonable and simple assumption that the normalized features in contrastive
learning follow a mixture of von Mises-Fisher (vMF) distributions on unit
space, which brings two-fold benefits. First, the distribution parameters can
be estimated using only the first sample moment, which can be efficiently
computed in an online manner across different batches. Second, based on the
estimated distribution, the vMF distribution allows us to sample an infinite
number of contrastive pairs and derive a closed form of the expected
contrastive loss for efficient optimization. Our code is available at
https://github.com/LeapLabTHU/ProCo.</div><div><a href='http://arxiv.org/abs/2403.06726v2'>2403.06726v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18133v2")'>Classes Are Not Equal: An Empirical Study on Image Recognition Fairness</div>
<div id='2402.18133v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:54:50Z</div><div>Authors: Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present an empirical study on image recognition fairness,
i.e., extreme class accuracy disparity on balanced data like ImageNet. We
experimentally demonstrate that classes are not equal and the fairness issue is
prevalent for image classification models across various datasets, network
architectures, and model capacities. Moreover, several intriguing properties of
fairness are identified. First, the unfairness lies in problematic
representation rather than classifier bias. Second, with the proposed concept
of Model Prediction Bias, we investigate the origins of problematic
representation during optimization. Our findings reveal that models tend to
exhibit greater prediction biases for classes that are more challenging to
recognize. It means that more other classes will be confused with harder
classes. Then the False Positives (FPs) will dominate the learning in
optimization, thus leading to their poor accuracy. Further, we conclude that
data augmentation and representation learning algorithms improve overall
performance by promoting fairness to some degree in image classification. The
Code is available at
https://github.com/dvlab-research/Parametric-Contrastive-Learning.</div><div><a href='http://arxiv.org/abs/2402.18133v2'>2402.18133v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18286v1")'>Self-Supervised Learning in Electron Microscopy: Towards a Foundation
  Model for Advanced Image Analysis</div>
<div id='2402.18286v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T12:25:01Z</div><div>Authors: Bashir Kazimi, Karina Ruzaeva, Stefan Sandfeld</div><div style='padding-top: 10px; width: 80ex'>In this work, we explore the potential of self-supervised learning from
unlabeled electron microscopy datasets, taking a step toward building a
foundation model in this field. We show how self-supervised pretraining
facilitates efficient fine-tuning for a spectrum of downstream tasks, including
semantic segmentation, denoising, noise &amp; background removal, and
super-resolution. Experimentation with varying model complexities and receptive
field sizes reveals the remarkable phenomenon that fine-tuned models of lower
complexity consistently outperform more complex models with random weight
initialization. We demonstrate the versatility of self-supervised pretraining
across various downstream tasks in the context of electron microscopy, allowing
faster convergence and better performance. We conclude that self-supervised
pretraining serves as a powerful catalyst, being especially advantageous when
limited annotated data are available and efficient scaling of computational
cost are important.</div><div><a href='http://arxiv.org/abs/2402.18286v1'>2402.18286v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13353v1")'>Combining unsupervised and supervised learning in microscopy enables
  defect analysis of a full 4H-SiC wafer</div>
<div id='2402.13353v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T20:04:23Z</div><div>Authors: Binh Duong Nguyen, Johannes Steiner, Peter Wellmann, Stefan Sandfeld</div><div style='padding-top: 10px; width: 80ex'>Detecting and analyzing various defect types in semiconductor materials is an
important prerequisite for understanding the underlying mechanisms as well as
tailoring the production processes. Analysis of microscopy images that reveal
defects typically requires image analysis tasks such as segmentation and object
detection. With the permanently increasing amount of data that is produced by
experiments, handling these tasks manually becomes more and more impossible. In
this work, we combine various image analysis and data mining techniques for
creating a robust and accurate, automated image analysis pipeline. This allows
for extracting the type and position of all defects in a microscopy image of a
KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000
individual images.</div><div><a href='http://arxiv.org/abs/2402.13353v1'>2402.13353v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11536v1")'>OCR is All you need: Importing Multi-Modality into Image-based Defect
  Detection System</div>
<div id='2403.11536v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T07:41:39Z</div><div>Authors: Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, Kuang-Ming Wu</div><div style='padding-top: 10px; width: 80ex'>Automatic optical inspection (AOI) plays a pivotal role in the manufacturing
process, predominantly leveraging high-resolution imaging instruments for
scanning purposes. It detects anomalies by analyzing image textures or
patterns, making it an essential tool in industrial manufacturing and quality
control. Despite its importance, the deployment of models for AOI often faces
challenges. These include limited sample sizes, which hinder effective feature
learning, variations among source domains, and sensitivities to changes in
lighting and camera positions during imaging. These factors collectively
compromise the accuracy of model predictions. Traditional AOI often fails to
capitalize on the rich mechanism-parameter information from machines or inside
images, including statistical parameters, which typically benefit AOI
classification. To address this, we introduce an external modality-guided data
mining framework, primarily rooted in optical character recognition (OCR), to
extract statistical features from images as a second modality to enhance
performance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the
alignment of external modality features, extracted using a single
modality-aware model, with image features encoded by a convolutional neural
network. This synergy enables a more refined fusion of semantic representations
from different modalities. We further introduce feature refinement and a gating
function in our OANet to optimize the combination of these features, enhancing
inference and decision-making capabilities. Experimental outcomes show that our
methodology considerably boosts the recall rate of the defect detection model
and maintains high robustness even in challenging scenarios.</div><div><a href='http://arxiv.org/abs/2403.11536v1'>2403.11536v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18527v1")'>Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep
  Structures</div>
<div id='2402.18527v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T18:07:47Z</div><div>Authors: Andrei Cozma, Landon Harris, Hairong Qi, Ping Ji, Wenpeng Guo, Song Yuan</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a robust approach for automated defect detection in
tire X-ray images by harnessing traditional feature extraction methods such as
Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features,
as well as Fourier and Wavelet-based features, complemented by advanced machine
learning techniques. Recognizing the challenges inherent in the complex
patterns and textures of tire X-ray images, the study emphasizes the
significance of feature engineering to enhance the performance of defect
detection systems. By meticulously integrating combinations of these features
with a Random Forest (RF) classifier and comparing them against advanced models
like YOLOv8, the research not only benchmarks the performance of traditional
features in defect detection but also explores the synergy between classical
and modern approaches. The experimental results demonstrate that these
traditional features, when fine-tuned and combined with machine learning
models, can significantly improve the accuracy and reliability of tire defect
detection, aiming to set a new standard in automated quality assurance in tire
manufacturing.</div><div><a href='http://arxiv.org/abs/2402.18527v1'>2402.18527v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13699v2")'>Explainable Classification Techniques for Quantum Dot Device
  Measurements</div>
<div id='2402.13699v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T11:00:23Z</div><div>Authors: Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak</div><div style='padding-top: 10px; width: 80ex'>In the physical sciences, there is an increased need for robust feature
representations of image data: image acquisition, in the generalized sense of
two-dimensional data, is now widespread across a large number of fields,
including quantum information science, which we consider here. While
traditional image features are widely utilized in such cases, their use is
rapidly being supplanted by Neural Network-based techniques that often
sacrifice explainability in exchange for high accuracy. To ameliorate this
trade-off, we propose a synthetic data-based technique that results in
explainable features. We show, using Explainable Boosting Machines (EBMs), that
this method offers superior explainability without sacrificing accuracy.
Specifically, we show that there is a meaningful benefit to this technique in
the context of quantum dot tuning, where human intervention is necessary at the
current stage of development.</div><div><a href='http://arxiv.org/abs/2402.13699v2'>2402.13699v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12493v1")'>A Trainable Feature Extractor Module for Deep Neural Networks and
  Scanpath Classification</div>
<div id='2403.12493v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T07:02:06Z</div><div>Authors: Wolfgang Fuhl</div><div style='padding-top: 10px; width: 80ex'>Scanpath classification is an area in eye tracking research with possible
applications in medicine, manufacturing as well as training systems for
students in various domains. In this paper we propose a trainable feature
extraction module for deep neural networks. The purpose of this module is to
transform a scanpath into a feature vector which is directly useable for the
deep neural network architecture. Based on the backpropagated error of the deep
neural network, the feature extraction module adapts its parameters to improve
the classification performance. Therefore, our feature extraction module is
jointly trainable with the deep neural network. The motivation to this feature
extraction module is based on classical histogram-based approaches which
usually compute distributions over a scanpath. We evaluated our module on three
public datasets and compared it to the state of the art approaches.</div><div><a href='http://arxiv.org/abs/2403.12493v1'>2403.12493v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03390v1")'>Performance Evaluation of Semi-supervised Learning Frameworks for
  Multi-Class Weed Detection</div>
<div id='2403.03390v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T00:59:51Z</div><div>Authors: Jiajia Li, Dong Chen, Xunyuan Yin, Zhaojian Li</div><div style='padding-top: 10px; width: 80ex'>Effective weed control plays a crucial role in optimizing crop yield and
enhancing agricultural product quality. However, the reliance on herbicide
application not only poses a critical threat to the environment but also
promotes the emergence of resistant weeds. Fortunately, recent advances in
precision weed management enabled by ML and DL provide a sustainable
alternative. Despite great progress, existing algorithms are mainly developed
based on supervised learning approaches, which typically demand large-scale
datasets with manual-labeled annotations, which is time-consuming and
labor-intensive. As such, label-efficient learning methods, especially
semi-supervised learning, have gained increased attention in the broader domain
of computer vision and have demonstrated promising performance. These methods
aim to utilize a small number of labeled data samples along with a great number
of unlabeled samples to develop high-performing models comparable to the
supervised learning counterpart trained on a large amount of labeled data
samples. In this study, we assess the effectiveness of a semi-supervised
learning framework for multi-class weed detection, employing two well-known
object detection frameworks, namely FCOS and Faster-RCNN. Specifically, we
evaluate a generalized student-teacher framework with an improved pseudo-label
generation module to produce reliable pseudo-labels for the unlabeled data. To
enhance generalization, an ensemble student network is employed to facilitate
the training process. Experimental results show that the proposed approach is
able to achieve approximately 76\% and 96\% detection accuracy as the
supervised methods with only 10\% of labeled data in CottenWeedDet3 and
CottonWeedDet12, respectively. We offer access to the source code, contributing
a valuable resource for ongoing semi-supervised learning research in weed
detection and beyond.</div><div><a href='http://arxiv.org/abs/2403.03390v1'>2403.03390v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14957v1")'>The Common Stability Mechanism behind most Self-Supervised Learning
  Approaches</div>
<div id='2402.14957v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T20:36:24Z</div><div>Authors: Abhishek Jha, Matthew B. Blaschko, Yuki M. Asano, Tinne Tuytelaars</div><div style='padding-top: 10px; width: 80ex'>Last couple of years have witnessed a tremendous progress in self-supervised
learning (SSL), the success of which can be attributed to the introduction of
useful inductive biases in the learning process to learn meaningful visual
representations while avoiding collapse. These inductive biases and constraints
manifest themselves in the form of different optimization formulations in the
SSL techniques, e.g. by utilizing negative examples in a contrastive
formulation, or exponential moving average and predictor in BYOL and SimSiam.
In this paper, we provide a framework to explain the stability mechanism of
these different SSL techniques: i) we discuss the working mechanism of
contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV,
SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite
different formulations these methods implicitly optimize a similar objective
function, i.e. minimizing the magnitude of the expected representation over all
data samples, or the mean of the data distribution, while maximizing the
magnitude of the expected representation of individual samples over different
data augmentations; iii) we provide mathematical and empirical evidence to
support our framework. We formulate different hypotheses and test them using
the Imagenet100 dataset.</div><div><a href='http://arxiv.org/abs/2402.14957v1'>2402.14957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00642v1")'>Rethinking The Uniformity Metric in Self-Supervised Learning</div>
<div id='2403.00642v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T16:22:05Z</div><div>Authors: Xianghong Fang, Jian Li, Qiang Sun, Benyou Wang</div><div style='padding-top: 10px; width: 80ex'>Uniformity plays a crucial role in the assessment of learned representations,
contributing to a deeper comprehension of self-supervised learning. The seminal
work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that
quantitatively measures the collapse degree of learned representations.
Directly optimizing this metric together with alignment proves to be effective
in preventing constant collapse. However, we present both theoretical and
empirical evidence revealing that this metric lacks sensitivity to dimensional
collapse, highlighting its limitations. To address this limitation and design a
more effective uniformity metric, this paper identifies five fundamental
properties, some of which the existing uniformity metric fails to meet. We
subsequently introduce a novel uniformity metric that satisfies all of these
desiderata and exhibits sensitivity to dimensional collapse. When applied as an
auxiliary loss in various established self-supervised methods, our proposed
uniformity metric consistently enhances their performance in downstream
tasks.Our code was released at
https://github.com/sunset-clouds/WassersteinUniformityMetric.</div><div><a href='http://arxiv.org/abs/2403.00642v1'>2403.00642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05966v1")'>Can Generative Models Improve Self-Supervised Representation Learning?</div>
<div id='2403.05966v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T17:17:07Z</div><div>Authors: Arash Afkanpour, Vahid Reza Khazaie, Sana Ayromlou, Fereshteh Forghani</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement in self-supervised learning (SSL) has highlighted its
potential to leverage unlabeled data for learning powerful visual
representations. However, existing SSL approaches, particularly those employing
different views of the same image, often rely on a limited set of predefined
data augmentations. This constrains the diversity and quality of
transformations, which leads to sub-optimal representations. In this paper, we
introduce a novel framework that enriches the SSL paradigm by utilizing
generative models to produce semantically consistent image augmentations. By
directly conditioning generative models on a source image representation, our
method enables the generation of diverse augmentations while maintaining the
semantics of the source image, thus offering a richer set of data for
self-supervised learning. Our experimental results demonstrate that our
framework significantly enhances the quality of learned visual representations.
This research demonstrates that incorporating generative models into the SSL
workflow opens new avenues for exploring the potential of unlabeled visual
data. This development paves the way for more robust and versatile
representation learning techniques.</div><div><a href='http://arxiv.org/abs/2403.05966v1'>2403.05966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00351v2")'>Machine Unlearning for Image-to-Image Generative Models</div>
<div id='2402.00351v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T05:35:25Z</div><div>Authors: Guihong Li, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu</div><div style='padding-top: 10px; width: 80ex'>Machine unlearning has emerged as a new paradigm to deliberately forget data
samples from a given model in order to adhere to stringent regulations.
However, existing machine unlearning methods have been primarily focused on
classification models, leaving the landscape of unlearning for generative
models relatively unexplored. This paper serves as a bridge, addressing the gap
by providing a unifying framework of machine unlearning for image-to-image
generative models. Within this framework, we propose a
computationally-efficient algorithm, underpinned by rigorous theoretical
analysis, that demonstrates negligible performance degradation on the retain
samples, while effectively removing the information from the forget samples.
Empirical studies on two large-scale datasets, ImageNet-1K and Places-365,
further show that our algorithm does not rely on the availability of the retain
samples, which further complies with data retention policy. To our best
knowledge, this work is the first that represents systemic, theoretical,
empirical explorations of machine unlearning specifically tailored for
image-to-image generative models. Our code is available at
https://github.com/jpmorganchase/l2l-generator-unlearning.</div><div><a href='http://arxiv.org/abs/2402.00351v2'>2402.00351v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12448v1")'>Do Generated Data Always Help Contrastive Learning?</div>
<div id='2403.12448v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T05:17:47Z</div><div>Authors: Yifei Wang, Jizhe Zhang, Yisen Wang</div><div style='padding-top: 10px; width: 80ex'>Contrastive Learning (CL) has emerged as one of the most successful paradigms
for unsupervised visual representation learning, yet it often depends on
intensive manual data augmentations. With the rise of generative models,
especially diffusion models, the ability to generate realistic images close to
the real data distribution has been well recognized. These generated
high-equality images have been successfully applied to enhance contrastive
representation learning, a technique termed ``data inflation''. However, we
find that the generated data (even from a good diffusion model like DDPM) may
sometimes even harm contrastive learning. We investigate the causes behind this
failure from the perspective of both data inflation and data augmentation. For
the first time, we reveal the complementary roles that stronger data inflation
should be accompanied by weaker augmentations, and vice versa. We also provide
rigorous theoretical explanations for these phenomena via deriving its
generalization bounds under data inflation. Drawing from these insights, we
propose Adaptive Inflation (AdaInf), a purely data-centric strategy without
introducing any extra computation cost. On benchmark datasets, AdaInf can bring
significant improvements for various contrastive learning methods. Notably,
without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10
with SimCLR, setting a new record that surpasses many sophisticated methods.
Code is available at https://github.com/PKU-ML/adainf.</div><div><a href='http://arxiv.org/abs/2403.12448v1'>2403.12448v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03311v1")'>HASSOD: Hierarchical Adaptive Self-Supervised Object Detection</div>
<div id='2402.03311v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:59:41Z</div><div>Authors: Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang</div><div style='padding-top: 10px; width: 80ex'>The human visual perception system demonstrates exceptional capabilities in
learning without explicit supervision and understanding the part-to-whole
composition of objects. Drawing inspiration from these two abilities, we
propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a
novel approach that learns to detect objects and understand their compositions
without human supervision. HASSOD employs a hierarchical adaptive clustering
strategy to group regions into object masks based on self-supervised visual
representations, adaptively determining the number of objects per image.
Furthermore, HASSOD identifies the hierarchical levels of objects in terms of
composition, by analyzing coverage relations between masks and constructing
tree structures. This additional self-supervised learning task leads to
improved detection performance and enhanced interpretability. Lastly, we
abandon the inefficient multi-round self-training process utilized in prior
methods and instead adapt the Mean Teacher framework from semi-supervised
learning, which leads to a smoother and more efficient training process.
Through extensive experiments on prevalent image datasets, we demonstrate the
superiority of HASSOD over existing methods, thereby advancing the state of the
art in self-supervised object detection. Notably, we improve Mask AR from 20.2
to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:
https://HASSOD-NeurIPS23.github.io.</div><div><a href='http://arxiv.org/abs/2402.03311v1'>2402.03311v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12820v1")'>DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained
  Self-supervised Vision Transformer</div>
<div id='2401.12820v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:53:32Z</div><div>Authors: Sonal Kumar, Arijit Sur, Rashmi Dutta Baruah</div><div style='padding-top: 10px; width: 80ex'>Successive proposals of several self-supervised training schemes continue to
emerge, taking one step closer to developing a universal foundation model. In
this process, the unsupervised downstream tasks are recognized as one of the
evaluation methods to validate the quality of visual features learned with a
self-supervised training scheme. However, unsupervised dense semantic
segmentation has not been explored as a downstream task, which can utilize and
evaluate the quality of semantic information introduced in patch-level feature
representations during self-supervised training of a vision transformer.
Therefore, this paper proposes a novel data-driven approach for unsupervised
semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates
semantically consistent and dense pseudo annotate segmentation masks for the
unlabeled image dataset without using any visual-prior or synchronized data. We
compare these pseudo-annotated segmentation masks with ground truth masks for
evaluating recent self-supervised training schemes to learn shared semantic
properties at the patch level and discriminative semantic properties at the
segment level. Finally, we evaluate existing state-of-the-art self-supervised
training schemes with our proposed downstream task, i.e., DatUS^2. Also, the
best version of DatUS^2 outperforms the existing state-of-the-art method for
the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47%
Pixel accuracy on the SUIM dataset. It also achieves a competitive level of
accuracy for a large-scale and complex dataset, i.e., the COCO dataset.</div><div><a href='http://arxiv.org/abs/2401.12820v1'>2401.12820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18117v1")'>PRCL: Probabilistic Representation Contrastive Learning for
  Semi-Supervised Semantic Segmentation</div>
<div id='2402.18117v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:10:37Z</div><div>Authors: Haoyu Xie, Changqi Wang, Jian Zhao, Yang Liu, Jun Dan, Chong Fu, Baigui Sun</div><div style='padding-top: 10px; width: 80ex'>Tremendous breakthroughs have been developed in Semi-Supervised Semantic
Segmentation (S4) through contrastive learning. However, due to limited
annotations, the guidance on unlabeled images is generated by the model itself,
which inevitably exists noise and disturbs the unsupervised training process.
To address this issue, we propose a robust contrastive-based S4 framework,
termed the Probabilistic Representation Contrastive Learning (PRCL) framework
to enhance the robustness of the unsupervised training process. We model the
pixel-wise representation as Probabilistic Representations (PR) via
multivariate Gaussian distribution and tune the contribution of the ambiguous
representations to tolerate the risk of inaccurate guidance in contrastive
learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by
gathering all PRs throughout the whole training process. Since the GDP contains
the information of all representations with the same class, it is robust from
the instant noise in representations and bears the intra-class variance of
representations. In addition, we generate Virtual Negatives (VNs) based on GDP
to involve the contrastive learning process. Extensive experiments on two
public benchmarks demonstrate the superiority of our PRCL framework.</div><div><a href='http://arxiv.org/abs/2402.18117v1'>2402.18117v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06546v1")'>OMH: Structured Sparsity via Optimally Matched Hierarchy for
  Unsupervised Semantic Segmentation</div>
<div id='2403.06546v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T09:46:41Z</div><div>Authors: Baran Ozaydin, Tong Zhang, Deblina Bhattacharjee, Sabine Süsstrunk, Mathieu Salzmann</div><div style='padding-top: 10px; width: 80ex'>Unsupervised Semantic Segmentation (USS) involves segmenting images without
relying on predefined labels, aiming to alleviate the burden of extensive human
labeling. Existing methods utilize features generated by self-supervised models
and specific priors for clustering. However, their clustering objectives are
not involved in the optimization of the features during training. Additionally,
due to the lack of clear class definitions in USS, the resulting segments may
not align well with the clustering objective. In this paper, we introduce a
novel approach called Optimally Matched Hierarchy (OMH) to simultaneously
address the above issues. The core of our method lies in imposing structured
sparsity on the feature space, which allows the features to encode information
with different levels of granularity. The structure of this sparsity stems from
our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy
among parallel clusters through Optimal Transport. Our OMH yields better
unsupervised segmentation performance compared to existing USS methods. Our
extensive experiments demonstrate the benefits of OMH when utilizing our
differentiable paradigm. We will make our code publicly available.</div><div><a href='http://arxiv.org/abs/2403.06546v1'>2403.06546v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12217v1")'>Exploring Simple Open-Vocabulary Semantic Segmentation</div>
<div id='2401.12217v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:59:29Z</div><div>Authors: Zihang Lai</div><div style='padding-top: 10px; width: 80ex'>Open-vocabulary semantic segmentation models aim to accurately assign a
semantic label to each pixel in an image from a set of arbitrary
open-vocabulary texts. In order to learn such pixel-level alignment, current
approaches typically rely on a combination of (i) image-level VL model (e.g.
CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this
paper, we introduce S-Seg, a novel model that can achieve surprisingly strong
performance without depending on any of the above elements. S-Seg leverages
pseudo-mask and language to train a MaskFormer, and can be easily trained from
publicly available image-text datasets. Contrary to prior works, our model
directly trains for pixel-level features and language alignment. Once trained,
S-Seg generalizes well to multiple testing datasets without requiring
fine-tuning. In addition, S-Seg has the extra benefits of scalability with data
and consistently improvement when augmented with self-training. We believe that
our simple yet effective approach will serve as a solid baseline for future
research.</div><div><a href='http://arxiv.org/abs/2401.12217v1'>2401.12217v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08400v1")'>Adaptive Hierarchical Certification for Segmentation using Randomized
  Smoothing</div>
<div id='2402.08400v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T11:59:43Z</div><div>Authors: Alaa Anani, Tobias Lorenz, Bernt Schiele, Mario Fritz</div><div style='padding-top: 10px; width: 80ex'>Common certification methods operate on a flat pre-defined set of
fine-grained classes. In this paper, however, we propose a novel, more general,
and practical setting, namely adaptive hierarchical certification for image
semantic segmentation. In this setting, the certification can be within a
multi-level hierarchical label space composed of fine to coarse levels. Unlike
classic methods where the certification would abstain for unstable components,
our approach adaptively relaxes the certification to a coarser level within the
hierarchy. This relaxation lowers the abstain rate whilst providing more
certified semantically meaningful information. We mathematically formulate the
problem setup and introduce, for the first time, an adaptive hierarchical
certification algorithm for image semantic segmentation, that certifies image
pixels within a hierarchy and prove the correctness of its guarantees. Since
certified accuracy does not take the loss of information into account when
traversing into a coarser hierarchy level, we introduce a novel evaluation
paradigm for adaptive hierarchical certification, namely the certified
information gain metric, which is proportional to the class granularity level.
Our evaluation experiments on real-world challenging datasets such as
Cityscapes and ACDC demonstrate that our adaptive algorithm achieves a higher
certified information gain and a lower abstain rate compared to the current
state-of-the-art certification method, as well as other non-adaptive versions
of it.</div><div><a href='http://arxiv.org/abs/2402.08400v1'>2402.08400v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14074v2")'>ProCNS: Progressive Prototype Calibration and Noise Suppression for
  Weakly-Supervised Medical Image Segmentation</div>
<div id='2401.14074v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T10:52:36Z</div><div>Authors: Y. Liu, L. Lin, K. K. Y. Wong, X. Tang</div><div style='padding-top: 10px; width: 80ex'>Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate
the conflict between annotation cost and model performance by adopting sparse
annotation formats (e.g., point, scribble, block, etc.). Typical approaches
attempt to exploit anatomy and topology priors to directly expand sparse
annotations into pseudo-labels. However, due to a lack of attention to the
ambiguous edges in medical images and insufficient exploration of sparse
supervision, existing approaches tend to generate erroneous and overconfident
pseudo proposals in noisy regions, leading to cumulative model error and
performance degradation. In this work, we propose a novel WSS approach, named
ProCNS, encompassing two synergistic modules devised with the principles of
progressive prototype calibration and noise suppression. Specifically, we
design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the
pair-wise affinities between spatial and semantic elements, providing our model
of interest with more reliable guidance. The affinities are derived from the
input images and the prototype-refined predictions. Meanwhile, we propose an
Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and
representative prototype representations, which adaptively identifies and masks
noisy regions within the pseudo proposals, reducing potential erroneous
interference during prototype computation. Furthermore, we generate specialized
soft pseudo-labels for the noisy regions identified by ANPM, providing
supplementary supervision. Extensive experiments on three medical image
segmentation tasks involving different modalities demonstrate that the proposed
framework significantly outperforms representative state-of-the-art methods</div><div><a href='http://arxiv.org/abs/2401.14074v2'>2401.14074v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17868v1")'>Convolution Meets LoRA: Parameter Efficient Finetuning for Segment
  Anything Model</div>
<div id='2401.17868v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T14:27:07Z</div><div>Authors: Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan</div><div style='padding-top: 10px; width: 80ex'>The Segment Anything Model (SAM) stands as a foundational framework for image
segmentation. While it exhibits remarkable zero-shot generalization in typical
scenarios, its advantage diminishes when applied to specialized domains like
medical imagery and remote sensing. To address this limitation, this paper
introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning
approach. By integrating ultra-lightweight convolutional parameters into
Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases
into the plain ViT encoder, further reinforcing SAM's local prior assumption.
Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge
but also revives its capacity of learning high-level image semantics, which is
constrained by SAM's foreground-background segmentation pretraining.
Comprehensive experimentation across diverse benchmarks spanning multiple
domains underscores Conv-LoRA's superiority in adapting SAM to real-world
semantic segmentation tasks.</div><div><a href='http://arxiv.org/abs/2401.17868v1'>2401.17868v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03273v1")'>DINOv2 based Self Supervised Learning For Few Shot Medical Image
  Segmentation</div>
<div id='2403.03273v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T19:13:45Z</div><div>Authors: Lev Ayzenberg, Raja Giryes, Hayit Greenspan</div><div style='padding-top: 10px; width: 80ex'>Deep learning models have emerged as the cornerstone of medical image
segmentation, but their efficacy hinges on the availability of extensive
manually labeled datasets and their adaptability to unforeseen categories
remains a challenge. Few-shot segmentation (FSS) offers a promising solution by
endowing models with the capacity to learn novel classes from limited labeled
examples. A leading method for FSS is ALPNet, which compares features between
the query image and the few available support segmented images. A key question
about using ALPNet is how to design its features. In this work, we delve into
the potential of using features from DINOv2, which is a foundational
self-supervised learning model in computer vision. Leveraging the strengths of
ALPNet and harnessing the feature extraction capabilities of DINOv2, we present
a novel approach to few-shot segmentation that not only enhances performance
but also paves the way for more robust and adaptable medical image analysis.</div><div><a href='http://arxiv.org/abs/2403.03273v1'>2403.03273v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03302v2")'>Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining</div>
<div id='2402.03302v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:58:11Z</div><div>Authors: Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, Shanshan Wang</div><div style='padding-top: 10px; width: 80ex'>Accurate medical image segmentation demands the integration of multi-scale
information, spanning from local features to global dependencies. However, it
is challenging for existing methods to model long-range global information,
where convolutional neural networks (CNNs) are constrained by their local
receptive fields, and vision transformers (ViTs) suffer from high quadratic
complexity of their attention mechanism. Recently, Mamba-based models have
gained great attention for their impressive ability in long sequence modeling.
Several studies have demonstrated that these models can outperform popular
vision models in various tasks, offering higher accuracy, lower memory
consumption, and less computational burden. However, existing Mamba-based
models are mostly trained from scratch and do not explore the power of
pretraining, which has been proven to be quite effective for data-efficient
medical image analysis. This paper introduces a novel Mamba-based model,
Swin-UMamba, designed specifically for medical image segmentation tasks,
leveraging the advantages of ImageNet-based pretraining. Our experimental
results reveal the vital role of ImageNet-based training in enhancing the
performance of Mamba-based models. Swin-UMamba demonstrates superior
performance with a large margin compared to CNNs, ViTs, and latest Mamba-based
models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba
outperforms its closest counterpart U-Mamba_Enc by an average score of 2.72%.</div><div><a href='http://arxiv.org/abs/2402.03302v2'>2402.03302v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06748v1")'>Shortcut Learning in Medical Image Segmentation</div>
<div id='2403.06748v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T14:14:52Z</div><div>Authors: Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo Søndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa Feragen</div><div style='padding-top: 10px; width: 80ex'>Shortcut learning is a phenomenon where machine learning models prioritize
learning simple, potentially misleading cues from data that do not generalize
well beyond the training set. While existing research primarily investigates
this in the realm of image classification, this study extends the exploration
of shortcut learning into medical image segmentation. We demonstrate that
clinical annotations such as calipers, and the combination of zero-padded
convolutions and center-cropped training sets in the dataset can inadvertently
serve as shortcuts, impacting segmentation accuracy. We identify and evaluate
the shortcut learning on two different but common medical image segmentation
tasks. In addition, we suggest strategies to mitigate the influence of shortcut
learning and improve the generalizability of the segmentation models. By
uncovering the presence and implications of shortcuts in medical image
segmentation, we provide insights and methodologies for evaluating and
overcoming this pervasive challenge and call for attention in the community for
shortcuts in segmentation.</div><div><a href='http://arxiv.org/abs/2403.06748v1'>2403.06748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08847v1")'>RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and
  Efficiency Assessment of Medical Image Segmentation Models</div>
<div id='2401.08847v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T21:45:08Z</div><div>Authors: Farhad Maleki, Linda Moy, Reza Forghani, Tapotosh Ghosh, Katie Ovens, Steve Langer, Pouria Rouzrokh, Bardia Khosravi, Ali Ganjizadeh, Daniel Warren, Roxana Daneshjou, Mana Moassefi, Atlas Haddadi Avval, Susan Sotardi, Neil Tenenholtz, Felipe Kitamura, Timothy Kline</div><div style='padding-top: 10px; width: 80ex'>Deep learning techniques, despite their potential, often suffer from a lack
of reproducibility and generalizability, impeding their clinical adoption.
Image segmentation is one of the critical tasks in medical image analysis, in
which one or several regions/volumes of interest should be annotated. This
paper introduces the RIDGE checklist, a framework for assessing the
Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of
deep learning-based medical image segmentation models. The checklist serves as
a guide for researchers to enhance the quality and transparency of their work,
ensuring that segmentation models are not only scientifically sound but also
clinically relevant.</div><div><a href='http://arxiv.org/abs/2401.08847v1'>2401.08847v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06759v1")'>Average Calibration Error: A Differentiable Loss for Improved
  Reliability in Image Segmentation</div>
<div id='2403.06759v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T14:31:03Z</div><div>Authors: Theodore Barfoot, Luis Garcia-Peraza-Herrera, Ben Glocker, Tom Vercauteren</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks for medical image segmentation often produce
overconfident results misaligned with empirical observations. Such
miscalibration, challenges their clinical translation. We propose to use
marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss
function to improve pixel-wise calibration without compromising segmentation
quality. We show that this loss, despite using hard binning, is directly
differentiable, bypassing the need for approximate but differentiable surrogate
or soft binning approaches. Our work also introduces the concept of dataset
reliability histograms which generalises standard reliability diagrams for
refined visual assessment of calibration in semantic segmentation aggregated at
the dataset level. Using mL1-ACE, we reduce average and maximum calibration
error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS
2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS</div><div><a href='http://arxiv.org/abs/2403.06759v1'>2403.06759v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06937v1")'>Assessing Uncertainty Estimation Methods for 3D Image Segmentation under
  Distribution Shifts</div>
<div id='2402.06937v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T12:23:08Z</div><div>Authors: Masoumeh Javanbakhat, Md Tasnimul Hasan, Cristoph Lippert</div><div style='padding-top: 10px; width: 80ex'>In recent years, machine learning has witnessed extensive adoption across
various sectors, yet its application in medical image-based disease detection
and diagnosis remains challenging due to distribution shifts in real-world
data. In practical settings, deployed models encounter samples that differ
significantly from the training dataset, especially in the health domain,
leading to potential performance issues. This limitation hinders the
expressiveness and reliability of deep learning models in health applications.
Thus, it becomes crucial to identify methods capable of producing reliable
uncertainty estimation in the context of distribution shifts in the health
sector. In this paper, we explore the feasibility of using cutting-edge
Bayesian and non-Bayesian methods to detect distributionally shifted samples,
aiming to achieve reliable and trustworthy diagnostic predictions in
segmentation task. Specifically, we compare three distinct uncertainty
estimation methods, each designed to capture either unimodal or multimodal
aspects in the posterior distribution. Our findings demonstrate that methods
capable of addressing multimodal characteristics in the posterior distribution,
offer more dependable uncertainty estimates. This research contributes to
enhancing the utility of deep learning in healthcare, making diagnostic
predictions more robust and trustworthy.</div><div><a href='http://arxiv.org/abs/2402.06937v1'>2402.06937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14349v2")'>Uncertainty-driven and Adversarial Calibration Learning for Epicardial
  Adipose Tissue Segmentation</div>
<div id='2402.14349v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T07:39:41Z</div><div>Authors: Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang Tang, Qiuyu Wang, Chunquan Li</div><div style='padding-top: 10px; width: 80ex'>Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete
large amounts of adipokines to affect the myocardium and coronary arteries. EAT
volume and density can be used as independent risk markers measurement of
volume by noninvasive magnetic resonance images is the best method of assessing
EAT. However, segmenting EAT is challenging due to the low contrast between EAT
and pericardial effusion and the presence of motion artifacts. we propose a
novel feature latent space multilevel supervision network (SPDNet) with
uncertainty-driven and adversarial calibration learning to enhance segmentation
for more accurate EAT volume estimation. The network first addresses the
blurring of EAT edges due to the medical images in the open medical
environments with low quality or out-of-distribution by modeling the
uncertainty as a Gaussian distribution in the feature latent space, which using
its Bayesian estimation as a regularization constraint to optimize SwinUNETR.
Second, an adversarial training strategy is introduced to calibrate the
segmentation feature map and consider the multi-scale feature differences
between the uncertainty-guided predictive segmentation and the ground truth
segmentation, synthesizing the multi-scale adversarial loss directly improves
the ability to discriminate the similarity between organizations. Experiments
on both the cardiac public MRI dataset (ACDC) and the real-world clinical
cohort EAT dataset show that the proposed network outperforms mainstream
models, validating that uncertainty-driven and adversarial calibration learning
can be used to provide additional information for modeling multi-scale
ambiguities.</div><div><a href='http://arxiv.org/abs/2402.14349v2'>2402.14349v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14022v1")'>Statistical validation of a deep learning algorithm for dental anomaly
  detection in intraoral radiographs using paired data</div>
<div id='2402.14022v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T13:46:36Z</div><div>Authors: Pieter Van Leemput, Johannes Keustermans, Wouter Mollemans</div><div style='padding-top: 10px; width: 80ex'>This article describes the clinical validation study setup, statistical
analysis and results for a deep learning algorithm which detects dental
anomalies in intraoral radiographic images, more specifically caries, apical
lesions, root canal treatment defects, marginal defects at crown restorations,
periodontal bone loss and calculus. The study compares the detection
performance of dentists using the deep learning algorithm to the prior
performance of these dentists evaluating the images without algorithmic
assistance. Calculating the marginal profit and loss of performance from the
annotated paired image data allows for a quantification of the hypothesized
change in sensitivity and specificity. The statistical significance of these
results is extensively proven using both McNemar's test and the binomial
hypothesis test. The average sensitivity increases from $60.7\%$ to $85.9\%$,
while the average specificity slightly decreases from $94.5\%$ to $92.7\%$. We
prove that the increase of the area under the localization ROC curve (AUC) is
significant (from $0.60$ to $0.86$ on average), while the average AUC is
bounded by the $95\%$ confidence intervals ${[}0.54, 0.65{]}$ and ${[}0.82,
0.90{]}$. When using the deep learning algorithm for diagnostic guidance, the
dentist can be $95\%$ confident that the average true population sensitivity is
bounded by the range $79.6\%$ to $91.9\%$. The proposed paired data setup and
statistical analysis can be used as a blueprint to thoroughly test the effect
of a modality change, like a deep learning based detection and/or segmentation,
on radiographic images.</div><div><a href='http://arxiv.org/abs/2402.14022v1'>2402.14022v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09190v1")'>Exploring the Role of Convolutional Neural Networks (CNN) in Dental
  Radiography Segmentation: A Comprehensive Systematic Literature Review</div>
<div id='2401.09190v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T13:00:57Z</div><div>Authors: Walid Brahmi, Imen Jdey, Fadoua Drira</div><div style='padding-top: 10px; width: 80ex'>In the field of dentistry, there is a growing demand for increased precision
in diagnostic tools, with a specific focus on advanced imaging techniques such
as computed tomography, cone beam computed tomography, magnetic resonance
imaging, ultrasound, and traditional intra-oral periapical X-rays. Deep
learning has emerged as a pivotal tool in this context, enabling the
implementation of automated segmentation techniques crucial for extracting
essential diagnostic data. This integration of cutting-edge technology
addresses the urgent need for effective management of dental conditions, which,
if left undetected, can have a significant impact on human health. The
impressive track record of deep learning across various domains, including
dentistry, underscores its potential to revolutionize early detection and
treatment of oral health issues. Objective: Having demonstrated significant
results in diagnosis and prediction, deep convolutional neural networks (CNNs)
represent an emerging field of multidisciplinary research. The goals of this
study were to provide a concise overview of the state of the art, standardize
the current debate, and establish baselines for future research. Method: In
this study, a systematic literature review is employed as a methodology to
identify and select relevant studies that specifically investigate the deep
learning technique for dental imaging analysis. This study elucidates the
methodological approach, including the systematic collection of data,
statistical analysis, and subsequent dissemination of outcomes. Conclusion:
This work demonstrates how Convolutional Neural Networks (CNNs) can be employed
to analyze images, serving as effective tools for detecting dental pathologies.
Although this research acknowledged some limitations, CNNs utilized for
segmenting and categorizing teeth exhibited their highest level of performance
overall.</div><div><a href='http://arxiv.org/abs/2401.09190v1'>2401.09190v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.15218v1")'>Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment
  Anything Model for Crowd-Sourcing Medical Image Annotations</div>
<div id='2403.15218v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:07:07Z</div><div>Authors: Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul H. Yi, Vishwa S. Parekh</div><div style='padding-top: 10px; width: 80ex'>Curating annotations for medical image segmentation is a labor-intensive and
time-consuming task that requires domain expertise, resulting in "narrowly"
focused deep learning (DL) models with limited translational utility. Recently,
foundation models like the Segment Anything Model (SAM) have revolutionized
semantic segmentation with exceptional zero-shot generalizability across
various domains, including medical imaging, and hold a lot of promise for
streamlining the annotation process. However, SAM has yet to be evaluated in a
crowd-sourced setting to curate annotations for training 3D DL segmentation
models. In this work, we explore the potential of SAM for crowd-sourcing
"sparse" annotations from non-experts to generate "dense" segmentation masks
for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our
results indicate that while SAM-generated annotations exhibit high mean Dice
scores compared to ground-truth annotations, nnU-Net models trained on
SAM-generated annotations perform significantly worse than nnU-Net models
trained on ground-truth annotations ($p&lt;0.001$, all).</div><div><a href='http://arxiv.org/abs/2403.15218v1'>2403.15218v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12695v1")'>Federated Semi-supervised Learning for Medical Image Segmentation with
  intra-client and inter-client Consistency</div>
<div id='2403.12695v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T12:52:38Z</div><div>Authors: Yubin Zheng, Peng Tang, Tianjie Ju, Weidong Qiu, Bo Yan</div><div style='padding-top: 10px; width: 80ex'>Medical image segmentation plays a vital role in clinic disease diagnosis and
medical image analysis. However, labeling medical images for segmentation task
is tough due to the indispensable domain expertise of radiologists.
Furthermore, considering the privacy and sensitivity of medical images, it is
impractical to build a centralized segmentation dataset from different medical
institutions. Federated learning aims to train a shared model of isolated
clients without local data exchange which aligns well with the scarcity and
privacy characteristics of medical data. To solve the problem of labeling hard,
many advanced semi-supervised methods have been proposed in a centralized data
setting. As for federated learning, how to conduct semi-supervised learning
under this distributed scenario is worth investigating. In this work, we
propose a novel federated semi-supervised learning framework for medical image
segmentation. The intra-client and inter-client consistency learning are
introduced to smooth predictions at the data level and avoid confirmation bias
of local models. They are achieved with the assistance of a Variational
Autoencoder (VAE) trained collaboratively by clients. The added VAE model plays
three roles: 1) extracting latent low-dimensional features of all labeled and
unlabeled data; 2) performing a novel type of data augmentation in calculating
intra-client consistency loss; 3) utilizing the generative ability of itself to
conduct inter-client consistency distillation. The proposed framework is
compared with other federated semi-supervised or self-supervised learning
methods. The experimental results illustrate that our method outperforms the
state-of-the-art method while avoiding a lot of computation and communication
overhead.</div><div><a href='http://arxiv.org/abs/2403.12695v1'>2403.12695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03642v1")'>Generative Active Learning with Variational Autoencoder for Radiology
  Data Generation in Veterinary Medicine</div>
<div id='2403.03642v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:02:07Z</div><div>Authors: In-Gyu Lee, Jun-Young Oh, Hee-Jung Yu, Jae-Hwan Kim, Ki-Dong Eom, Ji-Hoon Jeong</div><div style='padding-top: 10px; width: 80ex'>Recently, with increasing interest in pet healthcare, the demand for
computer-aided diagnosis (CAD) systems in veterinary medicine has increased.
The development of veterinary CAD has stagnated due to a lack of sufficient
radiology data. To overcome the challenge, we propose a generative active
learning framework based on a variational autoencoder. This approach aims to
alleviate the scarcity of reliable data for CAD systems in veterinary medicine.
This study utilizes datasets comprising cardiomegaly radiograph data. After
removing annotations and standardizing images, we employed a framework for data
augmentation, which consists of a data generation phase and a query phase for
filtering the generated data. The experimental results revealed that as the
data generated through this framework was added to the training data of the
generative model, the frechet inception distance consistently decreased from
84.14 to 50.75 on the radiograph. Subsequently, when the generated data were
incorporated into the training of the classification model, the false positive
of the confusion matrix also improved from 0.16 to 0.66 on the radiograph. The
proposed framework has the potential to address the challenges of data scarcity
in medical CAD, contributing to its advancement.</div><div><a href='http://arxiv.org/abs/2403.03642v1'>2403.03642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.08059v1")'>FluoroSAM: A Language-aligned Foundation Model for X-ray Image
  Segmentation</div>
<div id='2403.08059v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T20:11:38Z</div><div>Authors: Benjamin D. Killeen, Liam J. Wang, Han Zhang, Mehran Armand, Russell H. Taylor, Greg Osgood, Mathias Unberath</div><div style='padding-top: 10px; width: 80ex'>Automated X-ray image segmentation would accelerate research and development
in diagnostic and interventional precision medicine. Prior efforts have
contributed task-specific models capable of solving specific image analysis
problems, but the utility of these models is restricted to their particular
task domain, and expanding to broader use requires additional data, labels, and
retraining efforts. Recently, foundation models (FMs) -- machine learning
models trained on large amounts of highly variable data thus enabling broad
applicability -- have emerged as promising tools for automated image analysis.
Existing FMs for medical image analysis focus on scenarios and modalities where
objects are clearly defined by visually apparent boundaries, such as surgical
tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally
offer such clearly delineated boundaries or structure priors. During X-ray
image formation, complex 3D structures are projected in transmission onto the
imaging plane, resulting in overlapping features of varying opacity and shape.
To pave the way toward an FM for comprehensive and automated analysis of
arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned
variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic
X-ray images. FluoroSAM is trained on data including masks for 128 organ types
and 464 non-anatomical objects, such as tools and implants. In real X-ray
images of cadaveric specimens, FluoroSAM is able to segment bony anatomical
structures based on text-only prompting with 0.51 and 0.79 DICE with
point-based refinement, outperforming competing SAM variants for all
structures. FluoroSAM is also capable of zero-shot generalization to segmenting
classes beyond the training set thanks to its language alignment, which we
demonstrate for full lung segmentation on real chest X-rays.</div><div><a href='http://arxiv.org/abs/2403.08059v1'>2403.08059v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08469v1")'>Explanations of Classifiers Enhance Medical Image Segmentation via
  End-to-end Pre-training</div>
<div id='2401.08469v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T16:18:42Z</div><div>Authors: Jiamin Chen, Xuhong Li, Yanwu Xu, Mengnan Du, Haoyi Xiong</div><div style='padding-top: 10px; width: 80ex'>Medical image segmentation aims to identify and locate abnormal structures in
medical images, such as chest radiographs, using deep neural networks. These
networks require a large number of annotated images with fine-grained masks for
the regions of interest, making pre-training strategies based on classification
datasets essential for sample efficiency. Based on a large-scale medical image
classification dataset, our work collects explanations from well-trained
classifiers to generate pseudo labels of segmentation tasks. Specifically, we
offer a case study on chest radiographs and train image classifiers on the
CheXpert dataset to identify 14 pathological observations in radiology. We then
use Integrated Gradients (IG) method to distill and boost the explanations
obtained from the classifiers, generating massive diagnosis-oriented
localization labels (DoLL). These DoLL-annotated images are used for
pre-training the model before fine-tuning it for downstream segmentation tasks,
including COVID-19 infectious areas, lungs, heart, and clavicles. Our method
outperforms other baselines, showcasing significant advantages in model
performance and training efficiency across various segmentation settings.</div><div><a href='http://arxiv.org/abs/2401.08469v1'>2401.08469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11338v1")'>Ensembling and Test Augmentation for Covid-19 Detection and Covid-19
  Domain Adaptation from 3D CT-Scans</div>
<div id='2403.11338v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T20:44:38Z</div><div>Authors: Fares Bougourzi, Feryal Windal Moula, Halim Benhabiles, Fadi Dornaika, Abdelmalik Taleb-Ahmed</div><div style='padding-top: 10px; width: 80ex'>Since the emergence of Covid-19 in late 2019, medical image analysis using
artificial intelligence (AI) has emerged as a crucial research area,
particularly with the utility of CT-scan imaging for disease diagnosis. This
paper contributes to the 4th COV19D competition, focusing on Covid-19 Detection
and Covid-19 Domain Adaptation Challenges. Our approach centers on lung
segmentation and Covid-19 infection segmentation employing the recent CNN-based
segmentation architecture PDAtt-Unet, which simultaneously segments lung
regions and infections. Departing from traditional methods, we concatenate the
input slice (grayscale) with segmented lung and infection, generating three
input channels akin to color channels. Additionally, we employ three 3D CNN
backbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and
3D-Resnet-50 models to train Covid-19 recognition for both challenges.
Furthermore, we explore ensemble approaches and testing augmentation to enhance
performance. Comparison with baseline results underscores the substantial
efficiency of our approach, with a significant margin in terms of F1-score (14
%). This study advances the field by presenting a comprehensive methodology for
accurate Covid-19 detection and adaptation, leveraging cutting-edge AI
techniques in medical image analysis.</div><div><a href='http://arxiv.org/abs/2403.11338v1'>2403.11338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11505v1")'>Covid-19 detection from CT scans using EfficientNet and Attention
  mechanism</div>
<div id='2403.11505v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T06:20:49Z</div><div>Authors: Ramy Farag, Parth Upadhyay, Guilhermen DeSouza</div><div style='padding-top: 10px; width: 80ex'>Manual diagnosis and analysis of COVID-19 through the examination of lung
Computed Tomography (CT) scan images by physicians tends to result in
inefficiency, especially with high patient volumes and numerous images per
patient. We address the need for automation by developing a deep learning
model-based pipeline for COVID-19 detection from CT scan images of the lungs.
The Domain adaptation, Explainability, and Fairness in AI for Medical Image
Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D)
provides an opportunity to assess our designed pipeline for COVID-19 detection
from CT scan images. The proposed pipeline incorporates EfficientNet with an
Attention mechanism with a pre-processing step. Our pipeline outperforms last
year's teams on the validation set of the competition dataset.</div><div><a href='http://arxiv.org/abs/2403.11505v1'>2403.11505v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13148v1")'>SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced
  Digital Breast Tomosynthesis Image Classification</div>
<div id='2403.13148v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T20:52:31Z</div><div>Authors: Yuexi Du, Regina J. Hooley, John Lewin, Nicha C. Dvornek</div><div style='padding-top: 10px; width: 80ex'>Digital Breast Tomosynthesis (DBT) is a widely used medical imaging modality
for breast cancer screening and diagnosis, offering higher spatial resolution
and greater detail through its 3D-like breast volume imaging capability.
However, the increased data volume also introduces pronounced data imbalance
challenges, where only a small fraction of the volume contains suspicious
tissue. This further exacerbates the data imbalance due to the case-level
distribution in real-world data and leads to learning a trivial classification
model that only predicts the majority class. To address this, we propose a
novel method using view-level contrastive Self-supervised Initialization and
Fine-Tuning for identifying abnormal DBT images, namely SIFT-DBT. We further
introduce a patch-level multi-instance learning method to preserve spatial
resolution. The proposed method achieves 92.69% volume-wise AUC on an
evaluation of 970 unique studies.</div><div><a href='http://arxiv.org/abs/2403.13148v1'>2403.13148v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05570v1")'>Siamese Networks with Soft Labels for Unsupervised Lesion Detection and
  Patch Pretraining on Screening Mammograms</div>
<div id='2401.05570v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:27:37Z</div><div>Authors: Kevin Van Vorst, Li Shen</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning has become a popular way to pretrain a deep learning
model and then transfer it to perform downstream tasks. However, most of these
methods are developed on large-scale image datasets that contain natural
objects with clear textures, outlines, and distinct color contrasts. It remains
uncertain whether these methods are equally effective for medical imaging,
where the regions of interest often blend subtly and indistinctly with the
surrounding tissues. In this study, we propose an alternative method that uses
contralateral mammograms to train a neural network to encode similar embeddings
when a pair contains both normal images and different embeddings when a pair
contains normal and abnormal images. Our approach leverages the natural
symmetry of human body as weak labels to learn to distinguish abnormal lesions
from background tissues in a fully unsupervised manner. Our findings suggest
that it's feasible by incorporating soft labels derived from the Euclidean
distances between the embeddings of the image pairs into the Siamese network
loss. Our method demonstrates superior performance in mammogram patch
classification compared to existing self-supervised learning methods. This
approach not only leverages a vast amount of image data effectively but also
minimizes reliance on costly labels, a significant advantage particularly in
the field of medical imaging.</div><div><a href='http://arxiv.org/abs/2401.05570v1'>2401.05570v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10851v1")'>HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of
  Histological Tissue Type in Whole Slide Images</div>
<div id='2402.10851v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T17:44:11Z</div><div>Authors: Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Arash Mohammadi, Konstantinos N. Plataniotis</div><div style='padding-top: 10px; width: 80ex'>Digital pathology involves converting physical tissue slides into
high-resolution Whole Slide Images (WSIs), which pathologists analyze for
disease-affected tissues. However, large histology slides with numerous
microscopic fields pose challenges for visual search. To aid pathologists,
Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently
examining WSIs and identifying diagnostically relevant regions. This paper
presents a novel histopathological image analysis method employing Weakly
Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first
such application. The proposed model is evaluated using the Atlas of Digital
Pathology (ADP) dataset and its performance is compared with other
histopathological semantic segmentation methodologies. The findings underscore
the potential of Capsule Networks in enhancing the precision and efficiency of
histopathological image analysis. Experimental results show that the proposed
model outperforms traditional methods in terms of accuracy and the mean
Intersection-over-Union (mIoU) metric.</div><div><a href='http://arxiv.org/abs/2402.10851v1'>2402.10851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07743v2")'>Equipping Computational Pathology Systems with Artifact Processing
  Pipelines: A Showcase for Computation and Performance Trade-offs</div>
<div id='2403.07743v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T15:22:05Z</div><div>Authors: Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio, Carlos Monteagudo, Emiel A. M. Janssen, Tahlita C. M. Zuiverloon, Chunmig Rong, Kjersti Engan</div><div style='padding-top: 10px; width: 80ex'>Histopathology is a gold standard for cancer diagnosis under a microscopic
examination. However, histological tissue processing procedures result in
artifacts, which are ultimately transferred to the digitized version of glass
slides, known as whole slide images (WSIs). Artifacts are diagnostically
irrelevant areas and may result in wrong deep learning (DL) algorithms
predictions. Therefore, detecting and excluding artifacts in the computational
pathology (CPATH) system is essential for reliable automated diagnosis. In this
paper, we propose a mixture of experts (MoE) scheme for detecting five notable
artifacts, including damaged tissue, blur, folded tissue, air bubbles, and
histologically irrelevant blood from WSIs. First, we train independent binary
DL models as experts to capture particular artifact morphology. Then, we
ensemble their predictions using a fusion mechanism. We apply probabilistic
thresholding over the final probability distribution to improve the sensitivity
of the MoE. We developed DL pipelines using two MoEs and two multiclass models
of state-of-the-art deep convolutional neural networks (DCNNs) and vision
transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed
simpler multiclass models and were tested on datasets from different hospitals
and cancer types, where MoE using DCNNs yielded the best results. The proposed
MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining
less computational cost for inference than MoE using ViTs. This best
performance of MoEs comes with relatively higher computational trade-offs than
multiclass models. The proposed artifact detection pipeline will not only
ensure reliable CPATH predictions but may also provide quality control.</div><div><a href='http://arxiv.org/abs/2403.07743v2'>2403.07743v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01386v3")'>Tissue Artifact Segmentation and Severity Analysis for Automated
  Diagnosis Using Whole Slide Images</div>
<div id='2401.01386v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T19:58:36Z</div><div>Authors: Galib Muhammad Shahriar Himel</div><div style='padding-top: 10px; width: 80ex'>Traditionally, pathological analysis and diagnosis are performed by manually
eyeballing glass slide specimens under a microscope by an expert. The whole
slide image is the digital specimen produced from the glass slide. Whole slide
image enabled specimens to be observed on a computer screen and led to
computational pathology where computer vision and artificial intelligence are
utilized for automated analysis and diagnosis. With the current computational
advancement, the entire whole slide image can be analyzed autonomously without
human supervision. However, the analysis could fail or lead to wrong diagnosis
if the whole slide image is affected by tissue artifacts such as tissue fold or
air bubbles depending on the severity. Existing artifact detection methods rely
on experts for severity assessment to eliminate artifact affected regions from
the analysis. This process is time consuming, exhausting and undermines the
goal of automated analysis or removal of artifacts without evaluating their
severity, which could result in the loss of diagnostically important data.
Therefore, it is necessary to detect artifacts and then assess their severity
automatically. In this paper, we propose a system that incorporates severity
evaluation with artifact detection utilizing convolutional neural networks. The
proposed system uses DoubleUNet to segment artifacts and an ensemble network of
six fine tuned convolutional neural network models to determine severity. This
method outperformed current state of the art in accuracy by 9 percent for
artifact segmentation and achieved a strong correlation of 97 percent with the
evaluation of pathologists for severity assessment. The robustness of the
system was demonstrated using our proposed heterogeneous dataset and practical
usability was ensured by integrating it with an automated analysis system.</div><div><a href='http://arxiv.org/abs/2401.01386v3'>2401.01386v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13511v1")'>Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images</div>
<div id='2401.13511v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T15:09:12Z</div><div>Authors: Ruben T. Lucassen, Willeke A. M. Blokx, Mitko Veta</div><div style='padding-top: 10px; width: 80ex'>Tissue segmentation is a routine preprocessing step to reduce the
computational cost of whole slide image (WSI) analysis by excluding background
regions. Traditional image processing techniques are commonly used for tissue
segmentation, but often require manual adjustments to parameter values for
atypical cases, fail to exclude all slide and scanning artifacts from the
background, and are unable to segment adipose tissue. Pen marking artifacts in
particular can be a potential source of bias for subsequent analyses if not
removed. In addition, several applications require the separation of individual
cross-sections, which can be challenging due to tissue fragmentation and
adjacent positioning. To address these problems, we develop a convolutional
neural network for tissue and pen marking segmentation using a dataset of 200
H&amp;E stained WSIs. For separating tissue cross-sections, we propose a novel
post-processing method based on clustering predicted centroid locations of the
cross-sections in a 2D histogram. On an independent test set, the model
achieved a mean Dice score of 0.981$\pm$0.033 for tissue segmentation and a
mean Dice score of 0.912$\pm$0.090 for pen marking segmentation. The mean
absolute difference between the number of annotated and separated
cross-sections was 0.075$\pm$0.350. Our results demonstrate that the proposed
model can accurately segment H&amp;E stained tissue cross-sections and pen markings
in WSIs while being robust to many common slide and scanning artifacts. The
model with trained model parameters and post-processing method are made
publicly available as a Python package called SlideSegmenter.</div><div><a href='http://arxiv.org/abs/2401.13511v1'>2401.13511v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06545v1")'>ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico
  Data Generation</div>
<div id='2403.06545v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T09:45:34Z</div><div>Authors: Dominik Winter, Nicolas Triltsch, Philipp Plewa, Marco Rosati, Thomas Padel, Ross Hill, Markus Schick, Nicolas Brieu</div><div style='padding-top: 10px; width: 80ex'>The creation of in-silico datasets can expand the utility of existing
annotations to new domains with different staining patterns in computational
pathology. As such, it has the potential to significantly lower the cost
associated with building large and pixel precise datasets needed to train
supervised deep learning models. We propose a novel approach for the generation
of in-silico immunohistochemistry (IHC) images by disentangling morphology
specific IHC stains into separate image channels in immunofluorescence (IF)
images. The proposed approach qualitatively and quantitatively outperforms
baseline methods as proven by training nucleus segmentation models on the
created in-silico datasets.</div><div><a href='http://arxiv.org/abs/2403.06545v1'>2403.06545v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09990v1")'>TIAViz: A Browser-based Visualization Tool for Computational Pathology
  Models</div>
<div id='2402.09990v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:54:46Z</div><div>Authors: Mark Eastwood, John Pocock, Mostafa Jahanifar, Adam Shephard, Skiros Habib, Ethar Alzaid, Abdullah Alsalemi, Jan Lukas Robertus, Nasir Rajpoot, Shan Raza, Fayyaz Minhas</div><div style='padding-top: 10px; width: 80ex'>Digital pathology has gained significant traction in modern healthcare
systems. This shift from optical microscopes to digital imagery brings with it
the potential for improved diagnosis, efficiency, and the integration of AI
tools into the pathologists workflow. A critical aspect of this is
visualization. Throughout the development of a machine learning (ML) model in
digital pathology, it is crucial to have flexible, openly available tools to
visualize models, from their outputs and predictions to the underlying
annotations and images used to train or test a model. We introduce TIAViz, a
Python-based visualization tool built into TIAToolbox which allows flexible,
interactive, fully zoomable overlay of a wide variety of information onto whole
slide images, including graphs, heatmaps, segmentations, annotations and other
WSIs. The UI is browser-based, allowing use either locally, on a remote
machine, or on a server to provide publicly available demos. This tool is open
source and is made available at:
https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation
(pip install tiatoolbox) and conda as part of TIAToolbox.</div><div><a href='http://arxiv.org/abs/2402.09990v1'>2402.09990v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09100v1")'>Virtual birefringence imaging and histological staining of amyloid
  deposits in label-free tissue using autofluorescence microscopy and deep
  learning</div>
<div id='2403.09100v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T04:48:06Z</div><div>Authors: Xilin Yang, Bijie Bai, Yijie Zhang, Musa Aydin, Sahan Yoruc Selcuk, Zhen Guo, Gregory A. Fishbein, Karine Atlan, William Dean Wallace, Nir Pillar, Aydogan Ozcan</div><div style='padding-top: 10px; width: 80ex'>Systemic amyloidosis is a group of diseases characterized by the deposition
of misfolded proteins in various organs and tissues, leading to progressive
organ dysfunction and failure. Congo red stain is the gold standard chemical
stain for the visualization of amyloid deposits in tissue sections, as it forms
complexes with the misfolded proteins and shows a birefringence pattern under
polarized light microscopy. However, Congo red staining is tedious and costly
to perform, and prone to false diagnoses due to variations in the amount of
amyloid, staining quality and expert interpretation through manual examination
of tissue under a polarization microscope. Here, we report the first
demonstration of virtual birefringence imaging and virtual Congo red staining
of label-free human tissue to show that a single trained neural network can
rapidly transform autofluorescence images of label-free tissue sections into
brightfield and polarized light microscopy equivalent images, matching the
histochemically stained versions of the same samples. We demonstrate the
efficacy of our method with blind testing and pathologist evaluations on
cardiac tissue where the virtually stained images agreed well with the
histochemically stained ground truth images. Our virtually stained polarization
and brightfield images highlight amyloid birefringence patterns in a
consistent, reproducible manner while mitigating diagnostic challenges due to
variations in the quality of chemical staining and manual imaging processes as
part of the clinical workflow.</div><div><a href='http://arxiv.org/abs/2403.09100v1'>2403.09100v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05396v1")'>HistGen: Histopathology Report Generation via Local-Global Feature
  Encoding and Cross-modal Context Interaction</div>
<div id='2403.05396v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T15:51:43Z</div><div>Authors: Zhengrui Guo, Jiabo Ma, Yingxue Xu, Yihui Wang, Liansheng Wang, Hao Chen</div><div style='padding-top: 10px; width: 80ex'>Histopathology serves as the gold standard in cancer diagnosis, with clinical
reports being vital in interpreting and understanding this process, guiding
cancer treatment and patient care. The automation of histopathology report
generation with deep learning stands to significantly enhance clinical
efficiency and lessen the labor-intensive, time-consuming burden on
pathologists in report writing. In pursuit of this advancement, we introduce
HistGen, a multiple instance learning-empowered framework for histopathology
report generation together with the first benchmark dataset for evaluation.
Inspired by diagnostic and report-writing workflows, HistGen features two
delicately designed modules, aiming to boost report generation by aligning
whole slide images (WSIs) and diagnostic reports from local and global
granularity. To achieve this, a local-global hierarchical encoder is developed
for efficient visual feature aggregation from a region-to-slide perspective.
Meanwhile, a cross-modal context module is proposed to explicitly facilitate
alignment and interaction between distinct modalities, effectively bridging the
gap between the extensive visual sequences of WSIs and corresponding highly
summarized reports. Experimental results on WSI report generation show the
proposed model outperforms state-of-the-art (SOTA) models by a large margin.
Moreover, the results of fine-tuning our model on cancer subtyping and survival
analysis tasks further demonstrate superior performance compared to SOTA
methods, showcasing strong transfer learning capability. Dataset, model
weights, and source code are available in
https://github.com/dddavid4real/HistGen.</div><div><a href='http://arxiv.org/abs/2403.05396v1'>2403.05396v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07452v2")'>TriAug: Out-of-Distribution Detection for Imbalanced Breast Lesion in
  Ultrasound</div>
<div id='2402.07452v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T07:19:00Z</div><div>Authors: Yinyu Ye, Shijing Chen, Dong Ni, Ruobing Huang</div><div style='padding-top: 10px; width: 80ex'>Different diseases, such as histological subtypes of breast lesions, have
severely varying incidence rates. Even trained with substantial amount of
in-distribution (ID) data, models often encounter out-of-distribution (OOD)
samples belonging to unseen classes in clinical reality. To address this, we
propose a novel framework built upon a long-tailed OOD detection task for
breast ultrasound images. It is equipped with a triplet state augmentation
(TriAug) which improves ID classification accuracy while maintaining a
promising OOD detection performance. Meanwhile, we designed a balanced sphere
loss to handle the class imbalanced problem. Experimental results show that the
model outperforms state-of-art OOD approaches both in ID classification
(F1-score=42.12%) and OOD detection (AUROC=78.06%).</div><div><a href='http://arxiv.org/abs/2402.07452v2'>2402.07452v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09548v1")'>Breast Cancer Classification Using Gradient Boosting Algorithms Focusing
  on Reducing the False Negative and SHAP for Explainability</div>
<div id='2403.09548v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T16:35:43Z</div><div>Authors: João Manoel Herrera Pinheiro, Marcelo Becker</div><div style='padding-top: 10px; width: 80ex'>Cancer is one of the diseases that kill the most women in the world, with
breast cancer being responsible for the highest number of cancer cases and
consequently deaths. However, it can be prevented by early detection and,
consequently, early treatment. Any development for detection or perdition this
kind of cancer is important for a better healthy life. Many studies focus on a
model with high accuracy in cancer prediction, but sometimes accuracy alone may
not always be a reliable metric. This study implies an investigative approach
to studying the performance of different machine learning algorithms based on
boosting to predict breast cancer focusing on the recall metric. Boosting
machine learning algorithms has been proven to be an effective tool for
detecting medical diseases. The dataset of the University of California, Irvine
(UCI) repository has been utilized to train and test the model classifier that
contains their attributes. The main objective of this study is to use
state-of-the-art boosting algorithms such as AdaBoost, XGBoost, CatBoost and
LightGBM to predict and diagnose breast cancer and to find the most effective
metric regarding recall, ROC-AUC, and confusion matrix. Furthermore, our study
is the first to use these four boosting algorithms with Optuna, a library for
hyperparameter optimization, and the SHAP method to improve the
interpretability of our model, which can be used as a support to identify and
predict breast cancer. We were able to improve AUC or recall for all the models
and reduce the False Negative for AdaBoost and LigthGBM the final AUC were more
than 99.41\% for all models.</div><div><a href='http://arxiv.org/abs/2403.09548v1'>2403.09548v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08892v1")'>Weakly Supervised Segmentation of Vertebral Bodies with Iterative
  Slice-propagation</div>
<div id='2402.08892v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T01:56:31Z</div><div>Authors: Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao</div><div style='padding-top: 10px; width: 80ex'>Vertebral body (VB) segmentation is an important preliminary step towards
medical visual diagnosis for spinal diseases. However, most previous works
require pixel/voxel-wise strong supervisions, which is expensive, tedious and
time-consuming for experts to annotate. In this paper, we propose a Weakly
supervised Iterative Spinal Segmentation (WISS) method leveraging only four
corner landmark weak labels on a single sagittal slice to achieve automatic
volumetric segmentation from CT images for VBs. WISS first segments VBs on an
annotated sagittal slice in an iterative self-training manner. This
self-training method alternates between training and refining labels in the
training set. Then WISS proceeds to segment the whole VBs slice by slice with a
slice-propagation method to obtain volumetric segmentations. We evaluate the
performance of WISS on a private spinal metastases CT dataset and the public
lumbar CT dataset. On the first dataset, WISS achieves distinct improvements
with regard to two different backbones. For the second dataset, WISS achieves
dice coefficients of $91.7\%$ and $83.7\%$ for mid-sagittal slices and 3D CT
volumes, respectively, saving a lot of labeling costs and only sacrificing a
little segmentation performance.</div><div><a href='http://arxiv.org/abs/2402.08892v1'>2402.08892v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11001v1")'>Topologically faithful multi-class segmentation in medical images</div>
<div id='2403.11001v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T19:11:57Z</div><div>Authors: Alexander H. Berger, Nico Stucki, Laurin Lux, Vincent Buergin, Suprosanna Shit, Anna Banaszak, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold</div><div style='padding-top: 10px; width: 80ex'>Topological accuracy in medical image segmentation is a highly important
property for downstream applications such as network analysis and flow modeling
in vessels or cell counting. Recently, significant methodological advancements
have brought well-founded concepts from algebraic topology to binary
segmentation. However, these approaches have been underexplored in multi-class
segmentation scenarios, where topological errors are common. We propose a
general loss function for topologically faithful multi-class segmentation
extending the recent Betti matching concept, which is based on induced
matchings of persistence barcodes. We project the N-class segmentation problem
to N single-class segmentation tasks, which allows us to use 1-parameter
persistent homology making training of neural networks computationally
feasible. We validate our method on a comprehensive set of four medical
datasets with highly variant topological characteristics. Our loss formulation
significantly enhances topological correctness in cardiac, cell, artery-vein,
and Circle of Willis segmentation.</div><div><a href='http://arxiv.org/abs/2403.11001v1'>2403.11001v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15361v1")'>Learning Topological Representations for Deep Image Understanding</div>
<div id='2403.15361v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:23:37Z</div><div>Authors: Xiaoling Hu</div><div style='padding-top: 10px; width: 80ex'>In many scenarios, especially biomedical applications, the correct
delineation of complex fine-scaled structures such as neurons, tissues, and
vessels is critical for downstream analysis. Despite the strong predictive
power of deep learning methods, they do not provide a satisfactory
representation of these structures, thus creating significant barriers in
scalable annotation and downstream analysis. In this dissertation, we tackle
such challenges by proposing novel representations of these topological
structures in a deep learning framework. We leverage the mathematical tools
from topological data analysis, i.e., persistent homology and discrete Morse
theory, to develop principled methods for better segmentation and uncertainty
estimation, which will become powerful tools for scalable annotation.</div><div><a href='http://arxiv.org/abs/2403.15361v1'>2403.15361v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01160v1")'>Train-Free Segmentation in MRI with Cubical Persistent Homology</div>
<div id='2401.01160v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T11:43:49Z</div><div>Authors: Anton François, Raphaël Tinarrage</div><div style='padding-top: 10px; width: 80ex'>We describe a new general method for segmentation in MRI scans using
Topological Data Analysis (TDA), offering several advantages over traditional
machine learning approaches. It works in three steps, first identifying the
whole object to segment via automatic thresholding, then detecting a
distinctive subset whose topology is known in advance, and finally deducing the
various components of the segmentation. Although convoking classical ideas of
TDA, such an algorithm has never been proposed separately from deep learning
methods. To achieve this, our approach takes into account, in addition to the
homology of the image, the localization of representative cycles, a piece of
information that seems never to have been exploited in this context. In
particular, it offers the ability to perform segmentation without the need for
large annotated data sets. TDA also provides a more interpretable and stable
framework for segmentation by explicitly mapping topological features to
segmentation components. By adapting the geometric object to be detected, the
algorithm can be adjusted to a wide range of data segmentation challenges. We
carefully study the examples of glioblastoma segmentation in brain MRI, where a
sphere is to be detected, as well as myocardium in cardiac MRI, involving a
cylinder, and cortical plate detection in fetal brain MRI, whose 2D slices are
circles. We compare our method to state-of-the-art algorithms.</div><div><a href='http://arxiv.org/abs/2401.01160v1'>2401.01160v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07536v1")'>LaB-GATr: geometric algebra transformers for large biomedical surface
  and volume meshes</div>
<div id='2403.07536v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:19:46Z</div><div>Authors: Julian Suk, Baris Imre, Jelmer M. Wolterink</div><div style='padding-top: 10px; width: 80ex'>Many anatomical structures can be described by surface or volume meshes.
Machine learning is a promising tool to extract information from these 3D
models. However, high-fidelity meshes often contain hundreds of thousands of
vertices, which creates unique challenges in building deep neural network
architectures. Furthermore, patient-specific meshes may not be canonically
aligned which limits the generalisation of machine learning algorithms. We
propose LaB-GATr, a transfomer neural network with geometric tokenisation that
can effectively learn with large-scale (bio-)medical surface and volume meshes
through sequence compression and interpolation. Our method extends the recently
proposed geometric algebra transformer (GATr) and thus respects all Euclidean
symmetries, i.e. rotation, translation and reflection, effectively mitigating
the problem of canonical alignment between patients. LaB-GATr achieves
state-of-the-art results on three tasks in cardiovascular hemodynamics
modelling and neurodevelopmental phenotype prediction, featuring meshes of up
to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful
architecture for learning with high-fidelity meshes which has the potential to
enable interesting downstream applications. Our implementation is publicly
available.</div><div><a href='http://arxiv.org/abs/2403.07536v1'>2403.07536v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19062v2")'>Graph Convolutional Neural Networks for Automated Echocardiography View
  Recognition: A Holistic Approach</div>
<div id='2402.19062v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T11:45:24Z</div><div>Authors: Sarina Thomas, Cristiana Tiago, Børge Solli Andreassen, Svein Arne Aase, Jurica Šprem, Erik Steen, Anne Solberg, Guy Ben-Yosef</div><div style='padding-top: 10px; width: 80ex'>To facilitate diagnosis on cardiac ultrasound (US), clinical practice has
established several standard views of the heart, which serve as reference
points for diagnostic measurements and define viewports from which images are
acquired. Automatic view recognition involves grouping those images into
classes of standard views. Although deep learning techniques have been
successful in achieving this, they still struggle with fully verifying the
suitability of an image for specific measurements due to factors like the
correct location, pose, and potential occlusions of cardiac structures. Our
approach goes beyond view classification and incorporates a 3D mesh
reconstruction of the heart that enables several more downstream tasks, like
segmentation and pose estimation. In this work, we explore learning 3D heart
meshes via graph convolutions, using similar techniques to learn 3D meshes in
natural images, such as human pose estimation. As the availability of fully
annotated 3D images is limited, we generate synthetic US images from 3D meshes
by training an adversarial denoising diffusion model. Experiments were
conducted on synthetic and clinical cases for view recognition and structure
detection. The approach yielded good performance on synthetic images and,
despite being exclusively trained on synthetic data, it already showed
potential when applied to clinical images. With this proof-of-concept, we aim
to demonstrate the benefits of graphs to improve cardiac view recognition that
can ultimately lead to better efficiency in cardiac diagnosis.</div><div><a href='http://arxiv.org/abs/2402.19062v2'>2402.19062v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16937v1")'>Segmentation and Characterization of Macerated Fibers and Vessels Using
  Deep Learning</div>
<div id='2401.16937v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T12:04:56Z</div><div>Authors: Saqib Qamar, Abu Imran Baba, Stéphane Verger, Magnus Andersson</div><div style='padding-top: 10px; width: 80ex'>Purpose: Wood comprises different cell types, such as fibers and vessels,
defining its properties. Studying their shape, size, and arrangement in
microscopic images is crucial for understanding wood samples. Typically, this
involves macerating (soaking) samples in a solution to separate cells, then
spreading them on slides for imaging with a microscope that covers a wide area,
capturing thousands of cells. However, these cells often cluster and overlap in
images, making the segmentation difficult and time-consuming using standard
image-processing methods. Results: In this work, we develop an automatic deep
learning segmentation approach that utilizes the one-stage YOLOv8 model for
fast and accurate fiber and vessel segmentation and characterization in
microscopy images. The model can analyze 32640 x 25920 pixels images and
demonstrate effective cell detection and segmentation, achieving a mAP_0.5-0.95
of 78 %. To assess the model's robustness, we examined fibers from a
genetically modified tree line known for longer fibers. The outcomes were
comparable to previous manual measurements. Additionally, we created a
user-friendly web application for image analysis and provided the code for use
on Google Colab. Conclusion: By leveraging YOLOv8's advances, this work
provides a deep learning solution to enable efficient quantification and
analysis of wood cells suitable for practical applications.</div><div><a href='http://arxiv.org/abs/2401.16937v1'>2401.16937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02544v1")'>Coronary artery segmentation in non-contrast calcium scoring CT images
  using deep learning</div>
<div id='2403.02544v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T23:40:02Z</div><div>Authors: Mariusz Bujny, Katarzyna Jesionek, Jakub Nalepa, Karol Miszalski-Jamka, Katarzyna Widawka-Żak, Sabina Wolny, Marcin Kostur</div><div style='padding-top: 10px; width: 80ex'>Precise localization of coronary arteries in Computed Tomography (CT) scans
is critical from the perspective of medical assessment of coronary artery
disease. Although various methods exist that offer high-quality segmentation of
coronary arteries in cardiac contrast-enhanced CT scans, the potential of less
invasive, non-contrast CT in this area is still not fully exploited. Since such
fine anatomical structures are hardly visible in this type of medical images,
the existing methods are characterized by high recall and low precision, and
are used mainly for filtering of atherosclerotic plaques in the context of
calcium scoring. In this paper, we address this research gap and introduce a
deep learning algorithm for segmenting coronary arteries in multi-vendor
ECG-gated non-contrast cardiac CT images which benefits from a novel framework
for semi-automatic generation of Ground Truth (GT) via image registration. We
hypothesize that the proposed GT generation process is much more efficient in
this case than manual segmentation, since it allows for a fast generation of
large volumes of diverse data, which leads to well-generalizing models. To
investigate and thoroughly evaluate the segmentation quality based on such an
approach, we propose a novel method for manual mesh-to-image registration,
which is used to create our test-GT. The experimental study shows that the
trained model has significantly higher accuracy than the GT used for training,
and leads to the Dice and clDice metrics close to the interrater variability.</div><div><a href='http://arxiv.org/abs/2403.02544v1'>2403.02544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13049v1")'>CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography
  Angiography via Context-Aware Shifted Window Self-Attention</div>
<div id='2401.13049v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T19:17:20Z</div><div>Authors: Muhammad Imran, Jonathan R Krebs, Veera Rajasekhar Reddy Gopu, Brian Fazzone, Vishal Balaji Sivaraman, Amarjeet Kumar, Chelsea Viscardi, Robert Evans Heithaus, Benjamin Shickel, Yuyin Zhou, Michol A Cooper, Wei Shao</div><div style='padding-top: 10px; width: 80ex'>Advancements in medical imaging and endovascular grafting have facilitated
minimally invasive treatments for aortic diseases. Accurate 3D segmentation of
the aorta and its branches is crucial for interventions, as inaccurate
segmentation can lead to erroneous surgical planning and endograft
construction. Previous methods simplified aortic segmentation as a binary image
segmentation problem, overlooking the necessity of distinguishing between
individual aortic branches. In this paper, we introduce Context Infused
Swin-UNet (CIS-UNet), a deep learning model designed for multi-class
segmentation of the aorta and thirteen aortic branches. Combining the strengths
of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts
a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric
decoder, skip connections, and a novel Context-aware Shifted Window
Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a
unique utilization of the patch merging layer, distinct from conventional Swin
transformers. It efficiently condenses the feature map, providing a global
spatial context and enhancing performance when applied at the bottleneck layer,
offering superior computational efficiency and segmentation accuracy compared
to the Swin transformers. We trained our model on computed tomography (CT)
scans from 44 patients and tested it on 15 patients. CIS-UNet outperformed the
state-of-the-art SwinUNetR segmentation model, which is solely based on Swin
transformers, by achieving a superior mean Dice coefficient of 0.713 compared
to 0.697, and a mean surface distance of 2.78 mm compared to 3.39 mm.
CIS-UNet's superior 3D aortic segmentation offers improved precision and
optimization for planning endovascular treatments. Our dataset and code will be
publicly available.</div><div><a href='http://arxiv.org/abs/2401.13049v1'>2401.13049v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06224v1")'>Leveraging Frequency Domain Learning in 3D Vessel Segmentation</div>
<div id='2401.06224v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T19:07:58Z</div><div>Authors: Xinyuan Wang, Chengwei Pan, Hongming Dai, Gangming Zhao, Jinpeng Li, Xiao Zhang, Yizhou Yu</div><div style='padding-top: 10px; width: 80ex'>Coronary microvascular disease constitutes a substantial risk to human
health. Employing computer-aided analysis and diagnostic systems, medical
professionals can intervene early in disease progression, with 3D vessel
segmentation serving as a crucial component. Nevertheless, conventional U-Net
architectures tend to yield incoherent and imprecise segmentation outcomes,
particularly for small vessel structures. While models with attention
mechanisms, such as Transformers and large convolutional kernels, demonstrate
superior performance, their extensive computational demands during training and
inference lead to increased time complexity. In this study, we leverage Fourier
domain learning as a substitute for multi-scale convolutional kernels in 3D
hierarchical segmentation models, which can reduce computational expenses while
preserving global receptive fields within the network. Furthermore, a
zero-parameter frequency domain fusion method is designed to improve the skip
connections in U-Net architecture. Experimental results on a public dataset and
an in-house dataset indicate that our novel Fourier transformation-based
network achieves remarkable dice performance (84.37\% on ASACA500 and 80.32\%
on ImageCAS) in tubular vessel segmentation tasks and substantially reduces
computational requirements without compromising global receptive fields.</div><div><a href='http://arxiv.org/abs/2401.06224v1'>2401.06224v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01179v1")'>Freeze the backbones: A Parameter-Efficient Contrastive Approach to
  Robust Medical Vision-Language Pre-training</div>
<div id='2401.01179v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T12:14:41Z</div><div>Authors: Jiuming Qin, Che Liu, Sibo Cheng, Yike Guo, Rossella Arcucci</div><div style='padding-top: 10px; width: 80ex'>Modern healthcare often utilises radiographic images alongside textual
reports for diagnostics, encouraging the use of Vision-Language Self-Supervised
Learning (VL-SSL) with large pre-trained models to learn versatile medical
vision representations. However, most existing VL-SSL frameworks are trained
end-to-end, which is computation-heavy and can lose vital prior information
embedded in pre-trained encoders. To address both issues, we introduce the
backbone-agnostic Adaptor framework, which preserves medical knowledge in
pre-trained image and text encoders by keeping them frozen, and employs a
lightweight Adaptor module for cross-modal learning. Experiments on medical
image classification and segmentation tasks across three datasets reveal that
our framework delivers competitive performance while cutting trainable
parameters by over 90% compared to current pre-training approaches. Notably,
when fine-tuned with just 1% of data, Adaptor outperforms several
Transformer-based methods trained on full datasets in medical image
segmentation.</div><div><a href='http://arxiv.org/abs/2401.01179v1'>2401.01179v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04626v1")'>MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training
  with Masked Autoencoder</div>
<div id='2403.04626v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T16:11:43Z</div><div>Authors: Lei Li, Tianfang Zhang, Xinglin Zhang, Jiaqi Liu, Bingqi Ma, Yan Luo, Tao Chen</div><div style='padding-top: 10px; width: 80ex'>Within the domain of medical analysis, extensive research has explored the
potential of mutual learning between Masked Autoencoders(MAEs) and multimodal
data. However, the impact of MAEs on intermodality remains a key challenge. We
introduce MedFLIP, a Fast Language-Image Pre-training method for Medical
analysis. We explore MAEs for zero-shot learning with crossed domains, which
enhances the model ability to learn from limited data, a common scenario in
medical diagnostics. We verify that masking an image does not affect intermodal
learning. Furthermore, we propose the SVD loss to enhance the representation
learning for characteristics of medical images, aiming to improve
classification accuracy by leveraging the structural intricacies of such data.
Lastly, we validate using language will improve the zero-shot performance for
the medical image analysis. MedFLIP scaling of the masking process marks an
advancement in the field, offering a pathway to rapid and precise medical image
analysis without the traditional computational bottlenecks. Through experiments
and validation, MedFLIP demonstrates efficient performance improvements,
setting an explored standard for future research and application in medical
diagnostics.</div><div><a href='http://arxiv.org/abs/2403.04626v1'>2403.04626v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04722v1")'>U-Mamba: Enhancing Long-range Dependency for Biomedical Image
  Segmentation</div>
<div id='2401.04722v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T18:53:20Z</div><div>Authors: Jun Ma, Feifei Li, Bo Wang</div><div style='padding-top: 10px; width: 80ex'>Convolutional Neural Networks (CNNs) and Transformers have been the most
popular architectures for biomedical image segmentation, but both of them have
limited ability to handle long-range dependencies because of inherent locality
or computational complexity. To address this challenge, we introduce U-Mamba, a
general-purpose network for biomedical image segmentation. Inspired by the
State Space Sequence Models (SSMs), a new family of deep sequence models known
for their strong capability in handling long sequences, we design a hybrid
CNN-SSM block that integrates the local feature extraction power of
convolutional layers with the abilities of SSMs for capturing the long-range
dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it
to automatically adapt to various datasets without manual intervention. We
conduct extensive experiments on four diverse tasks, including the 3D abdominal
organ segmentation in CT and MR images, instrument segmentation in endoscopy
images, and cell segmentation in microscopy images. The results reveal that
U-Mamba outperforms state-of-the-art CNN-based and Transformer-based
segmentation networks across all tasks. This opens new avenues for efficient
long-range dependency modeling in biomedical image analysis. The code, models,
and data are publicly available at https://wanglab.ai/u-mamba.html.</div><div><a href='http://arxiv.org/abs/2401.04722v1'>2401.04722v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03230v2")'>Architecture Analysis and Benchmarking of 3D U-shaped Deep Learning
  Models for Thoracic Anatomical Segmentation</div>
<div id='2402.03230v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:43:02Z</div><div>Authors: Arash Harirpoush, Amirhossein Rasoulian, Marta Kersten-Oertel, Yiming Xiao</div><div style='padding-top: 10px; width: 80ex'>Recent rising interests in patient-specific thoracic surgical planning and
simulation require efficient and robust creation of digital anatomical models
from automatic medical image segmentation algorithms. Deep learning (DL) is now
state-of-the-art in various radiological tasks, and U-shaped DL models have
particularly excelled in medical image segmentation since the inception of the
2D UNet. To date, many variants of U-shaped models have been proposed by the
integration of different attention mechanisms and network configurations.
Systematic benchmark studies which analyze the architecture of these models by
leveraging the recent development of the multi-label databases, can provide
valuable insights for clinical deployment and future model designs, but such
studies are still rare. We conduct the first systematic benchmark study for
variants of 3D U-shaped models (3DUNet, STUNet, AttentionUNet, SwinUNETR,
FocalSegNet, and a novel 3D SwinUnet with four variants) with a focus on
CT-based anatomical segmentation for thoracic surgery. Our study systematically
examines the impact of different attention mechanisms, the number of resolution
stages, and network configurations on segmentation accuracy and computational
complexity. To allow cross-reference with other recent benchmarking studies, we
also included a performance assessment of the BTCV abdominal structural
segmentation. With the STUNet ranking at the top, our study demonstrated the
value of CNN-based U-shaped models for the investigated tasks and the benefit
of residual blocks in network configuration designs to boost segmentation
performance.</div><div><a href='http://arxiv.org/abs/2402.03230v2'>2402.03230v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07008v1")'>An Optimization Framework for Processing and Transfer Learning for the
  Brain Tumor Segmentation</div>
<div id='2402.07008v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T18:03:15Z</div><div>Authors: Tianyi Ren, Ethan Honey, Harshitha Rebala, Abhishek Sharma, Agamdeep Chopra, Mehmet Kurt</div><div style='padding-top: 10px; width: 80ex'>Tumor segmentation from multi-modal brain MRI images is a challenging task
due to the limited samples, high variance in shapes and uneven distribution of
tumor morphology. The performance of automated medical image segmentation has
been significant improvement by the recent advances in deep learning. However,
the model predictions have not yet reached the desired level for clinical use
in terms of accuracy and generalizability. In order to address the distinct
problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed
an optimization framework based on a 3D U-Net model for brain tumor
segmentation. This framework incorporates a range of techniques, including
various pre-processing and post-processing techniques, and transfer learning.
On the validation datasets, this multi-modality brain tumor segmentation
framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on
Challenges 1, 2, 3 respectively.</div><div><a href='http://arxiv.org/abs/2402.07008v1'>2402.07008v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08404v2")'>Training and Comparison of nnU-Net and DeepMedic Methods for
  Autosegmentation of Pediatric Brain Tumors</div>
<div id='2401.08404v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:44:06Z</div><div>Authors: Arastoo Vossough, Nastaran Khalili, Ariana M. Familiar, Deep Gandhi, Karthik Viswanathan, Wenxin Tu, Debanjan Haldar, Sina Bagheri, Hannah Anderson, Shuvanjan Haldar, Phillip B. Storm, Adam Resnick, Jeffrey B. Ware, Ali Nabavizadeh, Anahita Fathi Kazerooni</div><div style='padding-top: 10px; width: 80ex'>Brain tumors are the most common solid tumors and the leading cause of
cancer-related death among children. Tumor segmentation is essential in
surgical and treatment planning, and response assessment and monitoring.
However, manual segmentation is time-consuming and has high inter-operator
variability, underscoring the need for more efficient methods. We compared two
deep learning-based 3D segmentation models, DeepMedic and nnU-Net, after
training with pediatric-specific multi-institutional brain tumor data using
based on multi-parametric MRI scans.Multi-parametric preoperative MRI scans of
339 pediatric patients (n=293 internal and n=46 external cohorts) with a
variety of tumor subtypes, were preprocessed and manually segmented into four
tumor subregions, i.e., enhancing tumor (ET), non-enhancing tumor (NET), cystic
components (CC), and peritumoral edema (ED). After training, performance of the
two models on internal and external test sets was evaluated using Dice scores,
sensitivity, and Hausdorff distance with reference to ground truth manual
segmentations. Dice score for nnU-Net internal test sets was (mean +/- SD
(median)) 0.9+/-0.07 (0.94) for WT, 0.77+/-0.29 for ET, 0.66+/-0.32 for NET,
0.71+/-0.33 for CC, and 0.71+/-0.40 for ED, respectively. For DeepMedic the
Dice scores were 0.82+/-0.16 for WT, 0.66+/-0.32 for ET, 0.48+/-0.27, for NET,
0.48+/-0.36 for CC, and 0.19+/-0.33 for ED, respectively. Dice scores were
significantly higher for nnU-Net (p&lt;=0.01). External validation of the trained
nnU-Net model on the multi-institutional BraTS-PEDs 2023 dataset revealed high
generalization capability in segmentation of whole tumor and tumor core with
Dice scores of 0.87+/-0.13 (0.91) and 0.83+/-0.18 (0.89), respectively.
Pediatric-specific data trained nnU-Net model is superior to DeepMedic for
whole tumor and subregion segmentation of pediatric brain tumors.</div><div><a href='http://arxiv.org/abs/2401.08404v2'>2401.08404v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06499v1")'>Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner
  UNet</div>
<div id='2401.06499v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T10:46:19Z</div><div>Authors: Sumit Pandey, Satyasaran Changdar, Mathias Perslev, Erik B Dam</div><div style='padding-top: 10px; width: 80ex'>Automated segmentation of distinct tumor regions is critical for accurate
diagnosis and treatment planning in pediatric brain tumors. This study
evaluates the efficacy of the Multi-Planner U-Net (MPUnet) approach in
segmenting different tumor subregions across three challenging datasets:
Pediatrics Tumor Challenge (PED), Brain Metastasis Challenge (MET), and
Sub-Sahara-Africa Adult Glioma (SSA). These datasets represent diverse
scenarios and anatomical variations, making them suitable for assessing the
robustness and generalization capabilities of the MPUnet model. By utilizing
multi-planar information, the MPUnet architecture aims to enhance segmentation
accuracy. Our results show varying performance levels across the evaluated
challenges, with the tumor core (TC) class demonstrating relatively higher
segmentation accuracy. However, variability is observed in the segmentation of
other classes, such as the edema and enhancing tumor (ET) regions. These
findings emphasize the complexity of brain tumor segmentation and highlight the
potential for further refinement of the MPUnet approach and inclusion of MRI
more data and preprocessing.</div><div><a href='http://arxiv.org/abs/2401.06499v1'>2401.06499v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07092v1")'>A cascaded deep network for automated tumor detection and segmentation
  in clinical PET imaging of diffuse large B-cell lymphoma</div>
<div id='2403.07092v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T18:36:55Z</div><div>Authors: Shadab Ahamed, Natalia Dubljevic, Ingrid Bloise, Claire Gowdy, Patrick Martineau, Don Wilson, Carlos F. Uribe, Arman Rahmim, Fereshteh Yousefirizi</div><div style='padding-top: 10px; width: 80ex'>Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL)
from PET images has important implications for estimation of total metabolic
tumor volume, radiomics analysis, surgical intervention and radiotherapy.
Manual segmentation of tumors in whole-body PET images is time-consuming,
labor-intensive and operator-dependent. In this work, we develop and validate a
fast and efficient three-step cascaded deep learning model for automated
detection and segmentation of DLBCL tumors from PET images. As compared to a
single end-to-end network for segmentation of tumors in whole-body PET images,
our three-step model is more effective (improves 3D Dice score from 58.9% to
78.1%) since each of its specialized modules, namely the slice classifier, the
tumor detector and the tumor segmentor, can be trained independently to a high
degree of skill to carry out a specific task, rather than a single network with
suboptimal performance on overall segmentation.</div><div><a href='http://arxiv.org/abs/2403.07092v1'>2403.07092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07105v1")'>A slice classification neural network for automated classification of
  axial PET/CT slices from a multi-centric lymphoma dataset</div>
<div id='2403.07105v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T18:57:45Z</div><div>Authors: Shadab Ahamed, Yixi Xu, Ingrid Bloise, Joo H. O, Carlos F. Uribe, Rahul Dodhia, Juan L. Ferres, Arman Rahmim</div><div style='padding-top: 10px; width: 80ex'>Automated slice classification is clinically relevant since it can be
incorporated into medical image segmentation workflows as a preprocessing step
that would flag slices with a higher probability of containing tumors, thereby
directing physicians attention to the important slices. In this work, we train
a ResNet-18 network to classify axial slices of lymphoma PET/CT images
(collected from two institutions) depending on whether the slice intercepted a
tumor (positive slice) in the 3D image or if the slice did not (negative
slice). Various instances of the network were trained on 2D axial datasets
created in different ways: (i) slice-level split and (ii) patient-level split;
inputs of different types were used: (i) only PET slices and (ii) concatenated
PET and CT slices; and different training strategies were employed: (i)
center-aware (CAW) and (ii) center-agnostic (CAG). Model performances were
compared using the area under the receiver operating characteristic curve
(AUROC) and the area under the precision-recall curve (AUPRC), and various
binary classification metrics. We observe and describe a performance
overestimation in the case of slice-level split as compared to the
patient-level split training. The model trained using patient-level split data
with the network input containing only PET slices in the CAG training regime
was the best performing/generalizing model on a majority of metrics. Our models
were additionally more closely compared using the sensitivity metric on the
positive slices from their respective test sets.</div><div><a href='http://arxiv.org/abs/2403.07105v1'>2403.07105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11230v1")'>Simple 2D Convolutional Neural Network-based Approach for COVID-19
  Detection</div>
<div id='2403.11230v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T14:34:51Z</div><div>Authors: Chih-Chung Hsu, Chia-Ming Lee, Yang Fan Chiang, Yi-Shiuan Chou, Chih-Yu Jiang, Shen-Chieh Tai, Chi-Han Tsai</div><div style='padding-top: 10px; width: 80ex'>This study explores the use of deep learning techniques for analyzing lung
Computed Tomography (CT) images. Classic deep learning approaches face
challenges with varying slice counts and resolutions in CT images, a diversity
arising from the utilization of assorted scanning equipment. Typically,
predictions are made on single slices which are then combined for a
comprehensive outcome. Yet, this method does not incorporate learning features
specific to each slice, leading to a compromise in effectiveness. To address
these challenges, we propose an advanced Spatial-Slice Feature Learning
(SSFL++) framework specifically tailored for CT scans. It aims to filter out
out-of-distribution (OOD) data within the entire CT scan, allowing us to select
essential spatial-slice features for analysis by reducing data redundancy by
70\%. Additionally, we introduce a Kernel-Density-based slice Sampling (KDS)
method to enhance stability during training and inference phases, thereby
accelerating convergence and enhancing overall performance. Remarkably, our
experiments reveal that our model achieves promising results with a simple
EfficientNet-2D (E2D) model. The effectiveness of our approach is confirmed on
the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop.</div><div><a href='http://arxiv.org/abs/2403.11230v1'>2403.11230v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00257v1")'>Robust deep labeling of radiological emphysema subtypes using squeeze
  and excitation convolutional neural networks: The MESA Lung and SPIROMICS
  Studies</div>
<div id='2403.00257v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T03:45:56Z</div><div>Authors: Artur Wysoczanski, Nabil Ettehadi, Soroush Arabshahi, Yifei Sun, Karen Hinkley Stukovsky, Karol E. Watson, MeiLan K. Han, Erin D Michos, Alejandro P. Comellas, Eric A. Hoffman, Andrew F. Laine, R. Graham Barr, Elsa D. Angelini</div><div style='padding-top: 10px; width: 80ex'>Pulmonary emphysema, the progressive, irreversible loss of lung tissue, is
conventionally categorized into three subtypes identifiable on pathology and on
lung computed tomography (CT) images. Recent work has led to the unsupervised
learning of ten spatially-informed lung texture patterns (sLTPs) on lung CT,
representing distinct patterns of emphysematous lung parenchyma based on both
textural appearance and spatial location within the lung, and which aggregate
into 6 robust and reproducible CT Emphysema Subtypes (CTES). Existing methods
for sLTP segmentation, however, are slow and highly sensitive to changes in CT
acquisition protocol. In this work, we present a robust 3-D
squeeze-and-excitation CNN for supervised classification of sLTPs and CTES on
lung CT. Our results demonstrate that this model achieves accurate and
reproducible sLTP segmentation on lung CTscans, across two independent cohorts
and independently of scanner manufacturer and model.</div><div><a href='http://arxiv.org/abs/2403.00257v1'>2403.00257v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08042v1")'>CT evaluation of 2D and 3D holistic deep learning methods for the
  volumetric segmentation of airway lesions</div>
<div id='2403.08042v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T19:34:50Z</div><div>Authors: Amel Imene Hadj Bouzid, Baudouin Denis de Senneville, Fabien Baldacci, Pascal Desbarats, Patrick Berger, Ilyes Benlala, Gaël Dournes</div><div style='padding-top: 10px; width: 80ex'>This research embarked on a comparative exploration of the holistic
segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D
and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized
data from two CF reference centers, covering five major CF structural changes.
Initially, it compared the 2D and 3D models, highlighting the 3D model's
superior capability in capturing complex features like mucus plugs and
consolidations. To improve the 2D model's performance, a loss adapted to fine
structures segmentation was implemented and evaluated, significantly enhancing
its accuracy, though not surpassing the 3D model's performance. The models
underwent further validation through external evaluation against pulmonary
function tests (PFTs), confirming the robustness of the findings. Moreover,
this study went beyond comparing metrics; it also included comprehensive
assessments of the models' interpretability and reliability, providing valuable
insights for their clinical application.</div><div><a href='http://arxiv.org/abs/2403.08042v1'>2403.08042v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00728v1")'>MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for
  Chest X-Ray Image Classification</div>
<div id='2401.00728v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T11:50:01Z</div><div>Authors: Saurabh Agarwal, K. V. Arya, Yogesh Kumar Meena</div><div style='padding-top: 10px; width: 80ex'>Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary
diseases. However, manual interpretation of these images is time-consuming and
error-prone. Automated systems utilizing convolutional neural networks (CNNs)
have shown promise in improving the accuracy and efficiency of chest X-ray
image classification. While previous work has mainly focused on using feature
maps from the final convolution layer, there is a need to explore the benefits
of leveraging additional layers for improved disease classification. Extracting
robust features from limited medical image datasets remains a critical
challenge. In this paper, we propose a novel deep learning-based multilayer
multimodal fusion model that emphasizes extracting features from different
layers and fusing them. Our disease detection model considers the
discriminatory information captured by each layer. Furthermore, we propose the
fusion of different-sized feature maps (FDSFM) module to effectively merge
feature maps from diverse layers. The proposed model achieves a significantly
higher accuracy of 97.21% and 99.60% for both three-class and two-class
classifications, respectively. The proposed multilayer multimodal fusion model,
along with the FDSFM module, holds promise for accurate disease classification
and can also be extended to other disease classifications in chest X-ray
images.</div><div><a href='http://arxiv.org/abs/2401.00728v1'>2401.00728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15111v1")'>Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive
  Learning</div>
<div id='2401.15111v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:03:57Z</div><div>Authors: Mingquan Lin, Tianhao Li, Zhaoyi Sun, Gregory Holste, Ying Ding, Fei Wang, George Shih, Yifan Peng</div><div style='padding-top: 10px; width: 80ex'>Purpose: Limited studies exploring concrete methods or approaches to tackle
and enhance model fairness in the radiology domain. Our proposed AI model
utilizes supervised contrastive learning to minimize bias in CXR diagnosis.
  Materials and Methods: In this retrospective study, we evaluated our proposed
method on two datasets: the Medical Imaging and Data Resource Center (MIDRC)
dataset with 77,887 CXR images from 27,796 patients collected as of April 20,
2023 for COVID-19 diagnosis, and the NIH Chest X-ray (NIH-CXR) dataset with
112,120 CXR images from 30,805 patients collected between 1992 and 2015. In the
NIH-CXR dataset, thoracic abnormalities include atelectasis, cardiomegaly,
effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation,
edema, emphysema, fibrosis, pleural thickening, or hernia. Our proposed method
utilizes supervised contrastive learning with carefully selected positive and
negative samples to generate fair image embeddings, which are fine-tuned for
subsequent tasks to reduce bias in chest X-ray (CXR) diagnosis. We evaluated
the methods using the marginal AUC difference ($\delta$ mAUC).
  Results: The proposed model showed a significant decrease in bias across all
subgroups when compared to the baseline models, as evidenced by a paired T-test
(p&lt;0.0001). The $\delta$ mAUC obtained by our method were 0.0116 (95\% CI,
0.0110-0.0123), 0.2102 (95% CI, 0.2087-0.2118), and 0.1000 (95\% CI,
0.0988-0.1011) for sex, race, and age on MIDRC, and 0.0090 (95\% CI,
0.0082-0.0097) for sex and 0.0512 (95% CI, 0.0512-0.0532) for age on NIH-CXR,
respectively.
  Conclusion: Employing supervised contrastive learning can mitigate bias in
CXR diagnosis, addressing concerns of fairness and reliability in deep
learning-based diagnostic methods.</div><div><a href='http://arxiv.org/abs/2401.15111v1'>2401.15111v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17246v1")'>SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion
  Classification Using 3D Multi-Phase Imaging</div>
<div id='2402.17246v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T06:32:56Z</div><div>Authors: Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang, Yizhou Yu</div><div style='padding-top: 10px; width: 80ex'>Automated classification of liver lesions in multi-phase CT and MR scans is
of clinical significance but challenging. This study proposes a novel Siamese
Dual-Resolution Transformer (SDR-Former) framework, specifically designed for
liver lesion classification in 3D multi-phase CT and MR imaging with varying
phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural
Network (SNN) to process multi-phase imaging inputs, possessing robust feature
representations while maintaining computational efficiency. The weight-sharing
feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer
(DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored
3D Transformer for processing high- and low-resolution images, respectively.
This hybrid sub-architecture excels in capturing detailed local features and
understanding global contextual information, thereby, boosting the SNN's
feature extraction capabilities. Additionally, a novel Adaptive Phase Selection
Module (APSM) is introduced, promoting phase-specific intercommunication and
dynamically adjusting each phase's influence on the diagnostic outcome. The
proposed SDR-Former framework has been validated through comprehensive
experiments on two clinical datasets: a three-phase CT dataset and an
eight-phase MR dataset. The experimental results affirm the efficacy of the
proposed framework. To support the scientific community, we are releasing our
extensive multi-phase MR dataset for liver lesion analysis to the public. This
pioneering dataset, being the first publicly available multi-phase MR dataset
in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is
accessible at:https://bit.ly/3IyYlgN.</div><div><a href='http://arxiv.org/abs/2402.17246v1'>2402.17246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08910v1")'>Learning-based Bone Quality Classification Method for Spinal Metastasis</div>
<div id='2402.08910v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T02:53:51Z</div><div>Authors: Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao</div><div style='padding-top: 10px; width: 80ex'>Spinal metastasis is the most common disease in bone metastasis and may cause
pain, instability and neurological injuries. Early detection of spinal
metastasis is critical for accurate staging and optimal treatment. The
diagnosis is usually facilitated with Computed Tomography (CT) scans, which
requires considerable efforts from well-trained radiologists. In this paper, we
explore a learning-based automatic bone quality classification method for
spinal metastasis based on CT images. We simultaneously take the posterolateral
spine involvement classification task into account, and employ multi-task
learning (MTL) technique to improve the performance. MTL acts as a form of
inductive bias which helps the model generalize better on each task by sharing
representations between related tasks. Based on the prior knowledge that the
mixed type can be viewed as both blastic and lytic, we model the task of bone
quality classification as two binary classification sub-tasks, i.e., whether
blastic and whether lytic, and leverage a multiple layer perceptron to combine
their predictions. In order to make the model more robust and generalize
better, self-paced learning is adopted to gradually involve from easy to more
complex samples into the training process. The proposed learning-based method
is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our
method significantly outperforms an 121-layer DenseNet classifier in
sensitivities by $+12.54\%$, $+7.23\%$ and $+29.06\%$ for blastic, mixed and
lytic lesions, respectively, meanwhile $+12.33\%$, $+23.21\%$ and $+34.25\%$ at
vertebrae level.</div><div><a href='http://arxiv.org/abs/2402.08910v1'>2402.08910v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02106v2")'>Cadmium Zinc Telluride (CZT) photon counting detector Characterisation
  for soft tissue imaging</div>
<div id='2401.02106v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T07:28:56Z</div><div>Authors: K. Hameed, Rafidah Zainon, Mahbubunnabi Tamal</div><div style='padding-top: 10px; width: 80ex'>The use of photon counting detection technology has resulted in significant
X-ray imaging research interest in recent years. Computed Tomography (CT)
scanners can benefit from photon-counting detectors, which are new technology
with the potential to overcome key limitations of conventional CT detectors.
Researchers are still studying the effectiveness and sensitivity of
semiconductor detector materials in photon counting detectors for detecting
soft tissue contrasts. This study aimed to characterize the performance of the
Cadmium Zinc Telluride photon counting detector in identifying various tissues.
An optimal frame rate per second (FPS) of CZT detector was evaluated by setting
the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA
respectively by keeping the optimum FPS fixed, the detector energy thresholds
were set in small steps from 15 keV to 35 keV and the Currents were set for
X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between
voltage and current of the X-ray source and counts per second (CPS). The
samples i.e., fat, liver, muscles, paraffin wax, and contrast media were
stacked at six different thickness levels in a stair-step chamber made from
Plexi-glass. X-ray transmission at six different thicknesses of tissue samples
was also examined for five different energy (regions) thresholds (21 keV, 25
keV, 29 keV, 31 keV, and 45 keV) to determine the effect on count per second
(CPS). In this study, 12 frames per second is found to be the optimum frame
rate per second (FPS) based on the spectral response of an X-ray source and CPS
has a linear relationship with X-ray tube current as well. It was also noted
that A sample's thickness also affects its X-ray transmission at different
energy thresholds. A high sensitivity and linearity of the detectors make them
suitable for use in both preclinical and medical applications.</div><div><a href='http://arxiv.org/abs/2401.02106v2'>2401.02106v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03414v1")'>An end-to-end deep learning pipeline to derive blood input with partial
  volume corrections for automated parametric brain PET mapping</div>
<div id='2402.03414v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:02:30Z</div><div>Authors: Rugved Chavan, Gabriel Hyman, Zoraiz Qureshi, Nivetha Jayakumar, William Terrell, Stuart Berr, David Schiff, Megan Wardius, Nathan Fountain, Thomas Muttikkal, Mark Quigg, Miaomiao Zhang, Bijoy Kundu</div><div style='padding-top: 10px; width: 80ex'>Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography
(dFDG-PET) for human brain imaging has considerable clinical potential, yet its
utilization remains limited. A key challenge in the quantitative analysis of
dFDG-PET is characterizing a patient-specific blood input function,
traditionally reliant on invasive arterial blood sampling. This research
introduces a novel approach employing non-invasive deep learning model-based
computations from the internal carotid arteries (ICA) with partial volume (PV)
corrections, thereby eliminating the need for invasive arterial sampling. We
present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA
segmentation, alongside a Recurrent Neural Network (RNN) based MCIF-net for the
derivation of a model-corrected blood input function (MCIF) with PV
corrections. The developed 3D U-Net and RNN was trained and validated using a
5-fold cross-validation approach on 50 human brain FDG PET datasets. The
ICA-net achieved an average Dice score of 82.18% and an Intersection over Union
of 68.54% across all tested scans. Furthermore, the MCIF-net exhibited a
minimal root mean squared error of 0.0052. The application of this pipeline to
ground truth data for dFDG-PET brain scans resulted in the precise localization
of seizure onset regions, which contributed to a successful clinical outcome,
with the patient achieving a seizure-free state after treatment. These results
underscore the efficacy of the ICA-net and MCIF-net deep learning pipeline in
learning the ICA structure's distribution and automating MCIF computation with
PV corrections. This advancement marks a significant leap in non-invasive
neuroimaging.</div><div><a href='http://arxiv.org/abs/2402.03414v1'>2402.03414v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01303v1")'>Integrating Edges into U-Net Models with Explainable Activation Maps for
  Brain Tumor Segmentation using MR Images</div>
<div id='2401.01303v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T17:30:45Z</div><div>Authors: Subin Sahayam, Umarani Jayaraman</div><div style='padding-top: 10px; width: 80ex'>Manual delineation of tumor regions from magnetic resonance (MR) images is
time-consuming, requires an expert, and is prone to human error. In recent
years, deep learning models have been the go-to approach for the segmentation
of brain tumors. U-Net and its' variants for semantic segmentation of medical
images have achieved good results in the literature. However, U-Net and its'
variants tend to over-segment tumor regions and may not accurately segment the
tumor edges. The edges of the tumor are as important as the tumor regions for
accurate diagnosis, surgical precision, and treatment planning. In the proposed
work, the authors aim to extract edges from the ground truth using a
derivative-like filter followed by edge reconstruction to obtain an edge ground
truth in addition to the brain tumor ground truth. Utilizing both ground
truths, the author studies several U-Net and its' variant architectures with
and without tumor edges ground truth as a target along with the tumor ground
truth for brain tumor segmentation. The author used the BraTS2020 benchmark
dataset to perform the study and the results are tabulated for the dice and
Hausdorff95 metrics. The mean and median metrics are calculated for the whole
tumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the
baseline U-Net and its variants, the models that learned edges along with the
tumor regions performed well in core tumor regions in both training and
validation datasets. The improved performance of edge-trained models trained on
baseline models like U-Net and V-Net achieved performance similar to baseline
state-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target
trained models are capable of generating edge maps that can be useful for
treatment planning. Additionally, for further explainability of the results,
the activation map generated by the hybrid MR-U-Net has been studied.</div><div><a href='http://arxiv.org/abs/2401.01303v1'>2401.01303v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05340v1")'>Embedded Deployment of Semantic Segmentation in Medicine through
  Low-Resolution Inputs</div>
<div id='2403.05340v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T14:17:07Z</div><div>Authors: Erik Ostrowski, Muhammad Shafique</div><div style='padding-top: 10px; width: 80ex'>When deploying neural networks in real-life situations, the size and
computational effort are often the limiting factors. This is especially true in
environments where big, expensive hardware is not affordable, like in embedded
medical devices, where budgets are often tight. State-of-the-art proposed
multiple different lightweight solutions for such use cases, mostly by changing
the base model architecture, not taking the input and output resolution into
consideration. In this paper, we propose our architecture that takes advantage
of the fact that in hardware-limited environments, we often refrain from using
the highest available input resolutions to guarantee a higher throughput.
Although using lower-resolution input leads to a significant reduction in
computing and memory requirements, it may also incur reduced prediction
quality. Our architecture addresses this problem by exploiting the fact that we
can still utilize high-resolution ground-truths in training. The proposed model
inputs lower-resolution images and high-resolution ground truths, which can
improve the prediction quality by 5.5% while adding less than 200 parameters to
the model. %reducing the frames per second only from 25 to 20. We conduct an
extensive analysis to illustrate that our architecture enhances existing
state-of-the-art frameworks for lightweight semantic segmentation of cancer in
MRI images. We also tested the deployment speed of state-of-the-art lightweight
networks and our architecture on Nvidia's Jetson Nano to emulate deployment in
resource-constrained embedded scenarios.</div><div><a href='http://arxiv.org/abs/2403.05340v1'>2403.05340v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06066v1")'>CausalCellSegmenter: Causal Inference inspired Diversified Aggregation
  Convolution for Pathology Image Segmentation</div>
<div id='2403.06066v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T03:04:13Z</div><div>Authors: Dawei Fan, Yifan Gao, Jiaming Yu, Yanping Chen, Wencheng Li, Chuancong Lin, Kaibin Li, Changcai Yang, Riqing Chen, Lifang Wei</div><div style='padding-top: 10px; width: 80ex'>Deep learning models have shown promising performance for cell nucleus
segmentation in the field of pathology image analysis. However, training a
robust model from multiple domains remains a great challenge for cell nucleus
segmentation. Additionally, the shortcomings of background noise, highly
overlapping between cell nucleus, and blurred edges often lead to poor
performance. To address these challenges, we propose a novel framework termed
CausalCellSegmenter, which combines Causal Inference Module (CIM) with
Diversified Aggregation Convolution (DAC) techniques. The DAC module is
designed which incorporates diverse downsampling features through a simple,
parameter-free attention module (SimAM), aiming to overcome the problems of
false-positive identification and edge blurring. Furthermore, we introduce CIM
to leverage sample weighting by directly removing the spurious correlations
between features for every input sample and concentrating more on the
correlation between features and labels. Extensive experiments on the
MoNuSeg-2018 dataset achieves promising results, outperforming other
state-of-the-art methods, where the mIoU and DSC scores growing by 3.6% and
2.65%.</div><div><a href='http://arxiv.org/abs/2403.06066v1'>2403.06066v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09942v1")'>Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor
  Segmentation</div>
<div id='2403.09942v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T00:52:17Z</div><div>Authors: Ziya Ata Yazıcı, İlkay Öksüz, Hazım Kemal Ekenel</div><div style='padding-top: 10px; width: 80ex'>Glioblastoma is a highly aggressive and malignant brain tumor type that
requires early diagnosis and prompt intervention. Due to its heterogeneity in
appearance, developing automated detection approaches is challenging. To
address this challenge, Artificial Intelligence (AI)-driven approaches in
healthcare have generated interest in efficiently diagnosing and evaluating
brain tumors. The Brain Tumor Segmentation Challenge (BraTS) is a platform for
developing and assessing automated techniques for tumor analysis using
high-quality, clinically acquired MRI data. In our approach, we utilized a
multi-scale, attention-guided and hybrid U-Net-shaped model -- GLIMS -- to
perform 3D brain tumor segmentation in three regions: Enhancing Tumor (ET),
Tumor Core (TC), and Whole Tumor (WT). The multi-scale feature extraction
provides better contextual feature aggregation in high resolutions and the Swin
Transformer blocks improve the global feature extraction at deeper levels of
the model. The segmentation mask generation in the decoder branch is guided by
the attention-refined features gathered from the encoder branch to enhance the
important attributes. Moreover, hierarchical supervision is used to train the
model efficiently. Our model's performance on the validation set resulted in
92.19, 87.75, and 83.18 Dice Scores and 89.09, 84.67, and 82.15 Lesion-wise
Dice Scores in WT, TC, and ET, respectively. The code is publicly available at
https://github.com/yaziciz/GLIMS.</div><div><a href='http://arxiv.org/abs/2403.09942v1'>2403.09942v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15990v1")'>Gland segmentation via dual encoders and boundary-enhanced attention</div>
<div id='2401.15990v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T09:20:08Z</div><div>Authors: Huadeng Wang, Jiejiang Yu, Bingbing Li, Xipeng Pan, Zhenbing Liu, Rushi Lan, Xiaonan Luo</div><div style='padding-top: 10px; width: 80ex'>Accurate and automated gland segmentation on pathological images can assist
pathologists in diagnosing the malignancy of colorectal adenocarcinoma.
However, due to various gland shapes, severe deformation of malignant glands,
and overlapping adhesions between glands. Gland segmentation has always been
very challenging. To address these problems, we propose a DEA model. This model
consists of two branches: the backbone encoding and decoding network and the
local semantic extraction network. The backbone encoding and decoding network
extracts advanced Semantic features, uses the proposed feature decoder to
restore feature space information, and then enhances the boundary features of
the gland through boundary enhancement attention. The local semantic extraction
network uses the pre-trained DeepLabv3+ as a Local semantic-guided encoder to
realize the extraction of edge features. Experimental results on two public
datasets, GlaS and CRAG, confirm that the performance of our method is better
than other gland segmentation methods.</div><div><a href='http://arxiv.org/abs/2401.15990v1'>2401.15990v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06278v1")'>A Study on Self-Supervised Pretraining for Vision Problems in
  Gastrointestinal Endoscopy</div>
<div id='2401.06278v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T22:19:47Z</div><div>Authors: Edward Sanderson, Bogdan J. Matuszewski</div><div style='padding-top: 10px; width: 80ex'>Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally
use image encoders pretrained in a supervised manner with ImageNet-1k as
backbones. However, the use of modern self-supervised pretraining algorithms
and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may
allow for improvements. In this work, we study the fine-tuned performance of
models with ResNet50 and ViT-B backbones pretrained in self-supervised and
supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised
only) in a range of GIE vision tasks. In addition to identifying the most
suitable pretraining pipeline and backbone architecture for each task, out of
those considered, our results suggest: that self-supervised pretraining
generally produces more suitable backbones for GIE vision tasks than supervised
pretraining; that self-supervised pretraining with ImageNet-1k is typically
more suitable than pretraining with Hyperkvasir-unlabelled, with the notable
exception of monocular depth estimation in colonoscopy; and that ViT-Bs are
more suitable in polyp segmentation and monocular depth estimation in
colonoscopy, ResNet50s are more suitable in polyp detection, and both
architectures perform similarly in anatomical landmark recognition and
pathological finding characterisation. We hope this work draws attention to the
complexity of pretraining for GIE vision tasks, informs this development of
more suitable approaches than the convention, and inspires further research on
this topic to help advance this development. Code available:
\underline{github.com/ESandML/SSL4GIE}</div><div><a href='http://arxiv.org/abs/2401.06278v1'>2401.06278v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08409v1")'>Faster ISNet for Background Bias Mitigation on Deep Neural Networks</div>
<div id='2401.08409v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:49:26Z</div><div>Authors: Pedro R. A. S. Bassi, Sergio Decherchi, Andrea Cavalli</div><div style='padding-top: 10px; width: 80ex'>Image background features can constitute background bias (spurious
correlations) and impact deep classifiers decisions, causing shortcut learning
(Clever Hans effect) and reducing the generalization skill on real-world data.
The concept of optimizing Layer-wise Relevance Propagation (LRP) heatmaps, to
improve classifier behavior, was recently introduced by a neural network
architecture named ISNet. It minimizes background relevance in LRP maps, to
mitigate the influence of image background features on deep classifiers
decisions, hindering shortcut learning and improving generalization. For each
training image, the original ISNet produces one heatmap per possible class in
the classification task, hence, its training time scales linearly with the
number of classes. Here, we introduce reformulated architectures that allow the
training time to become independent from this number, rendering the
optimization process much faster. We challenged the enhanced models utilizing
the MNIST dataset with synthetic background bias, and COVID-19 detection in
chest X-rays, an application that is prone to shortcut learning due to
background bias. The trained models minimized background attention and hindered
shortcut learning, while retaining high accuracy. Considering external
(out-of-distribution) test datasets, they consistently proved more accurate
than multiple state-of-the-art deep neural network architectures, including a
dedicated image semantic segmenter followed by a classifier. The architectures
presented here represent a potentially massive improvement in training speed
over the original ISNet, thus introducing LRP optimization into a gamut of
applications that could not be feasibly handled by the original model.</div><div><a href='http://arxiv.org/abs/2401.08409v1'>2401.08409v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11985v1")'>Weakly Supervised Object Detection in Chest X-Rays with Differentiable
  ROI Proposal Networks and Soft ROI Pooling</div>
<div id='2402.11985v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:30:05Z</div><div>Authors: Philip Müller, Felix Meissen, Georgios Kaissis, Daniel Rueckert</div><div style='padding-top: 10px; width: 80ex'>Weakly supervised object detection (WSup-OD) increases the usefulness and
interpretability of image classification algorithms without requiring
additional supervision. The successes of multiple instance learning in this
task for natural images, however, do not translate well to medical images due
to the very different characteristics of their objects (i.e. pathologies). In
this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new
method for generating bounding box proposals on the fly using a specialized
region of interest-attention (ROI-attention) module. WSRPN integrates well with
classic backbone-head classification algorithms and is end-to-end trainable
with only image-label supervision. We experimentally demonstrate that our new
method outperforms existing methods in the challenging task of disease
localization in chest X-ray images. Code:
https://github.com/philip-mueller/wsrpn</div><div><a href='http://arxiv.org/abs/2402.11985v1'>2402.11985v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02192v1")'>Nodule detection and generation on chest X-rays: NODE21 Challenge</div>
<div id='2401.02192v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T10:54:05Z</div><div>Authors: Ecem Sogancioglu, Bram van Ginneken, Finn Behrendt, Marcel Bengs, Alexander Schlaefer, Miron Radu, Di Xu, Ke Sheng, Fabien Scalzo, Eric Marcus, Samuele Papa, Jonas Teuwen, Ernst Th. Scholten, Steven Schalekamp, Nils Hendrix, Colin Jacobs, Ward Hendrix, Clara I Sánchez, Keelin Murphy</div><div style='padding-top: 10px; width: 80ex'>Pulmonary nodules may be an early manifestation of lung cancer, the leading
cause of cancer-related deaths among both men and women. Numerous studies have
established that deep learning methods can yield high-performance levels in the
detection of lung nodules in chest X-rays. However, the lack of gold-standard
public datasets slows down the progression of the research and prevents
benchmarking of methods for this task. To address this, we organized a public
research challenge, NODE21, aimed at the detection and generation of lung
nodules in chest X-rays. While the detection track assesses state-of-the-art
nodule detection systems, the generation track determines the utility of nodule
generation algorithms to augment training data and hence improve the
performance of the detection systems. This paper summarizes the results of the
NODE21 challenge and performs extensive additional experiments to examine the
impact of the synthetically generated nodule training images on the detection
algorithm performance.</div><div><a href='http://arxiv.org/abs/2401.02192v1'>2401.02192v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.11585v3")'>PolypNextLSTM: A lightweight and fast polyp video segmentation network
  using ConvNext and ConvLSTM</div>
<div id='2402.11585v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T13:24:48Z</div><div>Authors: Debayan Bhattacharya, Konrad Reuter, Finn Behrendt, Lennart Maack, Sarah Grube, Alexander Schlaefer</div><div style='padding-top: 10px; width: 80ex'>Commonly employed in polyp segmentation, single image UNet architectures lack
the temporal insight clinicians gain from video data in diagnosing polyps. To
mirror clinical practices more faithfully, our proposed solution,
PolypNextLSTM, leverages video-based deep learning, harnessing temporal
information for superior segmentation performance with the least parameter
overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a
UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting
the last two layers to reduce parameter overhead. Our temporal fusion module, a
Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal
features. Our primary novelty lies in PolypNextLSTM, which stands out as the
leanest in parameters and the fastest model, surpassing the performance of five
state-of-the-art image and video-based deep learning models. The evaluation of
the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios,
along with videos containing challenging artefacts like fast motion and
occlusion. Comparison against 5 image-based and 5 video-based models
demonstrates PolypNextLSTM's superiority, achieving a Dice score of 0.7898 on
the hard-to-detect polyp test set, surpassing image-based PraNet (0.7519) and
video-based PNSPlusNet (0.7486). Notably, our model excels in videos featuring
complex artefacts such as ghosting and occlusion. PolypNextLSTM, integrating
pruned ConvNext-Tiny with ConvLSTM for temporal fusion, not only exhibits
superior segmentation performance but also maintains the highest frames per
speed among evaluated models. Access code here
https://github.com/mtec-tuhh/PolypNextLSTM</div><div><a href='http://arxiv.org/abs/2402.11585v3'>2402.11585v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11174v2")'>Pixel-Wise Recognition for Holistic Surgical Scene Understanding</div>
<div id='2401.11174v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T09:09:52Z</div><div>Authors: Nicolás Ayobi, Santiago Rodríguez, Alejandra Pérez, Isabela Hernández, Nicolás Aparicio, Eugénie Dessevres, Sebastián Peña, Jessica Santander, Juan Ignacio Caicedo, Nicolás Fernández, Pablo Arbeláez</div><div style='padding-top: 10px; width: 80ex'>This paper presents the Holistic and Multi-Granular Surgical Scene
Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that
models surgical scene understanding as a hierarchy of complementary tasks with
varying levels of granularity. Our approach enables a multi-level comprehension
of surgical activities, encompassing long-term tasks such as surgical phases
and steps recognition and short-term tasks including surgical instrument
segmentation and atomic visual actions detection. To exploit our proposed
benchmark, we introduce the Transformers for Actions, Phases, Steps, and
Instrument Segmentation (TAPIS) model, a general architecture that combines a
global video feature extractor with localized region proposals from an
instrument segmentation model to tackle the multi-granularity of our benchmark.
Through extensive experimentation, we demonstrate the impact of including
segmentation annotations in short-term recognition tasks, highlight the varying
granularity requirements of each task, and establish TAPIS's superiority over
previously proposed baselines and conventional CNN-based models. Additionally,
we validate the robustness of our method across multiple public benchmarks,
confirming the reliability and applicability of our dataset. This work
represents a significant step forward in Endoscopic Vision, offering a novel
and comprehensive framework for future research towards a holistic
understanding of surgical procedures.</div><div><a href='http://arxiv.org/abs/2401.11174v2'>2401.11174v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05949v2")'>General surgery vision transformer: A video pre-trained foundation model
  for general surgery</div>
<div id='2403.05949v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T16:02:46Z</div><div>Authors: Samuel Schmidgall, Ji Woong Kim, Jeffery Jopling, Axel Krieger</div><div style='padding-top: 10px; width: 80ex'>The absence of openly accessible data and specialized foundation models is a
major barrier for computational research in surgery. Toward this, (i) we
open-source the largest dataset of general surgery videos to-date, consisting
of 680 hours of surgical videos, including data from robotic and laparoscopic
techniques across 28 procedures; (ii) we propose a technique for video
pre-training a general surgery vision transformer (GSViT) on surgical videos
based on forward video prediction that can run in real-time for surgical
applications, toward which we open-source the code and weights of GSViT; (iii)
we also release code and weights for procedure-specific fine-tuned versions of
GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the
Cholec80 phase annotation task, displaying improved performance over
state-of-the-art single frame predictors.</div><div><a href='http://arxiv.org/abs/2403.05949v2'>2403.05949v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02791v2")'>Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery
  Videos</div>
<div id='2401.02791v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T13:05:02Z</div><div>Authors: Ryo Fujii, Ryo Hachiuma, Hideo Saito</div><div style='padding-top: 10px; width: 80ex'>Surgical tool detection is essential for analyzing and evaluating minimally
invasive surgery videos. Current approaches are mostly based on supervised
methods that require large, fully instance-level labels (i.e., bounding boxes).
However, large image datasets with instance-level labels are often limited
because of the burden of annotation. Thus, surgical tool detection is important
when providing image-level labels instead of instance-level labels since
image-level annotations are considerably more time-efficient than
instance-level annotations. In this work, we propose to strike a balance
between the extremely costly annotation burden and detection performance. We
further propose a co-occurrence loss, which considers a characteristic that
some tool pairs often co-occur together in an image to leverage image-level
labels. Encapsulating the knowledge of co-occurrence using the co-occurrence
loss helps to overcome the difficulty in classification that originates from
the fact that some tools have similar shapes and textures. Extensive
experiments conducted on the Endovis2018 dataset in various data settings show
the effectiveness of our method.</div><div><a href='http://arxiv.org/abs/2401.02791v2'>2401.02791v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02029v1")'>ScribFormer: Transformer Makes CNN Work Better for Scribble-based
  Medical Image Segmentation</div>
<div id='2402.02029v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T04:55:22Z</div><div>Authors: Zihan Li, Yuan Zheng, Dandan Shan, Shuzhou Yang, Qingde Li, Beizhan Wang, Yuanting Zhang, Qingqi Hong, Dinggang Shen</div><div style='padding-top: 10px; width: 80ex'>Most recent scribble-supervised segmentation methods commonly adopt a CNN
framework with an encoder-decoder architecture. Despite its multiple benefits,
this framework generally can only capture small-range feature dependency for
the convolutional layer with the local receptive field, which makes it
difficult to learn global shape information from the limited information
provided by scribble annotations. To address this issue, this paper proposes a
new CNN-Transformer hybrid solution for scribble-supervised medical image
segmentation called ScribFormer. The proposed ScribFormer model has a
triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer
branch, and an attention-guided class activation map (ACAM) branch.
Specifically, the CNN branch collaborates with the Transformer branch to fuse
the local features learned from CNN with the global representations obtained
from Transformer, which can effectively overcome limitations of existing
scribble-supervised segmentation methods. Furthermore, the ACAM branch assists
in unifying the shallow convolution features and the deep convolution features
to improve model's performance further. Extensive experiments on two public
datasets and one private dataset show that our ScribFormer has superior
performance over the state-of-the-art scribble-supervised segmentation methods,
and achieves even better results than the fully-supervised segmentation
methods. The code is released at https://github.com/HUANGLIZI/ScribFormer.</div><div><a href='http://arxiv.org/abs/2402.02029v1'>2402.02029v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07951v1")'>SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic
  Microscopy Segmentation</div>
<div id='2403.07951v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T02:28:29Z</div><div>Authors: Yiran Wang, Li Xiao</div><div style='padding-top: 10px; width: 80ex'>It has been shown that traditional deep learning methods for electronic
microscopy segmentation usually suffer from low transferability when samples
and annotations are limited, while large-scale vision foundation models are
more robust when transferring between different domains but facing sub-optimal
improvement under fine-tuning. In this work, we present a new few-shot domain
adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with
nnUNet in the embedding space to achieve high transferability and accuracy.
Specifically, we choose the Unet-based network as the "expert" component to
learn segmentation features efficiently and design a SAM-based adaptation
module as the "generic" component for domain transfer. By amalgamating the
"generic" and "expert" components, we mitigate the modality imbalance in the
complex pre-training knowledge inherent to large-scale Vision Foundation models
and the challenge of transferability inherent to traditional neural networks.
The effectiveness of our model is evaluated on two electron microscopic image
datasets with different modalities for mitochondria segmentation, which
improves the dice coefficient on the target domain by 6.7%. Also, the SAM-based
adaptor performs significantly better with only a single annotated image than
the 10-shot domain adaptation on nnUNet. We further verify our model on four
MRI datasets from different sources to prove its generalization ability.</div><div><a href='http://arxiv.org/abs/2403.07951v1'>2403.07951v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10516v1")'>FeatUp: A Model-Agnostic Framework for Features at Any Resolution</div>
<div id='2403.10516v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:57:06Z</div><div>Authors: Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, William T. Freeman</div><div style='padding-top: 10px; width: 80ex'>Deep features are a cornerstone of computer vision research, capturing image
semantics and enabling the community to solve downstream tasks even in the
zero- or few-shot regime. However, these features often lack the spatial
resolution to directly perform dense prediction tasks like segmentation and
depth prediction because models aggressively pool information over large areas.
In this work, we introduce FeatUp, a task- and model-agnostic framework to
restore lost spatial information in deep features. We introduce two variants of
FeatUp: one that guides features with high-resolution signal in a single
forward pass, and one that fits an implicit model to a single image to
reconstruct features at any resolution. Both approaches use a multi-view
consistency loss with deep analogies to NeRFs. Our features retain their
original semantics and can be swapped into existing applications to yield
resolution and performance gains even without re-training. We show that FeatUp
significantly outperforms other feature upsampling and image super-resolution
approaches in class activation map generation, transfer learning for
segmentation and depth prediction, and end-to-end training for semantic
segmentation.</div><div><a href='http://arxiv.org/abs/2403.10516v1'>2403.10516v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10997v1")'>N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields</div>
<div id='2403.10997v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T18:50:44Z</div><div>Authors: Yash Bhalgat, Iro Laina, João F. Henriques, Andrew Zisserman, Andrea Vedaldi</div><div style='padding-top: 10px; width: 80ex'>Understanding complex scenes at multiple levels of abstraction remains a
formidable challenge in computer vision. To address this, we introduce Nested
Neural Feature Fields (N2F2), a novel approach that employs hierarchical
supervision to learn a single feature field, wherein different dimensions
within the same high-dimensional feature encode scene properties at varying
granularities. Our method allows for a flexible definition of hierarchies,
tailored to either the physical dimensions or semantics or both, thereby
enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D
class-agnostic segmentation model to provide semantically meaningful pixel
groupings at arbitrary scales in the image space, and query the CLIP
vision-encoder to obtain language-aligned embeddings for each of these
segments. Our proposed hierarchical supervision method then assigns different
nested dimensions of the feature field to distill the CLIP embeddings using
deferred volumetric rendering at varying physical scales, creating a
coarse-to-fine representation. Extensive experiments show that our approach
outperforms the state-of-the-art feature field distillation methods on tasks
such as open-vocabulary 3D segmentation and localization, demonstrating the
effectiveness of the learned nested feature field.</div><div><a href='http://arxiv.org/abs/2403.10997v1'>2403.10997v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11678v1")'>Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous
  Scenes</div>
<div id='2403.11678v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T11:29:43Z</div><div>Authors: Antoine Schnepf, Karim Kassab, Jean-Yves Franceschi, Laurent Caraffa, Flavian Vasile, Jeremie Mary, Andrew Comport, Valérie Gouet-Brunet</div><div style='padding-top: 10px; width: 80ex'>We present a method enabling the scaling of NeRFs to learn a large number of
semantically-similar scenes. We combine two techniques to improve the required
training time and memory cost per scene. First, we learn a 3D-aware latent
space in which we train Tri-Plane scene representations, hence reducing the
resolution at which scenes are learned. Moreover, we present a way to share
common information across scenes, hence allowing for a reduction of model
complexity to learn a particular scene. Our method reduces effective per-scene
memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.
Our project page can be found at https://3da-ae.github.io .</div><div><a href='http://arxiv.org/abs/2403.11678v1'>2403.11678v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01203v1")'>Structured World Modeling via Semantic Vector Quantization</div>
<div id='2402.01203v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T08:13:18Z</div><div>Authors: Yi-Fu Wu, Minseung Lee, Sungjin Ahn</div><div style='padding-top: 10px; width: 80ex'>Neural discrete representations are crucial components of modern neural
networks. However, their main limitation is that the primary strategies such as
VQ-VAE can only provide representations at the patch level. Therefore, one of
the main goals of representation learning, acquiring structured, semantic, and
compositional abstractions such as the color and shape of an object, remains
elusive. In this paper, we present the first approach to semantic neural
discrete representation learning. The proposed model, called Semantic
Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in
unsupervised object-centric learning to address this limitation. Specifically,
we observe that a simple approach quantizing at the object level poses a
significant challenge and propose constructing scene representations
hierarchically, from low-level discrete concept schemas to object
representations. Additionally, we suggest a novel method for structured
semantic world modeling by training a prior over these representations,
enabling the ability to generate images by sampling the semantic properties of
the objects in the scene. In experiments on various 2D and 3D object-centric
datasets, we find that our model achieves superior generation performance
compared to non-semantic vector quantization methods such as VQ-VAE and
previous object-centric generative models. Furthermore, we find that the
semantic discrete representations can solve downstream scene understanding
tasks that require reasoning about the properties of different objects in the
scene.</div><div><a href='http://arxiv.org/abs/2402.01203v1'>2402.01203v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05049v1")'>Content-Aware Depth-Adaptive Image Restoration</div>
<div id='2401.05049v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T10:11:16Z</div><div>Authors: Tom Richard Vargis, Siavash Ghiasvand</div><div style='padding-top: 10px; width: 80ex'>This work prioritizes building a modular pipeline that utilizes existing
models to systematically restore images, rather than creating new restoration
models from scratch. Restoration is carried out at an object-specific level,
with each object regenerated using its corresponding class label information.
The approach stands out by providing complete user control over the entire
restoration process. Users can select models for specialized restoration steps,
customize the sequence of steps to meet their needs, and refine the resulting
regenerated image with depth awareness. The research provides two distinct
pathways for implementing image regeneration, allowing for a comparison of
their respective strengths and limitations. The most compelling aspect of this
versatile system is its adaptability. This adaptability enables users to target
particular object categories, including medical images, by providing models
that are trained on those object classes.</div><div><a href='http://arxiv.org/abs/2401.05049v1'>2401.05049v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03312v2")'>Test-Time Adaptation for Depth Completion</div>
<div id='2402.03312v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:59:52Z</div><div>Authors: Hyoungseob Park, Anjali Gupta, Alex Wong</div><div style='padding-top: 10px; width: 80ex'>It is common to observe performance degradation when transferring models
trained on some (source) datasets to target testing data due to a domain gap
between them. Existing methods for bridging this gap, such as domain adaptation
(DA), may require the source data on which the model was trained (often not
available), while others, i.e., source-free DA, require many passes through the
testing data. We propose an online test-time adaptation method for depth
completion, the task of inferring a dense depth map from a single image and
associated sparse depth map, that closes the performance gap in a single pass.
We first present a study on how the domain shift in each data modality affects
model performance. Based on our observations that the sparse depth modality
exhibits a much smaller covariate shift than the image, we design an embedding
module trained in the source domain that preserves a mapping from features
encoding only sparse depth to those encoding image and sparse depth. During
test time, sparse depth features are projected using this map as a proxy for
source domain features and are used as guidance to train a set of auxiliary
parameters (i.e., adaptation layer) to align image and sparse depth features
from the target test domain to that of the source domain. We evaluate our
method on indoor and outdoor scenarios and show that it improves over baselines
by an average of 21.1%.</div><div><a href='http://arxiv.org/abs/2402.03312v2'>2402.03312v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11735v2")'>LSKNet: A Foundation Lightweight Backbone for Remote Sensing</div>
<div id='2403.11735v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T12:43:38Z</div><div>Authors: Yuxuan Li, Xiang Li, Yimain Dai, Qibin Hou, Li Liu, Yongxiang Liu, Ming-Ming Cheng, Jian Yang</div><div style='padding-top: 10px; width: 80ex'>Remote sensing images pose distinct challenges for downstream tasks due to
their inherent complexity. While a considerable amount of research has been
dedicated to remote sensing classification, object detection and semantic
segmentation, most of these studies have overlooked the valuable prior
knowledge embedded within remote sensing scenarios. Such prior knowledge can be
useful because remote sensing objects may be mistakenly recognized without
referencing a sufficiently long-range context, which can vary for different
objects. This paper considers these priors and proposes a lightweight Large
Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its
large spatial receptive field to better model the ranging context of various
objects in remote sensing scenarios. To our knowledge, large and selective
kernel mechanisms have not been previously explored in remote sensing images.
Without bells and whistles, our lightweight LSKNet sets new state-of-the-art
scores on standard remote sensing classification, object detection and semantic
segmentation benchmarks. Our comprehensive analysis further validated the
significance of the identified priors and the effectiveness of LSKNet. The code
is available at https://github.com/zcablii/LSKNet.</div><div><a href='http://arxiv.org/abs/2403.11735v2'>2403.11735v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04385v1")'>Impacts of Color and Texture Distortions on Earth Observation Data in
  Deep Learning</div>
<div id='2403.04385v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T10:25:23Z</div><div>Authors: Martin Willbo, Aleksis Pirinen, John Martinsson, Edvin Listo Zec, Olof Mogren, Mikael Nilsson</div><div style='padding-top: 10px; width: 80ex'>Land cover classification and change detection are two important applications
of remote sensing and Earth observation (EO) that have benefited greatly from
the advances of deep learning. Convolutional and transformer-based U-net models
are the state-of-the-art architectures for these tasks, and their performances
have been boosted by an increased availability of large-scale annotated EO
datasets. However, the influence of different visual characteristics of the
input EO data on a model's predictions is not well understood. In this work we
systematically examine model sensitivities with respect to several color- and
texture-based distortions on the input EO data during inference, given models
that have been trained without such distortions. We conduct experiments with
multiple state-of-the-art segmentation networks for land cover classification
and show that they are in general more sensitive to texture than to color
distortions. Beyond revealing intriguing characteristics of widely used land
cover classification models, our results can also be used to guide the
development of more robust models within the EO domain.</div><div><a href='http://arxiv.org/abs/2403.04385v1'>2403.04385v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13918v3")'>BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for
  Cloud Detection and Segmentation in Remote Sensing Imagery</div>
<div id='2402.13918v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:32:43Z</div><div>Authors: Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane</div><div style='padding-top: 10px; width: 80ex'>Satellites equipped with optical sensors capture high-resolution imagery,
providing valuable insights into various environmental phenomena. In recent
years, there has been a surge of research focused on addressing some challenges
in remote sensing, ranging from water detection in diverse landscapes to the
segmentation of mountainous and terrains. Ongoing investigations goals to
enhance the precision and efficiency of satellite imagery analysis. Especially,
there is a growing emphasis on developing methodologies for accurate water body
detection, snow and clouds, important for environmental monitoring, resource
management, and disaster response. Within this context, this paper focus on the
cloud segmentation from remote sensing imagery. Accurate remote sensing data
analysis can be challenging due to the presence of clouds in optical
sensor-based applications. The quality of resulting products such as
applications and research is directly impacted by cloud detection, which plays
a key role in the remote sensing data processing pipeline. This paper examines
seven cutting-edge semantic segmentation and detection algorithms applied to
clouds identification, conducting a benchmark analysis to evaluate their
architectural approaches and identify the most performing ones. To increase the
model's adaptability, critical elements including the type of imagery and the
amount of spectral bands used during training are analyzed. Additionally, this
research tries to produce machine learning algorithms that can perform cloud
segmentation using only a few spectral bands, including RGB and RGBN-IR
combinations. The model's flexibility for a variety of applications and user
scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as
datasets. This benchmark can be reproduced using the material from this github
link: https://github.com/toelt-llc/cloud_segmentation_comparative.</div><div><a href='http://arxiv.org/abs/2402.13918v3'>2402.13918v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15105v1")'>Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote
  Sensing Imagery</div>
<div id='2401.15105v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T13:14:17Z</div><div>Authors: Jialu Sui, Yiyang Ma, Wenhan Yang, Xiaokang Zhang, Man-On Pun, Jiaying Liu</div><div style='padding-top: 10px; width: 80ex'>The presence of cloud layers severely compromises the quality and
effectiveness of optical remote sensing (RS) images. However, existing
deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties
in accurately reconstructing the original visual authenticity and detailed
semantic content of the images. To tackle this challenge, this work proposes to
encompass enhancements at the data and methodology fronts. On the data side, an
ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial
resolution is established. This benchmark incorporates rich detailed textures
and diverse cloud coverage, serving as a robust foundation for designing and
assessing CR models. From the methodology perspective, a novel diffusion-based
framework for CR called Diffusion Enhancement (DE) is proposed to perform
progressive texture detail recovery, which mitigates the training difficulty
with improved inference accuracy. Additionally, a Weight Allocation (WA)
network is developed to dynamically adjust the weights for feature fusion,
thereby further improving performance, particularly in the context of
ultra-resolution image generation. Furthermore, a coarse-to-fine training
strategy is applied to effectively expedite training convergence while reducing
the computational complexity required to handle ultra-resolution images.
Extensive experiments on the newly established CUHK-CR and existing datasets
such as RICE confirm that the proposed DE framework outperforms existing
DL-based methods in terms of both perceptual quality and signal fidelity.</div><div><a href='http://arxiv.org/abs/2401.15105v1'>2401.15105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16520v1")'>MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and
  Attention-based Regression for Cloud Property Retrieval</div>
<div id='2401.16520v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T19:50:50Z</div><div>Authors: Xingyan Li, Andrew M. Sayer, Ian T. Carroll, Xin Huang, Jianwu Wang</div><div style='padding-top: 10px; width: 80ex'>In the realm of Earth science, effective cloud property retrieval,
encompassing cloud masking, cloud phase classification, and cloud optical
thickness (COT) prediction, remains pivotal. Traditional methodologies
necessitate distinct models for each sensor instrument due to their unique
spectral characteristics. Recent strides in Earth Science research have
embraced machine learning and deep learning techniques to extract features from
satellite datasets' spectral observations. However, prevailing approaches lack
novel architectures accounting for hierarchical relationships among retrieval
tasks. Moreover, considering the spectral diversity among existing sensors, the
development of models with robust generalization capabilities over different
sensor datasets is imperative. Surprisingly, there is a dearth of methodologies
addressing the selection of an optimal model for diverse datasets. In response,
this paper introduces MT-HCCAR, an end-to-end deep learning model employing
multi-task learning to simultaneously tackle cloud masking, cloud phase
retrieval (classification tasks), and COT prediction (a regression task). The
MT-HCCAR integrates a hierarchical classification network (HC) and a
classification-assisted attention-based regression network (CAR), enhancing
precision and robustness in cloud labeling and COT prediction. Additionally, a
comprehensive model selection method rooted in K-fold cross-validation, one
standard error rule, and two introduced performance scores is proposed to
select the optimal model over three simulated satellite datasets OCI, VIIRS,
and ABI. The experiments comparing MT-HCCAR with baseline methods, the ablation
studies, and the model selection affirm the superiority and the generalization
capabilities of MT-HCCAR.</div><div><a href='http://arxiv.org/abs/2401.16520v1'>2401.16520v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14486v1")'>CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of
  Clouds</div>
<div id='2401.14486v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T19:44:19Z</div><div>Authors: Muhammad Ahmed Chaudhry, Lyna Kim, Jeremy Irvin, Yuzu Ido, Sonia Chu, Jared Thomas Isobe, Andrew Y. Ng, Duncan Watson-Parris</div><div style='padding-top: 10px; width: 80ex'>Clouds play a significant role in global temperature regulation through their
effect on planetary albedo. Anthropogenic emissions of aerosols can alter the
albedo of clouds, but the extent of this effect, and its consequent impact on
temperature change, remains uncertain. Human-induced clouds caused by ship
aerosol emissions, commonly referred to as ship tracks, provide visible
manifestations of this effect distinct from adjacent cloud regions and
therefore serve as a useful sandbox to study human-induced clouds. However, the
lack of large-scale ship track data makes it difficult to deduce their general
effects on cloud formation. Towards developing automated approaches to localize
ship tracks at scale, we present CloudTracks, a dataset containing 3,560
satellite images labeled with more than 12,000 ship track instance annotations.
We train semantic segmentation and instance segmentation model baselines on our
dataset and find that our best model substantially outperforms previous
state-of-the-art for ship track localization (61.29 vs. 48.65 IoU). We also
find that the best instance segmentation model is able to identify the number
of ship tracks in each image more accurately than the previous state-of-the-art
(1.64 vs. 4.99 MAE). However, we identify cases where the best model struggles
to accurately localize and count ship tracks, so we believe CloudTracks will
stimulate novel machine learning approaches to better detect elongated and
overlapping features in satellite images. We release our dataset openly at
{zenodo.org/records/10042922}.</div><div><a href='http://arxiv.org/abs/2401.14486v1'>2401.14486v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15113v1")'>Towards Global Glacier Mapping with Deep Learning and Open Earth
  Observation Data</div>
<div id='2401.15113v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:41:17Z</div><div>Authors: Konstantin A. Maslov, Claudio Persello, Thomas Schellenberger, Alfred Stein</div><div style='padding-top: 10px; width: 80ex'>Accurate global glacier mapping is critical for understanding climate change
impacts. It is challenged by glacier diversity, difficult-to-classify debris
and big data processing. Here we propose Glacier-VisionTransformer-U-Net
(GlaViTU), a convolutional-transformer deep learning model, and five strategies
for multitemporal global-scale glacier mapping using open satellite imagery.
Assessing the spatial, temporal and cross-sensor generalisation shows that our
best strategy achieves intersection over union &gt;0.85 on previously unobserved
images in most cases, which drops to &gt;0.75 for debris-rich areas such as
High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice.
Additionally, adding synthetic aperture radar data, namely, backscatter and
interferometric coherence, increases the accuracy in all regions where
available. The calibrated confidence for glacier extents is reported making the
predictions more reliable and interpretable. We also release a benchmark
dataset that covers 9% of glaciers worldwide. Our results support efforts
towards automated multitemporal and global glacier mapping.</div><div><a href='http://arxiv.org/abs/2401.15113v1'>2401.15113v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12080v1")'>Evaluating Terrain-Dependent Performance for Martian Frost Detection in
  Visible Satellite Observations</div>
<div id='2403.12080v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T04:13:46Z</div><div>Authors: Gary Doran, Serina Diniega, Steven Lu, Mark Wronkiewicz, Kiri L. Wagstaff</div><div style='padding-top: 10px; width: 80ex'>Seasonal frosting and defrosting on the surface of Mars is hypothesized to
drive both climate processes and the formation and evolution of
geomorphological features such as gullies. Past studies have focused on
manually analyzing the behavior of the frost cycle in the northern mid-latitude
region of Mars using high-resolution visible observations from orbit. Extending
these studies globally requires automating the detection of frost using data
science techniques such as convolutional neural networks. However, visible
indications of frost presence can vary significantly depending on the geologic
context on which the frost is superimposed. In this study, we (1) present a
novel approach for spatially partitioning data to reduce biases in model
performance estimation, (2) illustrate how geologic context affects automated
frost detection, and (3) propose mitigations to observed biases in automated
frost detection.</div><div><a href='http://arxiv.org/abs/2403.12080v1'>2403.12080v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06860v2")'>A Geospatial Approach to Predicting Desert Locust Breeding Grounds in
  Africa</div>
<div id='2403.06860v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:13:58Z</div><div>Authors: Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo, Arnu Pretorius</div><div style='padding-top: 10px; width: 80ex'>Desert locust swarms present a major threat to agriculture and food security.
Addressing this challenge, our study develops an operationally-ready model for
predicting locust breeding grounds, which has the potential to enhance early
warning systems and targeted control measures. We curated a dataset from the
United Nations Food and Agriculture Organization's (UN-FAO) locust observation
records and analyzed it using two types of spatio-temporal input features:
remotely-sensed environmental and climate data as well as multi-spectral earth
observation images. Our approach employed custom deep learning models
(three-dimensional and LSTM-based recurrent convolutional networks), along with
the geospatial foundational model Prithvi recently released by Jakubik et al.,
2023. These models notably outperformed existing baselines, with the
Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized
Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and
ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding
from our research is that multi-spectral earth observation images alone are
sufficient for effective locust breeding ground prediction without the need to
explicitly incorporate climatic or environmental features.</div><div><a href='http://arxiv.org/abs/2403.06860v2'>2403.06860v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06009v1")'>Sea ice detection using concurrent multispectral and synthetic aperture
  radar imagery</div>
<div id='2401.06009v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T16:14:30Z</div><div>Authors: Martin S J Rogers, Maria Fox, Andrew Fleming, Louisa van Zeeland, Jeremy Wilkinson, J. Scott Hosking</div><div style='padding-top: 10px; width: 80ex'>Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea
ice mapping due to its spatio-temporal coverage and the ability to detect sea
ice independent of cloud and lighting conditions. Automatic sea ice detection
using SAR imagery remains problematic due to the presence of ambiguous signal
and noise within the image. Conversely, ice and water are easily
distinguishable using multispectral imagery (MSI), but in the polar regions the
ocean's surface is often occluded by cloud or the sun may not appear above the
horizon for many months. To address some of these limitations, this paper
proposes a new tool trained using concurrent multispectral Visible and SAR
imagery for sea Ice Detection (ViSual\_IceD). ViSual\_IceD is a convolution
neural network (CNN) that builds on the classic U-Net architecture by
containing two parallel encoder stages, enabling the fusion and concatenation
of MSI and SAR imagery containing different spatial resolutions. The
performance of ViSual\_IceD is compared with U-Net models trained using
concatenated MSI and SAR imagery as well as models trained exclusively on MSI
or SAR imagery. ViSual\_IceD outperforms the other networks, with a F1 score
1.60\% points higher than the next best network, and results indicate that
ViSual\_IceD is selective in the image type it uses during image segmentation.
Outputs from ViSual\_IceD are compared to sea ice concentration products
derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how
ViSual\_IceD is a useful tool to use in conjunction with PMW data, particularly
in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery
continues to increase, ViSual\_IceD provides a new opportunity for robust,
accurate sea ice coverage detection in polar regions.</div><div><a href='http://arxiv.org/abs/2401.06009v1'>2401.06009v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07503v1")'>PolMERLIN: Self-Supervised Polarimetric Complex SAR Image Despeckling
  with Masked Networks</div>
<div id='2401.07503v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T07:06:36Z</div><div>Authors: Shunya Kato, Masaki Saito, Katsuhiko Ishiguro, Sol Cummings</div><div style='padding-top: 10px; width: 80ex'>Despeckling is a crucial noise reduction task in improving the quality of
synthetic aperture radar (SAR) images. Directly obtaining noise-free SAR images
is a challenging task that has hindered the development of accurate despeckling
algorithms. The advent of deep learning has facilitated the study of denoising
models that learn from only noisy SAR images. However, existing methods deal
solely with single-polarization images and cannot handle the multi-polarization
images captured by modern satellites. In this work, we present an extension of
the existing model for generating single-polarization SAR images to handle
multi-polarization SAR images. Specifically, we propose a novel self-supervised
despeckling approach called channel masking, which exploits the relationship
between polarizations. Additionally, we utilize a spatial masking method that
addresses pixel-to-pixel correlations to further enhance the performance of our
approach. By effectively incorporating multiple polarization information, our
method surpasses current state-of-the-art methods in quantitative evaluation in
both synthetic and real-world scenarios.</div><div><a href='http://arxiv.org/abs/2401.07503v1'>2401.07503v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.06994v1")'>A Change Detection Reality Check</div>
<div id='2402.06994v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T17:02:53Z</div><div>Authors: Isaac Corley, Caleb Robinson, Anthony Ortiz</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been an explosion of proposed change detection
deep learning architectures in the remote sensing literature. These approaches
claim to offer state-of the-art performance on different standard benchmark
datasets. However, has the field truly made significant progress? In this paper
we perform experiments which conclude a simple U-Net segmentation baseline
without training tricks or complicated architectural changes is still a top
performer for the task of change detection.</div><div><a href='http://arxiv.org/abs/2402.06994v1'>2402.06994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06252v1")'>AGSPNet: A framework for parcel-scale crop fine-grained semantic change
  detection from UAV high-resolution imagery with agricultural geographic scene
  constraints</div>
<div id='2401.06252v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T20:47:28Z</div><div>Authors: Shaochun Li, Yanjun Wang, Hengfan Cai, Lina Deng, Yunhao Lin</div><div style='padding-top: 10px; width: 80ex'>Real-time and accurate information on fine-grained changes in crop
cultivation is of great significance for crop growth monitoring, yield
prediction and agricultural structure adjustment. Aiming at the problems of
serious spectral confusion in visible high-resolution unmanned aerial vehicle
(UAV) images of different phases, interference of large complex background and
salt-and-pepper noise by existing semantic change detection (SCD) algorithms,
in order to effectively extract deep image features of crops and meet the
demand of agricultural practical engineering applications, this paper designs
and proposes an agricultural geographic scene and parcel-scale constrained SCD
framework for crops (AGSPNet). AGSPNet framework contains three parts:
agricultural geographic scene (AGS) division module, parcel edge extraction
module and crop SCD module. Meanwhile, we produce and introduce an UAV image
SCD dataset (CSCD) dedicated to agricultural monitoring, encompassing multiple
semantic variation types of crops in complex geographical scene. We conduct
comparative experiments and accuracy evaluations in two test areas of this
dataset, and the results show that the crop SCD results of AGSPNet consistently
outperform other deep learning SCD models in terms of quantity and quality,
with the evaluation metrics F1-score, kappa, OA, and mIoU obtaining
improvements of 0.038, 0.021, 0.011 and 0.062, respectively, on average over
the sub-optimal method. The method proposed in this paper can clearly detect
the fine-grained change information of crop types in complex scenes, which can
provide scientific and technical support for smart agriculture monitoring and
management, food policy formulation and food security assurance.</div><div><a href='http://arxiv.org/abs/2401.06252v1'>2401.06252v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07398v1")'>Cross Domain Early Crop Mapping using CropGAN and CNN Classifier</div>
<div id='2401.07398v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T00:27:41Z</div><div>Authors: Yiqun Wang, Hui Huang, Radu State</div><div style='padding-top: 10px; width: 80ex'>Driven by abundant satellite imagery, machine learning-based approaches have
recently been promoted to generate high-resolution crop cultivation maps to
support many agricultural applications. One of the major challenges faced by
these approaches is the limited availability of ground truth labels. In the
absence of ground truth, existing work usually adopts the "direct transfer
strategy" that trains a classifier using historical labels collected from other
regions and then applies the trained model to the target region. Unfortunately,
the spectral features of crops exhibit inter-region and inter-annual
variability due to changes in soil composition, climate conditions, and crop
progress, the resultant models perform poorly on new and unseen regions or
years. This paper presents the Crop Generative Adversarial Network (CropGAN) to
address the above cross-domain issue. Our approach does not need labels from
the target domain. Instead, it learns a mapping function to transform the
spectral features of the target domain to the source domain (with labels) while
preserving their local structure. The classifier trained by the source domain
data can be directly applied to the transformed data to produce high-accuracy
early crop maps of the target domain. Comprehensive experiments across various
regions and years demonstrate the benefits and effectiveness of the proposed
approach. Compared with the widely adopted direct transfer strategy, the F1
score after applying the proposed CropGAN is improved by 13.13% - 50.98%</div><div><a href='http://arxiv.org/abs/2401.07398v1'>2401.07398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02121v1")'>Enhancing crop classification accuracy by synthetic SAR-Optical data
  generation using deep learning</div>
<div id='2402.02121v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T11:07:50Z</div><div>Authors: Ali Mirzaei, Hossein Bagheri, Iman Khosravi</div><div style='padding-top: 10px; width: 80ex'>Crop classification using remote sensing data has emerged as a prominent
research area in recent decades. Studies have demonstrated that fusing SAR and
optical images can significantly enhance the accuracy of classification.
However, a major challenge in this field is the limited availability of
training data, which adversely affects the performance of classifiers. In
agricultural regions, the dominant crops typically consist of one or two
specific types, while other crops are scarce. Consequently, when collecting
training samples to create a map of agricultural products, there is an
abundance of samples from the dominant crops, forming the majority classes.
Conversely, samples from other crops are scarce, representing the minority
classes. Addressing this issue requires overcoming several challenges and
weaknesses associated with traditional data generation methods. These methods
have been employed to tackle the imbalanced nature of the training data.
Nevertheless, they still face limitations in effectively handling the minority
classes. Overall, the issue of inadequate training data, particularly for
minority classes, remains a hurdle that traditional methods struggle to
overcome. In this research, We explore the effectiveness of conditional tabular
generative adversarial network (CTGAN) as a synthetic data generation method
based on a deep learning network, in addressing the challenge of limited
training data for minority classes in crop classification using the fusion of
SAR-optical data. Our findings demonstrate that the proposed method generates
synthetic data with higher quality that can significantly increase the number
of samples for minority classes leading to better performance of crop
classifiers.</div><div><a href='http://arxiv.org/abs/2402.02121v1'>2402.02121v1</a></div>
</div></div>
    <div><a href="arxiv_16.html">Prev (16)</a></div>
    <div><a href="arxiv_18.html">Next (18)</a></div>
    