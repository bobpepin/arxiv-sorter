
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17173v1")'>Zero-Shot Reinforcement Learning via Function Encoders</div>
<div id='2401.17173v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:04:47Z</div><div>Authors: Tyler Ingebrand, Amy Zhang, Ufuk Topcu</div><div style='padding-top: 10px; width: 80ex'>Although reinforcement learning (RL) can solve many challenging sequential
decision making problems, achieving zero-shot transfer across related tasks
remains a challenge. The difficulty lies in finding a good representation for
the current task so that the agent understands how it relates to previously
seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a
representation learning algorithm which represents a function as a weighted
combination of learned, non-linear basis functions. By using a function encoder
to represent the reward function or the transition function, the agent has
information on how the current task relates to previously seen tasks via a
coherent vector representation. Thus, the agent is able to achieve transfer
between related tasks at run time with no additional training. We demonstrate
state-of-the-art data efficiency, asymptotic performance, and training
stability in three RL fields by augmenting basic RL algorithms with a function
encoder task representation.</div><div><a href='http://arxiv.org/abs/2401.17173v1'>2401.17173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14226v1")'>Sample Efficient Reinforcement Learning by Automatically Learning to
  Compose Subtasks</div>
<div id='2401.14226v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T15:06:40Z</div><div>Authors: Shuai Han, Mehdi Dastani, Shihan Wang</div><div style='padding-top: 10px; width: 80ex'>Improving sample efficiency is central to Reinforcement Learning (RL),
especially in environments where the rewards are sparse. Some recent approaches
have proposed to specify reward functions as manually designed or learned
reward structures whose integrations in the RL algorithms are claimed to
significantly improve the learning efficiency. Manually designed reward
structures can suffer from inaccuracy and existing automatically learning
methods are often computationally intractable for complex tasks. The
integration of inaccurate or partial reward structures in RL algorithms fail to
learn optimal policies. In this work, we propose an RL algorithm that can
automatically structure the reward function for sample efficiency, given a set
of labels that signify subtasks. Given such minimal knowledge about the task,
we train a high-level policy that selects optimal sub-tasks in each state
together with a low-level policy that efficiently learns to complete each
sub-task. We evaluate our algorithm in a variety of sparse-reward environments.
The experiment results show that our approach significantly outperforms the
state-of-art baselines as the difficulty of the task increases.</div><div><a href='http://arxiv.org/abs/2401.14226v1'>2401.14226v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07412v1")'>Auxiliary Reward Generation with Transition Distance Representation
  Learning</div>
<div id='2402.07412v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T05:13:44Z</div><div>Authors: Siyuan Li, Shijie Han, Yingnan Zhao, By Liang, Peng Liu</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) has shown its strength in challenging sequential
decision-making problems. The reward function in RL is crucial to the learning
performance, as it serves as a measure of the task completion degree. In
real-world problems, the rewards are predominantly human-designed, which
requires laborious tuning, and is easily affected by human cognitive biases. To
achieve automatic auxiliary reward generation, we propose a novel
representation learning approach that can measure the ``transition distance''
between states. Building upon these representations, we introduce an auxiliary
reward generation technique for both single-task and skill-chaining scenarios
without the need for human knowledge. The proposed approach is evaluated in a
wide range of manipulation tasks. The experiment results demonstrate the
effectiveness of measuring the transition distance between states and the
induced improvement by auxiliary rewards, which not only promotes better
learning efficiency but also increases convergent stability.</div><div><a href='http://arxiv.org/abs/2402.07412v1'>2402.07412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01361v1")'>To the Max: Reinventing Reward in Reinforcement Learning</div>
<div id='2402.01361v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:29:18Z</div><div>Authors: Grigorii Veviurko, Wendelin Böhmer, Mathijs de Weerdt</div><div style='padding-top: 10px; width: 80ex'>In reinforcement learning (RL), different rewards can define the same optimal
policy but result in drastically different learning performance. For some, the
agent gets stuck with a suboptimal behavior, and for others, it solves the task
efficiently. Choosing a good reward function is hence an extremely important
yet challenging problem. In this paper, we explore an alternative approach to
using rewards for learning. We introduce max-reward RL, where an agent
optimizes the maximum rather than the cumulative reward. Unlike earlier works,
our approach works for deterministic and stochastic environments and can be
easily combined with state-of-the-art RL algorithms. In the experiments, we
study the performance of max-reward RL algorithms in two goal-reaching
environments from Gymnasium-Robotics and demonstrate its benefits over standard
RL. The code is publicly available.</div><div><a href='http://arxiv.org/abs/2402.01361v1'>2402.01361v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14961v1")'>Reinforcement Learning with Elastic Time Steps</div>
<div id='2402.14961v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T20:49:04Z</div><div>Authors: Dong Wang, Giovanni Beltrame</div><div style='padding-top: 10px; width: 80ex'>Traditional Reinforcement Learning (RL) algorithms are usually applied in
robotics to learn controllers that act with a fixed control rate. Given the
discrete nature of RL algorithms, they are oblivious to the effects of the
choice of control rate: finding the correct control rate can be difficult and
mistakes often result in excessive use of computing resources or even lack of
convergence.
  We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic
algorithm to address this issue. SEAC implements elastic time steps, time steps
with a known, variable duration, which allow the agent to change its control
frequency to adapt to the situation. In practice, SEAC applies control only
when necessary, minimizing computational resources and data usage.
  We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze
navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the
SAC baseline in terms of energy efficiency and overall time management, and
most importantly without the need to identify a control frequency for the
learned controller. SEAC demonstrated faster and more stable training speeds
than SAC, especially at control rates where SAC struggled to converge.
  We also compared SEAC with a similar approach, the Continuous-Time
Continuous-Options (CTCO) model, and SEAC resulted in better task performance.
These findings highlight the potential of SEAC for practical, real-world RL
applications in robotics.</div><div><a href='http://arxiv.org/abs/2402.14961v1'>2402.14961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14508v1")'>Constrained Reinforcement Learning with Smoothed Log Barrier Function</div>
<div id='2403.14508v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T16:02:52Z</div><div>Authors: Baohe Zhang, Yuan Zhang, Lilli Frison, Thomas Brox, Joschka Bödecker</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning (RL) has been widely applied to many control tasks and
substantially improved the performances compared to conventional control
methods in many domains where the reward function is well defined. However, for
many real-world problems, it is often more convenient to formulate optimization
problems in terms of rewards and constraints simultaneously. Optimizing such
constrained problems via reward shaping can be difficult as it requires tedious
manual tuning of reward functions with several interacting terms. Recent
formulations which include constraints mostly require a pre-training phase,
which often needs human expertise to collect data or assumes having a
sub-optimal policy readily available. We propose a new constrained RL method
called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which
achieves competitive performance without any pre-training by applying a linear
smoothed log barrier function to an additional safety critic. It implements an
adaptive penalty for policy learning and alleviates the numerical issues that
are known to complicate the application of the log barrier function method. As
a result, we show that with CSAC-LB, we achieve state-of-the-art performance on
several constrained control tasks with different levels of difficulty and
evaluate our methods in a locomotion task on a real quadruped robot platform.</div><div><a href='http://arxiv.org/abs/2403.14508v1'>2403.14508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12210v1")'>Decomposing Control Lyapunov Functions for Efficient Reinforcement
  Learning</div>
<div id='2403.12210v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T19:51:17Z</div><div>Authors: Antonio Lopez, David Fridovich-Keil</div><div style='padding-top: 10px; width: 80ex'>Recent methods using Reinforcement Learning (RL) have proven to be successful
for training intelligent agents in unknown environments. However, RL has not
been applied widely in real-world robotics scenarios. This is because current
state-of-the-art RL methods require large amounts of data to learn a specific
task, leading to unreasonable costs when deploying the agent to collect data in
real-world applications. In this paper, we build from existing work that
reshapes the reward function in RL by introducing a Control Lyapunov Function
(CLF), which is demonstrated to reduce the sample complexity. Still, this
formulation requires knowing a CLF of the system, but due to the lack of a
general method, it is often a challenge to identify a suitable CLF. Existing
work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability
procedure. However, this class of methods becomes intractable on
high-dimensional systems, a problem that we address by using a system
decomposition technique to compute what we call Decomposed Control Lyapunov
Functions (DCLFs). We use the computed DCLF for reward shaping, which we show
improves RL performance. Through multiple examples, we demonstrate the
effectiveness of this approach, where our method finds a policy to successfully
land a quadcopter in less than half the amount of real-world data required by
the state-of-the-art Soft-Actor Critic algorithm.</div><div><a href='http://arxiv.org/abs/2403.12210v1'>2403.12210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04007v1")'>Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical
  Systems</div>
<div id='2403.04007v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T19:39:20Z</div><div>Authors: Wesley A. Suttle, Vipul K. Sharma, Krishna C. Kosaraju, S. Sivaranjani, Ji Liu, Vijay Gupta, Brian M. Sadler</div><div style='padding-top: 10px; width: 80ex'>We develop provably safe and convergent reinforcement learning (RL)
algorithms for control of nonlinear dynamical systems, bridging the gap between
the hard safety guarantees of control theory and the convergence guarantees of
RL theory. Recent advances at the intersection of control and RL follow a
two-stage, safety filter approach to enforcing hard safety constraints:
model-free RL is used to learn a potentially unsafe controller, whose actions
are projected onto safe sets prescribed, for example, by a control barrier
function. Though safe, such approaches lose any convergence guarantees enjoyed
by the underlying RL methods. In this paper, we develop a single-stage,
sampling-based approach to hard constraint satisfaction that learns RL
controllers enjoying classical convergence guarantees while satisfying hard
safety constraints throughout training and deployment. We validate the efficacy
of our approach in simulation, including safe control of a quadcopter in a
challenging obstacle avoidance problem, and demonstrate that it outperforms
existing benchmarks.</div><div><a href='http://arxiv.org/abs/2403.04007v1'>2403.04007v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15197v1")'>Safety Optimized Reinforcement Learning via Multi-Objective Policy
  Optimization</div>
<div id='2402.15197v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:58:38Z</div><div>Authors: Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran</div><div style='padding-top: 10px; width: 80ex'>Safe reinforcement learning (Safe RL) refers to a class of techniques that
aim to prevent RL algorithms from violating constraints in the process of
decision-making and exploration during trial and error. In this paper, a novel
model-free Safe RL algorithm, formulated based on the multi-objective policy
optimization framework is introduced where the policy is optimized towards
optimality and safety, simultaneously. The optimality is achieved by the
environment reward function that is subsequently shaped using a safety critic.
The advantage of the Safety Optimized RL (SORL) algorithm compared to the
traditional Safe RL algorithms is that it omits the need to constrain the
policy search space. This allows SORL to find a natural tradeoff between safety
and optimality without compromising the performance in terms of either safety
or optimality due to strict search space constraints. Through our theoretical
analysis of SORL, we propose a condition for SORL's converged policy to
guarantee safety and then use it to introduce an aggressiveness parameter that
allows for fine-tuning the mentioned tradeoff. The experimental results
obtained in seven different robotic environments indicate a considerable
reduction in the number of safety violations along with higher, or competitive,
policy returns, in comparison to six different state-of-the-art Safe RL
methods. The results demonstrate the significant superiority of the proposed
SORL algorithm in safety-critical applications.</div><div><a href='http://arxiv.org/abs/2402.15197v1'>2402.15197v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02025v1")'>A Survey of Constraint Formulations in Safe Reinforcement Learning</div>
<div id='2402.02025v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T04:40:31Z</div><div>Authors: Akifumi Wachi, Xun Shen, Yanan Sui</div><div style='padding-top: 10px; width: 80ex'>Ensuring safety is critical when applying reinforcement learning (RL) to
real-world problems. Consequently, safe RL emerges as a fundamental and
powerful paradigm for safely optimizing an agent's policy from experimental
data. A popular safe RL approach is based on a constrained criterion, which
solves the problem of maximizing expected cumulative reward under safety
constraints. Though there has been recently a surge of such attempts to achieve
safety in RL, a systematic understanding of the field is difficult due to 1)
the diversity of constraint representations and 2) little discussion of their
interrelations. To address this knowledge gap, we provide a comprehensive
review of representative constraint formulations, along with a curated
selection of algorithms specifically designed for each formulation.
Furthermore, we elucidate the theoretical underpinnings that reveal the
mathematical mutual relations among common problem formulations. We conclude
with a discussion of the current state and future directions of safe
reinforcement learning research.</div><div><a href='http://arxiv.org/abs/2402.02025v1'>2402.02025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15893v2")'>Concurrent Learning of Policy and Unknown Safety Constraints in
  Reinforcement Learning</div>
<div id='2402.15893v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T20:01:15Z</div><div>Authors: Lunet Yifru, Ali Baheri</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) has revolutionized decision-making across a wide
range of domains over the past few decades. Yet, deploying RL policies in
real-world scenarios presents the crucial challenge of ensuring safety.
Traditional safe RL approaches have predominantly focused on incorporating
predefined safety constraints into the policy learning process. However, this
reliance on predefined safety constraints poses limitations in dynamic and
unpredictable real-world settings where such constraints may not be available
or sufficiently adaptable. Bridging this gap, we propose a novel approach that
concurrently learns a safe RL control policy and identifies the unknown safety
constraint parameters of a given environment. Initializing with a parametric
signal temporal logic (pSTL) safety specification and a small initial labeled
dataset, we frame the problem as a bilevel optimization task, intricately
integrating constrained policy optimization, using a Lagrangian-variant of the
twin delayed deep deterministic policy gradient (TD3) algorithm, with Bayesian
optimization for optimizing parameters for the given pSTL safety specification.
Through experimentation in comprehensive case studies, we validate the efficacy
of this approach across varying forms of environmental constraints,
consistently yielding safe RL policies with high returns. Furthermore, our
findings indicate successful learning of STL safety constraint parameters,
exhibiting a high degree of conformity with true environmental safety
constraints. The performance of our model closely mirrors that of an ideal
scenario that possesses complete prior knowledge of safety constraints,
demonstrating its proficiency in accurately identifying environmental safety
constraints and learning safe policies that adhere to those constraints.</div><div><a href='http://arxiv.org/abs/2402.15893v2'>2402.15893v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17217v1")'>Temporal Logic Specification-Conditioned Decision Transformer for
  Offline Safe Reinforcement Learning</div>
<div id='2402.17217v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T05:16:59Z</div><div>Authors: Zijian Guo, Weichao Zhou, Wenchao Li</div><div style='padding-top: 10px; width: 80ex'>Offline safe reinforcement learning (RL) aims to train a constraint
satisfaction policy from a fixed dataset. Current state-of-the-art approaches
are based on supervised learning with a conditioned policy. However, these
approaches fall short in real-world applications that involve complex tasks
with rich temporal and logical structures. In this paper, we propose temporal
logic Specification-conditioned Decision Transformer (SDT), a novel framework
that harnesses the expressive power of signal temporal logic (STL) to specify
complex temporal rules that an agent should follow and the sequential modeling
capability of Decision Transformer (DT). Empirical evaluations on the DSRL
benchmarks demonstrate the better capacity of SDT in learning safe and
high-reward policies compared with existing approaches. In addition, SDT shows
good alignment with respect to different desired degrees of satisfaction of the
STL specification that it is conditioned on.</div><div><a href='http://arxiv.org/abs/2402.17217v1'>2402.17217v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10700v1")'>Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion
  Model</div>
<div id='2401.10700v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T14:05:09Z</div><div>Authors: Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu</div><div style='padding-top: 10px; width: 80ex'>Safe offline RL is a promising way to bypass risky online interactions
towards safe policy learning. Most existing methods only enforce soft
constraints, i.e., constraining safety violations in expectation below
thresholds predetermined. This can lead to potentially unsafe outcomes, thus
unacceptable in safety-critical scenarios. An alternative is to enforce the
hard constraint of zero violation. However, this can be challenging in offline
setting, as it needs to strike the right balance among three highly intricate
and correlated aspects: safety constraint satisfaction, reward maximization,
and behavior regularization imposed by offline datasets. Interestingly, we
discover that via reachability analysis of safe-control theory, the hard safety
constraint can be equivalently translated to identifying the largest feasible
region given the offline dataset. This seamlessly converts the original trilogy
problem to a feasibility-dependent objective, i.e., maximizing reward value
within the feasible region while minimizing safety risks in the infeasible
region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline
RL), which allows safety constraint adherence, reward maximization, and offline
policy learning to be realized via three decoupled processes, while offering
strong safety performance and stability. In FISOR, the optimal policy for the
translated optimization problem can be derived in a special form of weighted
behavior cloning. Thus, we propose a novel energy-guided diffusion model that
does not require training a complicated time-dependent classifier to extract
the policy, greatly simplifying the training. We compare FISOR against
baselines on DSRL benchmark for safe offline RL. Evaluation results show that
FISOR is the only method that can guarantee safety satisfaction in all tasks,
while achieving top returns in most tasks.</div><div><a href='http://arxiv.org/abs/2401.10700v1'>2401.10700v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06397v2")'>DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe
  Multi-Agent Reinforcement Learning</div>
<div id='2403.06397v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T03:17:33Z</div><div>Authors: Xuefeng Wang, Henglin Pu, Hyung Jun Kim, Husheng Li</div><div style='padding-top: 10px; width: 80ex'>Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained
attention in recent years, emphasizing the need for agents to not only optimize
the global return but also adhere to safety requirements through behavioral
constraints. Some recent work has integrated control theory with multi-agent
reinforcement learning to address the challenge of ensuring safety. However,
there have been only very limited applications of Model Predictive Control
(MPC) methods in this domain, primarily due to the complex and implicit
dynamics characteristic of multi-agent environments. To bridge this gap, we
propose a novel method called Deep Learning-Based Model Predictive Control for
Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of
DeepSafeMPC is leveraging a entralized deep learning model to well predict
environmental dynamics. Our method applies MARL principles to search for
optimal solutions. Through the employment of MPC, the actions of agents can be
restricted within safe states concurrently. We demonstrate the effectiveness of
our approach using the Safe Multi-agent MuJoCo environment, showcasing
significant advancements in addressing safety concerns in MARL.</div><div><a href='http://arxiv.org/abs/2403.06397v2'>2403.06397v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07553v1")'>Safe Reinforcement Learning with Free-form Natural Language Constraints
  and Pre-Trained Language Models</div>
<div id='2401.07553v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T09:37:03Z</div><div>Authors: Xingzhou Lou, Junge Zhang, Ziyan Wang, Kaiqi Huang, Yali Du</div><div style='padding-top: 10px; width: 80ex'>Safe reinforcement learning (RL) agents accomplish given tasks while adhering
to specific constraints. Employing constraints expressed via
easily-understandable human language offers considerable potential for
real-world applications due to its accessibility and non-reliance on domain
expertise. Previous safe RL methods with natural language constraints typically
adopt a recurrent neural network, which leads to limited capabilities when
dealing with various forms of human language input. Furthermore, these methods
often require a ground-truth cost function, necessitating domain expertise for
the conversion of language constraints into a well-defined cost function that
determines constraint violation. To address these issues, we proposes to use
pre-trained language models (LM) to facilitate RL agents' comprehension of
natural language constraints and allow them to infer costs for safe policy
learning. Through the use of pre-trained LMs and the elimination of the need
for a ground-truth cost, our method enhances safe policy learning under a
diverse set of human-derived free-form natural language constraints.
Experiments on grid-world navigation and robot control show that the proposed
method can achieve strong performance while adhering to given constraints. The
usage of pre-trained LMs allows our method to comprehend complicated
constraints and learn safe policies without the need for ground-truth cost at
any stage of training or evaluation. Extensive ablation studies are conducted
to demonstrate the efficacy of each part of our method.</div><div><a href='http://arxiv.org/abs/2401.07553v1'>2401.07553v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00816v1")'>Leveraging Approximate Model-based Shielding for Probabilistic Safety
  Guarantees in Continuous Environments</div>
<div id='2402.00816v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:55:08Z</div><div>Authors: Alexander W. Goodall, Francesco Belardinelli</div><div style='padding-top: 10px; width: 80ex'>Shielding is a popular technique for achieving safe reinforcement learning
(RL). However, classical shielding approaches come with quite restrictive
assumptions making them difficult to deploy in complex environments,
particularly those with continuous state or action spaces. In this paper we
extend the more versatile approximate model-based shielding (AMBS) framework to
the continuous setting. In particular we use Safety Gym as our test-bed,
allowing for a more direct comparison of AMBS with popular constrained RL
algorithms. We also provide strong probabilistic safety guarantees for the
continuous setting. In addition, we propose two novel penalty techniques that
directly modify the policy gradient, which empirically provide more stable
convergence in our experiments.</div><div><a href='http://arxiv.org/abs/2402.00816v1'>2402.00816v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14758v1")'>Off-Policy Primal-Dual Safe Reinforcement Learning</div>
<div id='2401.14758v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T10:33:38Z</div><div>Authors: Zifan Wu, Bo Tang, Qian Lin, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, Dong Wang</div><div style='padding-top: 10px; width: 80ex'>Primal-dual safe RL methods commonly perform iterations between the primal
update of the policy and the dual update of the Lagrange Multiplier. Such a
training paradigm is highly susceptible to the error in cumulative cost
estimation since this estimation serves as the key bond connecting the primal
and dual update processes. We show that this problem causes significant
underestimation of cost when using off-policy methods, leading to the failure
to satisfy the safety constraint. To address this issue, we propose
\textit{conservative policy optimization}, which learns a policy in a
constraint-satisfying area by considering the uncertainty in cost estimation.
This improves constraint satisfaction but also potentially hinders reward
maximization. We then introduce \textit{local policy convexification} to help
eliminate such suboptimality by gradually reducing the estimation uncertainty.
We provide theoretical interpretations of the joint coupling effect of these
two ingredients and further verify them by extensive experiments. Results on
benchmark tasks show that our method not only achieves an asymptotic
performance comparable to state-of-the-art on-policy methods while using much
fewer samples, but also significantly reduces constraint violation during
training. Our code is available at https://github.com/ZifanWu/CAL.</div><div><a href='http://arxiv.org/abs/2401.14758v1'>2401.14758v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00282v1")'>Scale-Invariant Gradient Aggregation for Constrained Multi-Objective
  Reinforcement Learning</div>
<div id='2403.00282v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T04:57:13Z</div><div>Authors: Dohyeong Kim, Mineui Hong, Jeongho Park, Songhwai Oh</div><div style='padding-top: 10px; width: 80ex'>Multi-objective reinforcement learning (MORL) aims to find a set of Pareto
optimal policies to cover various preferences. However, to apply MORL in
real-world applications, it is important to find policies that are not only
Pareto optimal but also satisfy pre-defined constraints for safety. To this
end, we propose a constrained MORL (CMORL) algorithm called Constrained
Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of
handling multiple objectives and constraints concurrently, CoMOGA relaxes the
original CMORL problem into a constrained optimization problem by transforming
the objectives into additional constraints. This novel transformation process
ensures that the converted constraints are invariant to the objective scales
while having the same effect as the original objectives. We show that the
proposed method converges to a local Pareto optimal policy while satisfying the
predefined constraints. Empirical evaluations across various tasks show that
the proposed method outperforms other baselines by consistently meeting
constraints and demonstrating invariance to the objective scales.</div><div><a href='http://arxiv.org/abs/2403.00282v1'>2403.00282v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09492v2")'>PMGDA: A Preference-based Multiple Gradient Descent Algorithm</div>
<div id='2402.09492v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T11:27:31Z</div><div>Authors: Xiaoyuan Zhang, Xi Lin, Qingfu Zhang</div><div style='padding-top: 10px; width: 80ex'>It is desirable in many multi-objective machine learning applications, such
as multi-task learning with conflicting objectives and multi-objective
reinforcement learning, to find a Pareto solution that can match a given
preference of a decision maker. These problems are often large-scale with
available gradient information but cannot be handled very well by the existing
algorithms. To tackle this critical issue, this paper proposes a novel
predict-and-correct framework for locating a Pareto solution that fits the
preference of a decision maker. In the proposed framework, a constraint
function is introduced in the search progress to align the solution with a
user-specific preference, which can be optimized simultaneously with multiple
objective functions. Experimental results show that our proposed method can
efficiently find a particular Pareto solution under the demand of a decision
maker for standard multiobjective benchmark, multi-task learning, and
multi-objective reinforcement learning problems with more than thousands of
decision variables.
  Code is available at: https://github.com/xzhang2523/pmgda. Our code is
current provided in the pgmda.rar attached file and will be open-sourced after
publication.}</div><div><a href='http://arxiv.org/abs/2402.09492v2'>2402.09492v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04099v1")'>Many-Objective Multi-Solution Transport</div>
<div id='2403.04099v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T23:03:12Z</div><div>Authors: Ziyue Li, Tian Li, Virginia Smith, Jeff Bilmes, Tianyi Zhou</div><div style='padding-top: 10px; width: 80ex'>Optimizing the performance of many objectives (instantiated by tasks or
clients) jointly with a few Pareto stationary solutions (models) is critical in
machine learning. However, previous multi-objective optimization methods often
focus on a few number of objectives and cannot scale to many objectives that
outnumber the solutions, leading to either subpar performance or ignored
objectives. We introduce Many-objective multi-solution Transport (MosT), a
framework that finds multiple diverse solutions in the Pareto front of many
objectives. Our insight is to seek multiple solutions, each performing as a
domain expert and focusing on a specific subset of objectives while
collectively covering all of them. MosT formulates the problem as a bi-level
optimization of weighted objectives for each solution, where the weights are
defined by an optimal transport between the objectives and solutions. Our
algorithm ensures convergence to Pareto stationary solutions for complementary
subsets of objectives. On a range of applications in federated learning,
multi-task learning, and mixture-of-prompt learning for LLMs, MosT distinctly
outperforms strong baselines, delivering high-quality, diverse solutions that
profile the entire Pareto frontier, thus ensuring balanced trade-offs across
many objectives.</div><div><a href='http://arxiv.org/abs/2403.04099v1'>2403.04099v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13930v1")'>Enhancing Reinforcement Learning Agents with Local Guides</div>
<div id='2402.13930v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:52:26Z</div><div>Authors: Paul Daoudi, Bogdan Robu, Christophe Prieur, Ludovic Dos Santos, Merwan Barlier</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the problem of integrating local guide policies into a
Reinforcement Learning agent. For this, we show how to adapt existing
algorithms to this setting before introducing a novel algorithm based on a
noisy policy-switching procedure. This approach builds on a proper Approximate
Policy Evaluation (APE) scheme to provide a perturbation that carefully leads
the local guides towards better actions. We evaluated our method on a set of
classical Reinforcement Learning problems, including safety-critical systems
where the agent cannot enter some areas at the risk of triggering catastrophic
consequences. In all the proposed environments, our agent proved to be
efficient at leveraging those policies to improve the performance of any
APE-based Reinforcement Learning algorithm, especially in its first learning
stages.</div><div><a href='http://arxiv.org/abs/2402.13930v1'>2402.13930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12847v2")'>Policy Bifurcation in Safe Reinforcement Learning</div>
<div id='2403.12847v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T15:54:38Z</div><div>Authors: Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, Keqiang Li</div><div style='padding-top: 10px; width: 80ex'>Safe reinforcement learning (RL) offers advanced solutions to constrained
optimal control problems. Existing studies in safe RL implicitly assume
continuity in policy functions, where policies map states to actions in a
smooth, uninterrupted manner; however, our research finds that in some
scenarios, the feasible policy should be discontinuous or multi-valued,
interpolating between discontinuous local optima can inevitably lead to
constraint violations. We are the first to identify the generating mechanism of
such a phenomenon, and employ topological analysis to rigorously prove the
existence of policy bifurcation in safe RL, which corresponds to the
contractibility of the reachable tuple. Our theorem reveals that in scenarios
where the obstacle-free state space is non-simply connected, a feasible policy
is required to be bifurcated, meaning its output action needs to change
abruptly in response to the varying state. To train such a bifurcated policy,
we propose a safe RL algorithm called multimodal policy optimization (MUPO),
which utilizes a Gaussian mixture distribution as the policy output. The
bifurcated behavior can be achieved by selecting the Gaussian component with
the highest mixing coefficient. Besides, MUPO also integrates spectral
normalization and forward KL divergence to enhance the policy's capability of
exploring different modes. Experiments with vehicle control tasks show that our
algorithm successfully learns the bifurcated policy and ensures satisfying
safety, while a continuous policy suffers from inevitable constraint
violations.</div><div><a href='http://arxiv.org/abs/2403.12847v2'>2403.12847v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00355v1")'>Adaptive Primal-Dual Method for Safe Reinforcement Learning</div>
<div id='2402.00355v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T05:53:44Z</div><div>Authors: Weiqin Chen, James Onyejizu, Long Vu, Lan Hoang, Dharmashankar Subramanian, Koushik Kar, Sandipan Mishra, Santiago Paternain</div><div style='padding-top: 10px; width: 80ex'>Primal-dual methods have a natural application in Safe Reinforcement Learning
(SRL), posed as a constrained policy optimization problem. In practice however,
applying primal-dual methods to SRL is challenging, due to the inter-dependency
of the learning rate (LR) and Lagrangian multipliers (dual variables) each time
an embedded unconstrained RL problem is solved. In this paper, we propose,
analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two
adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the
policy in each iteration. We theoretically establish the convergence,
optimality and feasibility of the APD algorithm. Finally, we conduct numerical
evaluation of the practical APD algorithm with four well-known environments in
Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian
and DDPG-Lagrangian. All experiments show that the practical APD algorithm
outperforms (or achieves comparable performance) and attains more stable
training than the constant LR cases. Additionally, we substantiate the
robustness of selecting the two adaptive LRs by empirical evidence.</div><div><a href='http://arxiv.org/abs/2402.00355v1'>2402.00355v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00313v1")'>Control in Stochastic Environment with Delays: A Model-based
  Reinforcement Learning Approach</div>
<div id='2402.00313v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T03:53:56Z</div><div>Authors: Zhiyuan Yao, Ionut Florescu, Chihoon Lee</div><div style='padding-top: 10px; width: 80ex'>In this paper we are introducing a new reinforcement learning method for
control problems in environments with delayed feedback. Specifically, our
method employs stochastic planning, versus previous methods that used
deterministic planning. This allows us to embed risk preference in the policy
optimization problem. We show that this formulation can recover the optimal
policy for problems with deterministic transitions. We contrast our policy with
two prior methods from literature. We apply the methodology to simple tasks to
understand its features. Then, we compare the performance of the methods in
controlling multiple Atari games.</div><div><a href='http://arxiv.org/abs/2402.00313v1'>2402.00313v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13020v1")'>A Safe Reinforcement Learning Algorithm for Supervisory Control of Power
  Plants</div>
<div id='2401.13020v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T17:52:49Z</div><div>Authors: Yixuan Sun, Sami Khairy, Richard B. Vilim, Rui Hu, Akshay J. Dave</div><div style='padding-top: 10px; width: 80ex'>Traditional control theory-based methods require tailored engineering for
each system and constant fine-tuning. In power plant control, one often needs
to obtain a precise representation of the system dynamics and carefully design
the control scheme accordingly. Model-free Reinforcement learning (RL) has
emerged as a promising solution for control tasks due to its ability to learn
from trial-and-error interactions with the environment. It eliminates the need
for explicitly modeling the environment's dynamics, which is potentially
inaccurate. However, the direct imposition of state constraints in power plant
control raises challenges for standard RL methods. To address this, we propose
a chance-constrained RL algorithm based on Proximal Policy Optimization for
supervisory control. Our method employs Lagrangian relaxation to convert the
constrained optimization problem into an unconstrained objective, where
trainable Lagrange multipliers enforce the state constraints. Our approach
achieves the smallest distance of violation and violation rate in a load-follow
maneuver for an advanced Nuclear Power Plant design.</div><div><a href='http://arxiv.org/abs/2401.13020v1'>2401.13020v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12849v1")'>Learning safety critics via a non-contractive binary bellman operator</div>
<div id='2401.12849v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T15:33:30Z</div><div>Authors: Agustin Castellano, Hancheng Min, Juan Andrés Bazerque, Enrique Mallada</div><div style='padding-top: 10px; width: 80ex'>The inability to naturally enforce safety in Reinforcement Learning (RL),
with limited failures, is a core challenge impeding its use in real-world
applications. One notion of safety of vast practical relevance is the ability
to avoid (unsafe) regions of the state space. Though such a safety goal can be
captured by an action-value-like function, a.k.a. safety critics, the
associated operator lacks the desired contraction and uniqueness properties
that the classical Bellman operator enjoys. In this work, we overcome the
non-contractiveness of safety critic operators by leveraging that safety is a
binary property. To that end, we study the properties of the binary safety
critic associated with a deterministic dynamical system that seeks to avoid
reaching an unsafe region. We formulate the corresponding binary Bellman
equation (B2E) for safety and study its properties. While the resulting
operator is still non-contractive, we fully characterize its fixed points
representing--except for a spurious solution--maximal persistently safe regions
of the state space that can always avoid failure. We provide an algorithm that,
by design, leverages axiomatic knowledge of safe data to avoid spurious fixed
points.</div><div><a href='http://arxiv.org/abs/2401.12849v1'>2401.12849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15650v1")'>Multi-Constraint Safe RL with Objective Suppression for Safety-Critical
  Applications</div>
<div id='2402.15650v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T23:22:06Z</div><div>Authors: Zihan Zhou, Jonathan Booher, Wei Liu, Aleksandr Petiushko, Animesh Garg</div><div style='padding-top: 10px; width: 80ex'>Safe reinforcement learning tasks with multiple constraints are a challenging
domain despite being very common in the real world. To address this challenge,
we propose Objective Suppression, a novel method that adaptively suppresses the
task reward maximizing objectives according to a safety critic. We benchmark
Objective Suppression in two multi-constraint safety domains, including an
autonomous driving domain where any incorrect behavior can lead to disastrous
consequences. Empirically, we demonstrate that our proposed method, when
combined with existing safe RL algorithms, can match the task reward achieved
by our baselines with significantly fewer constraint violations.</div><div><a href='http://arxiv.org/abs/2402.15650v1'>2402.15650v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05876v1")'>Safe reinforcement learning in uncertain contexts</div>
<div id='2401.05876v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T12:35:36Z</div><div>Authors: Dominik Baumann, Thomas B. Schön</div><div style='padding-top: 10px; width: 80ex'>When deploying machine learning algorithms in the real world, guaranteeing
safety is an essential asset. Existing safe learning approaches typically
consider continuous variables, i.e., regression tasks. However, in practice,
robotic systems are also subject to discrete, external environmental changes,
e.g., having to carry objects of certain weights or operating on frozen, wet,
or dry surfaces. Such influences can be modeled as discrete context variables.
In the existing literature, such contexts are, if considered, mostly assumed to
be known. In this work, we drop this assumption and show how we can perform
safe learning when we cannot directly measure the context variables. To achieve
this, we derive frequentist guarantees for multi-class classification, allowing
us to estimate the current context from measurements. Further, we propose an
approach for identifying contexts through experiments. We discuss under which
conditions we can retain theoretical guarantees and demonstrate the
applicability of our algorithm on a Furuta pendulum with camera measurements of
different weights that serve as contexts.</div><div><a href='http://arxiv.org/abs/2401.05876v1'>2401.05876v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18260v1")'>Efficiently Computable Safety Bounds for Gaussian Processes in Active
  Learning</div>
<div id='2402.18260v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T11:47:15Z</div><div>Authors: Jörn Tebbe, Christoph Zimmer, Ansgar Steland, Markus Lange-Hegermann, Fabian Mies</div><div style='padding-top: 10px; width: 80ex'>Active learning of physical systems must commonly respect practical safety
constraints, which restricts the exploration of the design space. Gaussian
Processes (GPs) and their calibrated uncertainty estimations are widely used
for this purpose. In many technical applications the design space is explored
via continuous trajectories, along which the safety needs to be assessed. This
is particularly challenging for strict safety requirements in GP methods, as it
employs computationally expensive Monte-Carlo sampling of high quantiles. We
address these challenges by providing provable safety bounds based on the
adaptively sampled median of the supremum of the posterior GP. Our method
significantly reduces the number of samples required for estimating high safety
probabilities, resulting in faster evaluation without sacrificing accuracy and
exploration speed. The effectiveness of our safe active learning approach is
demonstrated through extensive simulations and validated using a real-world
engine example.</div><div><a href='http://arxiv.org/abs/2402.18260v1'>2402.18260v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10497v1")'>Data-Driven Distributionally Robust Safety Verification Using Barrier
  Certificates and Conditional Mean Embeddings</div>
<div id='2403.10497v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:32:02Z</div><div>Authors: Oliver Schön, Zhengang Zhong, Sadegh Soudjani</div><div style='padding-top: 10px; width: 80ex'>Algorithmic verification of realistic systems to satisfy safety and other
temporal requirements has suffered from poor scalability of the employed formal
approaches. To design systems with rigorous guarantees, many approaches still
rely on exact models of the underlying systems. Since this assumption can
rarely be met in practice, models have to be inferred from measurement data or
are bypassed completely. Whilst former usually requires the model structure to
be known a-priori and immense amounts of data to be available, latter gives
rise to a plethora of restrictive mathematical assumptions about the unknown
dynamics. In a pursuit of developing scalable formal verification algorithms
without shifting the problem to unrealistic assumptions, we employ the concept
of barrier certificates, which can guarantee safety of the system, and learn
the certificate directly from a compact set of system trajectories. We use
conditional mean embeddings to embed data from the system into a reproducing
kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be
inflated to robustify the result w.r.t. a set of plausible transition kernels.
We show how to solve the resulting program efficiently using sum-of-squares
optimization and a Gaussian process envelope. Our approach lifts the need for
restrictive assumptions on the system dynamics and uncertainty, and suggests an
improvement in the sample complexity of verifying the safety of a system on a
tested case study compared to a state-of-the-art approach.</div><div><a href='http://arxiv.org/abs/2403.10497v1'>2403.10497v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08502v1")'>Provable Traffic Rule Compliance in Safe Reinforcement Learning on the
  Open Sea</div>
<div id='2402.08502v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T14:59:19Z</div><div>Authors: Hanna Krasowski, Matthias Althoff</div><div style='padding-top: 10px; width: 80ex'>Autonomous vehicles have to obey traffic rules. These rules are often
formalized using temporal logic, resulting in constraints that are hard to
solve using optimization-based motion planners. Reinforcement Learning (RL) is
a promising method to find motion plans adhering to temporal logic
specifications. However, vanilla RL algorithms are based on random exploration,
which is inherently unsafe. To address this issue, we propose a provably safe
RL approach that always complies with traffic rules. As a specific application
area, we consider vessels on the open sea, which must adhere to the Convention
on the International Regulations for Preventing Collisions at Sea (COLREGS). We
introduce an efficient verification approach that determines the compliance of
actions with respect to the COLREGS formalized using temporal logic. Our action
verification is integrated into the RL process so that the agent only selects
verified actions. In contrast to agents that only integrate the traffic rule
information in the reward function, our provably safe agent always complies
with the formalized rules in critical maritime traffic situations and, thus,
never causes a collision.</div><div><a href='http://arxiv.org/abs/2402.08502v1'>2402.08502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19275v1")'>Adaptive Testing Environment Generation for Connected and Automated
  Vehicles with Dense Reinforcement Learning</div>
<div id='2402.19275v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:42:33Z</div><div>Authors: Jingxuan Yang, Ruoxuan Bai, Haoyuan Ji, Yi Zhang, Jianming Hu, Shuo Feng</div><div style='padding-top: 10px; width: 80ex'>The assessment of safety performance plays a pivotal role in the development
and deployment of connected and automated vehicles (CAVs). A common approach
involves designing testing scenarios based on prior knowledge of CAVs (e.g.,
surrogate models), conducting tests in these scenarios, and subsequently
evaluating CAVs' safety performances. However, substantial differences between
CAVs and the prior knowledge can significantly diminish the evaluation
efficiency. In response to this issue, existing studies predominantly
concentrate on the adaptive design of testing scenarios during the CAV testing
process. Yet, these methods have limitations in their applicability to
high-dimensional scenarios. To overcome this challenge, we develop an adaptive
testing environment that bolsters evaluation robustness by incorporating
multiple surrogate models and optimizing the combination coefficients of these
surrogate models to enhance evaluation efficiency. We formulate the
optimization problem as a regression task utilizing quadratic programming. To
efficiently obtain the regression target via reinforcement learning, we propose
the dense reinforcement learning method and devise a new adaptive policy with
high sample efficiency. Essentially, our approach centers on learning the
values of critical scenes displaying substantial surrogate-to-real gaps. The
effectiveness of our method is validated in high-dimensional overtaking
scenarios, demonstrating that our approach achieves notable evaluation
efficiency.</div><div><a href='http://arxiv.org/abs/2402.19275v1'>2402.19275v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01795v1")'>Few-Shot Scenario Testing for Autonomous Vehicles Based on Neighborhood
  Coverage and Similarity</div>
<div id='2402.01795v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T04:47:14Z</div><div>Authors: Shu Li, Jingxuan Yang, Honglin He, Yi Zhang, Jianming Hu, Shuo Feng</div><div style='padding-top: 10px; width: 80ex'>Testing and evaluating the safety performance of autonomous vehicles (AVs) is
essential before the large-scale deployment. Practically, the acceptable cost
of testing specific AV model can be restricted within an extremely small limit
because of testing cost or time. With existing testing methods, the limitations
imposed by strictly restricted testing numbers often result in significant
uncertainties or challenges in quantifying testing results. In this paper, we
formulate this problem for the first time the "few-shot testing" (FST) problem
and propose a systematic FST framework to address this challenge. To alleviate
the considerable uncertainty inherent in a small testing scenario set and
optimize scenario utilization, we frame the FST problem as an optimization
problem and search for a small scenario set based on neighborhood coverage and
similarity. By leveraging the prior information on surrogate models (SMs), we
dynamically adjust the testing scenario set and the contribution of each
scenario to the testing result under the guidance of better generalization
ability on AVs. With certain hypotheses on SMs, a theoretical upper bound of
testing error is established to verify the sufficiency of testing accuracy
within given limited number of tests. The experiments of the cut-in scenario
using FST method demonstrate a notable reduction in testing error and variance
compared to conventional testing methods, especially for situations with a
strict limitation on the number of scenarios.</div><div><a href='http://arxiv.org/abs/2402.01795v1'>2402.01795v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02329v1")'>COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against
  Semantic Attacks</div>
<div id='2403.02329v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T18:57:11Z</div><div>Authors: Zijian Huang, Wenda Chu, Linyi Li, Chejian Xu, Bo Li</div><div style='padding-top: 10px; width: 80ex'>Multi-sensor fusion systems (MSFs) play a vital role as the perception module
in modern autonomous vehicles (AVs). Therefore, ensuring their robustness
against common and realistic adversarial semantic transformations, such as
rotation and shifting in the physical world, is crucial for the safety of AVs.
While empirical evidence suggests that MSFs exhibit improved robustness
compared to single-modal models, they are still vulnerable to adversarial
semantic transformations. Despite the proposal of empirical defenses, several
works show that these defenses can be attacked again by new adaptive attacks.
So far, there is no certified defense proposed for MSFs. In this work, we
propose the first robustness certification framework COMMIT certify robustness
of multi-sensor fusion systems against semantic attacks. In particular, we
propose a practical anisotropic noise mechanism that leverages randomized
smoothing with multi-modal data and performs a grid-based splitting method to
characterize complex semantic transformations. We also propose efficient
algorithms to compute the certification in terms of object detection accuracy
and IoU for large-scale MSF models. Empirically, we evaluate the efficacy of
COMMIT in different settings and provide a comprehensive benchmark of certified
robustness for different MSF models using the CARLA simulation platform. We
show that the certification for MSF models is at most 48.39% higher than that
of single-modal models, which validates the advantages of MSF models. We
believe our certification framework and benchmark will contribute an important
step towards certifiably robust AVs in practice.</div><div><a href='http://arxiv.org/abs/2403.02329v1'>2403.02329v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12307v1")'>Multi-View Conformal Learning for Heterogeneous Sensor Fusion</div>
<div id='2402.12307v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T17:30:09Z</div><div>Authors: Enrique Garcia-Ceja</div><div style='padding-top: 10px; width: 80ex'>Being able to assess the confidence of individual predictions in machine
learning models is crucial for decision making scenarios. Specially, in
critical applications such as medical diagnosis, security, and unmanned
vehicles, to name a few. In the last years, complex predictive models have had
great success in solving hard tasks and new methods are being proposed every
day. While the majority of new developments in machine learning models focus on
improving the overall performance, less effort is put on assessing the
trustworthiness of individual predictions, and even to a lesser extent, in the
context of sensor fusion. To this end, we build and test multi-view and
single-view conformal models for heterogeneous sensor fusion. Our models
provide theoretical marginal confidence guarantees since they are based on the
conformal prediction framework. We also propose a multi-view semi-conformal
model based on sets intersection. Through comprehensive experimentation, we
show that multi-view models perform better than single-view models not only in
terms of accuracy-based performance metrics (as it has already been shown in
several previous works) but also in conformal measures that provide uncertainty
estimation. Our results also showed that multi-view models generate prediction
sets with less uncertainty compared to single-view models.</div><div><a href='http://arxiv.org/abs/2402.12307v1'>2402.12307v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07503v1")'>Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement
  Learning Approach</div>
<div id='2403.07503v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:42:32Z</div><div>Authors: Shuchang Yan</div><div style='padding-top: 10px; width: 80ex'>Hybrid electric vehicles (HEVs) are becoming increasingly popular because
they can better combine the working characteristics of internal combustion
engines and electric motors. However, the minimum fuel consumption of an HEV
for a battery electrical balance case under a specific assembly condition and a
specific speed curve still needs to be clarified in academia and industry.
Regarding this problem, this work provides the mathematical expression of
constrained optimal fuel consumption (COFC) from the perspective of constrained
reinforcement learning (CRL) for the first time globally. Also, two mainstream
approaches of CRL, constrained variational policy optimization (CVPO) and
Lagrangian-based approaches, are utilized for the first time to obtain the
vehicle's minimum fuel consumption under the battery electrical balance
condition. We conduct case studies on the well-known Prius TOYOTA hybrid system
(THS) under the NEDC condition; we give vital steps to implement CRL approaches
and compare the performance between the CVPO and Lagrangian-based approaches.
Our case study found that CVPO and Lagrangian-based approaches can obtain the
lowest fuel consumption while maintaining the SOC balance constraint. The CVPO
approach converges stable, but the Lagrangian-based approach can obtain the
lowest fuel consumption at 3.95 L/100km, though with more significant
oscillations. This result verifies the effectiveness of our proposed CRL
approaches to the COFC problem.</div><div><a href='http://arxiv.org/abs/2403.07503v1'>2403.07503v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13148v1")'>NLBAC: A Neural Ordinary Differential Equations-based Framework for
  Stable and Safe Reinforcement Learning</div>
<div id='2401.13148v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T23:50:19Z</div><div>Authors: Liqun Zhao, Keyan Miao, Konstantinos Gatsis, Antonis Papachristodoulou</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) excels in applications such as video games and
robotics, but ensuring safety and stability remains challenging when using RL
to control real-world systems where using model-free algorithms suffering from
low sample efficiency might be prohibitive. This paper first provides safety
and stability definitions for the RL system, and then introduces a Neural
ordinary differential equations-based Lyapunov-Barrier Actor-Critic (NLBAC)
framework that leverages Neural Ordinary Differential Equations (NODEs) to
approximate system dynamics and integrates the Control Barrier Function (CBF)
and Control Lyapunov Function (CLF) frameworks with the actor-critic method to
assist in maintaining the safety and stability for the system. Within this
framework, we employ the augmented Lagrangian method to update the RL-based
controller parameters. Additionally, we introduce an extra backup controller in
situations where CBF constraints for safety and the CLF constraint for
stability cannot be satisfied simultaneously. Simulation results demonstrate
that the framework leads the system to approach the desired state and allows
fewer violations of safety constraints with better sample efficiency compared
to other methods.</div><div><a href='http://arxiv.org/abs/2401.13148v1'>2401.13148v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02290v1")'>Koopman-Assisted Reinforcement Learning</div>
<div id='2403.02290v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T18:19:48Z</div><div>Authors: Preston Rozwood, Edward Mehrez, Ludger Paehler, Wen Sun, Steven L. Brunton</div><div style='padding-top: 10px; width: 80ex'>The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman
(HJB) equation, are ubiquitous in reinforcement learning (RL) and control
theory. However, these equations quickly become intractable for systems with
high-dimensional states and nonlinearity. This paper explores the connection
between the data-driven Koopman operator and Markov Decision Processes (MDPs),
resulting in the development of two new RL algorithms to address these
limitations. We leverage Koopman operator techniques to lift a nonlinear system
into new coordinates where the dynamics become approximately linear, and where
HJB-based methods are more tractable. In particular, the Koopman operator is
able to capture the expectation of the time evolution of the value function of
a given system via linear dynamics in the lifted coordinates. By parameterizing
the Koopman operator with the control actions, we construct a ``Koopman
tensor'' that facilitates the estimation of the optimal value function. Then, a
transformation of Bellman's framework in terms of the Koopman tensor enables us
to reformulate two max-entropy RL algorithms: soft value iteration and soft
actor-critic (SAC). This highly flexible framework can be used for
deterministic or stochastic systems as well as for discrete or continuous-time
dynamics. Finally, we show that these Koopman Assisted Reinforcement Learning
(KARL) algorithms attain state-of-the-art (SOTA) performance with respect to
traditional neural network-based SAC and linear quadratic regulator (LQR)
baselines on four controlled dynamical systems: a linear state-space system,
the Lorenz system, fluid flow past a cylinder, and a double-well potential with
non-isotropic stochastic forcing.</div><div><a href='http://arxiv.org/abs/2403.02290v1'>2403.02290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12882v1")'>Model-Free $δ$-Policy Iteration Based on Damped Newton Method for
  Nonlinear Continuous-Time H$\infty$ Tracking Control</div>
<div id='2401.12882v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T16:22:50Z</div><div>Authors: Qi Wang</div><div style='padding-top: 10px; width: 80ex'>This paper presents a {\delta}-PI algorithm which is based on damped Newton
method for the H{\infty} tracking control problem of unknown continuous-time
nonlinear system. A discounted performance function and an augmented system are
used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation. Tracking HJI
equation is a nonlinear partial differential equation, traditional
reinforcement learning methods for solving the tracking HJI equation are mostly
based on the Newton method, which usually only satisfies local convergence and
needs a good initial guess. Based upon the damped Newton iteration operator
equation, a generalized tracking Bellman equation is derived firstly. The
{\delta}-PI algorithm can seek the optimal solution of the tracking HJI
equation by iteratively solving the generalized tracking Bellman equation.
On-policy learning and off-policy learning {\delta}-PI reinforcement learning
methods are provided, respectively. Off-policy version {\delta}-PI algorithm is
a model-free algorithm which can be performed without making use of a priori
knowledge of the system dynamics. NN-based implementation scheme for the
off-policy {\delta}-PI algorithms is shown. The suitability of the model-free
{\delta}-PI algorithm is illustrated with a nonlinear system simulation.</div><div><a href='http://arxiv.org/abs/2401.12882v1'>2401.12882v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17375v1")'>Impact of Computation in Integral Reinforcement Learning for
  Continuous-Time Control</div>
<div id='2402.17375v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T10:12:47Z</div><div>Authors: Wenhan Cao, Wei Pan</div><div style='padding-top: 10px; width: 80ex'>Integral reinforcement learning (IntRL) demands the precise computation of
the utility function's integral at its policy evaluation (PEV) stage. This is
achieved through quadrature rules, which are weighted sums of utility functions
evaluated from state samples obtained in discrete time. Our research reveals a
critical yet underexplored phenomenon: the choice of the computational method
-- in this case, the quadrature rule -- can significantly impact control
performance. This impact is traced back to the fact that computational errors
introduced in the PEV stage can affect the policy iteration's convergence
behavior, which in turn affects the learned controller. To elucidate how
computation impacts control, we draw a parallel between IntRL's policy
iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation.
In this light, computational error in PEV manifests as an extra error term in
each iteration of Newton's method, with its upper bound proportional to the
computational error. Further, we demonstrate that when the utility function
resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is
achievable by employing Bayesian quadrature with the RKHS-inducing kernel
function. We prove that the local convergence rates for IntRL using the
trapezoidal rule and Bayesian quadrature with a Mat\'ern kernel to be
$O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples
and $b$ is the Mat\'ern kernel's smoothness parameter. These theoretical
findings are finally validated by two canonical control tasks.</div><div><a href='http://arxiv.org/abs/2402.17375v1'>2402.17375v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14446v1")'>Model-Based Reinforcement Learning Control of Reaction-Diffusion
  Problems</div>
<div id='2402.14446v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T11:06:07Z</div><div>Authors: Christina Schenk, Aditya Vasudevan, Maciej Haranczyk, Ignacio Romero</div><div style='padding-top: 10px; width: 80ex'>Mathematical and computational tools have proven to be reliable in
decision-making processes. In recent times, in particular, machine
learning-based methods are becoming increasingly popular as advanced support
tools. When dealing with control problems, reinforcement learning has been
applied to decision-making in several applications, most notably in games. The
success of these methods in finding solutions to complex problems motivates the
exploration of new areas where they can be employed to overcome current
difficulties. In this paper, we explore the use of automatic control strategies
to initial boundary value problems in thermal and disease transport.
Specifically, in this work, we adapt an existing reinforcement learning
algorithm using a stochastic policy gradient method and we introduce two novel
reward functions to drive the flow of the transported field. The new
model-based framework exploits the interactions between a reaction-diffusion
model and the modified agent. The results show that certain controls can be
implemented successfully in these applications, although model simplifications
had to be assumed.</div><div><a href='http://arxiv.org/abs/2402.14446v1'>2402.14446v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16925v1")'>Minimize Control Inputs for Strong Structural Controllability Using
  Reinforcement Learning with Graph Neural Network</div>
<div id='2402.16925v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T11:18:53Z</div><div>Authors: Mengbang Zou, Weisi Guo, Bailu Jin</div><div style='padding-top: 10px; width: 80ex'>Strong structural controllability (SSC) guarantees networked system with
linear-invariant dynamics controllable for all numerical realizations of
parameters. Current research has established algebraic and graph-theoretic
conditions of SSC for zero/nonzero or zero/nonzero/arbitrary structure. One
relevant practical problem is how to fully control the system with the minimal
number of input signals and identify which nodes must be imposed signals.
Previous work shows that this optimization problem is NP-hard and it is
difficult to find the solution. To solve this problem, we formulate the graph
coloring process as a Markov decision process (MDP) according to the
graph-theoretical condition of SSC for both zero/nonzero and
zero/nonzero/arbitrary structure. We use Actor-critic method with Directed
graph neural network which represents the color information of graph to
optimize MDP. Our method is validated in a social influence network with real
data and different complex network models. We find that the number of input
nodes is determined by the average degree of the network and the input nodes
tend to select nodes with low in-degree and avoid high-degree nodes.</div><div><a href='http://arxiv.org/abs/2402.16925v1'>2402.16925v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03224v1")'>Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory</div>
<div id='2403.03224v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T16:46:15Z</div><div>Authors: Vedant Tapiavala, Joshua Piesner, Sourjyamoy Barman, Feng Fu</div><div style='padding-top: 10px; width: 80ex'>Live performances of music are always charming, with the unpredictability of
improvisation due to the dynamic between musicians and interactions with the
audience. Jazz improvisation is a particularly noteworthy example for further
investigation from a theoretical perspective. Here, we introduce a novel
mathematical game theory model for jazz improvisation, providing a framework
for studying music theory and improvisational methodologies. We use
computational modeling, mainly reinforcement learning, to explore diverse
stochastic improvisational strategies and their paired performance on
improvisation. We find that the most effective strategy pair is a strategy that
reacts to the most recent payoff (Stepwise Changes) with a reinforcement
learning strategy limited to notes in the given chord (Chord-Following
Reinforcement Learning). Conversely, a strategy that reacts to the partner's
last note and attempts to harmonize with it (Harmony Prediction) strategy pair
yields the lowest non-control payoff and highest standard deviation, indicating
that picking notes based on immediate reactions to the partner player can yield
inconsistent outcomes. On average, the Chord-Following Reinforcement Learning
strategy demonstrates the highest mean payoff, while Harmony Prediction
exhibits the lowest. Our work lays the foundation for promising applications
beyond jazz: including the use of artificial intelligence (AI) models to
extract data from audio clips to refine musical reward systems, and training
machine learning (ML) models on existing jazz solos to further refine
strategies within the game.</div><div><a href='http://arxiv.org/abs/2403.03224v1'>2403.03224v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05629v1")'>Learning Performance-Oriented Control Barrier Functions Under Complex
  Safety Constraints and Limited Actuation</div>
<div id='2401.05629v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T02:51:49Z</div><div>Authors: Shaoru Chen, Mahyar Fazlyab</div><div style='padding-top: 10px; width: 80ex'>Control Barrier Functions (CBFs) provide an elegant framework for designing
safety filters for nonlinear control systems by constraining their trajectories
to an invariant subset of a prespecified safe set. However, the task of finding
a CBF that concurrently maximizes the volume of the resulting control invariant
set while accommodating complex safety constraints, particularly in high
relative degree systems with actuation constraints, continues to pose a
substantial challenge. In this work, we propose a novel self-supervised
learning framework that holistically addresses these hurdles. Given a Boolean
composition of multiple state constraints that define the safe set, our
approach starts with building a single continuously differentiable function
whose 0-superlevel set provides an inner approximation of the safe set. We then
use this function together with a smooth neural network to parameterize the CBF
candidate. Finally, we design a training loss function based on a
Hamilton-Jacobi partial differential equation to train the CBF while enlarging
the volume of the induced control invariant set. We demonstrate the
effectiveness of our approach via numerical experiments.</div><div><a href='http://arxiv.org/abs/2401.05629v1'>2401.05629v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14907v1")'>Learning Local Control Barrier Functions for Safety Control of Hybrid
  Systems</div>
<div id='2401.14907v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T14:38:43Z</div><div>Authors: Shuo Yang, Yu Chen, Xiang Yin, Rahul Mangharam</div><div style='padding-top: 10px; width: 80ex'>Hybrid dynamical systems are ubiquitous as practical robotic applications
often involve both continuous states and discrete switchings. Safety is a
primary concern for hybrid robotic systems. Existing safety-critical control
approaches for hybrid systems are either computationally inefficient,
detrimental to system performance, or limited to small-scale systems. To amend
these drawbacks, in this paper, we propose a learningenabled approach to
construct local Control Barrier Functions (CBFs) to guarantee the safety of a
wide class of nonlinear hybrid dynamical systems. The end result is a safe
neural CBFbased switching controller. Our approach is computationally
efficient, minimally invasive to any reference controller, and applicable to
large-scale systems. We empirically evaluate our framework and demonstrate its
efficacy and flexibility through two robotic examples including a
high-dimensional autonomous racing case, against other CBF-based approaches and
model predictive control.</div><div><a href='http://arxiv.org/abs/2401.14907v1'>2401.14907v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18946v2")'>Real-Time Adaptive Safety-Critical Control with Gaussian Processes in
  High-Order Uncertain Models</div>
<div id='2402.18946v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T08:25:32Z</div><div>Authors: Yu Zhang, Long Wen, Xiangtong Yao, Zhenshan Bing, Linghuan Kong, Wei He, Alois Knoll</div><div style='padding-top: 10px; width: 80ex'>This paper presents an adaptive online learning framework for systems with
uncertain parameters to ensure safety-critical control in non-stationary
environments. Our approach consists of two phases. The initial phase is
centered on a novel sparse Gaussian process (GP) framework. We first integrate
a forgetting factor to refine a variational sparse GP algorithm, thus enhancing
its adaptability. Subsequently, the hyperparameters of the Gaussian model are
trained with a specially compound kernel, and the Gaussian model's online
inferential capability and computational efficiency are strengthened by
updating a solitary inducing point derived from new samples, in conjunction
with the learned hyperparameters. In the second phase, we propose a safety
filter based on high-order control barrier functions (HOCBFs), synergized with
the previously trained learning model. By leveraging the compound kernel from
the first phase, we effectively address the inherent limitations of GPs in
handling high-dimensional problems for real-time applications. The derived
controller ensures a rigorous lower bound on the probability of satisfying the
safety specification. Finally, the efficacy of our proposed algorithm is
demonstrated through real-time obstacle avoidance experiments executed using
both a simulation platform and a real-world 7-DOF robot.</div><div><a href='http://arxiv.org/abs/2402.18946v2'>2402.18946v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12948v1")'>On Safety in Safe Bayesian Optimization</div>
<div id='2403.12948v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:50:32Z</div><div>Authors: Christian Fiedler, Johanna Menn, Lukas Kreisköther, Sebastian Trimpe</div><div style='padding-top: 10px; width: 80ex'>Optimizing an unknown function under safety constraints is a central task in
robotics, biomedical engineering, and many other disciplines, and increasingly
safe Bayesian Optimization (BO) is used for this. Due to the safety critical
nature of these applications, it is of utmost importance that theoretical
safety guarantees for these algorithms translate into the real world. In this
work, we investigate three safety-related issues of the popular class of
SafeOpt-type algorithms. First, these algorithms critically rely on frequentist
uncertainty bounds for Gaussian Process (GP) regression, but concrete
implementations typically utilize heuristics that invalidate all safety
guarantees. We provide a detailed analysis of this problem and introduce
Real-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent
GP bounds and thus retains all theoretical guarantees. Second, we identify
assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of
the target function, a key technical assumption in SafeOpt-like algorithms, as
a central obstacle to real-world usage. To overcome this challenge, we
introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm,
which guarantees safety without an assumption on the RKHS bound, and
empirically show that this algorithm is not only safe, but also exhibits
superior performance compared to the state-of-the-art on several function
classes. Third, SafeOpt and derived algorithms rely on a discrete search space,
making them difficult to apply to higher-dimensional problems. To widen the
applicability of these algorithms, we introduce Lipschitz-only GP-UCB
(LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional
problems, while retaining safety.</div><div><a href='http://arxiv.org/abs/2403.12948v1'>2403.12948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04629v2")'>Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI
  Collaboration</div>
<div id='2403.04629v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T16:13:32Z</div><div>Authors: Julian Rodemann, Federico Croppi, Philipp Arens, Yusuf Sale, Julia Herbinger, Bernd Bischl, Eyke Hüllermeier, Thomas Augustin, Conor J. Walsh, Giuseppe Casalicchio</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimization (BO) with Gaussian processes (GP) has become an
indispensable algorithm for black box optimization problems. Not without a dash
of irony, BO is often considered a black box itself, lacking ways to provide
reasons as to why certain parameters are proposed to be evaluated. This is
particularly relevant in human-in-the-loop applications of BO, such as in
robotics. We address this issue by proposing ShapleyBO, a framework for
interpreting BO's proposals by game-theoretic Shapley values.They quantify each
parameter's contribution to BO's acquisition function. Exploiting the linearity
of Shapley values, we are further able to identify how strongly each parameter
drives BO's exploration and exploitation for additive acquisition functions
like the confidence bound. We also show that ShapleyBO can disentangle the
contributions to exploration into those that explore aleatoric and epistemic
uncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human
machine interface (HMI), allowing users to interfere with BO in case proposals
do not align with human reasoning. We demonstrate this HMI's benefits for the
use case of personalizing wearable robotic devices (assistive back exosuits) by
human-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO
can achieve lower regret than teams without.</div><div><a href='http://arxiv.org/abs/2403.04629v2'>2403.04629v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04442v1")'>Cooperative Bayesian Optimization for Imperfect Agents</div>
<div id='2403.04442v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T12:16:51Z</div><div>Authors: Ali Khoshvishkaie, Petrus Mikkola, Pierre-Alexandre Murena, Samuel Kaski</div><div style='padding-top: 10px; width: 80ex'>We introduce a cooperative Bayesian optimization problem for optimizing
black-box functions of two variables where two agents choose together at which
points to query the function but have only control over one variable each. This
setting is inspired by human-AI teamwork, where an AI-assistant helps its human
user solve a problem, in this simplest case, collaborative optimization. We
formulate the solution as sequential decision-making, where the agent we
control models the user as a computationally rational agent with prior
knowledge about the function. We show that strategic planning of the queries
enables better identification of the global maximum of the function as long as
the user avoids excessive exploration. This planning is made possible by using
Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model
that accounts for conservative belief updates and exploratory sampling of the
points to query.</div><div><a href='http://arxiv.org/abs/2403.04442v1'>2403.04442v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13334v1")'>Explainable Bayesian Optimization</div>
<div id='2401.13334v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T09:59:22Z</div><div>Authors: Tanmay Chakraborty, Christin Seifert, Christian Wirth</div><div style='padding-top: 10px; width: 80ex'>In industry, Bayesian optimization (BO) is widely applied in the human-AI
collaborative parameter tuning of cyber-physical systems. However, BO's
solutions may deviate from human experts' actual goal due to approximation
errors and simplified objectives, requiring subsequent tuning. The black-box
nature of BO limits the collaborative tuning process because the expert does
not trust the BO recommendations. Current explainable AI (XAI) methods are not
tailored for optimization and thus fall short of addressing this gap. To bridge
this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based
explainability method that produces high quality explanations through
multiobjective optimization. Our evaluation of benchmark optimization problems
and real-world hyperparameter optimization tasks demonstrates TNTRules'
superiority over state-of-the-art XAI methods in generating high quality
explanations. This work contributes to the intersection of BO and XAI,
providing interpretable optimization techniques for real-world applications.</div><div><a href='http://arxiv.org/abs/2401.13334v1'>2401.13334v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14488v1")'>Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action
  Selection in Robot Manipulation Tasks</div>
<div id='2403.14488v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T15:36:26Z</div><div>Authors: Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze</div><div style='padding-top: 10px; width: 80ex'>Safe and efficient object manipulation is a key enabler of many real-world
robot applications. However, this is challenging because robot operation must
be robust to a range of sensor and actuator uncertainties. In this paper, we
present a physics-informed causal-inference-based framework for a robot to
probabilistically reason about candidate actions in a block stacking task in a
partially observable setting. We integrate a physics-based simulation of the
rigid-body system dynamics with a causal Bayesian network (CBN) formulation to
define a causal generative probabilistic model of the robot decision-making
process. Using simulation-based Monte Carlo experiments, we demonstrate our
framework's ability to successfully: (1) predict block tower stability with
high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best
action for the block stacking task, for execution by an integrated robot
system, achieving 94.2% task success rate. We also demonstrate our framework's
suitability for real-world robot systems by demonstrating successful task
executions with a domestic support robot, with perception and manipulation
sub-system integration. Hence, we show that by embedding physics-based causal
reasoning into robots' decision-making processes, we can make robot task
execution safer, more reliable, and more robust to various types of
uncertainty.</div><div><a href='http://arxiv.org/abs/2403.14488v1'>2403.14488v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13869v2")'>Accurately Predicting Probabilities of Safety-Critical Rare Events for
  Intelligent Systems</div>
<div id='2403.13869v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T14:00:29Z</div><div>Authors: Ruoxuan Bai, Jingxuan Yang, Weiduo Gong, Yi Zhang, Qiujing Lu, Shuo Feng</div><div style='padding-top: 10px; width: 80ex'>Intelligent systems are increasingly integral to our daily lives, yet rare
safety-critical events present significant latent threats to their practical
deployment. Addressing this challenge hinges on accurately predicting the
probability of safety-critical events occurring within a given time step from
the current state, a metric we define as 'criticality'. The complexity of
predicting criticality arises from the extreme data imbalance caused by rare
events in high dimensional variables associated with the rare events, a
challenge we refer to as the curse of rarity. Existing methods tend to be
either overly conservative or prone to overlooking safety-critical events, thus
struggling to achieve both high precision and recall rates, which severely
limits their applicability. This study endeavors to develop a criticality
prediction model that excels in both precision and recall rates for evaluating
the criticality of safety-critical autonomous systems. We propose a multi-stage
learning framework designed to progressively densify the dataset, mitigating
the curse of rarity across stages. To validate our approach, we evaluate it in
two cases: lunar lander and bipedal walker scenarios. The results demonstrate
that our method surpasses traditional approaches, providing a more accurate and
dependable assessment of criticality in intelligent systems.</div><div><a href='http://arxiv.org/abs/2403.13869v2'>2403.13869v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09491v1")'>On using Machine Learning Algorithms for Motorcycle Collision Detection</div>
<div id='2403.09491v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:32:25Z</div><div>Authors: Philipp Rodegast, Steffen Maier, Jonas Kneifl, Jörg Fehr</div><div style='padding-top: 10px; width: 80ex'>Globally, motorcycles attract vast and varied users. However, since the rate
of severe injury and fatality in motorcycle accidents far exceeds passenger car
accidents, efforts have been directed toward increasing passive safety systems.
Impact simulations show that the risk of severe injury or death in the event of
a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped
with passive safety measures such as airbags and seat belts. For the passive
safety systems to be activated, a collision must be detected within
milliseconds for a wide variety of impact configurations, but under no
circumstances may it be falsely triggered. For the challenge of reliably
detecting impending collisions, this paper presents an investigation towards
the applicability of machine learning algorithms. First, a series of
simulations of accidents and driving operation is introduced to collect data to
train machine learning classification models. Their performance is henceforth
assessed and compared via multiple representative and application-oriented
criteria.</div><div><a href='http://arxiv.org/abs/2403.09491v1'>2403.09491v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02980v1")'>Review on Fault Diagnosis and Fault-Tolerant Control Scheme for Robotic
  Manipulators: Recent Advances in AI, Machine Learning, and Digital Twin</div>
<div id='2402.02980v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:12:33Z</div><div>Authors: Md Muzakkir Quamar, Ali Nasir</div><div style='padding-top: 10px; width: 80ex'>This comprehensive review article delves into the intricate realm of
fault-tolerant control (FTC) schemes tailored for robotic manipulators. Our
exploration spans the historical evolution of FTC, tracing its development over
time, and meticulously examines the recent breakthroughs fueled by the
synergistic integration of cutting-edge technologies such as artificial
intelligence (AI), machine learning (ML), and digital twin technologies (DTT).
The article places a particular emphasis on the transformative influence these
contemporary trends exert on the landscape of robotic manipulator control and
fault tolerance.
  By delving into the historical context, our aim is to provide a comprehensive
understanding of the evolution of FTC schemes. This journey encompasses the
transition from model-based and signal-based schemes to the role of sensors,
setting the stage for an exploration of the present-day paradigm shift enabled
by AI, ML, and DTT. The narrative unfolds as we dissect the intricate interplay
between these advanced technologies and their applications in enhancing fault
tolerance within the domain of robotic manipulators. Our review critically
evaluates the impact of these advancements, shedding light on the novel
methodologies, techniques, and applications that have emerged in recent times.
  The overarching goal of this article is to present a comprehensive
perspective on the current state of fault diagnosis and fault-tolerant control
within the context of robotic manipulators, positioning our exploration within
the broader framework of AI, ML, and DTT advancements. Through a meticulous
examination of both historical foundations and contemporary innovations, this
review significantly contributes to the existing body of knowledge, offering
valuable insights for researchers, practitioners, and enthusiasts navigating
the dynamic landscape of robotic manipulator control.</div><div><a href='http://arxiv.org/abs/2402.02980v1'>2402.02980v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00393v1")'>Loss Function Considering Dead Zone for Neural Networks</div>
<div id='2402.00393v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T07:28:55Z</div><div>Authors: Koki Inami, Koki Yamane, Sho Sakaino</div><div style='padding-top: 10px; width: 80ex'>It is important to reveal the inverse dynamics of manipulators to improve
control performance of model-based control. Neural networks (NNs) are promising
techniques to represent complicated inverse dynamics while they require a large
amount of motion data. However, motion data in dead zones of actuators is not
suitable for training models decreasing the number of useful training data. In
this study, based on the fact that the manipulator joint does not work
irrespective of input torque in dead zones, we propose a new loss function that
considers only errors of joints not in dead zones. The proposed method enables
to increase in the amount of motion data available for training and the
accuracy of the inverse dynamics computation. Experiments on actual equipment
using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than
conventional methods. We also confirmed and discussed the behavior of the model
of the proposed method in dead zones.</div><div><a href='http://arxiv.org/abs/2402.00393v1'>2402.00393v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03563v1")'>Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip
  Perception of Mobile Manipulation Robots</div>
<div id='2403.03563v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T09:15:53Z</div><div>Authors: Youngjae Yoo, Chung-Yeon Lee, Byoung-Tak Zhang</div><div style='padding-top: 10px; width: 80ex'>Object slip perception is essential for mobile manipulation robots to perform
manipulation tasks reliably in the dynamic real-world. Traditional approaches
to robot arms' slip perception use tactile or vision sensors. However, mobile
robots still have to deal with noise in their sensor signals caused by the
robot's movement in a changing environment. To solve this problem, we present
an anomaly detection method that utilizes multisensory data based on a deep
autoencoder model. The proposed framework integrates heterogeneous data streams
collected from various robot sensors, including RGB and depth cameras, a
microphone, and a force-torque sensor. The integrated data is used to train a
deep autoencoder to construct latent representations of the multisensory data
that indicate the normal status. Anomalies can then be identified by error
scores measured by the difference between the trained encoder's latent values
and the latent values of reconstructed input data. In order to evaluate the
proposed framework, we conducted an experiment that mimics an object slip by a
mobile service robot operating in a real-world environment with diverse
household objects and different moving patterns. The experimental results
verified that the proposed framework reliably detects anomalies in object slip
situations despite various object types and robot behaviors, and visual and
auditory noise in the environment.</div><div><a href='http://arxiv.org/abs/2403.03563v1'>2403.03563v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07216v1")'>Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter
  Control</div>
<div id='2403.07216v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T00:08:54Z</div><div>Authors: Mike Timmerman, Aryan Patel, Tim Reinhart</div><div style='padding-top: 10px; width: 80ex'>The paper presents a technique using reinforcement learning (RL) to adapt the
control gains of a quadcopter controller. Specifically, we employed Proximal
Policy Optimization (PPO) to train a policy which adapts the gains of a
cascaded feedback controller in-flight. The primary goal of this controller is
to minimize tracking error while following a specified trajectory. The paper's
key objective is to analyze the effectiveness of the adaptive gain policy and
compare it to the performance of a static gain control algorithm, where the
Integral Squared Error and Integral Time Squared Error are used as metrics. The
results show that the adaptive gain scheme achieves over 40$\%$ decrease in
tracking error as compared to the static gain controller.</div><div><a href='http://arxiv.org/abs/2403.07216v1'>2403.07216v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04960v1")'>Why Change Your Controller When You Can Change Your Planner: Drag-Aware
  Trajectory Generation for Quadrotor Systems</div>
<div id='2401.04960v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T07:00:07Z</div><div>Authors: Hanli Zhang, Anusha Srikanthan, Spencer Folk, Vijay Kumar, Nikolai Matni</div><div style='padding-top: 10px; width: 80ex'>Motivated by the increasing use of quadrotors for payload delivery, we
consider a joint trajectory generation and feedback control design problem for
a quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag
forces from carried payloads can lead to catastrophic outcomes. Prior work
model aerodynamic effects as residual dynamics or external disturbances in the
control problem leading to a reactive policy that could be catastrophic.
Moreover, redesigning controllers and tuning control gains on hardware
platforms is a laborious effort. In this paper, we argue that adapting the
trajectory generation component keeping the controller fixed can improve
trajectory tracking for quadrotor systems experiencing drag forces. To achieve
this, we formulate a drag-aware planning problem by applying a suitable
relaxation to an optimal quadrotor control problem, introducing a tracking cost
function which measures the ability of a controller to follow a reference
trajectory. This tracking cost function acts as a regularizer in trajectory
generation and is learned from data obtained from simulation. Our experiments
in both simulation and on the Crazyflie hardware platform show that changing
the planner reduces tracking error by as much as 83%. Evaluation on hardware
demonstrates that our planned path, as opposed to a baseline, avoids controller
saturation and catastrophic outcomes during aggressive maneuvers.</div><div><a href='http://arxiv.org/abs/2401.04960v1'>2401.04960v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00128v1")'>From Flies to Robots: Inverted Landing in Small Quadcopters with Dynamic
  Perching</div>
<div id='2403.00128v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T21:09:08Z</div><div>Authors: Bryan Habas, Bo Cheng</div><div style='padding-top: 10px; width: 80ex'>Inverted landing is a routine behavior among a number of animal fliers.
However, mastering this feat poses a considerable challenge for robotic fliers,
especially to perform dynamic perching with rapid body rotations (or flips) and
landing against gravity. Inverted landing in flies have suggested that optical
flow senses are closely linked to the precise triggering and control of body
flips that lead to a variety of successful landing behaviors. Building upon
this knowledge, we aimed to replicate the flies' landing behaviors in small
quadcopters by developing a control policy general to arbitrary
ceiling-approach conditions. First, we employed reinforcement learning in
simulation to optimize discrete sensory-motor pairs across a broad spectrum of
ceiling-approach velocities and directions. Next, we converted the
sensory-motor pairs to a two-stage control policy in a continuous
augmented-optical flow space. The control policy consists of a first-stage
Flip-Trigger Policy, which employs a one-class support vector machine, and a
second-stage Flip-Action Policy, implemented as a feed-forward neural network.
To transfer the inverted-landing policy to physical systems, we utilized domain
randomization and system identification techniques for a zero-shot sim-to-real
transfer. As a result, we successfully achieved a range of robust
inverted-landing behaviors in small quadcopters, emulating those observed in
flies.</div><div><a href='http://arxiv.org/abs/2403.00128v1'>2403.00128v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01062v1")'>Bio-Inspired Compensatory Strategies for Damage to Flapping Robotic
  Propulsors</div>
<div id='2402.01062v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T23:26:16Z</div><div>Authors: Meredith L. Hooper, Isabel Scherl, Morteza Gharib</div><div style='padding-top: 10px; width: 80ex'>To maintain full autonomy, autonomous robotic systems must have the ability
to self-repair. Self-repairing via compensatory mechanisms appears in nature:
for example, some fish can lose even 76% of their propulsive surface without
loss of thrust by altering stroke mechanics. However, direct transference of
these alterations from an organism to a robotic flapping propulsor may not be
optimal due to irrelevant evolutionary pressures. We instead seek to determine
what alterations to stroke mechanics are optimal for a damaged robotic system
via artificial evolution. To determine whether natural and machine-learned
optima differ, we employ a cyber-physical system using a Covariance Matrix
Adaptation Evolutionary Strategy to seek the most efficient trajectory for a
given force. We implement an online optimization with hardware-in-the-loop,
performing experimental function evaluations with an actuated flexible flat
plate. To recoup thrust production following partial amputation, the most
efficient learned strategy was to increase amplitude, increase frequency,
increase the amplitude of angle of attack, and phase shift the angle of attack
by approximately 110 degrees. In fish, only an amplitude increase is reported
by majority in the literature. To recoup side-force production, a more
challenging optimization landscape is encountered. Nesting of optimal angle of
attack traces is found in the resultant-based reference frame, but no clear
trend in amplitude or frequency are exhibited -- in contrast to the increase in
frequency reported in insect literature. These results suggest that how
mechanical flapping propulsors most efficiently adjust to damage of a flapping
propulsor may not align with natural swimmers and flyers.</div><div><a href='http://arxiv.org/abs/2402.01062v1'>2402.01062v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09075v1")'>Steady-State Error Compensation for Reinforcement Learning with
  Quadratic Rewards</div>
<div id='2402.09075v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:35:26Z</div><div>Authors: Liyao Wang, Zishun Zheng, Yuan Lin</div><div style='padding-top: 10px; width: 80ex'>The selection of a reward function in Reinforcement Learning (RL) has
garnered significant attention because of its impact on system performance.
Issues of steady-state error often manifest when quadratic reward functions are
employed. Although existing solutions using absolute-value-type reward
functions partially address this problem, they tend to induce substantial
fluctuations in specific system states, leading to abrupt changes. In response
to this challenge, this study proposes an approach that introduces an integral
term. By integrating this term into quadratic-type reward functions, the RL
algorithm is adeptly tuned, augmenting the system's consideration of long-term
rewards and, consequently, alleviating concerns related to steady-state errors.
Through experiments and performance evaluations on the Adaptive Cruise Control
(ACC) model and lane change models, we validate that the proposed method not
only effectively diminishes steady-state errors but also results in smoother
variations in system states.</div><div><a href='http://arxiv.org/abs/2402.09075v1'>2402.09075v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09930v1")'>Quality-Diversity Actor-Critic: Learning High-Performing and Diverse
  Behaviors via Value and Successor Features Critics</div>
<div id='2403.09930v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T00:09:47Z</div><div>Authors: Luca Grillotti, Maxence Faldor, Borja G. León, Antoine Cully</div><div style='padding-top: 10px; width: 80ex'>A key aspect of intelligence is the ability to demonstrate a broad spectrum
of behaviors for adapting to unexpected situations. Over the past decade,
advancements in deep reinforcement learning have led to groundbreaking
achievements to solve complex continuous control tasks. However, most
approaches return only one solution specialized for a specific problem. We
introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic
deep reinforcement learning algorithm that leverages a value function critic
and a successor features critic to learn high-performing and diverse behaviors.
In this framework, the actor optimizes an objective that seamlessly unifies
both critics using constrained optimization to (1) maximize return, while (2)
executing diverse skills. Compared with other Quality-Diversity methods, QDAC
achieves significantly higher performance and more diverse behaviors on six
challenging continuous control locomotion tasks. We also demonstrate that we
can harness the learned skills to adapt better than other baselines to five
perturbed environments. Finally, qualitative analyses showcase a range of
remarkable behaviors, available at: http://bit.ly/qdac.</div><div><a href='http://arxiv.org/abs/2403.09930v1'>2403.09930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00991v1")'>SELFI: Autonomous Self-Improvement with Reinforcement Learning for
  Social Navigation</div>
<div id='2403.00991v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T21:27:03Z</div><div>Authors: Noriaki Hirose, Dhruv Shah, Kyle Stachowicz, Ajay Sridhar, Sergey Levine</div><div style='padding-top: 10px; width: 80ex'>Autonomous self-improving robots that interact and improve with experience
are key to the real-world deployment of robotic systems. In this paper, we
propose an online learning method, SELFI, that leverages online robot
experience to rapidly fine-tune pre-trained control policies efficiently. SELFI
applies online model-free reinforcement learning on top of offline model-based
learning to bring out the best parts of both learning paradigms. Specifically,
SELFI stabilizes the online learning process by incorporating the same
model-based learning objective from offline pre-training into the Q-values
learned with online model-free reinforcement learning. We evaluate SELFI in
multiple real-world environments and report improvements in terms of collision
avoidance, as well as more socially compliant behavior, measured by a human
user study. SELFI enables us to quickly learn useful robotic behaviors with
less human interventions such as pre-emptive behavior for the pedestrians,
collision avoidance for small and transparent objects, and avoiding travel on
uneven floor surfaces. We provide supplementary videos to demonstrate the
performance of our fine-tuned policy on our project page.</div><div><a href='http://arxiv.org/abs/2403.00991v1'>2403.00991v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09793v1")'>Socially Integrated Navigation: A Social Acting Robot with Deep
  Reinforcement Learning</div>
<div id='2403.09793v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T18:25:40Z</div><div>Authors: Daniel Flögel, Lars Fischer, Thomas Rudolf, Tobias Schürmann, Sören Hohmann</div><div style='padding-top: 10px; width: 80ex'>Mobile robots are being used on a large scale in various crowded situations
and become part of our society. The socially acceptable navigation behavior of
a mobile robot with individual human consideration is an essential requirement
for scalable applications and human acceptance. Deep Reinforcement Learning
(DRL) approaches are recently used to learn a robot's navigation policy and to
model the complex interactions between robots and humans. We propose to divide
existing DRL-based navigation approaches based on the robot's exhibited social
behavior and distinguish between social collision avoidance with a lack of
social behavior and socially aware approaches with explicit predefined social
behavior. In addition, we propose a novel socially integrated navigation
approach where the robot's social behavior is adaptive and emerges from the
interaction with humans. The formulation of our approach is derived from a
sociological definition, which states that social acting is oriented toward the
acting of others. The DRL policy is trained in an environment where other
agents interact socially integrated and reward the robot's behavior
individually. The simulation results indicate that the proposed socially
integrated navigation approach outperforms a socially aware approach in terms
of distance traveled, time to completion, and negative impact on all agents
within the environment.</div><div><a href='http://arxiv.org/abs/2403.09793v1'>2403.09793v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12275v1")'>Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation</div>
<div id='2401.12275v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:58:22Z</div><div>Authors: Jiachen Li, Chuanbo Hua, Hengbo Ma, Jinkyoo Park, Victoria Dax, Mykel J. Kochenderfer</div><div style='padding-top: 10px; width: 80ex'>Social robot navigation can be helpful in various contexts of daily life but
requires safe human-robot interactions and efficient trajectory planning. While
modeling pairwise relations has been widely studied in multi-agent interacting
systems, the ability to capture larger-scale group-wise activities is limited.
In this paper, we propose a systematic relational reasoning approach with
explicit inference of the underlying dynamically evolving relational
structures, and we demonstrate its effectiveness for multi-agent trajectory
prediction and social robot navigation. In addition to the edges between pairs
of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect
multiple nodes to enable group-wise reasoning in an unsupervised manner. Our
approach infers dynamically evolving relation graphs and hypergraphs to capture
the evolution of relations, which the trajectory predictor employs to generate
future states. Meanwhile, we propose to regularize the sharpness and sparsity
of the learned relations and the smoothness of the relation evolution, which
proves to enhance training stability and model performance. The proposed
approach is validated on synthetic crowd simulations and real-world benchmark
datasets. Experiments demonstrate that the approach infers reasonable relations
and achieves state-of-the-art prediction performance. In addition, we present a
deep reinforcement learning (DRL) framework for social robot navigation, which
incorporates relational reasoning and trajectory prediction systematically. In
a group-based crowd simulation, our method outperforms the strongest baseline
by a significant margin in terms of safety, efficiency, and social compliance
in dense, interactive scenarios.</div><div><a href='http://arxiv.org/abs/2401.12275v1'>2401.12275v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06344v1")'>Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for
  Human Trajectory Prediction with Hypergraph Reasoning</div>
<div id='2401.06344v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T03:26:06Z</div><div>Authors: Weizheng Wang, Le Mao, Baijian Yang, Guohua Chen, Byung-Cheol Min</div><div style='padding-top: 10px; width: 80ex'>Predicting crowded intents and trajectories is crucial in varouls real-world
applications, including service robots and autonomous vehicles. Understanding
environmental dynamics is challenging, not only due to the complexities of
modeling pair-wise spatial and temporal interactions but also the diverse
influence of group-wise interactions. To decode the comprehensive pair-wise and
group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a
Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory
prediction. In Hyper-STTN, crowded group-wise correlations are constructed
using a set of multi-scale hypergraphs with varying group sizes, captured
through random-walk robability-based hypergraph spectral convolution.
Additionally, a spatial-temporal transformer is adapted to capture pedestrians'
pair-wise latent interactions in spatial-temporal dimensions. These
heterogeneous group-wise and pair-wise are then fused and aligned though a
multimodal transformer network. Hyper-STTN outperformes other state-of-the-art
baselines and ablation models on 5 real-world pedestrian motion datasets.</div><div><a href='http://arxiv.org/abs/2401.06344v1'>2401.06344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07019v1")'>Informativeness of Reward Functions in Reinforcement Learning</div>
<div id='2402.07019v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T18:36:42Z</div><div>Authors: Rati Devidze, Parameswaran Kamalaruban, Adish Singla</div><div style='padding-top: 10px; width: 80ex'>Reward functions are central in specifying the task we want a reinforcement
learning agent to perform. Given a task and desired optimal behavior, we study
the problem of designing informative reward functions so that the designed
rewards speed up the agent's convergence. In particular, we consider
expert-driven reward design settings where an expert or teacher seeks to
provide informative and interpretable rewards to a learning agent. Existing
works have considered several different reward design formulations; however,
the key challenge is formulating a reward informativeness criterion that adapts
w.r.t. the agent's current policy and can be optimized under specified
structural constraints to obtain interpretable rewards. In this paper, we
propose a novel reward informativeness criterion, a quantitative measure that
captures how the agent's current policy will improve if it receives rewards
from a specific reward function. We theoretically showcase the utility of the
proposed informativeness criterion for adaptively designing rewards for an
agent. Experimental results on two navigation tasks demonstrate the
effectiveness of our adaptive reward informativeness criterion.</div><div><a href='http://arxiv.org/abs/2402.07019v1'>2402.07019v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10105v1")'>Belief Aided Navigation using Bayesian Reinforcement Learning for
  Avoiding Humans in Blind Spots</div>
<div id='2403.10105v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T08:50:39Z</div><div>Authors: Jinyeob Kim, Daewon Kwak, Hyunwoo Rim, Donghan Kim</div><div style='padding-top: 10px; width: 80ex'>Recent research on mobile robot navigation has focused on socially aware
navigation in crowded environments. However, existing methods do not adequately
account for human robot interactions and demand accurate location information
from omnidirectional sensors, rendering them unsuitable for practical
applications. In response to this need, this study introduces a novel
algorithm, BNBRL+, predicated on the partially observable Markov decision
process framework to assess risks in unobservable areas and formulate movement
strategies under uncertainty. BNBRL+ consolidates belief algorithms with
Bayesian neural networks to probabilistically infer beliefs based on the
positional data of humans. It further integrates the dynamics between the
robot, humans, and inferred beliefs to determine the navigation paths and
embeds social norms within the reward function, thereby facilitating socially
aware navigation. Through experiments in various risk laden scenarios, this
study validates the effectiveness of BNBRL+ in navigating crowded environments
with blind spots. The model's ability to navigate effectively in spaces with
limited visibility and avoid obstacles dynamically can significantly improve
the safety and reliability of autonomous vehicles.</div><div><a href='http://arxiv.org/abs/2403.10105v1'>2403.10105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12527v1")'>The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning</div>
<div id='2402.12527v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:38:00Z</div><div>Authors: Anya Sims, Cong Lu, Yee Whye Teh</div><div style='padding-top: 10px; width: 80ex'>Offline reinforcement learning aims to enable agents to be trained from
pre-collected datasets, however, this comes with the added challenge of
estimating the value of behavior not covered in the dataset. Model-based
methods offer a solution by allowing agents to collect additional synthetic
data via rollouts in a learned dynamics model. The prevailing theoretical
understanding is that this can then be viewed as online reinforcement learning
in an approximate dynamics model, and any remaining gap is therefore assumed to
be due to the imperfect dynamics model. Surprisingly, however, we find that if
the learned dynamics model is replaced by the true error-free dynamics,
existing model-based methods completely fail. This reveals a major
misconception. Our subsequent investigation finds that the general procedure
used in model-based algorithms results in the existence of a set of
edge-of-reach states which trigger pathological value overestimation and
collapse in Bellman-based algorithms. We term this the edge-of-reach problem.
Based on this, we fill some gaps in existing theory and also explain how prior
model-based methods are inadvertently addressing the true underlying
edge-of-reach problem. Finally, we propose Reach-Aware Value Learning (RAVL), a
simple and robust method that directly addresses the edge-of-reach problem and
achieves strong performance across both proprioceptive and pixel-based
benchmarks. Code open-sourced at: https://github.com/anyasims/edge-of-reach.</div><div><a href='http://arxiv.org/abs/2402.12527v1'>2402.12527v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03661v1")'>Transductive Reward Inference on Graph</div>
<div id='2402.03661v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T03:31:28Z</div><div>Authors: Bohao Qu, Xiaofeng Cao, Qing Guo, Yi Chang, Ivor W. Tsang, Chengqi Zhang</div><div style='padding-top: 10px; width: 80ex'>In this study, we present a transductive inference approach on that reward
information propagation graph, which enables the effective estimation of
rewards for unlabelled data in offline reinforcement learning. Reward inference
is the key to learning effective policies in practical scenarios, while direct
environmental interactions are either too costly or unethical and the reward
functions are rarely accessible, such as in healthcare and robotics. Our
research focuses on developing a reward inference method based on the
contextual properties of information propagation on graphs that capitalizes on
a constrained number of human reward annotations to infer rewards for
unlabelled data. We leverage both the available data and limited reward
annotations to construct a reward propagation graph, wherein the edge weights
incorporate various influential factors pertaining to the rewards.
Subsequently, we employ the constructed graph for transductive reward
inference, thereby estimating rewards for unlabelled data. Furthermore, we
establish the existence of a fixed point during several iterations of the
transductive inference process and demonstrate its at least convergence to a
local optimum. Empirical evaluations on locomotion and robotic manipulation
tasks validate the effectiveness of our approach. The application of our
inferred rewards improves the performance in offline reinforcement learning
tasks.</div><div><a href='http://arxiv.org/abs/2402.03661v1'>2402.03661v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00823v2")'>SLIM: Skill Learning with Multiple Critics</div>
<div id='2402.00823v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:07:33Z</div><div>Authors: David Emukpere, Bingbing Wu, Julien Perez, Jean-Michel Renders</div><div style='padding-top: 10px; width: 80ex'>Self-supervised skill learning aims to acquire useful behaviors that leverage
the underlying dynamics of the environment. Latent variable models, based on
mutual information maximization, have been successful in this task but still
struggle in the context of robotic manipulation. As it requires impacting a
possibly large set of degrees of freedom composing the environment, mutual
information maximization fails alone in producing useful and safe manipulation
behaviors. Furthermore, tackling this by augmenting skill discovery rewards
with additional rewards through a naive combination might fail to produce
desired behaviors. To address this limitation, we introduce SLIM, a
multi-critic learning approach for skill discovery with a particular focus on
robotic manipulation. Our main insight is that utilizing multiple critics in an
actor-critic framework to gracefully combine multiple reward functions leads to
a significant improvement in latent-variable skill discovery for robotic
manipulation while overcoming possible interference occurring among rewards
which hinders convergence to useful skills. Furthermore, in the context of
tabletop manipulation, we demonstrate the applicability of our novel skill
discovery approach to acquire safe and efficient motor primitives in a
hierarchical reinforcement learning fashion and leverage them through planning,
significantly surpassing baseline approaches for skill discovery.</div><div><a href='http://arxiv.org/abs/2402.00823v2'>2402.00823v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14488v1")'>Scilab-RL: A software framework for efficient reinforcement learning and
  cognitive modeling research</div>
<div id='2401.14488v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T19:49:02Z</div><div>Authors: Jan Dohmen, Frank Röder, Manfred Eppe</div><div style='padding-top: 10px; width: 80ex'>One problem with researching cognitive modeling and reinforcement learning
(RL) is that researchers spend too much time on setting up an appropriate
computational framework for their experiments. Many open source implementations
of current RL algorithms exist, but there is a lack of a modular suite of tools
combining different robotic simulators and platforms, data visualization,
hyperparameter optimization, and baseline experiments. To address this problem,
we present Scilab-RL, a software framework for efficient research in cognitive
modeling and reinforcement learning for robotic agents. The framework focuses
on goal-conditioned reinforcement learning using Stable Baselines 3 and the
OpenAI gym interface. It enables native possibilities for experiment
visualizations and hyperparameter optimization. We describe how these features
enable researchers to conduct experiments with minimal time effort, thus
maximizing research output.</div><div><a href='http://arxiv.org/abs/2401.14488v1'>2401.14488v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08242v1")'>Towards Equitable Agile Research and Development of AI and Robotics</div>
<div id='2402.08242v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T06:13:17Z</div><div>Authors: Andrew Hundt, Julia Schuller, Severin Kacianka</div><div style='padding-top: 10px; width: 80ex'>Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to
replicate and amplify existing biases and prejudices, as do Robots with AI. For
example, robots with facial recognition have failed to identify Black Women as
human, while others have categorized people, such as Black Men, as criminals
based on appearance alone. A 'culture of modularity' means harms are perceived
as 'out of scope', or someone else's responsibility, throughout employment
positions in the 'AI supply chain'. Incidents are routine enough
(incidentdatabase.ai lists over 2000 examples) to indicate that few
organizations are capable of completely respecting peoples' rights; meeting
claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and
then addressing such failures in their organizations and artifacts. We propose
a framework for adapting widely practiced Research and Development (R&amp;D)
project management methodologies to build organizational equity capabilities
and better integrate known evidence-based best practices. We describe how
project teams can organize and operationalize the most promising practices,
skill sets, organizational cultures, and methods to detect and address
rights-based fairness, equity, accountability, and ethical problems as early as
possible when they are often less harmful and easier to mitigate; then monitor
for unforeseen incidents to adaptively and constructively address them. Our
primary example adapts an Agile development process based on Scrum, one of the
most widely adopted approaches to organizing R&amp;D teams. We also discuss
limitations of our proposed framework and future research directions.</div><div><a href='http://arxiv.org/abs/2402.08242v1'>2402.08242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00470v1")'>Autonomous Robotic Arm Manipulation for Planetary Missions using Causal
  Machine Learning</div>
<div id='2403.00470v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T11:54:25Z</div><div>Authors: C. McDonnell, M. Arana-Catania, S. Upadhyay</div><div style='padding-top: 10px; width: 80ex'>Autonomous robotic arm manipulators have the potential to make planetary
exploration and in-situ resource utilization missions more time efficient and
productive, as the manipulator can handle the objects itself and perform
goal-specific actions. We train a manipulator to autonomously study objects of
which it has no prior knowledge, such as planetary rocks. This is achieved
using causal machine learning in a simulated planetary environment. Here, the
manipulator interacts with objects, and classifies them based on differing
causal factors. These are parameters, such as mass or friction coefficient,
that causally determine the outcomes of its interactions. Through reinforcement
learning, the manipulator learns to interact in ways that reveal the underlying
causal factors. We show that this method works even without any prior knowledge
of the objects, or any previously-collected training data. We carry out the
training in planetary exploration conditions, with realistic manipulator
models.</div><div><a href='http://arxiv.org/abs/2403.00470v1'>2403.00470v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04001v1")'>Bidirectional Progressive Neural Networks with Episodic Return Progress
  for Emergent Task Sequencing and Robotic Skill Transfer</div>
<div id='2403.04001v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T19:17:49Z</div><div>Authors: Suzan Ece Ada, Hanne Say, Emre Ugur, Erhan Oztop</div><div style='padding-top: 10px; width: 80ex'>Human brain and behavior provide a rich venue that can inspire novel control
and learning methods for robotics. In an attempt to exemplify such a
development by inspiring how humans acquire knowledge and transfer skills among
tasks, we introduce a novel multi-task reinforcement learning framework named
Episodic Return Progress with Bidirectional Progressive Neural Networks
(ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved
manner by (2) autonomous task switching based on a novel intrinsic motivation
signal and, in contrast to existing methods, (3) allows bidirectional skill
transfer among tasks. ERP-BPNN is a general architecture applicable to several
multi-task learning settings; in this paper, we present the details of its
neural architecture and show its ability to enable effective learning and skill
transfer among morphologically different robots in a reaching task. The
developed Bidirectional Progressive Neural Network (BPNN) architecture enables
bidirectional skill transfer without requiring incremental training and
seamlessly integrates with online task arbitration. The task arbitration
mechanism developed is based on soft Episodic Return progress (ERP), a novel
intrinsic motivation (IM) signal. To evaluate our method, we use quantifiable
robotics metrics such as 'expected distance to goal' and 'path straightness' in
addition to the usual reward-based measure of episodic return common in
reinforcement learning. With simulation experiments, we show that ERP-BPNN
achieves faster cumulative convergence and improves performance in all metrics
considered among morphologically different robots compared to the baselines.</div><div><a href='http://arxiv.org/abs/2403.04001v1'>2403.04001v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03678v2")'>Logical Specifications-guided Dynamic Task Sampling for Reinforcement
  Learning Agents</div>
<div id='2402.03678v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T04:00:21Z</div><div>Authors: Yash Shukla, Tanushree Burman, Abhishek Kulkarni, Robert Wright, Alvaro Velasquez, Jivko Sinapov</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning (RL) has made significant strides in enabling
artificial agents to learn diverse behaviors. However, learning an effective
policy often requires a large number of environment interactions. To mitigate
sample complexity issues, recent approaches have used high-level task
specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward
Machines (RM), to guide the learning progress of the agent. In this work, we
propose a novel approach, called Logical Specifications-guided Dynamic Task
Sampling (LSTS), that learns a set of RL policies to guide an agent from an
initial state to a goal state based on a high-level task specification, while
minimizing the number of environmental interactions. Unlike previous work, LSTS
does not assume information about the environment dynamics or the Reward
Machine, and dynamically samples promising tasks that lead to successful goal
policies. We evaluate LSTS on a gridworld and show that it achieves improved
time-to-threshold performance on complex sequential decision-making problems
compared to state-of-the-art RM and Automaton-guided RL baselines, such as
Q-Learning for Reward Machines and Compositional RL from logical Specifications
(DIRL). Moreover, we demonstrate that our method outperforms RM and
Automaton-guided RL baselines in terms of sample-efficiency, both in a
partially observable robotic task and in a continuous control robotic
manipulation task.</div><div><a href='http://arxiv.org/abs/2402.03678v2'>2402.03678v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12497v1")'>Building Minimal and Reusable Causal State Abstractions for
  Reinforcement Learning</div>
<div id='2401.12497v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T05:43:15Z</div><div>Authors: Zizhao Wang, Caroline Wang, Xuesu Xiao, Yuke Zhu, Peter Stone</div><div style='padding-top: 10px; width: 80ex'>Two desiderata of reinforcement learning (RL) algorithms are the ability to
learn from relatively little experience and the ability to learn policies that
generalize to a range of problem specifications. In factored state spaces, one
approach towards achieving both goals is to learn state abstractions, which
only keep the necessary variables for learning the tasks at hand. This paper
introduces Causal Bisimulation Modeling (CBM), a method that learns the causal
relationships in the dynamics and reward functions for each task to derive a
minimal, task-specific abstraction. CBM leverages and improves implicit
modeling to train a high-fidelity causal dynamics model that can be reused for
all tasks in the same environment. Empirical validation on manipulation
environments and Deepmind Control Suite reveals that CBM's learned implicit
dynamics models identify the underlying causal relationships and state
abstractions more accurately than explicit ones. Furthermore, the derived state
abstractions allow a task learner to achieve near-oracle levels of sample
efficiency and outperform baselines on all tasks.</div><div><a href='http://arxiv.org/abs/2401.12497v1'>2401.12497v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07963v1")'>SMX: Sequential Monte Carlo Planning for Expert Iteration</div>
<div id='2402.07963v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T10:32:47Z</div><div>Authors: Matthew V Macfarlane, Edan Toledo, Donal Byrne, Siddarth Singh, Paul Duckworth, Alexandre Laterre</div><div style='padding-top: 10px; width: 80ex'>Developing agents that can leverage planning abilities during their decision
and learning processes is critical to the advancement of Artificial
Intelligence. Recent works have demonstrated the effectiveness of combining
tree-based search methods and self-play learning mechanisms. Yet, these methods
typically face scaling challenges due to the sequential nature of their search.
While practical engineering solutions can partly overcome this, they still
demand extensive computational resources, which hinders their applicability. In
this paper, we introduce SMX, a model-based planning algorithm that utilises
scalable Sequential Monte Carlo methods to create an effective self-learning
mechanism. Grounded in the theoretical framework of control as inference, SMX
benefits from robust theoretical underpinnings. Its sampling-based search
approach makes it adaptable to environments with both discrete and continuous
action spaces. Furthermore, SMX allows for high parallelisation and can run on
hardware accelerators to optimise computing efficiency. SMX demonstrates a
statistically significant improvement in performance compared to AlphaZero, as
well as demonstrating its performance as an improvement operator for a
model-free policy, matching or exceeding top model-free methods across both
continuous and discrete environments.</div><div><a href='http://arxiv.org/abs/2402.07963v1'>2402.07963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03072v1")'>Learning to Abstract Visuomotor Mappings using Meta-Reinforcement
  Learning</div>
<div id='2402.03072v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:02:35Z</div><div>Authors: Carlos A. Velazquez-Vargas, Isaac Ray Christian, Jordan A. Taylor, Sreejan Kumar</div><div style='padding-top: 10px; width: 80ex'>We investigated the human capacity to acquire multiple visuomotor mappings
for de novo skills. Using a grid navigation paradigm, we tested whether
contextual cues implemented as different "grid worlds", allow participants to
learn two distinct key-mappings more efficiently. Our results indicate that
when contextual information is provided, task performance is significantly
better. The same held true for meta-reinforcement learning agents that differed
in whether or not they receive contextual information when performing the task.
We evaluated their accuracy in predicting human performance in the task and
analyzed their internal representations. The results indicate that contextual
cues allow the formation of separate representations in space and time when
using different visuomotor mappings, whereas the absence of them favors sharing
one representation. While both strategies can allow learning of multiple
visuomotor mappings, we showed contextual cues provide a computational
advantage in terms of how many mappings can be learned.</div><div><a href='http://arxiv.org/abs/2402.03072v1'>2402.03072v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14244v1")'>MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback
  and Dynamic Distance Constraint</div>
<div id='2402.14244v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T03:11:09Z</div><div>Authors: Xinglin Zhou, Yifu Yuan, Shaofu Yang, Jianye Hao</div><div style='padding-top: 10px; width: 80ex'>Hierarchical reinforcement learning (HRL) provides a promising solution for
complex tasks with sparse rewards of intelligent agents, which uses a
hierarchical framework that divides tasks into subgoals and completes them
sequentially. However, current methods struggle to find suitable subgoals for
ensuring a stable learning process. Without additional guidance, it is
impractical to rely solely on exploration or heuristics methods to determine
subgoals in a large goal space. To address the issue, We propose a general
hierarchical reinforcement learning framework incorporating human feedback and
dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating
human feedback into high-level policy learning, to find better subgoals. As for
low-level policy, MENTOR designs a dual policy for exploration-exploitation
decoupling respectively to stabilize the training. Furthermore, although humans
can simply break down tasks into subgoals to guide the right learning
direction, subgoals that are too difficult or too easy can still hinder
downstream learning efficiency. We propose the Dynamic Distance Constraint
(DDC) mechanism dynamically adjusting the space of optional subgoals. Thus
MENTOR can generate subgoals matching the low-level policy learning process
from easy to hard. Extensive experiments demonstrate that MENTOR uses a small
amount of human feedback to achieve significant improvement in complex tasks
with sparse rewards.</div><div><a href='http://arxiv.org/abs/2402.14244v1'>2402.14244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09472v1")'>Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision</div>
<div id='2403.09472v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:12:38Z</div><div>Authors: Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, Chuang Gan</div><div style='padding-top: 10px; width: 80ex'>Current AI alignment methodologies rely on human-provided demonstrations or
judgments, and the learned capabilities of AI systems would be upper-bounded by
human capabilities as a result. This raises a challenging research question:
How can we keep improving the systems when their capabilities have surpassed
the levels of humans? This paper answers this question in the context of
tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from
human annotations on easier tasks (e.g., level 1-3 MATH problems), which we
term as \textit{easy-to-hard generalization}. Our key insight is that an
evaluator (reward model) trained on supervisions for easier tasks can be
effectively used for scoring candidate solutions of harder tasks and hence
facilitating easy-to-hard generalization over different levels of tasks. Based
on this insight, we propose a novel approach to scalable alignment, which
firstly trains the process-supervised reward models on easy problems (e.g.,
level 1-3), and then uses them to evaluate the performance of policy models on
hard problems. We show that such \textit{easy-to-hard generalization from
evaluators} can enable \textit{easy-to-hard generalizations in generators}
either through re-ranking or reinforcement learning (RL). Notably, our
process-supervised 7b RL model achieves an accuracy of 34.0\% on MATH500,
despite only using human supervision on easy problems. Our approach suggests a
promising path toward AI systems that advance beyond the frontier of human
supervision.</div><div><a href='http://arxiv.org/abs/2403.09472v1'>2403.09472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14690v1")'>Incorporating Graph Attention Mechanism into Geometric Problem Solving
  Based on Deep Reinforcement Learning</div>
<div id='2403.14690v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T11:00:09Z</div><div>Authors: Xiuqin Zhong, Shengyuan Yan, Gongqi Lin, Hongguang Fu, Liang Xu, Siwen Jiang, Lei Huang, Wei Fang</div><div style='padding-top: 10px; width: 80ex'>In the context of online education, designing an automatic solver for
geometric problems has been considered a crucial step towards general math
Artificial Intelligence (AI), empowered by natural language understanding and
traditional logical inference. In most instances, problems are addressed by
adding auxiliary components such as lines or points. However, adding auxiliary
components automatically is challenging due to the complexity in selecting
suitable auxiliary components especially when pivotal decisions have to be
made. The state-of-the-art performance has been achieved by exhausting all
possible strategies from the category library to identify the one with the
maximum likelihood. However, an extensive strategy search have to be applied to
trade accuracy for ef-ficiency. To add auxiliary components automatically and
efficiently, we present deep reinforcement learning framework based on the
language model, such as BERT. We firstly apply the graph attention mechanism to
reduce the strategy searching space, called AttnStrategy, which only focus on
the conclusion-related components. Meanwhile, a novel algorithm, named
Automatically Adding Auxiliary Components using Reinforcement Learning
framework (A3C-RL), is proposed by forcing an agent to select top strategies,
which incorporates the AttnStrategy and BERT as the memory components. Results
from extensive experiments show that the proposed A3C-RL algorithm can
substantially enhance the average precision by 32.7% compared to the
traditional MCTS. In addition, the A3C-RL algorithm outperforms humans on the
geometric questions from the annual University Entrance Mathematical
Examination of China.</div><div><a href='http://arxiv.org/abs/2403.14690v1'>2403.14690v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14407v1")'>Large-Scale Actionless Video Pre-Training via Discrete Diffusion for
  Efficient Policy Learning</div>
<div id='2402.14407v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:48:47Z</div><div>Authors: Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li</div><div style='padding-top: 10px; width: 80ex'>Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. In this paper, we introduce a novel framework that leverages a
unified discrete diffusion to combine generative pre-training on human videos
and policy fine-tuning on a small number of action-labeled robot videos. We
start by compressing both human and robot videos into unified video tokens. In
the pre-training stage, we employ a discrete diffusion model with a
mask-and-replace diffusion strategy to predict future video tokens in the
latent space. In the fine-tuning stage, we harness the imagined future videos
to guide low-level action learning trained on a limited set of robot data.
Experiments demonstrate that our method generates high-fidelity future videos
for planning and enhances the fine-tuned policies compared to previous
state-of-the-art approaches with superior generalization ability. Our project
website is available at https://video-diff.github.io/.</div><div><a href='http://arxiv.org/abs/2402.14407v1'>2402.14407v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02225v2")'>Trajectory-Oriented Policy Optimization with Sparse Rewards</div>
<div id='2401.02225v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T12:21:01Z</div><div>Authors: Guojian Wang, Faguo Wu, Xiao Zhang</div><div style='padding-top: 10px; width: 80ex'>Mastering deep reinforcement learning (DRL) proves challenging in tasks
featuring scant rewards. These limited rewards merely signify whether the task
is partially or entirely accomplished, necessitating various exploration
actions before the agent garners meaningful feedback. Consequently, the
majority of existing DRL exploration algorithms struggle to acquire practical
policies within a reasonable timeframe. To address this challenge, we introduce
an approach leveraging offline demonstration trajectories for swifter and more
efficient online RL in environments with sparse rewards. Our pivotal insight
involves treating offline demonstration trajectories as guidance, rather than
mere imitation, allowing our method to learn a policy whose distribution of
state-action visitation marginally matches that of offline demonstrations. We
specifically introduce a novel trajectory distance relying on maximum mean
discrepancy (MMD) and cast policy optimization as a distance-constrained
optimization problem. We then illustrate that this optimization problem can be
streamlined into a policy-gradient algorithm, integrating rewards shaped by
insights from offline demonstrations. The proposed algorithm undergoes
evaluation across extensive discrete and continuous control tasks with sparse
and misleading rewards. The experimental findings demonstrate the significant
superiority of our proposed algorithm over baseline methods concerning diverse
exploration and the acquisition of an optimal policy.</div><div><a href='http://arxiv.org/abs/2401.02225v2'>2401.02225v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13037v1")'>Align Your Intents: Offline Imitation Learning via Optimal Transport</div>
<div id='2402.13037v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T14:24:00Z</div><div>Authors: Maksim Bobrin, Nazar Buzun, Dmitrii Krylov, Dmitry V. Dylov</div><div style='padding-top: 10px; width: 80ex'>Offline reinforcement learning (RL) addresses the problem of sequential
decision-making by learning optimal policy through pre-collected data, without
interacting with the environment. As yet, it has remained somewhat impractical,
because one rarely knows the reward explicitly and it is hard to distill it
retrospectively. Here, we show that an imitating agent can still learn the
desired behavior merely from observing the expert, despite the absence of
explicit rewards or action labels. In our method, AILOT (Aligned Imitation
Learning via Optimal Transport), we involve special representation of states in
a form of intents that incorporate pairwise spatial distances within the data.
Given such representations, we define intrinsic reward function via optimal
transport distance between the expert's and the agent's trajectories. We report
that AILOT outperforms state-of-the art offline imitation learning algorithms
on D4RL benchmarks and improves the performance of other offline RL algorithms
in the sparse-reward tasks.</div><div><a href='http://arxiv.org/abs/2402.13037v1'>2402.13037v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01057v1")'>Expert Proximity as Surrogate Rewards for Single Demonstration Imitation
  Learning</div>
<div id='2402.01057v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T23:06:19Z</div><div>Authors: Chia-Cheng Chiang, Li-Cheng Lan, Wei-Fang Sun, Chien Feng, Cho-Jui Hsieh, Chun-Yi Lee</div><div style='padding-top: 10px; width: 80ex'>In this paper, we focus on single-demonstration imitation learning (IL), a
practical approach for real-world applications where obtaining numerous expert
demonstrations is costly or infeasible. In contrast to typical IL settings with
multiple demonstrations, single-demonstration IL involves an agent having
access to only one expert trajectory. We highlight the issue of sparse reward
signals in this setting and propose to mitigate this issue through our proposed
Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed
to address reward sparsity by introducing a denser surrogate reward function
that considers environmental dynamics. This surrogate reward function
encourages the agent to navigate towards states that are proximal to expert
states. In practice, TDIL trains a transition discriminator to differentiate
between valid and non-valid transitions in a given environment to compute the
surrogate rewards. The experiments demonstrate that TDIL outperforms existing
IL approaches and achieves expert-level performance in the single-demonstration
IL setting across five widely adopted MuJoCo benchmarks as well as the "Adroit
Door" environment.</div><div><a href='http://arxiv.org/abs/2402.01057v1'>2402.01057v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13147v1")'>SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal
  Demonstrations</div>
<div id='2402.13147v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T17:02:48Z</div><div>Authors: Huy Hoang, Tien Mai, Pradeep Varakantham</div><div style='padding-top: 10px; width: 80ex'>We consider offline imitation learning (IL), which aims to mimic the expert's
behavior from its demonstration without further interaction with the
environment. One of the main challenges in offline IL is dealing with the
limited support of expert demonstrations that cover only a small fraction of
the state-action spaces. In this work, we consider offline IL, where expert
demonstrations are limited but complemented by a larger set of sub-optimal
demonstrations of lower expertise levels. Most of the existing offline IL
methods developed for this setting are based on behavior cloning or
distribution matching, where the aim is to match the occupancy distribution of
the imitation policy with that of the expert policy. Such an approach often
suffers from over-fitting, as expert demonstrations are limited to accurately
represent any occupancy distribution. On the other hand, since sub-optimal sets
are much larger, there is a high chance that the imitation policy is trained
towards sub-optimal policies. In this paper, to address these issues, we
propose a new approach based on inverse soft-Q learning, where a regularization
term is added to the training objective, with the aim of aligning the learned
rewards with a pre-assigned reward function that allocates higher weights to
state-action pairs from expert demonstrations, and lower weights to those from
lower expertise levels. On standard benchmarks, our inverse soft-Q learning
significantly outperforms other offline IL baselines by a large margin.</div><div><a href='http://arxiv.org/abs/2402.13147v1'>2402.13147v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01886v1")'>Inverse Reinforcement Learning by Estimating Expertise of Demonstrators</div>
<div id='2402.01886v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:21:09Z</div><div>Authors: Mark Beliaev, Ramtin Pedarsani</div><div style='padding-top: 10px; width: 80ex'>In Imitation Learning (IL), utilizing suboptimal and heterogeneous
demonstrations presents a substantial challenge due to the varied nature of
real-world data. However, standard IL algorithms consider these datasets as
homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators.
Previous approaches to this issue typically rely on impractical assumptions
like high-quality data subsets, confidence rankings, or explicit environmental
knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by
Estimating Expertise of Demonstrators, a novel framework that overcomes these
hurdles without prior knowledge of demonstrator expertise. IRLEED enhances
existing Inverse Reinforcement Learning (IRL) algorithms by combining a general
model for demonstrator suboptimality to address reward bias and action
variance, with a Maximum Entropy IRL framework to efficiently derive the
optimal policy from diverse, suboptimal demonstrations. Experiments in both
online and offline IL settings, with simulated and human-generated data,
demonstrate IRLEED's adaptability and effectiveness, making it a versatile
solution for learning from suboptimal demonstrations.</div><div><a href='http://arxiv.org/abs/2402.01886v1'>2402.01886v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16772v1")'>Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator</div>
<div id='2401.16772v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:22:19Z</div><div>Authors: Ryoma Furuyama, Daiki Kuyoshi, Satoshi Yamane</div><div style='padding-top: 10px; width: 80ex'>Imitation learning is often used in addition to reinforcement learning in
environments where reward design is difficult or where the reward is sparse,
but it is difficult to be able to imitate well in unknown states from a small
amount of expert data and sampling data. Supervised learning methods such as
Behavioral Cloning do not require sampling data, but usually suffer from
distribution shift. The methods based on reinforcement learning, such as
inverse reinforcement learning and Generative Adversarial imitation learning
(GAIL), can learn from only a few expert data. However, they often need to
interact with the environment. Soft Q imitation learning (SQIL) addressed the
problems, and it was shown that it could learn efficiently by combining
Behavioral Cloning and soft Q-learning with constant rewards. In order to make
this algorithm more robust to distribution shift, we propose more efficient and
robust algorithm by adding to this method a reward function based on
adversarial inverse reinforcement learning that rewards the agent for
performing actions in status similar to the demo. We call this algorithm
Discriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo
environments.</div><div><a href='http://arxiv.org/abs/2401.16772v1'>2401.16772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14593v1")'>Rethinking Adversarial Inverse Reinforcement Learning: From the Angles
  of Policy Imitation and Transferable Reward Recovery</div>
<div id='2403.14593v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:48:38Z</div><div>Authors: Yangchun Zhang, Yirui Zhou</div><div style='padding-top: 10px; width: 80ex'>Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone
approach in imitation learning. This paper rethinks the two different angles of
AIRL: policy imitation and transferable reward recovery. We begin with
substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during
the policy optimization process to enhance sample efficiency, thanks to the
off-policy formulation of SAC and identifiable Markov decision process (MDP)
models with respect to AIRL. It indeed exhibits a significant improvement in
policy imitation but accidentally brings drawbacks to transferable reward
recovery. To learn this issue, we illustrate that the SAC algorithm itself is
not feasible to disentangle the reward function comprehensively during the AIRL
training process, and propose a hybrid framework, PPO-AIRL + SAC, for
satisfactory transfer effect. Additionally, we analyze the capability of
environments to extract disentangled rewards from an algebraic theory
perspective.</div><div><a href='http://arxiv.org/abs/2403.14593v1'>2403.14593v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13777v4")'>Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
  and Perspectives on Future Directions</div>
<div id='2402.13777v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:54:48Z</div><div>Authors: Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet Aggarwal</div><div style='padding-top: 10px; width: 80ex'>Deep generative models (DGMs) have demonstrated great success across various
domains, particularly in generating texts, images, and videos using models
trained from offline data. Similarly, data-driven decision-making and robotic
control also necessitate learning a generator function from the offline data to
serve as the strategy or policy. In this case, applying deep generative models
in offline policy learning exhibits great potential, and numerous studies have
explored in this direction. However, this field still lacks a comprehensive
review and so developments of different branches are relatively independent.
Thus, we provide the first systematic review on the applications of deep
generative models for offline policy learning. In particular, we cover five
mainstream deep generative models, including Variational Auto-Encoders,
Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion
Models, and their applications in both offline reinforcement learning (offline
RL) and imitation learning (IL). Offline RL and IL are two main branches of
offline policy learning and are widely-adopted techniques for sequential
decision-making. Specifically, for each type of DGM-based offline policy
learning, we distill its fundamental scheme, categorize related works based on
the usage of the DGM, and sort out the development process of algorithms in
that field. Subsequent to the main content, we provide in-depth discussions on
deep generative models and offline policy learning as a summary, based on which
we present our perspectives on future research directions. This work offers a
hands-on reference for the research progress in deep generative models for
offline policy learning, and aims to inspire improved DGM-based offline RL or
IL algorithms. For convenience, we maintain a paper list on
https://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning.</div><div><a href='http://arxiv.org/abs/2402.13777v4'>2402.13777v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00807v1")'>Distilling Conditional Diffusion Models for Offline Reinforcement
  Learning through Trajectory Stitching</div>
<div id='2402.00807v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:44:11Z</div><div>Authors: Shangzhe Li, Xinhua Zhang</div><div style='padding-top: 10px; width: 80ex'>Deep generative models have recently emerged as an effective approach to
offline reinforcement learning. However, their large model size poses
challenges in computation. We address this issue by proposing a knowledge
distillation method based on data augmentation. In particular, high-return
trajectories are generated from a conditional diffusion model, and they are
blended with the original trajectories through a novel stitching algorithm that
leverages a new reward generator. Applying the resulting dataset to behavioral
cloning, the learned shallow policy whose size is much smaller outperforms or
nearly matches deep generative planners on several D4RL benchmarks.</div><div><a href='http://arxiv.org/abs/2402.00807v1'>2402.00807v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02644v1")'>Simple Hierarchical Planning with Diffusion</div>
<div id='2401.02644v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T05:28:40Z</div><div>Authors: Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gulcehre, Sungjin Ahn</div><div style='padding-top: 10px; width: 80ex'>Diffusion-based generative methods have proven effective in modeling
trajectories with offline datasets. However, they often face computational
challenges and can falter in generalization, especially in capturing temporal
abstractions for long-horizon tasks. To overcome this, we introduce the
Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning
method combining the advantages of hierarchical and diffusion-based planning.
Our model adopts a "jumpy" planning strategy at the higher level, which allows
it to have a larger receptive field but at a lower computational cost -- a
crucial factor for diffusion-based planning methods, as we have empirically
verified. Additionally, the jumpy sub-goals guide our low-level planner,
facilitating a fine-tuning stage and further improving our approach's
effectiveness. We conducted empirical evaluations on standard offline
reinforcement learning benchmarks, demonstrating our method's superior
performance and efficiency in terms of training and planning speed compared to
the non-hierarchical Diffuser as well as other hierarchical planning methods.
Moreover, we explore our model's generalization capability, particularly on how
our method improves generalization capabilities on compositional
out-of-distribution tasks.</div><div><a href='http://arxiv.org/abs/2401.02644v1'>2401.02644v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03570v2")'>Diffusion World Model</div>
<div id='2402.03570v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:43:57Z</div><div>Authors: Zihan Ding, Amy Zhang, Yuandong Tian, Qinqing Zheng</div><div style='padding-top: 10px; width: 80ex'>We introduce Diffusion World Model (DWM), a conditional diffusion model
capable of predicting multistep future states and rewards concurrently. As
opposed to traditional one-step dynamics models, DWM offers long-horizon
predictions in a single forward pass, eliminating the need for recursive
queries. We integrate DWM into model-based value estimation, where the
short-term return is simulated by future trajectories sampled from DWM. In the
context of offline reinforcement learning, DWM can be viewed as a conservative
value regularization through generative modeling. Alternatively, it can be seen
as a data source that enables offline Q-learning with synthetic data. Our
experiments on the D4RL dataset confirm the robustness of DWM to long-horizon
simulation. In terms of absolute performance, DWM significantly surpasses
one-step dynamics models with a $44\%$ performance gain, and achieves
state-of-the-art performance.</div><div><a href='http://arxiv.org/abs/2402.03570v2'>2402.03570v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02186v1")'>Evolution Guided Generative Flow Networks</div>
<div id='2402.02186v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T15:28:53Z</div><div>Authors: Zarif Ikram, Ling Pan, Dianbo Liu</div><div style='padding-top: 10px; width: 80ex'>Generative Flow Networks (GFlowNets) are a family of probabilistic generative
models that learn to sample compositional objects proportional to their
rewards. One big challenge of GFlowNets is training them effectively when
dealing with long time horizons and sparse rewards. To address this, we propose
Evolution guided generative flow networks (EGFN), a simple but powerful
augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our
method can work on top of any GFlowNets training objective, by training a set
of agent parameters using EA, storing the resulting trajectories in the
prioritized replay buffer, and training the GFlowNets agent using the stored
trajectories. We present a thorough investigation over a wide range of toy and
real-world benchmark tasks showing the effectiveness of our method in handling
long trajectories and sparse rewards.</div><div><a href='http://arxiv.org/abs/2402.02186v1'>2402.02186v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03479v1")'>ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context
  Environment Design</div>
<div id='2402.03479v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T19:47:45Z</div><div>Authors: Samuel Garcin, James Doran, Shangmin Guo, Christopher G. Lucas, Stefano V. Albrecht</div><div style='padding-top: 10px; width: 80ex'>Autonomous agents trained using deep reinforcement learning (RL) often lack
the ability to successfully generalise to new environments, even when they
share characteristics with the environments they have encountered during
training. In this work, we investigate how the sampling of individual
environment instances, or levels, affects the zero-shot generalisation (ZSG)
ability of RL agents. We discover that, for deep actor-critic architectures
sharing their base layers, prioritising levels according to their value loss
minimises the mutual information between the agent's internal representation
and the set of training levels in the generated training data. This provides a
novel theoretical justification for the implicit regularisation achieved by
certain adaptive sampling strategies. We then turn our attention to
unsupervised environment design (UED) methods, which have more control over the
data generation mechanism. We find that existing UED methods can significantly
shift the training distribution, which translates to low ZSG performance. To
prevent both overfitting and distributional shift, we introduce in-context
environment design (ICED). ICED generates levels using a variational
autoencoder trained over an initial set of level parameters, reducing
distributional shift, and achieves significant improvements in ZSG over
adaptive level sampling strategies and UED methods.</div><div><a href='http://arxiv.org/abs/2402.03479v1'>2402.03479v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11604v1")'>Self-evolving Autoencoder Embedded Q-Network</div>
<div id='2402.11604v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T14:42:47Z</div><div>Authors: J. Senthilnath, Bangjian Zhou, Zhen Wei Ng, Deeksha Aggarwal, Rajdeep Dutta, Ji Wei Yoon, Aye Phyu Phyu Aung, Keyu Wu, Min Wu, Xiaoli Li</div><div style='padding-top: 10px; width: 80ex'>In the realm of sequential decision-making tasks, the exploration capability
of a reinforcement learning (RL) agent is paramount for achieving high rewards
through interactions with the environment. To enhance this crucial ability, we
propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is
embedded with a Q-Network (QN). In SAQN, the self-evolving autoencoder
architecture adapts and evolves as the agent explores the environment. This
evolution enables the autoencoder to capture a diverse range of raw
observations and represent them effectively in its latent space. By leveraging
the disentangled states extracted from the encoder generated latent space, the
QN is trained to determine optimal actions that improve rewards. During the
evolution of the autoencoder architecture, a bias-variance regulatory strategy
is employed to elicit the optimal response from the RL agent. This strategy
involves two key components: (i) fostering the growth of nodes to retain
previously acquired knowledge, ensuring a rich representation of the
environment, and (ii) pruning the least contributing nodes to maintain a more
manageable and tractable latent space. Extensive experimental evaluations
conducted on three distinct benchmark environments and a real-world molecular
environment demonstrate that the proposed SAQN significantly outperforms
state-of-the-art counterparts. The results highlight the effectiveness of the
self-evolving autoencoder and its collaboration with the Q-Network in tackling
sequential decision-making tasks.</div><div><a href='http://arxiv.org/abs/2402.11604v1'>2402.11604v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03185v1")'>Preventing Reward Hacking with Occupancy Measure Regularization</div>
<div id='2403.03185v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:22:15Z</div><div>Authors: Cassidy Laidlaw, Shivam Singhal, Anca Dragan</div><div style='padding-top: 10px; width: 80ex'>Reward hacking occurs when an agent performs very well with respect to a
"proxy" reward function (which may be hand-specified or learned), but poorly
with respect to the unknown true reward. Since ensuring good alignment between
the proxy and true reward is extremely difficult, one approach to prevent
reward hacking is optimizing the proxy conservatively. Prior work has
particularly focused on enforcing the learned policy to behave similarly to a
"safe" policy by penalizing the KL divergence between their action
distributions (AD). However, AD regularization doesn't always work well since a
small change in action distribution at a single state can lead to potentially
calamitous outcomes, while large changes might not be indicative of any
dangerous activity. Our insight is that when reward hacking, the agent visits
drastically different states from those reached by the safe policy, causing
large deviations in state occupancy measure (OM). Thus, we propose regularizing
based on the OM divergence between policies instead of AD divergence to prevent
reward hacking. We theoretically establish that OM regularization can more
effectively avoid large drops in true reward. Then, we empirically demonstrate
in a variety of realistic environments that OM divergence is superior to AD
divergence for preventing reward hacking by regularizing towards a safe policy.
Furthermore, we show that occupancy measure divergence can also regularize
learned policies away from reward hacking behavior. Our code and data are
available at https://github.com/cassidylaidlaw/orpo</div><div><a href='http://arxiv.org/abs/2403.03185v1'>2403.03185v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16075v1")'>Behavioral Refinement via Interpolant-based Policy Diffusion</div>
<div id='2402.16075v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T12:19:21Z</div><div>Authors: Kaiqi Chen, Eugene Lim, Kelvin Lin, Yiyang Chen, Harold Soh</div><div style='padding-top: 10px; width: 80ex'>Imitation learning empowers artificial agents to mimic behavior by learning
from demonstrations. Recently, diffusion models, which have the ability to
model high-dimensional and multimodal distributions, have shown impressive
performance on imitation learning tasks. These models learn to shape a policy
by diffusing actions (or states) from standard Gaussian noise. However, the
target policy to be learned is often significantly different from Gaussian and
this mismatch can result in poor performance when using a small number of
diffusion steps (to improve inference speed) and under limited data. The key
idea in this work is that initiating from a more informative source than
Gaussian enables diffusion methods to overcome the above limitations. We
contribute both theoretical results, a new method, and empirical findings that
show the benefits of using an informative source policy. Our method, which we
call BRIDGER, leverages the stochastic interpolants framework to bridge
arbitrary policies, thus enabling a flexible approach towards imitation
learning. It generalizes prior work in that standard Gaussians can still be
applied, but other source policies can be used if available. In experiments on
challenging benchmarks, BRIDGER outperforms state-of-the-art diffusion policies
and we provide further analysis on design considerations when applying BRIDGER.</div><div><a href='http://arxiv.org/abs/2402.16075v1'>2402.16075v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01059v1")'>Continuous Mean-Zero Disagreement-Regularized Imitation Learning
  (CMZ-DRIL)</div>
<div id='2403.01059v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T01:40:37Z</div><div>Authors: Noah Ford, Ryan W. Gardner, Austin Juhl, Nathan Larson</div><div style='padding-top: 10px; width: 80ex'>Machine-learning paradigms such as imitation learning and reinforcement
learning can generate highly performant agents in a variety of complex
environments. However, commonly used methods require large quantities of data
and/or a known reward function. This paper presents a method called Continuous
Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) that employs a
novel reward structure to improve the performance of imitation-learning agents
that have access to only a handful of expert demonstrations. CMZ-DRIL uses
reinforcement learning to minimize uncertainty among an ensemble of agents
trained to model the expert demonstrations. This method does not use any
environment-specific rewards, but creates a continuous and mean-zero reward
function from the action disagreement of the agent ensemble. As demonstrated in
a waypoint-navigation environment and in two MuJoCo environments, CMZ-DRIL can
generate performant agents that behave more similarly to the expert than
primary previous approaches in several key metrics.</div><div><a href='http://arxiv.org/abs/2403.01059v1'>2403.01059v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00550v1")'>Imitation Learning Datasets: A Toolkit For Creating Datasets, Training
  Agents and Benchmarking</div>
<div id='2403.00550v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T14:18:46Z</div><div>Authors: Nathan Gavenski, Michael Luck, Odinaldo Rodrigues</div><div style='padding-top: 10px; width: 80ex'>Imitation learning field requires expert data to train agents in a task. Most
often, this learning approach suffers from the absence of available data, which
results in techniques being tested on its dataset. Creating datasets is a
cumbersome process requiring researchers to train expert agents from scratch,
record their interactions and test each benchmark method with newly created
data. Moreover, creating new datasets for each new technique results in a lack
of consistency in the evaluation process since each dataset can drastically
vary in state and action distribution. In response, this work aims to address
these issues by creating Imitation Learning Datasets, a toolkit that allows
for: (i) curated expert policies with multithreaded support for faster dataset
creation; (ii) readily available datasets and techniques with precise
measurements; and (iii) sharing implementations of common imitation learning
techniques. Demonstration link:
https://nathangavenski.github.io/#/il-datasets-video</div><div><a href='http://arxiv.org/abs/2403.00550v1'>2403.00550v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06031v1")'>FairTargetSim: An Interactive Simulator for Understanding and Explaining
  the Fairness Effects of Target Variable Definition</div>
<div id='2403.06031v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T22:41:33Z</div><div>Authors: Dalia Gala, Milo Phillips-Brown, Naman Goel, Carinal Prunkl, Laura Alvarez Jubete, medb corcoran, Ray Eitel-Porter</div><div style='padding-top: 10px; width: 80ex'>Machine learning requires defining one's target variable for predictions or
decisions, a process that can have profound implications on fairness: biases
are often encoded in target variable definition itself, before any data
collection or training. We present an interactive simulator, FairTargetSim
(FTS), that illustrates how target variable definition impacts fairness. FTS is
a valuable tool for algorithm developers, researchers, and non-technical
stakeholders. FTS uses a case study of algorithmic hiring, using real-world
data and user-defined target variables. FTS is open-source and available at:
http://tinyurl.com/ftsinterface. The video accompanying this paper is here:
http://tinyurl.com/ijcaifts.</div><div><a href='http://arxiv.org/abs/2403.06031v1'>2403.06031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13008v1")'>Speedrunning and path integrals</div>
<div id='2403.13008v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T19:04:11Z</div><div>Authors: Gabriele Lami</div><div style='padding-top: 10px; width: 80ex'>In this article we will explore the concept of speedrunning as a
representation of a simplified version of quantum mechanics within a classical
simulation. This analogy can be seen as a simplified approach to understanding
the broader idea that quantum mechanics may emerge from classical mechanics
simulations due to the limitations of the simulation. The concept of
speedrunning will be explored from the perspective inside the simulation, where
the player is seen as a "force of nature" that can be interpreted through
Newton's first law. Starting from this general assumption, the aim is to build
a bridge between these two fields by using the mathematical representation of
path integrals. The use of such an approach as an intermediate layer between
machine learning techniques aimed at finding an optimal strategy and a game
simulation is also analysed. This article will focus primarily on the
relationship between classical and quantum physics within the simulation,
leaving aside more technical issues in field theory such as invariance with
respect to Lorentz transformations and virtual particles.</div><div><a href='http://arxiv.org/abs/2403.13008v1'>2403.13008v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13979v1")'>Leeroo Orchestrator: Elevating LLMs Performance Through Model
  Integration</div>
<div id='2401.13979v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T06:45:32Z</div><div>Authors: Alireza Mohammadshahi, Ali Shaikh, Majid Yazdani</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose an architecture to harness the collective knowledge
of multiple trained LLMs to create a new state-of-the-art. At the core of this
framework is a LLM-based orchestrator that is adept at picking the right
underlying LLM experts for optimal task execution. Inspired by self-play in
reinforcement learning, we created a loop of query generation, orchestration,
and evaluation to generate training data for the orchestrator. Our evaluation
focused on the MMLU benchmark, employing models with 7B, 13B, and 34B
parameters available on Hugging Face. The results demonstrate new
state-of-the-art open-source models: Our Leeroo orchestrator achieves
performance on par with the Mixtral model while incurring only two-thirds of
its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by
over 5% at the same cost level, reaching an accuracy of 75.9%. Further
enhancements were observed when integrating GPT4 into the underlying model
pool. The Leeroo orchestrator nearly matches GPT4's performance at half the
cost and even exceeds GPT4's results with a 25% cost reduction. These findings
illustrate the potential of our architecture in creating state-of-the-art and
cost-effective LLMs by optimizing the synergy between multiple LLMs to achieve
superior performance outcomes.</div><div><a href='http://arxiv.org/abs/2401.13979v1'>2401.13979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17768v1")'>Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning</div>
<div id='2402.17768v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:59:18Z</div><div>Authors: Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta</div><div style='padding-top: 10px; width: 80ex'>A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.</div><div><a href='http://arxiv.org/abs/2402.17768v1'>2402.17768v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07586v1")'>Federated Learning of Socially Appropriate Agent Behaviours in Simulated
  Home Environments</div>
<div id='2403.07586v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T12:16:40Z</div><div>Authors: Saksham Checker, Nikhil Churamani, Hatice Gunes</div><div style='padding-top: 10px; width: 80ex'>As social robots become increasingly integrated into daily life, ensuring
their behaviours align with social norms is crucial. For their widespread
open-world application, it is important to explore Federated Learning (FL)
settings where individual robots can learn about their unique environments
while also learning from each others' experiences. In this paper, we present a
novel FL benchmark that evaluates different strategies, using multi-label
regression objectives, where each client individually learns to predict the
social appropriateness of different robot actions while also sharing their
learning with others. Furthermore, splitting the training data by different
contexts such that each client incrementally learns across contexts, we present
a novel Federated Continual Learning (FCL) benchmark that adapts FL-based
methods to use state-of-the-art Continual Learning (CL) methods to continually
learn socially appropriate agent behaviours under different contextual
settings. Federated Averaging (FedAvg) of weights emerges as a robust FL
strategy while rehearsal-based FCL enables incrementally learning the social
appropriateness of robot actions, across contextual splits.</div><div><a href='http://arxiv.org/abs/2403.07586v1'>2403.07586v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06041v1")'>MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts</div>
<div id='2403.06041v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T23:28:54Z</div><div>Authors: Zhuo Xu, Rui Zhou, Yida Yin, Huidong Gao, Masayoshi Tomizuka, Jiachen Li</div><div style='padding-top: 10px; width: 80ex'>Data-driven methods have great advantages in modeling complicated human
behavioral dynamics and dealing with many human-robot interaction applications.
However, collecting massive and annotated real-world human datasets has been a
laborious task, especially for highly interactive scenarios. On the other hand,
algorithmic data generation methods are usually limited by their model
capacities, making them unable to offer realistic and diverse data needed by
various application users. In this work, we study trajectory-level data
generation for multi-human or human-robot interaction scenarios and propose a
learning-based automatic trajectory generation model, which we call Multi-Agent
TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of
generating interactive human behaviors in realistic diverse contexts. We
achieve this goal by modeling the explicit and interpretable objectives so that
MATRIX can generate human motions based on diverse destinations and
heterogeneous behaviors. We carried out extensive comparison and ablation
studies to illustrate the effectiveness of our approach across various metrics.
We also presented experiments that demonstrate the capability of MATRIX to
serve as data augmentation for imitation-based motion planning.</div><div><a href='http://arxiv.org/abs/2403.06041v1'>2403.06041v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03181v1")'>Behavior Generation with Latent Actions</div>
<div id='2403.03181v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:19:29Z</div><div>Authors: Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, Lerrel Pinto</div><div style='padding-top: 10px; width: 80ex'>Generative modeling of complex behaviors from labeled datasets has been a
longstanding problem in decision making. Unlike language or image generation,
decision making requires modeling actions - continuous-valued vectors that are
multimodal in their distribution, potentially drawn from uncurated sources,
where generation errors can compound in sequential prediction. A recent class
of models called Behavior Transformers (BeT) addresses this by discretizing
actions using k-means clustering to capture different modes. However, k-means
struggles to scale for high-dimensional action spaces or long sequences, and
lacks gradient information, and thus BeT suffers in modeling long-range
actions. In this work, we present Vector-Quantized Behavior Transformer
(VQ-BeT), a versatile model for behavior generation that handles multimodal
action prediction, conditional generation, and partial observations. VQ-BeT
augments BeT by tokenizing continuous actions with a hierarchical vector
quantization module. Across seven environments including simulated
manipulation, autonomous driving, and robotics, VQ-BeT improves on
state-of-the-art models such as BeT and Diffusion Policies. Importantly, we
demonstrate VQ-BeT's improved ability to capture behavior modes while
accelerating inference speed 5x over Diffusion Policies. Videos and code can be
found https://sjlee.cc/vq-bet</div><div><a href='http://arxiv.org/abs/2403.03181v1'>2403.03181v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19469v1")'>Humanoid Locomotion as Next Token Prediction</div>
<div id='2402.19469v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:57:37Z</div><div>Authors: Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik</div><div style='padding-top: 10px; width: 80ex'>We cast real-world humanoid control as a next token prediction problem, akin
to predicting the next word in language. Our model is a causal transformer
trained via autoregressive prediction of sensorimotor trajectories. To account
for the multi-modal nature of the data, we perform prediction in a
modality-aligned way, and for each input token predict the next token from the
same modality. This general formulation enables us to leverage data with
missing modalities, like video trajectories without actions. We train our model
on a collection of simulated trajectories coming from prior neural network
policies, model-based controllers, motion capture data, and YouTube videos of
humans. We show that our model enables a full-sized humanoid to walk in San
Francisco zero-shot. Our model can transfer to the real world even when trained
on only 27 hours of walking data, and can generalize to commands not seen
during training like walking backward. These findings suggest a promising path
toward learning challenging real-world control tasks by generative modeling of
sensorimotor trajectories.</div><div><a href='http://arxiv.org/abs/2402.19469v1'>2402.19469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17835v1")'>Predicting the Future with Simple World Models</div>
<div id='2401.17835v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T13:52:11Z</div><div>Authors: Tankred Saanum, Peter Dayan, Eric Schulz</div><div style='padding-top: 10px; width: 80ex'>World models can represent potentially high-dimensional pixel observations in
compact latent spaces, making it tractable to model the dynamics of the
environment. However, the latent dynamics inferred by these models may still be
highly complex. Abstracting the dynamics of the environment with simple models
can have several benefits. If the latent dynamics are simple, the model may
generalize better to novel transitions, and discover useful latent
representations of environment states. We propose a regularization scheme that
simplifies the world model's latent dynamics. Our model, the Parsimonious
Latent Space Model (PLSM), minimizes the mutual information between latent
states and the dynamics that arise between them. This makes the dynamics softly
state-invariant, and the effects of the agent's actions more predictable. We
combine the PLSM with three different model classes used for i) future latent
state prediction, ii) video prediction, and iii) planning. We find that our
regularization improves accuracy, generalization, and performance in downstream
tasks.</div><div><a href='http://arxiv.org/abs/2401.17835v1'>2401.17835v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10967v1")'>Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot
  Generalization</div>
<div id='2403.10967v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T16:29:40Z</div><div>Authors: Sai Prasanna, Karim Farid, Raghu Rajan, André Biedenkapp</div><div style='padding-top: 10px; width: 80ex'>Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for
creating generally capable embodied agents. To address the broader challenge,
we start with the simpler setting of contextual reinforcement learning (cRL),
assuming observability of the context values that parameterize the variation in
the system's dynamics, such as the mass or dimensions of a robot, without
making further simplifying assumptions about the observability of the Markovian
state. Toward the goal of ZSG to unseen variation in context, we propose the
contextual recurrent state-space model (cRSSM), which introduces changes to the
world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world
model to incorporate context for inferring latent Markovian states from the
observations and modeling the latent dynamics. Our experiments show that such
systematic incorporation of the context improves the ZSG of the policies
trained on the ``dreams'' of the world model. We further find qualitatively
that our approach allows Dreamer to disentangle the latent state from context,
allowing it to extrapolate its dreams to the many worlds of unseen contexts.
The code for all our experiments is available at
\url{https://github.com/sai-prasanna/dreaming_of_many_worlds}.</div><div><a href='http://arxiv.org/abs/2403.10967v1'>2403.10967v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07979v1")'>Do Agents Dream of Electric Sheep?: Improving Generalization in
  Reinforcement Learning through Generative Learning</div>
<div id='2403.07979v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T18:00:02Z</div><div>Authors: Giorgio Franceschelli, Mirco Musolesi</div><div style='padding-top: 10px; width: 80ex'>The Overfitted Brain hypothesis suggests dreams happen to allow
generalization in the human brain. Here, we ask if the same is true for
reinforcement learning agents as well. Given limited experience in a real
environment, we use imagination-based reinforcement learning to train a policy
on dream-like episodes, where non-imaginative, predicted trajectories are
modified through generative augmentations. Experiments on four ProcGen
environments show that, compared to classic imagination and offline training on
collected experience, our method can reach a higher level of generalization
when dealing with sparsely rewarded environments.</div><div><a href='http://arxiv.org/abs/2403.07979v1'>2403.07979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02439v2")'>DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based
  Trajectory Stitching</div>
<div id='2402.02439v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:30:23Z</div><div>Authors: Guanghe Li, Yixiang Shan, Zhengbang Zhu, Ting Long, Weinan Zhang</div><div style='padding-top: 10px; width: 80ex'>In offline reinforcement learning (RL), the performance of the learned policy
highly depends on the quality of offline datasets. However, in many cases, the
offline dataset contains very limited optimal trajectories, which poses a
challenge for offline RL algorithms as agents must acquire the ability to
transit to high-reward regions. To address this issue, we introduce
Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data
augmentation pipeline that systematically generates stitching transitions
between trajectories. DiffStitch effectively connects low-reward trajectories
with high-reward trajectories, forming globally optimal trajectories to address
the challenges faced by offline RL algorithms. Empirical experiments conducted
on D4RL datasets demonstrate the effectiveness of DiffStitch across RL
methodologies. Notably, DiffStitch demonstrates substantial enhancements in the
performance of one-step methods (IQL), imitation learning methods (TD3+BC), and
trajectory optimization methods (DT).</div><div><a href='http://arxiv.org/abs/2402.02439v2'>2402.02439v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16452v2")'>Context-Former: Stitching via Latent Conditioned Sequence Modeling</div>
<div id='2401.16452v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T06:05:14Z</div><div>Authors: Ziqi Zhang, Jingzehua Xu, Jinxin Liu, Zifeng Zhuang, Donglin Wang</div><div style='padding-top: 10px; width: 80ex'>Offline reinforcement learning (RL) algorithms can improve the decision
making via stitching sub-optimal trajectories to obtain more optimal ones. This
capability is a crucial factor in enabling RL to learn policies that are
superior to the behavioral policy. On the other hand, Decision Transformer (DT)
abstracts the decision-making as sequence modeling, showcasing competitive
performance on offline RL benchmarks, however, recent studies demonstrate that
DT lacks of stitching capability, thus exploit stitching capability for DT is
vital to further improve its performance. In order to endow stitching
capability to DT, we abstract trajectory stitching as expert matching and
introduce our approach, ContextFormer, which integrates contextual
information-based imitation learning (IL) and sequence modeling to stitch
sub-optimal trajectory fragments by emulating the representations of a limited
number of expert trajectories. To validate our claim, we conduct experiments
from two perspectives: 1) We conduct extensive experiments on D4RL benchmarks
under the settings of IL, and experimental results demonstrate ContextFormer
can achieve competitive performance in multi-IL settings. 2) More importantly,
we conduct a comparison of ContextFormer with diverse competitive DT variants
using identical training datasets. The experimental results unveiled
ContextFormer's superiority, as it outperformed all other variants, showcasing
its remarkable performance.</div><div><a href='http://arxiv.org/abs/2401.16452v2'>2401.16452v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17423v1")'>Reinforced In-Context Black-Box Optimization</div>
<div id='2402.17423v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T11:32:14Z</div><div>Authors: Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao, Zongzhang Zhang, Chao Qian</div><div style='padding-top: 10px; width: 80ex'>Black-Box Optimization (BBO) has found successful applications in many fields
of science and engineering. Recently, there has been a growing interest in
meta-learning particular components of BBO algorithms to speed up optimization
and get rid of tedious hand-crafted heuristics. As an extension, learning the
entire algorithm from data requires the least labor from experts and can
provide the most flexibility. In this paper, we propose RIBBO, a method to
reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.
RIBBO employs expressive sequence models to learn the optimization histories
produced by multiple behavior algorithms and tasks, leveraging the in-context
learning ability of large models to extract task information and make decisions
accordingly. Central to our method is to augment the optimization histories
with regret-to-go tokens, which are designed to represent the performance of an
algorithm based on cumulative regret of the histories. The integration of
regret-to-go tokens enables RIBBO to automatically generate sequences of query
points that satisfy the user-desired regret, which is verified by its
universally good empirical performance on diverse problems, including BBOB
functions, hyper-parameter optimization and robot control problems.</div><div><a href='http://arxiv.org/abs/2402.17423v1'>2402.17423v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03306v1")'>MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot
  Learning</div>
<div id='2401.03306v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T21:04:31Z</div><div>Authors: Rafael Rafailov, Kyle Hatch, Victor Kolev, John D. Martin, Mariano Phielipp, Chelsea Finn</div><div style='padding-top: 10px; width: 80ex'>We study the problem of offline pre-training and online fine-tuning for
reinforcement learning from high-dimensional observations in the context of
realistic robot tasks. Recent offline model-free approaches successfully use
online fine-tuning to either improve the performance of the agent over the data
collection policy or adapt to novel tasks. At the same time, model-based RL
algorithms have achieved significant progress in sample efficiency and the
complexity of the tasks they can solve, yet remain under-utilized in the
fine-tuning setting. In this work, we argue that existing model-based offline
RL methods are not suitable for offline-to-online fine-tuning in
high-dimensional domains due to issues with distribution shifts, off-dynamics
data, and non-stationary rewards. We propose an on-policy model-based method
that can efficiently reuse prior data through model-based value expansion and
policy regularization, while preventing model exploitation by controlling
epistemic uncertainty. We find that our approach successfully solves tasks from
the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation
environment completely from images. To the best of our knowledge, MOTO is the
first method to solve this environment from pixels.</div><div><a href='http://arxiv.org/abs/2401.03306v1'>2401.03306v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15025v1")'>Practice Makes Perfect: Planning to Learn Skill Parameter Policies</div>
<div id='2402.15025v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T23:58:26Z</div><div>Authors: Nishanth Kumar, Tom Silver, Willie McClinton, Linfeng Zhao, Stephen Proulx, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Jennifer Barry</div><div style='padding-top: 10px; width: 80ex'>One promising approach towards effective robot decision making in complex,
long-horizon tasks is to sequence together parameterized skills. We consider a
setting where a robot is initially equipped with (1) a library of parameterized
skills, (2) an AI planner for sequencing together the skills given a goal, and
(3) a very general prior distribution for selecting skill parameters. Once
deployed, the robot should rapidly and autonomously learn to improve its
performance by specializing its skill parameter selection policy to the
particular objects, goals, and constraints in its environment. In this work, we
focus on the active learning problem of choosing which skills to practice to
maximize expected future task success. We propose that the robot should
estimate the competence of each skill, extrapolate the competence (asking: "how
much would the competence improve through practice?"), and situate the skill in
the task distribution through competence-aware planning. This approach is
implemented within a fully autonomous system where the robot repeatedly plans,
practices, and learns without any environment resets. Through experiments in
simulation, we find that our approach learns effective parameter policies more
sample-efficiently than several baselines. Experiments in the real-world
demonstrate our approach's ability to handle noise from perception and control
and improve the robot's ability to solve two long-horizon mobile-manipulation
tasks after a few hours of autonomous practice.</div><div><a href='http://arxiv.org/abs/2402.15025v1'>2402.15025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13245v1")'>Federated reinforcement learning for robot motion planning with
  zero-shot generalization</div>
<div id='2403.13245v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T02:16:54Z</div><div>Authors: Zhenyuan Yuan, Siyuan Xu, Minghui Zhu</div><div style='padding-top: 10px; width: 80ex'>This paper considers the problem of learning a control policy for robot
motion planning with zero-shot generalization, i.e., no data collection and
policy adaptation is needed when the learned policy is deployed in new
environments. We develop a federated reinforcement learning framework that
enables collaborative learning of multiple learners and a central server, i.e.,
the Cloud, without sharing their raw data. In each iteration, each learner
uploads its local control policy and the corresponding estimated normalized
arrival time to the Cloud, which then computes the global optimum among the
learners and broadcasts the optimal policy to the learners. Each learner then
selects between its local control policy and that from the Cloud for next
iteration. The proposed framework leverages on the derived zero-shot
generalization guarantees on arrival time and safety. Theoretical guarantees on
almost-sure convergence, almost consensus, Pareto improvement and optimality
gap are also provided. Monte Carlo simulation is conducted to evaluate the
proposed framework.</div><div><a href='http://arxiv.org/abs/2403.13245v1'>2403.13245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16562v1")'>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</div>
<div id='2402.16562v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T13:39:04Z</div><div>Authors: Mahmood Alqaseer, Yossra H. Ali, Tarik A. Rashid</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.</div><div><a href='http://arxiv.org/abs/2402.16562v1'>2402.16562v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12662v1")'>Integrating Human Expertise in Continuous Spaces: A Novel Interactive
  Bayesian Optimization Framework with Preference Expected Improvement</div>
<div id='2401.12662v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T11:14:59Z</div><div>Authors: Nikolaus Feith, Elmar Rueckert</div><div style='padding-top: 10px; width: 80ex'>Interactive Machine Learning (IML) seeks to integrate human expertise into
machine learning processes. However, most existing algorithms cannot be applied
to Realworld Scenarios because their state spaces and/or action spaces are
limited to discrete values. Furthermore, the interaction of all existing
methods is restricted to deciding between multiple proposals. We therefore
propose a novel framework based on Bayesian Optimization (BO). Interactive
Bayesian Optimization (IBO) enables collaboration between machine learning
algorithms and humans. This framework captures user preferences and provides an
interface for users to shape the strategy by hand. Additionally, we've
incorporated a new acquisition function, Preference Expected Improvement (PEI),
to refine the system's efficiency using a probabilistic model of the user
preferences. Our approach is geared towards ensuring that machines can benefit
from human expertise, aiming for a more aligned and effective learning process.
In the course of this work, we applied our method to simulations and in a real
world task using a Franka Panda robot to show human-robot collaboration.</div><div><a href='http://arxiv.org/abs/2401.12662v1'>2401.12662v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15757v1")'>Batch Active Learning of Reward Functions from Human Preferences</div>
<div id='2402.15757v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T08:07:48Z</div><div>Authors: Erdem Bıyık, Nima Anari, Dorsa Sadigh</div><div style='padding-top: 10px; width: 80ex'>Data generation and labeling are often expensive in robot learning.
Preference-based learning is a concept that enables reliable labeling by
querying users with preference questions. Active querying methods are commonly
employed in preference-based learning to generate more informative data at the
expense of parallelization and computation time. In this paper, we develop a
set of novel algorithms, batch active preference-based learning methods, that
enable efficient learning of reward functions using as few data samples as
possible while still having short query generation times and also retaining
parallelizability. We introduce a method based on determinantal point processes
(DPP) for active batch generation and several heuristic-based alternatives.
Finally, we present our experimental results for a variety of robotics tasks in
simulation. Our results suggest that our batch active learning algorithm
requires only a few queries that are computed in a short amount of time. We
showcase one of our algorithms in a study to learn human users' preferences.</div><div><a href='http://arxiv.org/abs/2402.15757v1'>2402.15757v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02974v1")'>Online Learning of Human Constraints from Feedback in Shared Autonomy</div>
<div id='2403.02974v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:53:48Z</div><div>Authors: Shibei Zhu, Tran Nguyen Le, Samuel Kaski, Ville Kyrki</div><div style='padding-top: 10px; width: 80ex'>Real-time collaboration with humans poses challenges due to the different
behavior patterns of humans resulting from diverse physical constraints.
Existing works typically focus on learning safety constraints for
collaboration, or how to divide and distribute the subtasks between the
participating agents to carry out the main task. In contrast, we propose to
learn a human constraints model that, in addition, considers the diverse
behaviors of different human operators. We consider a type of collaboration in
a shared-autonomy fashion, where both a human operator and an assistive robot
act simultaneously in the same task space that affects each other's actions.
The task of the assistive agent is to augment the skill of humans to perform a
shared task by supporting humans as much as possible, both in terms of reducing
the workload and minimizing the discomfort for the human operator. Therefore,
we propose an augmentative assistant agent capable of learning and adapting to
human physical constraints, aligning its actions with the ergonomic preferences
and limitations of the human operator.</div><div><a href='http://arxiv.org/abs/2403.02974v1'>2403.02974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00225v2")'>Robust Policy Learning via Offline Skill Diffusion</div>
<div id='2403.00225v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T02:00:44Z</div><div>Authors: Woo Kyung Kim, Minjong Yoo, Honguk Woo</div><div style='padding-top: 10px; width: 80ex'>Skill-based reinforcement learning (RL) approaches have shown considerable
promise, especially in solving long-horizon tasks via hierarchical structures.
These skills, learned task-agnostically from offline datasets, can accelerate
the policy learning process for new tasks. Yet, the application of these skills
in different domains remains restricted due to their inherent dependency on the
datasets, which poses a challenge when attempting to learn a skill-based policy
via RL for a target domain different from the datasets' domains. In this paper,
we present a novel offline skill learning framework DuSkill which employs a
guided Diffusion model to generate versatile skills extended from the limited
skills in datasets, thereby enhancing the robustness of policy learning for
tasks in different domains. Specifically, we devise a guided diffusion-based
skill decoder in conjunction with the hierarchical encoding to disentangle the
skill embedding space into two distinct representations, one for encapsulating
domain-invariant behaviors and the other for delineating the factors that
induce domain variations in the behaviors. Our DuSkill framework enhances the
diversity of skills learned offline, thus enabling to accelerate the learning
procedure of high-level policies for different domains. Through experiments, we
show that DuSkill outperforms other skill-based imitation learning and RL
algorithms for several long-horizon tasks, demonstrating its benefits in
few-shot imitation and online RL.</div><div><a href='http://arxiv.org/abs/2403.00225v2'>2403.00225v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06880v2")'>Unveiling the Significance of Toddler-Inspired Reward Transition in
  Goal-Oriented Reinforcement Learning</div>
<div id='2403.06880v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:34:23Z</div><div>Authors: Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim, Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang</div><div style='padding-top: 10px; width: 80ex'>Toddlers evolve from free exploration with sparse feedback to exploiting
prior experiences for goal-directed learning with denser rewards. Drawing
inspiration from this Toddler-Inspired Reward Transition, we set out to explore
the implications of varying reward transitions when incorporated into
Reinforcement Learning (RL) tasks. Central to our inquiry is the transition
from sparse to potential-based dense rewards, which share optimal strategies
regardless of reward changes. Through various experiments, including those in
egocentric navigation and robotic arm manipulation tasks, we found that proper
reward transitions significantly influence sample efficiency and success rates.
Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense
(S2D) transition. Beyond these performance metrics, using Cross-Density
Visualizer technique, we observed that transitions, especially the S2D, smooth
the policy loss landscape, promoting wide minima that enhance generalization in
RL models.</div><div><a href='http://arxiv.org/abs/2403.06880v2'>2403.06880v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06328v1")'>Transferable Reinforcement Learning via Generalized Occupancy Models</div>
<div id='2403.06328v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T22:27:21Z</div><div>Authors: Chuning Zhu, Xinqi Wang, Tyler Han, Simon S. Du, Abhishek Gupta</div><div style='padding-top: 10px; width: 80ex'>Intelligent agents must be generalists - showing the ability to quickly adapt
and generalize to varying tasks. Within the framework of reinforcement learning
(RL), model-based RL algorithms learn a task-agnostic dynamics model of the
world, in principle allowing them to generalize to arbitrary rewards. However,
one-step models naturally suffer from compounding errors, making them
ineffective for problems with long horizons and large state spaces. In this
work, we propose a novel class of models - generalized occupancy models (GOMs)
- that retain the generality of model-based RL while avoiding compounding
error. The key idea behind GOMs is to model the distribution of all possible
long-term outcomes from a given state under the coverage of a stationary
dataset, along with a policy that realizes a particular outcome from the given
state. These models can then quickly be used to select the optimal action for
arbitrary new tasks, without having to redo policy optimization. By directly
modeling long-term outcomes, GOMs avoid compounding error while retaining
generality across arbitrary reward functions. We provide a practical
instantiation of GOMs using diffusion models and show its efficacy as a new
class of transferable models, both theoretically and empirically across a
variety of simulated robotics problems. Videos and code at
https://weirdlabuw.github.io/gom/.</div><div><a href='http://arxiv.org/abs/2403.06328v1'>2403.06328v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09243v1")'>DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven
  Policy Learning</div>
<div id='2401.09243v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T14:43:59Z</div><div>Authors: Sabariswaran Mani, Abhranil Chandra, Sreyas Venkataraman, Adyan Rizvi, Yash Sirvi, Soumojit Bhattacharya, Aritra Hazra</div><div style='padding-top: 10px; width: 80ex'>Robot learning tasks are extremely compute-intensive and hardware-specific.
Thus the avenues of tackling these challenges, using a diverse dataset of
offline demonstrations that can be used to train robot manipulation agents, is
very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a
well-curated open-source dataset for offline training comprised mostly of
expert data and also benchmark scores of the common offline-RL and behaviour
cloning agents. In this paper, we introduce DiffClone, an offline algorithm of
enhanced behaviour cloning agent with diffusion-based policy learning, and
measured the efficacy of our method on real online physical robots at test
time. This is also our official submission to the Train-Offline-Test-Online
(TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both
pre-trained visual representation and agent policies. In our experiments, we
find that MOCO finetuned ResNet50 performs the best in comparison to other
finetuned representations. Goal state conditioning and mapping to transitions
resulted in a minute increase in the success rate and mean-reward. As for the
agent policy, we developed DiffClone, a behaviour cloning agent improved using
conditional diffusion.</div><div><a href='http://arxiv.org/abs/2401.09243v1'>2401.09243v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12046v2")'>Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D</div>
<div id='2401.12046v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T15:38:29Z</div><div>Authors: Haojie Huang, Owen Howell, Dian Wang, Xupeng Zhu, Robin Walters, Robert Platt</div><div style='padding-top: 10px; width: 80ex'>Many complex robotic manipulation tasks can be decomposed as a sequence of
pick and place actions. Training a robotic agent to learn this sequence over
many different starting conditions typically requires many iterations or
demonstrations, especially in 3D environments. In this work, we propose Fourier
Transporter (FourTran) which leverages the two-fold SE(d)xSE(d) symmetry in the
pick-place problem to achieve much higher sample efficiency. FourTran is an
open-loop behavior cloning method trained using expert demonstrations to
predict pick-place actions on new environments. FourTran is constrained to
incorporate symmetries of the pick and place actions independently. Our method
utilizes a fiber space Fourier transformation that allows for memory-efficient
construction. We test our proposed network on the RLbench benchmark and achieve
state-of-the-art results across various tasks.</div><div><a href='http://arxiv.org/abs/2401.12046v2'>2401.12046v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07563v1")'>Learning Generalizable Feature Fields for Mobile Manipulation</div>
<div id='2403.07563v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:51:55Z</div><div>Authors: Ri-Zhao Qiu, Yafei Hu, Ge Yang, Yuchen Song, Yang Fu, Jianglong Ye, Jiteng Mu, Ruihan Yang, Nikolay Atanasov, Sebastian Scherer, Xiaolong Wang</div><div style='padding-top: 10px; width: 80ex'>An open problem in mobile manipulation is how to represent objects and scenes
in a unified manner, so that robots can use it both for navigating in the
environment and manipulating objects. The latter requires capturing intricate
geometry while understanding fine-grained semantics, whereas the former
involves capturing the complexity inherit to an expansive physical scale. In
this work, we present GeFF (Generalizable Feature Fields), a scene-level
generalizable neural feature field that acts as a unified representation for
both navigation and manipulation that performs in real-time. To do so, we treat
generative novel view synthesis as a pre-training task, and then align the
resulting rich scene priors with natural language via CLIP feature
distillation. We demonstrate the effectiveness of this approach by deploying
GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's
ability to generalize to open-set objects as well as running time, when
performing open-vocabulary mobile manipulation in dynamic scenes.</div><div><a href='http://arxiv.org/abs/2403.07563v1'>2403.07563v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10672v1")'>Riemannian Flow Matching Policy for Robot Motion Learning</div>
<div id='2403.10672v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T20:48:41Z</div><div>Authors: Max Braun, Noémie Jaquier, Leonel Rozo, Tamim Asfour</div><div style='padding-top: 10px; width: 80ex'>We introduce Riemannian Flow Matching Policies (RFMP), a novel model for
learning and synthesizing robot visuomotor policies. RFMP leverages the
efficient training and inference capabilities of flow matching methods. By
design, RFMP inherits the strengths of flow matching: the ability to encode
high-dimensional multimodal distributions, commonly encountered in robotic
tasks, and a very simple and fast inference process. We demonstrate the
applicability of RFMP to both state-based and vision-conditioned robot motion
policies. Notably, as the robot state resides on a Riemannian manifold, RFMP
inherently incorporates geometric awareness, which is crucial for realistic
robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments,
comparing its performance against Diffusion Policies. Although both approaches
successfully learn the considered tasks, our results show that RFMP provides
smoother action trajectories with significantly lower inference times.</div><div><a href='http://arxiv.org/abs/2403.10672v1'>2403.10672v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03954v1")'>3D Diffusion Policy</div>
<div id='2403.03954v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T18:58:49Z</div><div>Authors: Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe Xu</div><div style='padding-top: 10px; width: 80ex'>Imitation learning provides an efficient way to teach robots dexterous
skills; however, learning complex skills robustly and generalizablely usually
consumes large amounts of human demonstrations. To tackle this challenging
problem, we present 3D Diffusion Policy (DP3), a novel visual imitation
learning approach that incorporates the power of 3D visual representations into
diffusion policies, a class of conditional action generative models. The core
design of DP3 is the utilization of a compact 3D visual representation,
extracted from sparse point clouds with an efficient point encoder. In our
experiments involving 72 simulation tasks, DP3 successfully handles most tasks
with just 10 demonstrations and surpasses baselines with a 55.3% relative
improvement. In 4 real robot tasks, DP3 demonstrates precise control with a
high success rate of 85%, given only 40 demonstrations of each task, and shows
excellent generalization abilities in diverse aspects, including space,
viewpoint, appearance, and instance. Interestingly, in real robot experiments,
DP3 rarely violates safety requirements, in contrast to baseline methods which
frequently do, necessitating human intervention. Our extensive evaluation
highlights the critical importance of 3D representations in real-world robot
learning. Videos, code, and data are available on
https://3d-diffusion-policy.github.io .</div><div><a href='http://arxiv.org/abs/2403.03954v1'>2403.03954v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10885v2")'>3D Diffuser Actor: Policy Diffusion with 3D Scene Representations</div>
<div id='2402.10885v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:43:02Z</div><div>Authors: Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki</div><div style='padding-top: 10px; width: 80ex'>We marry diffusion policies and 3D scene representations for robot
manipulation. Diffusion policies learn the action distribution conditioned on
the robot and environment state using conditional diffusion models. They have
recently shown to outperform both deterministic and alternative
state-conditioned action distribution learning methods. 3D robot policies use
3D scene feature representations aggregated from a single or multiple camera
views using sensed depth. They have shown to generalize better than their 2D
counterparts across camera viewpoints. We unify these two lines of work and
present 3D Diffuser Actor, a neural policy architecture that, given a language
instruction, builds a 3D representation of the visual scene and conditions on
it to iteratively denoise 3D rotations and translations for the robot's
end-effector. At each denoising iteration, our model represents end-effector
pose estimates as 3D scene tokens and predicts the 3D translation and rotation
error for each of them, by featurizing them using 3D relative attention to
other 3D visual and language tokens. 3D Diffuser Actor sets a new
state-of-the-art on RLBench with an absolute performance gain of 16.3% over the
current SOTA on a multi-view setup and an absolute gain of 13.1% on a
single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in
the setting of zero-shot unseen scene generalization by being able to
successfully run 0.2 more tasks, a 7% relative increase. It also works in the
real world from a handful of demonstrations. We ablate our model's
architectural design choices, such as 3D scene featurization and 3D relative
attentions, and show they all help generalization. Our results suggest that 3D
scene representations and powerful generative modeling are keys to efficient
robot learning from demonstrations.</div><div><a href='http://arxiv.org/abs/2402.10885v2'>2402.10885v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02500v1")'>Point Cloud Matters: Rethinking the Impact of Different Observation
  Spaces on Robot Learning</div>
<div id='2402.02500v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T14:18:45Z</div><div>Authors: Haoyi Zhu, Yating Wang, Di Huang, Weicai Ye, Wanli Ouyang, Tong He</div><div style='padding-top: 10px; width: 80ex'>In this study, we explore the influence of different observation spaces on
robot learning, focusing on three predominant modalities: RGB, RGB-D, and point
cloud. Through extensive experimentation on over 17 varied contact-rich
manipulation tasks, conducted across two benchmarks and simulators, we have
observed a notable trend: point cloud-based methods, even those with the
simplest designs, frequently surpass their RGB and RGB-D counterparts in
performance. This remains consistent in both scenarios: training from scratch
and utilizing pretraining. Furthermore, our findings indicate that point cloud
observations lead to improved policy zero-shot generalization in relation to
various geometry and visual clues, including camera viewpoints, lighting
conditions, noise levels and background appearance. The outcomes suggest that
3D point cloud is a valuable observation modality for intricate robotic tasks.
We will open-source all our codes and checkpoints, hoping that our insights can
help design more generalizable and robust robotic models.</div><div><a href='http://arxiv.org/abs/2402.02500v1'>2402.02500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08191v1")'>THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic
  Manipulation</div>
<div id='2402.08191v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T03:25:33Z</div><div>Authors: Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, Dieter Fox</div><div style='padding-top: 10px; width: 80ex'>To realize effective large-scale, real-world robotic applications, we must
evaluate how well our robot policies adapt to changes in environmental
conditions. Unfortunately, a majority of studies evaluate robot performance in
environments closely resembling or even identical to the training setup. We
present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse
manipulation tasks, that enables systematical evaluation of models across 12
axes of environmental perturbations. These perturbations include changes in
color, texture, and size of objects, table-tops, and backgrounds; we also vary
lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4
state-of-the-art manipulation models to reveal that their success rate degrades
between 30-50% across these perturbation factors. When multiple perturbations
are applied in unison, the success rate degrades $\geq$75%. We identify that
changing the number of distractor objects, target object color, or lighting
conditions are the perturbations that reduce model performance the most. To
verify the ecological validity of our results, we show that our results in
simulation are correlated ($\bar{R}^2 = 0.614$) to similar perturbations in
real-world experiments. We open source code for others to use THE COLOSSEUM,
and also release code to 3D print the objects used to replicate the real-world
perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark
to identify modeling decisions that systematically improve generalization for
manipulation. See https://robot-colosseum.github.io/ for more details.</div><div><a href='http://arxiv.org/abs/2402.08191v1'>2402.08191v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03949v1")'>Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach
  for Robust Manipulation</div>
<div id='2403.03949v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T18:55:36Z</div><div>Authors: Marcel Torne, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, Pulkit Agrawal</div><div style='padding-top: 10px; width: 80ex'>Imitation learning methods need significant human supervision to learn
policies robust to changes in object poses, physical disturbances, and visual
distractors. Reinforcement learning, on the other hand, can explore the
environment autonomously to learn robust behaviors but may require impractical
amounts of unsafe real-world data collection. To learn performant, robust
policies without the burden of unsafe real-world data collection or extensive
human supervision, we propose RialTo, a system for robustifying real-world
imitation learning policies via reinforcement learning in "digital twin"
simulation environments constructed on the fly from small amounts of real-world
data. To enable this real-to-sim-to-real pipeline, RialTo proposes an
easy-to-use interface for quickly scanning and constructing digital twins of
real-world environments. We also introduce a novel "inverse distillation"
procedure for bringing real-world demonstrations into simulated environments
for efficient fine-tuning, with minimal human intervention and engineering
required. We evaluate RialTo across a variety of robotic manipulation problems
in the real world, such as robustly stacking dishes on a rack, placing books on
a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness
without requiring extensive human data collection. Project website and videos
at https://real-to-sim-to-real.github.io/RialTo/</div><div><a href='http://arxiv.org/abs/2403.03949v1'>2403.03949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13181v1")'>DINOBot: Robot Manipulation via Retrieval and Alignment with Vision
  Foundation Models</div>
<div id='2402.13181v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T17:48:11Z</div><div>Authors: Norman Di Palo, Edward Johns</div><div style='padding-top: 10px; width: 80ex'>We propose DINOBot, a novel imitation learning framework for robot
manipulation, which leverages the image-level and pixel-level capabilities of
features extracted from Vision Transformers trained with DINO. When interacting
with a novel object, DINOBot first uses these features to retrieve the most
visually similar object experienced during human demonstrations, and then uses
this object to align its end-effector with the novel object to enable effective
interaction. Through a series of real-world experiments on everyday tasks, we
show that exploiting both the image-level and pixel-level properties of vision
foundation models enables unprecedented learning efficiency and generalisation.
Videos and code are available at https://www.robot-learning.uk/dinobot.</div><div><a href='http://arxiv.org/abs/2402.13181v1'>2402.13181v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07869v2")'>TeleMoMa: A Modular and Versatile Teleoperation System for Mobile
  Manipulation</div>
<div id='2403.07869v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:58:01Z</div><div>Authors: Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan Zhang, Peter Stone, Ben Abbatematteo, Roberto Martín-Martín</div><div style='padding-top: 10px; width: 80ex'>A critical bottleneck limiting imitation learning in robotics is the lack of
data. This problem is more severe in mobile manipulation, where collecting
demonstrations is harder than in stationary manipulation due to the lack of
available and easy-to-use teleoperation interfaces. In this work, we
demonstrate TeleMoMa, a general and modular interface for whole-body
teleoperation of mobile manipulators. TeleMoMa unifies multiple human
interfaces including RGB and depth cameras, virtual reality controllers,
keyboard, joysticks, etc., and any combination thereof. In its more accessible
version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering
the entry bar for humans to provide mobile manipulation demonstrations. We
demonstrate the versatility of TeleMoMa by teleoperating several existing
mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and
the real world. We demonstrate the quality of the demonstrations collected with
TeleMoMa by training imitation learning policies for mobile manipulation tasks
involving synchronized whole-body motion. Finally, we also show that TeleMoMa's
teleoperation channel enables teleoperation on site, looking at the robot, or
remote, sending commands and observations through a computer network, and
perform user studies to evaluate how easy it is for novice users to learn to
collect demonstrations with different combinations of human interfaces enabled
by our system. We hope TeleMoMa becomes a helpful tool for the community
enabling researchers to collect whole-body mobile manipulation demonstrations.
For more information and video results,
https://robin-lab.cs.utexas.edu/telemoma-web.</div><div><a href='http://arxiv.org/abs/2403.07869v2'>2403.07869v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02117v1")'>Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost
  Whole-Body Teleoperation</div>
<div id='2401.02117v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T07:55:53Z</div><div>Authors: Zipeng Fu, Tony Z. Zhao, Chelsea Finn</div><div style='padding-top: 10px; width: 80ex'>Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io</div><div><a href='http://arxiv.org/abs/2401.02117v1'>2401.02117v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07788v1")'>DexCap: Scalable and Portable Mocap Data Collection System for Dexterous
  Manipulation</div>
<div id='2403.07788v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:23:49Z</div><div>Authors: Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu</div><div style='padding-top: 10px; width: 80ex'>Imitation learning from human hand motion data presents a promising avenue
for imbuing robots with human-like dexterity in real-world manipulation tasks.
Despite this potential, substantial challenges persist, particularly with the
portability of existing hand motion capture (mocap) systems and the difficulty
of translating mocap data into effective control policies. To tackle these
issues, we introduce DexCap, a portable hand motion capture system, alongside
DexIL, a novel imitation algorithm for training dexterous robot skills directly
from human hand mocap data. DexCap offers precise, occlusion-resistant tracking
of wrist and finger motions based on SLAM and electromagnetic field together
with 3D observations of the environment. Utilizing this rich dataset, DexIL
employs inverse kinematics and point cloud-based imitation learning to
replicate human actions with robot hands. Beyond learning from human motion,
DexCap also offers an optional human-in-the-loop correction mechanism to refine
and further improve robot performance. Through extensive evaluation across six
dexterous manipulation tasks, our approach not only demonstrates superior
performance but also showcases the system's capability to effectively learn
from in-the-wild mocap data, paving the way for future data collection methods
for dexterous manipulation. More details can be found at
https://dex-cap.github.io</div><div><a href='http://arxiv.org/abs/2403.07788v1'>2403.07788v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12496v1")'>DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity</div>
<div id='2401.12496v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T05:37:32Z</div><div>Authors: Kang-Won Lee, Yuzhe Qin, Xiaolong Wang, Soo-Chul Lim</div><div style='padding-top: 10px; width: 80ex'>The sense of touch is an essential ability for skillfully performing a
variety of tasks, providing the capacity to search and manipulate objects
without relying on visual information. Extensive research has been conducted
over time to apply these human tactile abilities to robots. In this paper, we
introduce a multi-finger robot system designed to search for and manipulate
objects using the sense of touch without relying on visual information.
Randomly located target objects are searched using tactile sensors, and the
objects are manipulated for tasks that mimic daily-life. The objective of the
study is to endow robots with human-like tactile capabilities. To achieve this,
binary tactile sensors are implemented on one side of the robot hand to
minimize the Sim2Real gap. Training the policy through reinforcement learning
in simulation and transferring the trained policy to the real environment, we
demonstrate that object search and manipulation using tactile sensors is
possible even in an environment without vision information. In addition, an
ablation study was conducted to analyze the effect of tactile information on
manipulative tasks. Our project page is available at
https://lee-kangwon.github.io/dextouch/</div><div><a href='http://arxiv.org/abs/2401.12496v1'>2401.12496v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11898v1")'>Visuo-Tactile Pretraining for Cable Plugging</div>
<div id='2403.11898v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:56:44Z</div><div>Authors: Abraham George, Selam Gano, Pranav Katragadda, Amir Barati Farimani</div><div style='padding-top: 10px; width: 80ex'>Tactile information is a critical tool for fine-grain manipulation. As
humans, we rely heavily on tactile information to understand objects in our
environments and how to interact with them. We use touch not only to perform
manipulation tasks but also to learn how to perform these tasks. Therefore, to
create robotic agents that can learn to complete manipulation tasks at a human
or super-human level of performance, we need to properly incorporate tactile
information into both skill execution and skill learning. In this paper, we
investigate how we can incorporate tactile information into imitation learning
platforms to improve performance on complex tasks. To do this, we tackle the
challenge of plugging in a USB cable, a dexterous manipulation task that relies
on fine-grain visuo-tactile serving. By incorporating tactile information into
imitation learning frameworks, we are able to train a robotic agent to plug in
a USB cable - a first for imitation learning. Additionally, we explore how
tactile information can be used to train non-tactile agents through a
contrastive-loss pretraining process. Our results show that by pretraining with
tactile information, the performance of a non-tactile agent can be
significantly improved, reaching a level on par with visuo-tactile agents.
  For demonstration videos and access to our codebase, see the project website:
https://sites.google.com/andrew.cmu.edu/visuo-tactile-cable-plugging/home</div><div><a href='http://arxiv.org/abs/2403.11898v1'>2403.11898v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10689v1")'>Latent Object Characteristics Recognition with Visual to Haptic-Audio
  Cross-modal Transfer Learning</div>
<div id='2403.10689v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T21:18:14Z</div><div>Authors: Namiko Saito, Joao Moura, Hiroki Uchida, Sethu Vijayakumar</div><div style='padding-top: 10px; width: 80ex'>Recognising the characteristics of objects while a robot handles them is
crucial for adjusting motions that ensure stable and efficient interactions
with containers. Ahead of realising stable and efficient robot motions for
handling/transferring the containers, this work aims to recognise the latent
unobservable object characteristics. While vision is commonly used for object
recognition by robots, it is ineffective for detecting hidden objects. However,
recognising objects indirectly using other sensors is a challenging task. To
address this challenge, we propose a cross-modal transfer learning approach
from vision to haptic-audio. We initially train the model with vision, directly
observing the target object. Subsequently, we transfer the latent space learned
from vision to a second module, trained only with haptic-audio and motor data.
This transfer learning framework facilitates the representation of object
characteristics using indirect sensor data, thereby improving recognition
accuracy. For evaluating the recognition accuracy of our proposed learning
framework we selected shape, position, and orientation as the object
characteristics. Finally, we demonstrate online recognition of both trained and
untrained objects using the humanoid robot Nextage Open.</div><div><a href='http://arxiv.org/abs/2403.10689v1'>2403.10689v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14403v2")'>Adaptive Mobile Manipulation for Articulated Objects In the Open World</div>
<div id='2401.14403v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:59:44Z</div><div>Authors: Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak</div><div style='padding-top: 10px; width: 80ex'>Deploying robots in open-ended unstructured environments such as homes has
been a long-standing research problem. However, robots are often studied only
in closed-off lab settings, and prior mobile manipulation work is restricted to
pick-move-place, which is arguably just the tip of the iceberg in this area. In
this paper, we introduce Open-World Mobile Manipulation System, a full-stack
approach to tackle realistic articulated object operation, e.g. real-world
doors, cabinets, drawers, and refrigerators in open-ended unstructured
environments. The robot utilizes an adaptive learning framework to initially
learns from a small set of data through behavior cloning, followed by learning
from online practice on novel objects that fall outside the training
distribution. We also develop a low-cost mobile manipulation hardware platform
capable of safe and autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we utilize 20 articulate
objects across 4 buildings in the CMU campus. With less than an hour of online
learning for each object, the system is able to increase success rate from 50%
of BC pre-training to 95% using online adaptation. Video results at
https://open-world-mobilemanip.github.io/</div><div><a href='http://arxiv.org/abs/2401.14403v2'>2401.14403v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01647v1")'>Build Your Own Robot Friend: An Open-Source Learning Module for
  Accessible and Engaging AI Education</div>
<div id='2402.01647v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:03:08Z</div><div>Authors: Zhonghao Shi, Allison O'Connell, Zongjian Li, Siqi Liu, Jennifer Ayissi, Guy Hoffman, Mohammad Soleymani, Maja J. Matarić</div><div style='padding-top: 10px; width: 80ex'>As artificial intelligence (AI) is playing an increasingly important role in
our society and global economy, AI education and literacy have become necessary
components in college and K-12 education to prepare students for an AI-powered
society. However, current AI curricula have not yet been made accessible and
engaging enough for students and schools from all socio-economic backgrounds
with different educational goals. In this work, we developed an open-source
learning module for college and high school students, which allows students to
build their own robot companion from the ground up. This open platform can be
used to provide hands-on experience and introductory knowledge about various
aspects of AI, including robotics, machine learning (ML), software engineering,
and mechanical engineering. Because of the social and personal nature of a
socially assistive robot companion, this module also puts a special emphasis on
human-centered AI, enabling students to develop a better understanding of
human-AI interaction and AI ethics through hands-on learning activities. With
open-source documentation, assembling manuals and affordable materials,
students from different socio-economic backgrounds can personalize their
learning experience based on their individual educational goals. To evaluate
the student-perceived quality of our module, we conducted a usability testing
workshop with 15 college students recruited from a minority-serving
institution. Our results indicate that our AI module is effective,
easy-to-follow, and engaging, and it increases student interest in studying
AI/ML and robotics in the future. We hope that this work will contribute toward
accessible and engaging AI education in human-AI interaction for college and
high school students.</div><div><a href='http://arxiv.org/abs/2402.01647v1'>2402.01647v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00776v1")'>Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study
  in the Autism Spectrum Disorder Therapy</div>
<div id='2401.00776v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T14:45:19Z</div><div>Authors: Qin Yang</div><div style='padding-top: 10px; width: 80ex'>In recent years, edge computing has served as a paradigm that enables many
future technologies like AI, Robotics, IoT, and high-speed wireless sensor
networks (like 5G) by connecting cloud computing facilities and services to the
end users. Especially in medical and healthcare applications, it provides
remote patient monitoring and increases voluminous multimedia. From the
robotics angle, robot-assisted therapy (RAT) is an active-assistive robotic
technology in rehabilitation robotics, attracting many researchers to study and
benefit people with disability like autism spectrum disorder (ASD) children.
However, the main challenge of RAT is that the model capable of detecting the
affective states of ASD people exists and can recall individual preferences.
Moreover, involving expert diagnosis and recommendations to guide robots in
updating the therapy approach to adapt to different statuses and scenarios is a
crucial part of the ASD therapy process. This paper proposes the architecture
of edge cognitive computing by combining human experts and assisted robots
collaborating in the same framework to help ASD patients with long-term
support. By integrating the real-time computing and analysis of a new cognitive
robotic model for ASD therapy, the proposed architecture can achieve a seamless
remote diagnosis, round-the-clock symptom monitoring, emergency warning,
therapy alteration, and advanced assistance.</div><div><a href='http://arxiv.org/abs/2401.00776v1'>2401.00776v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02499v1")'>Robot Trajectron: Trajectory Prediction-based Shared Control for Robot
  Manipulation</div>
<div id='2402.02499v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T14:18:20Z</div><div>Authors: Pinhao Song, Pengteng Li, Erwin Aertbelien, Renaud Detry</div><div style='padding-top: 10px; width: 80ex'>We address the problem of (a) predicting the trajectory of an arm reaching
motion, based on a few seconds of the motion's onset, and (b) leveraging this
predictor to facilitate shared-control manipulation tasks, easing the cognitive
load of the operator by assisting them in their anticipated direction of
motion. Our novel intent estimator, dubbed the \emph{Robot Trajectron} (RT),
produces a probabilistic representation of the robot's anticipated trajectory
based on its recent position, velocity and acceleration history. Taking arm
dynamics into account allows RT to capture the operator's intent better than
other SOTA models that only use the arm's position, making it particularly
well-suited to assist in tasks where the operator's intent is susceptible to
change. We derive a novel shared-control solution that combines RT's predictive
capacity to a representation of the locations of potential reaching targets.
Our experiments demonstrate RT's effectiveness in both intent estimation and
shared-control tasks. We will make the code and data supporting our experiments
publicly available at https://github.com/mousecpn/Robot-Trajectron.git.</div><div><a href='http://arxiv.org/abs/2402.02499v1'>2402.02499v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04109v1")'>Using Causal Trees to Estimate Personalized Task Difficulty in
  Post-Stroke Individuals</div>
<div id='2403.04109v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T23:43:51Z</div><div>Authors: Nathaniel Dennler, Stefanos Nikolaidis, Maja Matarić</div><div style='padding-top: 10px; width: 80ex'>Adaptive training programs are crucial for recovery post stroke. However,
developing programs that automatically adapt depends on quantifying how
difficult a task is for a specific individual at a particular stage of their
recovery. In this work, we propose a method that automatically generates
regions of different task difficulty levels based on an individual's
performance. We show that this technique explains the variance in user
performance for a reaching task better than previous approaches to estimating
task difficulty.</div><div><a href='http://arxiv.org/abs/2403.04109v1'>2403.04109v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14525v1")'>Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers</div>
<div id='2402.14525v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T13:19:02Z</div><div>Authors: Yasemin Göksu, Antonio De Almeida Correia, Vignesh Prasad, Alap Kshirsagar, Dorothea Koert, Jan Peters, Georgia Chalvatzaki</div><div style='padding-top: 10px; width: 80ex'>Bimanual handovers are crucial for transferring large, deformable or delicate
objects. This paper proposes a framework for generating kinematically
constrained human-like bimanual robot motions to ensure seamless and natural
robot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to
reactively generate suitable response trajectories for a robot based on the
observed human partner's motion. The trajectories are adapted with task space
constraints to ensure accurate handovers. Results from a pilot study show that
our approach is perceived as more human--like compared to a baseline Inverse
Kinematics approach.</div><div><a href='http://arxiv.org/abs/2402.14525v1'>2402.14525v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14597v1")'>Extended Reality for Enhanced Human-Robot Collaboration: a
  Human-in-the-Loop Approach</div>
<div id='2403.14597v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:50:22Z</div><div>Authors: Yehor Karpichev, Todd Charter, Homayoun Najjaran</div><div style='padding-top: 10px; width: 80ex'>The rise of automation has provided an opportunity to achieve higher
efficiency in manufacturing processes, yet it often compromises the flexibility
required to promptly respond to evolving market needs and meet the demand for
customization. Human-robot collaboration attempts to tackle these challenges by
combining the strength and precision of machines with human ingenuity and
perceptual understanding. In this paper, we conceptualize and propose an
implementation framework for an autonomous, machine learning-based manipulator
that incorporates human-in-the-loop principles and leverages Extended Reality
(XR) to facilitate intuitive communication and programming between humans and
robots. Furthermore, the conceptual framework foresees human involvement
directly in the robot learning process, resulting in higher adaptability and
task generalization. The paper highlights key technologies enabling the
proposed framework, emphasizing the importance of developing the digital
ecosystem as a whole. Additionally, we review the existent implementation
approaches of XR in human-robot collaboration, showcasing diverse perspectives
and methodologies. The challenges and future outlooks are discussed, delving
into the major obstacles and potential research avenues of XR for more natural
human-robot interaction and integration in the industrial landscape.</div><div><a href='http://arxiv.org/abs/2403.14597v1'>2403.14597v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14502v1")'>MResT: Multi-Resolution Sensing for Real-Time Control with
  Vision-Language Models</div>
<div id='2401.14502v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:39:27Z</div><div>Authors: Saumya Saxena, Mohit Sharma, Oliver Kroemer</div><div style='padding-top: 10px; width: 80ex'>Leveraging sensing modalities across diverse spatial and temporal resolutions
can improve performance of robotic manipulation tasks. Multi-spatial resolution
sensing provides hierarchical information captured at different spatial scales
and enables both coarse and precise motions. Simultaneously multi-temporal
resolution sensing enables the agent to exhibit high reactivity and real-time
control. In this work, we propose a framework, MResT (Multi-Resolution
Transformer), for learning generalizable language-conditioned multi-task
policies that utilize sensing at different spatial and temporal resolutions
using networks of varying capacities to effectively perform real time control
of precise and reactive tasks. We leverage off-the-shelf pretrained
vision-language models to operate on low-frequency global features along with
small non-pretrained models to adapt to high frequency local feedback. Through
extensive experiments in 3 domains (coarse, precise and dynamic manipulation
tasks), we show that our approach significantly improves (2X on average) over
recent multi-task baselines. Further, our approach generalizes well to visual
and geometric variations in target objects and to varying interaction forces.</div><div><a href='http://arxiv.org/abs/2401.14502v1'>2401.14502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12910v1")'>Yell At Your Robot: Improving On-the-Fly from Language Corrections</div>
<div id='2403.12910v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:08:24Z</div><div>Authors: Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn</div><div style='padding-top: 10px; width: 80ex'>Hierarchical policies that combine language and low-level control have been
shown to perform impressively long-horizon robotic tasks, by leveraging either
zero-shot high-level planners like pretrained language and vision-language
models (LLMs/VLMs) or models trained on annotated robotic demonstrations.
However, for complex and dexterous skills, attaining high success rates on
long-horizon tasks still represents a major challenge -- the longer the task
is, the more likely it is that some stage will fail. Can humans help the robot
to continuously improve its long-horizon task performance through intuitive and
natural feedback? In this paper, we make the following observation: high-level
policies that index into sufficiently rich and expressive low-level
language-conditioned skills can be readily supervised with human feedback in
the form of language corrections. We show that even fine-grained corrections,
such as small movements ("move a bit to the left"), can be effectively
incorporated into high-level policies, and that such corrections can be readily
obtained from humans observing the robot and making occasional suggestions.
This framework enables robots not only to rapidly adapt to real-time language
feedback, but also incorporate this feedback into an iterative training scheme
that improves the high-level policy's ability to correct errors in both
low-level execution and high-level decision-making purely from verbal feedback.
Our evaluation on real hardware shows that this leads to significant
performance improvement in long-horizon, dexterous manipulation tasks without
the need for any additional teleoperation. Videos and code are available at
https://yay-robot.github.io/.</div><div><a href='http://arxiv.org/abs/2403.12910v1'>2403.12910v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03081v1")'>Preference-Conditioned Language-Guided Abstraction</div>
<div id='2402.03081v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:12:15Z</div><div>Authors: Andi Peng, Andreea Bobu, Belinda Z. Li, Theodore R. Sumers, Ilia Sucholutsky, Nishanth Kumar, Thomas L. Griffiths, Julie A. Shah</div><div style='padding-top: 10px; width: 80ex'>Learning from demonstrations is a common way for users to teach robots, but
it is prone to spurious feature correlations. Recent work constructs state
abstractions, i.e. visual representations containing task-relevant features,
from language as a way to perform more generalizable learning. However, these
abstractions also depend on a user's preference for what matters in a task,
which may be hard to describe or infeasible to exhaustively specify using
language alone. How do we construct abstractions to capture these latent
preferences? We observe that how humans behave reveals how they see the world.
Our key insight is that changes in human behavior inform us that there are
differences in preferences for how humans see the world, i.e. their state
abstractions. In this work, we propose using language models (LMs) to query for
those preferences directly given knowledge that a change in behavior has
occurred. In our framework, we use the LM in two ways: first, given a text
description of the task and knowledge of behavioral change between states, we
query the LM for possible hidden preferences; second, given the most likely
preference, we query the LM to construct the state abstraction. In this
framework, the LM is also able to ask the human directly when uncertain about
its own estimate. We demonstrate our framework's ability to construct effective
preference-conditioned abstractions in simulated experiments, a user study, as
well as on a real Spot robot performing mobile manipulation tasks.</div><div><a href='http://arxiv.org/abs/2402.03081v1'>2402.03081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18759v2")'>Learning with Language-Guided State Abstractions</div>
<div id='2402.18759v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T23:57:04Z</div><div>Authors: Andi Peng, Ilia Sucholutsky, Belinda Z. Li, Theodore R. Sumers, Thomas L. Griffiths, Jacob Andreas, Julie A. Shah</div><div style='padding-top: 10px; width: 80ex'>We describe a framework for using natural language to design state
abstractions for imitation learning. Generalizable policy learning in
high-dimensional observation spaces is facilitated by well-designed state
representations, which can surface important features of an environment and
hide irrelevant ones. These state representations are typically manually
specified, or derived from other labor-intensive labeling procedures. Our
method, LGA (language-guided abstraction), uses a combination of natural
language supervision and background knowledge from language models (LMs) to
automatically build state representations tailored to unseen tasks. In LGA, a
user first provides a (possibly incomplete) description of a target task in
natural language; next, a pre-trained LM translates this task description into
a state abstraction function that masks out irrelevant features; finally, an
imitation policy is trained using a small number of demonstrations and
LGA-generated abstract states. Experiments on simulated robotic tasks show that
LGA yields state abstractions similar to those designed by humans, but in a
fraction of the time, and that these abstractions improve generalization and
robustness in the presence of spurious correlations and ambiguous
specifications. We illustrate the utility of the learned abstractions on mobile
manipulation tasks with a Spot robot.</div><div><a href='http://arxiv.org/abs/2402.18759v2'>2402.18759v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12963v1")'>AutoRT: Embodied Foundation Models for Large Scale Orchestration of
  Robotic Agents</div>
<div id='2401.12963v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T18:45:54Z</div><div>Authors: Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Isabel Leal, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu</div><div style='padding-top: 10px; width: 80ex'>Foundation models that incorporate language, vision, and more recently
actions have revolutionized the ability to harness internet scale data to
reason about useful tasks. However, one of the key challenges of training
embodied foundation models is the lack of data grounded in the physical world.
In this paper, we propose AutoRT, a system that leverages existing foundation
models to scale up the deployment of operational robots in completely unseen
scenarios with minimal human supervision. AutoRT leverages vision-language
models (VLMs) for scene understanding and grounding, and further uses large
language models (LLMs) for proposing diverse and novel instructions to be
performed by a fleet of robots. Guiding data collection by tapping into the
knowledge of foundation models enables AutoRT to effectively reason about
autonomy tradeoffs and safety while significantly scaling up data collection
for robot learning. We demonstrate AutoRT proposing instructions to over 20
robots across multiple buildings and collecting 77k real robot episodes via
both teleoperation and autonomous robot policies. We experimentally show that
such "in-the-wild" data collected by AutoRT is significantly more diverse, and
that AutoRT's use of LLMs allows for instruction following data collection
robots that can align to human preferences.</div><div><a href='http://arxiv.org/abs/2401.12963v1'>2401.12963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12014v1")'>EnvGen: Generating and Adapting Environments via LLMs for Training
  Embodied Agents</div>
<div id='2403.12014v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:51:16Z</div><div>Authors: Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal</div><div style='padding-top: 10px; width: 80ex'>Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller embodied RL agents learn useful skills that they are weak at? We
propose EnvGen, a novel framework to address this question. First, we prompt an
LLM to generate training environments that allow agents to quickly learn
different tasks in parallel. Concretely, the LLM is given the task description
and simulator objectives that the agents should learn and is then asked to
generate a set of environment configurations (e.g., different terrains, items
given to agents, etc.). Next, we train a small RL agent in a mixture of the
original and LLM-generated environments. Then, we enable the LLM to
continuously adapt the generated environments to progressively improve the
skills that the agent is weak at, by providing feedback to the LLM in the form
of the agent's performance. We demonstrate the usefulness of EnvGen with
comprehensive experiments in Crafter and Heist environments. We find that a
small RL agent trained with EnvGen can outperform SOTA methods, including a
GPT-4 agent, and learns long-horizon tasks significantly faster. We show
qualitatively how the LLM adapts training environments to help improve RL
agents' weaker skills over time. Additionally, EnvGen is substantially more
efficient as it only uses a small number of LLM calls (e.g., 4 in total),
whereas LLM agents require thousands of LLM calls. Lastly, we present detailed
ablation studies for our design choices.</div><div><a href='http://arxiv.org/abs/2403.12014v1'>2403.12014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12624v2")'>Knowledge Distillation from Language-Oriented to Emergent Communication
  for Multi-Agent Remote Control</div>
<div id='2401.12624v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:23:13Z</div><div>Authors: Yongjun Kim, Sejin Seo, Jihong Park, Mehdi Bennis, Seong-Lyun Kim, Junil Choi</div><div style='padding-top: 10px; width: 80ex'>In this work, we compare emergent communication (EC) built upon multi-agent
deep reinforcement learning (MADRL) and language-oriented semantic
communication (LSC) empowered by a pre-trained large language model (LLM) using
human language. In a multi-agent remote navigation task, with multimodal input
data comprising location and channel maps, it is shown that EC incurs high
training cost and struggles when using multimodal data, whereas LSC yields high
inference computing cost due to the LLM's large size. To address their
respective bottlenecks, we propose a novel framework of language-guided EC
(LEC) by guiding the EC training using LSC via knowledge distillation (KD).
Simulations corroborate that LEC achieves faster travel time while avoiding
areas with poor channel conditions, as well as speeding up the MADRL training
convergence by up to 61.8% compared to EC.</div><div><a href='http://arxiv.org/abs/2401.12624v2'>2401.12624v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03462v1")'>Interactive Continual Learning Architecture for Long-Term
  Personalization of Home Service Robots</div>
<div id='2403.03462v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T04:55:39Z</div><div>Authors: Ali Ayub, Chrystopher Nehaniv, Kerstin Dautenhahn</div><div style='padding-top: 10px; width: 80ex'>For robots to perform assistive tasks in unstructured home environments, they
must learn and reason on the semantic knowledge of the environments. Despite a
resurgence in the development of semantic reasoning architectures, these
methods assume that all the training data is available a priori. However, each
user's environment is unique and can continue to change over time, which makes
these methods unsuitable for personalized home service robots. Although
research in continual learning develops methods that can learn and adapt over
time, most of these methods are tested in the narrow context of object
classification on static image datasets. In this paper, we combine ideas from
continual learning, semantic reasoning, and interactive machine learning
literature and develop a novel interactive continual learning architecture for
continual learning of semantic knowledge in a home environment through
human-robot interaction. The architecture builds on core cognitive principles
of learning and memory for efficient and real-time learning of new knowledge
from humans. We integrate our architecture with a physical mobile manipulator
robot and perform extensive system evaluations in a laboratory environment over
two months. Our results demonstrate the effectiveness of our architecture to
allow a physical robot to continually adapt to the changes in the environment
from limited data provided by the users (experimenters), and use the learned
knowledge to perform object fetching tasks.</div><div><a href='http://arxiv.org/abs/2403.03462v1'>2403.03462v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02514v1")'>Purpose for Open-Ended Learning Robots: A Computational Taxonomy,
  Definition, and Operationalisation</div>
<div id='2403.02514v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T22:03:49Z</div><div>Authors: Gianluca Baldassarre, Richard J. Duro, Emilio Cartoni, Mehdi Khamassi, Alejandro Romero, Vieri Giuliano Santucci</div><div style='padding-top: 10px; width: 80ex'>Autonomous open-ended learning (OEL) robots are able to cumulatively acquire
new skills and knowledge through direct interaction with the environment, for
example relying on the guidance of intrinsic motivations and self-generated
goals. OEL robots have a high relevance for applications as they can use the
autonomously acquired knowledge to accomplish tasks relevant for their human
users. OEL robots, however, encounter an important limitation: this may lead to
the acquisition of knowledge that is not so much relevant to accomplish the
users' tasks. This work analyses a possible solution to this problem that
pivots on the novel concept of `purpose'. Purposes indicate what the designers
and/or users want from the robot. The robot should use internal representations
of purposes, called here `desires', to focus its open-ended exploration towards
the acquisition of knowledge relevant to accomplish them. This work contributes
to develop a computational framework on purpose in two ways. First, it
formalises a framework on purpose based on a three-level motivational hierarchy
involving: (a) the purposes; (b) the desires, which are domain independent; (c)
specific domain dependent state-goals. Second, the work highlights key
challenges highlighted by the framework such as: the `purpose-desire alignment
problem', the `purpose-goal grounding problem', and the `arbitration between
desires'. Overall, the approach enables OEL robots to learn in an autonomous
way but also to focus on acquiring goals and skills that meet the purposes of
the designers and users.</div><div><a href='http://arxiv.org/abs/2403.02514v1'>2403.02514v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16354v1")'>Language-guided Skill Learning with Temporal Variational Inference</div>
<div id='2402.16354v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:19:23Z</div><div>Authors: Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris, Nicolas Le Roux, Marc-Alexandre Côté, Xingdi Yuan</div><div style='padding-top: 10px; width: 80ex'>We present an algorithm for skill discovery from expert demonstrations. The
algorithm first utilizes Large Language Models (LLMs) to propose an initial
segmentation of the trajectories. Following that, a hierarchical variational
inference framework incorporates the LLM-generated segmentation information to
discover reusable skills by merging trajectory segments. To further control the
trade-off between compression and reusability, we introduce a novel auxiliary
objective based on the Minimum Description Length principle that helps guide
this skill discovery process. Our results demonstrate that agents equipped with
our method are able to discover skills that help accelerate learning and
outperform baseline skill learning approaches on new long-horizon tasks in
BabyAI, a grid world navigation environment, as well as ALFRED, a household
simulation environment.</div><div><a href='http://arxiv.org/abs/2402.16354v1'>2402.16354v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18137v1")'>DecisionNCE: Embodied Multimodal Representations via Implicit Preference
  Learning</div>
<div id='2402.18137v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:58:24Z</div><div>Authors: Jianxiong Li, Jinliang Zheng, Yinan Zheng, Liyuan Mao, Xiao Hu, Sijie Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan</div><div style='padding-top: 10px; width: 80ex'>Multimodal pretraining has emerged as an effective strategy for the trinity
of goals of representation learning in autonomous robots: 1) extracting both
local and global task progression information; 2) enforcing temporal
consistency of visual representation; 3) capturing trajectory-level language
grounding. Most existing methods approach these via separate objectives, which
often reach sub-optimal solutions. In this paper, we propose a universal
unified objective that can simultaneously extract meaningful task progression
information from image sequences and seamlessly align them with language
instructions. We discover that via implicit preferences, where a visual
trajectory inherently aligns better with its corresponding language instruction
than mismatched pairs, the popular Bradley-Terry model can transform into
representation learning through proper reward reparameterizations. The resulted
framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively
tailored for decision-making tasks, providing an embodied representation
learning framework that elegantly extracts both local and global task
progression features, with temporal consistency enforced through implicit time
contrastive learning, while ensuring trajectory-level instruction grounding via
multimodal joint encoding. Evaluation on both simulated and real robots
demonstrates that DecisionNCE effectively facilitates diverse downstream policy
learning tasks, offering a versatile solution for unified representation and
reward learning. Project Page: https://2toinf.github.io/DecisionNCE/</div><div><a href='http://arxiv.org/abs/2402.18137v1'>2402.18137v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07548v2")'>Online Continual Learning For Interactive Instruction Following Agents</div>
<div id='2403.07548v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:33:48Z</div><div>Authors: Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi</div><div style='padding-top: 10px; width: 80ex'>In learning an embodied agent executing daily tasks via language directives,
the literature largely assumes that the agent learns all training data at the
beginning. We argue that such a learning scenario is less realistic since a
robotic agent is supposed to learn the world continuously as it explores and
perceives it. To take a step towards a more realistic embodied agent learning
scenario, we propose two continual learning setups for embodied agents;
learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new
environments (Environment Incremental Learning, Environment-IL) For the tasks,
previous 'data prior' based continual learning methods maintain logits for the
past tasks. However, the stored information is often insufficiently learned
information and requires task boundary information, which might not always be
available. Here, we propose to update them based on confidence scores without
task boundary information during training (i.e., task-free) in a moving average
fashion, named Confidence-Aware Moving Average (CAMA). In the proposed
Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state
of the art in our empirical validations by noticeable margins. The project page
including codes is https://github.com/snumprlab/cl-alfred.</div><div><a href='http://arxiv.org/abs/2403.07548v2'>2403.07548v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17767v1")'>Opening Cabinets and Drawers in the Real World using a Commodity Mobile
  Manipulator</div>
<div id='2402.17767v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:58:54Z</div><div>Authors: Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta</div><div style='padding-top: 10px; width: 80ex'>Pulling open cabinets and drawers presents many difficult technical
challenges in perception (inferring articulation parameters for objects from
onboard sensors), planning (producing motion plans that conform to tight task
constraints), and control (making and maintaining contact while applying forces
on the environment). In this work, we build an end-to-end system that enables a
commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in
diverse previously unseen real world environments. We conduct 4 days of real
world testing of this system spanning 31 different objects from across 13
different real world environments. Our system achieves a success rate of 61% on
opening novel cabinets and drawers in unseen environments zero-shot. An
analysis of the failure modes suggests that errors in perception are the most
significant challenge for our system. We will open source code and models for
others to replicate and build upon our system.</div><div><a href='http://arxiv.org/abs/2402.17767v1'>2402.17767v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15487v1")'>RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for
  Robotic Manipulation</div>
<div id='2402.15487v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:27:17Z</div><div>Authors: Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, Yunzhu Li</div><div style='padding-top: 10px; width: 80ex'>Robots need to explore their surroundings to adapt to and tackle tasks in
unknown environments. Prior work has proposed building scene graphs of the
environment but typically assumes that the environment is static, omitting
regions that require active interactions. This severely limits their ability to
handle more complex tasks in household and office environments: before setting
up a table, robots must explore drawers and cabinets to locate all utensils and
condiments. In this work, we introduce the novel task of interactive scene
exploration, wherein robots autonomously explore environments and produce an
action-conditioned scene graph (ACSG) that captures the structure of the
underlying environment. The ACSG accounts for both low-level information, such
as geometry and semantics, and high-level information, such as the
action-conditioned relationships between different entities in the scene. To
this end, we present the Robotic Exploration (RoboEXP) system, which
incorporates the Large Multimodal Model (LMM) and an explicit memory design to
enhance our system's capabilities. The robot reasons about what and how to
explore an object, accumulating new information through the interaction process
and incrementally constructing the ACSG. We apply our system across various
real-world settings in a zero-shot manner, demonstrating its effectiveness in
exploring and modeling environments it has never seen before. Leveraging the
constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP
system in facilitating a wide range of real-world manipulation tasks involving
rigid, articulated objects, nested objects like Matryoshka dolls, and
deformable objects like cloth.</div><div><a href='http://arxiv.org/abs/2402.15487v1'>2402.15487v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.08969v1")'>The Full-scale Assembly Simulation Testbed (FAST) Dataset</div>
<div id='2403.08969v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-13T21:30:01Z</div><div>Authors: Alec G. Moore, Tiffany D. Do, Nayan N. Chawla, Antonia Jimenez Iriarte, Ryan P. McMahan</div><div style='padding-top: 10px; width: 80ex'>In recent years, numerous researchers have begun investigating how virtual
reality (VR) tracking and interaction data can be used for a variety of machine
learning purposes, including user identification, predicting cybersickness, and
estimating learning gains. One constraint for this research area is the dearth
of open datasets. In this paper, we present a new open dataset captured with
our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset
consists of data collected from 108 participants (50 females, 56 males, 2
non-binary) learning how to assemble two distinct full-scale structures in VR.
In addition to explaining how the dataset was collected and describing the data
included, we discuss how the dataset may be used by future researchers.</div><div><a href='http://arxiv.org/abs/2403.08969v1'>2403.08969v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05110v1")'>Efficient Data Collection for Robotic Manipulation via Compositional
  Generalization</div>
<div id='2403.05110v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T07:15:38Z</div><div>Authors: Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, Dorsa Sadigh</div><div style='padding-top: 10px; width: 80ex'>Data collection has become an increasingly important problem in robotic
manipulation, yet there still lacks much understanding of how to effectively
collect data to facilitate broad generalization. Recent works on large-scale
robotic data collection typically vary a wide range of environmental factors
during data collection, such as object types and table textures. While these
works attempt to cover a diverse variety of scenarios, they do not explicitly
account for the possible compositional abilities of policies trained on the
data. If robot policies are able to compose different environmental factors of
variation (e.g., object types, table heights) from their training data to
succeed when encountering unseen factor combinations, then we can exploit this
to avoid collecting data for situations that composition would address. To
investigate this possibility, we conduct thorough empirical studies both in
simulation and on a real robot that compare data collection strategies and
assess whether visual imitation learning policies can compose environmental
factors. We find that policies do exhibit composition, although leveraging
prior robotic datasets is critical for this on a real robot. We use these
insights to provide better practices for in-domain data collection by proposing
data collection strategies that exploit composition, which can induce better
generalization than naive approaches for the same amount of effort during data
collection. We further demonstrate that a real robot policy trained on data
from such a strategy achieves a success rate of 77.5% when transferred to
entirely new environments that encompass unseen combinations of environmental
factors, whereas policies trained using data collected without accounting for
environmental variation fail to transfer effectively, with a success rate of
only 2.5%. We provide videos at http://iliad.stanford.edu/robot-data-comp/.</div><div><a href='http://arxiv.org/abs/2403.05110v1'>2403.05110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02339v1")'>Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation</div>
<div id='2402.02339v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T04:28:02Z</div><div>Authors: Ti Wang, Mengyuan Liu, Hong Liu, Bin Ren, Yingxuan You, Wenhao Li, Nicu Sebe, Xia Li</div><div style='padding-top: 10px; width: 80ex'>Although data-driven methods have achieved success in 3D human pose
estimation, they often suffer from domain gaps and exhibit limited
generalization. In contrast, optimization-based methods excel in fine-tuning
for specific cases but are generally inferior to data-driven methods in overall
performance. We observe that previous optimization-based methods commonly rely
on projection constraint, which only ensures alignment in 2D space, potentially
leading to the overfitting problem. To address this, we propose an
Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the
prior information of pre-trained model and alleviates the overfitting problem
using the uncertainty of joints. Specifically, during the training phase, we
design an effective 2D-to-3D network for estimating the corresponding 3D pose
while quantifying the uncertainty of each 3D joint. For optimization during
testing, the proposed optimization framework freezes the pre-trained model and
optimizes only a latent state. Projection loss is then employed to ensure the
generated poses are well aligned in 2D space for high-quality optimization.
Furthermore, we utilize the uncertainty of each joint to determine how much
each joint is allowed for optimization. The effectiveness and superiority of
the proposed framework are validated through extensive experiments on two
challenging datasets: Human3.6M and MPI-INF-3DHP. Notably, our approach
outperforms the previous best result by a large margin of 4.5% on Human3.6M.
Our source code will be open-sourced.</div><div><a href='http://arxiv.org/abs/2402.02339v1'>2402.02339v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14973v1")'>Trajectory Regularization Enhances Self-Supervised Geometric
  Representation</div>
<div id='2403.14973v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T06:04:11Z</div><div>Authors: Jiayun Wang, Stella X. Yu, Yubei Chen</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) has proven effective in learning high-quality
representations for various downstream tasks, with a primary focus on semantic
tasks. However, its application in geometric tasks remains underexplored,
partially due to the absence of a standardized evaluation method for geometric
representations. To address this gap, we introduce a new pose-estimation
benchmark for assessing SSL geometric representations, which demands training
without semantic or pose labels and achieving proficiency in both semantic and
geometric downstream tasks. On this benchmark, we study enhancing SSL geometric
representations without sacrificing semantic classification accuracy. We find
that leveraging mid-layer representations improves pose-estimation performance
by 10-20%. Further, we introduce an unsupervised trajectory-regularization
loss, which improves performance by an additional 4% and improves
generalization ability on out-of-distribution data. We hope the proposed
benchmark and methods offer new insights and improvements in self-supervised
geometric representation learning.</div><div><a href='http://arxiv.org/abs/2403.14973v1'>2403.14973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10940v1")'>ViSaRL: Visual Reinforcement Learning Guided by Human Saliency</div>
<div id='2403.10940v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T14:52:26Z</div><div>Authors: Anthony Liang, Jesse Thomason, Erdem Bıyık</div><div style='padding-top: 10px; width: 80ex'>Training robots to perform complex control tasks from high-dimensional pixel
input using reinforcement learning (RL) is sample-inefficient, because image
observations are comprised primarily of task-irrelevant information. By
contrast, humans are able to visually attend to task-relevant objects and
areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement
Learning (ViSaRL). Using ViSaRL to learn visual representations significantly
improves the success rate, sample efficiency, and generalization of an RL agent
on diverse tasks including DeepMind Control benchmark, robot manipulation in
simulation and on a real robot. We present approaches for incorporating
saliency into both CNN and Transformer-based encoders. We show that visual
representations learned using ViSaRL are robust to various sources of visual
perturbations including perceptual noise and scene variations. ViSaRL nearly
doubles success rate on the real-robot tasks compared to the baseline which
does not use saliency.</div><div><a href='http://arxiv.org/abs/2403.10940v1'>2403.10940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03681v3")'>RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model
  Feedback</div>
<div id='2402.03681v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T04:06:06Z</div><div>Authors: Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, Zackory Erickson</div><div style='padding-top: 10px; width: 80ex'>Reward engineering has long been a challenge in Reinforcement Learning (RL)
research, as it often requires extensive human effort and iterative processes
of trial-and-error to design effective reward functions. In this paper, we
propose RL-VLM-F, a method that automatically generates reward functions for
agents to learn new tasks, using only a text description of the task goal and
the agent's visual observations, by leveraging feedbacks from vision language
foundation models (VLMs). The key to our approach is to query these models to
give preferences over pairs of the agent's image observations based on the text
description of the task goal, and then learn a reward function from the
preference labels, rather than directly prompting these models to output a raw
reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F
successfully produces effective rewards and policies across various domains -
including classic control, as well as manipulation of rigid, articulated, and
deformable objects - without the need for human supervision, outperforming
prior methods that use large pretrained models for reward generation under the
same assumptions. Videos can be found on our project website:
https://rlvlmf2024.github.io/</div><div><a href='http://arxiv.org/abs/2402.03681v3'>2402.03681v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00564v1")'>EfficientZero V2: Mastering Discrete and Continuous Control with Limited
  Data</div>
<div id='2403.00564v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T14:42:25Z</div><div>Authors: Shengjie Wang, Shaohuai Liu, Weirui Ye, Jiacheng You, Yang Gao</div><div style='padding-top: 10px; width: 80ex'>Sample efficiency remains a crucial challenge in applying Reinforcement
Learning (RL) to real-world tasks. While recent algorithms have made
significant strides in improving sample efficiency, none have achieved
consistently superior performance across diverse domains. In this paper, we
introduce EfficientZero V2, a general framework designed for sample-efficient
RL algorithms. We have expanded the performance of EfficientZero to multiple
domains, encompassing both continuous and discrete actions, as well as visual
and low-dimensional inputs. With a series of improvements we propose,
EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a
significant margin in diverse tasks under the limited data setting.
EfficientZero V2 exhibits a notable advancement over the prevailing general
algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks
across diverse benchmarks, such as Atari 100k, Proprio Control, and Vision
Control.</div><div><a href='http://arxiv.org/abs/2403.00564v1'>2403.00564v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07127v1")'>Learning by Watching: A Review of Video-based Learning Approaches for
  Robot Manipulation</div>
<div id='2402.07127v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T08:41:42Z</div><div>Authors: Chrisantus Eze, Christopher Crick</div><div style='padding-top: 10px; width: 80ex'>Robot learning of manipulation skills is hindered by the scarcity of diverse,
unbiased datasets. While curated datasets can help, challenges remain in
generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild"
video datasets have driven progress in computer vision through self-supervised
techniques. Translating this to robotics, recent works have explored learning
manipulation skills by passively watching abundant videos sourced online.
Showing promising results, such video-based learning paradigms provide scalable
supervision while reducing dataset bias. This survey reviews foundations such
as video feature representation learning techniques, object affordance
understanding, 3D hand/body modeling, and large-scale robot resources, as well
as emerging techniques for acquiring robot manipulation skills from
uncontrolled video demonstrations. We discuss how learning only from observing
large-scale human videos can enhance generalization and sample efficiency for
robotic manipulation. The survey summarizes video-based learning approaches,
analyses their benefits over standard datasets, survey metrics, and benchmarks,
and discusses open challenges and future directions in this nascent domain at
the intersection of computer vision, natural language processing, and robot
learning.</div><div><a href='http://arxiv.org/abs/2402.07127v1'>2402.07127v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03037v1")'>A Backpack Full of Skills: Egocentric Video Understanding with Diverse
  Task Perspectives</div>
<div id='2403.03037v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T15:18:02Z</div><div>Authors: Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Giuseppe Averta</div><div style='padding-top: 10px; width: 80ex'>Human comprehension of a video stream is naturally broad: in a few instants,
we are able to understand what is happening, the relevance and relationship of
objects, and forecast what will follow in the near future, everything all at
once. We believe that - to effectively transfer such an holistic perception to
intelligent machines - an important role is played by learning to correlate
concepts and to abstract knowledge coming from different tasks, to
synergistically exploit them when learning novel skills. To accomplish this, we
seek for a unified approach to video understanding which combines shared
temporal modelling of human actions with minimal overhead, to support multiple
downstream tasks and enable cooperation when learning novel skills. We then
propose EgoPack, a solution that creates a collection of task perspectives that
can be carried across downstream tasks and used as a potential source of
additional insights, as a backpack of skills that a robot can carry around and
use when needed. We demonstrate the effectiveness and efficiency of our
approach on four Ego4D benchmarks, outperforming current state-of-the-art
methods.</div><div><a href='http://arxiv.org/abs/2403.03037v1'>2403.03037v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10805v1")'>Learning to Visually Connect Actions and their Effects</div>
<div id='2401.10805v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:48:49Z</div><div>Authors: Eric Peh, Paritosh Parmar, Basura Fernando</div><div style='padding-top: 10px; width: 80ex'>In this work, we introduce the novel concept of visually Connecting Actions
and Their Effects (CATE) in video understanding. CATE can have applications in
areas like task planning and learning from demonstration. We propose different
CATE-based task formulations, such as action selection and action
specification, where video understanding models connect actions and effects at
semantic and fine-grained levels. We observe that different formulations
produce representations capturing intuitive action properties. We also design
various baseline models for action selection and action specification. Despite
the intuitive nature of the task, we observe that models struggle, and humans
outperform them by a large margin. The study aims to establish a foundation for
future efforts, showcasing the flexibility and versatility of connecting
actions and effects in video understanding, with the hope of inspiring advanced
formulations and models.</div><div><a href='http://arxiv.org/abs/2401.10805v1'>2401.10805v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08525v1")'>GATS: Gather-Attend-Scatter</div>
<div id='2401.08525v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:43:42Z</div><div>Authors: Konrad Zolna, Serkan Cabi, Yutian Chen, Eric Lau, Claudio Fantacci, Jurgis Pasukonis, Jost Tobias Springenberg, Sergio Gomez Colmenarejo</div><div style='padding-top: 10px; width: 80ex'>As the AI community increasingly adopts large-scale models, it is crucial to
develop general and flexible tools to integrate them. We introduce
Gather-Attend-Scatter (GATS), a novel module that enables seamless combination
of pretrained foundation models, both trainable and frozen, into larger
multimodal networks. GATS empowers AI systems to process and generate
information across multiple modalities at different rates. In contrast to
traditional fine-tuning, GATS allows for the original component models to
remain frozen, avoiding the risk of them losing important knowledge acquired
during the pretraining phase. We demonstrate the utility and versatility of
GATS with a few experiments across games, robotics, and multimodal input-output
systems.</div><div><a href='http://arxiv.org/abs/2401.08525v1'>2401.08525v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06557v1")'>Data-driven architecture to encode information in the kinematics of
  robots and artificial avatars</div>
<div id='2403.06557v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:00:26Z</div><div>Authors: Francesco De Lellis, Marco Coraggio, Nathan C. Foster, Riccardo Villa, Cristina Becchio, Mario di Bernardo</div><div style='padding-top: 10px; width: 80ex'>We present a data-driven control architecture for modifying the kinematics of
robots and artificial avatars to encode specific information such as the
presence or not of an emotion in the movements of an avatar or robot driven by
a human operator. We validate our approach on an experimental dataset obtained
during the reach-to-grasp phase of a pick-and-place task.</div><div><a href='http://arxiv.org/abs/2403.06557v1'>2403.06557v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.11145v1")'>Supporting Experts with a Multimodal Machine-Learning-Based Tool for
  Human Behavior Analysis of Conversational Videos</div>
<div id='2402.11145v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:27:04Z</div><div>Authors: Riku Arakawa, Kiyosu Maeda, Hiromu Yakura</div><div style='padding-top: 10px; width: 80ex'>Multimodal scene search of conversations is essential for unlocking valuable
insights into social dynamics and enhancing our communication. While experts in
conversational analysis have their own knowledge and skills to find key scenes,
a lack of comprehensive, user-friendly tools that streamline the processing of
diverse multimodal queries impedes efficiency and objectivity. To solve it, we
developed Providence, a visual-programming-based tool based on design
considerations derived from a formative study with experts. It enables experts
to combine various machine learning algorithms to capture human behavioral cues
without writing code. Our study showed its preferable usability and
satisfactory output with less cognitive load imposed in accomplishing scene
search tasks of conversations, verifying the importance of its customizability
and transparency. Furthermore, through the in-the-wild trial, we confirmed the
objectivity and reusability of the tool transform experts' workflow, suggesting
the advantage of expert-AI teaming in a highly human-contextual domain.</div><div><a href='http://arxiv.org/abs/2402.11145v1'>2402.11145v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.00678v1")'>General-purpose foundation models for increased autonomy in
  robot-assisted surgery</div>
<div id='2401.00678v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T06:15:16Z</div><div>Authors: Samuel Schmidgall, Ji Woong Kim, Alan Kuntz, Ahmed Ezzat Ghazi, Axel Krieger</div><div style='padding-top: 10px; width: 80ex'>The dominant paradigm for end-to-end robot learning focuses on optimizing
task-specific objectives that solve a single robotic problem such as picking up
an object or reaching a target position. However, recent work on high-capacity
models in robotics has shown promise toward being trained on large collections
of diverse and task-agnostic datasets of video demonstrations. These models
have shown impressive levels of generalization to unseen circumstances,
especially as the amount of data and the model complexity scale. Surgical robot
systems that learn from data have struggled to advance as quickly as other
fields of robot learning for a few reasons: (1) there is a lack of existing
large-scale open-source data to train models, (2) it is challenging to model
the soft-body deformations that these robots work with during surgery because
simulation cannot match the physical and visual complexity of biological
tissue, and (3) surgical robots risk harming patients when tested in clinical
trials and require more extensive safety measures. This perspective article
aims to provide a path toward increasing robot autonomy in robot-assisted
surgery through the development of a multi-modal, multi-task,
vision-language-action model for surgical robots. Ultimately, we argue that
surgical robots are uniquely positioned to benefit from general-purpose models
and provide three guiding actions toward increased autonomy in robot-assisted
surgery.</div><div><a href='http://arxiv.org/abs/2401.00678v1'>2401.00678v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15079v1")'>Automated Feature Selection for Inverse Reinforcement Learning</div>
<div id='2403.15079v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T10:05:21Z</div><div>Authors: Daulet Baimukashev, Gokhan Alcan, Ville Kyrki</div><div style='padding-top: 10px; width: 80ex'>Inverse reinforcement learning (IRL) is an imitation learning approach to
learning reward functions from expert demonstrations. Its use avoids the
difficult and tedious procedure of manual reward specification while retaining
the generalization power of reinforcement learning. In IRL, the reward is
usually represented as a linear combination of features. In continuous state
spaces, the state variables alone are not sufficiently rich to be used as
features, but which features are good is not known in general. To address this
issue, we propose a method that employs polynomial basis functions to form a
candidate set of features, which are shown to allow the matching of statistical
moments of state distributions. Feature selection is then performed for the
candidates by leveraging the correlation between trajectory probabilities and
feature expectations. We demonstrate the approach's effectiveness by recovering
reward functions that capture expert policies across non-linear control tasks
of increasing complexity. Code, data, and videos are available at
https://sites.google.com/view/feature4irl.</div><div><a href='http://arxiv.org/abs/2403.15079v1'>2403.15079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16123v2")'>Looking for a better fit? An Incremental Learning Multimodal Object
  Referencing Framework adapting to Individual Drivers</div>
<div id='2401.16123v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T12:48:56Z</div><div>Authors: Amr Gomaa, Guillermo Reyes, Michael Feld, Antonio Krüger</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of the automotive industry towards automated and
semi-automated vehicles has rendered traditional methods of vehicle
interaction, such as touch-based and voice command systems, inadequate for a
widening range of non-driving related tasks, such as referencing objects
outside of the vehicle. Consequently, research has shifted toward gestural
input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of
interaction during driving. However, due to the dynamic nature of driving and
individual variation, there are significant differences in drivers' gestural
input performance. While, in theory, this inherent variability could be
moderated by substantial data-driven machine learning models, prevalent
methodologies lean towards constrained, single-instance trained models for
object referencing. These models show a limited capacity to continuously adapt
to the divergent behaviors of individual drivers and the variety of driving
scenarios. To address this, we propose \textit{IcRegress}, a novel
regression-based incremental learning approach that adapts to changing behavior
and the unique characteristics of drivers engaged in the dual task of driving
and referencing objects. We suggest a more personalized and adaptable solution
for multimodal gestural interfaces, employing continuous lifelong learning to
enhance driver experience, safety, and convenience. Our approach was evaluated
using an outside-the-vehicle object referencing use case, highlighting the
superiority of the incremental learning models adapted over a single trained
model across various driver traits such as handedness, driving experience, and
numerous driving conditions. Finally, to facilitate reproducibility, ease
deployment, and promote further research, we offer our approach as an
open-source framework at \url{https://github.com/amrgomaaelhady/IcRegress}.</div><div><a href='http://arxiv.org/abs/2401.16123v2'>2401.16123v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07739v1")'>Task-conditioned adaptation of visual features in multi-task policy
  learning</div>
<div id='2402.07739v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:57:31Z</div><div>Authors: Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf</div><div style='padding-top: 10px; width: 80ex'>Successfully addressing a wide variety of tasks is a core ability of
autonomous agents, which requires flexibly adapting the underlying
decision-making strategies and, as we argue in this work, also adapting the
underlying perception modules. An analogical argument would be the human visual
system, which uses top-down signals to focus attention determined by the
current task. Similarly, in this work, we adapt pre-trained large vision models
conditioned on specific downstream tasks in the context of multi-task policy
learning. We introduce task-conditioned adapters that do not require finetuning
any pre-trained weights, combined with a single policy trained with behavior
cloning and capable of addressing multiple tasks. We condition the policy and
visual adapters on task embeddings, which can be selected at inference if the
task is known, or alternatively inferred from a set of example demonstrations.
To this end, we propose a new optimization-based estimator. We evaluate the
method on a wide variety of tasks of the CortexBench benchmark and show that,
compared to existing work, it can be addressed with a single policy. In
particular, we demonstrate that adapting visual features is a key design choice
and that the method generalizes to unseen tasks given visual demonstrations.</div><div><a href='http://arxiv.org/abs/2402.07739v1'>2402.07739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08381v1")'>Robotic Imitation of Human Actions</div>
<div id='2401.08381v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:11:54Z</div><div>Authors: Josua Spisak, Matthias Kerzel, Stefan Wermter</div><div style='padding-top: 10px; width: 80ex'>Imitation can allow us to quickly gain an understanding of a new task.
Through a demonstration, we can gain direct knowledge about which actions need
to be performed and which goals they have. In this paper, we introduce a new
approach to imitation learning that tackles the challenges of a robot imitating
a human, such as the change in perspective and body schema. Our approach can
use a single human demonstration to abstract information about the demonstrated
task, and use that information to generalise and replicate it. We facilitate
this ability by a new integration of two state-of-the-art methods: a diffusion
action segmentation model to abstract temporal information from the
demonstration and an open vocabulary object detector for spatial information.
Furthermore, we refine the abstracted information and use symbolic reasoning to
create an action plan utilising inverse kinematics, to allow the robot to
imitate the demonstrated action.</div><div><a href='http://arxiv.org/abs/2401.08381v1'>2401.08381v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11658v1")'>Dynamic planning in hierarchical active inference</div>
<div id='2402.11658v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T17:32:53Z</div><div>Authors: Matteo Priorelli, Ivilin Peev Stoianov</div><div style='padding-top: 10px; width: 80ex'>By dynamic planning, we refer to the ability of the human brain to infer and
impose motor trajectories related to cognitive decisions. A recent paradigm,
active inference, brings fundamental insights into the adaptation of biological
organisms, constantly striving to minimize prediction errors to restrict
themselves to life-compatible states. Over the past years, many studies have
shown how human and animal behavior could be explained in terms of an active
inferential process -- either as discrete decision-making or continuous motor
control -- inspiring innovative solutions in robotics and artificial
intelligence. Still, the literature lacks a comprehensive outlook on how to
effectively plan actions in changing environments. Setting ourselves the goal
of modeling tool use, we delve into the topic of dynamic planning in active
inference, keeping in mind two crucial aspects of biological goal-directed
behavior: the capacity to understand and exploit affordances for object
manipulation, and to learn the hierarchical interactions between the self and
the environment, including other agents. We start from a simple unit and
gradually describe more advanced structures, comparing recently proposed design
choices and providing basic examples for each section. This study distances
itself from traditional views centered on neural networks and reinforcement
learning, and points toward a yet unexplored direction in active inference:
hybrid representations in hierarchical models.</div><div><a href='http://arxiv.org/abs/2402.11658v1'>2402.11658v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10088v1")'>Hierarchical hybrid modeling for flexible tool use</div>
<div id='2402.10088v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:15:25Z</div><div>Authors: Matteo Priorelli, Ivilin Peev Stoianov</div><div style='padding-top: 10px; width: 80ex'>In a recent computational framework called active inference, discrete models
can be linked to their continuous counterparts to perform decision-making in
changing environments. From another perspective, simple agents can be combined
to better capture the causal relationships of the world. How can we use these
two features together to achieve efficient goal-directed behavior? We present
an architecture composed of several hybrid -- continuous and discrete -- units
replicating the agent's configuration, controlled by a high-level discrete
model that achieves dynamic planning and synchronized behavior. Additional
factorizations within each level allow to represent hierarchically other agents
and objects in relation to the self. We evaluate this hierarchical hybrid model
on a non-trivial task: reaching a moving object after having picked a moving
tool. This study extends past work on control as inference and proposes an
alternative direction to deep reinforcement learning.</div><div><a href='http://arxiv.org/abs/2402.10088v1'>2402.10088v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13820v1")'>FLD: Fourier Latent Dynamics for Structured Motion Representation and
  Learning</div>
<div id='2402.13820v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T13:59:21Z</div><div>Authors: Chenhao Li, Elijah Stanger-Jones, Steve Heim, Sangbae Kim</div><div style='padding-top: 10px; width: 80ex'>Motion trajectories offer reliable references for physics-based motion
learning but suffer from sparsity, particularly in regions that lack sufficient
data coverage. To address this challenge, we introduce a self-supervised,
structured representation and generation method that extracts spatial-temporal
relationships in periodic or quasi-periodic motions. The motion dynamics in a
continuously parameterized latent space enable our method to enhance the
interpolation and generalization capabilities of motion learning algorithms.
The motion learning controller, informed by the motion parameterization,
operates online tracking of a wide range of motions, including targets unseen
during training. With a fallback mechanism, the controller dynamically adapts
its tracking strategy and automatically resorts to safe action execution when a
potentially risky target is proposed. By leveraging the identified
spatial-temporal structure, our work opens new possibilities for future
advancements in general motion representation and learning algorithms.</div><div><a href='http://arxiv.org/abs/2402.13820v1'>2402.13820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03730v1")'>Learning 3D object-centric representation through prediction</div>
<div id='2403.03730v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T14:19:11Z</div><div>Authors: John Day, Tushar Arora, Jirui Liu, Li Erran Li, Ming Bo Cai</div><div style='padding-top: 10px; width: 80ex'>As part of human core knowledge, the representation of objects is the
building block of mental representation that supports high-level concepts and
symbolic reasoning. While humans develop the ability of perceiving objects
situated in 3D environments without supervision, models that learn the same set
of abilities with similar constraints faced by human infants are lacking.
Towards this end, we developed a novel network architecture that simultaneously
learns to 1) segment objects from discrete images, 2) infer their 3D locations,
and 3) perceive depth, all while using only information directly available to
the brain as training data, namely: sequences of images and self-motion. The
core idea is treating objects as latent causes of visual input which the brain
uses to make efficient predictions of future scenes. This results in object
representations being learned as an essential byproduct of learning to predict.</div><div><a href='http://arxiv.org/abs/2403.03730v1'>2403.03730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14328v1")'>Distilling Reinforcement Learning Policies for Interpretable Robot
  Locomotion: Gradient Boosting Machines and Symbolic Regression</div>
<div id='2403.14328v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T11:54:45Z</div><div>Authors: Fernando Acero, Zhibin Li</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in reinforcement learning (RL) have led to remarkable
achievements in robot locomotion capabilities. However, the complexity and
``black-box'' nature of neural network-based RL policies hinder their
interpretability and broader acceptance, particularly in applications demanding
high levels of safety and reliability. This paper introduces a novel approach
to distill neural RL policies into more interpretable forms using Gradient
Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic
Regression. By leveraging the inherent interpretability of generalized additive
models, decision trees, and analytical expressions, we transform opaque neural
network policies into more transparent ``glass-box'' models. We train expert
neural network policies using RL and subsequently distill them into (i) GBMs,
(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution
shift challenge of behavioral cloning, we propose to use the Dataset
Aggregation (DAgger) algorithm with a curriculum of episode-dependent
alternation of actions between expert and distilled policies, to enable
efficient distillation of feedback control policies. We evaluate our approach
on various robot locomotion gaits -- walking, trotting, bounding, and pacing --
and study the importance of different observations in joint actions for
distilled policies using various methods. We train neural expert policies for
205 hours of simulated experience and distill interpretable policies with only
10 minutes of simulated interaction for each gait using the proposed method.</div><div><a href='http://arxiv.org/abs/2403.14328v1'>2403.14328v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10236v1")'>Discovering Sensorimotor Agency in Cellular Automata using Diversity
  Search</div>
<div id='2402.10236v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T14:30:42Z</div><div>Authors: Gautier Hamon, Mayalen Etcheverry, Bert Wang-Chak Chan, Clément Moulin-Frier, Pierre-Yves Oudeyer</div><div style='padding-top: 10px; width: 80ex'>The research field of Artificial Life studies how life-like phenomena such as
autopoiesis, agency, or self-regulation can self-organize in computer
simulations. In cellular automata (CA), a key open-question has been whether it
it is possible to find environment rules that self-organize robust
"individuals" from an initial state with no prior existence of things like
"bodies", "brain", "perception" or "action". In this paper, we leverage recent
advances in machine learning, combining algorithms for diversity search,
curriculum learning and gradient descent, to automate the search of such
"individuals", i.e. localized structures that move around with the ability to
react in a coherent manner to external obstacles and maintain their integrity,
hence primitive forms of sensorimotor agency. We show that this approach
enables to find systematically environmental conditions in CA leading to
self-organization of such basic forms of agency. Through multiple experiments,
we show that the discovered agents have surprisingly robust capabilities to
move, maintain their body integrity and navigate among various obstacles. They
also show strong generalization abilities, with robustness to changes of scale,
random updates or perturbations from the environment not seen during training.
We discuss how this approach opens new perspectives in AI and synthetic
bioengineering.</div><div><a href='http://arxiv.org/abs/2402.10236v1'>2402.10236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08424v1")'>Conditional Neural Expert Processes for Learning from Demonstration</div>
<div id='2402.08424v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T12:52:02Z</div><div>Authors: Yigit Yildirim, Emre Ugur</div><div style='padding-top: 10px; width: 80ex'>Learning from Demonstration (LfD) is a widely used technique for skill
acquisition in robotics. However, demonstrations of the same skill may exhibit
significant variances, or learning systems may attempt to acquire different
means of the same skill simultaneously, making it challenging to encode these
motions into movement primitives. To address these challenges, we propose an
LfD framework, namely the Conditional Neural Expert Processes (CNEP), that
learns to assign demonstrations from different modes to distinct expert
networks utilizing the inherent information within the latent space to match
experts with the encoded representations. CNEP does not require supervision on
which mode the trajectories belong to. Provided experiments on artificially
generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare
the performance of CNEP with another LfD framework, namely Conditional Neural
Movement Primitives (CNMP), on a range of tasks, including experiments on a
real robot. The results reveal enhanced modeling performance for movement
primitives, leading to the synthesis of trajectories that more accurately
reflect those demonstrated by experts, particularly when the model inputs
include intersection points from various trajectories. Additionally, CNEP
offers improved interpretability and faster convergence by promoting expert
specialization. Furthermore, we show that the CNEP model accomplishes obstacle
avoidance tasks with a real manipulator when provided with novel start and
destination points, in contrast to the CNMP model, which leads to collisions
with the obstacle.</div><div><a href='http://arxiv.org/abs/2402.08424v1'>2402.08424v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00929v2")'>PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for
  Data-Efficient Imitation Learning</div>
<div id='2403.00929v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T19:19:56Z</div><div>Authors: Tian Gao, Soroush Nasiriany, Huihan Liu, Quantao Yang, Yuke Zhu</div><div style='padding-top: 10px; width: 80ex'>Imitation learning has shown great potential for enabling robots to acquire
complex manipulation behaviors. However, these algorithms suffer from high
sample complexity in long-horizon tasks, where compounding errors accumulate
over the task horizons. We present PRIME (PRimitive-based IMitation with data
Efficiency), a behavior primitive-based framework designed for improving the
data efficiency of imitation learning. PRIME scaffolds robot tasks by
decomposing task demonstrations into primitive sequences, followed by learning
a high-level control policy to sequence primitives through imitation learning.
Our experiments demonstrate that PRIME achieves a significant performance
improvement in multi-stage manipulation tasks, with 10-34% higher success rates
in simulation over state-of-the-art baselines and 20-48% on physical hardware.</div><div><a href='http://arxiv.org/abs/2403.00929v2'>2403.00929v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13910v1")'>Augmented Reality Demonstrations for Scalable Robot Imitation Learning</div>
<div id='2403.13910v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T18:30:12Z</div><div>Authors: Yue Yang, Bryce Ikeda, Gedas Bertasius, Daniel Szafir</div><div style='padding-top: 10px; width: 80ex'>Robot Imitation Learning (IL) is a widely used method for training robots to
perform manipulation tasks that involve mimicking human demonstrations to
acquire skills. However, its practicality has been limited due to its
requirement that users be trained in operating real robot arms to provide
demonstrations. This paper presents an innovative solution: an Augmented
Reality (AR)-assisted framework for demonstration collection, empowering
non-roboticist users to produce demonstrations for robot IL using devices like
the HoloLens 2. Our framework facilitates scalable and diverse demonstration
collection for real-world tasks. We validate our approach with experiments on
three classical robotics tasks: reach, push, and pick-and-place. The real robot
performs each task successfully while replaying demonstrations collected via
AR.</div><div><a href='http://arxiv.org/abs/2403.13910v1'>2403.13910v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15239v1")'>Guided Decoding for Robot Motion Generation and Adaption</div>
<div id='2403.15239v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:32:27Z</div><div>Authors: Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt</div><div style='padding-top: 10px; width: 80ex'>We address motion generation for high-DoF robot arms in complex settings with
obstacles, via points, etc. A significant advancement in this domain is
achieved by integrating Learning from Demonstration (LfD) into the motion
generation process. This integration facilitates rapid adaptation to new tasks
and optimizes the utilization of accumulated expertise by allowing robots to
learn and generalize from demonstrated trajectories.
  We train a transformer architecture on a large dataset of simulated
trajectories. This architecture, based on a conditional variational autoencoder
transformer, learns essential motion generation skills and adapts these to meet
auxiliary tasks and constraints. Our auto-regressive approach enables real-time
integration of feedback from the physical system, enhancing the adaptability
and efficiency of motion generation. We show that our model can generate motion
from initial and target points, but also that it can adapt trajectories in
navigating complex tasks, including obstacle avoidance, via points, and meeting
velocity and acceleration constraints, across platforms.</div><div><a href='http://arxiv.org/abs/2403.15239v1'>2403.15239v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09352v1")'>Neural Contractive Dynamical Systems</div>
<div id='2401.09352v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T17:18:21Z</div><div>Authors: Hadi Beik-Mohammadi, Søren Hauberg, Georgios Arvanitidis, Nadia Figueroa, Gerhard Neumann, Leonel Rozo</div><div style='padding-top: 10px; width: 80ex'>Stability guarantees are crucial when ensuring a fully autonomous robot does
not take undesirable or potentially harmful actions. Unfortunately, global
stability guarantees are hard to provide in dynamical systems learned from
data, especially when the learned dynamics are governed by neural networks. We
propose a novel methodology to learn neural contractive dynamical systems,
where our neural architecture ensures contraction, and hence, global stability.
To efficiently scale the method to high-dimensional dynamical systems, we
develop a variant of the variational autoencoder that learns dynamics in a
low-dimensional latent representation space while retaining contractive
stability after decoding. We further extend our approach to learning
contractive systems on the Lie group of rotations to account for full-pose
end-effector dynamic motions. The result is the first highly flexible learning
architecture that provides contractive stability guarantees with capability to
perform obstacle avoidance. Empirically, we demonstrate that our approach
encodes the desired dynamics more accurately than the current state-of-the-art,
which provides less strong stability guarantees.</div><div><a href='http://arxiv.org/abs/2401.09352v1'>2401.09352v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11948v1")'>Learning Dynamical Systems Encoding Non-Linearity within Space Curvature</div>
<div id='2403.11948v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:42:39Z</div><div>Authors: Bernardo Fichera, Aude Billard</div><div style='padding-top: 10px; width: 80ex'>Dynamical Systems (DS) are an effective and powerful means of shaping
high-level policies for robotics control. They provide robust and reactive
control while ensuring the stability of the driving vector field. The
increasing complexity of real-world scenarios necessitates DS with a higher
degree of non-linearity, along with the ability to adapt to potential changes
in environmental conditions, such as obstacles. Current learning strategies for
DSs often involve a trade-off, sacrificing either stability guarantees or
offline computational efficiency in order to enhance the capabilities of the
learned DS. Online local adaptation to environmental changes is either not
taken into consideration or treated as a separate problem. In this paper, our
objective is to introduce a method that enhances the complexity of the learned
DS without compromising efficiency during training or stability guarantees.
Furthermore, we aim to provide a unified approach for seamlessly integrating
the initially learned DS's non-linearity with any local non-linearities that
may arise due to changes in the environment. We propose a geometrical approach
to learn asymptotically stable non-linear DS for robotics control. Each DS is
modeled as a harmonic damped oscillator on a latent manifold. By learning the
manifold's Euclidean embedded representation, our approach encodes the
non-linearity of the DS within the curvature of the space. Having an explicit
embedded representation of the manifold allows us to showcase obstacle
avoidance by directly inducing local deformations of the space. We demonstrate
the effectiveness of our methodology through two scenarios: first, the 2D
learning of synthetic vector fields, and second, the learning of 3D robotic
end-effector motions in real-world settings.</div><div><a href='http://arxiv.org/abs/2403.11948v1'>2403.11948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08662v2")'>Learning Emergent Gaits with Decentralized Phase Oscillators: on the
  role of Observations, Rewards, and Feedback</div>
<div id='2402.08662v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:46:10Z</div><div>Authors: Jenny Zhang, Steve Heim, Se Hwan Jeon, Sangbae Kim</div><div style='padding-top: 10px; width: 80ex'>We present a minimal phase oscillator model for learning quadrupedal
locomotion. Each of the four oscillators is coupled only to itself and its
corresponding leg through local feedback of the ground reaction force, which
can be interpreted as an observer feedback gain. We interpret the oscillator
itself as a latent contact state-estimator. Through a systematic ablation
study, we show that the combination of phase observations, simple phase-based
rewards, and the local feedback dynamics induces policies that exhibit emergent
gait preferences, while using a reduced set of simple rewards, and without
prescribing a specific gait. The code is open-source, and a video synopsis
available at https://youtu.be/1NKQ0rSV3jU.</div><div><a href='http://arxiv.org/abs/2402.08662v2'>2402.08662v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02387v1")'>Brain-Body-Task Co-Adaptation can Improve Autonomous Learning and Speed
  of Bipedal Walking</div>
<div id='2402.02387v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T07:57:52Z</div><div>Authors: Darío Urbina-Meléndez, Hesam Azadjou, Francisco J. Valero-Cuevas</div><div style='padding-top: 10px; width: 80ex'>Inspired by animals that co-adapt their brain and body to interact with the
environment, we present a tendon-driven and over-actuated (i.e., n joint, n+1
actuators) bipedal robot that (i) exploits its backdrivable mechanical
properties to manage body-environment interactions without explicit control,
and (ii) uses a simple 3-layer neural network to learn to walk after only 2
minutes of 'natural' motor babbling (i.e., an exploration strategy that is
compatible with leg and task dynamics; akin to childsplay). This brain-body
collaboration first learns to produce feet cyclical movements 'in air' and,
without further tuning, can produce locomotion when the biped is lowered to be
in slight contact with the ground. In contrast, training with 2 minutes of
'naive' motor babbling (i.e., an exploration strategy that ignores leg task
dynamics), does not produce consistent cyclical movements 'in air', and
produces erratic movements and no locomotion when in slight contact with the
ground. When further lowering the biped and making the desired leg trajectories
reach 1cm below ground (causing the desired-vs-obtained trajectories error to
be unavoidable), cyclical movements based on either natural or naive babbling
presented almost equally persistent trends, and locomotion emerged with naive
babbling. Therefore, we show how continual learning of walking in unforeseen
circumstances can be driven by continual physical adaptation rooted in the
backdrivable properties of the plant and enhanced by exploration strategies
that exploit plant dynamics. Our studies also demonstrate that the bio-inspired
codesign and co-adaptations of limbs and control strategies can produce
locomotion without explicit control of trajectory errors.</div><div><a href='http://arxiv.org/abs/2402.02387v1'>2402.02387v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12076v1")'>Neuron-centric Hebbian Learning</div>
<div id='2403.12076v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T17:38:28Z</div><div>Authors: Andrea Ferigo, Elia Cunegatti, Giovanni Iacca</div><div style='padding-top: 10px; width: 80ex'>One of the most striking capabilities behind the learning mechanisms of the
brain is the adaptation, through structural and functional plasticity, of its
synapses. While synapses have the fundamental role of transmitting information
across the brain, several studies show that it is the neuron activations that
produce changes on synapses. Yet, most plasticity models devised for artificial
Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than
neurons, therefore optimizing synaptic-specific Hebbian parameters. This
approach, however, increases the complexity of the optimization process since
each synapse is associated to multiple Hebbian parameters. To overcome this
limitation, we propose a novel plasticity model, called Neuron-centric Hebbian
Learning (NcHL), where optimization focuses on neuron- rather than
synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces
the parameters from $5W$ to $5N$, being $W$ and $N$ the number of weights and
neurons, and usually $N \ll W$. We also devise a "weightless" NcHL model, which
requires less memory by approximating the weights based on a record of neuron
activations. Our experiments on two robotic locomotion tasks reveal that NcHL
performs comparably to the ABCD rule, despite using up to $\sim97$ times less
parameters, thus allowing for scalable plasticity.</div><div><a href='http://arxiv.org/abs/2403.12076v1'>2403.12076v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11984v1")'>Hebbian Learning based Orthogonal Projection for Continual Learning of
  Spiking Neural Networks</div>
<div id='2402.11984v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:29:37Z</div><div>Authors: Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, Zhouchen Lin</div><div style='padding-top: 10px; width: 80ex'>Neuromorphic computing with spiking neural networks is promising for
energy-efficient artificial intelligence (AI) applications. However, different
from humans who continually learn different tasks in a lifetime, neural network
models suffer from catastrophic forgetting. How could neuronal operations solve
this problem is an important question for AI and neuroscience. Many previous
studies draw inspiration from observed neuroscience phenomena and propose
episodic replay or synaptic metaplasticity, but they are not guaranteed to
explicitly preserve knowledge for neuron populations. Other works focus on
machine learning methods with more mathematical grounding, e.g., orthogonal
projection on high dimensional spaces, but there is no neural correspondence
for neuromorphic computing. In this work, we develop a new method with neuronal
operations based on lateral connections and Hebbian learning, which can protect
knowledge by projecting activity traces of neurons into an orthogonal subspace
so that synaptic weight update will not interfere with old tasks. We show that
Hebbian and anti-Hebbian learning on recurrent lateral connections can
effectively extract the principal subspace of neural activities and enable
orthogonal projection. This provides new insights into how neural circuits and
Hebbian learning can help continual learning, and also how the concept of
orthogonal projection can be realized in neuronal systems. Our method is also
flexible to utilize arbitrary training methods based on presynaptic
activities/traces. Experiments show that our method consistently solves
forgetting for spiking neural networks with nearly zero forgetting under
various supervised training methods with different error propagation
approaches, and outperforms previous approaches under various settings. Our
method can pave a solid path for building continual neuromorphic computing
systems.</div><div><a href='http://arxiv.org/abs/2402.11984v1'>2402.11984v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10956v1")'>Sleep-Like Unsupervised Replay Improves Performance when Data are
  Limited or Unbalanced</div>
<div id='2402.10956v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:25:41Z</div><div>Authors: Anthony Bazhenov, Pahan Dewasurendra, Giri Krishnan, Jean Erik Delanois</div><div style='padding-top: 10px; width: 80ex'>The performance of artificial neural networks (ANNs) degrades when training
data are limited or imbalanced. In contrast, the human brain can learn quickly
from just a few examples. Here, we investigated the role of sleep in improving
the performance of ANNs trained with limited data on the MNIST and Fashion
MNIST datasets. Sleep was implemented as an unsupervised phase with local
Hebbian type learning rules. We found a significant boost in accuracy after the
sleep phase for models trained with limited data in the range of 0.5-10% of
total MNIST or Fashion MNIST datasets. When more than 10% of the total data was
used, sleep alone had a slight negative impact on performance, but this was
remedied by fine-tuning on the original data. This study sheds light on a
potential synaptic weight dynamics strategy employed by the brain during sleep
to enhance memory performance when training data are limited or imbalanced.</div><div><a href='http://arxiv.org/abs/2402.10956v1'>2402.10956v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10289v1")'>Design and development of opto-neural processors for simulation of
  neural networks trained in image detection for potential implementation in
  hybrid robotics</div>
<div id='2401.10289v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T04:42:49Z</div><div>Authors: Sanjana Shetty</div><div style='padding-top: 10px; width: 80ex'>Neural networks have been employed for a wide range of processing
applications like image processing, motor control, object detection and many
others. Living neural networks offer advantages of lower power consumption,
faster processing, and biological realism. Optogenetics offers high spatial and
temporal control over biological neurons and presents potential in training
live neural networks. This work proposes a simulated living neural network
trained indirectly by backpropagating STDP based algorithms using precision
activation by optogenetics achieving accuracy comparable to traditional neural
network training algorithms.</div><div><a href='http://arxiv.org/abs/2401.10289v1'>2401.10289v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14928v1")'>Learning Inverse Kinodynamics for Autonomous Vehicle Drifting</div>
<div id='2402.14928v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T19:24:56Z</div><div>Authors: M. Suvarna, O. Tehrani</div><div style='padding-top: 10px; width: 80ex'>In this work, we explore a data-driven learning-based approach to learning
the kinodynamic model of a small autonomous vehicle, and observe the effect it
has on motion planning, specifically autonomous drifting. When executing a
motion plan in the real world, there are numerous causes for error, and what is
planned is often not what is executed on the actual car. Learning a kinodynamic
planner based off of inertial measurements and executed commands can help us
learn the world state. In our case, we look towards the realm of drifting; it
is a complex maneuver that requires a smooth enough surface, high enough speed,
and a drastic change in velocity. We attempt to learn the kinodynamic model for
these drifting maneuvers, and attempt to tighten the slip of the car. Our
approach is able to learn a kinodynamic model for high-speed circular
navigation, and is able to avoid obstacles on an autonomous drift at high speed
by correcting an executed curvature for loose drifts. We seek to adjust our
kinodynamic model for success in tighter drifts in future work.</div><div><a href='http://arxiv.org/abs/2402.14928v1'>2402.14928v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08090v2")'>Learning Neural Contracting Dynamics: Extended Linearization and Global
  Guarantees</div>
<div id='2402.08090v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:17:28Z</div><div>Authors: Sean Jaffe, Alexander Davydov, Deniz Lapsekili, Ambuj Singh, Francesco Bullo</div><div style='padding-top: 10px; width: 80ex'>Global stability and robustness guarantees in learned dynamical systems are
essential to ensure well-behavedness of the systems in the face of uncertainty.
We present Extended Linearized Contracting Dynamics (ELCD), the first neural
network-based dynamical system with global contractivity guarantees in
arbitrary metrics. The key feature of ELCD is a parametrization of the extended
linearization of the nonlinear vector field. In its most basic form, ELCD is
guaranteed to be (i) globally exponentially stable, (ii) equilibrium
contracting, and (iii) globally contracting with respect to some metric. To
allow for contraction with respect to more general metrics in the data space,
we train diffeomorphisms between the data space and a latent space and enforce
contractivity in the latent space, which ensures global contractivity in the
data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D
LASA datasets.</div><div><a href='http://arxiv.org/abs/2402.08090v2'>2402.08090v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04118v1")'>Globally Stable Neural Imitation Policies</div>
<div id='2403.04118v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T00:20:11Z</div><div>Authors: Amin Abyaneh, Mariana Sosa Guzmán, Hsiu-Chin Lin</div><div style='padding-top: 10px; width: 80ex'>Imitation learning presents an effective approach to alleviate the
resource-intensive and time-consuming nature of policy learning from scratch in
the solution space. Even though the resulting policy can mimic expert
demonstrations reliably, it often lacks predictability in unexplored regions of
the state-space, giving rise to significant safety concerns in the face of
perturbations. To address these challenges, we introduce the Stable Neural
Dynamical System (SNDS), an imitation learning regime which produces a policy
with formal stability guarantees. We deploy a neural policy architecture that
facilitates the representation of stability based on Lyapunov theorem, and
jointly train the policy and its corresponding Lyapunov candidate to ensure
global stability. We validate our approach by conducting extensive experiments
in simulation and successfully deploying the trained policies on a real-world
manipulator arm. The experimental results demonstrate that our method overcomes
the instability, accuracy, and computational intensity problems associated with
previous imitation learning methods, making our method a promising solution for
stable policy learning in complex planning scenarios.</div><div><a href='http://arxiv.org/abs/2403.04118v1'>2403.04118v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16719v2")'>OptiState: State Estimation of Legged Robots using Gated Networks with
  Transformer-based Vision and Kalman Filtering</div>
<div id='2401.16719v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T03:34:25Z</div><div>Authors: Alexander Schperberg, Yusuke Tanaka, Saviz Mowlavi, Feng Xu, Bharathan Balaji, Dennis Hong</div><div style='padding-top: 10px; width: 80ex'>State estimation for legged robots is challenging due to their highly dynamic
motion and limitations imposed by sensor accuracy. By integrating Kalman
filtering, optimization, and learning-based modalities, we propose a hybrid
solution that combines proprioception and exteroceptive information for
estimating the state of the robot's trunk. Leveraging joint encoder and IMU
measurements, our Kalman filter is enhanced through a single-rigid body model
that incorporates ground reaction force control outputs from convex Model
Predictive Control optimization. The estimation is further refined through
Gated Recurrent Units, which also considers semantic insights and robot height
from a Vision Transformer autoencoder applied on depth images. This framework
not only furnishes accurate robot state estimates, including uncertainty
evaluations, but can minimize the nonlinear errors that arise from sensor
measurements and model simplifications through learning. The proposed
methodology is evaluated in hardware using a quadruped robot on various
terrains, yielding a 65% improvement on the Root Mean Squared Error compared to
our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState</div><div><a href='http://arxiv.org/abs/2401.16719v2'>2401.16719v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09077v1")'>DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning
  of Gough-Stewart Platform</div>
<div id='2402.09077v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:40:09Z</div><div>Authors: Huizhi Zhu, Wenxia Xu, Jian Huang, Jiaxin Li</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a graph neural network, DisGNet, for learning the
graph distance matrix to address the forward kinematics problem of the
Gough-Stewart platform. DisGNet employs the k-FWL algorithm for
message-passing, providing high expressiveness with a small parameter count,
making it suitable for practical deployment. Additionally, we introduce the
GPU-friendly Newton-Raphson method, an efficient parallelized optimization
method executed on the GPU to refine DisGNet's output poses, achieving
ultra-high-precision pose. This novel two-stage approach delivers ultra-high
precision output while meeting real-time requirements. Our results indicate
that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at
79.8\% and 98.2\%, respectively. As executed on a GPU, our two-stage method can
ensure the requirement for real-time computation. Codes are released at
https://github.com/FLAMEZZ5201/DisGNet.</div><div><a href='http://arxiv.org/abs/2402.09077v1'>2402.09077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13695v1")'>Loss Regularizing Robotic Terrain Classification</div>
<div id='2403.13695v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T15:57:44Z</div><div>Authors: Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi Jha, Suddhasil De</div><div style='padding-top: 10px; width: 80ex'>Locomotion mechanics of legged robots are suitable when pacing through
difficult terrains. Recognising terrains for such robots are important to fully
yoke the versatility of their movements. Consequently, robotic terrain
classification becomes significant to classify terrains in real time with high
accuracy. The conventional classifiers suffer from overfitting problem, low
accuracy problem, high variance problem, and not suitable for live dataset. On
the other hand, classifying a growing dataset is difficult for convolution
based terrain classification. Supervised recurrent models are also not
practical for this classification. Further, the existing recurrent
architectures are still evolving to improve accuracy of terrain classification
based on live variable-length sensory data collected from legged robots. This
paper proposes a new semi-supervised method for terrain classification of
legged robots, avoiding preprocessing of long variable-length dataset. The
proposed method has a stacked Long Short-Term Memory architecture, including a
new loss regularization. The proposed method solves the existing problems and
improves accuracy. Comparison with the existing architectures show the
improvements.</div><div><a href='http://arxiv.org/abs/2403.13695v1'>2403.13695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03156v1")'>DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for
  Blind Person Navigation</div>
<div id='2402.03156v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:24:12Z</div><div>Authors: Artem Bazhenov, Vladimir Berman, Sergei Satsevich, Olga Shalopanova, Miguel Altamirano Cabrera, Artem Lykov, Dzmitry Tsetserukou</div><div style='padding-top: 10px; width: 80ex'>This paper introduces DogSurf - a newapproach of using quadruped robots to
help visually impaired people navigate in real world. The presented method
allows the quadruped robot to detect slippery surfaces, and to use audio and
haptic feedback to inform the user when to stop. A state-of-the-art GRU-based
neural network architecture with mean accuracy of 99.925% was proposed for the
task of multiclass surface classification for quadruped robots. A dataset was
collected on a Unitree Go1 Edu robot. The dataset and code have been posted to
the public domain.</div><div><a href='http://arxiv.org/abs/2402.03156v1'>2402.03156v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11728v1")'>PITA: Physics-Informed Trajectory Autoencoder</div>
<div id='2403.11728v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T12:37:41Z</div><div>Authors: Johannes Fischer, Kevin Rösch, Martin Lauer, Christoph Stiller</div><div style='padding-top: 10px; width: 80ex'>Validating robotic systems in safety-critical appli-cations requires testing
in many scenarios including rare edgecases that are unlikely to occur,
requiring to complement real-world testing with testing in simulation.
Generative models canbe used to augment real-world datasets with generated data
toproduce edge case scenarios by sampling in a learned latentspace.
Autoencoders can learn said latent representation for aspecific domain by
learning to reconstruct the input data froma lower-dimensional intermediate
representation. However, theresulting trajectories are not necessarily
physically plausible, butinstead typically contain noise that is not present in
the inputtrajectory. To resolve this issue, we propose the novel
Physics-Informed Trajectory Autoencoder (PITA) architecture, whichincorporates
a physical dynamics model into the loss functionof the autoencoder. This
results in smooth trajectories that notonly reconstruct the input trajectory
but also adhere to thephysical model. We evaluate PITA on a real-world dataset
ofvehicle trajectories and compare its performance to a normalautoencoder and a
state-of-the-art action-space autoencoder.</div><div><a href='http://arxiv.org/abs/2403.11728v1'>2403.11728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05765v1")'>Physics-informed Neural Motion Planning on Constraint Manifolds</div>
<div id='2403.05765v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T02:24:02Z</div><div>Authors: Ruiqi Ni, Ahmed H. Qureshi</div><div style='padding-top: 10px; width: 80ex'>Constrained Motion Planning (CMP) aims to find a collision-free path between
the given start and goal configurations on the kinematic constraint manifolds.
These problems appear in various scenarios ranging from object manipulation to
legged-robot locomotion. However, the zero-volume nature of manifolds makes the
CMP problem challenging, and the state-of-the-art methods still take several
seconds to find a path and require a computationally expansive path dataset for
imitation learning. Recently, physics-informed motion planning methods have
emerged that directly solve the Eikonal equation through neural networks for
motion planning and do not require expert demonstrations for learning. Inspired
by these approaches, we propose the first physics-informed CMP framework that
solves the Eikonal equation on the constraint manifolds and trains neural
function for CMP without expert data. Our results show that the proposed
approach efficiently solves various CMP problems in both simulation and
real-world, including object manipulation under orientation constraints and
door opening with a high-dimensional 6-DOF robot manipulator. In these complex
settings, our method exhibits high success rates and finds paths in
sub-seconds, which is many times faster than the state-of-the-art CMP methods.</div><div><a href='http://arxiv.org/abs/2403.05765v1'>2403.05765v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15281v3")'>Neural Implicit Swept Volume Models for Fast Collision Detection</div>
<div id='2402.15281v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:06:48Z</div><div>Authors: Dominik Joho, Jonas Schwinn, Kirill Safronov</div><div style='padding-top: 10px; width: 80ex'>Collision detection is one of the most time-consuming operations during
motion planning. Thus, there is an increasing interest in exploring machine
learning techniques to speed up collision detection and sampling-based motion
planning. A recent line of research focuses on utilizing neural signed distance
functions of either the robot geometry or the swept volume of the robot motion.
Building on this, we present a novel neural implicit swept volume model to
continuously represent arbitrary motions parameterized by their start and goal
configurations. This allows to quickly compute signed distances for any point
in the task space to the robot motion. Further, we present an algorithm
combining the speed of the deep learning-based signed distance computations
with the strong accuracy guarantees of geometric collision checkers. We
validate our approach in simulated and real-world robotic experiments, and
demonstrate that it is able to speed up a commercial bin picking application.</div><div><a href='http://arxiv.org/abs/2402.15281v3'>2402.15281v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05571v1")'>Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with
  Constrained Diffusion Model</div>
<div id='2403.05571v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T03:52:17Z</div><div>Authors: Anjian Li, Zihan Ding, Adji Bousso Dieng, Ryne Beeson</div><div style='padding-top: 10px; width: 80ex'>Trajectory optimization in robotics poses a challenging non-convex problem
due to complex dynamics and environmental settings. Traditional numerical
optimization methods are time-consuming in finding feasible solutions, whereas
data-driven approaches lack safety guarantees for the output trajectories. In
this paper, we introduce a general and fully parallelizable framework that
combines diffusion models and numerical solvers for non-convex trajectory
optimization, ensuring both computational efficiency and constraint
satisfaction. A novel constrained diffusion model is proposed with an
additional constraint violation loss for training. It aims to approximate the
distribution of locally optimal solutions while minimizing constraint
violations during sampling. The samples are then used as initial guesses for a
numerical solver to refine and derive final solutions with formal verification
of feasibility and optimality. Experimental evaluations on three tasks over
different robotics domains verify the improved constraint satisfaction and
computational efficiency with 4$\times$ to 22$\times$ acceleration using our
proposed method, which generalizes across trajectory optimization problems and
scales well with problem complexity.</div><div><a href='http://arxiv.org/abs/2403.05571v1'>2403.05571v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02338v1")'>Twisting Lids Off with Two Hands</div>
<div id='2403.02338v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T18:59:30Z</div><div>Authors: Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, Jitendra Malik</div><div style='padding-top: 10px; width: 80ex'>Manipulating objects with two multi-fingered hands has been a long-standing
challenge in robotics, attributed to the contact-rich nature of many
manipulation tasks and the complexity inherent in coordinating a
high-dimensional bimanual system. In this work, we consider the problem of
twisting lids of various bottle-like objects with two hands, and demonstrate
that policies trained in simulation using deep reinforcement learning can be
effectively transferred to the real world. With novel engineering insights into
physical modeling, real-time perception, and reward design, the policy
demonstrates generalization capabilities across a diverse set of unseen
objects, showcasing dynamic and dexterous behaviors. Our findings serve as
compelling evidence that deep reinforcement learning combined with sim-to-real
transfer remains a promising approach for addressing manipulation problems of
unprecedented complexity.</div><div><a href='http://arxiv.org/abs/2403.02338v1'>2403.02338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15038v1")'>Dynamics-Guided Diffusion Model for Robot Manipulator Design</div>
<div id='2402.15038v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T01:19:30Z</div><div>Authors: Xiaomeng Xu, Huy Ha, Shuran Song</div><div style='padding-top: 10px; width: 80ex'>We present Dynamics-Guided Diffusion Model, a data-driven framework for
generating manipulator geometry designs for a given manipulation task. Instead
of training different design models for each task, our approach employs a
learned dynamics network shared across tasks. For a new manipulation task, we
first decompose it into a collection of individual motion targets which we call
target interaction profile, where each individual motion can be modeled by the
shared dynamics network. The design objective constructed from the target and
predicted interaction profiles provides a gradient to guide the refinement of
finger geometry for the task. This refinement process is executed as a
classifier-guided diffusion process, where the design objective acts as the
classifier guidance. We evaluate our framework on various manipulation tasks,
under the sensor-less setting using only an open-loop parallel jaw motion. Our
generated designs outperform optimization-based and unguided diffusion
baselines relatively by 31.5% and 45.3% on average manipulation success rate.
With the ability to generate a design within 0.8 seconds, our framework could
facilitate rapid design iteration and enhance the adoption of data-driven
approaches for robotic mechanism design.</div><div><a href='http://arxiv.org/abs/2402.15038v1'>2402.15038v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12861v1")'>D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous
  Deformable Manipulation</div>
<div id='2403.12861v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T16:05:51Z</div><div>Authors: Jun Yamada, Shaohong Zhong, Jack Collins, Ingmar Posner</div><div style='padding-top: 10px; width: 80ex'>Mastering dexterous robotic manipulation of deformable objects is vital for
overcoming the limitations of parallel grippers in real-world applications.
Current trajectory optimisation approaches often struggle to solve such tasks
due to the large search space and the limited task information available from a
cost function. In this work, we propose D-Cubed, a novel trajectory
optimisation method using a latent diffusion model (LDM) trained from a
task-agnostic play dataset to solve dexterous deformable object manipulation
tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions
in the play dataset using a VAE and trains a LDM to compose the skill latents
into a skill trajectory, representing a long-horizon action trajectory in the
dataset. To optimise a trajectory for a target task, we introduce a novel
gradient-free guided sampling method that employs the Cross-Entropy method
within the reverse diffusion process. In particular, D-Cubed samples a small
number of noisy skill trajectories using the LDM for exploration and evaluates
the trajectories in simulation. Then, D-Cubed selects the trajectory with the
lowest cost for the subsequent reverse process. This effectively explores
promising solution areas and optimises the sampled trajectories towards a
target task throughout the reverse diffusion process. Through empirical
evaluation on a public benchmark of dexterous deformable object manipulation
tasks, we demonstrate that D-Cubed outperforms traditional trajectory
optimisation and competitive baseline approaches by a significant margin. We
further demonstrate that trajectories found by D-Cubed readily transfer to a
real-world LEAP hand on a folding task.</div><div><a href='http://arxiv.org/abs/2403.12861v1'>2403.12861v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02989v1")'>DexDiffuser: Generating Dexterous Grasps with Diffusion Models</div>
<div id='2402.02989v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:27:41Z</div><div>Authors: Zehang Weng, Haofei Lu, Danica Kragic, Jens Lundell</div><div style='padding-top: 10px; width: 80ex'>We introduce DexDiffuser, a novel dexterous grasping method that generates,
evaluates, and refines grasps on partial object point clouds. DexDiffuser
includes the conditional diffusion-based grasp sampler DexSampler and the
dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality
grasps conditioned on object point clouds by iterative denoising of randomly
sampled grasps. We also introduce two grasp refinement strategies:
Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).
Our simulation and real-world experiments on the Allegro Hand consistently
demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger
grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp
success rate.</div><div><a href='http://arxiv.org/abs/2402.02989v1'>2402.02989v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06173v1")'>Speeding up 6-DoF Grasp Sampling with Quality-Diversity</div>
<div id='2403.06173v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T10:58:54Z</div><div>Authors: Johann Huber, François Hélénon, Mathilde Kappel, Elie Chelly, Mahdi Khoramshahi, Faïz Ben Amar, Stéphane Doncieux</div><div style='padding-top: 10px; width: 80ex'>Recent advances in AI have led to significant results in robotic learning,
including natural language-conditioned planning and efficient optimization of
controllers using generative models. However, the interaction data remains the
bottleneck for generalization. Getting data for grasping is a critical
challenge, as this skill is required to complete many manipulation tasks.
Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse,
high-performing solutions to a given problem. This paper investigates how QD
can be combined with priors to speed up the generation of diverse grasps poses
in simulation compared to standard 6-DoF grasp sampling schemes. Experiments
conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD
outperforms commonly used methods by a large margin. Further experiments show
that QD optimization automatically finds some efficient priors that are usually
hard coded. The deployment of generated grasps on a 2-finger gripper and an
Allegro hand shows that the diversity produced maintains sim-to-real
transferability. We believe these results to be a significant step toward the
generation of large datasets that can lead to robust and generalizing robotic
grasping policies.</div><div><a href='http://arxiv.org/abs/2403.06173v1'>2403.06173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03890v1")'>Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic
  Manipulation</div>
<div id='2403.03890v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:50:26Z</div><div>Authors: Xiao Ma, Sumit Patidar, Iain Haughton, Stephen James</div><div style='padding-top: 10px; width: 80ex'>This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical
agent for multi-task robotic manipulation. HDP factorises a manipulation policy
into a hierarchical structure: a high-level task-planning agent which predicts
a distant next-best end-effector pose (NBP), and a low-level goal-conditioned
diffusion policy which generates optimal motion trajectories. The factorised
policy representation allows HDP to tackle both long-horizon task planning
while generating fine-grained low-level actions. To generate context-aware
motion trajectories while satisfying robot kinematics constraints, we present a
novel kinematics-aware goal-conditioned control agent, Robot Kinematics
Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the
end-effector pose and joint position trajectories, and distill the accurate but
kinematics-unaware end-effector pose diffuser to the kinematics-aware but less
accurate joint position diffuser via differentiable kinematics. Empirically, we
show that HDP achieves a significantly higher success rate than the
state-of-the-art methods in both simulation and real-world.</div><div><a href='http://arxiv.org/abs/2403.03890v1'>2403.03890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13634v1")'>Learning Dual-arm Object Rearrangement for Cartesian Robots</div>
<div id='2402.13634v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:13:08Z</div><div>Authors: Shishun Zhang, Qijin She, Wenhao Li, Chenyang Zhu, Yongjun Wang, Ruizhen Hu, Kai Xu</div><div style='padding-top: 10px; width: 80ex'>This work focuses on the dual-arm object rearrangement problem abstracted
from a realistic industrial scenario of Cartesian robots. The goal of this
problem is to transfer all the objects from sources to targets with the minimum
total completion time. To achieve the goal, the core idea is to develop an
effective object-to-arm task assignment strategy for minimizing the cumulative
task execution time and maximizing the dual-arm cooperation efficiency. One of
the difficulties in the task assignment is the scalability problem. As the
number of objects increases, the computation time of traditional
offline-search-based methods grows strongly for computational complexity.
Encouraged by the adaptability of reinforcement learning (RL) in long-sequence
task decisions, we propose an online task assignment decision method based on
RL, and the computation time of our method only increases linearly with the
number of objects. Further, we design an attention-based network to model the
dependencies between the input states during the whole task execution process
to help find the most reasonable object-to-arm correspondence in each task
assignment round. In the experimental part, we adapt some search-based methods
to this specific setting and compare our method with them. Experimental result
shows that our approach achieves outperformance over search-based methods in
total execution time and computational efficiency, and also verifies the
generalization of our method to different numbers of objects. In addition, we
show the effectiveness of our method deployed on the real robot in the
supplementary video.</div><div><a href='http://arxiv.org/abs/2402.13634v1'>2402.13634v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13231v2")'>DittoGym: Learning to Control Soft Shape-Shifting Robots</div>
<div id='2401.13231v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T05:03:05Z</div><div>Authors: Suning Huang, Boyuan Chen, Huazhe Xu, Vincent Sitzmann</div><div style='padding-top: 10px; width: 80ex'>Robot co-design, where the morphology of a robot is optimized jointly with a
learned policy to solve a specific task, is an emerging area of research. It
holds particular promise for soft robots, which are amenable to novel
manufacturing techniques that can realize learned morphologies and actuators.
Inspired by nature and recent novel robot designs, we propose to go a step
further and explore the novel reconfigurable robots, defined as robots that can
change their morphology within their lifetime. We formalize control of
reconfigurable soft robots as a high-dimensional reinforcement learning (RL)
problem. We unify morphology change, locomotion, and environment interaction in
the same action space, and introduce an appropriate, coarse-to-fine curriculum
that enables us to discover policies that accomplish fine-grained control of
the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark
for reconfigurable soft robots that require fine-grained morphology changes to
accomplish the tasks. Finally, we evaluate our proposed coarse-to-fine
algorithm on DittoGym and demonstrate robots that learn to change their
morphology several times within a sequence, uniquely enabled by our RL
algorithm. More results are available at https://dittogym.github.io.</div><div><a href='http://arxiv.org/abs/2401.13231v2'>2401.13231v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18002v1")'>Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial
  Observability with a Soft Wrist</div>
<div id='2402.18002v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T02:30:59Z</div><div>Authors: Hai Nguyen, Tadashi Kozuno, Cristian C. Beltran-Hernandez, Masashi Hamaya</div><div style='padding-top: 10px; width: 80ex'>This study tackles the representative yet challenging contact-rich
peg-in-hole task of robotic assembly, using a soft wrist that can operate more
safely and tolerate lower-frequency control signals than a rigid one. Previous
studies often use a fully observable formulation, requiring external setups or
estimators for the peg-to-hole pose. In contrast, we use a partially observable
formulation and deep reinforcement learning from demonstrations to learn a
memory-based agent that acts purely on haptic and proprioceptive signals.
Moreover, previous works do not incorporate potential domain symmetry and thus
must search for solutions in a bigger space. Instead, we propose to leverage
the symmetry for sample efficiency by augmenting the training data and
constructing auxiliary losses to force the agent to adhere to the symmetry.
Results in simulation with five different symmetric peg shapes show that our
proposed agent can be comparable to or even outperform a state-based agent. In
particular, the sample efficiency also allows us to learn directly on the real
robot within 3 hours.</div><div><a href='http://arxiv.org/abs/2402.18002v1'>2402.18002v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15402v1")'>Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy
  Structure Prior</div>
<div id='2402.15402v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:05:51Z</div><div>Authors: Kechun Xu, Zhongxiang Zhou, Jun Wu, Haojian Lu, Rong Xiong, Yue Wang</div><div style='padding-top: 10px; width: 80ex'>We focus on the task of unknown object rearrangement, where a robot is
supposed to re-configure the objects into a desired goal configuration
specified by an RGB-D image. Recent works explore unknown object rearrangement
systems by incorporating learning-based perception modules. However, they are
sensitive to perception error, and pay less attention to task-level
performance. In this paper, we aim to develop an effective system for unknown
object rearrangement amidst perception noise. We theoretically reveal the noisy
perception impacts grasp and place in a decoupled way, and show such a
decoupled structure is non-trivial to improve task optimality. We propose GSP,
a dual-loop system with the decoupled structure as prior. For the inner loop,
we learn an active seeing policy for self-confident object matching to improve
the perception of place. For the outer loop, we learn a grasp policy aware of
object matching and grasp capability guided by task-level rewards. We leverage
the foundation model CLIP for object matching, policy learning and
self-termination. A series of experiments indicate that GSP can conduct unknown
object rearrangement with higher completion rate and less steps.</div><div><a href='http://arxiv.org/abs/2402.15402v1'>2402.15402v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10187v1")'>Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning
  with Instance Segmentation to Grasp Arbitrary Objects</div>
<div id='2403.10187v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T10:48:16Z</div><div>Authors: Malte Mosbach, Sven Behnke</div><div style='padding-top: 10px; width: 80ex'>Interactive grasping from clutter, akin to human dexterity, is one of the
longest-standing problems in robot learning. Challenges stem from the
intricacies of visual perception, the demand for precise motor skills, and the
complex interplay between the two. In this work, we present Teacher-Augmented
Policy Gradient (TAPG), a novel two-stage learning framework that synergizes
reinforcement learning and policy distillation. After training a teacher policy
to master the motor control based on object pose information, TAPG facilitates
guided, yet adaptive, learning of a sensorimotor policy, based on object
segmentation. We zero-shot transfer from simulation to a real robot by using
Segment Anything Model for promptable object segmentation. Our trained policies
adeptly grasp a wide variety of objects from cluttered scenarios in simulation
and the real world based on human-understandable prompts. Furthermore, we show
robust zero-shot transfer to novel objects. Videos of our experiments are
available at \url{https://maltemosbach.github.io/grasp_anything}.</div><div><a href='http://arxiv.org/abs/2403.10187v1'>2403.10187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13201v1")'>Tiny Reinforcement Learning for Quadruped Locomotion using Decision
  Transformers</div>
<div id='2402.13201v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:10:39Z</div><div>Authors: Orhan Eren Akgün, Néstor Cuevas, Matheus Farias, Daniel Garces</div><div style='padding-top: 10px; width: 80ex'>Resource-constrained robotic platforms are particularly useful for tasks that
require low-cost hardware alternatives due to the risk of losing the robot,
like in search-and-rescue applications, or the need for a large number of
devices, like in swarm robotics. For this reason, it is crucial to find
mechanisms for adapting reinforcement learning techniques to the constraints
imposed by lower computational power and smaller memory capacities of these
ultra low-cost robotic platforms. We try to address this need by proposing a
method for making imitation learning deployable onto resource-constrained
robotic platforms. Here we cast the imitation learning problem as a conditional
sequence modeling task and we train a decision transformer using expert
demonstrations augmented with a custom reward. Then, we compress the resulting
generative model using software optimization schemes, including quantization
and pruning. We test our method in simulation using Isaac Gym, a realistic
physics simulation environment designed for reinforcement learning. We
empirically demonstrate that our method achieves natural looking gaits for
Bittle, a resource-constrained quadruped robot. We also run multiple
simulations to show the effects of pruning and quantization on the performance
of the model. Our results show that quantization (down to 4 bits) and pruning
reduce model size by around 30\% while maintaining a competitive reward, making
the model deployable in a resource-constrained system.</div><div><a href='http://arxiv.org/abs/2402.13201v1'>2402.13201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10506v1")'>HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion
  and Manipulation</div>
<div id='2403.10506v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:45:44Z</div><div>Authors: Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter Abbeel</div><div style='padding-top: 10px; width: 80ex'>Humanoid robots hold great promise in assisting humans in diverse
environments and tasks, due to their flexibility and adaptability leveraging
human-like morphology. However, research in humanoid robots is often
bottlenecked by the costly and fragile hardware setups. To accelerate
algorithmic research in humanoid robots, we present a high-dimensional,
simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot
equipped with dexterous hands and a variety of challenging whole-body
manipulation and locomotion tasks. Our findings reveal that state-of-the-art
reinforcement learning algorithms struggle with most tasks, whereas a
hierarchical learning baseline achieves superior performance when supported by
robust low-level policies, such as walking or reaching. With HumanoidBench, we
provide the robotics community with a platform to identify the challenges
arising when solving diverse tasks with humanoid robots, facilitating prompt
verification of algorithms and ideas. The open-source code is available at
https://sferrazza.cc/humanoidbench_site.</div><div><a href='http://arxiv.org/abs/2403.10506v1'>2403.10506v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16796v2")'>Expressive Whole-Body Control for Humanoid Robots</div>
<div id='2402.16796v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:09:24Z</div><div>Authors: Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong Wang</div><div style='padding-top: 10px; width: 80ex'>Can we enable humanoid robots to generate rich, diverse, and expressive
motions in the real world? We propose to learn a whole-body control policy on a
human-sized robot to mimic human motions as realistic as possible. To train
such a policy, we leverage the large-scale human motion capture data from the
graphics community in a Reinforcement Learning framework. However, directly
performing imitation learning with the motion capture dataset would not work on
the real humanoid robot, given the large gap in degrees of freedom and physical
capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this
problem by encouraging the upper humanoid body to imitate a reference motion,
while relaxing the imitation constraint on its two legs and only requiring them
to follow a given velocity robustly. With training in simulation and Sim2Real
transfer, our policy can control a humanoid robot to walk in different styles,
shake hands with humans, and even dance with a human in the real world. We
conduct extensive studies and comparisons on diverse motions in both simulation
and the real world to show the effectiveness of our approach.</div><div><a href='http://arxiv.org/abs/2402.16796v2'>2402.16796v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04436v1")'>Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation</div>
<div id='2403.04436v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T12:10:41Z</div><div>Authors: Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi</div><div style='padding-top: 10px; width: 80ex'>We present Human to Humanoid (H2O), a reinforcement learning (RL) based
framework that enables real-time whole-body teleoperation of a full-sized
humanoid robot with only an RGB camera. To create a large-scale retargeted
motion dataset of human movements for humanoid robots, we propose a scalable
"sim-to-data" process to filter and pick feasible motions using a privileged
motion imitator. Afterwards, we train a robust real-time humanoid motion
imitator in simulation using these refined motions and transfer it to the real
humanoid robot in a zero-shot manner. We successfully achieve teleoperation of
dynamic whole-body motions in real-world scenarios, including walking, back
jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our
knowledge, this is the first demonstration to achieve learning-based real-time
whole-body humanoid teleoperation.</div><div><a href='http://arxiv.org/abs/2403.04436v1'>2403.04436v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06966v1")'>Acquiring Diverse Skills using Curriculum Reinforcement Learning with
  Mixture of Experts</div>
<div id='2403.06966v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:49:18Z</div><div>Authors: Onur Celik, Aleksandar Taranovic, Gerhard Neumann</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) is a powerful approach for acquiring a
good-performing policy. However, learning diverse skills is challenging in RL
due to the commonly used Gaussian policy parameterization. We propose
\textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for
learning diverse skills using Mixture of Experts, where each expert formalizes
a skill as a contextual motion primitive. Di-SkilL optimizes each expert and
its associate context distribution to a maximum entropy objective that
incentivizes learning diverse skills in similar contexts. The per-expert
context distribution enables automatic curricula learning, allowing each expert
to focus on its best-performing sub-region of the context space. To overcome
hard discontinuities and multi-modalities without any prior knowledge of the
environment's unknown context probability space, we leverage energy-based
models to represent the per-expert context distributions and demonstrate how we
can efficiently train them using the standard policy gradient objective. We
show on challenging robot simulation tasks that Di-SkilL can learn diverse and
performant skills.</div><div><a href='http://arxiv.org/abs/2403.06966v1'>2403.06966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02904v1")'>Replication of Impedance Identification Experiments on a
  Reinforcement-Learning-Controlled Digital Twin of Human Elbows</div>
<div id='2402.02904v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:16:32Z</div><div>Authors: Hao Yu, Zebin Huang, Qingbo Liu, Ignacio Carlucho, Mustafa Suphi Erden</div><div style='padding-top: 10px; width: 80ex'>This study presents a pioneering effort to replicate human neuromechanical
experiments within a virtual environment utilising a digital human model. By
employing MyoSuite, a state-of-the-art human motion simulation platform
enhanced by Reinforcement Learning (RL), multiple types of impedance
identification experiments of human elbow were replicated on a musculoskeletal
model. We compared the elbow movement controlled by an RL agent with the motion
of an actual human elbow in terms of the impedance identified in
torque-perturbation experiments. The findings reveal that the RL agent exhibits
higher elbow impedance to stabilise the target elbow motion under perturbation
than a human does, likely due to its shorter reaction time and superior sensory
capabilities. This study serves as a preliminary exploration into the potential
of virtual environment simulations for neuromechanical research, offering an
initial yet promising alternative to conventional experimental approaches. An
RL-controlled digital twin with complete musculoskeletal models of the human
body is expected to be useful in designing experiments and validating
rehabilitation theory before experiments on real human subjects.</div><div><a href='http://arxiv.org/abs/2402.02904v1'>2402.02904v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17032v1")'>M2CURL: Sample-Efficient Multimodal Reinforcement Learning via
  Self-Supervised Representation Learning for Robotic Manipulation</div>
<div id='2401.17032v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T14:09:35Z</div><div>Authors: Fotios Lygerakis, Vedant Dave, Elmar Rueckert</div><div style='padding-top: 10px; width: 80ex'>One of the most critical aspects of multimodal Reinforcement Learning (RL) is
the effective integration of different observation modalities. Having robust
and accurate representations derived from these modalities is key to enhancing
the robustness and sample efficiency of RL algorithms. However, learning
representations in RL settings for visuotactile data poses significant
challenges, particularly due to the high dimensionality of the data and the
complexity involved in correlating visual and tactile inputs with the dynamic
environment and task objectives. To address these challenges, we propose
Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our
approach employs a novel multimodal self-supervised learning technique that
learns efficient representations and contributes to faster convergence of RL
algorithms. Our method is agnostic to the RL algorithm, thus enabling its
integration with any available RL algorithm. We evaluate M2CURL on the Tactile
Gym 2 simulator and we show that it significantly enhances the learning
efficiency in different manipulation tasks. This is evidenced by faster
convergence rates and higher cumulative rewards per episode, compared to
standard RL algorithms without our representation learning approach.</div><div><a href='http://arxiv.org/abs/2401.17032v1'>2401.17032v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03848v1")'>Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement
  Learning</div>
<div id='2403.03848v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:49:08Z</div><div>Authors: Zifan Xu, Amir Hossain Raj, Xuesu Xiao, Peter Stone</div><div style='padding-top: 10px; width: 80ex'>Recent advances of locomotion controllers utilizing deep reinforcement
learning (RL) have yielded impressive results in terms of achieving rapid and
robust locomotion across challenging terrain, such as rugged rocks, non-rigid
ground, and slippery surfaces. However, while these controllers primarily
address challenges underneath the robot, relatively little research has
investigated legged mobility through confined 3D spaces, such as narrow tunnels
or irregular voids, which impose all-around constraints. The cyclic gait
patterns resulted from existing RL-based methods to learn parameterized
locomotion skills characterized by motion parameters, such as velocity and body
height, may not be adequate to navigate robots through challenging confined 3D
spaces, requiring both agile 3D obstacle avoidance and robust legged
locomotion. Instead, we propose to learn locomotion skills end-to-end from
goal-oriented navigation in confined 3D spaces. To address the inefficiency of
tracking distant navigation goals, we introduce a hierarchical locomotion
controller that combines a classical planner tasked with planning waypoints to
reach a faraway global goal location, and an RL-based policy trained to follow
these waypoints by generating low-level motion commands. This approach allows
the policy to explore its own locomotion skills within the entire solution
space and facilitates smooth transitions between local goals, enabling
long-term navigation towards distant goals. In simulation, our hierarchical
approach succeeds at navigating through demanding confined 3D environments,
outperforming both pure end-to-end learning approaches and parameterized
locomotion skills. We further demonstrate the successful real-world deployment
of our simulation-trained controller on a real robot.</div><div><a href='http://arxiv.org/abs/2403.03848v1'>2403.03848v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17583v1")'>Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion</div>
<div id='2401.17583v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T03:58:28Z</div><div>Authors: Tairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, Guanya Shi</div><div style='padding-top: 10px; width: 80ex'>Legged robots navigating cluttered environments must be jointly agile for
efficient task execution and safe to avoid collisions with obstacles or humans.
Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure
safety, or focus on agility without considering potentially fatal collisions.
This paper introduces Agile But Safe (ABS), a learning-based control framework
that enables agile and collision-free locomotion for quadrupedal robots. ABS
involves an agile policy to execute agile motor skills amidst obstacles and a
recovery policy to prevent failures, collaboratively achieving high-speed and
collision-free navigation. The policy switch in ABS is governed by a learned
control-theoretic reach-avoid value network, which also guides the recovery
policy as an objective function, thereby safeguarding the robot in a closed
loop. The training process involves the learning of the agile policy, the
reach-avoid value network, the recovery policy, and an exteroception
representation network, all in simulation. These trained modules can be
directly deployed in the real world with onboard sensing and computation,
leading to high-speed and collision-free navigation in confined indoor and
outdoor spaces with both static and dynamic obstacles.</div><div><a href='http://arxiv.org/abs/2401.17583v1'>2401.17583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01617v1")'>A GP-based Robust Motion Planning Framework for Agile Autonomous Robot
  Navigation and Recovery in Unknown Environments</div>
<div id='2402.01617v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:27:21Z</div><div>Authors: Nicholas Mohammad, Jacob Higgins, Nicola Bezzo</div><div style='padding-top: 10px; width: 80ex'>For autonomous mobile robots, uncertainties in the environment and system
model can lead to failure in the motion planning pipeline, resulting in
potential collisions. In order to achieve a high level of robust autonomy,
these robots should be able to proactively predict and recover from such
failures. To this end, we propose a Gaussian Process (GP) based model for
proactively detecting the risk of future motion planning failure. When this
risk exceeds a certain threshold, a recovery behavior is triggered that
leverages the same GP model to find a safe state from which the robot may
continue towards the goal. The proposed approach is trained in simulation only
and can generalize to real world environments on different robotic platforms.
Simulations and physical experiments demonstrate that our framework is capable
of both predicting planner failures and recovering the robot to states where
planner success is likely, all while producing agile motion.</div><div><a href='http://arxiv.org/abs/2402.01617v1'>2402.01617v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04586v1")'>Learning Agility Adaptation for Flight in Clutter</div>
<div id='2403.04586v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T15:30:54Z</div><div>Authors: Guangyu Zhao, Tianyue Wu, Yeke Chen, Fei Gao</div><div style='padding-top: 10px; width: 80ex'>Animals learn to adapt agility of their movements to their capabilities and
the environment they operate in. Mobile robots should also demonstrate this
ability to combine agility and safety. The aim of this work is to endow flight
vehicles with the ability of agility adaptation in prior unknown and partially
observable cluttered environments. We propose a hierarchical learning and
planning framework where we utilize both trial and error to comprehensively
learn an agility policy with the vehicle's observation as the input, and
well-established methods of model-based trajectory generation. Technically, we
use online model-free reinforcement learning and a pre-training-fine-tuning
reward scheme to obtain the deployable policy. The statistical results in
simulation demonstrate the advantages of our method over the constant agility
baselines and an alternative method in terms of flight efficiency and safety.
In particular, the policy leads to intelligent behaviors, such as perception
awareness, which distinguish it from other approaches. By deploying the policy
to hardware, we verify that these advantages can be brought to the real world.</div><div><a href='http://arxiv.org/abs/2403.04586v1'>2403.04586v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15283v1")'>When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination</div>
<div id='2402.15283v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:27:48Z</div><div>Authors: Martin Benfeghoul, Umais Zahid, Qinghai Guo, Zafeirios Fountas</div><div style='padding-top: 10px; width: 80ex'>In an unfamiliar setting, a model-based reinforcement learning agent can be
limited by the accuracy of its world model. In this work, we present a novel,
training-free approach to improving the performance of such agents separately
from planning and learning. We do so by applying iterative inference at
decision-time, to fine-tune the inferred agent states based on the coherence of
future state representations. Our approach achieves a consistent improvement in
both reconstruction accuracy and task performance when applied to visual 3D
navigation tasks. We go on to show that considering more future states further
improves the performance of the agent in partially-observable environments, but
not in a fully-observable one. Finally, we demonstrate that agents with less
training pre-evaluation benefit most from our approach.</div><div><a href='http://arxiv.org/abs/2402.15283v1'>2402.15283v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12498v1")'>Feudal Networks for Visual Navigation</div>
<div id='2402.12498v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:05:41Z</div><div>Authors: Faith Johnson, Bryan Bo Cao, Kristin Dana, Shubham Jain, Ashwin Ashok</div><div style='padding-top: 10px; width: 80ex'>Visual navigation follows the intuition that humans can navigate without
detailed maps. A common approach is interactive exploration while building a
topological graph with images at nodes that can be used for planning. Recent
variations learn from passive videos and can navigate using complex social and
semantic cues. However, a significant number of training videos are needed,
large graphs are utilized, and scenes are not unseen since odometry is
utilized. We introduce a new approach to visual navigation using feudal
learning, which employs a hierarchical structure consisting of a worker agent,
a mid-level manager, and a high-level manager. Key to the feudal learning
paradigm, agents at each level see a different aspect of the task and operate
at different spatial and temporal scales. Two unique modules are developed in
this framework. For the high-level manager, we learn a memory proxy map in a
self supervised manner to record prior observations in a learned latent space
and avoid the use of graphs and odometry. For the mid-level manager, we develop
a waypoint network that outputs intermediate subgoals imitating human waypoint
selection during local navigation. This waypoint network is pre-trained using a
new, small set of teleoperation videos that we make publicly available, with
training environments different from testing environments. The resulting feudal
navigation network achieves near SOTA performance, while providing a novel
no-RL, no-graph, no-odometry, no-metric map approach to the image goal
navigation task.</div><div><a href='http://arxiv.org/abs/2402.12498v1'>2402.12498v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18866v1")'>Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming</div>
<div id='2402.18866v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T05:34:05Z</div><div>Authors: Hany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, Sungjin Ahn</div><div style='padding-top: 10px; width: 80ex'>Model-based reinforcement learning (MBRL) has been a primary approach to
ameliorating the sample efficiency issue as well as to make a generalist agent.
However, there has not been much effort toward enhancing the strategy of
dreaming itself. Therefore, it is a question whether and how an agent can
"dream better" in a more structured and strategic way. In this paper, inspired
by the observation from cognitive science suggesting that humans use a spatial
divide-and-conquer strategy in planning, we propose a new MBRL agent, called
Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed
agent realizes a version of divide-and-conquer-like strategy in dreaming. This
is achieved by learning a set of latent landmarks and then utilizing these to
learn a landmark-conditioned highway policy. With the highway policy, the agent
can first learn in the dream to move to a landmark, and from there it tackles
the exploration and achievement task in a more focused way. In experiments, we
show that the proposed model outperforms prior pixel-based MBRL methods in
various visually complex and partially observable navigation tasks. The source
code will be available at https://github.com/ahn-ml/drstrategy</div><div><a href='http://arxiv.org/abs/2402.18866v1'>2402.18866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02426v1")'>Hybrid-Prediction Integrated Planning for Autonomous Driving</div>
<div id='2402.02426v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:51:19Z</div><div>Authors: Haochen Liu, Zhiyu Huang, Wenhui Huang, Haohan Yang, Xiaoyu Mo, Chen Lv</div><div style='padding-top: 10px; width: 80ex'>Autonomous driving systems require the ability to fully understand and
predict the surrounding environment to make informed decisions in complex
scenarios. Recent advancements in learning-based systems have highlighted the
importance of integrating prediction and planning modules. However, this
integration has brought forth three major challenges: inherent trade-offs by
sole prediction, consistency between prediction patterns, and social coherence
in prediction and planning. To address these challenges, we introduce a
hybrid-prediction integrated planning (HPP) system, which possesses three
novelly designed modules. First, we introduce marginal-conditioned occupancy
prediction to align joint occupancy with agent-wise perceptions. Our proposed
MS-OccFormer module achieves multi-stage alignment per occupancy forecasting
with consistent awareness from agent-wise motion predictions. Second, we
propose a game-theoretic motion predictor, GTFormer, to model the interactive
future among individual agents with their joint predictive awareness. Third,
hybrid prediction patterns are concurrently integrated with Ego Planner and
optimized by prediction guidance. HPP achieves state-of-the-art performance on
the nuScenes dataset, demonstrating superior accuracy and consistency for
end-to-end paradigms in prediction and planning. Moreover, we test the
long-term open-loop and closed-loop performance of HPP on the Waymo Open Motion
Dataset and CARLA benchmark, surpassing other integrated prediction and
planning pipelines with enhanced accuracy and compatibility.</div><div><a href='http://arxiv.org/abs/2402.02426v1'>2402.02426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07232v1")'>Tractable Joint Prediction and Planning over Discrete Behavior Modes for
  Urban Driving</div>
<div id='2403.07232v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T01:00:52Z</div><div>Authors: Adam Villaflor, Brian Yang, Huangyuan Su, Katerina Fragkiadaki, John Dolan, Jeff Schneider</div><div style='padding-top: 10px; width: 80ex'>Significant progress has been made in training multimodal trajectory
forecasting models for autonomous driving. However, effectively integrating
these models with downstream planners and model-based control approaches is
still an open problem. Although these models have conventionally been evaluated
for open-loop prediction, we show that they can be used to parameterize
autoregressive closed-loop models without retraining. We consider recent
trajectory prediction approaches which leverage learned anchor embeddings to
predict multiple trajectories, finding that these anchor embeddings can
parameterize discrete and distinct modes representing high-level driving
behaviors. We propose to perform fully reactive closed-loop planning over these
discrete latent modes, allowing us to tractably model the causal interactions
between agents at each step. We validate our approach on a suite of more
dynamic merging scenarios, finding that our approach avoids the $\textit{frozen
robot problem}$ which is pervasive in conventional planners. Our approach also
outperforms the previous state-of-the-art in CARLA on challenging dense traffic
scenarios when evaluated at realistic speeds.</div><div><a href='http://arxiv.org/abs/2403.07232v1'>2403.07232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12856v1")'>Equivariant Ensembles and Regularization for Reinforcement Learning in
  Map-based Path Planning</div>
<div id='2403.12856v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T16:01:25Z</div><div>Authors: Mirco Theile, Hongpeng Cao, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</div><div style='padding-top: 10px; width: 80ex'>In reinforcement learning (RL), exploiting environmental symmetries can
significantly enhance efficiency, robustness, and performance. However,
ensuring that the deep RL policy and value networks are respectively
equivariant and invariant to exploit these symmetries is a substantial
challenge. Related works try to design networks that are equivariant and
invariant by construction, limiting them to a very restricted library of
components, which in turn hampers the expressiveness of the networks. This
paper proposes a method to construct equivariant policies and invariant value
functions without specialized neural network components, which we term
equivariant ensembles. We further add a regularization term for adding
inductive bias during training. In a map-based path planning case study, we
show how equivariant ensembles and regularization benefit sample efficiency and
performance.</div><div><a href='http://arxiv.org/abs/2403.12856v1'>2403.12856v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04855v3")'>LPAC: Learnable Perception-Action-Communication Loops with Applications
  to Coverage Control</div>
<div id='2401.04855v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T00:08:00Z</div><div>Authors: Saurav Agarwal, Ramya Muthukrishnan, Walker Gosrich, Vijay Kumar, Alejandro Ribeiro</div><div style='padding-top: 10px; width: 80ex'>Coverage control is the problem of navigating a robot swarm to
collaboratively monitor features or a phenomenon of interest not known a
priori. The problem is challenging in decentralized settings with robots that
have limited communication and sensing capabilities. We propose a learnable
Perception-Action-Communication (LPAC) architecture for the problem, wherein a
convolution neural network (CNN) processes localized perception; a graph neural
network (GNN) facilitates robot communications; finally, a shallow multi-layer
perceptron (MLP) computes robot actions. The GNN enables collaboration in the
robot swarm by computing what information to communicate with nearby robots and
how to incorporate received information. Evaluations show that the LPAC models
-- trained using imitation learning -- outperform standard decentralized and
centralized coverage control algorithms. The learned policy generalizes to
environments different from the training dataset, transfers to larger
environments with more robots, and is robust to noisy position estimates. The
results indicate the suitability of LPAC architectures for decentralized
navigation in robot swarms to achieve collaborative behavior.</div><div><a href='http://arxiv.org/abs/2401.04855v3'>2401.04855v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03267v1")'>Autonomous Navigation in Complex Environments</div>
<div id='2401.03267v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T18:05:06Z</div><div>Authors: Andrew Gerstenslager, Jomol Lewis, Liam McKenna, Poorva Patel</div><div style='padding-top: 10px; width: 80ex'>This paper explores the application of CNN-DNN network fusion to construct a
robot navigation controller within a simulated environment. The simulated
environment is constructed to model a subterranean rescue situation, such that
an autonomous agent is tasked with finding a goal within an unknown cavernous
system. Imitation learning is used to train the control algorithm to use LiDAR
and camera data to navigate the space and find the goal. The trained model is
then tested for robustness using Monte-Carlo.</div><div><a href='http://arxiv.org/abs/2401.03267v1'>2401.03267v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12067v1")'>Interpretable Brain-Inspired Representations Improve RL Performance on
  Visual Navigation Tasks</div>
<div id='2402.12067v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T11:35:01Z</div><div>Authors: Moritz Lange, Raphael C. Engelhardt, Wolfgang Konen, Laurenz Wiskott</div><div style='padding-top: 10px; width: 80ex'>Visual navigation requires a whole range of capabilities. A crucial one of
these is the ability of an agent to determine its own location and heading in
an environment. Prior works commonly assume this information as given, or use
methods which lack a suitable inductive bias and accumulate error over time. In
this work, we show how the method of slow feature analysis (SFA), inspired by
neuroscience research, overcomes both limitations by generating interpretable
representations of visual data that encode location and heading of an agent. We
employ SFA in a modern reinforcement learning context, analyse and compare
representations and illustrate where hierarchical SFA can outperform other
feature extractors on navigation tasks.</div><div><a href='http://arxiv.org/abs/2402.12067v1'>2402.12067v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12309v1")'>Reinforcement Learning from Delayed Observations via World Models</div>
<div id='2403.12309v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:18:27Z</div><div>Authors: Armin Karamzade, Kyungmin Kim, Montek Kalsi, Roy Fox</div><div style='padding-top: 10px; width: 80ex'>In standard Reinforcement Learning settings, agents typically assume
immediate feedback about the effects of their actions after taking them.
However, in practice, this assumption may not hold true due to physical
constraints and can significantly impact the performance of RL algorithms. In
this paper, we focus on addressing observation delays in partially observable
environments. We propose leveraging world models, which have shown success in
integrating past observations and learning dynamics, to handle observation
delays. By reducing delayed POMDPs to delayed MDPs with world models, our
methods can effectively handle partial observability, where existing approaches
achieve sub-optimal performance or even degrade quickly as observability
decreases. Experiments suggest that one of our methods can outperform a naive
model-based approach by up to %30. Moreover, we evaluate our methods on visual
input based delayed environment, for the first time showcasing delay-aware
reinforcement learning on visual observations.</div><div><a href='http://arxiv.org/abs/2403.12309v1'>2403.12309v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15160v3")'>Spatially-Aware Transformer for Embodied Agents</div>
<div id='2402.15160v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T07:46:30Z</div><div>Authors: Junmo Cho, Jaesik Yoon, Sungjin Ahn</div><div style='padding-top: 10px; width: 80ex'>Episodic memory plays a crucial role in various cognitive processes, such as
the ability to mentally recall past events. While cognitive science emphasizes
the significance of spatial context in the formation and retrieval of episodic
memory, the current primary approach to implementing episodic memory in AI
systems is through transformers that store temporally ordered experiences,
which overlooks the spatial dimension. As a result, it is unclear how the
underlying structure could be extended to incorporate the spatial axis beyond
temporal order alone and thereby what benefits can be obtained. To address
this, this paper explores the use of Spatially-Aware Transformer models that
incorporate spatial information. These models enable the creation of
place-centric episodic memory that considers both temporal and spatial
dimensions. Adopting this approach, we demonstrate that memory utilization
efficiency can be improved, leading to enhanced accuracy in various
place-centric downstream tasks. Additionally, we propose the Adaptive Memory
Allocator, a memory management method based on reinforcement learning that aims
to optimize efficiency of memory utilization. Our experiments demonstrate the
advantages of our proposed model in various environments and across multiple
downstream tasks, including prediction, generation, reasoning, and
reinforcement learning. The source code for our models and experiments will be
available at https://github.com/junmokane/spatially-aware-transformer.</div><div><a href='http://arxiv.org/abs/2402.15160v3'>2402.15160v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02244v1")'>Policy-regularized Offline Multi-objective Reinforcement Learning</div>
<div id='2401.02244v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T12:54:10Z</div><div>Authors: Qian Lin, Chao Yu, Zongkai Liu, Zifan Wu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we aim to utilize only offline trajectory data to train a
policy for multi-objective RL. We extend the offline policy-regularized method,
a widely-adopted approach for single-objective offline RL problems, into the
multi-objective setting in order to achieve the above goal. However, such
methods face a new challenge in offline MORL settings, namely the
preference-inconsistent demonstration problem. We propose two solutions to this
problem: 1) filtering out preference-inconsistent demonstrations via
approximating behavior preferences, and 2) adopting regularization techniques
with high policy expressiveness. Moreover, we integrate the
preference-conditioned scalarized update method into policy-regularized offline
RL, in order to simultaneously learn a set of policies using a single policy
network, thus reducing the computational cost induced by the training of a
large number of individual policies for various preferences. Finally, we
introduce Regularization Weight Adaptation to dynamically determine appropriate
regularization weights for arbitrary target preferences during deployment.
Empirical results on various multi-objective datasets demonstrate the
capability of our approach in solving offline MORL problems.</div><div><a href='http://arxiv.org/abs/2401.02244v1'>2401.02244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06826v1")'>In-context Exploration-Exploitation for Reinforcement Learning</div>
<div id='2403.06826v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:43:14Z</div><div>Authors: Zhenwen Dai, Federico Tomasi, Sina Ghiassian</div><div style='padding-top: 10px; width: 80ex'>In-context learning is a promising approach for online policy learning of
offline reinforcement learning (RL) methods, which can be achieved at inference
time without gradient optimization. However, this method is hindered by
significant computational costs resulting from the gathering of large training
trajectory sets and the need to train large Transformer models. We address this
challenge by introducing an In-context Exploration-Exploitation (ICEE)
algorithm, designed to optimize the efficiency of in-context policy learning.
Unlike existing models, ICEE performs an exploration-exploitation trade-off at
inference time within a Transformer model, without the need for explicit
Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems
as efficiently as Gaussian process biased methods do, but in significantly less
time. Through experiments in grid world environments, we demonstrate that ICEE
can learn to solve new RL tasks using only tens of episodes, marking a
substantial improvement over the hundreds of episodes needed by the previous
in-context learning method.</div><div><a href='http://arxiv.org/abs/2403.06826v1'>2403.06826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11317v1")'>Debiased Offline Representation Learning for Fast Online Adaptation in
  Non-stationary Dynamics</div>
<div id='2402.11317v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T16:03:35Z</div><div>Authors: Xinyu Zhang, Wenjie Qiu, Yi-Chen Li, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu</div><div style='padding-top: 10px; width: 80ex'>Developing policies that can adjust to non-stationary environments is
essential for real-world reinforcement learning applications. However, learning
such adaptable policies in offline settings, with only a limited set of
pre-collected trajectories, presents significant challenges. A key difficulty
arises because the limited offline data makes it hard for the context encoder
to differentiate between changes in the environment dynamics and shifts in the
behavior policy, often leading to context misassociations. To address this
issue, we introduce a novel approach called Debiased Offline Representation for
fast online Adaptation (DORA). DORA incorporates an information bottleneck
principle that maximizes mutual information between the dynamics encoding and
the environmental data, while minimizing mutual information between the
dynamics encoding and the actions of the behavior policy. We present a
practical implementation of DORA, leveraging tractable bounds of the
information bottleneck principle. Our experimental evaluation across six
benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only
achieves a more precise dynamics encoding but also significantly outperforms
existing baselines in terms of performance.</div><div><a href='http://arxiv.org/abs/2402.11317v1'>2402.11317v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15957v1")'>DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement
  Learning</div>
<div id='2402.15957v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T02:36:03Z</div><div>Authors: Anthony Liang, Guy Tennenholtz, Chih-wei Hsu, Yinlam Chow, Erdem Bıyık, Craig Boutilier</div><div style='padding-top: 10px; width: 80ex'>We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to
approximate inference in environments where the latent state evolves at varying
rates. We model episode sessions - parts of the episode where the latent state
is fixed - and propose three key modifications to existing meta-RL methods:
consistency of latent information within sessions, session masking, and prior
latent conditioning. We demonstrate the importance of these modifications in
various domains, ranging from discrete Gridworld environments to
continuous-control and simulated robot assistive tasks, demonstrating that
DynaMITE-RL significantly outperforms state-of-the-art baselines in sample
efficiency and inference returns.</div><div><a href='http://arxiv.org/abs/2402.15957v1'>2402.15957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11237v2")'>Closing the Gap between TD Learning and Supervised Learning -- A
  Generalisation Point of View</div>
<div id='2401.11237v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T14:23:25Z</div><div>Authors: Raj Ghugare, Matthieu Geist, Glen Berseth, Benjamin Eysenbach</div><div style='padding-top: 10px; width: 80ex'>Some reinforcement learning (RL) algorithms can stitch pieces of experience
to solve a task never seen before during training. This oft-sought property is
one of the few ways in which RL methods based on dynamic-programming differ
from RL methods based on supervised-learning (SL). Yet, certain RL methods
based on off-the-shelf SL algorithms achieve excellent results without an
explicit mechanism for stitching; it remains unclear whether those methods
forgo this important stitching property. This paper studies this question for
the problems of achieving a target goal state and achieving a target return
value. Our main result is to show that the stitching property corresponds to a
form of combinatorial generalization: after training on a distribution of
(state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen
together in the training data. Our analysis shows that this sort of
generalization is different from i.i.d. generalization. This connection between
stitching and generalisation reveals why we should not expect SL-based RL
methods to perform stitching, even in the limit of large datasets and models.
Based on this analysis, we construct new datasets to explicitly test for this
property, revealing that SL-based methods lack this stitching property and
hence fail to perform combinatorial generalization. Nonetheless, the connection
between stitching and combinatorial generalisation also suggests a simple
remedy for improving generalisation in SL: data augmentation. We propose a
temporal data augmentation and demonstrate that adding it to SL-based methods
enables them to successfully complete tasks not seen together during training.
On a high level, this connection illustrates the importance of combinatorial
generalization for data efficiency in time-series data beyond tasks beyond RL,
like audio, video, or text.</div><div><a href='http://arxiv.org/abs/2401.11237v2'>2401.11237v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13765v1")'>Towards Principled Representation Learning from Videos for Reinforcement
  Learning</div>
<div id='2403.13765v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:28:17Z</div><div>Authors: Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford</div><div style='padding-top: 10px; width: 80ex'>We study pre-training representations for decision-making using video data,
which is abundantly available for tasks such as game agents and software
testing. Even though significant empirical advances have been made on this
problem, a theoretical understanding remains absent. We initiate the
theoretical investigation into principled approaches for representation
learning and focus on learning the latent state representations of the
underlying MDP using video data. We study two types of settings: one where
there is iid noise in the observation, and a more challenging setting where
there is also the presence of exogenous noise, which is non-iid noise that is
temporally correlated, such as the motion of people or cars in the background.
We study three commonly used approaches: autoencoding, temporal contrastive
learning, and forward modeling. We prove upper bounds for temporal contrastive
learning and forward modeling in the presence of only iid noise. We show that
these approaches can learn the latent state and use it to do efficient
downstream RL with polynomial sample complexity. When exogenous noise is also
present, we establish a lower bound result showing that the sample complexity
of learning from video data can be exponentially worse than learning from
action-labeled trajectory data. This partially explains why reinforcement
learning with video pre-training is hard. We evaluate these representational
learning methods in two visual domains, yielding results that are consistent
with our theoretical findings.</div><div><a href='http://arxiv.org/abs/2403.13765v1'>2403.13765v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08902v2")'>Auto-Encoding Bayesian Inverse Games</div>
<div id='2402.08902v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T02:17:37Z</div><div>Authors: Xinjie Liu, Lasse Peters, Javier Alonso-Mora, Ufuk Topcu, David Fridovich-Keil</div><div style='padding-top: 10px; width: 80ex'>When multiple agents interact in a common environment, each agent's actions
impact others' future decisions, and noncooperative dynamic games naturally
capture this coupling. In interactive motion planning, however, agents
typically do not have access to a complete model of the game, e.g., due to
unknown objectives of other players. Therefore, we consider the inverse game
problem, in which some properties of the game are unknown a priori and must be
inferred from observations. Existing maximum likelihood estimation (MLE)
approaches to solve inverse games provide only point estimates of unknown
parameters without quantifying uncertainty, and perform poorly when many
parameter values explain the observed behavior. To address these limitations,
we take a Bayesian perspective and construct posterior distributions of game
parameters. To render inference tractable, we employ a variational autoencoder
(VAE) with an embedded differentiable game solver. This structured VAE can be
trained from an unlabeled dataset of observed interactions, naturally handles
continuous, multi-modal distributions, and supports efficient sampling from the
inferred posteriors without computing game solutions at runtime. Extensive
evaluations in simulated driving scenarios demonstrate that the proposed
approach successfully learns the prior and posterior objective distributions,
provides more accurate objective estimates than MLE baselines, and facilitates
safer and more efficient game-theoretic motion planning.</div><div><a href='http://arxiv.org/abs/2402.08902v2'>2402.08902v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15328v1")'>Towards Principled Task Grouping for Multi-Task Learning</div>
<div id='2402.15328v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:51:20Z</div><div>Authors: Chenguang Wang, Xuanhao Pan, Tianshu Yu</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel approach to task grouping in Multitask Learning
(MTL), advancing beyond existing methods by addressing key theoretical and
practical limitations. Unlike prior studies, our approach offers a more
theoretically grounded method that does not rely on restrictive assumptions for
constructing transfer gains. We also propose a flexible mathematical
programming formulation which can accommodate a wide spectrum of resource
constraints, thus enhancing its versatility. Experimental results across
diverse domains, including computer vision datasets, combinatorial optimization
benchmarks and time series tasks, demonstrate the superiority of our method
over extensive baselines, validating its effectiveness and general
applicability in MTL.</div><div><a href='http://arxiv.org/abs/2402.15328v1'>2402.15328v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07087v1")'>Self-Correcting Self-Consuming Loops for Generative Model Training</div>
<div id='2402.07087v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T02:34:42Z</div><div>Authors: Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin Luo, Yonglong Tian, Chen Sun</div><div style='padding-top: 10px; width: 80ex'>As synthetic data becomes higher quality and proliferates on the internet,
machine learning models are increasingly trained on a mix of human- and
machine-generated data. Despite the successful stories of using synthetic data
for representation learning, using synthetic data for generative model training
creates "self-consuming loops" which may lead to training instability or even
collapse, unless certain conditions are met. Our paper aims to stabilize
self-consuming generative model training. Our theoretical results demonstrate
that by introducing an idealized correction function, which maps a data point
to be more likely under the true data distribution, self-consuming loops can be
made exponentially more stable. We then propose self-correction functions,
which rely on expert knowledge (e.g. the laws of physics programmed in a
simulator), and aim to approximate the idealized corrector automatically and at
scale. We empirically validate the effectiveness of self-correcting
self-consuming loops on the challenging human motion synthesis task, and
observe that it successfully avoids model collapse, even when the ratio of
synthetic data to real data is as high as 100%.</div><div><a href='http://arxiv.org/abs/2402.07087v1'>2402.07087v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13117v1")'>Optimal Flow Matching: Learning Straight Trajectories in Just One Step</div>
<div id='2403.13117v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:44:54Z</div><div>Authors: Nikita Kornilov, Alexander Gasnikov, Alexander Korotin</div><div style='padding-top: 10px; width: 80ex'>Over the several recent years, there has been a boom in development of flow
matching methods for generative modeling. One intriguing property pursued by
the community is the ability to learn flows with straight trajectories which
realize the optimal transport (OT) displacements. Straightness is crucial for
fast integration of the learned flow's paths. Unfortunately, most existing flow
straightening methods are based on non-trivial iterative procedures which
accumulate the error during training or exploit heuristic minibatch OT
approximations. To address this issue, we develop a novel optimal flow matching
approach which recovers the straight OT displacement for the quadratic cost in
just one flow matching step.</div><div><a href='http://arxiv.org/abs/2403.13117v1'>2403.13117v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03020v2")'>SplAgger: Split Aggregation for Meta-Reinforcement Learning</div>
<div id='2403.03020v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T14:57:04Z</div><div>Authors: Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, Shimon Whiteson</div><div style='padding-top: 10px; width: 80ex'>A core ambition of reinforcement learning (RL) is the creation of agents
capable of rapid learning in novel tasks. Meta-RL aims to achieve this by
directly learning such agents. Black box methods do so by training
off-the-shelf sequence models end-to-end. By contrast, task inference methods
explicitly infer a posterior distribution over the unknown task, typically
using distinct objectives and sequence models designed to enable task
inference. Recent work has shown that task inference methods are not necessary
for strong performance. However, it remains unclear whether task inference
sequence models are beneficial even when task inference objectives are not. In
this paper, we present strong evidence that task inference sequence models are
still beneficial. In particular, we investigate sequence models with
permutation invariant aggregation, which exploit the fact that, due to the
Markov property, the task posterior does not depend on the order of data. We
empirically confirm the advantage of permutation invariant sequence models
without the use of task inference objectives. However, we also find,
surprisingly, that there are multiple conditions under which permutation
variance remains useful. Therefore, we propose SplAgger, which uses both
permutation variant and invariant components to achieve the best of both
worlds, outperforming all baselines on continuous control and memory
environments.</div><div><a href='http://arxiv.org/abs/2403.03020v2'>2403.03020v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09900v2")'>Revisiting Recurrent Reinforcement Learning with Memory Monoids</div>
<div id='2402.09900v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T11:56:53Z</div><div>Authors: Steven Morad, Chris Lu, Ryan Kortvelesy, Stephan Liwicki, Jakob Foerster, Amanda Prorok</div><div style='padding-top: 10px; width: 80ex'>Memory models such as Recurrent Neural Networks (RNNs) and Transformers
address Partially Observable Markov Decision Processes (POMDPs) by mapping
trajectories to latent Markov states. Neither model scales particularly well to
long sequences, especially compared to an emerging class of memory models
sometimes called linear recurrent models. We discover that we can model the
recurrent update of these models using a monoid, leading us to reformulate
existing models using a novel memory monoid framework. We revisit the
traditional approach to batching in recurrent RL, highlighting both theoretical
and empirical deficiencies. We leverage the properties of memory monoids to
propose a batching method that improves sample efficiency, increases the
return, and simplifies the implementation of recurrent loss functions in RL.</div><div><a href='http://arxiv.org/abs/2402.09900v2'>2402.09900v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07102v1")'>Future Prediction Can be a Strong Evidence of Good History
  Representation in Partially Observable Environments</div>
<div id='2402.07102v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T04:53:40Z</div><div>Authors: Jeongyeol Kwon, Liu Yang, Robert Nowak, Josiah Hanna</div><div style='padding-top: 10px; width: 80ex'>Learning a good history representation is one of the core challenges of
reinforcement learning (RL) in partially observable environments. Recent works
have shown the advantages of various auxiliary tasks for facilitating
representation learning. However, the effectiveness of such auxiliary tasks has
not been fully convincing, especially in partially observable environments that
require long-term memorization and inference. In this empirical study, we
investigate the effectiveness of future prediction for learning the
representations of histories, possibly of extensive length, in partially
observable environments. We first introduce an approach that decouples the task
of learning history representations from policy optimization via future
prediction. Then, our main contributions are two-fold: (a) we demonstrate that
the performance of reinforcement learning is strongly correlated with the
prediction accuracy of future observations in partially observable
environments, and (b) our approach can significantly improve the overall
end-to-end approach by preventing high-variance noisy signals from
reinforcement learning objectives to influence the representation learning. We
illustrate our claims on three types of benchmarks that necessitate the ability
to process long histories for high returns.</div><div><a href='http://arxiv.org/abs/2402.07102v1'>2402.07102v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.18012v1")'>Causal Coordinated Concurrent Reinforcement Learning</div>
<div id='2401.18012v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T17:20:28Z</div><div>Authors: Tim Tse, Isaac Chan, Zhitang Chen</div><div style='padding-top: 10px; width: 80ex'>In this work, we propose a novel algorithmic framework for data sharing and
coordinated exploration for the purpose of learning more data-efficient and
better performing policies under a concurrent reinforcement learning (CRL)
setting. In contrast to other work which make the assumption that all agents
act under identical environments, we relax this restriction and instead
consider the formulation where each agent acts within an environment which
shares a global structure but also exhibits individual variations. Our
algorithm leverages a causal inference algorithm in the form of Additive Noise
Model - Mixture Model (ANM-MM) in extracting model parameters governing
individual differentials via independence enforcement. We propose a new data
sharing scheme based on a similarity measure of the extracted model parameters
and demonstrate superior learning speeds on a set of autoregressive, pendulum
and cart-pole swing-up tasks and finally, we show the effectiveness of diverse
action selection between common agents under a sparse reward setting. To the
best of our knowledge, this is the first work in considering non-identical
environments in CRL and one of the few works which seek to integrate causal
inference with reinforcement learning (RL).</div><div><a href='http://arxiv.org/abs/2401.18012v1'>2401.18012v1</a></div>
</div></div>
    <div><a href="arxiv_10.html">Prev (10)</a></div>
    <div><a href="arxiv_12.html">Next (12)</a></div>
    