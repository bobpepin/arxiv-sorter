
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14327v1")'>Investigating the validity of structure learning algorithms in
  identifying risk factors for intervention in patients with diabetes</div>
<div id='2403.14327v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T11:51:42Z</div><div>Authors: Sheresh Zahoor, Anthony C. Constantinou, Tim M Curtis, Mohammed Hasanuzzaman</div><div style='padding-top: 10px; width: 80ex'>Diabetes, a pervasive and enduring health challenge, imposes significant
global implications on health, financial healthcare systems, and societal
well-being. This study undertakes a comprehensive exploration of various
structural learning algorithms to discern causal pathways amongst potential
risk factors influencing diabetes progression. The methodology involves the
application of these algorithms to relevant diabetes data, followed by the
conversion of their output graphs into Causal Bayesian Networks (CBNs),
enabling predictive analysis and the evaluation of discrepancies in the effect
of hypothetical interventions within our context-specific case study.
  This study highlights the substantial impact of algorithm selection on
intervention outcomes. To consolidate insights from diverse algorithms, we
employ a model-averaging technique that helps us obtain a unique causal model
for diabetes derived from a varied set of structural learning algorithms. We
also investigate how each of those individual graphs, as well as the average
graph, compare to the structures elicited by a domain expert who categorised
graph edges into high confidence, moderate, and low confidence types, leading
into three individual graphs corresponding to the three levels of confidence.
  The resulting causal model and data are made available online, and serve as a
valuable resource and a guide for informed decision-making by healthcare
practitioners, offering a comprehensive understanding of the interactions
between relevant risk factors and the effect of hypothetical interventions.
Therefore, this research not only contributes to the academic discussion on
diabetes, but also provides practical guidance for healthcare professionals in
developing efficient intervention and risk management strategies.</div><div><a href='http://arxiv.org/abs/2403.14327v1'>2403.14327v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14481v1")'>Towards Automated Causal Discovery: a case study on 5G telecommunication
  data</div>
<div id='2402.14481v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T12:13:58Z</div><div>Authors: Konstantina Biza, Antonios Ntroumpogiannis, Sofia Triantafillou, Ioannis Tsamardinos</div><div style='padding-top: 10px; width: 80ex'>We introduce the concept of Automated Causal Discovery (AutoCD), defined as
any system that aims to fully automate the application of causal discovery and
causal reasoning methods. AutoCD's goal is to deliver all causal information
that an expert human analyst would and answer a user's causal queries. We
describe the architecture of such a platform, and illustrate its performance on
synthetic data sets. As a case study, we apply it on temporal telecommunication
data. The system is general and can be applied to a plethora of causal
discovery problems.</div><div><a href='http://arxiv.org/abs/2402.14481v1'>2402.14481v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05946v2")'>Unveiling Latent Causal Rules: A Temporal Point Process Approach for
  Abnormal Event Explanation</div>
<div id='2402.05946v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T06:21:33Z</div><div>Authors: Yiling Kuang, Chao Yang, Yang Yang, Shuang Li</div><div style='padding-top: 10px; width: 80ex'>In high-stakes systems such as healthcare, it is critical to understand the
causal reasons behind unusual events, such as sudden changes in patient's
health. Unveiling the causal reasons helps with quick diagnoses and precise
treatment planning. In this paper, we propose an automated method for
uncovering "if-then" logic rules to explain observational events. We introduce
temporal point processes to model the events of interest, and discover the set
of latent rules to explain the occurrence of events. To achieve this, we employ
an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the
likelihood of each event being explained by each discovered rule. In the
M-step, we update both the rule set and model parameters to enhance the
likelihood function's lower bound. Notably, we optimize the rule set in a
differential manner. Our approach demonstrates accurate performance in both
discovering rules and identifying root causes. We showcase its promising
results using synthetic and real healthcare datasets.</div><div><a href='http://arxiv.org/abs/2402.05946v2'>2402.05946v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02357v1")'>Multi-modal Causal Structure Learning and Root Cause Analysis</div>
<div id='2402.02357v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:50:38Z</div><div>Authors: Lecheng Zheng, Zhengzhang Chen, Jingrui He, Haifeng Chen</div><div style='padding-top: 10px; width: 80ex'>Effective root cause analysis (RCA) is vital for swiftly restoring services,
minimizing losses, and ensuring the smooth operation and management of complex
systems. Previous data-driven RCA methods, particularly those employing causal
discovery techniques, have primarily focused on constructing dependency or
causal graphs for backtracking the root causes. However, these methods often
fall short as they rely solely on data from a single modality, thereby
resulting in suboptimal solutions. In this work, we propose Mulan, a unified
multi-modal causal structure learning method for root cause localization. We
leverage a log-tailored language model to facilitate log representation
learning, converting log sequences into time-series data. To explore intricate
relationships across different modalities, we propose a contrastive
learning-based approach to extract modality-invariant and modality-specific
representations within a shared latent space. Additionally, we introduce a
novel key performance indicator-aware attention mechanism for assessing
modality reliability and co-learning a final causal graph. Finally, we employ
random walk with restart to simulate system fault propagation and identify
potential root causes. Extensive experiments on three real-world datasets
validate the effectiveness of our proposed framework.</div><div><a href='http://arxiv.org/abs/2402.02357v1'>2402.02357v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01140v1")'>Root Cause Analysis In Microservice Using Neural Granger Causal
  Discovery</div>
<div id='2402.01140v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T04:43:06Z</div><div>Authors: Cheng-Ming Lin, Ching Chang, Wei-Yao Wang, Kuang-Da Wang, Wen-Chih Peng</div><div style='padding-top: 10px; width: 80ex'>In recent years, microservices have gained widespread adoption in IT
operations due to their scalability, maintenance, and flexibility. However, it
becomes challenging for site reliability engineers (SREs) to pinpoint the root
cause due to the complex relationships in microservices when facing system
malfunctions. Previous research employed structured learning methods (e.g.,
PC-algorithm) to establish causal relationships and derive root causes from
causal graphs. Nevertheless, they ignored the temporal order of time series
data and failed to leverage the rich information inherent in the temporal
relationships. For instance, in cases where there is a sudden spike in CPU
utilization, it can lead to an increase in latency for other microservices.
However, in this scenario, the anomaly in CPU utilization occurs before the
latency increase, rather than simultaneously. As a result, the PC-algorithm
fails to capture such characteristics. To address these challenges, we propose
RUN, a novel approach for root cause analysis using neural Granger causal
discovery with contrastive learning. RUN enhances the backbone encoder by
integrating contextual information from time series, and leverages a time
series forecasting model to conduct neural Granger causal discovery. In
addition, RUN incorporates Pagerank with a personalization vector to
efficiently recommend the top-k root causes. Extensive experiments conducted on
the synthetic and real-world microservice-based datasets demonstrate that RUN
noticeably outperforms the state-of-the-art root cause analysis methods.
Moreover, we provide an analysis scenario for the sock-shop case to showcase
the practicality and efficacy of RUN in microservice-based applications. Our
code is publicly available at https://github.com/zmlin1998/RUN.</div><div><a href='http://arxiv.org/abs/2402.01140v1'>2402.01140v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02921v1")'>Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation</div>
<div id='2402.02921v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:41:37Z</div><div>Authors: Mehdi Acheli, Daniela Grigori, Matthias Weidlich</div><div style='padding-top: 10px; width: 80ex'>Process mining provides methods to analyse event logs generated by
information systems during the execution of processes. It thereby supports the
design, validation, and execution of processes in domains ranging from
healthcare, through manufacturing, to e-commerce. To explore the regularities
of flexible processes that show a large behavioral variability, it was
suggested to mine recurrent behavioral patterns that jointly describe the
underlying process. Existing approaches to behavioral pattern mining, however,
suffer from two limitations. First, they show limited scalability as
incremental computation is incorporated only in the generation of pattern
candidates, but not in the evaluation of their quality. Second, process
analysis based on mined patterns shows limited effectiveness due to an
overwhelmingly large number of patterns obtained in practical application
scenarios, many of which are redundant. In this paper, we address these
limitations to facilitate the analysis of complex, flexible processes based on
behavioral patterns. Specifically, we improve COBPAM, our initial behavioral
pattern mining algorithm, by an incremental procedure to evaluate the quality
of pattern candidates, optimizing thereby its efficiency. Targeting a more
effective use of the resulting patterns, we further propose pruning strategies
for redundant patterns and show how relations between the remaining patterns
are extracted and visualized to provide process insights. Our experiments with
diverse real-world datasets indicate a considerable reduction of the runtime
needed for pattern mining, while a qualitative assessment highlights how
relations between patterns guide the analysis of the underlying process.</div><div><a href='http://arxiv.org/abs/2402.02921v1'>2402.02921v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14847v1")'>Extracting Process-Aware Decision Models from Object-Centric Process
  Data</div>
<div id='2401.14847v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T13:27:35Z</div><div>Authors: Alexandre Goossens, Johannes De Smedt, Jan Vanthienen</div><div style='padding-top: 10px; width: 80ex'>Organizations execute decisions within business processes on a daily basis
whilst having to take into account multiple stakeholders who might require
multiple point of views of the same process. Moreover, the complexity of the
information systems running these business processes is generally high as they
are linked to databases storing all the relevant data and aspects of the
processes. Given the presence of multiple objects within an information system
which support the processes in their enactment, decisions are naturally
influenced by both these perspectives, logged in object-centric process logs.
However, the discovery of such decisions from object-centric process logs is
not straightforward as it requires to correctly link the involved objects
whilst considering the sequential constraints that business processes impose as
well as correctly discovering what a decision actually does. This paper
proposes the first object-centric decision-mining algorithm called Integrated
Object-centric Decision Discovery Algorithm (IODDA). IODDA is able to discover
how a decision is structured as well as how a decision is made. Moreover, IODDA
is able to discover which activities and object types are involved in the
decision-making process. Next, IODDA is demonstrated with the first artificial
knowledge-intensive process logs whose log generators are provided to the
research community.</div><div><a href='http://arxiv.org/abs/2401.14847v1'>2401.14847v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11642v1")'>Guiding the generation of counterfactual explanations through temporal
  background knowledge for Predictive Process Monitoring</div>
<div id='2403.11642v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T10:34:40Z</div><div>Authors: Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Ivan Donadello, Fabrizio Maria Maggi</div><div style='padding-top: 10px; width: 80ex'>Counterfactual explanations suggest what should be different in the input
instance to change the outcome of an AI system. When dealing with
counterfactual explanations in the field of Predictive Process Monitoring,
however, control flow relationships among events have to be carefully
considered. A counterfactual, indeed, should not violate control flow
relationships among activities (temporal background knowledege). Within the
field of Explainability in Predictive Process Monitoring, there have been a
series of works regarding counterfactual explanations for outcome-based
predictions. However, none of them consider the inclusion of temporal
background knowledge when generating these counterfactuals. In this work, we
adapt state-of-the-art techniques for counterfactual generation in the domain
of XAI that are based on genetic algorithms to consider a series of temporal
constraints at runtime. We assume that this temporal background knowledge is
given, and we adapt the fitness function, as well as the crossover and mutation
operators, to maintain the satisfaction of the constraints. The proposed
methods are evaluated with respect to state-of-the-art genetic algorithms for
counterfactual generation and the results are presented. We showcase that the
inclusion of temporal background knowledge allows the generation of
counterfactuals more conformant to the temporal background knowledge, without
however losing in terms of the counterfactual traditional quality metrics.</div><div><a href='http://arxiv.org/abs/2403.11642v1'>2403.11642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18477v1")'>Signature Kernel Conditional Independence Tests in Causal Discovery for
  Stochastic Processes</div>
<div id='2402.18477v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T16:58:31Z</div><div>Authors: Georg Manten, Cecilia Casolo, Emilio Ferrucci, Søren Wengel Mogensen, Cristopher Salvi, Niki Kilbertus</div><div style='padding-top: 10px; width: 80ex'>Inferring the causal structure underlying stochastic dynamical systems from
observational data holds great promise in domains ranging from science and
health to finance. Such processes can often be accurately modeled via
stochastic differential equations (SDEs), which naturally imply causal
relationships via "which variables enter the differential of which other
variables". In this paper, we develop a kernel-based test of conditional
independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent
advances in signature kernels. We demonstrate strictly superior performance of
our proposed CI test compared to existing approaches on path-space. Then, we
develop constraint-based causal discovery algorithms for acyclic stochastic
dynamical systems (allowing for loops) that leverage temporal information to
recover the entire directed graph. Assuming faithfulness and a CI oracle, our
algorithm is sound and complete. We empirically verify that our developed CI
test in conjunction with the causal discovery algorithm reliably outperforms
baselines across a range of settings.</div><div><a href='http://arxiv.org/abs/2402.18477v1'>2402.18477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05759v1")'>Membership Testing in Markov Equivalence Classes via Independence Query
  Oracles</div>
<div id='2403.05759v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T02:10:08Z</div><div>Authors: Jiaqi Zhang, Kirankumar Shiragur, Caroline Uhler</div><div style='padding-top: 10px; width: 80ex'>Understanding causal relationships between variables is a fundamental problem
with broad impact in numerous scientific fields. While extensive research has
been dedicated to learning causal graphs from data, its complementary concept
of testing causal relationships has remained largely unexplored. While learning
involves the task of recovering the Markov equivalence class (MEC) of the
underlying causal graph from observational data, the testing counterpart
addresses the following critical question: Given a specific MEC and
observational data from some causal graph, can we determine if the
data-generating causal graph belongs to the given MEC?
  We explore constraint-based testing methods by establishing bounds on the
required number of conditional independence tests. Our bounds are in terms of
the size of the maximum undirected clique ($s$) of the given MEC. In the worst
case, we show a lower bound of $\exp(\Omega(s))$ independence tests. We then
give an algorithm that resolves the task with $\exp(O(s))$ tests, matching our
lower bound. Compared to the learning problem, where algorithms often use a
number of independence tests that is exponential in the maximum in-degree, this
shows that testing is relatively easier. In particular, it requires
exponentially less independence tests in graphs featuring high in-degrees and
small clique sizes. Additionally, using the DAG associahedron, we provide a
geometric interpretation of testing versus learning and discuss how our testing
result can aid learning.</div><div><a href='http://arxiv.org/abs/2403.05759v1'>2403.05759v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14843v1")'>Local Causal Discovery with Linear non-Gaussian Cyclic Models</div>
<div id='2403.14843v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T21:27:39Z</div><div>Authors: Haoyue Dai, Ignavier Ng, Yujia Zheng, Zhengqing Gao, Kun Zhang</div><div style='padding-top: 10px; width: 80ex'>Local causal discovery is of great practical significance, as there are often
situations where the discovery of the global causal structure is unnecessary,
and the interest lies solely on a single target variable. Most existing local
methods utilize conditional independence relations, providing only a partially
directed graph, and assume acyclicity for the ground-truth structure, even
though real-world scenarios often involve cycles like feedback mechanisms. In
this work, we present a general, unified local causal discovery method with
linear non-Gaussian models, whether they are cyclic or acyclic. We extend the
application of independent component analysis from the global context to
independent subspace analysis, enabling the exact identification of the
equivalent local directed structures and causal strengths from the Markov
blanket of the target variable. We also propose an alternative regression-based
method in the particular acyclic scenarios. Our identifiability results are
empirically validated using both synthetic and real-world datasets.</div><div><a href='http://arxiv.org/abs/2403.14843v1'>2403.14843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14535v1")'>CaRiNG: Learning Temporal Causal Representation under Non-Invertible
  Generation Process</div>
<div id='2401.14535v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T22:01:07Z</div><div>Authors: Guangyi Chen, Yifan Shen, Zhenhao Chen, Xiangchen Song, Yuewen Sun, Weiran Yao, Xiao Liu, Kun Zhang</div><div style='padding-top: 10px; width: 80ex'>Identifying the underlying time-delayed latent causal processes in sequential
data is vital for grasping temporal dynamics and making downstream reasoning.
While some recent methods can robustly identify these latent causal variables,
they rely on strict assumptions about the invertible generation process from
latent variables to observed data. However, these assumptions are often hard to
satisfy in real-world applications containing information loss. For instance,
the visual perception process translates a 3D space into 2D images, or the
phenomenon of persistence of vision incorporates historical data into current
perceptions. To address this challenge, we establish an identifiability theory
that allows for the recovery of independent latent components even when they
come from a nonlinear and non-invertible mix. Using this theory as a
foundation, we propose a principled approach, CaRiNG, to learn the CAusal
RepresentatIon of Non-invertible Generative temporal data with identifiability
guarantees. Specifically, we utilize temporal context to recover lost latent
information and apply the conditions in our theory to guide the training
process. Through experiments conducted on synthetic datasets, we validate that
our CaRiNG method reliably identifies the causal process, even when the
generation process is non-invertible. Moreover, we demonstrate that our
approach considerably improves temporal understanding and reasoning in
practical applications.</div><div><a href='http://arxiv.org/abs/2401.14535v1'>2401.14535v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09830v1")'>Towards the Reusability and Compositionality of Causal Representations</div>
<div id='2403.09830v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T19:36:07Z</div><div>Authors: Davide Talon, Phillip Lippe, Stuart James, Alessio Del Bue, Sara Magliacane</div><div style='padding-top: 10px; width: 80ex'>Causal Representation Learning (CRL) aims at identifying high-level causal
factors and their relationships from high-dimensional observations, e.g.,
images. While most CRL works focus on learning causal representations in a
single environment, in this work we instead propose a first step towards
learning causal representations from temporal sequences of images that can be
adapted in a new environment, or composed across multiple related environments.
In particular, we introduce DECAF, a framework that detects which causal
factors can be reused and which need to be adapted from previously learned
causal representations. Our approach is based on the availability of
intervention targets, that indicate which variables are perturbed at each time
step. Experiments on three benchmark datasets show that integrating our
framework with four state-of-the-art CRL approaches leads to accurate
representations in a new environment with only a few samples.</div><div><a href='http://arxiv.org/abs/2403.09830v1'>2403.09830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10240v2")'>A Dynamical View of the Question of Why</div>
<div id='2402.10240v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:44:05Z</div><div>Authors: Mehdi Fatemi, Sindhu Gowda</div><div style='padding-top: 10px; width: 80ex'>We address causal reasoning in multivariate time series data generated by
stochastic processes. Existing approaches are largely restricted to static
settings, ignoring the continuity and emission of variations across time. In
contrast, we propose a learning paradigm that directly establishes causation
between events in the course of time. We present two key lemmas to compute
causal contributions and frame them as reinforcement learning problems. Our
approach offers formal and computational tools for uncovering and quantifying
causal relationships in diffusion processes, subsuming various important
settings such as discrete-time Markov decision processes. Finally, in fairly
intricate experiments and through sheer learning, our framework reveals and
quantifies causal links, which otherwise seem inexplicable.</div><div><a href='http://arxiv.org/abs/2402.10240v2'>2402.10240v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09698v1")'>Combining Evidence Across Filtrations</div>
<div id='2402.09698v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T04:16:59Z</div><div>Authors: Yo Joong Choe, Aaditya Ramdas</div><div style='padding-top: 10px; width: 80ex'>In anytime-valid sequential inference, it is known that any admissible
inference procedure must be based on test martingales and their composite
generalization, called e-processes, which are nonnegative processes whose
expectation at any arbitrary stopping time is upper-bounded by one. An
e-process quantifies the accumulated evidence against a composite null
hypothesis over a sequence of outcomes. This paper studies methods for
combining e-processes that are computed using different information sets, i.e.,
filtrations, for a null hypothesis. Even though e-processes constructed on the
same filtration can be combined effortlessly (e.g., by averaging), e-processes
constructed on different filtrations cannot be combined as easily because their
validity in a coarser filtration does not translate to validity in a finer
filtration. We discuss three concrete examples of such e-processes in the
literature: exchangeability tests, independence tests, and tests for evaluating
and comparing forecasts with lags. Our main result establishes that these
e-processes can be lifted into any finer filtration using adjusters, which are
functions that allow betting on the running maximum of the accumulated wealth
(thereby insuring against the loss of evidence). We also develop randomized
adjusters that can improve the power of the resulting sequential inference
procedure.</div><div><a href='http://arxiv.org/abs/2402.09698v1'>2402.09698v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00623v1")'>Bayesian Causal Inference with Gaussian Process Networks</div>
<div id='2402.00623v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T14:39:59Z</div><div>Authors: Enrico Giudice, Jack Kuipers, Giusi Moffa</div><div style='padding-top: 10px; width: 80ex'>Causal discovery and inference from observational data is an essential
problem in statistics posing both modeling and computational challenges. These
are typically addressed by imposing strict assumptions on the joint
distribution such as linearity. We consider the problem of the Bayesian
estimation of the effects of hypothetical interventions in the Gaussian Process
Network (GPN) model, a flexible causal framework which allows describing the
causal relationships nonparametrically. We detail how to perform causal
inference on GPNs by simulating the effect of an intervention across the whole
network and propagating the effect of the intervention on downstream variables.
We further derive a simpler computational approximation by estimating the
intervention distribution as a function of local variables only, modeling the
conditional distributions via additive Gaussian processes. We extend both
frameworks beyond the case of a known causal graph, incorporating uncertainty
about the causal structure via Markov chain Monte Carlo methods. Simulation
studies show that our approach is able to identify the effects of hypothetical
interventions with non-Gaussian, non-linear observational data and accurately
reflect the posterior uncertainty of the causal estimates. Finally we compare
the results of our GPN-based causal inference approach to existing methods on a
dataset of $A.~thaliana$ gene expressions.</div><div><a href='http://arxiv.org/abs/2402.00623v1'>2402.00623v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16661v3")'>Generalization of LiNGAM that allows confounding</div>
<div id='2401.16661v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T01:24:43Z</div><div>Authors: Joe Suzuki, Tian-Le Yang</div><div style='padding-top: 10px; width: 80ex'>LiNGAM determines the variable order from cause to effect using additive
noise models, but it faces challenges with confounding. Previous methods
maintained LiNGAM's fundamental structure while trying to identify and address
variables affected by confounding. As a result, these methods required
significant computational resources regardless of the presence of confounding,
and they did not ensure the detection of all confounding types. In contrast,
this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies
the magnitude of confounding using KL divergence and arranges the variables to
minimize its impact. This method efficiently achieves a globally optimal
variable order through the shortest path problem formulation. LiNGAM-MMI
processes data as efficiently as traditional LiNGAM in scenarios without
confounding while effectively addressing confounding situations. Our
experimental results suggest that LiNGAM-MMI more accurately determines the
correct variable order, both in the presence and absence of confounding.</div><div><a href='http://arxiv.org/abs/2401.16661v3'>2401.16661v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04890v1")'>Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse
  Actions, Interventions and Sparse Temporal Dependencies</div>
<div id='2401.04890v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T02:38:21Z</div><div>Authors: Sébastien Lachapelle, Pau Rodríguez López, Yash Sharma, Katie Everett, Rémi Le Priol, Alexandre Lacoste, Simon Lacoste-Julien</div><div style='padding-top: 10px; width: 80ex'>This work introduces a novel principle for disentanglement we call mechanism
sparsity regularization, which applies when the latent factors of interest
depend sparsely on observed auxiliary variables and/or past latent factors. We
propose a representation learning method that induces disentanglement by
simultaneously learning the latent factors and the sparse causal graphical
model that explains them. We develop a nonparametric identifiability theory
that formalizes this principle and shows that the latent factors can be
recovered by regularizing the learned causal graph to be sparse. More
precisely, we show identifiablity up to a novel equivalence relation we call
"consistency", which allows some latent factors to remain entangled (hence the
term partial disentanglement). To describe the structure of this entanglement,
we introduce the notions of entanglement graphs and graph preserving functions.
We further provide a graphical criterion which guarantees complete
disentanglement, that is identifiability up to permutations and element-wise
transformations. We demonstrate the scope of the mechanism sparsity principle
as well as the assumptions it relies on with several worked out examples. For
instance, the framework shows how one can leverage multi-node interventions
with unknown targets on the latent factors to disentangle them. We further draw
connections between our nonparametric results and the now popular exponential
family assumption. Lastly, we propose an estimation procedure based on
variational autoencoders and a sparsity constraint and demonstrate it on
various synthetic datasets. This work is meant to be a significantly extended
version of Lachapelle et al. (2022).</div><div><a href='http://arxiv.org/abs/2401.04890v1'>2401.04890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00849v2")'>Score-based Causal Representation Learning: Linear and General
  Transformations</div>
<div id='2402.00849v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:40:03Z</div><div>Authors: Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, Ali Tajer</div><div style='padding-top: 10px; width: 80ex'>This paper addresses intervention-based causal representation learning (CRL)
under a general nonparametric latent causal model and an unknown transformation
that maps the latent variables to the observed variables. Linear and general
transformations are investigated. The paper addresses both the identifiability
and achievability aspects. Identifiability refers to determining
algorithm-agnostic conditions that ensure recovering the true latent causal
variables and the latent causal graph underlying them. Achievability refers to
the algorithmic aspects and addresses designing algorithms that achieve
identifiability guarantees. By drawing novel connections between score
functions (i.e., the gradients of the logarithm of density functions) and CRL,
this paper designs a score-based class of algorithms that ensures both
identifiability and achievability. First, the paper focuses on linear
transformations and shows that one stochastic hard intervention per node
suffices to guarantee identifiability. It also provides partial identifiability
guarantees for soft interventions, including identifiability up to ancestors
for general causal models and perfect latent graph recovery for sufficiently
non-linear causal models. Secondly, it focuses on general transformations and
shows that two stochastic hard interventions per node suffice for
identifiability. Notably, one does not need to know which pair of
interventional environments have the same node intervened.</div><div><a href='http://arxiv.org/abs/2402.00849v2'>2402.00849v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07578v1")'>Confounded Budgeted Causal Bandits</div>
<div id='2401.07578v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T10:26:13Z</div><div>Authors: Fateme Jamshidi, Jalal Etesami, Negar Kiyavash</div><div style='padding-top: 10px; width: 80ex'>We study the problem of learning 'good' interventions in a stochastic
environment modeled by its underlying causal graph. Good interventions refer to
interventions that maximize rewards. Specifically, we consider the setting of a
pre-specified budget constraint, where interventions can have non-uniform
costs. We show that this problem can be formulated as maximizing the expected
reward for a stochastic multi-armed bandit with side information. We propose an
algorithm to minimize the cumulative regret in general causal graphs. This
algorithm trades off observations and interventions based on their costs to
achieve the optimal reward. This algorithm generalizes the state-of-the-art
methods by allowing non-uniform costs and hidden confounders in the causal
graph. Furthermore, we develop an algorithm to minimize the simple regret in
the budgeted setting with non-uniform costs and also general causal graphs. We
provide theoretical guarantees, including both upper and lower bounds, as well
as empirical evaluations of our algorithms. Our empirical results showcase that
our algorithms outperform the state of the art.</div><div><a href='http://arxiv.org/abs/2401.07578v1'>2401.07578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00233v1")'>Causal Bandits with General Causal Models and Interventions</div>
<div id='2403.00233v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T02:28:49Z</div><div>Authors: Zirui Yan, Dennis Wei, Dmitriy Katz-Rogozhnikov, Prasanna Sattigeri, Ali Tajer</div><div style='padding-top: 10px; width: 80ex'>This paper considers causal bandits (CBs) for the sequential design of
interventions in a causal system. The objective is to optimize a reward
function via minimizing a measure of cumulative regret with respect to the best
sequence of interventions in hindsight. The paper advances the results on CBs
in three directions. First, the structural causal models (SCMs) are assumed to
be unknown and drawn arbitrarily from a general class $\mathcal{F}$ of
Lipschitz-continuous functions. Existing results are often focused on
(generalized) linear SCMs. Second, the interventions are assumed to be
generalized soft with any desired level of granularity, resulting in an
infinite number of possible interventions. The existing literature, in
contrast, generally adopts atomic and hard interventions. Third, we provide
general upper and lower bounds on regret. The upper bounds subsume (and
improve) known bounds for special cases. The lower bounds are generally
hitherto unknown. These bounds are characterized as functions of the (i) graph
parameters, (ii) eluder dimension of the space of SCMs, denoted by
$\operatorname{dim}(\mathcal{F})$, and (iii) the covering number of the
function space, denoted by ${\rm cn}(\mathcal{F})$. Specifically, the
cumulative achievable regret over horizon $T$ is $\mathcal{O}(K
d^{L-1}\sqrt{T\operatorname{dim}(\mathcal{F}) \log({\rm cn}(\mathcal{F}))})$,
where $K$ is related to the Lipschitz constants, $d$ is the graph's maximum
in-degree, and $L$ is the length of the longest causal path. The upper bound is
further refined for special classes of SCMs (neural network, polynomial, and
linear), and their corresponding lower bounds are provided.</div><div><a href='http://arxiv.org/abs/2403.00233v1'>2403.00233v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14777v1")'>Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent
  Factor Models</div>
<div id='2402.14777v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:37:33Z</div><div>Authors: Alvaro Ribot, Chandler Squires, Caroline Uhler</div><div style='padding-top: 10px; width: 80ex'>We consider the task of causal imputation, where we aim to predict the
outcomes of some set of actions across a wide range of possible contexts. As a
running example, we consider predicting how different drugs affect cells from
different cell types. We study the index-only setting, where the actions and
contexts are categorical variables with a finite number of possible values.
Even in this simple setting, a practical challenge arises, since often only a
small subset of possible action-context pairs have been studied. Thus, models
must extrapolate to novel action-context pairs, which can be framed as a form
of matrix completion with rows indexed by actions, columns indexed by contexts,
and matrix entries corresponding to outcomes. We introduce a novel SCM-based
model class, where the outcome is expressed as a counterfactual, actions are
expressed as interventions on an instrumental variable, and contexts are
defined based on the initial state of the system. We show that, under a
linearity assumption, this setup induces a latent factor model over the matrix
of outcomes, with an additional fixed effect term. To perform causal prediction
based on this model class, we introduce simple extension to the Synthetic
Interventions estimator (Agarwal et al., 2020). We evaluate several matrix
completion approaches on the PRISM drug repurposing dataset, showing that our
method outperforms all other considered matrix completion approaches.</div><div><a href='http://arxiv.org/abs/2402.14777v1'>2402.14777v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12710v1")'>Integrating Active Learning in Causal Inference with Interference: A
  Novel Approach in Online Experiments</div>
<div id='2402.12710v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:13:59Z</div><div>Authors: Hongtao Zhu, Sizhe Zhang, Yang Su, Zhenyu Zhao, Nan Chen</div><div style='padding-top: 10px; width: 80ex'>In the domain of causal inference research, the prevalent potential outcomes
framework, notably the Rubin Causal Model (RCM), often overlooks individual
interference and assumes independent treatment effects. This assumption,
however, is frequently misaligned with the intricate realities of real-world
scenarios, where interference is not merely a possibility but a common
occurrence. Our research endeavors to address this discrepancy by focusing on
the estimation of direct and spillover treatment effects under two assumptions:
(1) network-based interference, where treatments on neighbors within connected
networks affect one's outcomes, and (2) non-random treatment assignments
influenced by confounders. To improve the efficiency of estimating potentially
complex effects functions, we introduce an novel active learning approach:
Active Learning in Causal Inference with Interference (ACI). This approach uses
Gaussian process to flexibly model the direct and spillover treatment effects
as a function of a continuous measure of neighbors' treatment assignment. The
ACI framework sequentially identifies the experimental settings that demand
further data. It further optimizes the treatment assignments under the network
interference structure using genetic algorithms to achieve efficient learning
outcome. By applying our method to simulation data and a Tencent game dataset,
we demonstrate its feasibility in achieving accurate effects estimations with
reduced data requirements. This ACI approach marks a significant advancement in
the realm of data efficiency for causal inference, offering a robust and
efficient alternative to traditional methodologies, particularly in scenarios
characterized by complex interference patterns.</div><div><a href='http://arxiv.org/abs/2402.12710v1'>2402.12710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11124v1")'>Disentanglement in Implicit Causal Models via Switch Variable</div>
<div id='2402.11124v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T23:17:00Z</div><div>Authors: Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, Mark Crowley</div><div style='padding-top: 10px; width: 80ex'>Learning causal representations from observational and interventional data in
the absence of known ground-truth graph structures necessitates implicit latent
causal representation learning. Implicitly learning causal mechanisms typically
involves two categories of interventional data: hard and soft interventions. In
real-world scenarios, soft interventions are often more realistic than hard
interventions, as the latter require fully controlled environments. Unlike hard
interventions, which directly force changes in a causal variable, soft
interventions exert influence indirectly by affecting the causal mechanism. In
this paper, we tackle implicit latent causal representation learning in a
Variational Autoencoder (VAE) framework through soft interventions. Our
approach models soft interventions effects by employing a causal mechanism
switch variable designed to toggle between different causal mechanisms. In our
experiments, we consistently observe improved learning of identifiable, causal
representations, compared to baseline approaches.</div><div><a href='http://arxiv.org/abs/2402.11124v1'>2402.11124v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16974v1")'>CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement
  Learning</div>
<div id='2401.16974v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T12:57:52Z</div><div>Authors: Andreas W. M. Sauter, Nicolò Botteghi, Erman Acar, Aske Plaat</div><div style='padding-top: 10px; width: 80ex'>Causal discovery is the challenging task of inferring causal structure from
data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive
observations alone are not enough to distinguish correlation from causation,
there has been a recent push to incorporate interventions into machine learning
research. Reinforcement learning provides a convenient framework for such an
active approach to learning. This paper presents CORE, a deep reinforcement
learning-based approach for causal discovery and intervention planning. CORE
learns to sequentially reconstruct causal graphs from data while learning to
perform informative interventions. Our results demonstrate that CORE
generalizes to unseen graphs and efficiently uncovers causal structures.
Furthermore, CORE scales to larger graphs with up to 10 variables and
outperforms existing approaches in structure estimation accuracy and sample
efficiency. All relevant code and supplementary material can be found at
https://github.com/sa-and/CORE</div><div><a href='http://arxiv.org/abs/2401.16974v1'>2401.16974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17233v1")'>Hybrid Square Neural ODE Causal Modeling</div>
<div id='2402.17233v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T06:01:56Z</div><div>Authors: Bob Junyi Zou, Matthew E. Levine, Dessi P. Zaharieva, Ramesh Johari, Emily B. Fox</div><div style='padding-top: 10px; width: 80ex'>Hybrid models combine mechanistic ODE-based dynamics with flexible and
expressive neural network components. Such models have grown rapidly in
popularity, especially in scientific domains where such ODE-based modeling
offers important interpretability and validated causal grounding (e.g., for
counterfactual reasoning). The incorporation of mechanistic models also
provides inductive bias in standard blackbox modeling approaches, critical when
learning from small datasets or partially observed, complex systems.
Unfortunately, as hybrid models become more flexible, the causal grounding
provided by the mechanistic model can quickly be lost. We address this problem
by leveraging another common source of domain knowledge: ranking of treatment
effects for a set of interventions, even if the precise treatment effect is
unknown. We encode this information in a causal loss that we combine with the
standard predictive loss to arrive at a hybrid loss that biases our learning
towards causally valid hybrid models. We demonstrate our ability to achieve a
win-win -- state-of-the-art predictive performance and causal validity -- in
the challenging task of modeling glucose dynamics during exercise.</div><div><a href='http://arxiv.org/abs/2402.17233v1'>2402.17233v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02154v1")'>Disentangle Estimation of Causal Effects from Cross-Silo Data</div>
<div id='2401.02154v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T09:05:37Z</div><div>Authors: Yuxuan Liu, Haozhao Wang, Shuang Wang, Zhiming He, Wenchao Xu, Jialiang Zhu, Fan Yang</div><div style='padding-top: 10px; width: 80ex'>Estimating causal effects among different events is of great importance to
critical fields such as drug development. Nevertheless, the data features
associated with events may be distributed across various silos and remain
private within respective parties, impeding direct information exchange between
them. This, in turn, can result in biased estimations of local causal effects,
which rely on the characteristics of only a subset of the covariates. To tackle
this challenge, we introduce an innovative disentangle architecture designed to
facilitate the seamless cross-silo transmission of model parameters, enriched
with causal mechanisms, through a combination of shared and private branches.
Besides, we introduce global constraints into the equation to effectively
mitigate bias within the various missing domains, thereby elevating the
accuracy of our causal effect estimation. Extensive experiments conducted on
new semi-synthetic datasets show that our method outperforms state-of-the-art
baselines.</div><div><a href='http://arxiv.org/abs/2401.02154v1'>2401.02154v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07735v2")'>Graph Structure Inference with BAM: Introducing the Bilinear Attention
  Mechanism</div>
<div id='2402.07735v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:48:58Z</div><div>Authors: Philipp Froehlich, Heinz Koeppl</div><div style='padding-top: 10px; width: 80ex'>In statistics and machine learning, detecting dependencies in datasets is a
central challenge. We propose a novel neural network model for supervised graph
structure learning, i.e., the process of learning a mapping between
observational data and their underlying dependence structure. The model is
trained with variably shaped and coupled simulated input data and requires only
a single forward pass through the trained network for inference. By leveraging
structural equation models and employing randomly generated multivariate
Chebyshev polynomials for the simulation of training data, our method
demonstrates robust generalizability across both linear and various types of
non-linear dependencies. We introduce a novel bilinear attention mechanism
(BAM) for explicit processing of dependency information, which operates on the
level of covariance matrices of transformed data and respects the geometry of
the manifold of symmetric positive definite matrices. Empirical evaluation
demonstrates the robustness of our method in detecting a wide range of
dependencies, excelling in undirected graph estimation and proving competitive
in completed partially directed acyclic graph estimation through a novel
two-step approach.</div><div><a href='http://arxiv.org/abs/2402.07735v2'>2402.07735v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11332v1")'>Graph Neural Network based Double Machine Learning Estimator of Network
  Causal Effects</div>
<div id='2403.11332v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T20:23:42Z</div><div>Authors: Seyedeh Baharan Khatami, Harsh Parikh, Haowei Chen, Sudeepa Roy, Babak Salimi</div><div style='padding-top: 10px; width: 80ex'>Our paper addresses the challenge of inferring causal effects in social
network data, characterized by complex interdependencies among individuals
resulting in challenges such as non-independence of units, interference (where
a unit's outcome is affected by neighbors' treatments), and introduction of
additional confounding factors from neighboring units. We propose a novel
methodology combining graph neural networks and double machine learning,
enabling accurate and efficient estimation of direct and peer effects using a
single observational social network. Our approach utilizes graph isomorphism
networks in conjunction with double machine learning to effectively adjust for
network confounders and consistently estimate the desired causal effects. We
demonstrate that our estimator is both asymptotically normal and
semiparametrically efficient. A comprehensive evaluation against four
state-of-the-art baseline methods using three semi-synthetic social network
datasets reveals our method's on-par or superior efficacy in precise causal
effect estimation. Further, we illustrate the practical application of our
method through a case study that investigates the impact of Self-Help Group
participation on financial risk tolerance. The results indicate a significant
positive direct effect, underscoring the potential of our approach in social
network analysis. Additionally, we explore the effects of network sparsity on
estimation performance.</div><div><a href='http://arxiv.org/abs/2403.11332v1'>2403.11332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06864v1")'>Deep Learning With DAGs</div>
<div id='2401.06864v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T19:35:54Z</div><div>Authors: Sourabh Balgi, Adel Daoud, Jose M. Peña, Geoffrey T. Wodtke, Jesse Zhou</div><div style='padding-top: 10px; width: 80ex'>Social science theories often postulate causal relationships among a set of
variables or events. Although directed acyclic graphs (DAGs) are increasingly
used to represent these theories, their full potential has not yet been
realized in practice. As non-parametric causal models, DAGs require no
assumptions about the functional form of the hypothesized relationships.
Nevertheless, to simplify the task of empirical evaluation, researchers tend to
invoke such assumptions anyway, even though they are typically arbitrary and do
not reflect any theoretical content or prior knowledge. Moreover, functional
form assumptions can engender bias, whenever they fail to accurately capture
the complexity of the causal system under investigation. In this article, we
introduce causal-graphical normalizing flows (cGNFs), a novel approach to
causal inference that leverages deep neural networks to empirically evaluate
theories represented as DAGs. Unlike conventional approaches, cGNFs model the
full joint distribution of the data according to a DAG supplied by the analyst,
without relying on stringent assumptions about functional form. In this way,
the method allows for flexible, semi-parametric estimation of any causal
estimand that can be identified from the DAG, including total effects,
conditional effects, direct and indirect effects, and path-specific effects. We
illustrate the method with a reanalysis of Blau and Duncan's (1967) model of
status attainment and Zhou's (2019) model of conditional versus controlled
mobility. To facilitate adoption, we provide open-source software together with
a series of online tutorials for implementing cGNFs. The article concludes with
a discussion of current limitations and directions for future development.</div><div><a href='http://arxiv.org/abs/2401.06864v1'>2401.06864v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16986v2")'>Causal Machine Learning for Cost-Effective Allocation of Development Aid</div>
<div id='2401.16986v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T13:15:59Z</div><div>Authors: Milan Kuzmanovic, Dennis Frauen, Tobias Hatt, Stefan Feuerriegel</div><div style='padding-top: 10px; width: 80ex'>The Sustainable Development Goals (SDGs) of the United Nations provide a
blueprint of a better future by 'leaving no one behind', and, to achieve the
SDGs by 2030, poor countries require immense volumes of development aid. In
this paper, we develop a causal machine learning framework for predicting
heterogeneous treatment effects of aid disbursements to inform effective aid
allocation. Specifically, our framework comprises three components: (i) a
balancing autoencoder that uses representation learning to embed
high-dimensional country characteristics while addressing treatment selection
bias; (ii) a counterfactual generator to compute counterfactual outcomes for
varying aid volumes to address small sample-size settings; and (iii) an
inference model that is used to predict heterogeneous treatment-response
curves. We demonstrate the effectiveness of our framework using data with
official development aid earmarked to end HIV/AIDS in 105 countries, amounting
to more than USD 5.2 billion. For this, we first show that our framework
successfully computes heterogeneous treatment-response curves using
semi-synthetic data. Then, we demonstrate our framework using real-world HIV
data. Our framework points to large opportunities for a more effective aid
allocation, suggesting that the total number of new HIV infections could be
reduced by up to 3.3% (~50,000 cases) compared to the current allocation
practice.</div><div><a href='http://arxiv.org/abs/2401.16986v2'>2401.16986v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06557v1")'>Treatment-Aware Hyperbolic Representation Learning for Causal Effect
  Estimation with Social Networks</div>
<div id='2401.06557v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T13:02:39Z</div><div>Authors: Ziqiang Cui, Xing Tang, Yang Qiao, Bowei He, Liang Chen, Xiuqiang He, Chen Ma</div><div style='padding-top: 10px; width: 80ex'>Estimating the individual treatment effect (ITE) from observational data is a
crucial research topic that holds significant value across multiple domains.
How to identify hidden confounders poses a key challenge in ITE estimation.
Recent studies have incorporated the structural information of social networks
to tackle this challenge, achieving notable advancements. However, these
methods utilize graph neural networks to learn the representation of hidden
confounders in Euclidean space, disregarding two critical issues: (1) the
social networks often exhibit a scalefree structure, while Euclidean embeddings
suffer from high distortion when used to embed such graphs, and (2) each
ego-centric network within a social network manifests a treatment-related
characteristic, implying significant patterns of hidden confounders. To address
these issues, we propose a novel method called Treatment-Aware Hyperbolic
Representation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic
space to encode the social networks, thereby effectively reducing the
distortion of confounder representation caused by Euclidean embeddings.
Secondly, we design a treatment-aware relationship identification module that
enhances the representation of hidden confounders by identifying whether an
individual and her neighbors receive the same treatment. Extensive experiments
on two benchmark datasets are conducted to demonstrate the superiority of our
method.</div><div><a href='http://arxiv.org/abs/2401.06557v1'>2401.06557v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06489v1")'>Graph Neural Network with Two Uplift Estimators for Label-Scarcity
  Individual Uplift Modeling</div>
<div id='2403.06489v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T07:51:27Z</div><div>Authors: Dingyuan Zhu, Daixin Wang, Zhiqiang Zhang, Kun Kuang, Yan Zhang, Yulin Kang, Jun Zhou</div><div style='padding-top: 10px; width: 80ex'>Uplift modeling aims to measure the incremental effect, which we call uplift,
of a strategy or action on the users from randomized experiments or
observational data. Most existing uplift methods only use individual data,
which are usually not informative enough to capture the unobserved and complex
hidden factors regarding the uplift. Furthermore, uplift modeling scenario
usually has scarce labeled data, especially for the treatment group, which also
poses a great challenge for model training. Considering that the neighbors'
features and the social relationships are very informative to characterize a
user's uplift, we propose a graph neural network-based framework with two
uplift estimators, called GNUM, to learn from the social graph for uplift
estimation. Specifically, we design the first estimator based on a
class-transformed target. The estimator is general for all types of outcomes,
and is able to comprehensively model the treatment and control group data
together to approach the uplift. When the outcome is discrete, we further
design the other uplift estimator based on our defined partial labels, which is
able to utilize more labeled data from both the treatment and control groups,
to further alleviate the label scarcity problem. Comprehensive experiments on a
public dataset and two industrial datasets show a superior performance of our
proposed framework over state-of-the-art methods under various evaluation
metrics. The proposed algorithms have been deployed online to serve real-world
uplift estimation scenarios.</div><div><a href='http://arxiv.org/abs/2403.06489v1'>2403.06489v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00178v1")'>Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent
  Dynamical Systems</div>
<div id='2403.00178v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T23:07:07Z</div><div>Authors: Zijie Huang, Jeehyun Hwang, Junkai Zhang, Jinwoo Baik, Weitong Zhang, Dominik Wodarz, Yizhou Sun, Quanquan Gu, Wei Wang</div><div style='padding-top: 10px; width: 80ex'>Real-world multi-agent systems are often dynamic and continuous, where the
agents co-evolve and undergo changes in their trajectories and interactions
over time. For example, the COVID-19 transmission in the U.S. can be viewed as
a multi-agent system, where states act as agents and daily population movements
between them are interactions. Estimating the counterfactual outcomes in such
systems enables accurate future predictions and effective decision-making, such
as formulating COVID-19 policies. However, existing methods fail to model the
continuous dynamic effects of treatments on the outcome, especially when
multiple treatments (e.g., "stay-at-home" and "get-vaccine" policies) are
applied simultaneously. To tackle this challenge, we propose Causal Graph
Ordinary Differential Equations (CAG-ODE), a novel model that captures the
continuous interaction among agents using a Graph Neural Network (GNN) as the
ODE function. The key innovation of our model is to learn time-dependent
representations of treatments and incorporate them into the ODE function,
enabling precise predictions of potential outcomes. To mitigate confounding
bias, we further propose two domain adversarial learning-based objectives,
which enable our model to learn balanced continuous representations that are
not affected by treatments or interference. Experiments on two datasets (i.e.,
COVID-19 and tumor growth) demonstrate the superior performance of our proposed
model.</div><div><a href='http://arxiv.org/abs/2403.00178v1'>2403.00178v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08105v1")'>Learning Cartesian Product Graphs with Laplacian Constraints</div>
<div id='2402.08105v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:48:30Z</div><div>Authors: Changhao Shi, Gal Mishne</div><div style='padding-top: 10px; width: 80ex'>Graph Laplacian learning, also known as network topology inference, is a
problem of great interest to multiple communities. In Gaussian graphical models
(GM), graph learning amounts to endowing covariance selection with the
Laplacian structure. In graph signal processing (GSP), it is essential to infer
the unobserved graph from the outputs of a filtering system. In this paper, we
study the problem of learning Cartesian product graphs under Laplacian
constraints. The Cartesian graph product is a natural way for modeling
higher-order conditional dependencies and is also the key for generalizing GSP
to multi-way tensors. We establish statistical consistency for the penalized
maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and
propose an efficient algorithm to solve the problem. We also extend our method
for efficient joint graph learning and imputation in the presence of structural
missing values. Experiments on synthetic and real-world datasets demonstrate
that our method is superior to previous GSP and GM methods.</div><div><a href='http://arxiv.org/abs/2402.08105v1'>2402.08105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02608v1")'>DNNLasso: Scalable Graph Learning for Matrix-Variate Data</div>
<div id='2403.02608v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T02:49:00Z</div><div>Authors: Meixia Lin, Yangjing Zhang</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of jointly learning row-wise and column-wise
dependencies of matrix-variate observations, which are modelled separately by
two precision matrices. Due to the complicated structure of Kronecker-product
precision matrices in the commonly used matrix-variate Gaussian graphical
models, a sparser Kronecker-sum structure was proposed recently based on the
Cartesian product of graphs. However, existing methods for estimating
Kronecker-sum structured precision matrices do not scale well to large scale
datasets. In this paper, we introduce DNNLasso, a diagonally non-negative
graphical lasso model for estimating the Kronecker-sum structured precision
matrix, which outperforms the state-of-the-art methods by a large margin in
both accuracy and computational time. Our code is available at
https://github.com/YangjingZhang/DNNLasso.</div><div><a href='http://arxiv.org/abs/2403.02608v1'>2403.02608v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07012v1")'>An ADRC-Incorporated Stochastic Gradient Descent Algorithm for Latent
  Factor Analysis</div>
<div id='2401.07012v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T08:38:54Z</div><div>Authors: Jinli Li, Ye Yuan</div><div style='padding-top: 10px; width: 80ex'>High-dimensional and incomplete (HDI) matrix contains many complex
interactions between numerous nodes. A stochastic gradient descent (SGD)-based
latent factor analysis (LFA) model is remarkably effective in extracting
valuable information from an HDI matrix. However, such a model commonly
encounters the problem of slow convergence because a standard SGD algorithm
only considers the current learning error to compute the stochastic gradient
without considering the historical and future state of the learning error. To
address this critical issue, this paper innovatively proposes an
ADRC-incorporated SGD (ADS) algorithm by refining the instance learning error
by considering the historical and future state by following the principle of an
ADRC controller. With it, an ADS-based LFA model is further achieved for fast
and accurate latent factor analysis on an HDI matrix. Empirical studies on two
HDI datasets demonstrate that the proposed model outperforms the
state-of-the-art LFA models in terms of computational efficiency and accuracy
for predicting the missing data of an HDI matrix.</div><div><a href='http://arxiv.org/abs/2401.07012v1'>2401.07012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11948v1")'>Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model</div>
<div id='2402.11948v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:43:00Z</div><div>Authors: Jialiang Wang, Weiling Li, Yurong Zhong, Xin Luo</div><div style='padding-top: 10px; width: 80ex'>Interactions among large number of entities is naturally high-dimensional and
incomplete (HDI) in many big data related tasks. Behavioral characteristics of
users are hidden in these interactions, hence, effective representation of the
HDI data is a fundamental task for understanding user behaviors. Latent factor
analysis (LFA) model has proven to be effective in representing HDI data. The
performance of an LFA model relies heavily on its training process, which is a
non-convex optimization. It has been proven that incorporating local curvature
and preprocessing gradients during its training process can lead to superior
performance compared to LFA models built with first-order family methods.
However, with the escalation of data volume, the feasibility of second-order
algorithms encounters challenges. To address this pivotal issue, this paper
proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for
building an LFA model. It leverages the dominant diagonal blocks in the
generalized Gauss-Newton matrix based on the analysis of the Hessian matrix of
LFA model and serves as an intermediary strategy bridging the gap between
first-order and second-order optimization methods. Experiment results indicate
that, with Mini-Hes, the LFA model outperforms several state-of-the-art models
in addressing missing data estimation task on multiple real HDI datasets from
recommender system. (The source code of Mini-Hes is available at
https://github.com/Goallow/Mini-Hes)</div><div><a href='http://arxiv.org/abs/2402.11948v1'>2402.11948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07012v1")'>Non-Intrusive Load Monitoring with Missing Data Imputation Based on
  Tensor Decomposition</div>
<div id='2403.07012v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T10:01:49Z</div><div>Authors: DengYu Shi</div><div style='padding-top: 10px; width: 80ex'>With the widespread adoption of Non-Intrusive Load Monitoring (NILM) in
building energy management, ensuring the high quality of NILM data has become
imperative. However, practical applications of NILM face challenges associated
with data loss, significantly impacting accuracy and reliability in energy
management. This paper addresses the issue of NILM data loss by introducing an
innovative tensor completion(TC) model- Proportional-Integral-Derivative
(PID)-incorporated Non-negative Latent Factorization of Tensors (PNLFT) with
twofold ideas: 1) To tackle the issue of slow convergence in Latent
Factorization of Tensors (LFT) using Stochastic Gradient Descent (SGD), a
Proportional-Integral-Derivative controller is introduced during the learning
process. The PID controller utilizes historical and current information to
control learning residuals. 2) Considering the characteristics of NILM data,
non-negative update rules are proposed in the model's learning scheme.
Experimental results on three datasets demonstrate that, compared to
state-of-the-art models, the proposed model exhibits noteworthy enhancements in
both convergence speed and accuracy.</div><div><a href='http://arxiv.org/abs/2403.07012v1'>2403.07012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.13769v1")'>Multiview Graph Learning with Consensus Graph</div>
<div id='2401.13769v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T19:35:54Z</div><div>Authors: Abdullah Karaaslanli, Selin Aviyente</div><div style='padding-top: 10px; width: 80ex'>Graph topology inference, i.e., learning graphs from a given set of nodal
observations, is a significant task in many application domains. Existing
approaches are mostly limited to learning a single graph assuming that the
observed data is homogeneous. This is problematic because many modern datasets
are heterogeneous or mixed and involve multiple related graphs, i.e., multiview
graphs. Recent work proposing to learn multiview graphs ensures the similarity
of learned view graphs through pairwise regularization, where each pair of
views is encouraged to have similar structures. However, this approach cannot
infer the shared structure across views. In this work, we propose an
alternative method based on consensus regularization, where views are ensured
to be similar through a learned consensus graph representing the common
structure of the views. In particular, we propose an optimization problem,
where graph data is assumed to be smooth over the multiview graph and the
topology of the individual views and that of the consensus graph are learned,
simultaneously. Our optimization problem is designed to be general in the sense
that different regularization functions can be used depending on what the
shared structure across views is. Moreover, we propose two regularization
functions that extend fused and group graphical lasso to consensus based
regularization. Proposed multiview graph learning is evaluated on simulated
data and shown to have better performance than existing methods. It is also
employed to infer the functional brain connectivity networks of multiple
subjects from their electroencephalogram (EEG) recordings. The proposed method
reveals the structure shared by subjects as well as the characteristics unique
to each subject.</div><div><a href='http://arxiv.org/abs/2401.13769v1'>2401.13769v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07745v1")'>Probabilistic Easy Variational Causal Effect</div>
<div id='2403.07745v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T15:28:21Z</div><div>Authors: Usef Faghihi, Amir Saki</div><div style='padding-top: 10px; width: 80ex'>Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one
hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the
total variation and the flux of $g$, we develop a point of view in causal
inference capable of dealing with a broad domain of causal problems. Indeed, we
focus on a function, called Probabilistic Easy Variational Causal Effect
(PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect
to continuously and interventionally changing the values of $X$ while keeping
the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree
managing the strengths of probability density values $f(x|z)$. On the other
hand, we generalize the above idea for the discrete case and show its
compatibility with the continuous case. Further, we investigate some properties
of PEACE using measure theoretical concepts. Furthermore, we provide some
identifiability criteria and several examples showing the generic capability of
PEACE. We note that PEACE can deal with the causal problems for which
micro-level or just macro-level changes in the value of the input variables are
important. Finally, PEACE is stable under small changes in $\partial
g_{in}/\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is
obtained from $g$ by removing all functional relationships defining $X$ and
$Z$.</div><div><a href='http://arxiv.org/abs/2403.07745v1'>2403.07745v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05358v1")'>Variational Inference of Parameters in Opinion Dynamics Models</div>
<div id='2403.05358v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T14:45:18Z</div><div>Authors: Jacopo Lenti, Fabrizio Silvestri, Gianmarco De Francisci Morales</div><div style='padding-top: 10px; width: 80ex'>Despite the frequent use of agent-based models (ABMs) for studying social
phenomena, parameter estimation remains a challenge, often relying on costly
simulation-based heuristics. This work uses variational inference to estimate
the parameters of an opinion dynamics ABM, by transforming the estimation
problem into an optimization task that can be solved directly.
  Our proposal relies on probabilistic generative ABMs (PGABMs): we start by
synthesizing a probabilistic generative model from the ABM rules. Then, we
transform the inference process into an optimization problem suitable for
automatic differentiation. In particular, we use the Gumbel-Softmax
reparameterization for categorical agent attributes and stochastic variational
inference for parameter estimation. Furthermore, we explore the trade-offs of
using variational distributions with different complexity: normal distributions
and normalizing flows.
  We validate our method on a bounded confidence model with agent roles
(leaders and followers). Our approach estimates both macroscopic (bounded
confidence intervals and backfire thresholds) and microscopic ($200$
categorical, agent-level roles) more accurately than simulation-based and MCMC
methods. Consequently, our technique enables experts to tune and validate their
ABMs against real-world observations, thus providing insights into human
behavior in social systems via data-driven analysis.</div><div><a href='http://arxiv.org/abs/2403.05358v1'>2403.05358v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17087v1")'>A Note on Bayesian Networks with Latent Root Variables</div>
<div id='2402.17087v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T23:53:34Z</div><div>Authors: Marco Zaffalon, Alessandro Antonucci</div><div style='padding-top: 10px; width: 80ex'>We characterise the likelihood function computed from a Bayesian network with
latent variables as root nodes. We show that the marginal distribution over the
remaining, manifest, variables also factorises as a Bayesian network, which we
call empirical. A dataset of observations of the manifest variables allows us
to quantify the parameters of the empirical Bayesian net. We prove that (i) the
likelihood of such a dataset from the original Bayesian network is dominated by
the global maximum of the likelihood from the empirical one; and that (ii) such
a maximum is attained if and only if the parameters of the Bayesian network are
consistent with those of the empirical model.</div><div><a href='http://arxiv.org/abs/2402.17087v1'>2402.17087v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18651v1")'>Quantifying Human Priors over Social and Navigation Networks</div>
<div id='2402.18651v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T19:00:36Z</div><div>Authors: Gecia Bravo-Hermsdorff</div><div style='padding-top: 10px; width: 80ex'>Human knowledge is largely implicit and relational -- do we have a friend in
common? can I walk from here to there? In this work, we leverage the
combinatorial structure of graphs to quantify human priors over such relational
data. Our experiments focus on two domains that have been continuously relevant
over evolutionary timescales: social interaction and spatial navigation. We
find that some features of the inferred priors are remarkably consistent, such
as the tendency for sparsity as a function of graph size. Other features are
domain-specific, such as the propensity for triadic closure in social
interactions. More broadly, our work demonstrates how nonclassical statistical
analysis of indirect behavioral experiments can be used to efficiently model
latent biases in the data.</div><div><a href='http://arxiv.org/abs/2402.18651v1'>2402.18651v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14229v1")'>Sample-Efficient Linear Regression with Self-Selection Bias</div>
<div id='2402.14229v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T02:20:24Z</div><div>Authors: Jason Gaitonde, Elchanan Mossel</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of linear regression with self-selection bias in the
unknown-index setting, as introduced in recent work by Cherapanamjeri,
Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$
i.i.d. samples $(\mathbf{x}_{\ell},z_{\ell})_{\ell=1}^m$ where
$z_{\ell}=\max_{i\in [k]}\{\mathbf{x}_{\ell}^T\mathbf{w}_i+\eta_{i,\ell}\}$,
but the maximizing index $i_{\ell}$ is unobserved. Here, the
$\mathbf{x}_{\ell}$ are assumed to be $\mathcal{N}(0,I_n)$ and the noise
distribution $\mathbf{\eta}_{\ell}\sim \mathcal{D}$ is centered and independent
of $\mathbf{x}_{\ell}$. We provide a novel and near optimally sample-efficient
(in terms of $k$) algorithm to recover $\mathbf{w}_1,\ldots,\mathbf{w}_k\in
\mathbb{R}^n$ up to additive $\ell_2$-error $\varepsilon$ with polynomial
sample complexity $\tilde{O}(n)\cdot \mathsf{poly}(k,1/\varepsilon)$ and
significantly improved time complexity
$\mathsf{poly}(n,k,1/\varepsilon)+O(\log(k)/\varepsilon)^{O(k)}$. When
$k=O(1)$, our algorithm runs in $\mathsf{poly}(n,1/\varepsilon)$ time,
generalizing the polynomial guarantee of an explicit moment matching algorithm
of Cherapanamjeri, et al. for $k=2$ and when it is known that
$\mathcal{D}=\mathcal{N}(0,I_k)$. Our algorithm succeeds under significantly
relaxed noise assumptions, and therefore also succeeds in the related setting
of max-linear regression where the added noise is taken outside the maximum.
For this problem, our algorithm is efficient in a much larger range of $k$ than
the state-of-the-art due to Ghosh, Pananjady, Guntuboyina, and Ramchandran
[IEEE Trans. Inf. Theory 2022] for not too small $\varepsilon$, and leads to
improved algorithms for any $\varepsilon$ by providing a warm start for
existing local convergence methods.</div><div><a href='http://arxiv.org/abs/2402.14229v1'>2402.14229v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03562v1")'>Efficient Algorithms for Empirical Group Distributional Robust
  Optimization and Beyond</div>
<div id='2403.03562v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T09:14:24Z</div><div>Authors: Dingzhi Yu, Yunuo Cai, Wei Jiang, Lijun Zhang</div><div style='padding-top: 10px; width: 80ex'>We investigate the empirical counterpart of group distributionally robust
optimization (GDRO), which aims to minimize the maximal empirical risk across
$m$ distinct groups. We formulate empirical GDRO as a $\textit{two-level}$
finite-sum convex-concave minimax optimization problem and develop a stochastic
variance reduced mirror prox algorithm. Unlike existing methods, we construct
the stochastic gradient by per-group sampling technique and perform variance
reduction for all groups, which fully exploits the $\textit{two-level}$
finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot
and mirror snapshot point by a one-index-shifted weighted average, which
distinguishes us from the naive ergodic average. Our algorithm also supports
non-constant learning rates, which is different from existing literature. We
establish convergence guarantees both in expectation and with high probability,
demonstrating a complexity of
$\mathcal{O}\left(\frac{m\sqrt{\bar{n}\ln{m}}}{\varepsilon}\right)$, where
$\bar n$ is the average number of samples among $m$ groups. Remarkably, our
approach outperforms the state-of-the-art method by a factor of $\sqrt{m}$.
Furthermore, we extend our methodology to deal with the empirical minimax
excess risk optimization (MERO) problem and manage to give the expectation
bound and the high probability bound, accordingly. The complexity of our
empirical MERO algorithm matches that of empirical GDRO at
$\mathcal{O}\left(\frac{m\sqrt{\bar{n}\ln{m}}}{\varepsilon}\right)$,
significantly surpassing the bounds of existing methods.</div><div><a href='http://arxiv.org/abs/2403.03562v1'>2403.03562v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03589v1")'>Active Adaptive Experimental Design for Treatment Effect Estimation with
  Covariate Choices</div>
<div id='2403.03589v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T10:24:44Z</div><div>Authors: Masahiro Kato, Akihiro Oga, Wataru Komatsubara, Ryo Inokuchi</div><div style='padding-top: 10px; width: 80ex'>This study designs an adaptive experiment for efficiently estimating average
treatment effect (ATEs). We consider an adaptive experiment where an
experimenter sequentially samples an experimental unit from a covariate density
decided by the experimenter and assigns a treatment. After assigning a
treatment, the experimenter observes the corresponding outcome immediately. At
the end of the experiment, the experimenter estimates an ATE using gathered
samples. The objective of the experimenter is to estimate the ATE with a
smaller asymptotic variance. Existing studies have designed experiments that
adaptively optimize the propensity score (treatment-assignment probability). As
a generalization of such an approach, we propose a framework under which an
experimenter optimizes the covariate density, as well as the propensity score,
and find that optimizing both covariate density and propensity score reduces
the asymptotic variance more than optimizing only the propensity score. Based
on this idea, in each round of our experiment, the experimenter optimizes the
covariate density and propensity score based on past observations. To design an
adaptive experiment, we first derive the efficient covariate density and
propensity score that minimizes the semiparametric efficiency bound, a lower
bound for the asymptotic variance given a fixed covariate density and a fixed
propensity score. Next, we design an adaptive experiment using the efficient
covariate density and propensity score sequentially estimated during the
experiment. Lastly, we propose an ATE estimator whose asymptotic variance
aligns with the minimized semiparametric efficiency bound.</div><div><a href='http://arxiv.org/abs/2403.03589v1'>2403.03589v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02520v1")'>Structured Matrix Learning under Arbitrary Entrywise Dependence and
  Estimation of Markov Transition Kernel</div>
<div id='2401.02520v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T20:13:23Z</div><div>Authors: Jinhang Chai, Jianqing Fan</div><div style='padding-top: 10px; width: 80ex'>The problem of structured matrix estimation has been studied mostly under
strong noise dependence assumptions. This paper considers a general framework
of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come
from any joint distribution with arbitrary dependence across entries. We
propose an incoherent-constrained least-square estimator and prove its
tightness both in the sense of deterministic lower bound and matching minimax
risks under various noise distributions. To attain this, we establish a novel
result asserting that the difference between two arbitrary low-rank incoherent
matrices must spread energy out across its entries, in other words cannot be
too sparse, which sheds light on the structure of incoherent low-rank matrices
and may be of independent interest. We then showcase the applications of our
framework to several important statistical machine learning problems. In the
problem of estimating a structured Markov transition kernel, the proposed
method achieves the minimax optimality and the result can be extended to
estimating the conditional mean operator, a crucial component in reinforcement
learning. The applications to multitask regression and structured covariance
estimation are also presented. We propose an alternating minimization algorithm
to approximately solve the potentially hard optimization problem. Numerical
results corroborate the effectiveness of our method which typically converges
in a few steps.</div><div><a href='http://arxiv.org/abs/2401.02520v1'>2401.02520v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07298v1")'>Efficient Frameworks for Generalized Low-Rank Matrix Bandit Problems</div>
<div id='2401.07298v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T14:14:19Z</div><div>Authors: Yue Kang, Cho-Jui Hsieh, Thomas C. M. Lee</div><div style='padding-top: 10px; width: 80ex'>In the stochastic contextual low-rank matrix bandit problem, the expected
reward of an action is given by the inner product between the action's feature
matrix and some fixed, but initially unknown $d_1$ by $d_2$ matrix $\Theta^*$
with rank $r \ll \{d_1, d_2\}$, and an agent sequentially takes actions based
on past experience to maximize the cumulative reward. In this paper, we study
the generalized low-rank matrix bandit problem, which has been recently
proposed in \cite{lu2021low} under the Generalized Linear Model (GLM)
framework. To overcome the computational infeasibility and theoretical restrain
of existing algorithms on this problem, we first propose the G-ESTT framework
that modifies the idea from \cite{jun2019bilinear} by using Stein's method on
the subspace estimation and then leverage the estimated subspaces via a
regularization idea. Furthermore, we remarkably improve the efficiency of
G-ESTT by using a novel exclusion idea on the estimated subspace instead, and
propose the G-ESTS framework. We also show that G-ESTT can achieve the
$\tilde{O}(\sqrt{(d_1+d_2)MrT})$ bound of regret while G-ESTS can achineve the
$\tilde{O}(\sqrt{(d_1+d_2)^{3/2}Mr^{3/2}T})$ bound of regret under mild
assumption up to logarithm terms, where $M$ is some problem dependent value.
Under a reasonable assumption that $M = O((d_1+d_2)^2)$ in our problem setting,
the regret of G-ESTT is consistent with the current best regret of
$\tilde{O}((d_1+d_2)^{3/2} \sqrt{rT}/D_{rr})$~\citep{lu2021low} ($D_{rr}$ will
be defined later). For completeness, we conduct experiments to illustrate that
our proposed algorithms, especially G-ESTS, are also computationally tractable
and consistently outperform other state-of-the-art (generalized) linear matrix
bandit methods based on a suite of simulations.</div><div><a href='http://arxiv.org/abs/2401.07298v1'>2401.07298v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15739v1")'>Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery</div>
<div id='2402.15739v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T06:36:08Z</div><div>Authors: Yassir Jedra, William Réveillard, Stefan Stojanovic, Alexandre Proutiere</div><div style='padding-top: 10px; width: 80ex'>We study contextual bandits with low-rank structure where, in each round, if
the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner
observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward
matrix. Successive contexts are generated randomly in an i.i.d. manner and are
revealed to the learner. For such bandits, we present efficient algorithms for
policy evaluation, best policy identification and regret minimization. For
policy evaluation and best policy identification, we show that our algorithms
are nearly minimax optimal. For instance, the number of samples required to
return an $\varepsilon$-optimal policy with probability at least $1-\delta$
typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret
minimization algorithm enjoys minimax guarantees scaling as
$r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the
proposed algorithms consist of two phases: they first leverage spectral methods
to estimate the left and right singular subspaces of the low-rank reward
matrix. We show that these estimates enjoy tight error guarantees in the
two-to-infinity norm. This in turn allows us to reformulate our problems as a
misspecified linear bandit problem with dimension roughly $r(m+n)$ and
misspecification controlled by the subspace recovery error, as well as to
design the second phase of our algorithms efficiently.</div><div><a href='http://arxiv.org/abs/2402.15739v1'>2402.15739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12042v1")'>Linear bandits with polylogarithmic minimax regret</div>
<div id='2402.12042v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T10:56:47Z</div><div>Authors: Josep Lumbreras, Marco Tomamichel</div><div style='padding-top: 10px; width: 80ex'>We study a noise model for linear stochastic bandits for which the
subgaussian noise parameter vanishes linearly as we select actions on the unit
sphere closer and closer to the unknown vector. We introduce an algorithm for
this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time
horizon $T$, in stark contrast the square root scaling of this regret for
typical bandit algorithms. Our strategy, based on weighted least-squares
estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega
(\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step
$t$ through geometrical arguments that are independent of the noise model and
might be of independent interest. This allows us to tightly control the
expected regret in each time step to be of the order $O(\frac1{t})$, leading to
the logarithmic scaling of the cumulative regret.</div><div><a href='http://arxiv.org/abs/2402.12042v1'>2402.12042v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03361v1")'>Chained Information-Theoretic bounds and Tight Regret Rate for Linear
  Bandit Problems</div>
<div id='2403.03361v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T23:08:18Z</div><div>Authors: Amaury Gouverneur, Borja Rodríguez-Gálvez, Tobias J. Oechtering, Mikael Skoglund</div><div style='padding-top: 10px; width: 80ex'>This paper studies the Bayesian regret of a variant of the Thompson-Sampling
algorithm for bandit problems. It builds upon the information-theoretic
framework of [Russo and Van Roy, 2015] and, more specifically, on the
rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a
bound with regret rate of $O(d\sqrt{T \log(T)})$ for the $d$-dimensional linear
bandit setting. We focus on bandit problems with a metric action space and,
using a chaining argument, we establish new bounds that depend on the metric
entropy of the action space for a variant of Thompson-Sampling.
  Under suitable continuity assumption of the rewards, our bound offers a tight
rate of $O(d\sqrt{T})$ for $d$-dimensional linear bandit problems.</div><div><a href='http://arxiv.org/abs/2403.03361v1'>2403.03361v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07062v1")'>Fast UCB-type algorithms for stochastic bandits with heavy and super
  heavy symmetric noise</div>
<div id='2402.07062v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T22:38:21Z</div><div>Authors: Yuriy Dorn, Aleksandr Katrutsa, Ilgam Latypov, Andrey Pudovikov</div><div style='padding-top: 10px; width: 80ex'>In this study, we propose a new method for constructing UCB-type algorithms
for stochastic multi-armed bandits based on general convex optimization methods
with an inexact oracle. We derive the regret bounds corresponding to the
convergence rates of the optimization methods. We propose a new algorithm
Clipped-SGD-UCB and show, both theoretically and empirically, that in the case
of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$
regret bound instead of $O\left (T^{\frac{1}{1+\alpha}}
K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution
satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$
($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general
lower bound for bandits with heavy-tails. Moreover, the same bound holds even
when the reward distribution does not have the expectation, that is, when
$\alpha&lt;0$.</div><div><a href='http://arxiv.org/abs/2402.07062v1'>2402.07062v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15171v1")'>Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial
  Semi-Bandits</div>
<div id='2402.15171v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:07:54Z</div><div>Authors: Julien Zhou, Pierre Gaillard, Thibaud Rahier, Houssam Zenati, Julyan Arbel</div><div style='padding-top: 10px; width: 80ex'>We address the problem of stochastic combinatorial semi-bandits, where a
player can select from P subsets of a set containing d base items. Most
existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the
reward distribution, like an upper bound on a sub-Gaussian proxy-variance,
which is hard to estimate tightly. In this work, we design a variance-adaptive
version of OLS-UCB, relying on an online estimation of the covariance
structure. Estimating the coefficients of a covariance matrix is much more
manageable in practical settings and results in improved regret upper bounds
compared to proxy variance-based algorithms. When covariance coefficients are
all non-negative, we show that our approach efficiently leverages the
semi-bandit feedback and provably outperforms bandit feedback approaches, not
only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is
not straightforward from most existing analyses.</div><div><a href='http://arxiv.org/abs/2402.15171v1'>2402.15171v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10379v1")'>Regret Minimization via Saddle Point Optimization</div>
<div id='2403.10379v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:09:13Z</div><div>Authors: Johannes Kirschner, Seyed Alireza Bakhtiari, Kushagra Chandak, Volodymyr Tkachuk, Csaba Szepesvári</div><div style='padding-top: 10px; width: 80ex'>A long line of works characterizes the sample complexity of regret
minimization in sequential decision-making by min-max programs. In the
corresponding saddle-point game, the min-player optimizes the sampling
distribution against an adversarial max-player that chooses confusing models
leading to large regret. The most recent instantiation of this idea is the
decision-estimation coefficient (DEC), which was shown to provide nearly tight
lower and upper bounds on the worst-case expected regret in structured bandits
and reinforcement learning. By re-parametrizing the offset DEC with the
confidence radius and solving the corresponding min-max program, we derive an
anytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly,
the algorithm optimizes the exploration-exploitation trade-off online instead
of via the analysis. Our formulation leads to a practical algorithm for finite
model classes and linear feedback models. We further point out connections to
the information ratio, decoupling coefficient and PAC-DEC, and numerically
evaluate the performance of E2D on simple examples.</div><div><a href='http://arxiv.org/abs/2403.10379v1'>2403.10379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01077v1")'>Constrained Online Two-stage Stochastic Optimization: Algorithm with
  (and without) Predictions</div>
<div id='2401.01077v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T07:46:33Z</div><div>Authors: Piao Hu, Jiashuo Jiang, Guodong Lyu, Hao Su</div><div style='padding-top: 10px; width: 80ex'>We consider an online two-stage stochastic optimization with long-term
constraints over a finite horizon of $T$ periods. At each period, we take the
first-stage action, observe a model parameter realization and then take the
second-stage action from a feasible set that depends both on the first-stage
decision and the model parameter. We aim to minimize the cumulative objective
value while guaranteeing that the long-term average second-stage decision
belongs to a set. We develop online algorithms for the online two-stage problem
from adversarial learning algorithms. Also, the regret bound of our algorithm
can be reduced to the regret bound of embedded adversarial learning algorithms.
Based on this framework, we obtain new results under various settings. When the
model parameters are drawn from unknown non-stationary distributions and we are
given machine-learned predictions of the distributions, we develop a new
algorithm from our framework with a regret $O(W_T+\sqrt{T})$, where $W_T$
measures the total inaccuracy of the machine-learned predictions. We then
develop another algorithm that works when no machine-learned predictions are
given and show the performances.</div><div><a href='http://arxiv.org/abs/2401.01077v1'>2401.01077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03672v2")'>Learning Adversarial MDPs with Stochastic Hard Constraints</div>
<div id='2403.03672v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:49:08Z</div><div>Authors: Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti</div><div style='padding-top: 10px; width: 80ex'>We study online learning problems in constrained Markov decision processes
(CMDPs) with adversarial losses and stochastic hard constraints. We consider
two different scenarios. In the first one, we address general CMDPs, where we
design an algorithm that attains sublinear regret and cumulative positive
constraints violation. In the second scenario, under the mild assumption that a
policy strictly satisfying the constraints exists and is known to the learner,
we design an algorithm that achieves sublinear regret while ensuring that the
constraints are satisfied at every episode with high probability. To the best
of our knowledge, our work is the first to study CMDPs involving both
adversarial losses and hard constraints. Indeed, previous works either focus on
much weaker soft constraints--allowing for positive violation to cancel out
negative ones--or are restricted to stochastic losses. Thus, our algorithms can
deal with general non-stationary environments subject to requirements much
stricter than those manageable with state-of-the-art algorithms. This enables
their adoption in a much wider range of real-world applications, ranging from
autonomous driving to online advertising and recommender systems.</div><div><a href='http://arxiv.org/abs/2403.03672v2'>2403.03672v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00930v1")'>Scale-free Adversarial Reinforcement Learning</div>
<div id='2403.00930v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T19:21:10Z</div><div>Authors: Mingyu Chen, Xuezhou Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper initiates the study of scale-free learning in Markov Decision
Processes (MDPs), where the scale of rewards/losses is unknown to the learner.
We design a generic algorithmic framework, \underline{S}cale
\underline{C}lipping \underline{B}ound (\texttt{SCB}), and instantiate this
framework in both the adversarial Multi-armed Bandit (MAB) setting and the
adversarial MDP setting. Through this framework, we achieve the first minimax
optimal expected regret bound and the first high-probability regret bound in
scale-free adversarial MABs, resolving an open problem raised in
\cite{hadiji2023adaptation}. On adversarial MDPs, our framework also give birth
to the first scale-free RL algorithm with a $\tilde{\mathcal{O}}(\sqrt{T})$
high-probability regret guarantee.</div><div><a href='http://arxiv.org/abs/2403.00930v1'>2403.00930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01111v1")'>Near-Optimal Reinforcement Learning with Self-Play under Adaptivity
  Constraints</div>
<div id='2402.01111v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T03:00:40Z</div><div>Authors: Dan Qiao, Yu-Xiang Wang</div><div style='padding-top: 10px; width: 80ex'>We study the problem of multi-agent reinforcement learning (MARL) with
adaptivity constraints -- a new problem motivated by real-world applications
where deployments of new policies are costly and the number of policy updates
must be minimized. For two-player zero-sum Markov Games, we design a (policy)
elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3
S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above,
$S$ denotes the number of states, $A,B$ are the number of actions for the two
players respectively, $H$ is the horizon and $K$ is the number of episodes.
Furthermore, we prove a batch complexity lower bound
$\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with
$\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to
logarithmic factors. As a byproduct, our techniques naturally extend to
learning bandit games and reward-free MARL within near optimal batch
complexity. To the best of our knowledge, these are the first line of results
towards understanding MARL with low adaptivity.</div><div><a href='http://arxiv.org/abs/2402.01111v1'>2402.01111v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07082v1")'>Refined Sample Complexity for Markov Games with Independent Linear
  Function Approximation</div>
<div id='2402.07082v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T01:51:15Z</div><div>Authors: Yan Dai, Qiwen Cui, Simon S. Du</div><div style='padding-top: 10px; width: 80ex'>Markov Games (MG) is an important model for Multi-Agent Reinforcement
Learning (MARL). It was long believed that the "curse of multi-agents" (i.e.,
the algorithmic performance drops exponentially with the number of agents) is
unavoidable until several recent works (Daskalakis et al., 2023; Cui et al.,
2023; Wang et al., 2023. While these works did resolve the curse of
multi-agents, when the state spaces are prohibitively large and (linear)
function approximations are deployed, they either had a slower convergence rate
of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions
$A_{\max}$ -- which is avoidable in single-agent cases even when the loss
functions can arbitrarily vary with time (Dai et al., 2023). This paper first
refines the `AVLPR` framework by Wang et al. (2023), with an insight of
*data-dependent* (i.e., stochastic) pessimistic estimation of the
sub-optimality gap, allowing a broader choice of plug-in algorithms. When
specialized to MGs with independent linear function approximations, we propose
novel *action-dependent bonuses* to cover occasionally extreme estimation
errors. With the help of state-of-the-art techniques from the single-agent RL
literature, we give the first algorithm that tackles the curse of multi-agents,
attains the optimal $O(T^{-1/2})$ convergence rate, and avoids
$\text{poly}(A_{\max})$ dependency simultaneously.</div><div><a href='http://arxiv.org/abs/2402.07082v1'>2402.07082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09701v2")'>A Natural Extension To Online Algorithms For Hybrid RL With Limited
  Coverage</div>
<div id='2403.09701v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T19:39:47Z</div><div>Authors: Kevin Tan, Ziping Xu</div><div style='padding-top: 10px; width: 80ex'>Hybrid Reinforcement Learning (RL), leveraging both online and offline data,
has garnered recent interest, yet research on its provable benefits remains
sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023;
Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on
the offline dataset, but we show that this is unnecessary. A well-designed
online algorithm should "fill in the gaps" in the offline dataset, exploring
states and actions that the behavior policy did not explore. Unlike previous
approaches that focus on estimating the offline data distribution to guide
online exploration (Li et al., 2023b), we show that a natural extension to
standard optimistic online algorithms -- warm-starting them by including the
offline dataset in the experience replay buffer -- achieves similar provable
gains from hybrid data even when the offline dataset does not have
single-policy concentrability. We accomplish this by partitioning the
state-action space into two, bounding the regret on each partition through an
offline and an online complexity measure, and showing that the regret of this
hybrid RL algorithm can be characterized by the best partition -- despite the
algorithm not knowing the partition itself. As an example, we propose
DISC-GOLF, a modification of an existing optimistic online algorithm with
general function approximation called GOLF used in Jin et al. (2021); Xie et
al. (2022a), and show that it demonstrates provable gains over both online-only
and offline-only reinforcement learning, with competitive bounds when
specialized to the tabular, linear and block MDP cases. Numerical simulations
further validate our theory that hybrid data facilitates more efficient
exploration, supporting the potential of hybrid RL in various scenarios.</div><div><a href='http://arxiv.org/abs/2403.09701v2'>2403.09701v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06571v1")'>Scalable Online Exploration via Coverability</div>
<div id='2403.06571v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:14:06Z</div><div>Authors: Philip Amortila, Dylan J. Foster, Akshay Krishnamurthy</div><div style='padding-top: 10px; width: 80ex'>Exploration is a major challenge in reinforcement learning, especially for
high-dimensional domains that require function approximation. We propose
exploration objectives -- policy optimization objectives that enable downstream
maximization of any reward function -- as a conceptual framework to systematize
the study of exploration. Within this framework, we introduce a new objective,
$L_1$-Coverage, which generalizes previous exploration schemes and supports
three fundamental desiderata:
  1. Intrinsic complexity control. $L_1$-Coverage is associated with a
structural parameter, $L_1$-Coverability, which reflects the intrinsic
statistical difficulty of the underlying MDP, subsuming Block and Low-Rank
MDPs.
  2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently
reduces to standard policy optimization, allowing flexible integration with
off-the-shelf methods such as policy gradient and Q-learning approaches.
  3. Efficient exploration. $L_1$-Coverage enables the first computationally
efficient model-based and model-free algorithms for online (reward-free or
reward-driven) reinforcement learning in MDPs with low coverability.
  Empirically, we find that $L_1$-Coverage effectively drives off-the-shelf
policy optimization algorithms to explore the state space.</div><div><a href='http://arxiv.org/abs/2403.06571v1'>2403.06571v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14156v1")'>Policy Mirror Descent with Lookahead</div>
<div id='2403.14156v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T06:10:51Z</div><div>Authors: Kimon Protopapas, Anas Barakat</div><div style='padding-top: 10px; width: 80ex'>Policy Mirror Descent (PMD) stands as a versatile algorithmic framework
encompassing several seminal policy gradient algorithms such as natural policy
gradient, with connections with state-of-the-art reinforcement learning (RL)
algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration
algorithm implementing regularized 1-step greedy policy improvement. However,
1-step greedy policies might not be the best choice and recent remarkable
empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that
greedy approaches with respect to multiple steps outperform their 1-step
counterpart. In this work, we propose a new class of PMD algorithms called
$h$-PMD which incorporates multi-step greedy policy improvement with lookahead
depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov
Decision Processes with discount factor $\gamma$, we show that $h$-PMD which
generalizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linear
convergence rate, contingent on the computation of multi-step greedy policies.
We propose an inexact version of $h$-PMD where lookahead action values are
estimated. Under a generative model, we establish a sample complexity for
$h$-PMD which improves over prior work. Finally, we extend our result to linear
function approximation to scale to large state spaces. Under suitable
assumptions, our sample complexity only involves dependence on the dimension of
the feature map space instead of the state space size.</div><div><a href='http://arxiv.org/abs/2403.14156v1'>2403.14156v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12034v1")'>When Do Off-Policy and On-Policy Policy Gradient Methods Align?</div>
<div id='2402.12034v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T10:42:34Z</div><div>Authors: Davide Mambelli, Stephan Bongers, Onno Zoeter, Matthijs T. J. Spaan, Frans A. Oliehoek</div><div style='padding-top: 10px; width: 80ex'>Policy gradient methods are widely adopted reinforcement learning algorithms
for tasks with continuous action spaces. These methods succeeded in many
application domains, however, because of their notorious sample inefficiency
their use remains limited to problems where fast and accurate simulations are
available. A common way to improve sample efficiency is to modify their
objective function to be computable from off-policy samples without importance
sampling. A well-established off-policy objective is the excursion objective.
This work studies the difference between the excursion objective and the
traditional on-policy objective, which we refer to as the on-off gap. We
provide the first theoretical analysis showing conditions to reduce the on-off
gap while establishing empirical evidence of shortfalls arising when these
conditions are not met.</div><div><a href='http://arxiv.org/abs/2402.12034v1'>2402.12034v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13662v2")'>The Definitive Guide to Policy Gradients in Deep Reinforcement Learning:
  Theory, Algorithms and Implementations</div>
<div id='2401.13662v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T18:56:53Z</div><div>Authors: Matthias Lehmann</div><div style='padding-top: 10px; width: 80ex'>In recent years, various powerful policy gradient algorithms have been
proposed in deep reinforcement learning. While all these algorithms build on
the Policy Gradient Theorem, the specific design choices differ significantly
across algorithms. We provide a holistic overview of on-policy policy gradient
algorithms to facilitate the understanding of both their theoretical
foundations and their practical implementations. In this overview, we include a
detailed proof of the continuous version of the Policy Gradient Theorem,
convergence results and a comprehensive discussion of practical algorithms. We
compare the most prominent algorithms on continuous control environments and
provide insights on the benefits of regularization. All code is available at
https://github.com/Matt00n/PolicyGradientsJax.</div><div><a href='http://arxiv.org/abs/2401.13662v2'>2401.13662v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06604v3")'>Identifying Policy Gradient Subspaces</div>
<div id='2401.06604v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T14:40:55Z</div><div>Authors: Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel Häufle, Bernhard Schölkopf, Dieter Büchler</div><div style='padding-top: 10px; width: 80ex'>Policy gradient methods hold great potential for solving complex continuous
control tasks. Still, their training efficiency can be improved by exploiting
structure within the optimization problem. Recent work indicates that
supervised learning can be accelerated by leveraging the fact that gradients
lie in a low-dimensional and slowly-changing subspace. In this paper, we
conduct a thorough evaluation of this phenomenon for two popular deep policy
gradient methods on various simulated benchmark tasks. Our results demonstrate
the existence of such gradient subspaces despite the continuously changing data
distribution inherent to reinforcement learning. These findings reveal
promising directions for future work on more efficient reinforcement learning,
e.g., through improving parameter-space exploration or enabling second-order
optimization.</div><div><a href='http://arxiv.org/abs/2401.06604v3'>2401.06604v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00162v1")'>Behind the Myth of Exploration in Policy Gradients</div>
<div id='2402.00162v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T20:37:09Z</div><div>Authors: Adrien Bolland, Gaspard Lambrechts, Damien Ernst</div><div style='padding-top: 10px; width: 80ex'>Policy-gradient algorithms are effective reinforcement learning methods for
solving control problems with continuous state and action spaces. To compute
near-optimal policies, it is essential in practice to include exploration terms
in the learning objective. Although the effectiveness of these terms is usually
justified by an intrinsic need to explore environments, we propose a novel
analysis and distinguish two different implications of these techniques. First,
they make it possible to smooth the learning objective and to eliminate local
optima while preserving the global maximum. Second, they modify the gradient
estimates, increasing the probability that the stochastic parameter update
eventually provides an optimal policy. In light of these effects, we discuss
and illustrate empirically exploration strategies based on entropy bonuses,
highlighting their limitations and opening avenues for future works in the
design and analysis of such strategies.</div><div><a href='http://arxiv.org/abs/2402.00162v1'>2402.00162v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07875v1")'>Implicit Bias of Policy Gradient in Linear Quadratic Control:
  Extrapolation to Unseen Initial States</div>
<div id='2402.07875v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:41:31Z</div><div>Authors: Noam Razin, Yotam Alexander, Edo Cohen-Karlik, Raja Giryes, Amir Globerson, Nadav Cohen</div><div style='padding-top: 10px; width: 80ex'>In modern machine learning, models can often fit training data in numerous
ways, some of which perform well on unseen (test) data, while others do not.
Remarkably, in such cases gradient descent frequently exhibits an implicit bias
that leads to excellent performance on unseen data. This implicit bias was
extensively studied in supervised learning, but is far less understood in
optimal control (reinforcement learning). There, learning a controller applied
to a system via gradient descent is known as policy gradient, and a question of
prime importance is the extent to which a learned controller extrapolates to
unseen initial states. This paper theoretically studies the implicit bias of
policy gradient in terms of extrapolation to unseen initial states. Focusing on
the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the
extent of extrapolation depends on the degree of exploration induced by the
system when commencing from initial states included in training. Experiments
corroborate our theory, and demonstrate its conclusions on problems beyond LQR,
where systems are non-linear and controllers are neural networks. We
hypothesize that real-world optimal control may be greatly improved by
developing methods for informed selection of initial states to train on.</div><div><a href='http://arxiv.org/abs/2402.07875v1'>2402.07875v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12546v1")'>On Building Myopic MPC Policies using Supervised Learning</div>
<div id='2401.12546v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T08:08:09Z</div><div>Authors: Christopher A. Orrico, Bokan Yang, Dinesh Krishnamoorthy</div><div style='padding-top: 10px; width: 80ex'>The application of supervised learning techniques in combination with model
predictive control (MPC) has recently generated significant interest,
particularly in the area of approximate explicit MPC, where function
approximators like deep neural networks are used to learn the MPC policy via
optimal state-action pairs generated offline. While the aim of approximate
explicit MPC is to closely replicate the MPC policy, substituting online
optimization with a trained neural network, the performance guarantees that
come with solving the online optimization problem are typically lost. This
paper considers an alternative strategy, where supervised learning is used to
learn the optimal value function offline instead of learning the optimal
policy. This can then be used as the cost-to-go function in a myopic MPC with a
very short prediction horizon, such that the online computation burden reduces
significantly without affecting the controller performance. This approach
differs from existing work on value function approximations in the sense that
it learns the cost-to-go function by using offline-collected state-value pairs,
rather than closed-loop performance data. The cost of generating the
state-value pairs used for training is addressed using a sensitivity-based data
augmentation scheme.</div><div><a href='http://arxiv.org/abs/2401.12546v1'>2401.12546v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16899v3")'>A priori Estimates for Deep Residual Network in Continuous-time
  Reinforcement Learning</div>
<div id='2402.16899v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T06:31:43Z</div><div>Authors: Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo</div><div style='padding-top: 10px; width: 80ex'>Deep reinforcement learning excels in numerous large-scale practical
applications. However, existing performance analyses ignores the unique
characteristics of continuous-time control problems, is unable to directly
estimate the generalization error of the Bellman optimal loss and require a
boundedness assumption. Our work focuses on continuous-time control problems
and proposes a method that is applicable to all such problems where the
transition function satisfies semi-group and Lipschitz properties. Under this
method, we can directly analyze the \emph{a priori} generalization error of the
Bellman optimal loss. The core of this method lies in two transformations of
the loss function. To complete the transformation, we propose a decomposition
method for the maximum operator. Additionally, this analysis method does not
require a boundedness assumption. Finally, we obtain an \emph{a priori}
generalization error without the curse of dimensionality.</div><div><a href='http://arxiv.org/abs/2402.16899v3'>2402.16899v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00761v1")'>Control-Theoretic Techniques for Online Adaptation of Deep Neural
  Networks in Dynamical Systems</div>
<div id='2402.00761v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:51:11Z</div><div>Authors: Jacob G. Elkins, Farbod Fahimi</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks (DNNs), trained with gradient-based optimization and
backpropagation, are currently the primary tool in modern artificial
intelligence, machine learning, and data science. In many applications, DNNs
are trained offline, through supervised learning or reinforcement learning, and
deployed online for inference. However, training DNNs with standard
backpropagation and gradient-based optimization gives no intrinsic performance
guarantees or bounds on the DNN, which is essential for applications such as
controls. Additionally, many offline-training and online-inference problems,
such as sim2real transfer of reinforcement learning policies, experience domain
shift from the training distribution to the real-world distribution. To address
these stability and transfer learning issues, we propose using techniques from
control theory to update DNN parameters online. We formulate the
fully-connected feedforward DNN as a continuous-time dynamical system, and we
propose novel last-layer update laws that guarantee desirable error convergence
under various conditions on the time derivative of the DNN input vector. We
further show that training the DNN under spectral normalization controls the
upper bound of the error trajectories of the online DNN predictions, which is
desirable when numerically differentiated quantities or noisy state
measurements are input to the DNN. The proposed online DNN adaptation laws are
validated in simulation to learn the dynamics of the Van der Pol system under
domain shift, where parameters are varied in inference from the training
dataset. The simulations demonstrate the effectiveness of using
control-theoretic techniques to derive performance improvements and guarantees
in DNN-based learning systems.</div><div><a href='http://arxiv.org/abs/2402.00761v1'>2402.00761v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15592v1")'>Neural optimal controller for stochastic systems via pathwise HJB
  operator</div>
<div id='2402.15592v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T20:19:06Z</div><div>Authors: Zhe Jiao, Xiaoyan Luo, Xinlei Yi</div><div style='padding-top: 10px; width: 80ex'>The aim of this work is to develop deep learning-based algorithms for
high-dimensional stochastic control problems based on physics-informed learning
and dynamic programming. Unlike classical deep learning-based methods relying
on a probabilistic representation of the solution to the
Hamilton--Jacobi--Bellman (HJB) equation, we introduce a pathwise operator
associated with the HJB equation so that we can define a problem of
physics-informed learning. According to whether the optimal control has an
explicit representation, two numerical methods are proposed to solve the
physics-informed learning problem. We provide an error analysis on how the
truncation, approximation and optimization errors affect the accuracy of these
methods. Numerical results on various applications are presented to illustrate
the performance of the proposed algorithms.</div><div><a href='http://arxiv.org/abs/2402.15592v1'>2402.15592v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15267v1")'>Parametric PDE Control with Deep Reinforcement Learning and
  Differentiable L0-Sparse Polynomial Policies</div>
<div id='2403.15267v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T15:06:31Z</div><div>Authors: Nicolò Botteghi, Urban Fasel</div><div style='padding-top: 10px; width: 80ex'>Optimal control of parametric partial differential equations (PDEs) is
crucial in many applications in engineering and science. In recent years, the
progress in scientific machine learning has opened up new frontiers for the
control of parametric PDEs. In particular, deep reinforcement learning (DRL)
has the potential to solve high-dimensional and complex control problems in a
large variety of applications. Most DRL methods rely on deep neural network
(DNN) control policies. However, for many dynamical systems, DNN-based control
policies tend to be over-parametrized, which means they need large amounts of
training data, show limited robustness, and lack interpretability. In this
work, we leverage dictionary learning and differentiable L$_0$ regularization
to learn sparse, robust, and interpretable control policies for parametric
PDEs. Our sparse policy architecture is agnostic to the DRL method and can be
used in different policy-gradient and actor-critic DRL algorithms without
changing their policy-optimization procedure. We test our approach on the
challenging tasks of controlling parametric Kuramoto-Sivashinsky and
convection-diffusion-reaction PDEs. We show that our method (1) outperforms
baseline DNN-based DRL policies, (2) allows for the derivation of interpretable
equations of the learned optimal control laws, and (3) generalizes to unseen
parameters of the PDE without retraining the policies.</div><div><a href='http://arxiv.org/abs/2403.15267v1'>2403.15267v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09110v1")'>SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning</div>
<div id='2403.09110v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T05:17:39Z</div><div>Authors: Nicholas Zolman, Urban Fasel, J. Nathan Kutz, Steven L. Brunton</div><div style='padding-top: 10px; width: 80ex'>Deep reinforcement learning (DRL) has shown significant promise for
uncovering sophisticated control policies that interact in environments with
complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak
fusion reactor or minimizing the drag force exerted on an object in a fluid
flow. However, these algorithms require an abundance of training examples and
may become prohibitively expensive for many applications. In addition, the
reliance on deep neural networks often results in an uninterpretable, black-box
policy that may be too computationally expensive to use with certain embedded
systems. Recent advances in sparse dictionary learning, such as the sparse
identification of nonlinear dynamics (SINDy), have shown promise for creating
efficient and interpretable data-driven models in the low-data regime. In this
work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to
create efficient, interpretable, and trustworthy representations of the
dynamics model, reward function, and control policy. We demonstrate the
effectiveness of our approaches on benchmark control environments and
challenging fluids problems. SINDy-RL achieves comparable performance to
state-of-the-art DRL algorithms using significantly fewer interactions in the
environment and results in an interpretable control policy orders of magnitude
smaller than a deep neural network policy.</div><div><a href='http://arxiv.org/abs/2403.09110v1'>2403.09110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06313v1")'>Optimal Policy Sparsification and Low Rank Decomposition for Deep
  Reinforcement Learning</div>
<div id='2403.06313v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T21:18:54Z</div><div>Authors: Vikram Goddla</div><div style='padding-top: 10px; width: 80ex'>Deep reinforcement learning(DRL) has shown significant promise in a wide
range of applications including computer games and robotics. Yet, training DRL
policies consume extraordinary computing resources resulting in dense policies
which are prone to overfitting. Moreover, inference with dense DRL policies
limit their practical applications, especially in edge computing. Techniques
such as pruning and singular value decomposition have been used with deep
learning models to achieve sparsification and model compression to limit
overfitting and reduce memory consumption. However, these techniques resulted
in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$
regularization techniques have been proposed for neural network sparsification
and sparse auto-encoder development, but their implementation in DRL
environments has not been apparent. We propose a novel
$L_0$-norm-regularization technique using an optimal sparsity map to sparsify
DRL policies and promote their decomposition to a lower rank without decay in
rewards. We evaluated our $L_0$-norm-regularization technique across five
different environments (Cartpole-v1, Acrobat-v1, LunarLander-v2,
SuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and
off-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL
policy in the SuperMarioBros environment achieved 93% sparsity and gained 70%
compression when subjected to low-rank decomposition, while significantly
outperforming the dense policy. Additionally, the $L_0$-norm-regularized DRL
policy in the Surgical Robot Learning environment achieved a 36% sparsification
and gained 46% compression when decomposed to a lower rank, while being
performant. The results suggest that our custom $L_0$-norm-regularization
technique for sparsification of DRL policies is a promising avenue to reduce
computational resources and limit overfitting.</div><div><a href='http://arxiv.org/abs/2403.06313v1'>2403.06313v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13447v1")'>Symbolic Equation Solving via Reinforcement Learning</div>
<div id='2401.13447v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T13:42:24Z</div><div>Authors: Lennart Dabelow, Masahito Ueda</div><div style='padding-top: 10px; width: 80ex'>Machine-learning methods are gradually being adopted in a great variety of
social, economic, and scientific contexts, yet they are notorious for
struggling with exact mathematics. A typical example is computer algebra, which
includes tasks like simplifying mathematical terms, calculating formal
derivatives, or finding exact solutions of algebraic equations. Traditional
software packages for these purposes are commonly based on a huge database of
rules for how a specific operation (e.g., differentiation) transforms a certain
term (e.g., sine function) into another one (e.g., cosine function). Thus far,
these rules have usually needed to be discovered and subsequently programmed by
humans. Focusing on the paradigmatic example of solving linear equations in
symbolic form, we demonstrate how the process of finding elementary
transformation rules and step-by-step solutions can be automated using
reinforcement learning with deep neural networks.</div><div><a href='http://arxiv.org/abs/2401.13447v1'>2401.13447v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00254v1")'>Vertical Symbolic Regression via Deep Policy Gradient</div>
<div id='2402.00254v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T00:54:48Z</div><div>Authors: Nan Jiang, Md Nasim, Yexiang Xue</div><div style='padding-top: 10px; width: 80ex'>Vertical Symbolic Regression (VSR) recently has been proposed to expedite the
discovery of symbolic equations with many independent variables from
experimental data. VSR reduces the search spaces following the vertical
discovery path by building from reduced-form equations involving a subset of
independent variables to full-fledged ones. Proved successful by many symbolic
regressors, deep neural networks are expected to further scale up VSR.
Nevertheless, directly combining VSR with deep neural networks will result in
difficulty in passing gradients and other engineering issues. We propose
Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and
demonstrate that VSR-DPG can recover ground-truth equations involving multiple
input variables, significantly beyond both deep reinforcement learning-based
approaches and previous VSR variants. Our VSR-DPG models symbolic regression as
a sequential decision-making process, in which equations are built from
repeated applications of grammar rules. The integrated deep model is trained to
maximize a policy gradient objective. Experimental results demonstrate that our
VSR-DPG significantly outperforms popular baselines in identifying both
algebraic equations and ordinary differential equations on a series of
benchmarks.</div><div><a href='http://arxiv.org/abs/2402.00254v1'>2402.00254v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12923v2")'>Deep multitask neural networks for solving some stochastic optimal
  control problems</div>
<div id='2401.12923v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T17:20:48Z</div><div>Authors: Christian Yeo</div><div style='padding-top: 10px; width: 80ex'>Most existing neural network-based approaches for solving stochastic optimal
control problems using the associated backward dynamic programming principle
rely on the ability to simulate the underlying state variables. However, in
some problems, this simulation is infeasible, leading to the discretization of
state variable space and the need to train one neural network for each data
point. This approach becomes computationally inefficient when dealing with
large state variable spaces. In this paper, we consider a class of this type of
stochastic optimal control problems and introduce an effective solution
employing multitask neural networks. To train our multitask neural network, we
introduce a novel scheme that dynamically balances the learning across tasks.
Through numerical experiments on real-world derivatives pricing problems, we
prove that our method outperforms state-of-the-art approaches.</div><div><a href='http://arxiv.org/abs/2401.12923v2'>2401.12923v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00381v1")'>Structured Deep Neural Networks-Based Backstepping Trajectory Tracking
  Control for Lagrangian Systems</div>
<div id='2403.00381v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T09:09:37Z</div><div>Authors: Jiajun Qian, Liang Xu, Xiaoqiang Ren, Xiaofan Wang</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks (DNN) are increasingly being used to learn controllers
due to their excellent approximation capabilities. However, their black-box
nature poses significant challenges to closed-loop stability guarantees and
performance analysis. In this paper, we introduce a structured DNN-based
controller for the trajectory tracking control of Lagrangian systems using
backing techniques. By properly designing neural network structures, the
proposed controller can ensure closed-loop stability for any compatible neural
network parameters. In addition, improved control performance can be achieved
by further optimizing neural network parameters. Besides, we provide explicit
upper bounds on tracking errors in terms of controller parameters, which allows
us to achieve the desired tracking performance by properly selecting the
controller parameters. Furthermore, when system models are unknown, we propose
an improved Lagrangian neural network (LNN) structure to learn the system
dynamics and design the controller. We show that in the presence of model
approximation errors and external disturbances, the closed-loop stability and
tracking control performance can still be guaranteed. The effectiveness of the
proposed approach is demonstrated through simulations.</div><div><a href='http://arxiv.org/abs/2403.00381v1'>2403.00381v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18836v1")'>A Model-Based Approach for Improving Reinforcement Learning Efficiency
  Leveraging Expert Observations</div>
<div id='2402.18836v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T03:53:02Z</div><div>Authors: Erhan Can Ozcan, Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis</div><div style='padding-top: 10px; width: 80ex'>This paper investigates how to incorporate expert observations (without
explicit information on expert actions) into a deep reinforcement learning
setting to improve sample efficiency. First, we formulate an augmented policy
loss combining a maximum entropy reinforcement learning objective with a
behavioral cloning loss that leverages a forward dynamics model. Then, we
propose an algorithm that automatically adjusts the weights of each component
in the augmented loss function. Experiments on a variety of continuous control
tasks demonstrate that the proposed algorithm outperforms various benchmarks by
effectively utilizing available expert observations.</div><div><a href='http://arxiv.org/abs/2402.18836v1'>2402.18836v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07704v1")'>Symmetric Q-learning: Reducing Skewness of Bellman Error in Online
  Reinforcement Learning</div>
<div id='2403.07704v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T14:49:19Z</div><div>Authors: Motoki Omura, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada</div><div style='padding-top: 10px; width: 80ex'>In deep reinforcement learning, estimating the value function to evaluate the
quality of states and actions is essential. The value function is often trained
using the least squares method, which implicitly assumes a Gaussian error
distribution. However, a recent study suggested that the error distribution for
training the value function is often skewed because of the properties of the
Bellman operator, and violates the implicit assumption of normal error
distribution in the least squares method. To address this, we proposed a method
called Symmetric Q-learning, in which the synthetic noise generated from a
zero-mean distribution is added to the target values to generate a Gaussian
error distribution. We evaluated the proposed method on continuous control
benchmark tasks in MuJoCo. It improved the sample efficiency of a
state-of-the-art reinforcement learning method by reducing the skewness of the
error distribution.</div><div><a href='http://arxiv.org/abs/2403.07704v1'>2403.07704v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05251v1")'>ReACT: Reinforcement Learning for Controller Parametrization using
  B-Spline Geometries</div>
<div id='2401.05251v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T16:27:30Z</div><div>Authors: Thomas Rudolf, Daniel Flögel, Tobias Schürmann, Simon Süß, Stefan Schwab, Sören Hohmann</div><div style='padding-top: 10px; width: 80ex'>Robust and performant controllers are essential for industrial applications.
However, deriving controller parameters for complex and nonlinear systems is
challenging and time-consuming. To facilitate automatic controller
parametrization, this work presents a novel approach using deep reinforcement
learning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the
control of parameter-variant systems, a class of systems with complex behavior
which depends on the operating conditions. For this system class,
gain-scheduling control structures are widely used in applications across
industries due to well-known design principles. Facilitating the expensive
controller parametrization task regarding these control structures, we deploy
an DRL agent. Based on control system observations, the agent autonomously
decides how to adapt the controller parameters. We make the adaptation process
more efficient by introducing BSGs to map the controller parameters which may
depend on numerous operating conditions. To preprocess time-series data and
extract a fixed-length feature vector, we use a long short-term memory (LSTM)
neural networks. Furthermore, this work contributes actor regularizations that
are relevant to real-world environments which differ from training.
Accordingly, we apply dropout layer normalization to the actor and critic
networks of the truncated quantile critic (TQC) algorithm. To show our
approach's working principle and effectiveness, we train and evaluate the DRL
agent on the parametrization task of an industrial control structure with
parameter lookup tables.</div><div><a href='http://arxiv.org/abs/2401.05251v1'>2401.05251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13654v1")'>Improving a Proportional Integral Controller with Reinforcement Learning
  on a Throttle Valve Benchmark</div>
<div id='2402.13654v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:40:26Z</div><div>Authors: Paul Daoudi, Bojan Mavkov, Bogdan Robu, Christophe Prieur, Emmanuel Witrant, Merwan Barlier, Ludovic Dos Santos</div><div style='padding-top: 10px; width: 80ex'>This paper presents a learning-based control strategy for non-linear throttle
valves with an asymmetric hysteresis, leading to a near-optimal controller
without requiring any prior knowledge about the environment. We start with a
carefully tuned Proportional Integrator (PI) controller and exploit the recent
advances in Reinforcement Learning (RL) with Guides to improve the closed-loop
behavior by learning from the additional interactions with the valve. We test
the proposed control method in various scenarios on three different valves, all
highlighting the benefits of combining both PI and RL frameworks to improve
control performance in non-linear stochastic systems. In all the experimental
test cases, the resulting agent has a better sample efficiency than traditional
RL agents and outperforms the PI controller.</div><div><a href='http://arxiv.org/abs/2402.13654v1'>2402.13654v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02868v1")'>Fine-tuning Reinforcement Learning Models is Secretly a Forgetting
  Mitigation Problem</div>
<div id='2402.02868v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:30:47Z</div><div>Authors: Maciej Wołczyk, Bartłomiej Cupiał, Mateusz Ostaszewski, Michał Bortkiewicz, Michał Zając, Razvan Pascanu, Łukasz Kuciński, Piotr Miłoś</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning is a widespread technique that allows practitioners to transfer
pre-trained capabilities, as recently showcased by the successful applications
of foundation models. However, fine-tuning reinforcement learning (RL) models
remains a challenge. This work conceptualizes one specific cause of poor
transfer, accentuated in the RL setting by the interplay between actions and
observations: forgetting of pre-trained capabilities. Namely, a model
deteriorates on the state subspace of the downstream task not visited in the
initial phase of fine-tuning, on which the model behaved well due to
pre-training. This way, we lose the anticipated transfer benefits. We identify
conditions when this problem occurs, showing that it is common and, in many
cases, catastrophic. Through a detailed empirical analysis of the challenging
NetHack and Montezuma's Revenge environments, we show that standard knowledge
retention techniques mitigate the problem and thus allow us to take full
advantage of the pre-trained capabilities. In particular, in NetHack, we
achieve a new state-of-the-art for neural models, improving the previous best
score from $5$K to over $10$K points in the Human Monk scenario.</div><div><a href='http://arxiv.org/abs/2402.02868v1'>2402.02868v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08609v1")'>Mixtures of Experts Unlock Parameter Scaling for Deep RL</div>
<div id='2402.08609v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T17:18:56Z</div><div>Authors: Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, Pablo Samuel Castro</div><div style='padding-top: 10px; width: 80ex'>The recent rapid progress in (self) supervised learning models is in large
part predicted by empirical scaling laws: a model's performance scales
proportionally to its size. Analogous scaling laws remain elusive for
reinforcement learning domains, however, where increasing the parameter count
of a model often hurts its final performance. In this paper, we demonstrate
that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs
(Puigcerver et al., 2023), into value-based networks results in more
parameter-scalable models, evidenced by substantial performance increases
across a variety of training regimes and model sizes. This work thus provides
strong empirical evidence towards developing scaling laws for reinforcement
learning.</div><div><a href='http://arxiv.org/abs/2402.08609v1'>2402.08609v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19212v3")'>Deep Reinforcement Learning: A Convex Optimization Approach</div>
<div id='2402.19212v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T14:41:31Z</div><div>Authors: Ather Gattami</div><div style='padding-top: 10px; width: 80ex'>In this paper, we consider reinforcement learning of nonlinear systems with
continuous state and action spaces. We present an episodic learning algorithm,
where we for each episode use convex optimization to find a two-layer neural
network approximation of the optimal $Q$-function. The convex optimization
approach guarantees that the weights calculated at each episode are optimal,
with respect to the given sampled states and actions of the current episode.
For stable nonlinear systems, we show that the algorithm converges and that the
converging parameters of the trained neural network can be made arbitrarily
close to the optimal neural network parameters. In particular, if the
regularization parameter is $\rho$ and the time horizon is $T$, then the
parameters of the trained neural network converge to $w$, where the distance
between $w$ from the optimal parameters $w^\star$ is bounded by
$\mathcal{O}(\rho T^{-1})$. That is, when the number of episodes goes to
infinity, there exists a constant $C$ such that \[\|w-w^\star\| \le
C\cdot\frac{\rho}{T}.\] In particular, our algorithm converges arbitrarily
close to the optimal neural network parameters as the time horizon increases or
as the regularization parameter decreases.</div><div><a href='http://arxiv.org/abs/2402.19212v3'>2402.19212v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10069v1")'>Learning fast changing slow in spiking neural networks</div>
<div id='2402.10069v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T12:03:10Z</div><div>Authors: Cristiano Capone, Paolo Muratore</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) faces substantial challenges when applied to
real-life problems, primarily stemming from the scarcity of available data due
to limited interactions with the environment. This limitation is exacerbated by
the fact that RL often demands a considerable volume of data for effective
learning. The complexity escalates further when implementing RL in recurrent
spiking networks, where inherent noise introduced by spikes adds a layer of
difficulty. Life-long learning machines must inherently resolve the
plasticity-stability paradox. Striking a balance between acquiring new
knowledge and maintaining stability is crucial for artificial agents. In this
context, we take inspiration from machine learning technology and introduce a
biologically plausible implementation of proximal policy optimization, arguing
that it significantly alleviates this challenge. Our approach yields two
notable advancements: first, the ability to assimilate new information without
necessitating alterations to the current policy, and second, the capability to
replay experiences without succumbing to policy divergence. Furthermore, when
contrasted with other experience replay (ER) techniques, our method
demonstrates the added advantage of being computationally efficient in an
online setting. We demonstrate that the proposed methodology enhances the
efficiency of learning, showcasing its potential impact on neuromorphic and
real-world applications.</div><div><a href='http://arxiv.org/abs/2402.10069v1'>2402.10069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07044v1")'>BP(λ): Online Learning via Synthetic Gradients</div>
<div id='2401.07044v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T11:13:06Z</div><div>Authors: Joseph Pemberton, Rui Ponte Costa</div><div style='padding-top: 10px; width: 80ex'>Training recurrent neural networks typically relies on backpropagation
through time (BPTT). BPTT depends on forward and backward passes to be
completed, rendering the network locked to these computations before loss
gradients are available. Recently, Jaderberg et al. proposed synthetic
gradients to alleviate the need for full BPTT. In their implementation
synthetic gradients are learned through a mixture of backpropagated gradients
and bootstrapped synthetic gradients, analogous to the temporal difference (TD)
algorithm in Reinforcement Learning (RL). However, as in TD learning, heavy use
of bootstrapping can result in bias which leads to poor synthetic gradient
estimates. Inspired by the accumulate $\mathrm{TD}(\lambda)$ in RL, we propose
a fully online method for learning synthetic gradients which avoids the use of
BPTT altogether: accumulate $BP(\lambda)$. As in accumulate
$\mathrm{TD}(\lambda)$, we show analytically that accumulate
$\mathrm{BP}(\lambda)$ can control the level of bias by using a mixture of
temporal difference errors and recursively defined eligibility traces. We next
demonstrate empirically that our model outperforms the original implementation
for learning synthetic gradients in a variety of tasks, and is particularly
suited for capturing longer timescales. Finally, building on recent work we
reflect on accumulate $\mathrm{BP}(\lambda)$ as a principle for learning in
biological circuits. In summary, inspired by RL principles we introduce an
algorithm capable of bias-free online learning via synthetic gradients.</div><div><a href='http://arxiv.org/abs/2401.07044v1'>2401.07044v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17760v1")'>Learning to Program Variational Quantum Circuits with Fast Weights</div>
<div id='2402.17760v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:53:18Z</div><div>Authors: Samuel Yen-Chi Chen</div><div style='padding-top: 10px; width: 80ex'>Quantum Machine Learning (QML) has surfaced as a pioneering framework
addressing sequential control tasks and time-series modeling. It has
demonstrated empirical quantum advantages notably within domains such as
Reinforcement Learning (RL) and time-series prediction. A significant
advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically
tailored for memory-intensive tasks encompassing partially observable
environments and non-linear time-series prediction. Nevertheless, QRNN-based
models encounter challenges, notably prolonged training duration stemming from
the necessity to compute quantum gradients using backpropagation-through-time
(BPTT). This predicament exacerbates when executing the complete model on
quantum devices, primarily due to the substantial demand for circuit evaluation
arising from the parameter-shift rule. This paper introduces the Quantum Fast
Weight Programmers (QFWP) as a solution to the temporal or sequential learning
challenge. The QFWP leverages a classical neural network (referred to as the
'slow programmer') functioning as a quantum programmer to swiftly modify the
parameters of a variational quantum circuit (termed the 'fast programmer').
Instead of completely overwriting the fast programmer at each time-step, the
slow programmer generates parameter changes or updates for the quantum circuit
parameters. This approach enables the fast programmer to incorporate past
observations or information. Notably, the proposed QFWP model achieves learning
of temporal dependencies without necessitating the use of quantum recurrent
neural networks. Numerical simulations conducted in this study showcase the
efficacy of the proposed QFWP model in both time-series prediction and RL
tasks. The model exhibits performance levels either comparable to or surpassing
those achieved by QLSTM-based models.</div><div><a href='http://arxiv.org/abs/2402.17760v1'>2402.17760v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01084v2")'>Global Convergence of Natural Policy Gradient with Hessian-aided
  Momentum Variance Reduction</div>
<div id='2401.01084v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T07:56:17Z</div><div>Authors: Jie Feng, Ke Wei, Jinchi Chen</div><div style='padding-top: 10px; width: 80ex'>Natural policy gradient (NPG) and its variants are widely-used policy search
methods in reinforcement learning. Inspired by prior work, a new NPG variant
coined NPG-HM is developed in this paper, which utilizes the Hessian-aided
momentum technique for variance reduction, while the sub-problem is solved via
the stochastic gradient descent method. It is shown that NPG-HM can achieve the
global last iterate $\epsilon$-optimality with a sample complexity of
$\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policy
gradient type methods under the generic Fisher non-degenerate policy
parameterizations. The convergence analysis is built upon a relaxed weak
gradient dominance property tailored for NPG under the compatible function
approximation framework, as well as a neat way to decompose the error when
handling the sub-problem. Moreover, numerical experiments on Mujoco-based
environments demonstrate the superior performance of NPG-HM over other
state-of-the-art policy gradient methods.</div><div><a href='http://arxiv.org/abs/2401.01084v2'>2401.01084v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05899v1")'>Optimistic Model Rollouts for Pessimistic Offline Policy Optimization</div>
<div id='2401.05899v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T13:19:45Z</div><div>Authors: Yuanzhao Zhai, Yiying Li, Zijian Gao, Xudong Gong, Kele Xu, Dawei Feng, Ding Bo, Huaimin Wang</div><div style='padding-top: 10px; width: 80ex'>Model-based offline reinforcement learning (RL) has made remarkable progress,
offering a promising avenue for improving generalization with synthetic model
rollouts. Existing works primarily focus on incorporating pessimism for policy
optimization, usually via constructing a Pessimistic Markov Decision Process
(P-MDP). However, the P-MDP discourages the policies from learning in
out-of-distribution (OOD) regions beyond the support of offline datasets, which
can under-utilize the generalization ability of dynamics models. In contrast,
we propose constructing an Optimistic MDP (O-MDP). We initially observed the
potential benefits of optimism brought by encouraging more OOD rollouts.
Motivated by this observation, we present ORPO, a simple yet effective
model-based offline RL framework. ORPO generates Optimistic model Rollouts for
Pessimistic offline policy Optimization. Specifically, we train an optimistic
rollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel
the sampled state-action pairs with penalized rewards and optimize the output
policy in the P-MDP. Theoretically, we demonstrate that the performance of
policies trained with ORPO can be lower-bounded in linear MDPs. Experimental
results show that our framework significantly outperforms P-MDP baselines by a
margin of 30%, achieving state-of-the-art performance on the widely-used
benchmark. Moreover, ORPO exhibits notable advantages in problems that require
generalization.</div><div><a href='http://arxiv.org/abs/2401.05899v1'>2401.05899v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07262v1")'>Advantage-Aware Policy Optimization for Offline Reinforcement Learning</div>
<div id='2403.07262v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T02:43:41Z</div><div>Authors: Yunpeng Qing, Shunyu liu, Jingyuan Cong, Kaixuan Chen, Yihe Zhou, Mingli Song</div><div style='padding-top: 10px; width: 80ex'>Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to
craft effective agent policy without online interaction, which imposes proper
conservative constraints with the support of behavior policies to tackle the
Out-Of-Distribution (OOD) problem. However, existing works often suffer from
the constraint conflict issue when offline datasets are collected from multiple
behavior policies, i.e., different behavior policies may exhibit inconsistent
actions with distinct returns across the state space. To remedy this issue,
recent Advantage-Weighted (AW) methods prioritize samples with high advantage
values for agent training while inevitably leading to overfitting on these
samples. In this paper, we introduce a novel Advantage-Aware Policy
Optimization (A2PO) method to explicitly construct advantage-aware policy
constraints for offline learning under mixed-quality datasets. Specifically,
A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the
action distributions of intertwined behavior policies by modeling the advantage
values of all training data as conditional variables. Then the agent can follow
such disentangled action distribution constraints to optimize the
advantage-aware policy towards high advantage values. Extensive experiments
conducted on both the single-quality and mixed-quality datasets of the D4RL
benchmark demonstrate that A2PO yields results superior to state-of-the-art
counterparts. Our code will be made publicly available.</div><div><a href='http://arxiv.org/abs/2403.07262v1'>2403.07262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07261v1")'>Disentangling Policy from Offline Task Representation Learning via
  Adversarial Data Augmentation</div>
<div id='2403.07261v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T02:38:36Z</div><div>Authors: Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu, Lei Yuan, Zongzhang Zhang, Yang Yu</div><div style='padding-top: 10px; width: 80ex'>Offline meta-reinforcement learning (OMRL) proficiently allows an agent to
tackle novel tasks while solely relying on a static dataset. For precise and
efficient task identification, existing OMRL research suggests learning
separate task representations that be incorporated with policy input, thus
forming a context-based meta-policy. A major approach to train task
representations is to adopt contrastive learning using multi-task offline data.
The dataset typically encompasses interactions from various policies (i.e., the
behavior policies), thus providing a plethora of contextual information
regarding different tasks. Nonetheless, amassing data from a substantial number
of policies is not only impractical but also often unattainable in realistic
settings. Instead, we resort to a more constrained yet practical scenario,
where multi-task data collection occurs with a limited number of policies. We
observed that learned task representations from previous OMRL methods tend to
correlate spuriously with the behavior policy instead of reflecting the
essential characteristics of the task, resulting in unfavorable
out-of-distribution generalization. To alleviate this issue, we introduce a
novel algorithm to disentangle the impact of behavior policy from task
representation learning through a process called adversarial data augmentation.
Specifically, the objective of adversarial data augmentation is not merely to
generate data analogous to offline data distribution; instead, it aims to
create adversarial examples designed to confound learned task representations
and lead to incorrect task identification. Our experiments show that learning
from such adversarial samples significantly enhances the robustness and
effectiveness of the task identification process and realizes satisfactory
out-of-distribution generalization.</div><div><a href='http://arxiv.org/abs/2403.07261v1'>2403.07261v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12570v1")'>Offline Multi-task Transfer RL with Representational Penalization</div>
<div id='2402.12570v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T21:52:44Z</div><div>Authors: Avinandan Bose, Simon Shaolei Du, Maryam Fazel</div><div style='padding-top: 10px; width: 80ex'>We study the problem of representation transfer in offline Reinforcement
Learning (RL), where a learner has access to episodic data from a number of
source tasks collected a priori, and aims to learn a shared representation to
be used in finding a good policy for a target task. Unlike in online RL where
the agent interacts with the environment while learning a policy, in the
offline setting there cannot be such interactions in either the source tasks or
the target task; thus multi-task offline RL can suffer from incomplete
coverage.
  We propose an algorithm to compute pointwise uncertainty measures for the
learnt representation, and establish a data-dependent upper bound for the
suboptimality of the learnt policy for the target task. Our algorithm leverages
the collective exploration done by source tasks to mitigate poor coverage at
some points by a few tasks, thus overcoming the limitation of needing uniformly
good coverage for a meaningful transfer by existing offline algorithms. We
complement our theoretical results with empirical evaluation on a
rich-observation MDP which requires many samples for complete coverage. Our
findings illustrate the benefits of penalizing and quantifying the uncertainty
in the learnt representation.</div><div><a href='http://arxiv.org/abs/2402.12570v1'>2402.12570v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11574v1")'>Offline Multitask Representation Learning for Reinforcement Learning</div>
<div id='2403.11574v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T08:50:30Z</div><div>Authors: Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, Doina Precup</div><div style='padding-top: 10px; width: 80ex'>We study offline multitask representation learning in reinforcement learning
(RL), where a learner is provided with an offline dataset from different tasks
that share a common representation and is asked to learn the shared
representation. We theoretically investigate offline multitask low-rank RL, and
propose a new algorithm called MORL for offline multitask representation
learning. Furthermore, we examine downstream RL in reward-free, offline and
online scenarios, where a new task is introduced to the agent that shares the
same representation as the upstream offline tasks. Our theoretical results
demonstrate the benefits of using the learned representation from the upstream
offline task instead of directly learning the representation of the low-rank
model.</div><div><a href='http://arxiv.org/abs/2403.11574v1'>2403.11574v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08478v1")'>Solving Continual Offline Reinforcement Learning with Decision
  Transformer</div>
<div id='2401.08478v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T16:28:32Z</div><div>Authors: Kaixin Huang, Li Shen, Chen Zhao, Chun Yuan, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Continuous offline reinforcement learning (CORL) combines continuous and
offline reinforcement learning, enabling agents to learn multiple tasks from
static datasets without forgetting prior tasks. However, CORL faces challenges
in balancing stability and plasticity. Existing methods, employing Actor-Critic
structures and experience replay (ER), suffer from distribution shifts, low
efficiency, and weak knowledge-sharing. We aim to investigate whether Decision
Transformer (DT), another offline RL paradigm, can serve as a more suitable
offline continuous learner to address these issues. We first compare AC-based
offline algorithms with DT in the CORL framework. DT offers advantages in
learning efficiency, distribution shift mitigation, and zero-shot
generalization but exacerbates the forgetting problem during supervised
parameter updates. We introduce multi-head DT (MH-DT) and low-rank adaptation
DT (LoRA-DT) to mitigate DT's forgetting problem. MH-DT stores task-specific
knowledge using multiple heads, facilitating knowledge sharing with common
components. It employs distillation and selective rehearsal to enhance current
task learning when a replay buffer is available. In buffer-unavailable
scenarios, LoRA-DT merges less influential weights and fine-tunes DT's decisive
MLP layer to adapt to the current task. Extensive experiments on MoJuCo and
Meta-World benchmarks demonstrate that our methods outperform SOTA CORL
baselines and showcase enhanced learning capabilities and superior memory
efficiency.</div><div><a href='http://arxiv.org/abs/2401.08478v1'>2401.08478v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05066v1")'>Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual
  Reinforcement Learning</div>
<div id='2403.05066v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T05:37:59Z</div><div>Authors: Hongjoon Ahn, Jinu Hyeon, Youngmin Oh, Bosun Hwang, Taesup Moon</div><div style='padding-top: 10px; width: 80ex'>We argue that one of the main obstacles for developing effective Continual
Reinforcement Learning (CRL) algorithms is the negative transfer issue
occurring when the new task to learn arrives. Through comprehensive
experimental validation, we demonstrate that such issue frequently exists in
CRL and cannot be effectively addressed by several recent work on mitigating
plasticity loss of RL agents. To that end, we develop Reset &amp; Distill (R&amp;D), a
simple yet highly effective method, to overcome the negative transfer problem
in CRL. R&amp;D combines a strategy of resetting the agent's online actor and
critic networks to learn a new task and an offline learning step for distilling
the knowledge from the online actor and previous expert's action probabilities.
We carried out extensive experiments on long sequence of Meta-World tasks and
show that our method consistently outperforms recent baselines, achieving
significantly higher success rates across a range of tasks. Our findings
highlight the importance of considering negative transfer in CRL and emphasize
the need for robust strategies like R&amp;D to mitigate its detrimental effects.</div><div><a href='http://arxiv.org/abs/2403.05066v1'>2403.05066v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03055v1")'>Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes
  Uncertainty</div>
<div id='2402.03055v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:42:45Z</div><div>Authors: Bahareh Tasdighi, Nicklas Werge, Yi-Shan Wu, Melih Kandemir</div><div style='padding-top: 10px; width: 80ex'>We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning
algorithm with improved continuous control performance thanks to its ability to
mitigate the exploration-exploitation trade-off. PAC achieves this by
seamlessly integrating stochastic policies and critics, creating a dynamic
synergy between the estimation of critic uncertainty and actor training. The
key contribution of our PAC algorithm is that it explicitly models and infers
epistemic uncertainty in the critic through Probably Approximately
Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty
enables PAC to adapt its exploration strategy as it learns, guiding the actor's
decision-making process. PAC compares favorably against fixed or pre-scheduled
exploration schemes of the prior art. The synergy between stochastic policies
and critics, guided by PAC-Bayes analysis, represents a fundamental step
towards a more adaptive and effective exploration strategy in deep
reinforcement learning. We report empirical evaluations demonstrating PAC's
enhanced stability and improved performance over the state of the art in
diverse continuous control problems.</div><div><a href='http://arxiv.org/abs/2402.03055v1'>2402.03055v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09078v1")'>Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic
  Methods</div>
<div id='2402.09078v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:44:03Z</div><div>Authors: Alberto Sinigaglia, Niccolò Turcato, Alberto Dalla Libera, Ruggero Carli, Gian Antonio Susto</div><div style='padding-top: 10px; width: 80ex'>This paper introduces innovative methods in Reinforcement Learning (RL),
focusing on addressing and exploiting estimation biases in Actor-Critic methods
for continuous control tasks, using Deep Double Q-Learning. We propose two
novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3)
and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3).
ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a
balance between computational efficiency and performance, while BE-TD3 is
designed to dynamically select the most advantageous estimation bias during
training. Our extensive experiments across various continuous control tasks
demonstrate the effectiveness of our approaches. We show that these algorithms
can either match or surpass existing methods like TD3, particularly in
environments where estimation biases significantly impact learning. The results
underline the importance of bias exploitation in improving policy learning in
RL.</div><div><a href='http://arxiv.org/abs/2402.09078v1'>2402.09078v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14528v1")'>ACE : Off-Policy Actor-Critic with Causality-Aware Entropy
  Regularization</div>
<div id='2402.14528v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T13:22:06Z</div><div>Authors: Tianying Ji, Yongyuan Liang, Yan Zeng, Yu Luo, Guowei Xu, Jiawei Guo, Ruijie Zheng, Furong Huang, Fuchun Sun, Huazhe Xu</div><div style='padding-top: 10px; width: 80ex'>The varying significance of distinct primitive behaviors during the policy
learning process has been overlooked by prior model-free RL algorithms.
Leveraging this insight, we explore the causal relationship between different
action dimensions and rewards to evaluate the significance of various primitive
behaviors during training. We introduce a causality-aware entropy term that
effectively identifies and prioritizes actions with high potential impacts for
efficient exploration. Furthermore, to prevent excessive focus on specific
primitive behaviors, we analyze the gradient dormancy phenomenon and introduce
a dormancy-guided reset mechanism to further enhance the efficacy of our
method. Our proposed algorithm, ACE: Off-policy Actor-critic with
Causality-aware Entropy regularization, demonstrates a substantial performance
advantage across 29 diverse continuous control tasks spanning 7 domains
compared to model-free RL baselines, which underscores the effectiveness,
versatility, and efficient sample efficiency of our approach. Benchmark results
and videos are available at https://ace-rl.github.io/.</div><div><a href='http://arxiv.org/abs/2402.14528v1'>2402.14528v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00629v1")'>Adversarially Trained Actor Critic for offline CMDPs</div>
<div id='2401.00629v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T01:44:58Z</div><div>Authors: Honghao Wei, Xiyue Peng, Xin Liu, Arnob Ghosh</div><div style='padding-top: 10px; width: 80ex'>We propose a Safe Adversarial Trained Actor Critic (SATAC) algorithm for
offline reinforcement learning (RL) with general function approximation in the
presence of limited data coverage. SATAC operates as a two-player Stackelberg
game featuring a refined objective function. The actor (leader player)
optimizes the policy against two adversarially trained value critics (follower
players), who focus on scenarios where the actor's performance is inferior to
the behavior policy. Our framework provides both theoretical guarantees and a
robust deep-RL implementation. Theoretically, we demonstrate that when the
actor employs a no-regret optimization oracle, SATAC achieves two guarantees:
(i) For the first time in the offline RL setting, we establish that SATAC can
produce a policy that outperforms the behavior policy while maintaining the
same level of safety, which is critical to designing an algorithm for offline
RL. (ii) We demonstrate that the algorithm guarantees policy improvement across
a broad range of hyperparameters, indicating its practical robustness.
Additionally, we offer a practical version of SATAC and compare it with
existing state-of-the-art offline safe-RL algorithms in continuous control
environments. SATAC outperforms all baselines across a range of tasks, thus
validating the theoretical performance.</div><div><a href='http://arxiv.org/abs/2401.00629v1'>2401.00629v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09695v1")'>Reward Poisoning Attack Against Offline Reinforcement Learning</div>
<div id='2402.09695v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T04:08:49Z</div><div>Authors: Yinglun Xu, Rohan Gumaste, Gagandeep Singh</div><div style='padding-top: 10px; width: 80ex'>We study the problem of reward poisoning attacks against general offline
reinforcement learning with deep neural networks for function approximation. We
consider a black-box threat model where the attacker is completely oblivious to
the learning algorithm and its budget is limited by constraining both the
amount of corruption at each data point, and the total perturbation. We propose
an attack strategy called `policy contrast attack'. The high-level idea is to
make some low-performing policies appear as high-performing while making
high-performing policies appear as low-performing. To the best of our
knowledge, we propose the first black-box reward poisoning attack in the
general offline RL setting. We provide theoretical insights on the attack
design and empirically show that our attack is efficient against current
state-of-the-art offline RL algorithms in different kinds of learning datasets.</div><div><a href='http://arxiv.org/abs/2402.09695v1'>2402.09695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05963v1")'>Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement
  Learning Using Unique Experiences</div>
<div id='2402.05963v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:04:00Z</div><div>Authors: Nikhil Kumar Singh, Indranil Saha</div><div style='padding-top: 10px; width: 80ex'>Efficient utilization of the replay buffer plays a significant role in the
off-policy actor-critic reinforcement learning (RL) algorithms used for
model-free control policy synthesis for complex dynamical systems. We propose a
method for achieving sample efficiency, which focuses on selecting unique
samples and adding them to the replay buffer during the exploration with the
goal of reducing the buffer size and maintaining the independent and
identically distributed (IID) nature of the samples. Our method is based on
selecting an important subset of the set of state variables from the
experiences encountered during the initial phase of random exploration,
partitioning the state space into a set of abstract states based on the
selected important state variables, and finally selecting the experiences with
unique state-reward combination by using a kernel density estimator. We
formally prove that the off-policy actor-critic algorithm incorporating the
proposed method for unique experience accumulation converges faster than the
vanilla off-policy actor-critic algorithm. Furthermore, we evaluate our method
by comparing it with two state-of-the-art actor-critic RL algorithms on several
continuous control benchmarks available in the Gym environment. Experimental
results demonstrate that our method achieves a significant reduction in the
size of the replay buffer for all the benchmarks while achieving either faster
convergent or better reward accumulation compared to the baseline algorithms.</div><div><a href='http://arxiv.org/abs/2402.05963v1'>2402.05963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05951v2")'>MinMaxMin $Q$-learning</div>
<div id='2402.05951v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T21:58:06Z</div><div>Authors: Nitsan Soffair, Shie Mannor</div><div style='padding-top: 10px; width: 80ex'>MinMaxMin $Q$-learning is a novel optimistic Actor-Critic algorithm that
addresses the problem of overestimation bias ($Q$-estimations are
overestimating the real $Q$-values) inherent in conservative RL algorithms. Its
core formula relies on the disagreement among $Q$-networks in the form of the
min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and
used as the priority experience replay sampling-rule. We implement MinMaxMin on
top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art
continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet
environments. The results show a consistent performance improvement of
MinMaxMin over DDPG, TD3, and TD7 across all tested tasks.</div><div><a href='http://arxiv.org/abs/2402.05951v2'>2402.05951v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05950v2")'>SQT -- std $Q$-target</div>
<div id='2402.05950v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T21:36:22Z</div><div>Authors: Nitsan Soffair, Dotan Di-Castro, Orly Avner, Shie Mannor</div><div style='padding-top: 10px; width: 80ex'>Std $Q$-target is a conservative, actor-critic, ensemble, $Q$-learning-based
algorithm, which is based on a single key $Q$-formula: $Q$-networks standard
deviation, which is an "uncertainty penalty", and, serves as a minimalistic
solution to the problem of overestimation bias. We implement SQT on top of
TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic
algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our
results demonstrate SQT's $Q$-target formula superiority over TD3's $Q$-target
formula as a conservative solution to overestimation bias in RL, while SQT
shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on
all tasks.</div><div><a href='http://arxiv.org/abs/2402.05950v2'>2402.05950v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00514v1")'>Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter
  Lesson of Reinforcement Learning</div>
<div id='2403.00514v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:25:10Z</div><div>Authors: Michal Nauman, Michał Bortkiewicz, Mateusz Ostaszewski, Piotr Miłoś, Tomasz Trzciński, Marek Cygan</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in off-policy Reinforcement Learning (RL) have
significantly improved sample efficiency, primarily due to the incorporation of
various forms of regularization that enable more gradient update steps than
traditional agents. However, many of these techniques have been tested in
limited settings, often on tasks from single simulation benchmarks and against
well-known algorithms rather than a range of regularization approaches. This
limits our understanding of the specific mechanisms driving RL improvements. To
address this, we implemented over 60 different off-policy agents, each
integrating established regularization techniques from recent state-of-the-art
algorithms. We tested these agents across 14 diverse tasks from 2 simulation
benchmarks. Our findings reveal that while the effectiveness of a specific
regularization setup varies with the task, certain combinations consistently
demonstrate robust and superior performance. Notably, a simple Soft
Actor-Critic agent, appropriately regularized, reliably solves dog tasks, which
were previously solved mainly through model-based approaches.</div><div><a href='http://arxiv.org/abs/2403.00514v1'>2403.00514v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05996v1")'>Dissecting Deep RL with High Update Ratios: Combatting Value
  Overestimation and Divergence</div>
<div id='2403.05996v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T19:56:40Z</div><div>Authors: Marcel Hussing, Claas Voelcker, Igor Gilitschenski, Amir-massoud Farahmand, Eric Eaton</div><div style='padding-top: 10px; width: 80ex'>We show that deep reinforcement learning can maintain its ability to learn
without resetting network parameters in settings where the number of gradient
updates greatly exceeds the number of environment samples. Under such large
update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the
emergence of a primacy bias, in which agents overfit early interactions and
downplay later experience, impairing their ability to learn. In this work, we
dissect the phenomena underlying the primacy bias. We inspect the early stages
of training that ought to cause the failure to learn and find that a
fundamental challenge is a long-standing acquaintance: value overestimation.
Overinflated Q-values are found not only on out-of-distribution but also
in-distribution data and can be traced to unseen action prediction propelled by
optimizer momentum. We employ a simple unit-ball normalization that enables
learning under large update ratios, show its efficacy on the widely used
dm_control suite, and obtain strong performance on the challenging dog tasks,
competitive with model-based approaches. Our results question, in parts, the
prior explanation for sub-optimal learning due to overfitting on early data.</div><div><a href='http://arxiv.org/abs/2403.05996v1'>2403.05996v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12479v1")'>In deep reinforcement learning, a pruned network is a good network</div>
<div id='2402.12479v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:34:07Z</div><div>Authors: Johan Obando-Ceron, Aaron Courville, Pablo Samuel Castro</div><div style='padding-top: 10px; width: 80ex'>Recent work has shown that deep reinforcement learning agents have difficulty
in effectively using their network parameters. We leverage prior insights into
the advantages of sparse training techniques and demonstrate that gradual
magnitude pruning enables agents to maximize parameter effectiveness. This
results in networks that yield dramatic performance improvements over
traditional networks and exhibit a type of "scaling law", using only a small
fraction of the full network parameters.</div><div><a href='http://arxiv.org/abs/2402.12479v1'>2402.12479v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18762v1")'>Disentangling the Causes of Plasticity Loss in Neural Networks</div>
<div id='2402.18762v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T00:02:33Z</div><div>Authors: Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan Pascanu, James Martens, Will Dabney</div><div style='padding-top: 10px; width: 80ex'>Underpinning the past decades of work on the design, initialization, and
optimization of neural networks is a seemingly innocuous assumption: that the
network is trained on a \textit{stationary} data distribution. In settings
where this assumption is violated, e.g.\ deep reinforcement learning, learning
algorithms become unstable and brittle with respect to hyperparameters and even
random seeds. One factor driving this instability is the loss of plasticity,
meaning that updating the network's predictions in response to new information
becomes more difficult as training progresses. While many recent works provide
analyses and partial solutions to this phenomenon, a fundamental question
remains unanswered: to what extent do known mechanisms of plasticity loss
overlap, and how can mitigation strategies be combined to best maintain the
trainability of a network? This paper addresses these questions, showing that
loss of plasticity can be decomposed into multiple independent mechanisms and
that, while intervening on any single mechanism is insufficient to avoid the
loss of plasticity in all cases, intervening on multiple mechanisms in
conjunction results in highly robust learning algorithms. We show that a
combination of layer normalization and weight decay is highly effective at
maintaining plasticity in a variety of synthetic nonstationary learning tasks,
and further demonstrate its effectiveness on naturally arising
nonstationarities, including reinforcement learning in the Arcade Learning
Environment.</div><div><a href='http://arxiv.org/abs/2402.18762v1'>2402.18762v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12181v1")'>Revisiting Data Augmentation in Deep Reinforcement Learning</div>
<div id='2402.12181v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:42:10Z</div><div>Authors: Jianshu Hu, Yunpeng Jiang, Paul Weng</div><div style='padding-top: 10px; width: 80ex'>Various data augmentation techniques have been recently proposed in
image-based deep reinforcement learning (DRL). Although they empirically
demonstrate the effectiveness of data augmentation for improving sample
efficiency or generalization, which technique should be preferred is not always
clear. To tackle this question, we analyze existing methods to better
understand them and to uncover how they are connected. Notably, by expressing
the variance of the Q-targets and that of the empirical actor/critic losses of
these methods, we can analyze the effects of their different components and
compare them. We furthermore formulate an explanation about how these methods
may be affected by choosing different data augmentation transformations in
calculating the target Q-values. This analysis suggests recommendations on how
to exploit data augmentation in a more principled way. In addition, we include
a regularization term called tangent prop, previously proposed in computer
vision, but whose adaptation to DRL is novel to the best of our knowledge. We
evaluate our proposition and validate our analysis in several domains. Compared
to different relevant baselines, we demonstrate that it achieves
state-of-the-art performance in most environments and shows higher sample
efficiency and better generalization ability in some complex environments.</div><div><a href='http://arxiv.org/abs/2402.12181v1'>2402.12181v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01371v1")'>Critic-Actor for Average Reward MDPs with Function Approximation: A
  Finite-Time Analysis</div>
<div id='2402.01371v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:48:49Z</div><div>Authors: Prashansa Panda, Shalabh Bhatnagar</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a lot of research work activity focused on
carrying out asymptotic and non-asymptotic convergence analyses for
two-timescale actor critic algorithms where the actor updates are performed on
a timescale that is slower than that of the critic. In a recent work, the
critic-actor algorithm has been presented for the infinite horizon discounted
cost setting in the look-up table case where the timescales of the actor and
the critic are reversed and asymptotic convergence analysis has been presented.
In our work, we present the first critic-actor algorithm with function
approximation and in the long-run average reward setting and present the first
finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal
learning rates and prove that our algorithm achieves a sample complexity of
$\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the
critic to be upper bounded by $\epsilon$ which is better than the one obtained
for actor-critic in a similar setting. We also show the results of numerical
experiments on three benchmark settings and observe that the critic-actor
algorithm competes well with the actor-critic algorithm.</div><div><a href='http://arxiv.org/abs/2402.01371v1'>2402.01371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01014v1")'>A Case for Validation Buffer in Pessimistic Actor-Critic</div>
<div id='2403.01014v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T22:24:11Z</div><div>Authors: Michal Nauman, Mateusz Ostaszewski, Marek Cygan</div><div style='padding-top: 10px; width: 80ex'>In this paper, we investigate the issue of error accumulation in critic
networks updated via pessimistic temporal difference objectives. We show that
the critic approximation error can be approximated via a recursive fixed-point
model similar to that of the Bellman value. We use such recursive definition to
retrieve the conditions under which the pessimistic critic is unbiased.
Building on these insights, we propose Validation Pessimism Learning (VPL)
algorithm. VPL uses a small validation buffer to adjust the levels of pessimism
throughout the agent training, with the pessimism set such that the
approximation error of the critic targets is minimized. We investigate the
proposed approach on a variety of locomotion and manipulation tasks and report
improvements in sample efficiency and performance.</div><div><a href='http://arxiv.org/abs/2403.01014v1'>2403.01014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07752v1")'>Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL
  with Continuous Action Domains</div>
<div id='2402.07752v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:21:50Z</div><div>Authors: Yasin Findik, S. Reza Ahmadzadeh</div><div style='padding-top: 10px; width: 80ex'>Tackling multi-agent learning problems efficiently is a challenging task in
continuous action domains. While value-based algorithms excel in sample
efficiency when applied to discrete action domains, they are usually
inefficient when dealing with continuous actions. Policy-based algorithms, on
the other hand, attempt to address this challenge by leveraging critic networks
for guiding the learning process and stabilizing the gradient estimation. The
limitations in the estimation of true return and falling into local optima in
these methods result in inefficient and often sub-optimal policies. In this
paper, we diverge from the trend of further enhancing critic networks, and
focus on improving the effectiveness of value-based methods in multi-agent
continuous domains by concurrently evaluating numerous actions. We propose a
novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired
from the idea of Q-Functionals, that enables agents to transform their states
into basis functions. Our algorithm fosters collaboration among agents by
mixing their action-values. We evaluate the efficacy of our algorithm in six
cooperative multi-agent scenarios. Our empirical findings reveal that MQF
outperforms four variants of Deep Deterministic Policy Gradient through rapid
action evaluation and increased sample efficiency.</div><div><a href='http://arxiv.org/abs/2402.07752v1'>2402.07752v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08850v2")'>REValueD: Regularised Ensemble Value-Decomposition for Factorisable
  Markov Decision Processes</div>
<div id='2401.08850v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T21:47:23Z</div><div>Authors: David Ireland, Giovanni Montana</div><div style='padding-top: 10px; width: 80ex'>Discrete-action reinforcement learning algorithms often falter in tasks with
high-dimensional discrete action spaces due to the vast number of possible
actions. A recent advancement leverages value-decomposition, a concept from
multi-agent reinforcement learning, to tackle this challenge. This study delves
deep into the effects of this value-decomposition, revealing that whilst it
curtails the over-estimation bias inherent to Q-learning algorithms, it
amplifies target variance. To counteract this, we present an ensemble of
critics to mitigate target variance. Moreover, we introduce a regularisation
loss that helps to mitigate the effects that exploratory actions in one
dimension can have on the value of optimal actions in other dimensions. Our
novel algorithm, REValueD, tested on discretised versions of the DeepMind
Control Suite tasks, showcases superior performance, especially in the
challenging humanoid and dog tasks. We further dissect the factors influencing
REValueD's performance, evaluating the significance of the regularisation loss
and the scalability of REValueD with increasing sub-actions per dimension.</div><div><a href='http://arxiv.org/abs/2401.08850v2'>2401.08850v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03137v1")'>SPQR: Controlling Q-ensemble Independence with Spiked Random Model for
  Reinforcement Learning</div>
<div id='2401.03137v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:39:06Z</div><div>Authors: Dohyeok Lee, Seungyub Han, Taehyun Cho, Jungwoo Lee</div><div style='padding-top: 10px; width: 80ex'>Alleviating overestimation bias is a critical challenge for deep
reinforcement learning to achieve successful performance on more complex tasks
or offline datasets containing out-of-distribution data. In order to overcome
overestimation bias, ensemble methods for Q-learning have been investigated to
exploit the diversity of multiple Q-functions. Since network initialization has
been the predominant approach to promote diversity in Q-functions,
heuristically designed diversity injection methods have been studied in the
literature. However, previous studies have not attempted to approach guaranteed
independence over an ensemble from a theoretical perspective. By introducing a
novel regularization loss for Q-ensemble independence based on random matrix
theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR)
for reinforcement learning. Specifically, we modify the intractable hypothesis
testing criterion for the Q-ensemble independence into a tractable KL
divergence between the spectral distribution of the Q-ensemble and the target
Wigner's semicircle distribution. We implement SPQR in several online and
offline ensemble Q-learning algorithms. In the experiments, SPQR outperforms
the baseline algorithms in both online and offline RL benchmarks.</div><div><a href='http://arxiv.org/abs/2401.03137v1'>2401.03137v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05710v1")'>The Distributional Reward Critic Architecture for Perturbed-Reward
  Reinforcement Learning</div>
<div id='2401.05710v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T07:25:28Z</div><div>Authors: Xi Chen, Zhihui Zhu, Andrew Perrault</div><div style='padding-top: 10px; width: 80ex'>We study reinforcement learning in the presence of an unknown reward
perturbation. Existing methodologies for this problem make strong assumptions
including reward smoothness, known perturbations, and/or perturbations that do
not modify the optimal policy. We study the case of unknown arbitrary
perturbations that discretize and shuffle reward space, but have the property
that the true reward belongs to the most frequently observed class after
perturbation. This class of perturbations generalizes existing classes (and, in
the limit, all continuous bounded perturbations) and defeats existing methods.
We introduce an adaptive distributional reward critic and show theoretically
that it can recover the true rewards under technical conditions. Under the
targeted perturbation in discrete and continuous control tasks, we win/tie the
highest return in 40/57 settings (compared to 16/57 for the best baseline).
Even under the untargeted perturbation, we still win an edge over the baseline
designed especially for that setting.</div><div><a href='http://arxiv.org/abs/2401.05710v1'>2401.05710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00348v1")'>ODICE: Revealing the Mystery of Distribution Correction Estimation via
  Orthogonal-gradient Update</div>
<div id='2402.00348v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T05:30:51Z</div><div>Authors: Liyuan Mao, Haoran Xu, Weinan Zhang, Xianyuan Zhan</div><div style='padding-top: 10px; width: 80ex'>In this study, we investigate the DIstribution Correction Estimation (DICE)
methods, an important line of work in offline reinforcement learning (RL) and
imitation learning (IL). DICE-based methods impose state-action-level behavior
constraint, which is an ideal choice for offline learning. However, they
typically perform much worse than current state-of-the-art (SOTA) methods that
solely use action-level behavior constraint. After revisiting DICE-based
methods, we find there exist two gradient terms when learning the value
function using true-gradient update: forward gradient (taken on the current
state) and backward gradient (taken on the next state). Using forward gradient
bears a large similarity to many offline RL methods, and thus can be regarded
as applying action-level constraint. However, directly adding the backward
gradient may degenerate or cancel out its effect if these two gradients have
conflicting directions. To resolve this issue, we propose a simple yet
effective modification that projects the backward gradient onto the normal
plane of the forward gradient, resulting in an orthogonal-gradient update, a
new learning rule for DICE-based methods. We conduct thorough theoretical
analyses and find that the projected backward gradient brings state-level
behavior regularization, which reveals the mystery of DICE-based methods: the
value learning objective does try to impose state-action-level constraint, but
needs to be used in a corrected way. Through toy examples and extensive
experiments on complex offline RL and IL tasks, we demonstrate that DICE-based
methods using orthogonal-gradient updates (O-DICE) achieve SOTA performance and
great robustness.</div><div><a href='http://arxiv.org/abs/2402.00348v1'>2402.00348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08552v1")'>Confronting Reward Overoptimization for Diffusion Models: A Perspective
  of Inductive and Primacy Biases</div>
<div id='2402.08552v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T15:55:41Z</div><div>Authors: Ziyi Zhang, Sen Zhang, Yibing Zhan, Yong Luo, Yonggang Wen, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Bridging the gap between diffusion models and human preferences is crucial
for their integration into practical generative workflows. While optimizing
downstream reward models has emerged as a promising alignment strategy,
concerns arise regarding the risk of excessive optimization with learned reward
models, which potentially compromises ground-truth performance. In this work,
we confront the reward overoptimization problem in diffusion model alignment
through the lenses of both inductive and primacy biases. We first identify the
divergence of current methods from the temporal inductive bias inherent in the
multi-step denoising process of diffusion models as a potential source of
overoptimization. Then, we surprisingly discover that dormant neurons in our
critic model act as a regularization against overoptimization, while active
neurons reflect primacy bias in this setting. Motivated by these observations,
we propose Temporal Diffusion Policy Optimization with critic active neuron
Reset (TDPO-R), a policy gradient algorithm that exploits the temporal
inductive bias of intermediate timesteps, along with a novel reset strategy
that targets active neurons to counteract the primacy bias. Empirical results
demonstrate the superior efficacy of our algorithms in mitigating reward
overoptimization.</div><div><a href='http://arxiv.org/abs/2402.08552v1'>2402.08552v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01112v2")'>Efficient Episodic Memory Utilization of Cooperative Multi-Agent
  Reinforcement Learning</div>
<div id='2403.01112v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T07:37:05Z</div><div>Authors: Hyungho Na, Yunkyeong Seo, Il-chul Moon</div><div style='padding-top: 10px; width: 80ex'>In cooperative multi-agent reinforcement learning (MARL), agents aim to
achieve a common goal, such as defeating enemies or scoring a goal. Existing
MARL algorithms are effective but still require significant learning time and
often get trapped in local optima by complex tasks, subsequently failing to
discover a goal-reaching policy. To address this, we introduce Efficient
episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a)
accelerating reinforcement learning by leveraging semantically coherent memory
from an episodic buffer and (b) selectively promoting desirable transitions to
prevent local convergence. To achieve (a), EMU incorporates a trainable
encoder/decoder structure alongside MARL, creating coherent memory embeddings
that facilitate exploratory memory recall. To achieve (b), EMU introduces a
novel reward structure called episodic incentive based on the desirability of
states. This reward improves the TD target in Q-learning and acts as an
additional incentive for desirable transitions. We provide theoretical support
for the proposed incentive and demonstrate the effectiveness of EMU compared to
conventional episodic control. The proposed method is evaluated in StarCraft II
and Google Research Football, and empirical results indicate further
performance improvement over state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.01112v2'>2403.01112v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01348v1")'>CORE: Mitigating Catastrophic Forgetting in Continual Learning through
  Cognitive Replay</div>
<div id='2402.01348v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:04:44Z</div><div>Authors: Jianshu Zhang, Yankai Fu, Ziheng Peng, Dongyu Yao, Kun He</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel perspective to significantly mitigate
catastrophic forgetting in continuous learning (CL), which emphasizes models'
capacity to preserve existing knowledge and assimilate new information. Current
replay-based methods treat every task and data sample equally and thus can not
fully exploit the potential of the replay buffer. In response, we propose
COgnitive REplay (CORE), which draws inspiration from human cognitive review
processes. CORE includes two key strategies: Adaptive Quantity Allocation and
Quality-Focused Data Selection. The former adaptively modulates the replay
buffer allocation for each task based on its forgetting rate, while the latter
guarantees the inclusion of representative data that best encapsulates the
characteristics of each task within the buffer. Our approach achieves an
average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline
method by 6.52%. Additionally, it significantly enhances the accuracy of the
poorest-performing task by 6.30% compared to the top baseline.</div><div><a href='http://arxiv.org/abs/2402.01348v1'>2402.01348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09542v1")'>Layerwise Proximal Replay: A Proximal Point Method for Online Continual
  Learning</div>
<div id='2402.09542v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T19:34:28Z</div><div>Authors: Jason Yoo, Yunpeng Liu, Frank Wood, Geoff Pleiss</div><div style='padding-top: 10px; width: 80ex'>In online continual learning, a neural network incrementally learns from a
non-i.i.d. data stream. Nearly all online continual learning methods employ
experience replay to simultaneously prevent catastrophic forgetting and
underfitting on past data. Our work demonstrates a limitation of this approach:
networks trained with experience replay tend to have unstable optimization
trajectories, impeding their overall accuracy. Surprisingly, these
instabilities persist even when the replay buffer stores all previous training
examples, suggesting that this issue is orthogonal to catastrophic forgetting.
We minimize these instabilities through a simple modification of the
optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances
learning from new and replay data while only allowing for gradual changes in
the hidden activation of past data. We demonstrate that LPR consistently
improves replay-based online continual learning methods across multiple problem
settings, regardless of the amount of available replay memory.</div><div><a href='http://arxiv.org/abs/2402.09542v1'>2402.09542v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13249v1")'>A Unified and General Framework for Continual Learning</div>
<div id='2403.13249v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T02:21:44Z</div><div>Authors: Zhenyi Wang, Yan Li, Li Shen, Heng Huang</div><div style='padding-top: 10px; width: 80ex'>Continual Learning (CL) focuses on learning from dynamic and changing data
distributions while retaining previously acquired knowledge. Various methods
have been developed to address the challenge of catastrophic forgetting,
including regularization-based, Bayesian-based, and memory-replay-based
techniques. However, these methods lack a unified framework and common
terminology for describing their approaches. This research aims to bridge this
gap by introducing a comprehensive and overarching framework that encompasses
and reconciles these existing methodologies. Notably, this new framework is
capable of encompassing established CL approaches as special instances within a
unified and general optimization objective. An intriguing finding is that
despite their diverse origins, these methods share common mathematical
structures. This observation highlights the compatibility of these seemingly
distinct techniques, revealing their interconnectedness through a shared
underlying optimization objective. Moreover, the proposed general framework
introduces an innovative concept called refresh learning, specifically designed
to enhance the CL performance. This novel approach draws inspiration from
neuroscience, where the human brain often sheds outdated information to improve
the retention of crucial knowledge and facilitate the acquisition of new
information. In essence, refresh learning operates by initially unlearning
current data and subsequently relearning it. It serves as a versatile plug-in
that seamlessly integrates with existing CL methods, offering an adaptable and
effective enhancement to the learning process. Extensive experiments on CL
benchmarks and theoretical analysis demonstrate the effectiveness of the
proposed refresh learning. Code is available at
\url{https://github.com/joey-wang123/CL-refresh-learning}.</div><div><a href='http://arxiv.org/abs/2403.13249v1'>2403.13249v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05175v1")'>Continual Learning and Catastrophic Forgetting</div>
<div id='2403.05175v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T09:32:43Z</div><div>Authors: Gido M. van de Ven, Nicholas Soures, Dhireesha Kudithipudi</div><div style='padding-top: 10px; width: 80ex'>This book chapter delves into the dynamics of continual learning, which is
the process of incrementally learning from a non-stationary stream of data.
Although continual learning is a natural skill for the human brain, it is very
challenging for artificial neural networks. An important reason is that, when
learning something new, these networks tend to quickly and drastically forget
what they had learned before, a phenomenon known as catastrophic forgetting.
Especially in the last decade, continual learning has become an extensively
studied topic in deep learning. This book chapter reviews the insights that
this field has generated.</div><div><a href='http://arxiv.org/abs/2403.05175v1'>2403.05175v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06398v2")'>On the Diminishing Returns of Width for Continual Learning</div>
<div id='2403.06398v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T03:19:45Z</div><div>Authors: Etash Guha, Vihan Lakshman</div><div style='padding-top: 10px; width: 80ex'>While deep neural networks have demonstrated groundbreaking performance in
various settings, these models often suffer from \emph{catastrophic forgetting}
when trained on new tasks in sequence. Several works have empirically
demonstrated that increasing the width of a neural network leads to a decrease
in catastrophic forgetting but have yet to characterize the exact relationship
between width and continual learning. We design one of the first frameworks to
analyze Continual Learning Theory and prove that width is directly related to
forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that
increasing network widths to reduce forgetting yields diminishing returns. We
empirically verify our claims at widths hitherto unexplored in prior studies
where the diminishing returns are clearly observed as predicted by our theory.</div><div><a href='http://arxiv.org/abs/2403.06398v2'>2403.06398v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12617v2")'>The Joint Effect of Task Similarity and Overparameterization on
  Catastrophic Forgetting -- An Analytical Model</div>
<div id='2401.12617v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:16:44Z</div><div>Authors: Daniel Goldfarb, Itay Evron, Nir Weinberger, Daniel Soudry, Paul Hand</div><div style='padding-top: 10px; width: 80ex'>In continual learning, catastrophic forgetting is affected by multiple
aspects of the tasks. Previous works have analyzed separately how forgetting is
affected by either task similarity or overparameterization. In contrast, our
paper examines how task similarity and overparameterization jointly affect
forgetting in an analyzable model. Specifically, we focus on two-task continual
linear regression, where the second task is a random orthogonal transformation
of an arbitrary first task (an abstraction of random permutation tasks). We
derive an exact analytical expression for the expected forgetting - and uncover
a nuanced pattern. In highly overparameterized models, intermediate task
similarity causes the most forgetting. However, near the interpolation
threshold, forgetting decreases monotonically with the expected task
similarity. We validate our findings with linear regression on synthetic data,
and with neural networks on established permutation task benchmarks.</div><div><a href='http://arxiv.org/abs/2401.12617v2'>2401.12617v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12465v1")'>Neuro-mimetic Task-free Unsupervised Online Learning with Continual
  Self-Organizing Maps</div>
<div id='2402.12465v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:11:22Z</div><div>Authors: Hitesh Vaidya, Travis Desell, Ankur Mali, Alexander Ororbia</div><div style='padding-top: 10px; width: 80ex'>An intelligent system capable of continual learning is one that can process
and extract knowledge from potentially infinitely long streams of pattern
vectors. The major challenge that makes crafting such a system difficult is
known as catastrophic forgetting - an agent, such as one based on artificial
neural networks (ANNs), struggles to retain previously acquired knowledge when
learning from new samples. Furthermore, ensuring that knowledge is preserved
for previous tasks becomes more challenging when input is not supplemented with
task boundary information. Although forgetting in the context of ANNs has been
studied extensively, there still exists far less work investigating it in terms
of unsupervised architectures such as the venerable self-organizing map (SOM),
a neural model often used in clustering and dimensionality reduction. While the
internal mechanisms of SOMs could, in principle, yield sparse representations
that improve memory retention, we observe that, when a fixed-size SOM processes
continuous data streams, it experiences concept drift. In light of this, we
propose a generalization of the SOM, the continual SOM (CSOM), which is capable
of online unsupervised learning under a low memory budget. Our results, on
benchmarks including MNIST, Kuzushiji-MNIST, and Fashion-MNIST, show almost a
two times increase in accuracy, and CIFAR-10 demonstrates a state-of-the-art
result when tested on (online) unsupervised class incremental learning setting.</div><div><a href='http://arxiv.org/abs/2402.12465v1'>2402.12465v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15973v1")'>Sample Weight Estimation Using Meta-Updates for Online Continual
  Learning</div>
<div id='2401.15973v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T09:04:45Z</div><div>Authors: Hamed Hemati, Damian Borth</div><div style='padding-top: 10px; width: 80ex'>The loss function plays an important role in optimizing the performance of a
learning system. A crucial aspect of the loss function is the assignment of
sample weights within a mini-batch during loss computation. In the context of
continual learning (CL), most existing strategies uniformly treat samples when
calculating the loss value, thereby assigning equal weights to each sample.
While this approach can be effective in certain standard benchmarks, its
optimal effectiveness, particularly in more complex scenarios, remains
underexplored. This is particularly pertinent in training "in the wild," such
as with self-training, where labeling is automated using a reference model.
This paper introduces the Online Meta-learning for Sample Importance (OMSI)
strategy that approximates sample weights for a mini-batch in an online CL
stream using an inner- and meta-update mechanism. This is done by first
estimating sample weight parameters for each sample in the mini-batch, then,
updating the model with the adapted sample weights. We evaluate OMSI in two
distinct experimental settings. First, we show that OMSI enhances both learning
and retained accuracy in a controlled noisy-labeled data stream. Then, we test
the strategy in three standard benchmarks and compare it with other popular
replay-based strategies. This research aims to foster the ongoing exploration
in the area of self-adaptive CL.</div><div><a href='http://arxiv.org/abs/2401.15973v1'>2401.15973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03545v1")'>Online Feature Updates Improve Online (Generalized) Label Shift
  Adaptation</div>
<div id='2402.03545v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:03:25Z</div><div>Authors: Ruihan Wu, Siddhartha Datta, Yi Su, Dheeraj Baby, Yu-Xiang Wang, Kilian Q. Weinberger</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the prevalent issue of label shift in an online setting
with missing labels, where data distributions change over time and obtaining
timely labels is challenging. While existing methods primarily focus on
adjusting or updating the final layer of a pre-trained classifier, we explore
the untapped potential of enhancing feature representations using unlabeled
data at test-time. Our novel method, Online Label Shift adaptation with Online
Feature Updates (OLS-OFU), leverages self-supervised learning to refine the
feature extraction process, thereby improving the prediction model. Theoretical
analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on
self-supervised learning for feature refinement. Empirical studies on various
datasets, under both online label shift and generalized label shift conditions,
underscore the effectiveness and robustness of OLS-OFU, especially in cases of
domain shifts.</div><div><a href='http://arxiv.org/abs/2402.03545v1'>2402.03545v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09859v1")'>MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning</div>
<div id='2403.09859v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T20:40:36Z</div><div>Authors: Zohar Rimon, Tom Jurgenson, Orr Krupnik, Gilad Adler, Aviv Tamar</div><div style='padding-top: 10px; width: 80ex'>Meta-reinforcement learning (meta-RL) is a promising framework for tackling
challenging domains requiring efficient exploration. Existing meta-RL
algorithms are characterized by low sample efficiency, and mostly focus on
low-dimensional task distributions. In parallel, model-based RL methods have
been successful in solving partially observable MDPs, of which meta-RL is a
special case. In this work, we leverage this success and propose a new
model-based approach to meta-RL, based on elements from existing
state-of-the-art model-based and meta-RL methods. We demonstrate the
effectiveness of our approach on common meta-RL benchmark domains, attaining
greater return with better sample efficiency (up to $15\times$) while requiring
very little hyperparameter tuning. In addition, we validate our approach on a
slate of more challenging, higher-dimensional domains, taking a step towards
real-world generalizing agents.</div><div><a href='http://arxiv.org/abs/2403.09859v1'>2403.09859v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16801v1")'>Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement
  Learning</div>
<div id='2402.16801v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:19:07Z</div><div>Authors: Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel Coward, Jakob Foerster</div><div style='padding-top: 10px; width: 80ex'>Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms. We identify that existing benchmarks
used for research into open-ended learning fall into one of two categories.
Either they are too slow for meaningful research to be performed without
enormous computational resources, like Crafter, NetHack and Minecraft, or they
are not complex enough to pose a significant challenge, like Minigrid and
Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite
of Crafter in JAX that runs up to 250x faster than the Python-native original.
A run of PPO using 1 billion environment interactions finishes in under an hour
using only a single GPU and averages 90% of the optimal reward. To provide a
more compelling challenge we present the main Craftax benchmark, a significant
extension of the Crafter mechanics with elements inspired from NetHack. Solving
Craftax requires deep exploration, long term planning and memory, as well as
continual adaptation to novel situations as more of the world is discovered. We
show that existing methods including global and episodic exploration, as well
as unsupervised environment design fail to make material progress on the
benchmark. We believe that Craftax can for the first time allow researchers to
experiment in a complex, open-ended environment with limited computational
resources.</div><div><a href='http://arxiv.org/abs/2402.16801v1'>2402.16801v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16650v2")'>Augmenting Replay in World Models for Continual Reinforcement Learning</div>
<div id='2401.16650v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T00:48:26Z</div><div>Authors: Luke Yang, Levin Kuhlmann, Gideon Kowadlo</div><div style='padding-top: 10px; width: 80ex'>Continual RL is a challenging problem where the agent is exposed to a
sequence of tasks; it should learn new tasks without forgetting old ones, and
learning the new task should improve performance on previous and future tasks.
The most common approaches use model-free RL algorithms as a base, and replay
buffers have been used to overcome catastrophic forgetting. However, the
buffers are often very large making scalability difficult. Also, the concept of
replay comes from biological inspiration, where evidence suggests that replay
is applied to a world model, which implies model-based RL -- and model-based RL
should have benefits for continual RL, where it is possible to exploit
knowledge independent of the policy. We present WMAR, World Models with
Augmented Replay, a model-based RL algorithm with a world model and memory
efficient distribution matching replay buffer. It is based on the well-known
DreamerV3 algorithm, which has a simple FIFO buffer and was not tested in a
continual RL setting. We evaluated WMAR vs WMAR (FIFO only) on tasks with and
without shared structure from OpenAI ProcGen and Atari respectively, and
without a task oracle. We found that WMAR has favourable properties on
continual RL with significantly reduced computational overhead compared to WMAR
(FIFO only). WMAR had small benefits over DreamerV3 on tasks with shared
structure and substantially better forgetting characteristics on tasks without
shared structure; but at the cost of lower plasticity seen in a lower maximum
on new tasks. The results suggest that model-based RL using a world model with
a memory efficient replay buffer can be an effective and practical approach to
continual RL, justifying future work.</div><div><a href='http://arxiv.org/abs/2401.16650v2'>2401.16650v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04253v1")'>Mastering Memory Tasks with World Models</div>
<div id='2403.04253v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T06:35:59Z</div><div>Authors: Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar</div><div style='padding-top: 10px; width: 80ex'>Current model-based reinforcement learning (MBRL) agents struggle with
long-term dependencies. This limits their ability to effectively solve tasks
involving extended time gaps between actions and outcomes, or tasks demanding
the recalling of distant observations to inform current actions. To improve
temporal coherence, we integrate a new family of state space models (SSMs) in
world models of MBRL agents to present a new method, Recall to Imagine (R2I).
This integration aims to enhance both long-term memory and long-horizon credit
assignment. Through a diverse set of illustrative tasks, we systematically
demonstrate that R2I not only establishes a new state-of-the-art for
challenging memory and credit assignment RL tasks, such as BSuite and POPGym,
but also showcases superhuman performance in the complex memory domain of
Memory Maze. At the same time, it upholds comparable performance in classic RL
tasks, such as Atari and DMC, suggesting the generality of our method. We also
show that R2I is faster than the state-of-the-art MBRL method, DreamerV3,
resulting in faster wall-time convergence.</div><div><a href='http://arxiv.org/abs/2403.04253v1'>2403.04253v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08112v1")'>A Competition Winning Deep Reinforcement Learning Agent in microRTS</div>
<div id='2402.08112v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T23:08:17Z</div><div>Authors: Scott Goodfriend</div><div style='padding-top: 10px; width: 80ex'>Scripted agents have predominantly won the five previous iterations of the
IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep
Reinforcement Learning (DRL) algorithms making significant strides in real-time
strategy (RTS) games, their adoption in this primarily academic competition has
been limited due to the considerable training resources required and the
complexity inherent in creating and debugging such agents. RAISocketAI is the
first DRL agent to win the IEEE microRTS competition. In a benchmark without
performance constraints, RAISocketAI regularly defeated the two prior
competition winners. This first competition-winning DRL submission can be a
benchmark for future microRTS competitions and a starting point for future DRL
research. Iteratively fine-tuning the base policy and transfer learning to
specific maps were critical to RAISocketAI's winning performance. These
strategies can be used to economically train future DRL agents. Further work in
Imitation Learning using Behavior Cloning and fine-tuning these models with DRL
has proven promising as an efficient way to bootstrap models with demonstrated,
competitive behaviors.</div><div><a href='http://arxiv.org/abs/2402.08112v1'>2402.08112v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00420v1")'>Robust Deep Reinforcement Learning Through Adversarial Attacks and
  Training : A Survey</div>
<div id='2403.00420v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T10:16:46Z</div><div>Authors: Lucas Schott, Josephine Delas, Hatem Hajri, Elies Gherbi, Reda Yaich, Nora Boulahia-Cuppens, Frederic Cuppens, Sylvain Lamprier</div><div style='padding-top: 10px; width: 80ex'>Deep Reinforcement Learning (DRL) is an approach for training autonomous
agents across various complex environments. Despite its significant performance
in well known environments, it remains susceptible to minor conditions
variations, raising concerns about its reliability in real-world applications.
To improve usability, DRL must demonstrate trustworthiness and robustness. A
way to improve robustness of DRL to unknown changes in the conditions is
through Adversarial Training, by training the agent against well suited
adversarial attacks on the dynamics of the environment. Addressing this
critical issue, our work presents an in-depth analysis of contemporary
adversarial attack methodologies, systematically categorizing them and
comparing their objectives and operational mechanisms. This classification
offers a detailed insight into how adversarial attacks effectively act for
evaluating the resilience of DRL agents, thereby paving the way for enhancing
their robustness.</div><div><a href='http://arxiv.org/abs/2403.00420v1'>2403.00420v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02349v1")'>A Survey Analyzing Generalization in Deep Reinforcement Learning</div>
<div id='2401.02349v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T16:45:01Z</div><div>Authors: Ezgi Korkmaz</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to self driving vehicles, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will outline the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
robustness and generalization capabilities. Furthermore, we will formalize and
unify the diverse solution approaches to increase generalization, and overcome
overfitting in state-action value functions. We believe our study can provide a
compact systematic unified analysis for the current advancements in deep
reinforcement learning, and help to construct robust deep neural policies with
improved generalization abilities.</div><div><a href='http://arxiv.org/abs/2401.02349v1'>2401.02349v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16543v1")'>Model-based deep reinforcement learning for accelerated learning from
  flow simulations</div>
<div id='2402.16543v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T13:01:45Z</div><div>Authors: Andre Weiner, Janis Geise</div><div style='padding-top: 10px; width: 80ex'>In recent years, deep reinforcement learning has emerged as a technique to
solve closed-loop flow control problems. Employing simulation-based
environments in reinforcement learning enables a priori end-to-end optimization
of the control system, provides a virtual testbed for safety-critical control
applications, and allows to gain a deep understanding of the control
mechanisms. While reinforcement learning has been applied successfully in a
number of rather simple flow control benchmarks, a major bottleneck toward
real-world applications is the high computational cost and turnaround time of
flow simulations. In this contribution, we demonstrate the benefits of
model-based reinforcement learning for flow control applications. Specifically,
we optimize the policy by alternating between trajectories sampled from flow
simulations and trajectories sampled from an ensemble of environment models.
The model-based learning reduces the overall training time by up to $85\%$ for
the fluidic pinball test case. Even larger savings are expected for more
demanding flow simulations.</div><div><a href='http://arxiv.org/abs/2402.16543v1'>2402.16543v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17402v1")'>Beacon, a lightweight deep reinforcement learning benchmark library for
  flow control</div>
<div id='2402.17402v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T10:48:56Z</div><div>Authors: Jonathan Viquerat, Philippe Meliga, Pablo Jeken, Elie Hachem</div><div style='padding-top: 10px; width: 80ex'>Recently, the increasing use of deep reinforcement learning for flow control
problems has led to a new area of research, focused on the coupling and the
adaptation of the existing algorithms to the control of numerical fluid
dynamics environments. Although still in its infancy, the field has seen
multiple successes in a short time span, and its fast development pace can
certainly be partly imparted to the open-source effort that drives the
expansion of the community. Yet, this emerging domain still misses a common
ground to (i) ensure the reproducibility of the results, and (ii) offer a
proper ad-hoc benchmarking basis. To this end, we propose Beacon, an
open-source benchmark library composed of seven lightweight 1D and 2D flow
control problems with various characteristics, action and observation space
characteristics, and CPU requirements. In this contribution, the seven
considered problems are described, and reference control solutions are
provided. The sources for the following work are available at
https://github.com/jviquerat/beacon.</div><div><a href='http://arxiv.org/abs/2402.17402v1'>2402.17402v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11515v1")'>Optimal Parallelization Strategies for Active Flow Control in Deep
  Reinforcement Learning-Based Computational Fluid Dynamics</div>
<div id='2402.11515v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T09:07:30Z</div><div>Authors: Wang Jia, Hang Xu</div><div style='padding-top: 10px; width: 80ex'>Deep Reinforcement Learning (DRL) has emerged as a promising approach for
handling highly dynamic and nonlinear Active Flow Control (AFC) problems.
However, the computational cost associated with training DRL models presents a
significant performance bottleneck. To address this challenge and enable
efficient scaling on high-performance computing architectures, this study
focuses on optimizing DRL-based algorithms in parallel settings. We validate an
existing state-of-the-art DRL framework used for AFC problems and discuss its
efficiency bottlenecks. Subsequently, by deconstructing the overall framework
and conducting extensive scalability benchmarks for individual components, we
investigate various hybrid parallelization configurations and propose efficient
parallelization strategies. Moreover, we refine input/output (I/O) operations
in multi-environment DRL training to tackle critical overhead associated with
data movement. Finally, we demonstrate the optimized framework for a typical
AFC problem where near-linear scaling can be obtained for the overall
framework. We achieve a significant boost in parallel efficiency from around
49% to approximately 78%, and the training process is accelerated by
approximately 47 times using 60 CPU cores. These findings are expected to
provide valuable insights for further advancements in DRL-based AFC studies.</div><div><a href='http://arxiv.org/abs/2402.11515v1'>2402.11515v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04329v1")'>A mechanism-informed reinforcement learning framework for shape
  optimization of airfoils</div>
<div id='2403.04329v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T08:48:42Z</div><div>Authors: Jingfeng Wang, Guanghui Hu</div><div style='padding-top: 10px; width: 80ex'>In this study, we present the mechanism-informed reinforcement learning
framework for airfoil shape optimization. By leveraging the twin delayed deep
deterministic policy gradient algorithm for its notable stability, our approach
addresses the complexities of optimizing shapes governed by fluid dynamics. The
PDEs-based solver is adopted for its accuracy even when the configurations and
geometries are extraordinarily changed during the exploration. Dual-weighted
residual-based mesh refinement strategy is applied to ensure the accurate
calculation of target functionals. To streamline the iterative optimization
process and handle geometric deformations, our approach integrates Laplacian
smoothing, adaptive refinement, and a B\'ezier fitting strategy. This
combination not only remits mesh tangling but also guarantees a precise
manipulation of the airfoil geometry. Our neural network architecture leverages
B\'ezier curves for efficient dimensionality reduction, thereby enhancing the
learning process and ensuring the geometric accuracy of the airfoil shapes. An
attention mechanism is embedded within the network to calculate potential
action on the state as well. Furthermore, we have introduced different reward
and penalty mechanisms tailored to the specific challenges of airfoil
optimization. This algorithm is designed to support the optimization task,
facilitating a more targeted and effective approach for airfoil shape
optimization.</div><div><a href='http://arxiv.org/abs/2403.04329v1'>2403.04329v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.15091v1")'>Improved Long Short-Term Memory-based Wastewater Treatment Simulators
  for Deep Reinforcement Learning</div>
<div id='2403.15091v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T10:20:09Z</div><div>Authors: Esmaeel Mohammadi, Daniel Ortiz-Arroyo, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen, Petar Durdevic</div><div style='padding-top: 10px; width: 80ex'>Even though Deep Reinforcement Learning (DRL) showed outstanding results in
the fields of Robotics and Games, it is still challenging to implement it in
the optimization of industrial processes like wastewater treatment. One of the
challenges is the lack of a simulation environment that will represent the
actual plant as accurately as possible to train DRL policies. Stochasticity and
non-linearity of wastewater treatment data lead to unstable and incorrect
predictions of models over long time horizons. One possible reason for the
models' incorrect simulation behavior can be related to the issue of
compounding error, which is the accumulation of errors throughout the
simulation. The compounding error occurs because the model utilizes its
predictions as inputs at each time step. The error between the actual data and
the prediction accumulates as the simulation continues. We implemented two
methods to improve the trained models for wastewater treatment data, which
resulted in more accurate simulators: 1- Using the model's prediction data as
input in the training step as a tool of correction, and 2- Change in the loss
function to consider the long-term predicted shape (dynamics). The experimental
results showed that implementing these methods can improve the behavior of
simulators in terms of Dynamic Time Warping throughout a year up to 98%
compared to the base model. These improvements demonstrate significant promise
in creating simulators for biological processes that do not need pre-existing
knowledge of the process but instead depend exclusively on time series data
obtained from the system.</div><div><a href='http://arxiv.org/abs/2403.15091v1'>2403.15091v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12822v1")'>Deep Learning Based Simulators for the Phosphorus Removal Process
  Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms</div>
<div id='2401.12822v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:55:46Z</div><div>Authors: Esmaeel Mohammadi, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen, Per Halkjær Nielsen, Daniel Ortiz-Arroyo, Petar Durdevic</div><div style='padding-top: 10px; width: 80ex'>Phosphorus removal is vital in wastewater treatment to reduce reliance on
limited resources. Deep reinforcement learning (DRL) is a machine learning
technique that can optimize complex and nonlinear systems, including the
processes in wastewater treatment plants, by learning control policies through
trial and error. However, applying DRL to chemical and biological processes is
challenging due to the need for accurate simulators. This study trained six
models to identify the phosphorus removal process and used them to create a
simulator for the DRL environment. Although the models achieved high accuracy
(&gt;97%), uncertainty and incorrect prediction behavior limited their performance
as simulators over longer horizons. Compounding errors in the models'
predictions were identified as one of the causes of this problem. This approach
for improving process control involves creating simulation environments for DRL
algorithms, using data from supervisory control and data acquisition (SCADA)
systems with a sufficient historical horizon without complex system modeling or
parameter estimation.</div><div><a href='http://arxiv.org/abs/2401.12822v1'>2401.12822v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11217v1")'>A Hybrid Approach of Transfer Learning and Physics-Informed Modeling:
  Improving Dissolved Oxygen Concentration Prediction in an Industrial
  Wastewater Treatment Plant</div>
<div id='2401.11217v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T11:53:08Z</div><div>Authors: Ece S. Koksal, Erdal Aydin</div><div style='padding-top: 10px; width: 80ex'>Constructing first principles models is a challenging task for nonlinear and
complex systems such as a wastewater treatment unit. In recent years,
data-driven models are widely used to overcome the complexity. However, they
often suffer from issues such as missing, low quality or noisy data. Transfer
learning is a solution for this issue where knowledge from another task is
transferred to target one to increase the prediction performance. In this work,
the objective is increasing the prediction performance of an industrial
wastewater treatment plant by transferring the knowledge of (i) an open-source
simulation model that captures the underlying physics of the process, albeit
with dissimilarities to the target plant, (ii) another industrial plant
characterized by noisy and limited data but located in the same refinery, and
(iii) the model in (ii) and making the objective function of the training
problem physics informed where the physics information derived from the
open-source model in (ii). The results have shown that test and validation
performance are improved up to 27% and 59%, respectively.</div><div><a href='http://arxiv.org/abs/2401.11217v1'>2401.11217v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04195v1")'>Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for
  Reservoir Operation Decision and Control</div>
<div id='2403.04195v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T03:55:56Z</div><div>Authors: Sadegh Sadeghi Tabas, Vidya Samadi</div><div style='padding-top: 10px; width: 80ex'>Changes in demand, various hydrological inputs, and environmental stressors
are among the issues that water managers and policymakers face on a regular
basis. These concerns have sparked interest in applying different techniques to
determine reservoir operation policy decisions. As the resolution of the
analysis increases, it becomes more difficult to effectively represent a
real-world system using traditional methods such as Dynamic Programming (DP)
and Stochastic Dynamic Programming (SDP) for determining the best reservoir
operation policy. One of the challenges is the "curse of dimensionality," which
means the number of samples needed to estimate an arbitrary function with a
given level of accuracy grows exponentially with respect to the number of input
variables (i.e., dimensionality) of the function. Deep Reinforcement Learning
(DRL) is an intelligent approach to overcome the curses of stochastic
optimization problems for reservoir operation policy decisions. To our
knowledge, this study is the first attempt that examine various novel DRL
continuous-action policy gradient methods (PGMs), including Deep Deterministic
Policy Gradients (DDPG), Twin Delayed DDPG (TD3), and two different versions of
Soft Actor-Critic (SAC18 and SAC19) for optimizing reservoir operation policy.
In this study, multiple DRL techniques were implemented in order to find the
optimal operation policy of Folsom Reservoir in California, USA. The reservoir
system supplies agricultural, municipal, hydropower, and environmental flow
demands and flood control operations to the City of Sacramento. Analysis
suggests that the TD3 and SAC are robust to meet the Folsom Reservoir's demands
and optimize reservoir operation policies.</div><div><a href='http://arxiv.org/abs/2403.04195v1'>2403.04195v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01273v1")'>Learning-based agricultural management in partially observable
  environments subject to climate variability</div>
<div id='2401.01273v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T16:18:53Z</div><div>Authors: Zhaoan Wang, Shaoping Xiao, Junchao Li, Jun Wang</div><div style='padding-top: 10px; width: 80ex'>Agricultural management, with a particular focus on fertilization strategies,
holds a central role in shaping crop yield, economic profitability, and
environmental sustainability. While conventional guidelines offer valuable
insights, their efficacy diminishes when confronted with extreme weather
conditions, such as heatwaves and droughts. In this study, we introduce an
innovative framework that integrates Deep Reinforcement Learning (DRL) with
Recurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we train
an intelligent agent to master optimal nitrogen fertilization management.
Through a series of simulation experiments conducted on corn crops in Iowa, we
compare Partially Observable Markov Decision Process (POMDP) models with Markov
Decision Process (MDP) models. Our research underscores the advantages of
utilizing sequential observations in developing more efficient nitrogen input
policies. Additionally, we explore the impact of climate variability,
particularly during extreme weather events, on agricultural outcomes and
management. Our findings demonstrate the adaptability of fertilization policies
to varying climate conditions. Notably, a fixed policy exhibits resilience in
the face of minor climate fluctuations, leading to commendable corn yields,
cost-effectiveness, and environmental conservation. However, our study
illuminates the need for agent retraining to acquire new optimal policies under
extreme weather events. This research charts a promising course toward
adaptable fertilization strategies that can seamlessly align with dynamic
climate scenarios, ultimately contributing to the optimization of crop
management practices.</div><div><a href='http://arxiv.org/abs/2401.01273v1'>2401.01273v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08832v1")'>Intelligent Agricultural Management Considering N$_2$O Emission and
  Climate Variability with Uncertainties</div>
<div id='2402.08832v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T22:29:40Z</div><div>Authors: Zhaoan Wang, Shaoping Xiao, Jun Wang, Ashwin Parab, Shivam Patel</div><div style='padding-top: 10px; width: 80ex'>This study examines how artificial intelligence (AI), especially
Reinforcement Learning (RL), can be used in farming to boost crop yields,
fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse
gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate
change and limited agricultural knowledge, we use Partially Observable Markov
Decision Processes (POMDPs) with a crop simulator to model AI agents'
interactions with farming environments. We apply deep Q-learning with Recurrent
Neural Network (RNN)-based Q networks for training agents on optimal actions.
Also, we develop Machine Learning (ML) models to predict N$_2$O emissions,
integrating these predictions into the simulator. Our research tackles
uncertainties in N$_2$O emission estimates with a probabilistic ML approach and
climate variability through a stochastic weather model, offering a range of
emission outcomes to improve forecast reliability and decision-making. By
incorporating climate change effects, we enhance agents' climate adaptability,
aiming for resilient agricultural practices. Results show these agents can
align crop productivity with environmental concerns by penalizing N$_2$O
emissions, adapting effectively to climate shifts like warmer temperatures and
less rain. This strategy improves farm management under climate change,
highlighting AI's role in sustainable agriculture.</div><div><a href='http://arxiv.org/abs/2402.08832v1'>2402.08832v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09387v1")'>Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs
  with Neural Differential Equations and Reinforcement Learning</div>
<div id='2402.09387v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:37:40Z</div><div>Authors: Allen M. Wang, Oswin So, Charles Dawson, Darren T. Garnier, Cristina Rea, Chuchu Fan</div><div style='padding-top: 10px; width: 80ex'>The tokamak offers a promising path to fusion energy, but plasma disruptions
pose a major economic risk, motivating considerable advances in disruption
avoidance. This work develops a reinforcement learning approach to this problem
by training a policy to safely ramp-down the plasma current while avoiding
limits on a number of quantities correlated with disruptions. The policy
training environment is a hybrid physics and machine learning model trained on
simulations of the SPARC primary reference discharge (PRD) ramp-down, an
upcoming burning plasma scenario which we use as a testbed. To address physics
uncertainty and model inaccuracies, the simulation environment is massively
parallelized on GPU with randomized physics parameters during policy training.
The trained policy is then successfully transferred to a higher fidelity
simulator where it successfully ramps down the plasma while avoiding
user-specified disruptive limits. We also address the crucial issue of safety
criticality by demonstrating that a constraint-conditioned policy can be used
as a trajectory design assistant to design a library of feed-forward
trajectories to handle different physics conditions and user settings. As a
library of trajectories is more interpretable and verifiable offline, we argue
such an approach is a promising path for leveraging the capabilities of
reinforcement learning in the safety-critical context of burning plasma
tokamaks. Finally, we demonstrate how the training environment can be a useful
platform for other feed-forward optimization approaches by using an
evolutionary algorithm to perform optimization of feed-forward trajectories
that are robust to physics uncertainty</div><div><a href='http://arxiv.org/abs/2402.09387v1'>2402.09387v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11432v1")'>Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle
  Decision-Making</div>
<div id='2403.11432v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T02:59:13Z</div><div>Authors: Hanxi Wan, Pei Li, Arpan Kusari</div><div style='padding-top: 10px; width: 80ex'>With the advent of universal function approximators in the domain of
reinforcement learning, the number of practical applications leveraging deep
reinforcement learning (DRL) has exploded. Decision-making in automated driving
tasks has emerged as a chief application among them, taking the sensor data or
the higher-order kinematic variables as the input and providing a discrete
choice or continuous control output. However, the black-box nature of the
models presents an overwhelming limitation that restricts the real-world
deployment of DRL in autonomous vehicles (AVs). Therefore, in this research
work, we focus on the interpretability of an attention-based DRL framework. We
use a continuous proximal policy optimization-based DRL algorithm as the
baseline model and add a multi-head attention framework in an open-source AV
simulation environment. We provide some analytical techniques for discussing
the interpretability of the trained models in terms of explainability and
causality for spatial and temporal correlations. We show that the weights in
the first head encode the positions of the neighboring vehicles while the
second head focuses on the leader vehicle exclusively. Also, the ego vehicle's
action is causally dependent on the vehicles in the target lane spatially and
temporally. Through these findings, we reliably show that these techniques can
help practitioners decipher the results of the DRL algorithms.</div><div><a href='http://arxiv.org/abs/2403.11432v1'>2403.11432v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03160v3")'>Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning
  for Safe and Efficient Autonomous Driving</div>
<div id='2401.03160v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:30:14Z</div><div>Authors: Zilin Huang, Zihao Sheng, Chengyuan Ma, Sikai Chen</div><div style='padding-top: 10px; width: 80ex'>Despite significant progress in autonomous vehicles (AVs), the development of
driving policies that ensure both the safety of AVs and traffic flow efficiency
has not yet been fully explored. In this paper, we propose an enhanced
human-in-the-loop reinforcement learning method, termed the Human as AI
mentor-based deep reinforcement learning (HAIM-DRL) framework, which
facilitates safe and efficient autonomous driving in mixed traffic platoon.
Drawing inspiration from the human learning process, we first introduce an
innovative learning paradigm that effectively injects human intelligence into
AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves
as a mentor to the AI agent. While allowing the agent to sufficiently explore
uncertain environments, the human expert can take control in dangerous
situations and demonstrate correct actions to avoid potential accidents. On the
other hand, the agent could be guided to minimize traffic flow disturbance,
thereby optimizing traffic flow efficiency. In detail, HAIM-DRL leverages data
collected from free exploration and partial human demonstrations as its two
training sources. Remarkably, we circumvent the intricate process of manually
designing reward functions; instead, we directly derive proxy state-action
values from partial human demonstrations to guide the agents' policy learning.
Additionally, we employ a minimal intervention technique to reduce the human
mentor's cognitive load. Comparative results show that HAIM-DRL outperforms
traditional methods in driving safety, sampling efficiency, mitigation of
traffic flow disturbance, and generalizability to unseen traffic scenarios. The
code and demo videos for this paper can be accessed at:
https://zilin-huang.github.io/HAIM-DRL-website/</div><div><a href='http://arxiv.org/abs/2401.03160v3'>2401.03160v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13729v1")'>Reinforcement Learning for Online Testing of Autonomous Driving Systems:
  a Replication and Extension Study</div>
<div id='2403.13729v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T16:39:17Z</div><div>Authors: Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo, Paolo Tonella</div><div style='padding-top: 10px; width: 80ex'>In a recent study, Reinforcement Learning (RL) used in combination with
many-objective search, has been shown to outperform alternative techniques
(random search and many-objective search) for online testing of Deep Neural
Network-enabled systems. The empirical evaluation of these techniques was
conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a
replication and extension of that empirical study. Our replication shows that
RL does not outperform pure random test generation in a comparison conducted
under the same settings of the original study, but with no confounding factor
coming from the way collisions are measured. Our extension aims at eliminating
some of the possible reasons for the poor performance of RL observed in our
replication: (1) the presence of reward components providing contrasting or
useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)
which requires discretization of an intrinsically continuous state space.
Results show that our new RL agent is able to converge to an effective policy
that outperforms random testing. Results also highlight other possible
improvements, which open to further investigations on how to best leverage RL
for online ADS testing.</div><div><a href='http://arxiv.org/abs/2403.13729v1'>2403.13729v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.14421v1")'>Multi-Agent Based Transfer Learning for Data-Driven Air Traffic
  Applications</div>
<div id='2401.14421v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T22:21:07Z</div><div>Authors: Chuhao Deng, Hong-Cheol Choi, Hyunsang Park, Inseok Hwang</div><div style='padding-top: 10px; width: 80ex'>Research in developing data-driven models for Air Traffic Management (ATM)
has gained a tremendous interest in recent years. However, data-driven models
are known to have long training time and require large datasets to achieve good
performance. To address the two issues, this paper proposes a Multi-Agent
Bidirectional Encoder Representations from Transformers (MA-BERT) model that
fully considers the multi-agent characteristic of the ATM system and learns air
traffic controllers' decisions, and a pre-training and fine-tuning transfer
learning framework. By pre-training the MA-BERT on a large dataset from a major
airport and then fine-tuning it to other airports and specific air traffic
applications, a large amount of the total training time can be saved. In
addition, for newly adopted procedures and constructed airports where no
historical data is available, this paper shows that the pre-trained MA-BERT can
achieve high performance by updating regularly with little data. The proposed
transfer learning framework and MA-BERT are tested with the automatic dependent
surveillance-broadcast data recorded in 3 airports in South Korea in 2019.</div><div><a href='http://arxiv.org/abs/2401.14421v1'>2401.14421v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02882v1")'>Autonomous vehicle decision and control through reinforcement learning
  with traffic flow randomization</div>
<div id='2403.02882v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:41:43Z</div><div>Authors: Yuan Lin, Antai Xie, Xiao Liu</div><div style='padding-top: 10px; width: 80ex'>Most of the current studies on autonomous vehicle decision-making and control
tasks based on reinforcement learning are conducted in simulated environments.
The training and testing of these studies are carried out under rule-based
microscopic traffic flow, with little consideration of migrating them to real
or near-real environments to test their performance. It may lead to a
degradation in performance when the trained model is tested in more realistic
traffic scenes. In this study, we propose a method to randomize the driving
style and behavior of surrounding vehicles by randomizing certain parameters of
the car-following model and the lane-changing model of rule-based microscopic
traffic flow in SUMO. We trained policies with deep reinforcement learning
algorithms under the domain randomized rule-based microscopic traffic flow in
freeway and merging scenes, and then tested them separately in rule-based
microscopic traffic flow and high-fidelity microscopic traffic flow. Results
indicate that the policy trained under domain randomization traffic flow has
significantly better success rate and calculative reward compared to the models
trained under other microscopic traffic flows.</div><div><a href='http://arxiv.org/abs/2403.02882v1'>2403.02882v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03359v2")'>RACE-SM: Reinforcement Learning Based Autonomous Control for Social
  On-Ramp Merging</div>
<div id='2403.03359v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T23:03:56Z</div><div>Authors: Jordan Poots</div><div style='padding-top: 10px; width: 80ex'>Autonomous parallel-style on-ramp merging in human controlled traffic
continues to be an existing issue for autonomous vehicle control. Existing
non-learning based solutions for vehicle control rely on rules and optimization
primarily. These methods have been seen to present significant challenges.
Recent advancements in Deep Reinforcement Learning have shown promise and have
received significant academic interest however the available learning based
approaches show inadequate attention to other highway vehicles and often rely
on inaccurate road traffic assumptions. In addition, the parallel-style case is
rarely considered. A novel learning based model for acceleration and lane
change decision making that explicitly considers the utility to both the ego
vehicle and its surrounding vehicles which may be cooperative or uncooperative
to produce behaviour that is socially acceptable is proposed. The novel reward
function makes use of Social Value Orientation to weight the vehicle's level of
social cooperation and is divided into ego vehicle and surrounding vehicle
utility which are weighted according to the model's designated Social Value
Orientation. A two-lane highway with an on-ramp divided into a taper-style and
parallel-style section is considered. Simulation results indicated the
importance of considering surrounding vehicles in reward function design and
show that the proposed model matches or surpasses those in literature in terms
of collisions while also introducing socially courteous behaviour avoiding near
misses and anti-social behaviour through direct consideration of the effect of
merging on surrounding vehicles.</div><div><a href='http://arxiv.org/abs/2403.03359v2'>2403.03359v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03589v1")'>A Reinforcement Learning Approach for Dynamic Rebalancing in
  Bike-Sharing System</div>
<div id='2402.03589v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T23:46:42Z</div><div>Authors: Jiaqi Liang, Sanjay Dominik Jena, Defeng Liu, Andrea Lodi</div><div style='padding-top: 10px; width: 80ex'>Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the
alleviation of traffic congestion and to healthier lifestyles. Efficiently
operating such systems and maintaining high customer satisfaction is
challenging due to the stochastic nature of trip demand, leading to full or
empty stations. Devising effective rebalancing strategies using vehicles to
redistribute bikes among stations is therefore of uttermost importance for
operators. As a promising alternative to classical mathematical optimization,
reinforcement learning is gaining ground to solve sequential decision-making
problems. This paper introduces a spatio-temporal reinforcement learning
algorithm for the dynamic rebalancing problem with multiple vehicles. We first
formulate the problem as a Multi-agent Markov Decision Process in a continuous
time framework. This allows for independent and cooperative vehicle
rebalancing, eliminating the impractical restriction of time-discretized models
where vehicle departures are synchronized. A comprehensive simulator under the
first-arrive-first-serve rule is then developed to facilitate the learning
process by computing immediate rewards under diverse demand scenarios. To
estimate the value function and learn the rebalancing policy, various Deep
Q-Network configurations are tested, minimizing the lost demand. Experiments
are carried out on various datasets generated from historical data, affected by
both temporal and weather factors. The proposed algorithms outperform
benchmarks, including a multi-period Mixed-Integer Programming model, in terms
of lost demand. Once trained, it yields immediate decisions, making it suitable
for real-time applications. Our work offers practical insights for operators
and enriches the integration of reinforcement learning into dynamic rebalancing
problems, paving the way for more intelligent and robust urban mobility
solutions.</div><div><a href='http://arxiv.org/abs/2402.03589v1'>2402.03589v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00446v1")'>Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for
  Discretionary Lane Change</div>
<div id='2403.00446v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T11:03:17Z</div><div>Authors: Ruichen Xu, Xiao Liu, Jinming Xu, Yuan Lin</div><div style='padding-top: 10px; width: 80ex'>Autonomous lane-change, a key feature of advanced driver-assistance systems,
can enhance traffic efficiency and reduce the incidence of accidents. However,
safe driving of autonomous vehicles remains challenging in complex
environments. How to perform safe and appropriate lane change is a popular
topic of research in the field of autonomous driving. Currently, few papers
consider the safety of reinforcement learning in autonomous lane-change
scenarios. We introduce safe hybrid-action reinforcement learning into
discretionary lane change for the first time and propose Parameterized Soft
Actor-Critic with PID Lagrangian (PASAC-PIDLag) algorithm. Furthermore, we
conduct a comparative analysis of the Parameterized Soft Actor-Critic (PASAC),
which is an unsafe version of PASAC-PIDLag. Both algorithms are employed to
train the lane-change strategy of autonomous vehicles to output discrete
lane-change decision and longitudinal vehicle acceleration. Our simulation
results indicate that at a traffic density of 15 vehicles per kilometer (15
veh/km), the PASAC-PIDLag algorithm exhibits superior safety with a collision
rate of 0%, outperforming the PASAC algorithm, which has a collision rate of
1%. The outcomes of the generalization assessments reveal that at low traffic
density levels, both the PASAC-PIDLag and PASAC algorithms are proficient in
attaining a 0% collision rate. Under conditions of high traffic flow density,
the PASAC-PIDLag algorithm surpasses PASAC in terms of both safety and
optimality.</div><div><a href='http://arxiv.org/abs/2403.00446v1'>2403.00446v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01116v1")'>Scalable Multi-modal Model Predictive Control via Duality-based
  Interaction Predictions</div>
<div id='2402.01116v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T03:19:54Z</div><div>Authors: Hansung Kim, Siddharth H. Nair, Francesco Borrelli</div><div style='padding-top: 10px; width: 80ex'>We propose a hierarchical architecture designed for scalable real-time Model
Predictive Control (MPC) in complex, multi-modal traffic scenarios. This
architecture comprises two key components: 1) RAID-Net, a novel attention-based
Recurrent Neural Network that predicts relevant interactions along the MPC
prediction horizon between the autonomous vehicle and the surrounding vehicles
using Lagrangian duality, and 2) a reduced Stochastic MPC problem that
eliminates irrelevant collision avoidance constraints, enhancing computational
efficiency. Our approach is demonstrated in a simulated traffic intersection
with interactive surrounding vehicles, showcasing a 12x speed-up in solving the
motion planning problem. A video demonstrating the proposed architecture in
multiple complex traffic scenarios can be found here:
https://youtu.be/-TcMeolCLWc</div><div><a href='http://arxiv.org/abs/2402.01116v1'>2402.01116v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08121v1")'>CycLight: learning traffic signal cooperation with a cycle-level
  strategy</div>
<div id='2401.08121v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T05:28:12Z</div><div>Authors: Gengyue Han, Xiaohan Liu, Xianyue Peng, Hao Wang, Yu Han</div><div style='padding-top: 10px; width: 80ex'>This study introduces CycLight, a novel cycle-level deep reinforcement
learning (RL) approach for network-level adaptive traffic signal control
(NATSC) systems. Unlike most traditional RL-based traffic controllers that
focus on step-by-step decision making, CycLight adopts a cycle-level strategy,
optimizing cycle length and splits simultaneously using Parameterized Deep
Q-Networks (PDQN) algorithm. This cycle-level approach effectively reduces the
computational burden associated with frequent data communication, meanwhile
enhancing the practicality and safety of real-world applications. A
decentralized framework is formulated for multi-agent cooperation, while
attention mechanism is integrated to accurately assess the impact of the
surroundings on the current intersection. CycLight is tested in a large
synthetic traffic grid using the microscopic traffic simulation tool, SUMO.
Experimental results not only demonstrate the superiority of CycLight over
other state-of-the-art approaches but also showcase its robustness against
information transmission delays.</div><div><a href='http://arxiv.org/abs/2401.08121v1'>2401.08121v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14886v1")'>Applying Reinforcement Learning to Optimize Traffic Light Cycles</div>
<div id='2402.14886v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T07:37:04Z</div><div>Authors: Seungah Son, Juhee Jin</div><div style='padding-top: 10px; width: 80ex'>Manual optimization of traffic light cycles is a complex and time-consuming
task, necessitating the development of automated solutions. In this paper, we
propose the application of reinforcement learning to optimize traffic light
cycles in real-time. We present a case study using the Simulation Urban
Mobility simulator to train a Deep Q-Network algorithm. The experimental
results showed 44.16% decrease in the average number of Emergency stops,
showing the potential of our approach to reduce traffic congestion and improve
traffic flow. Furthermore, we discuss avenues for future research and
enhancements to the reinforcement learning model.</div><div><a href='http://arxiv.org/abs/2402.14886v1'>2402.14886v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04232v1")'>Generalizing Cooperative Eco-driving via Multi-residual Task Learning</div>
<div id='2403.04232v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T05:25:34Z</div><div>Authors: Vindula Jayawardana, Sirui Li, Cathy Wu, Yashar Farid, Kentaro Oguchi</div><div style='padding-top: 10px; width: 80ex'>Conventional control, such as model-based control, is commonly utilized in
autonomous driving due to its efficiency and reliability. However, real-world
autonomous driving contends with a multitude of diverse traffic scenarios that
are challenging for these planning algorithms. Model-free Deep Reinforcement
Learning (DRL) presents a promising avenue in this direction, but learning DRL
control policies that generalize to multiple traffic scenarios is still a
challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a
generic learning framework based on multi-task learning that, for a set of task
scenarios, decomposes the control into nominal components that are effectively
solved by conventional control methods and residual terms which are solved
using learning. We employ MRTL for fleet-level emission reduction in mixed
traffic using autonomous vehicles as a means of system control. By analyzing
the performance of MRTL across nearly 600 signalized intersections and 1200
traffic scenarios, we demonstrate that it emerges as a promising approach to
synergize the strengths of DRL and conventional methods in generalizable
control.</div><div><a href='http://arxiv.org/abs/2403.04232v1'>2403.04232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02903v1")'>Deep Reinforcement Learning for Local Path Following of an Autonomous
  Formula SAE Vehicle</div>
<div id='2401.02903v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T17:04:43Z</div><div>Authors: Harvey Merton, Thomas Delamore, Karl Stol, Henry Williams</div><div style='padding-top: 10px; width: 80ex'>With the continued introduction of driverless events to Formula:Society of
Automotive Engineers (F:SAE) competitions around the world, teams are
investigating all aspects of the autonomous vehicle stack. This paper presents
the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning
(IRL) to map locally-observed cone positions to a desired steering angle for
race track following. Two state-of-the-art algorithms not previously tested in
this context: soft actor critic (SAC) and adversarial inverse reinforcement
learning (AIRL), are used to train models in a representative simulation. Three
novel reward functions for use by RL algorithms in an autonomous racing context
are also discussed. Tests performed in simulation and the real world suggest
that both algorithms can successfully train models for local path following.
Suggestions for future work are presented to allow these models to scale to a
full F:SAE vehicle.</div><div><a href='http://arxiv.org/abs/2401.02903v1'>2401.02903v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06524v1")'>Tactical Decision Making for Autonomous Trucks by Deep Reinforcement
  Learning with Total Cost of Operation Based Reward</div>
<div id='2403.06524v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T08:58:42Z</div><div>Authors: Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani</div><div style='padding-top: 10px; width: 80ex'>We develop a deep reinforcement learning framework for tactical decision
making in an autonomous truck, specifically for Adaptive Cruise Control (ACC)
and lane change maneuvers in a highway scenario. Our results demonstrate that
it is beneficial to separate high-level decision-making processes and low-level
control actions between the reinforcement learning agent and the low-level
controllers based on physical models. In the following, we study optimizing the
performance with a realistic and multi-objective reward function based on Total
Cost of Operation (TCOP) of the truck using different approaches; by adding
weights to reward components, by normalizing the reward components and by using
curriculum learning techniques.</div><div><a href='http://arxiv.org/abs/2403.06524v1'>2403.06524v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10996v1")'>A Scalable and Parallelizable Digital Twin Framework for Sustainable
  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems</div>
<div id='2403.10996v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T18:47:04Z</div><div>Authors: Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Krovi</div><div style='padding-top: 10px; width: 80ex'>This work presents a sustainable multi-agent deep reinforcement learning
framework capable of selectively scaling parallelized training workloads
on-demand, and transferring the trained policies from simulation to reality
using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an
enabling digital twin framework to train, deploy, and transfer cooperative as
well as competitive multi-agent reinforcement learning policies from simulation
to reality. Particularly, we first investigate an intersection traversal
problem of 4 cooperative vehicles (Nigel) that share limited state information
in single as well as multi-agent learning settings using a common policy
approach. We then investigate an adversarial autonomous racing problem of 2
vehicles (F1TENTH) using an individual policy approach. In either set of
experiments, a decentralized learning architecture was adopted, which allowed
robust training and testing of the policies in stochastic environments. The
agents were provided with realistically sparse observation spaces, and were
restricted to sample control actions that implicitly satisfied the imposed
kinodynamic and safety constraints. The experimental results for both problem
statements are reported in terms of quantitative metrics and qualitative
remarks for training as well as deployment phases. We also discuss agent and
environment parallelization techniques adopted to efficiently accelerate MARL
training, while analyzing their computational performance. Finally, we
demonstrate a resource-aware transition of the trained policies from simulation
to reality using the proposed digital twin framework.</div><div><a href='http://arxiv.org/abs/2403.10996v1'>2403.10996v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17666v1")'>Multi-Agent Deep Reinforcement Learning for Distributed Satellite
  Routing</div>
<div id='2402.17666v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T16:36:53Z</div><div>Authors: Federico Lozano-Cuadra, Beatriz Soret</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL)
approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each
satellite is an independent decision-making agent with a partial knowledge of
the environment, and supported by feedback received from the nearby agents.
Building on our previous work that introduced a Q-routing solution, the
contribution of this paper is to extend it to a deep learning framework able to
quickly adapt to the network and traffic changes, and based on two phases: (1)
An offline exploration learning phase that relies on a global Deep Neural
Network (DNN) to learn the optimal paths at each possible position and
congestion level; (2) An online exploitation phase with local, on-board,
pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes
offline that are then loaded for an efficient distributed routing online.</div><div><a href='http://arxiv.org/abs/2402.17666v1'>2402.17666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02350v1")'>Interference-Aware Emergent Random Access Protocol for Downlink LEO
  Satellite Networks</div>
<div id='2402.02350v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:27:59Z</div><div>Authors: Chang-Yong Lim, Jihong Park, Jinho Choi, Ju-Hyung Lee, Daesub Oh, Heewook Kim</div><div style='padding-top: 10px; width: 80ex'>In this article, we propose a multi-agent deep reinforcement learning (MADRL)
framework to train a multiple access protocol for downlink low earth orbit
(LEO) satellite networks. By improving the existing learned protocol, emergent
random access channel (eRACH), our proposed method, coined centralized and
compressed emergent signaling for eRACH (Ce2RACH), can mitigate inter-satellite
interference by exchanging additional signaling messages jointly learned
through the MADRL training process. Simulations demonstrate that Ce2RACH
achieves up to 36.65% higher network throughput compared to eRACH, while the
cost of signaling messages increase linearly with the number of users.</div><div><a href='http://arxiv.org/abs/2402.02350v1'>2402.02350v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03337v1")'>Reinforcement-learning robotic sailboats: simulator and preliminary
  results</div>
<div id='2402.03337v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T09:04:05Z</div><div>Authors: Eduardo Charles Vasconcellos, Ronald M Sampaio, André P D Araújo, Esteban Walter Gonzales Clua, Philippe Preux, Raphael Guerra, Luiz M G Gonçalves, Luis Martí, Hernan Lira, Nayat Sanchez-Pi</div><div style='padding-top: 10px; width: 80ex'>This work focuses on the main challenges and problems in developing a virtual
oceanic environment reproducing real experiments using Unmanned Surface
Vehicles (USV) digital twins. We introduce the key features for building
virtual worlds, considering using Reinforcement Learning (RL) agents for
autonomous navigation and control. With this in mind, the main problems concern
the definition of the simulation equations (physics and mathematics), their
effective implementation, and how to include strategies for simulated control
and perception (sensors) to be used with RL. We present the modeling,
implementation steps, and challenges required to create a functional digital
twin based on a real robotic sailing vessel. The application is immediate for
developing navigation algorithms based on RL to be applied on real boats.</div><div><a href='http://arxiv.org/abs/2402.03337v1'>2402.03337v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01022v1")'>Autonomous Strike UAVs for Counterterrorism Missions: Challenges and
  Preliminary Solutions</div>
<div id='2403.01022v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T22:52:30Z</div><div>Authors: Meshari Aljohani, Ravi Mukkamalai, Stephen Olariu</div><div style='padding-top: 10px; width: 80ex'>Unmanned Aircraft Vehicles (UAVs) are becoming a crucial tool in modern
warfare, primarily due to their cost-effectiveness, risk reduction, and ability
to perform a wider range of activities. The use of autonomous UAVs to conduct
strike missions against highly valuable targets is the focus of this research.
Due to developments in ledger technology, smart contracts, and machine
learning, such activities formerly carried out by professionals or remotely
flown UAVs are now feasible. Our study provides the first in-depth analysis of
challenges and preliminary solutions for successful implementation of an
autonomous UAV mission. Specifically, we identify challenges that have to be
overcome and propose possible technical solutions for the challenges
identified. We also derive analytical expressions for the success probability
of an autonomous UAV mission, and describe a machine learning model to train
the UAV.</div><div><a href='http://arxiv.org/abs/2403.01022v1'>2403.01022v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11914v1")'>Single-Agent Actor Critic for Decentralized Cooperative Driving</div>
<div id='2403.11914v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:13:02Z</div><div>Authors: Shengchao Yan, Lukas König, Wolfram Burgard</div><div style='padding-top: 10px; width: 80ex'>Active traffic management incorporating autonomous vehicles (AVs) promises a
future with diminished congestion and enhanced traffic flow. However,
developing algorithms for real-world application requires addressing the
challenges posed by continuous traffic flow and partial observability. To
bridge this gap and advance the field of active traffic management towards
greater decentralization, we introduce a novel asymmetric actor-critic model
aimed at learning decentralized cooperative driving policies for autonomous
vehicles using single-agent reinforcement learning. Our approach employs
attention neural networks with masking to handle the dynamic nature of
real-world traffic flow and partial observability. Through extensive
evaluations against baseline controllers across various traffic scenarios, our
model shows great potential for improving traffic flow at diverse bottleneck
locations within the road system. Additionally, we explore the challenge
associated with the conservative driving behaviors of autonomous vehicles that
adhere strictly to traffic regulations. The experiment results illustrate that
our proposed cooperative policy can mitigate potential traffic slowdowns
without compromising safety.</div><div><a href='http://arxiv.org/abs/2403.11914v1'>2403.11914v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01425v1")'>SwapTransformer: highway overtaking tactical planner model via imitation
  learning on OSHA dataset</div>
<div id='2401.01425v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T20:28:06Z</div><div>Authors: Alireza Shamsoshoara, Safin B Salih, Pedram Aghazadeh</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.</div><div><a href='http://arxiv.org/abs/2401.01425v1'>2401.01425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05969v1")'>Spatial-Aware Deep Reinforcement Learning for the Traveling Officer
  Problem</div>
<div id='2401.05969v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T15:16:20Z</div><div>Authors: Niklas Strauß, Matthias Schubert</div><div style='padding-top: 10px; width: 80ex'>The traveling officer problem (TOP) is a challenging stochastic optimization
task. In this problem, a parking officer is guided through a city equipped with
parking sensors to fine as many parking offenders as possible. A major
challenge in TOP is the dynamic nature of parking offenses, which randomly
appear and disappear after some time, regardless of whether they have been
fined. Thus, solutions need to dynamically adjust to currently fineable parking
offenses while also planning ahead to increase the likelihood that the officer
arrives during the offense taking place. Though various solutions exist, these
methods often struggle to take the implications of actions on the ability to
fine future parking violations into account. This paper proposes SATOP, a novel
spatial-aware deep reinforcement learning approach for TOP. Our novel state
encoder creates a representation of each action, leveraging the spatial
relationships between parking spots, the agent, and the action. Furthermore, we
propose a novel message-passing module for learning future inter-action
correlations in the given environment. Thus, the agent can estimate the
potential to fine further parking violations after executing an action. We
evaluate our method using an environment based on real-world data from
Melbourne. Our results show that SATOP consistently outperforms
state-of-the-art TOP agents and is able to fine up to 22% more parking
offenses.</div><div><a href='http://arxiv.org/abs/2401.05969v1'>2401.05969v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10761v1")'>Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement
  Learning</div>
<div id='2403.10761v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T01:51:42Z</div><div>Authors: Jizhe Dou, Haotian Zhang, Guodong Sun</div><div style='padding-top: 10px; width: 80ex'>Recently there has been a growing interest in industry and academia,
regarding the use of wireless chargers to prolong the operational longevity of
unmanned aerial vehicles (commonly knowns as drones). In this paper we consider
a charger-assisted drone application: a drone is deployed to observe a set
points of interest, while a charger can move to recharge the drone's battery.
We focus on the route and charging schedule of the drone and the mobile
charger, to obtain high observation utility with the shortest possible time,
while ensuring the drone remains operational during task execution.
Essentially, this proposed drone-charger scheduling problem is a multi-stage
decision-making process, in which the drone and the mobile charger act as two
agents who cooperate to finish a task. The discrete-continuous hybrid action
space of the two agents poses a significant challenge in our problem. To
address this issue, we present a hybrid-action deep reinforcement learning
framework, called HaDMC, which uses a standard policy learning algorithm to
generate latent continuous actions. Motivated by representation learning, we
specifically design and train an action decoder. It involves two pipelines to
convert the latent continuous actions into original discrete and continuous
actions, by which the drone and the charger can directly interact with
environment. We embed a mutual learning scheme in model training, emphasizing
the collaborative rather than individual actions. We conduct extensive
numerical experiments to evaluate HaDMC and compare it with state-of-the-art
deep reinforcement learning approaches. The experimental results show the
effectiveness and efficiency of our solution.</div><div><a href='http://arxiv.org/abs/2403.10761v1'>2403.10761v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06757v1")'>Synthetic Data Generation Framework, Dataset, and Efficient Deep Model
  for Pedestrian Intention Prediction</div>
<div id='2401.06757v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:44:01Z</div><div>Authors: Muhammad Naveed Riaz, Maciej Wielgosz, Abel Garcia Romera, Antonio M. Lopez</div><div style='padding-top: 10px; width: 80ex'>Pedestrian intention prediction is crucial for autonomous driving. In
particular, knowing if pedestrians are going to cross in front of the
ego-vehicle is core to performing safe and comfortable maneuvers. Creating
accurate and fast models that predict such intentions from sequential images is
challenging. A factor contributing to this is the lack of datasets with diverse
crossing and non-crossing (C/NC) scenarios. We address this scarceness by
introducing a framework, named ARCANE, which allows programmatically generating
synthetic datasets consisting of C/NC video clip samples. As an example, we use
ARCANE to generate a large and diverse dataset named PedSynth. We will show how
PedSynth complements widely used real-world datasets such as JAAD and PIE, so
enabling more accurate models for C/NC prediction. Considering the onboard
deployment of C/NC prediction models, we also propose a deep model named
PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a
GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to
predict crossing intentions.</div><div><a href='http://arxiv.org/abs/2401.06757v1'>2401.06757v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02199v1")'>LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System</div>
<div id='2401.02199v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T11:09:15Z</div><div>Authors: Anil Ranjitbhai Patel, Peter Liggesmeyer</div><div style='padding-top: 10px; width: 80ex'>As the horizon of intelligent transportation expands with the evolution of
Automated Driving Systems (ADS), ensuring paramount safety becomes more
imperative than ever. Traditional risk assessment methodologies, primarily
crafted for human-driven vehicles, grapple to adequately adapt to the
multifaceted, evolving environments of ADS. This paper introduces a framework
for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of
Artificial Neural Networks (ANNs).
  Our proposed solution transcends these limitations, drawing upon ANNs, a
cornerstone of deep learning, to meticulously analyze and categorize risk
dimensions using real-time On-board Sensor (OBS) data. This learning-centric
approach not only elevates the ADS's situational awareness but also enriches
its understanding of immediate operational contexts. By dissecting OBS data,
the system is empowered to pinpoint its current risk profile, thereby enhancing
safety prospects for onboard passengers and the broader traffic ecosystem.
  Through this framework, we chart a direction in risk assessment, bridging the
conventional voids and enhancing the proficiency of ADS. By utilizing ANNs, our
methodology offers a perspective, allowing ADS to adeptly navigate and react to
potential risk factors, ensuring safer and more informed autonomous journeys.</div><div><a href='http://arxiv.org/abs/2401.02199v1'>2401.02199v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12417v1")'>Predicting trucking accidents with truck drivers 'safety climate
  perception across companies: A transfer learning approach</div>
<div id='2402.12417v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:27:53Z</div><div>Authors: Kailai Sun, Tianxiang Lan, Say Hong Kam, Yang Miang Goh, Yueng-Hsiang Huang</div><div style='padding-top: 10px; width: 80ex'>There is a rising interest in using artificial intelligence (AI)-powered
safety analytics to predict accidents in the trucking industry. Companies may
face the practical challenge, however, of not having enough data to develop
good safety analytics models. Although pretrained models may offer a solution
for such companies, existing safety research using transfer learning has mostly
focused on computer vision and natural language processing, rather than
accident analytics. To fill the above gap, we propose a pretrain-then-fine-tune
transfer learning approach to help any company leverage other companies' data
to develop AI models for a more accurate prediction of accident risk. We also
develop SafeNet, a deep neural network algorithm for classification tasks
suitable for accident prediction. Using the safety climate survey data from
seven trucking companies with different data sizes, we show that our proposed
approach results in better model performance compared to training the model
from scratch using only the target company's data. We also show that for the
transfer learning model to be effective, the pretrained model should be
developed with larger datasets from diverse sources. The trucking industry may,
thus, consider pooling safety analytics data from a wide range of companies to
develop pretrained models and share them within the industry for better
knowledge and resource transfer. The above contributions point to the promise
of advanced safety analytics to make the industry safer and more sustainable.</div><div><a href='http://arxiv.org/abs/2402.12417v1'>2402.12417v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16036v1")'>Machine Learning-Based Vehicle Intention Trajectory Recognition and
  Prediction for Autonomous Driving</div>
<div id='2402.16036v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T09:28:20Z</div><div>Authors: Hanyi Yu, Shuning Huo, Mengran Zhu, Yulu Gong, Yafei Xiang</div><div style='padding-top: 10px; width: 80ex'>In recent years, the expansion of internet technology and advancements in
automation have brought significant attention to autonomous driving technology.
Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have
progressively introduced products ranging from assisted-driving vehicles to
semi-autonomous vehicles. However, this period has also witnessed several
traffic safety incidents involving self-driving vehicles. For instance, in
March 2016, a Google self-driving car was involved in a minor collision with a
bus. At the time of the accident, the autonomous vehicle was attempting to
merge into the right lane but failed to dynamically respond to the real-time
environmental information during the lane change. It incorrectly assumed that
the approaching bus would slow down to avoid it, leading to a low-speed
collision with the bus. This incident highlights the current technological
shortcomings and safety concerns associated with autonomous lane-changing
behavior, despite the rapid advancements in autonomous driving technology.
Lane-changing is among the most common and hazardous behaviors in highway
driving, significantly impacting traffic safety and flow. Therefore,
lane-changing is crucial for traffic safety, and accurately predicting drivers'
lane change intentions can markedly enhance driving safety. This paper
introduces a deep learning-based prediction method for autonomous driving lane
change behavior, aiming to facilitate safe lane changes and thereby improve
road safety.</div><div><a href='http://arxiv.org/abs/2402.16036v1'>2402.16036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06993v1")'>Automatic driving lane change safety prediction model based on LSTM</div>
<div id='2403.06993v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T12:34:04Z</div><div>Authors: Wenjian Sun, Linying Pan, Jingyu Xu, Weixiang Wan, Yong Wang</div><div style='padding-top: 10px; width: 80ex'>Autonomous driving technology can improve traffic safety and reduce traffic
accidents. In addition, it improves traffic flow, reduces congestion, saves
energy and increases travel efficiency. In the relatively mature automatic
driving technology, the automatic driving function is divided into several
modules: perception, decision-making, planning and control, and a reasonable
division of labor can improve the stability of the system. Therefore,
autonomous vehicles need to have the ability to predict the trajectory of
surrounding vehicles in order to make reasonable decision planning and safety
measures to improve driving safety. By using deep learning method, a
safety-sensitive deep learning model based on short term memory (LSTM) network
is proposed. This model can alleviate the shortcomings of current automatic
driving trajectory planning, and the output trajectory not only ensures high
accuracy but also improves safety. The cell state simulation algorithm
simulates the trackability of the trajectory generated by this model. The
research results show that compared with the traditional model-based method,
the trajectory prediction method based on LSTM network has obvious advantages
in predicting the trajectory in the long time domain. The intention recognition
module considering interactive information has higher prediction and accuracy,
and the algorithm results show that the trajectory is very smooth based on the
premise of safe prediction and efficient lane change. And autonomous vehicles
can efficiently and safely complete lane changes.</div><div><a href='http://arxiv.org/abs/2403.06993v1'>2403.06993v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02652v1")'>Adaptive Discounting of Training Time Attacks</div>
<div id='2401.02652v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T06:03:14Z</div><div>Authors: Ridhima Bector, Abhay Aradhya, Chai Quek, Zinovi Rabinovich</div><div style='padding-top: 10px; width: 80ex'>Among the most insidious attacks on Reinforcement Learning (RL) solutions are
training-time attacks (TTAs) that create loopholes and backdoors in the learned
behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are
now available, where the attacker forces a specific, target behaviour upon a
training RL agent (victim). However, even state-of-the-art C-TTAs focus on
target behaviours that could be naturally adopted by the victim if not for a
particular feature of the environment dynamics, which C-TTAs exploit. In this
work, we show that a C-TTA is possible even when the target behaviour is
un-adoptable due to both environment dynamics as well as non-optimality with
respect to the victim objective(s). To find efficient attacks in this context,
we develop a specialised flavour of the DDPG algorithm, which we term
gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically
alters the attack policy planning horizon based on the victim's current
behaviour. This improves effort distribution throughout the attack timeline and
reduces the effect of uncertainty the attacker has about the victim. To
demonstrate the features of our method and better relate the results to prior
research, we borrow a 3D grid domain from a state-of-the-art C-TTA for our
experiments. Code is available at "bit.ly/github-rb-gDDPG".</div><div><a href='http://arxiv.org/abs/2401.02652v1'>2401.02652v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07078v1")'>Improving deep learning with prior knowledge and cognitive models: A
  survey on enhancing explainability, adversarial robustness and zero-shot
  learning</div>
<div id='2403.07078v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T18:11:00Z</div><div>Authors: Fuseinin Mumuni, Alhassan Mumuni</div><div style='padding-top: 10px; width: 80ex'>We review current and emerging knowledge-informed and brain-inspired
cognitive systems for realizing adversarial defenses, eXplainable Artificial
Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep
learning models have achieved remarkable performance and demonstrated
capabilities surpassing human experts in many applications. Yet, their
inability to exploit domain knowledge leads to serious performance limitations
in practical applications. In particular, deep learning systems are exposed to
adversarial attacks, which can trick them into making glaringly incorrect
decisions. Moreover, complex data-driven models typically lack interpretability
or explainability, i.e., their decisions cannot be understood by human
subjects. Furthermore, models are usually trained on standard datasets with a
closed-world assumption. Hence, they struggle to generalize to unseen cases
during inference in practical open-world environments, thus, raising the zero-
or few-shot generalization problem. Although many conventional solutions exist,
explicit domain knowledge, brain-inspired neural network and cognitive
architectures offer powerful new dimensions towards alleviating these problems.
Prior knowledge is represented in appropriate forms and incorporated in deep
learning frameworks to improve performance. Brain-inspired cognition methods
use computational models that mimic the human mind to enhance intelligent
behavior in artificial agents and autonomous robots. Ultimately, these models
achieve better explainability, higher adversarial robustness and data-efficient
learning, and can, in turn, provide insights for cognitive science and
neuroscience-that is, to deepen human understanding on how the brain works in
general, and how it handles these problems.</div><div><a href='http://arxiv.org/abs/2403.07078v1'>2403.07078v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07263v1")'>BET: Explaining Deep Reinforcement Learning through The Error-Prone
  Decisions</div>
<div id='2401.07263v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T11:45:05Z</div><div>Authors: Xiao Liu, Jie Zhao, Wubing Chen, Mao Tan, Yongxing Su</div><div style='padding-top: 10px; width: 80ex'>Despite the impressive capabilities of Deep Reinforcement Learning (DRL)
agents in many challenging scenarios, their black-box decision-making process
significantly limits their deployment in safety-sensitive domains. Several
previous self-interpretable works focus on revealing the critical states of the
agent's decision. However, they cannot pinpoint the error-prone states. To
address this issue, we propose a novel self-interpretable structure, named
Backbone Extract Tree (BET), to better explain the agent's behavior by identify
the error-prone states. At a high level, BET hypothesizes that states in which
the agent consistently executes uniform decisions exhibit a reduced propensity
for errors. To effectively model this phenomenon, BET expresses these states
within neighborhoods, each defined by a curated set of representative states.
Therefore, states positioned at a greater distance from these representative
benchmarks are more prone to error. We evaluate BET in various popular RL
environments and show its superiority over existing self-interpretable models
in terms of explanation fidelity. Furthermore, we demonstrate a use case for
providing explanations for the agents in StarCraft II, a sophisticated
multi-agent cooperative game. To the best of our knowledge, we are the first to
explain such a complex scenarios using a fully transparent structure.</div><div><a href='http://arxiv.org/abs/2401.07263v1'>2401.07263v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16742v1")'>Generative AI-based closed-loop fMRI system</div>
<div id='2401.16742v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T04:40:49Z</div><div>Authors: Mikihiro Kasahara, Taiki Oka, Vincent Taschereau-Dumouchel, Mitsuo Kawato, Hiroki Takakura, Aurelio Cortese</div><div style='padding-top: 10px; width: 80ex'>While generative AI is now widespread and useful in society, there are
potential risks of misuse, e.g., unconsciously influencing cognitive processes
or decision-making. Although this causes a security problem in the cognitive
domain, there has been no research about neural and computational mechanisms
counteracting the impact of malicious generative AI in humans. We propose
DecNefGAN, a novel framework that combines a generative adversarial system and
a neural reinforcement model. More specifically, DecNefGAN bridges human and
generative AI in a closed-loop system, with the AI creating stimuli that induce
specific mental states, thus exerting external control over neural activity.
The objective of the human is the opposite, to compete and reach an orthogonal
mental state. This framework can contribute to elucidating how the human brain
responds to and counteracts the potential influence of generative AI.</div><div><a href='http://arxiv.org/abs/2401.16742v1'>2401.16742v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04162v1")'>Noisy Spiking Actor Network for Exploration</div>
<div id='2403.04162v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T02:47:08Z</div><div>Authors: Ding Chen, Peixi Peng, Tiejun Huang, Yonghong Tian</div><div style='padding-top: 10px; width: 80ex'>As a general method for exploration in deep reinforcement learning (RL),
NoisyNet can produce problem-specific exploration strategies. Spiking neural
networks (SNNs), due to their binary firing mechanism, have strong robustness
to noise, making it difficult to realize efficient exploration with local
disturbances. To solve this exploration problem, we propose a noisy spiking
actor network (NoisySAN) that introduces time-correlated noise during charging
and transmission. Moreover, a noise reduction method is proposed to find a
stable policy for the agent. Extensive experimental results demonstrate that
our method outperforms the state-of-the-art performance on a wide range of
continuous control tasks from OpenAI gym.</div><div><a href='http://arxiv.org/abs/2403.04162v1'>2403.04162v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05693v3")'>Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking</div>
<div id='2403.05693v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T22:04:25Z</div><div>Authors: Robert Reed, Hanspeter Schaub, Morteza Lahijanian</div><div style='padding-top: 10px; width: 80ex'>Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL)
has become a rapidly growing research area. However, the construction of
shields and the definition of tasking remains informal, resulting in policies
with no guarantees on safety and ambiguous goals for the RL agent. In this
paper, we first explore the use of formal languages, namely Linear Temporal
Logic (LTL), to formalize spacecraft tasks and safety requirements. We then
define a manner in which to construct a reward function from a co-safe LTL
specification automatically for effective training in SDRL framework. We also
investigate methods for constructing a shield from a safe LTL specification for
spacecraft applications and propose three designs that provide probabilistic
guarantees. We show how these shields interact with different policies and the
flexibility of the reward structure through several experiments.</div><div><a href='http://arxiv.org/abs/2403.05693v3'>2403.05693v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14102v1")'>DouRN: Improving DouZero by Residual Neural Networks</div>
<div id='2403.14102v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T03:25:49Z</div><div>Authors: Yiquan Chen, Yingchao Lyu, Di Zhang</div><div style='padding-top: 10px; width: 80ex'>Deep reinforcement learning has made significant progress in games with
imperfect information, but its performance in the card game Doudizhu (Chinese
Poker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from
conventional games as it involves three players and combines elements of
cooperation and confrontation, resulting in a large state and action space. In
2021, a Doudizhu program called DouZero\cite{zha2021douzero} surpassed previous
models without prior knowledge by utilizing traditional Monte Carlo methods and
multilayer perceptrons. Building on this work, our study incorporates residual
networks into the model, explores different architectural designs, and conducts
multi-role testing. Our findings demonstrate that this model significantly
improves the winning rate within the same training time. Additionally, we
introduce a call scoring system to assist the agent in deciding whether to
become a landlord. With these enhancements, our model consistently outperforms
the existing version of DouZero and even experienced human players.
\footnote{The source code is available at
\url{https://github.com/Yingchaol/Douzero_Resnet.git.}</div><div><a href='http://arxiv.org/abs/2403.14102v1'>2403.14102v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13582v1")'>Mastering the Game of Guandan with Deep Reinforcement Learning and
  Behavior Regulating</div>
<div id='2402.13582v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T07:26:06Z</div><div>Authors: Yifan Yanggong, Hao Pan, Lei Wang</div><div style='padding-top: 10px; width: 80ex'>Games are a simplified model of reality and often serve as a favored platform
for Artificial Intelligence (AI) research. Much of the research is concerned
with game-playing agents and their decision making processes. The game of
Guandan (literally, "throwing eggs") is a challenging game where even
professional human players struggle to make the right decision at times. In
this paper we propose a framework named GuanZero for AI agents to master this
game using Monte-Carlo methods and deep neural networks. The main contribution
of this paper is about regulating agents' behavior through a carefully designed
neural network encoding scheme. We then demonstrate the effectiveness of the
proposed framework by comparing it with state-of-the-art approaches.</div><div><a href='http://arxiv.org/abs/2402.13582v1'>2402.13582v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16852v2")'>Checkmating One, by Using Many: Combining Mixture of Experts with MCTS
  to Improve in Chess</div>
<div id='2401.16852v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T09:55:14Z</div><div>Authors: Felix Helfenstein, Jannis Blüml, Johannes Czech, Kristian Kersting</div><div style='padding-top: 10px; width: 80ex'>This paper presents a new approach that integrates deep learning with
computational chess, using both the Mixture of Experts (MoE) method and
Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized
models, each designed to respond to specific changes in the game's input data.
This results in a framework with sparsely activated models, which provides
significant computational benefits. Our framework combines the MoE method with
MCTS, in order to align it with the strategic phases of chess, thus departing
from the conventional ``one-for-all'' model. Instead, we utilize distinct game
phase definitions to effectively distribute computational tasks across multiple
expert neural networks. Our empirical research shows a substantial improvement
in playing strength, surpassing the traditional single-model framework. This
validates the efficacy of our integrated approach and highlights the potential
of incorporating expert knowledge and strategic principles into neural network
design. The fusion of MoE and MCTS offers a promising avenue for advancing
machine learning architectures.</div><div><a href='http://arxiv.org/abs/2401.16852v2'>2401.16852v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15923v1")'>Predicting Outcomes in Video Games with Long Short Term Memory Networks</div>
<div id='2402.15923v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T22:36:23Z</div><div>Authors: Kittimate Chulajata, Sean Wu, Fabien Scalzo, Eun Sang Cha</div><div style='padding-top: 10px; width: 80ex'>Forecasting winners in E-sports with real-time analytics has the potential to
further engage audiences watching major tournament events. However, making such
real-time predictions is challenging due to unpredictable variables within the
game involving diverse player strategies and decision-making. Our work attempts
to enhance audience engagement within video game tournaments by introducing a
real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)
based approach enables efficient predictions of win-lose outcomes by only using
the health indicator of each player as a time series. As a proof of concept, we
evaluate our model's performance within a classic, two-player arcade game,
Super Street Fighter II Turbo. We also benchmark our method against state of
the art methods for time series forecasting; i.e. Transformer models found in
large language models (LLMs). Finally, we open-source our data set and code in
hopes of furthering work in predictive analysis for arcade games.</div><div><a href='http://arxiv.org/abs/2402.15923v1'>2402.15923v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04572v1")'>Robust Imitation Learning for Automated Game Testing</div>
<div id='2401.04572v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T14:18:25Z</div><div>Authors: Pierluigi Vito Amadori, Timothy Bradley, Ryan Spick, Guy Moss</div><div style='padding-top: 10px; width: 80ex'>Game development is a long process that involves many stages before a product
is ready for the market. Human play testing is among the most time consuming,
as testers are required to repeatedly perform tasks in the search for errors in
the code. Therefore, automated testing is seen as a key technology for the
gaming industry, as it would dramatically improve development costs and
efficiency. Toward this end, we propose EVOLUTE, a novel imitation
learning-based architecture that combines behavioural cloning (BC) with energy
based models (EBMs). EVOLUTE is a two-stream ensemble model that splits the
action space of autonomous agents into continuous and discrete tasks. The EBM
stream handles the continuous tasks, to have a more refined and adaptive
control, while the BC stream handles discrete actions, to ease training. We
evaluate the performance of EVOLUTE in a shooting-and-driving game, where the
agent is required to navigate and continuously identify targets to attack. The
proposed model has higher generalisation capabilities than standard BC
approaches, showing a wider range of behaviours and higher performances. Also,
EVOLUTE is easier to train than a pure end-to-end EBM model, as discrete tasks
can be quite sparse in the dataset and cause model training to explore a much
wider set of possible actions while training.</div><div><a href='http://arxiv.org/abs/2401.04572v1'>2401.04572v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02429v1")'>Towards an Information Theoretic Framework of Context-Based Offline
  Meta-Reinforcement Learning</div>
<div id='2402.02429v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:58:42Z</div><div>Authors: Lanqing Li, Hai Zhang, Xinyu Zhang, Shatong Zhu, Junqiao Zhao, Pheng-Ann Heng</div><div style='padding-top: 10px; width: 80ex'>As a marriage between offline RL and meta-RL, the advent of offline
meta-reinforcement learning (OMRL) has shown great promise in enabling RL
agents to multi-task and quickly adapt while acquiring knowledge safely. Among
which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a
universal policy conditioned on effective task representations. In this work,
by examining several key milestones in the field of COMRL, we propose to
integrate these seemingly independent methodologies into a unified information
theoretic framework. Most importantly, we show that the pre-existing COMRL
algorithms are essentially optimizing the same mutual information objective
between the task variable $\boldsymbol{M}$ and its latent representation
$\boldsymbol{Z}$ by implementing various approximate bounds. Based on the
theoretical insight and the information bottleneck principle, we arrive at a
novel algorithm dubbed UNICORN, which exhibits remarkable generalization across
a broad spectrum of RL benchmarks, context shift scenarios, data qualities and
deep learning architectures, attaining the new state-of-the-art. We believe
that our framework could open up avenues for new optimality bounds and COMRL
algorithms.</div><div><a href='http://arxiv.org/abs/2402.02429v1'>2402.02429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13097v1")'>Simple Ingredients for Offline Reinforcement Learning</div>
<div id='2403.13097v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T18:57:53Z</div><div>Authors: Edoardo Cetin, Andrea Tirinzoni, Matteo Pirotta, Alessandro Lazaric, Yann Ollivier, Ahmed Touati</div><div style='padding-top: 10px; width: 80ex'>Offline reinforcement learning algorithms have proven effective on datasets
highly connected to the target downstream task. Yet, leveraging a novel testbed
(MOOD) in which trajectories come from heterogeneous sources, we show that
existing methods struggle with diverse data: their performance considerably
deteriorates as data collected for related but different tasks is simply added
to the offline buffer. In light of this finding, we conduct a large empirical
study where we formulate and test several hypotheses to explain this failure.
Surprisingly, we find that scale, more than algorithmic considerations, is the
key factor influencing performance. We show that simple methods like AWAC and
IQL with increased network size overcome the paradoxical failure modes from the
inclusion of additional data in MOOD, and notably outperform prior
state-of-the-art algorithms on the canonical D4RL benchmark.</div><div><a href='http://arxiv.org/abs/2403.13097v1'>2403.13097v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13034v2")'>Locality Sensitive Sparse Encoding for Learning World Models Online</div>
<div id='2401.13034v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T19:00:02Z</div><div>Authors: Zichen Liu, Chao Du, Wee Sun Lee, Min Lin</div><div style='padding-top: 10px; width: 80ex'>Acquiring an accurate world model online for model-based reinforcement
learning (MBRL) is challenging due to data nonstationarity, which typically
causes catastrophic forgetting for neural networks (NNs). From the online
learning perspective, a Follow-The-Leader (FTL) world model is desirable, which
optimally fits all previous experiences at each round. Unfortunately, NN-based
models need re-training on all accumulated data at every interaction step to
achieve FTL, which is computationally expensive for lifelong agents. In this
paper, we revisit models that can achieve FTL with incremental updates.
Specifically, our world model is a linear regression model supported by
nonlinear random features. The linear part ensures efficient FTL update while
the nonlinear random feature empowers the fitting of complex environments. To
best trade off model capacity and computation efficiency, we introduce a
locality sensitive sparse encoding, which allows us to conduct efficient sparse
updates even with very high dimensional nonlinear features. We validate the
representation power of our encoding and verify that it allows efficient online
learning under data covariate shift. We also show, in the Dyna MBRL setting,
that our world models learned online using a single pass of trajectory data
either surpass or match the performance of deep world models trained with
replay and other continual learning methods.</div><div><a href='http://arxiv.org/abs/2401.13034v2'>2401.13034v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02355v2")'>Symbol: Generating Flexible Black-Box Optimizers through Symbolic
  Equation Learning</div>
<div id='2402.02355v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:41:27Z</div><div>Authors: Jiacheng Chen, Zeyuan Ma, Hongshu Guo, Yining Ma, Jie Zhang, Yue-Jiao Gong</div><div style='padding-top: 10px; width: 80ex'>Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness
neural networks to meta-learn configurations of traditional black-box
optimizers. Despite their success, they are inevitably restricted by the
limitations of predefined hand-crafted optimizers. In this paper, we present
\textsc{Symbol}, a novel framework that promotes the automated discovery of
black-box optimizers through symbolic equation learning. Specifically, we
propose a Symbolic Equation Generator (SEG) that allows closed-form
optimization rules to be dynamically generated for specific tasks and
optimization steps. Within \textsc{Symbol}, we then develop three distinct
strategies based on reinforcement learning, so as to meta-learn the SEG
efficiently. Extensive experiments reveal that the optimizers generated by
\textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO
baselines, but also exhibit exceptional zero-shot generalization abilities
across entirely unseen tasks with different problem dimensions, population
sizes, and optimization horizons. Furthermore, we conduct in-depth analyses of
our \textsc{Symbol} framework and the optimization rules that it generates,
underscoring its desirable flexibility and interpretability.</div><div><a href='http://arxiv.org/abs/2402.02355v2'>2402.02355v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14048v1")'>PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial
  Optimization</div>
<div id='2402.14048v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:38:14Z</div><div>Authors: André Hottung, Mridul Mahajan, Kevin Tierney</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning-based methods for constructing solutions to
combinatorial optimization problems are rapidly approaching the performance of
human-designed algorithms. To further narrow the gap, learning-based approaches
must efficiently explore the solution space during the search process. Recent
approaches artificially increase exploration by enforcing diverse solution
generation through handcrafted rules, however, these rules can impair solution
quality and are difficult to design for more complex problems. In this paper,
we introduce PolyNet, an approach for improving exploration of the solution
space by learning complementary solution strategies. In contrast to other
works, PolyNet uses only a single-decoder and a training schema that does not
enforce diverse solution generation through handcrafted rules. We evaluate
PolyNet on four combinatorial optimization problems and observe that the
implicit diversity mechanism allows PolyNet to find better solutions than
approaches the explicitly enforce diverse solution generation.</div><div><a href='http://arxiv.org/abs/2402.14048v1'>2402.14048v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15180v1")'>Self-Improvement for Neural Combinatorial Optimization: Sample without
  Replacement, but Improvement</div>
<div id='2403.15180v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T13:09:10Z</div><div>Authors: Jonathan Pirnay, Dominik G. Grimm</div><div style='padding-top: 10px; width: 80ex'>Current methods for end-to-end constructive neural combinatorial optimization
usually train a policy using behavior cloning from expert solutions or policy
gradient methods from reinforcement learning. While behavior cloning is
straightforward, it requires expensive expert solutions, and policy gradient
methods are often computationally demanding and complex to fine-tune. In this
work, we bridge the two and simplify the training process by sampling multiple
solutions for random instances using the current model in each epoch and then
selecting the best solution as an expert trajectory for supervised imitation
learning. To achieve progressively improving solutions with minimal sampling,
we introduce a method that combines round-wise Stochastic Beam Search with an
update strategy derived from a provable policy improvement. This strategy
refines the policy between rounds by utilizing the advantage of the sampled
sequences with almost no computational overhead. We evaluate our approach on
the Traveling Salesman Problem and the Capacitated Vehicle Routing Problem. The
models trained with our method achieve comparable performance and
generalization to those trained with expert data. Additionally, we apply our
method to the Job Shop Scheduling Problem using a transformer-based
architecture and outperform existing state-of-the-art methods by a wide margin.</div><div><a href='http://arxiv.org/abs/2403.15180v1'>2403.15180v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13608v1")'>Convergence Acceleration of Markov Chain Monte Carlo-based Gradient
  Descent by Deep Unfolding</div>
<div id='2402.13608v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T08:21:48Z</div><div>Authors: Ryo Hagiwara, Satoshi Takabe</div><div style='padding-top: 10px; width: 80ex'>This study proposes a trainable sampling-based solver for combinatorial
optimization problems (COPs) using a deep-learning technique called deep
unfolding. The proposed solver is based on the Ohzeki method that combines
Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are
trained by minimizing a loss function. In the training process, we propose a
sampling-based gradient estimation that substitutes auto-differentiation with a
variance estimation, thereby circumventing the failure of back propagation due
to the non-differentiability of MCMC. The numerical results for a few COPs
demonstrated that the proposed solver significantly accelerated the convergence
speed compared with the original Ohzeki method.</div><div><a href='http://arxiv.org/abs/2402.13608v1'>2402.13608v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13380v1")'>Toward TransfORmers: Revolutionizing the Solution of Mixed Integer
  Programs with Transformers</div>
<div id='2402.13380v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T21:13:38Z</div><div>Authors: Joshua F. Cooper, Seung Jin Choi, I. Esra Buyuktahtakin</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce an innovative deep learning framework that
employs a transformer model to address the challenges of mixed-integer
programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP).
Our approach, to our knowledge, is the first to utilize transformers to predict
the binary variables of a mixed-integer programming (MIP) problem.
Specifically, our approach harnesses the encoder decoder transformer's ability
to process sequential data, making it well-suited for predicting binary
variables indicating production setup decisions in each period of the CLSP.
This problem is inherently dynamic, and we need to handle sequential decision
making under constraints. We present an efficient algorithm in which CLSP
solutions are learned through a transformer neural network. The proposed
post-processed transformer algorithm surpasses the state-of-the-art solver,
CPLEX and Long Short-Term Memory (LSTM) in solution time, optimal gap, and
percent infeasibility over 240K benchmark CLSP instances tested. After the ML
model is trained, conducting inference on the model, including post-processing,
reduces the MIP into a linear program (LP). This transforms the ML-based
algorithm, combined with an LP solver, into a polynomial-time approximation
algorithm to solve a well-known NP-Hard problem, with almost perfect solution
quality.</div><div><a href='http://arxiv.org/abs/2402.13380v1'>2402.13380v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14847v1")'>Deep learning-driven scheduling algorithm for a single machine problem
  minimizing the total tardiness</div>
<div id='2402.14847v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:34:09Z</div><div>Authors: Michal Bouška, Přemysl Šůcha, Antonín Novák, Zdeněk Hanzálek</div><div style='padding-top: 10px; width: 80ex'>In this paper, we investigate the use of the deep learning method for solving
a well-known NP-hard single machine scheduling problem with the objective of
minimizing the total tardiness. We propose a deep neural network that acts as a
polynomial-time estimator of the criterion value used in a single-pass
scheduling algorithm based on Lawler's decomposition and symmetric
decomposition proposed by Della Croce et al. Essentially, the neural network
guides the algorithm by estimating the best splitting of the problem into
subproblems. The paper also describes a new method for generating the training
data set, which speeds up the training dataset generation and reduces the
average optimality gap of solutions. The experimental results show that our
machine learning-driven approach can efficiently generalize information from
the training phase to significantly larger instances. Even though the instances
used in the training phase have from 75 to 100 jobs, the average optimality gap
on instances with up to 800 jobs is 0.26%, which is almost five times less than
the gap of the state-of-the-art heuristic.</div><div><a href='http://arxiv.org/abs/2402.14847v1'>2402.14847v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08979v1")'>Learning-enabled Flexible Job-shop Scheduling for Scalable Smart
  Manufacturing</div>
<div id='2402.08979v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T06:49:23Z</div><div>Authors: Sihoon Moon, Sanghoon Lee, Kyung-Joon Park</div><div style='padding-top: 10px; width: 80ex'>In smart manufacturing systems (SMSs), flexible job-shop scheduling with
transportation constraints (FJSPT) is essential to optimize solutions for
maximizing productivity, considering production flexibility based on automated
guided vehicles (AGVs). Recent developments in deep reinforcement learning
(DRL)-based methods for FJSPT have encountered a scale generalization
challenge. These methods underperform when applied to environment at scales
different from their training set, resulting in low-quality solutions. To
address this, we introduce a novel graph-based DRL method, named the
Heterogeneous Graph Scheduler (HGS). Our method leverages locally extracted
relational knowledge among operations, machines, and vehicle nodes for
scheduling, with a graph-structured decision-making framework that reduces
encoding complexity and enhances scale generalization. Our performance
evaluation, conducted with benchmark datasets, reveals that the proposed method
outperforms traditional dispatching rules, meta-heuristics, and existing
DRL-based approaches in terms of makespan performance, even on large-scale
instances that have not been experienced during training.</div><div><a href='http://arxiv.org/abs/2402.08979v1'>2402.08979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00046v1")'>Introducing PetriRL: An Innovative Framework for JSSP Resolution
  Integrating Petri nets and Event-based Reinforcement Learning</div>
<div id='2402.00046v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T12:30:49Z</div><div>Authors: Sofiene Lassoued, Andreas Schwung</div><div style='padding-top: 10px; width: 80ex'>Quality scheduling in industrial job shops is crucial. Although neural
networks excel in solving these problems, their limited explainability hinders
their widespread industrial adoption. In this research, we introduce an
innovative framework for solving job shop scheduling problems (JSSP). Our
methodology leverages Petri nets to model the job shop, not only improving
explainability but also enabling direct incorporation of raw data without the
need to preprocess JSSP instances into disjunctive graphs. The Petri net, with
its controlling capacities, also governs the automated components of the
process, allowing the agent to focus on critical decision-making, particularly
resource allocation. The integration of event-based control and action masking
in our approach yields competitive performance on public test benchmarks.
Comparative analyses across a wide spectrum of optimization solutions,
including heuristics, metaheuristics, and learning-based algorithms, highlight
the competitiveness of our approach in large instances and its superiority over
all competitors in small to medium-sized scenarios. Ultimately, our approach
not only demonstrates a robust ability to generalize across various instance
sizes but also leverages the Petri net's graph nature to dynamically add job
operations during the inference phase without the need for agent retraining,
thereby enhancing flexibility.</div><div><a href='http://arxiv.org/abs/2402.00046v1'>2402.00046v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00318v1")'>Deep Reinforcement Learning for Solving Management Problems: Towards A
  Large Management Mode</div>
<div id='2403.00318v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T06:40:02Z</div><div>Authors: Jinyang Jiang, Xiaotian Liu, Tao Ren, Qinghao Wang, Yi Zheng, Yufu Du, Yijie Peng, Cheng Zhang</div><div style='padding-top: 10px; width: 80ex'>We introduce a deep reinforcement learning (DRL) approach for solving
management problems including inventory management, dynamic pricing, and
recommendation. This DRL approach has the potential to lead to a large
management model based on certain transformer neural network structures,
resulting in an artificial general intelligence paradigm for various management
tasks. Traditional methods have limitations for solving complex real-world
problems, and we demonstrate how DRL can surpass existing heuristic approaches
for solving management tasks. We aim to solve the problems in a unified
framework, considering the interconnections between different tasks. Central to
our methodology is the development of a foundational decision model
coordinating decisions across the different domains through generative
decision-making. Our experimental results affirm the effectiveness of our
DRL-based framework in complex and dynamic business environments. This work
opens new pathways for the application of DRL in management problems,
highlighting its potential to revolutionize traditional business management.</div><div><a href='http://arxiv.org/abs/2403.00318v1'>2403.00318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03525v1")'>Deep Reinforcement Learning for Picker Routing Problem in Warehousing</div>
<div id='2402.03525v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T21:25:45Z</div><div>Authors: George Dunn, Hadi Charkhgard, Ali Eshragh, Sasan Mahmoudinazlou, Elizabeth Stojanovski</div><div style='padding-top: 10px; width: 80ex'>Order Picker Routing is a critical issue in Warehouse Operations Management.
Due to the complexity of the problem and the need for quick solutions,
suboptimal algorithms are frequently employed in practice. However,
Reinforcement Learning offers an appealing alternative to traditional
heuristics, potentially outperforming existing methods in terms of speed and
accuracy. We introduce an attention based neural network for modeling picker
tours, which is trained using Reinforcement Learning. Our method is evaluated
against existing heuristics across a range of problem parameters to demonstrate
its efficacy. A key advantage of our proposed method is its ability to offer an
option to reduce the perceived complexity of routes.</div><div><a href='http://arxiv.org/abs/2402.03525v1'>2402.03525v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07028v1")'>An Efficient Learning-based Solver Comparable to Metaheuristics for the
  Capacitated Arc Routing Problem</div>
<div id='2403.07028v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T02:17:42Z</div><div>Authors: Runze Guo, Feng Xue, Anlong Ming, Nicu Sebe</div><div style='padding-top: 10px; width: 80ex'>Recently, neural networks (NN) have made great strides in combinatorial
optimization. However, they face challenges when solving the capacitated arc
routing problem (CARP) which is to find the minimum-cost tour covering all
required edges on a graph, while within capacity constraints. In tackling CARP,
NN-based approaches tend to lag behind advanced metaheuristics, since they lack
directed arc modeling and efficient learning methods tailored for complex CARP.
In this paper, we introduce an NN-based solver to significantly narrow the gap
with advanced metaheuristics while exhibiting superior efficiency. First, we
propose the direction-aware attention model (DaAM) to incorporate
directionality into the embedding process, facilitating more effective
one-stage decision-making. Second, we design a supervised reinforcement
learning scheme that involves supervised pre-training to establish a robust
initial policy for subsequent reinforcement fine-tuning. It proves particularly
valuable for solving CARP that has a higher complexity than the node routing
problems (NRPs). Finally, a path optimization method is proposed to adjust the
depot return positions within the path generated by DaAM. Experiments
illustrate that our approach surpasses heuristics and achieves decision quality
comparable to state-of-the-art metaheuristics for the first time while
maintaining superior efficiency.</div><div><a href='http://arxiv.org/abs/2403.07028v1'>2403.07028v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00026v1")'>Learning to Deliver: a Foundation Model for the Montreal Capacitated
  Vehicle Routing Problem</div>
<div id='2403.00026v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T16:02:29Z</div><div>Authors: Samuel J. K. Chin, Matthias Winkenbach, Akash Srivastava</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present the Foundation Model for the Montreal Capacitated
Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that
approximates high-quality solutions to a variant of the Capacitated Vehicle
Routing Problem (CVRP) that characterizes many real-world applications. The
so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally
described by Bengio et al. (2021), is defined on a fixed and finite graph,
which is analogous to a city. Each MCVRP instance is essentially the sub-graph
connecting a randomly sampled subset of the nodes in the fixed graph, which
represent a set of potential addresses in a real-world delivery problem on a
given day. Our work exploits this problem structure to frame the MCVRP as an
analogous Natural Language Processing (NLP) task. Specifically, we leverage a
Transformer architecture embedded in a Large Language Model (LLM) framework to
train our model in a supervised manner on computationally inexpensive,
sub-optimal MCVRP solutions obtained algorithmically. Through comprehensive
computational experiments, we show that FM-MCVRP produces better MCVRP
solutions than the training data and generalizes to larger sized problem
instances not seen during training. Even when compared to near-optimal
solutions from state-of-the-art heuristics, FM-MCVRP yields competitive results
despite being trained on inferior data. For instance, for 400-customer
problems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our
results further demonstrate that unlike prior works in the literature, FM-MCVRP
is a unified model, which performs consistently and reliably on a range of
problem instance sizes and parameter values such as the vehicle capacity.</div><div><a href='http://arxiv.org/abs/2403.00026v1'>2403.00026v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16891v1")'>Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot
  Generalization</div>
<div id='2402.16891v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:25:23Z</div><div>Authors: Fei Liu, Xi Lin, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan</div><div style='padding-top: 10px; width: 80ex'>Vehicle routing problems (VRPs), which can be found in numerous real-world
applications, have been an important research topic for several decades.
Recently, the neural combinatorial optimization (NCO) approach that leverages a
learning-based model to solve VRPs without manual algorithm design has gained
substantial attention. However, current NCO methods typically require building
one model for each routing problem, which significantly hinders their practical
application for real-world industry problems with diverse attributes. In this
work, we make the first attempt to tackle the crucial challenge of
cross-problem generalization. In particular, we formulate VRPs as different
combinations of a set of shared underlying attributes and solve them
simultaneously via a single model through attribute composition. In this way,
our proposed model can successfully solve VRPs with unseen attribute
combinations in a zero-shot generalization manner. Extensive experiments are
conducted on eleven VRP variants, benchmark datasets, and industry logistic
scenarios. The results show that the unified model demonstrates superior
performance in the eleven VRPs, reducing the average gap to around 5% from over
20% in the existing approach and achieving a significant performance boost on
benchmark datasets as well as a real-world logistics application.</div><div><a href='http://arxiv.org/abs/2402.16891v1'>2402.16891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04851v1")'>Graph Learning-based Fleet Scheduling for Urban Air Mobility under
  Operational Constraints, Varying Demand &amp; Uncertainties</div>
<div id='2401.04851v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T23:46:22Z</div><div>Authors: Steve Paul, Jhoel Witter, Souma Chowdhury</div><div style='padding-top: 10px; width: 80ex'>This paper develops a graph reinforcement learning approach to online
planning of the schedule and destinations of electric aircraft that comprise an
urban air mobility (UAM) fleet operating across multiple vertiports. This fleet
scheduling problem is formulated to consider time-varying demand, constraints
related to vertiport capacity, aircraft capacity and airspace safety
guidelines, uncertainties related to take-off delay, weather-induced route
closures, and unanticipated aircraft downtime. Collectively, such a formulation
presents greater complexity, and potentially increased realism, than in
existing UAM fleet planning implementations. To address these complexities, a
new policy architecture is constructed, primary components of which include:
graph capsule conv-nets for encoding vertiport and aircraft-fleet states both
abstracted as graphs; transformer layers encoding time series information on
demand and passenger fare; and a Multi-head Attention-based decoder that uses
the encoded information to compute the probability of selecting each available
destination for an aircraft. Trained with Proximal Policy Optimization, this
policy architecture shows significantly better performance in terms of daily
averaged profits on unseen test scenarios involving 8 vertiports and 40
aircraft, when compared to a random baseline and genetic algorithm-derived
optimal solutions, while being nearly 1000 times faster in execution than the
latter.</div><div><a href='http://arxiv.org/abs/2401.04851v1'>2401.04851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17606v1")'>Learning Topological Representations with Bidirectional Graph Attention
  Network for Solving Job Shop Scheduling Problem</div>
<div id='2402.17606v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T15:33:20Z</div><div>Authors: Cong Zhang, Zhiguang Cao, Yaoxin Wu, Wen Song, Jing Sun</div><div style='padding-top: 10px; width: 80ex'>Existing learning-based methods for solving job shop scheduling problem
(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and
neglect the rich and meaningful topological structures of disjunctive graphs
(DGs). This paper proposes the topology-aware bidirectional graph attention
network (TBGAT), a novel GNN architecture based on the attention mechanism, to
embed the DG for solving JSSP in a local search framework. Specifically, TBGAT
embeds the DG from a forward and a backward view, respectively, where the
messages are propagated by following the different topologies of the views and
aggregated via graph attention. Then, we propose a novel operator based on the
message-passing mechanism to calculate the forward and backward topological
sorts of the DG, which are the features for characterizing the topological
structures and exploited by our model. In addition, we theoretically and
experimentally show that TBGAT has linear computational complexity to the
number of jobs and machines, respectively, which strengthens the practical
value of our method. Besides, extensive experiments on five synthetic datasets
and seven classic benchmarks show that TBGAT achieves new SOTA results by
outperforming a wide range of neural methods by a large margin.</div><div><a href='http://arxiv.org/abs/2402.17606v1'>2402.17606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06979v1")'>Distance-aware Attention Reshaping: Enhance Generalization of Neural
  Solver for Large-scale Vehicle Routing Problems</div>
<div id='2401.06979v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T05:01:14Z</div><div>Authors: Yang Wang, Ya-Hui Jia, Wei-Neng Chen, Yi Mei</div><div style='padding-top: 10px; width: 80ex'>Neural solvers based on attention mechanism have demonstrated remarkable
effectiveness in solving vehicle routing problems. However, in the
generalization process from small scale to large scale, we find a phenomenon of
the dispersion of attention scores in existing neural solvers, which leads to
poor performance. To address this issue, this paper proposes a distance-aware
attention reshaping method, assisting neural solvers in solving large-scale
vehicle routing problems. Specifically, without the need for additional
training, we utilize the Euclidean distance information between current nodes
to adjust attention scores. This enables a neural solver trained on small-scale
instances to make rational choices when solving a large-scale problem.
Experimental results show that the proposed method significantly outperforms
existing state-of-the-art neural solvers on the large-scale CVRPLib dataset.</div><div><a href='http://arxiv.org/abs/2401.06979v1'>2401.06979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15872v1")'>A Deep Q-Network Based on Radial Basis Functions for Multi-Echelon
  Inventory Management</div>
<div id='2401.15872v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T04:11:56Z</div><div>Authors: Liqiang Cheng, Jun Luo, Weiwei Fan, Yidong Zhang, Yuan Li</div><div style='padding-top: 10px; width: 80ex'>This paper addresses a multi-echelon inventory management problem with a
complex network topology where deriving optimal ordering decisions is
difficult. Deep reinforcement learning (DRL) has recently shown potential in
solving such problems, while designing the neural networks in DRL remains a
challenge. In order to address this, a DRL model is developed whose Q-network
is based on radial basis functions. The approach can be more easily constructed
compared to classic DRL models based on neural networks, thus alleviating the
computational burden of hyperparameter tuning. Through a series of simulation
experiments, the superior performance of this approach is demonstrated compared
to the simple base-stock policy, producing a better policy in the multi-echelon
system and competitive performance in the serial system where the base-stock
policy is optimal. In addition, the approach outperforms current DRL
approaches.</div><div><a href='http://arxiv.org/abs/2401.15872v1'>2401.15872v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14110v1")'>Heuristic Algorithm-based Action Masking Reinforcement Learning
  (HAAM-RL) with Ensemble Inference Method</div>
<div id='2403.14110v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T03:42:39Z</div><div>Authors: Kyuwon Choi, Cheolkyun Rho, Taeyoun Kim, Daewoo Choi</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel reinforcement learning (RL) approach called
HAAM-RL (Heuristic Algorithm-based Action Masking Reinforcement Learning) for
optimizing the color batching re-sequencing problem in automobile painting
processes. The existing heuristic algorithms have limitations in adequately
reflecting real-world constraints and accurately predicting logistics
performance. Our methodology incorporates several key techniques including a
tailored Markov Decision Process (MDP) formulation, reward setting including
Potential-Based Reward Shaping, action masking using heuristic algorithms
(HAAM-RL), and an ensemble inference method that combines multiple RL models.
The RL agent is trained and evaluated using FlexSim, a commercial 3D simulation
software, integrated with our RL MLOps platform BakingSoDA. Experimental
results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference
method achieves a 16.25% performance improvement over the conventional
heuristic algorithm, with stable and consistent results. The proposed approach
exhibits superior performance and generalization capability, indicating its
effectiveness in optimizing complex manufacturing processes. The study also
discusses future research directions, including alternative state
representations, incorporating model-based RL methods, and integrating
additional real-world constraints.</div><div><a href='http://arxiv.org/abs/2403.14110v1'>2403.14110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.05955v1")'>A Hyper-Transformer model for Controllable Pareto Front Learning with
  Split Feasibility Constraints</div>
<div id='2402.05955v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:21:03Z</div><div>Authors: Tran Anh Tuan, Nguyen Viet Dung, Tran Ngoc Thang</div><div style='padding-top: 10px; width: 80ex'>Controllable Pareto front learning (CPFL) approximates the Pareto solution
set and then locates a Pareto optimal solution with respect to a given
reference vector. However, decision-maker objectives were limited to a
constraint region in practice, so instead of training on the entire decision
space, we only trained on the constraint region. Controllable Pareto front
learning with Split Feasibility Constraints (SFC) is a way to find the best
Pareto solutions to a split multi-objective optimization problem that meets
certain constraints. In the previous study, CPFL used a Hypernetwork model
comprising multi-layer perceptron (Hyper-MLP) blocks. With the substantial
advancement of transformer architecture in deep learning, transformers can
outperform other architectures in various tasks. Therefore, we have developed a
hyper-transformer (Hyper-Trans) model for CPFL with SFC. We use the theory of
universal approximation for the sequence-to-sequence function to show that the
Hyper-Trans model makes MED errors smaller in computational experiments than
the Hyper-MLP model.</div><div><a href='http://arxiv.org/abs/2402.05955v1'>2402.05955v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15482v1")'>Unsupervised Solution Operator Learning for Mean-Field Games via
  Sampling-Invariant Parametrizations</div>
<div id='2401.15482v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T19:07:49Z</div><div>Authors: Han Huang, Rongjie Lai</div><div style='padding-top: 10px; width: 80ex'>Recent advances in deep learning has witnessed many innovative frameworks
that solve high dimensional mean-field games (MFG) accurately and efficiently.
These methods, however, are restricted to solving single-instance MFG and
demands extensive computational time per instance, limiting practicality. To
overcome this, we develop a novel framework to learn the MFG solution operator.
Our model takes a MFG instances as input and output their solutions with one
forward pass. To ensure the proposed parametrization is well-suited for
operator learning, we introduce and prove the notion of sampling invariance for
our model, establishing its convergence to a continuous operator in the
sampling limit. Our method features two key advantages. First, it is
discretization-free, making it particularly suitable for learning operators of
high-dimensional MFGs. Secondly, it can be trained without the need for access
to supervised labels, significantly reducing the computational overhead
associated with creating training datasets in existing operator learning
methods. We test our framework on synthetic and realistic datasets with varying
complexity and dimensionality to substantiate its robustness.</div><div><a href='http://arxiv.org/abs/2401.15482v1'>2401.15482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11040v1")'>Surpassing legacy approaches and human intelligence with hybrid single-
  and multi-objective Reinforcement Learning-based optimization and
  interpretable AI to enable the economic operation of the US nuclear fleet</div>
<div id='2402.11040v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T19:35:58Z</div><div>Authors: Paul Seurin, Koroush Shirvan</div><div style='padding-top: 10px; width: 80ex'>The nuclear sector represents the primary source of carbon-free energy in the
United States. Nevertheless, existing nuclear power plants face the threat of
early shutdowns due to their inability to compete economically against
alternatives such as gas power plants. Optimizing the fuel cycle cost through
the optimization of core loading patterns is one approach to addressing this
lack of competitiveness. However, this optimization task involves multiple
objectives and constraints, resulting in a vast number of candidate solutions
that cannot be explicitly solved. While stochastic optimization (SO)
methodologies are utilized by various nuclear utilities and vendors for fuel
cycle reload design, manual design remains the preferred approach. To advance
the state-of-the-art in core reload patterns, we have developed methods based
on Deep Reinforcement Learning. Previous research has laid the groundwork for
this approach and demonstrated its ability to discover high-quality patterns
within a reasonable timeframe. However, there is a need for comparison against
legacy methods to demonstrate its utility in a single-objective setting. While
RL methods have shown superiority in multi-objective settings, they have not
yet been applied to address the competitiveness issue effectively. In this
paper, we rigorously compare our RL-based approach against the most commonly
used SO-based methods, namely Genetic Algorithm (GA), Simulated Annealing (SA),
and Tabu Search (TS). Subsequently, we introduce a new hybrid paradigm to
devise innovative designs, resulting in economic gains ranging from 2.8 to 3.3
million dollars per year per plant. This development leverages interpretable
AI, enabling improved algorithmic efficiency by making black-box optimizations
interpretable. Future work will focus on scaling this method to address a
broader range of core designs.</div><div><a href='http://arxiv.org/abs/2402.11040v1'>2402.11040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15932v1")'>Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A
  Reinforcement Learning Approach</div>
<div id='2402.15932v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T23:25:35Z</div><div>Authors: Alaa Selim, Yanzhu Ye, Junbo Zhao, Bo Yang</div><div style='padding-top: 10px; width: 80ex'>In the rapidly evolving domain of electrical power systems, the Volt-VAR
optimization (VVO) is increasingly critical, especially with the burgeoning
integration of renewable energy sources. Traditional approaches to
learning-based VVO in expansive and dynamically changing power systems are
often hindered by computational complexities. To address this challenge, our
research presents a novel framework that harnesses the potential of Deep
Reinforcement Learning (DRL), specifically utilizing the Importance Weighted
Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform.
This framework, built upon RLlib-an industry-standard in Reinforcement
Learning-ingeniously capitalizes on the distributed computing capabilities and
advanced hyperparameter tuning offered by RAY. This design significantly
expedites the exploration and exploitation phases in the VVO solution space.
Our empirical results demonstrate that our approach not only surpasses existing
DRL methods in achieving superior reward outcomes but also manifests a
remarkable tenfold reduction in computational requirements. The integration of
our DRL agent with the RAY platform facilitates the creation of RLlib-IMPALA, a
novel framework that efficiently uses RAY's resources to improve system
adaptability and control. RLlib-IMPALA leverages RAY's toolkit to enhance
analytical capabilities and significantly speeds up training to become more
than 10 times faster than other state-of-the-art DRL methods.</div><div><a href='http://arxiv.org/abs/2402.15932v1'>2402.15932v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02653v1")'>A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in
  Smart Grids</div>
<div id='2401.02653v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T06:04:46Z</div><div>Authors: Viorica Rozina Chifu, Tudor Cioara, Cristina Bianca Pop, Horia Rusu, Ionut Anghel</div><div style='padding-top: 10px; width: 80ex'>Economic and policy factors are driving the continuous increase in the
adoption and usage of electrical vehicles (EVs). However, despite being a
cleaner alternative to combustion engine vehicles, EVs have negative impacts on
the lifespan of microgrid equipment and energy balance due to increased power
demand and the timing of their usage. In our view grid management should
leverage on EVs scheduling flexibility to support local network balancing
through active participation in demand response programs. In this paper, we
propose a model-free solution, leveraging Deep Q-Learning to schedule the
charging and discharging activities of EVs within a microgrid to align with a
target energy profile provided by the distribution system operator. We adapted
the Bellman Equation to assess the value of a state based on specific rewards
for EV scheduling actions and used a neural network to estimate Q-values for
available actions and the epsilon-greedy algorithm to balance exploitation and
exploration to meet the target energy profile. The results are promising
showing that the proposed solution can effectively schedule the EVs charging
and discharging actions to align with the target profile with a Person
coefficient of 0.99, handling effective EVs scheduling situations that involve
dynamicity given by the e-mobility features, relying only on data with no
knowledge of EVs and microgrid dynamics.</div><div><a href='http://arxiv.org/abs/2401.02653v1'>2401.02653v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15108v1")'>Multi-agent Deep Reinforcement Learning for Dynamic Pricing by
  Fast-charging Electric Vehicle Hubs in ccompetition</div>
<div id='2401.15108v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T16:51:52Z</div><div>Authors: Diwas Paudel, Tapas K. Das</div><div style='padding-top: 10px; width: 80ex'>Fast-charging hubs for electric vehicles will soon become part of the newly
built infrastructure for transportation electrification across the world. These
hubs are expected to host many DC fast-charging stations and will admit EVs
only for charging. Like the gasoline refueling stations, fast-charging hubs in
a neighborhood will dynamically vary their prices to compete for the same pool
of EV owners. These hubs will interact with the electric power network by
making purchase commitments for a significant part of their power needs in the
day-ahead (DA) electricity market and meeting the difference from the real-time
(RT) market. Hubs may have supplemental battery storage systems (BSS), which
they will use for arbitrage. In this paper, we develop a two-step data-driven
dynamic pricing methodology for hubs in price competition. We first obtain the
DA commitment by solving a stochastic DA commitment model. Thereafter we obtain
the hub pricing strategies by modeling the game as a competitive Markov
decision process (CMDP) and solving it using a multi-agent deep reinforcement
learning (MADRL) approach. We develop a numerical case study for a pricing game
between two charging hubs. We solve the case study with our methodology by
using combinations of two different DRL algorithms, DQN and SAC, and two
different neural networks (NN) architectures, a feed-forward (FF) neural
network, and a multi-head attention (MHA) neural network. We construct a
measure of collusion (index) using the hub profits. A value of zero for this
index indicates no collusion (perfect competition) and a value of one indicates
full collusion (monopolistic behavior). Our results show that the collusion
index varies approximately between 0.14 and 0.45 depending on the combinations
of the algorithms and the architectures chosen by the hubs.</div><div><a href='http://arxiv.org/abs/2401.15108v1'>2401.15108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19110v1")'>Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in
  Energy and Contingency Reserve Markets</div>
<div id='2402.19110v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T12:41:54Z</div><div>Authors: Jinhao Li, Changlong Wang, Yanru Zhang, Hao Wang</div><div style='padding-top: 10px; width: 80ex'>The battery energy storage system (BESS) has immense potential for enhancing
grid reliability and security through its participation in the electricity
market. BESS often seeks various revenue streams by taking part in multiple
markets to unlock its full potential, but effective algorithms for joint-market
participation under price uncertainties are insufficiently explored in the
existing research. To bridge this gap, we develop a novel BESS joint bidding
strategy that utilizes deep reinforcement learning (DRL) to bid in the spot and
contingency frequency control ancillary services (FCAS) markets. Our approach
leverages a transformer-based temporal feature extractor to effectively respond
to price fluctuations in seven markets simultaneously and helps DRL learn the
best BESS bidding strategy in joint-market participation. Additionally, unlike
conventional "black-box" DRL model, our approach is more interpretable and
provides valuable insights into the temporal bidding behavior of BESS in the
dynamic electricity market. We validate our method using realistic market
prices from the Australian National Electricity Market. The results show that
our strategy outperforms benchmarks, including both optimization-based and
other DRL-based strategies, by substantial margins. Our findings further
suggest that effective temporal-aware bidding can significantly increase
profits in the spot and contingency FCAS markets compared to individual market
participation.</div><div><a href='http://arxiv.org/abs/2402.19110v1'>2402.19110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15853v1")'>Attentive Convolutional Deep Reinforcement Learning for Optimizing
  Solar-Storage Systems in Real-Time Electricity Markets</div>
<div id='2401.15853v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T03:04:43Z</div><div>Authors: Jinhao Li, Changlong Wang, Hao Wang</div><div style='padding-top: 10px; width: 80ex'>This paper studies the synergy of solar-battery energy storage system (BESS)
and develops a viable strategy for the BESS to unlock its economic potential by
serving as a backup to reduce solar curtailments while also participating in
the electricity market. We model the real-time bidding of the solar-battery
system as two Markov decision processes for the solar farm and the BESS,
respectively. We develop a novel deep reinforcement learning (DRL) algorithm to
solve the problem by leveraging attention mechanism (AC) and multi-grained
feature convolution to process DRL input for better bidding decisions.
Simulation results demonstrate that our AC-DRL outperforms two
optimization-based and one DRL-based benchmarks by generating 23%, 20%, and 11%
higher revenue, as well as improving curtailment responses. The excess solar
generation can effectively charge the BESS to bid in the market, significantly
reducing solar curtailments by 76% and creating synergy for the solar-battery
system to be more viable.</div><div><a href='http://arxiv.org/abs/2401.15853v1'>2401.15853v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06820v1")'>QCQP-Net: Reliably Learning Feasible Alternating Current Optimal Power
  Flow Solutions Under Constraints</div>
<div id='2401.06820v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T20:17:44Z</div><div>Authors: Sihan Zeng, Youngdae Kim, Yuxuan Ren, Kibaek Kim</div><div style='padding-top: 10px; width: 80ex'>At the heart of power system operations, alternating current optimal power
flow (ACOPF) studies the generation of electric power in the most economical
way under network-wide load requirement, and can be formulated as a highly
structured non-convex quadratically constrained quadratic program (QCQP).
Optimization-based solutions to ACOPF (such as ADMM or interior-point method),
as the classic approach, require large amount of computation and cannot meet
the need to repeatedly solve the problem as load requirement frequently
changes. On the other hand, learning-based methods that directly predict the
ACOPF solution given the load input incur little computational cost but often
generates infeasible solutions (i.e. violate the constraints of ACOPF). In this
work, we combine the best of both worlds -- we propose an innovated framework
for learning ACOPF, where the input load is mapped to the ACOPF solution
through a neural network in a computationally efficient and reliable manner.
Key to our innovation is a specific-purpose "activation function" defined
implicitly by a QCQP and a novel loss, which enforce constraint satisfaction.
We show through numerical simulations that our proposed method achieves
superior feasibility rate and generation cost in situations where the existing
learning-based approaches fail.</div><div><a href='http://arxiv.org/abs/2401.06820v1'>2401.06820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07465v1")'>Power Flow Analysis Using Deep Neural Networks in Three-Phase Unbalanced
  Smart Distribution Grids</div>
<div id='2401.07465v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T04:43:37Z</div><div>Authors: Deepak Tiwari, Mehdi Jabbari Zideh, Veeru Talreja, Vishal Verma, Sarika K. Solanki, Jignesh Solanki</div><div style='padding-top: 10px; width: 80ex'>Most power systems' approaches are currently tending towards stochastic and
probabilistic methods due to the high variability of renewable sources and the
stochastic nature of loads. Conventional power flow (PF) approaches such as
forward-backward sweep (FBS) and Newton-Raphson require a high number of
iterations to solve non-linear PF equations making them computationally very
intensive. PF is the most important study performed by utility, required in all
stages of the power system, especially in operations and planning. This paper
discusses the applications of deep learning (DL) to predict PF solutions for
three-phase unbalanced power distribution grids. Three deep neural networks
(DNNs); Radial Basis Function Network (RBFnet), Multi-Layer Perceptron (MLP),
and Convolutional Neural Network (CNN), are proposed in this paper to predict
PF solutions. The PF problem is formulated as a multi-output regression model
where two or more output values are predicted based on the inputs. The training
and testing data are generated through the OpenDSS-MATLAB COM interface. These
methods are completely data-driven where the training relies on reducing the
mismatch at each node without the need for the knowledge of the system. The
novelty of the proposed methodology is that the models can accurately predict
the PF solutions for the unbalanced distribution grids with mutual coupling and
are robust to different R/X ratios, topology changes as well as generation and
load variability introduced by the integration of distributed energy resources
(DERs) and electric vehicles (EVs). To test the efficacy of the DNN models,
they are applied to IEEE 4-node and 123-node test cases, and the American
Electric Power (AEP) feeder model. The PF results for RBFnet, MLP, and CNN
models are discussed in this paper demonstrating that all three DNN models
provide highly accurate results in predicting PF solutions.</div><div><a href='http://arxiv.org/abs/2401.07465v1'>2401.07465v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00892v2")'>PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase
  Distribution Systems</div>
<div id='2403.00892v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:47:39Z</div><div>Authors: Salah Ghamizi, Jun Cao, Aoxiang Ma, Pedro Rodriguez</div><div style='padding-top: 10px; width: 80ex'>Efficiently solving unbalanced three-phase power flow in distribution grids
is pivotal for grid analysis and simulation. There is a pressing need for
scalable algorithms capable of handling large-scale unbalanced power grids that
can provide accurate and fast solutions. To address this, deep learning
techniques, especially Graph Neural Networks (GNNs), have emerged. However,
existing literature primarily focuses on balanced networks, leaving a critical
gap in supporting unbalanced three-phase power grids. This letter introduces
PowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for
unbalanced three-phase power grids. The proposed approach models each phase
separately in a multigraph representation, effectively capturing the inherent
asymmetry in unbalanced grids. A graph embedding mechanism utilizing message
passing is introduced to capture spatial dependencies within the power system
network. PowerFlowMultiNet outperforms traditional methods and other deep
learning approaches in terms of accuracy and computational speed. Rigorous
testing reveals significantly lower error rates and a notable hundredfold
increase in computational speed for large power networks compared to
model-based methods.</div><div><a href='http://arxiv.org/abs/2403.00892v2'>2403.00892v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02771v3")'>Powerformer: A Section-adaptive Transformer for Power Flow Adjustment</div>
<div id='2401.02771v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T12:01:19Z</div><div>Authors: Kaixuan Chen, Wei Luo, Shunyu Liu, Yaoquan Wei, Yihe Zhou, Yunpeng Qing, Quan Zhang, Jie Song, Mingli Song</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present a novel transformer architecture tailored for
learning robust power system state representations, which strives to optimize
power dispatch for the power flow adjustment across different transmission
sections. Specifically, our proposed approach, named Powerformer, develops a
dedicated section-adaptive attention mechanism, separating itself from the
self-attention used in conventional transformers. This mechanism effectively
integrates power system states with transmission section information, which
facilitates the development of robust state representations. Furthermore, by
considering the graph topology of power system and the electrical attributes of
bus nodes, we introduce two customized strategies to further enhance the
expressiveness: graph neural network propagation and multi-factor attention
mechanism. Extensive evaluations are conducted on three power system scenarios,
including the IEEE 118-bus system, a realistic 300-bus system in China, and a
large-scale European system with 9241 buses, where Powerformer demonstrates its
superior performance over several baseline methods.</div><div><a href='http://arxiv.org/abs/2401.02771v3'>2401.02771v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15363v1")'>Cascading Blackout Severity Prediction with Statistically-Augmented
  Graph Neural Networks</div>
<div id='2403.15363v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:31:21Z</div><div>Authors: Joe Gorka, Tim Hsu, Wenting Li, Yury Maximov, Line Roald</div><div style='padding-top: 10px; width: 80ex'>Higher variability in grid conditions, resulting from growing renewable
penetration and increased incidence of extreme weather events, has increased
the difficulty of screening for scenarios that may lead to catastrophic
cascading failures. Traditional power-flow-based tools for assessing cascading
blackout risk are too slow to properly explore the space of possible failures
and load/generation patterns. We add to the growing literature of faster
graph-neural-network (GNN)-based techniques, developing two novel techniques
for the estimation of blackout magnitude from initial grid conditions. First we
propose several methods for employing an initial classification step to filter
out safe "non blackout" scenarios prior to magnitude estimation. Second, using
insights from the statistical properties of cascading blackouts, we propose a
method for facilitating non-local message passing in our GNN models. We
validate these two approaches on a large simulated dataset, and show the
potential of both to increase blackout size estimation performance.</div><div><a href='http://arxiv.org/abs/2403.15363v1'>2403.15363v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15321v1")'>Localization of Dummy Data Injection Attacks in Power Systems
  Considering Incomplete Topological Information: A Spatio-Temporal Graph
  Wavelet Convolutional Neural Network Approach</div>
<div id='2401.15321v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T06:50:32Z</div><div>Authors: Zhaoyang Qu, Yunchang Dong, Yang Li, Siqi Song, Tao Jiang, Min Li, Qiming Wang, Lei Wang, Xiaoyong Bo, Jiye Zang, Qi Xu</div><div style='padding-top: 10px; width: 80ex'>The emergence of novel the dummy data injection attack (DDIA) poses a severe
threat to the secure and stable operation of power systems. These attacks are
particularly perilous due to the minimal Euclidean spatial separation between
the injected malicious data and legitimate data, rendering their precise
detection challenging using conventional distance-based methods. Furthermore,
existing research predominantly focuses on various machine learning techniques,
often analyzing the temporal data sequences post-attack or relying solely on
Euclidean spatial characteristics. Unfortunately, this approach tends to
overlook the inherent topological correlations within the non-Euclidean spatial
attributes of power grid data, consequently leading to diminished accuracy in
attack localization. To address this issue, this study takes a comprehensive
approach. Initially, it examines the underlying principles of these new DDIAs
on power systems. Here, an intricate mathematical model of the DDIA is
designed, accounting for incomplete topological knowledge and alternating
current (AC) state estimation from an attacker's perspective. Subsequently, by
integrating a priori knowledge of grid topology and considering the temporal
correlations within measurement data and the topology-dependent attributes of
the power grid, this study introduces temporal and spatial attention matrices.
These matrices adaptively capture the spatio-temporal correlations within the
attacks. Leveraging gated stacked causal convolution and graph wavelet sparse
convolution, the study jointly extracts spatio-temporal DDIA features. Finally,
the research proposes a DDIA localization method based on spatio-temporal graph
neural networks. The accuracy and effectiveness of the DDIA model are
rigorously demonstrated through comprehensive analytical cases.</div><div><a href='http://arxiv.org/abs/2401.15321v1'>2401.15321v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05896v1")'>The Role of Deep Learning in Advancing Proactive Cybersecurity Measures
  for Smart Grid Networks: A Survey</div>
<div id='2401.05896v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T13:14:40Z</div><div>Authors: Nima Abdi, Abdullatif Albaseer, Mohamed Abdallah</div><div style='padding-top: 10px; width: 80ex'>As smart grids (SG) increasingly rely on advanced technologies like sensors
and communication systems for efficient energy generation, distribution, and
consumption, they become enticing targets for sophisticated cyberattacks. These
evolving threats demand robust security measures to maintain the stability and
resilience of modern energy systems. While extensive research has been
conducted, a comprehensive exploration of proactive cyber defense strategies
utilizing Deep Learning (DL) in {SG} remains scarce in the literature. This
survey bridges this gap, studying the latest DL techniques for proactive cyber
defense. The survey begins with an overview of related works and our distinct
contributions, followed by an examination of SG infrastructure. Next, we
classify various cyber defense techniques into reactive and proactive
categories. A significant focus is placed on DL-enabled proactive defenses,
where we provide a comprehensive taxonomy of DL approaches, highlighting their
roles and relevance in the proactive security of SG. Subsequently, we analyze
the most significant DL-based methods currently in use. Further, we explore
Moving Target Defense, a proactive defense strategy, and its interactions with
DL methodologies. We then provide an overview of benchmark datasets used in
this domain to substantiate the discourse.{ This is followed by a critical
discussion on their practical implications and broader impact on cybersecurity
in Smart Grids.} The survey finally lists the challenges associated with
deploying DL-based security systems within SG, followed by an outlook on future
developments in this key field.</div><div><a href='http://arxiv.org/abs/2401.05896v1'>2401.05896v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14949v1")'>Enhancing Power Quality Event Classification with AI Transformer Models</div>
<div id='2402.14949v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T20:09:58Z</div><div>Authors: Ahmad Mohammad Saber, Amr Youssef, Davor Svetinovic, Hatem Zeineldin, Deepa Kundur, Ehab El-Saadany</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been a growing interest in utilizing machine learning for
accurate classification of power quality events (PQEs). However, most of these
studies are performed assuming an ideal situation, while in reality, we can
have measurement noise, DC offset, and variations in the voltage signal's
amplitude and frequency. Building on the prior PQE classification works using
deep learning, this paper proposes a deep-learning framework that leverages
attention-enabled Transformers as a tool to accurately classify PQEs under the
aforementioned considerations. The proposed framework can operate directly on
the voltage signals with no need for a separate feature extraction or
calculation phase. Our results show that the proposed framework outperforms
recently proposed learning-based techniques. It can accurately classify PQEs
under the aforementioned conditions with an accuracy varying between
99.81%$-$91.43% depending on the signal-to-noise ratio, DC offsets, and
variations in the signal amplitude and frequency.</div><div><a href='http://arxiv.org/abs/2402.14949v1'>2402.14949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16102v1")'>Flexible Parallel Neural Network Architecture Model for Early Prediction
  of Lithium Battery Life</div>
<div id='2401.16102v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T12:20:17Z</div><div>Authors: Lidang Jiang, Zhuoxiang Li, Changyan Hu, Qingsong Huang, Ge He</div><div style='padding-top: 10px; width: 80ex'>The early prediction of battery life (EPBL) is vital for enhancing the
efficiency and extending the lifespan of lithium batteries. Traditional models
with fixed architectures often encounter underfitting or overfitting issues due
to the diverse data distributions in different EPBL tasks. An interpretable
deep learning model of flexible parallel neural network (FPNN) is proposed,
which includes an InceptionBlock, a 3D convolutional neural network (CNN), a 2D
CNN, and a dual-stream network. The proposed model effectively extracts
electrochemical features from video-like formatted data using the 3D CNN and
achieves advanced multi-scale feature abstraction through the InceptionBlock.
The FPNN can adaptively adjust the number of InceptionBlocks to flexibly handle
tasks of varying complexity in EPBL. The test on the MIT dataset shows that the
FPNN model achieves outstanding predictive accuracy in EPBL tasks, with MAPEs
of 2.47%, 1.29%, 1.08%, and 0.88% when the input cyclic data volumes are 10,
20, 30, and 40, respectively. The interpretability of the FPNN is mainly
reflected in its flexible unit structure and parameter selection: its diverse
branching structure enables the model to capture features at different scales,
thus allowing the machine to learn informative features. The approach presented
herein provides an accurate, adaptable, and comprehensible solution for early
life prediction of lithium batteries, opening new possibilities in the field of
battery health monitoring.</div><div><a href='http://arxiv.org/abs/2401.16102v1'>2401.16102v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02317v2")'>INViT: A Generalizable Routing Problem Solver with Invariant Nested View
  Transformer</div>
<div id='2402.02317v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T02:09:30Z</div><div>Authors: Han Fang, Zhihao Song, Paul Weng, Yutong Ban</div><div style='padding-top: 10px; width: 80ex'>Recently, deep reinforcement learning has shown promising results for
learning fast heuristics to solve routing problems. Meanwhile, most of the
solvers suffer from generalizing to an unseen distribution or distributions
with different scales. To address this issue, we propose a novel architecture,
called Invariant Nested View Transformer (INViT), which is designed to enforce
a nested design together with invariant views inside the encoders to promote
the generalizability of the learned solver. It applies a modified policy
gradient algorithm enhanced with data augmentations. We demonstrate that the
proposed INViT achieves a dominant generalization performance on both TSP and
CVRP problems with various distributions and different problem scales.</div><div><a href='http://arxiv.org/abs/2402.02317v2'>2402.02317v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07917v2")'>A Neural-Evolutionary Algorithm for Autonomous Transit Network Design</div>
<div id='2403.07917v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T15:59:15Z</div><div>Authors: Andrew Holliday, Gregory Dudek</div><div style='padding-top: 10px; width: 80ex'>Planning a public transit network is a challenging optimization problem, but
essential in order to realize the benefits of autonomous buses. We propose a
novel algorithm for planning networks of routes for autonomous buses. We first
train a graph neural net model as a policy for constructing route networks, and
then use the policy as one of several mutation operators in a evolutionary
algorithm. We evaluate this algorithm on a standard set of benchmarks for
transit network design, and find that it outperforms the learned policy alone
by up to 20% and a plain evolutionary algorithm approach by up to 53% on
realistic benchmark instances.</div><div><a href='http://arxiv.org/abs/2403.07917v2'>2403.07917v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03647v1")'>CAMBranch: Contrastive Learning with Augmented MILPs for Branching</div>
<div id='2402.03647v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T02:47:16Z</div><div>Authors: Jiacheng Lin, Meng Xu, Zhihua Xiong, Huangang Wang</div><div style='padding-top: 10px; width: 80ex'>Recent advancements have introduced machine learning frameworks to enhance
the Branch and Bound (B\&amp;B) branching policies for solving Mixed Integer Linear
Programming (MILP). These methods, primarily relying on imitation learning of
Strong Branching, have shown superior performance. However, collecting expert
samples for imitation learning, particularly for Strong Branching, is a
time-consuming endeavor. To address this challenge, we propose
\textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for
\textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs
(AMILPs) by applying variable shifting to limited expert data from their
original MILPs. This approach enables the acquisition of a considerable number
of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for
imitation learning and employs contrastive learning to enhance the model's
ability to capture MILP features, thereby improving the quality of branching
decisions. Experimental results demonstrate that CAMBranch, trained with only
10\% of the complete dataset, exhibits superior performance. Ablation studies
further validate the effectiveness of our method.</div><div><a href='http://arxiv.org/abs/2402.03647v1'>2402.03647v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09570v1")'>Multi-Fidelity Bayesian Optimization With Across-Task Transferable
  Max-Value Entropy Search</div>
<div id='2403.09570v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:00:01Z</div><div>Authors: Yunchuan Zhang, Sangwoo Park, Osvaldo Simeone</div><div style='padding-top: 10px; width: 80ex'>In many applications, ranging from logistics to engineering, a designer is
faced with a sequence of optimization tasks for which the objectives are in the
form of black-box functions that are costly to evaluate. For example, the
designer may need to tune the hyperparameters of neural network models for
different learning tasks over time. Rather than evaluating the objective
function for each candidate solution, the designer may have access to
approximations of the objective functions, for which higher-fidelity
evaluations entail a larger cost. Existing multi-fidelity black-box
optimization strategies select candidate solutions and fidelity levels with the
goal of maximizing the information accrued about the optimal value or solution
for the current task. Assuming that successive optimization tasks are related,
this paper introduces a novel information-theoretic acquisition function that
balances the need to acquire information about the current task with the goal
of collecting information transferable to future tasks. The proposed method
includes shared inter-task latent variables, which are transferred across tasks
by implementing particle-based variational Bayesian updates. Experimental
results across synthetic and real-world examples reveal that the proposed
provident acquisition strategy that caters to future tasks can significantly
improve the optimization efficiency as soon as a sufficient number of tasks is
processed.</div><div><a href='http://arxiv.org/abs/2403.09570v1'>2403.09570v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01275v1")'>Parametric-Task MAP-Elites</div>
<div id='2402.01275v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:04:29Z</div><div>Authors: Timothée Anne, Jean-Baptiste Mouret</div><div style='padding-top: 10px; width: 80ex'>Optimizing a set of functions simultaneously by leveraging their similarity
is called multi-task optimization. Current black-box multi-task algorithms only
solve a finite set of tasks, even when the tasks originate from a continuous
space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel
black-box algorithm to solve continuous multi-task optimization problems. This
algorithm (1) solves a new task at each iteration, effectively covering the
continuous space, and (2) exploits a new variation operator based on local
linear regression. The resulting dataset of solutions makes it possible to
create a function that maps any task parameter to its optimal solution. We show
on two parametric-task toy problems and a more realistic and challenging
robotic problem in simulation that PT-ME outperforms all baselines, including
the deep reinforcement learning algorithm PPO.</div><div><a href='http://arxiv.org/abs/2402.01275v1'>2402.01275v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07652v1")'>Harder Tasks Need More Experts: Dynamic Routing in MoE Models</div>
<div id='2403.07652v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T13:41:15Z</div><div>Authors: Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce a novel dynamic expert selection framework for
Mixture of Experts (MoE) models, aiming to enhance computational efficiency and
model performance by adjusting the number of activated experts based on input
difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,
which activates a predetermined number of experts regardless of the input's
complexity, our method dynamically selects experts based on the confidence
level in expert selection for each input. This allows for a more efficient
utilization of computational resources, activating more experts for complex
tasks requiring advanced reasoning and fewer for simpler tasks. Through
extensive evaluations, our dynamic routing method demonstrates substantial
improvements over conventional Top-2 routing across various benchmarks,
achieving an average improvement of 0.7% with less than 90% activated
parameters. Further analysis shows our model dispatches more experts to tasks
requiring complex reasoning skills, like BBH, confirming its ability to
dynamically allocate computational resources in alignment with the input's
complexity. Our findings also highlight a variation in the number of experts
needed across different layers of the transformer model, offering insights into
the potential for designing heterogeneous MoE frameworks. The code and models
are available at https://github.com/ZhenweiAn/Dynamic_MoE.</div><div><a href='http://arxiv.org/abs/2403.07652v1'>2403.07652v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02777v1")'>A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire
  Navigation</div>
<div id='2403.02777v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:46:54Z</div><div>Authors: Valentina Scarponi, Michel Duprez, Florent Nageotte, Stéphane Cotin</div><div style='padding-top: 10px; width: 80ex'>Purpose: The treatment of cardiovascular diseases requires complex and
challenging navigation of a guidewire and catheter. This often leads to lengthy
interventions during which the patient and clinician are exposed to X-ray
radiation. Deep Reinforcement Learning approaches have shown promise in
learning this task and may be the key to automating catheter navigation during
robotized interventions. Yet, existing training methods show limited
capabilities at generalizing to unseen vascular anatomies, requiring to be
retrained each time the geometry changes. Methods: In this paper, we propose a
zero-shot learning strategy for three-dimensional autonomous endovascular
navigation. Using a very small training set of branching patterns, our
reinforcement learning algorithm is able to learn a control that can then be
applied to unseen vascular anatomies without retraining. Results: We
demonstrate our method on 4 different vascular systems, with an average success
rate of 95% at reaching random targets on these anatomies. Our strategy is also
computationally efficient, allowing the training of our controller to be
performed in only 2 hours. Conclusion: Our training method proved its ability
to navigate unseen geometries with different characteristics, thanks to a
nearly shape-invariant observation space.</div><div><a href='http://arxiv.org/abs/2403.02777v1'>2403.02777v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10820v1")'>Goal-Conditioned Offline Reinforcement Learning via Metric Learning</div>
<div id='2402.10820v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:46:53Z</div><div>Authors: Alfredo Reichlin, Miguel Vasco, Hang Yin, Danica Kragic</div><div style='padding-top: 10px; width: 80ex'>In this work, we address the problem of learning optimal behavior from
sub-optimal datasets in the context of goal-conditioned offline reinforcement
learning. To do so, we propose a novel way of approximating the optimal value
function for goal-conditioned offline RL problems under sparse rewards,
symmetric and deterministic actions. We study a property for representations to
recover optimality and propose a new optimization objective that leads to such
property. We use the learned value function to guide the learning of a policy
in an actor-critic fashion, a method we name MetricRL. Experimentally, we show
how our method consistently outperforms other offline RL baselines in learning
from sub-optimal offline datasets. Moreover, we show the effectiveness of our
method in dealing with high-dimensional observations and in multi-goal tasks.</div><div><a href='http://arxiv.org/abs/2402.10820v1'>2402.10820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04453v1")'>Vlearn: Off-Policy Learning with Efficient State-Value Function
  Estimation</div>
<div id='2403.04453v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T12:45:51Z</div><div>Authors: Fabian Otto, Philipp Becker, Vien Ang Ngo, Gerhard Neumann</div><div style='padding-top: 10px; width: 80ex'>Existing off-policy reinforcement learning algorithms typically necessitate
an explicit state-action-value function representation, which becomes
problematic in high-dimensional action spaces. These algorithms often encounter
challenges where they struggle with the curse of dimensionality, as maintaining
a state-action-value function in such spaces becomes data-inefficient. In this
work, we propose a novel off-policy trust region optimization approach, called
Vlearn, that eliminates the requirement for an explicit state-action-value
function. Instead, we demonstrate how to efficiently leverage just a
state-value function as the critic, thus overcoming several limitations of
existing methods. By doing so, Vlearn addresses the computational challenges
posed by high-dimensional action spaces. Furthermore, Vlearn introduces an
efficient approach to address the challenges associated with pure state-value
function learning in the off-policy setting. This approach not only simplifies
the implementation of off-policy policy gradient algorithms but also leads to
consistent and robust performance across various benchmark tasks. Specifically,
by removing the need for a state-action-value function Vlearn simplifies the
learning process and allows for more efficient exploration and exploitation in
complex environments</div><div><a href='http://arxiv.org/abs/2403.04453v1'>2403.04453v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09290v1")'>Learning Interpretable Policies in Hindsight-Observable POMDPs through
  Partially Supervised Reinforcement Learning</div>
<div id='2402.09290v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T16:23:23Z</div><div>Authors: Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, Yevgeniy Vorobeychik</div><div style='padding-top: 10px; width: 80ex'>Deep reinforcement learning has demonstrated remarkable achievements across
diverse domains such as video games, robotic control, autonomous driving, and
drug discovery. Common methodologies in partially-observable domains largely
lean on end-to-end learning from high-dimensional observations, such as images,
without explicitly reasoning about true state. We suggest an alternative
direction, introducing the Partially Supervised Reinforcement Learning (PSRL)
framework. At the heart of PSRL is the fusion of both supervised and
unsupervised learning. The approach leverages a state estimator to distill
supervised semantic state information from high-dimensional observations which
are often fully observable at training time. This yields more interpretable
policies that compose state predictions with control. In parallel, it captures
an unsupervised latent representation. These two-the semantic state and the
latent state-are then fused and utilized as inputs to a policy network. This
juxtaposition offers practitioners a flexible and dynamic spectrum: from
emphasizing supervised state information to integrating richer, latent
insights. Extensive experimental results indicate that by merging these dual
representations, PSRL offers a potent balance, enhancing model interpretability
while preserving, and often significantly outperforming, the performance
benchmarks set by traditional methods in terms of reward and convergence speed.</div><div><a href='http://arxiv.org/abs/2402.09290v1'>2402.09290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10516v1")'>Episodic Reinforcement Learning with Expanded State-reward Space</div>
<div id='2401.10516v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T06:14:36Z</div><div>Authors: Dayang Liang, Yaru Zhang, Yunlong Liu</div><div style='padding-top: 10px; width: 80ex'>Empowered by deep neural networks, deep reinforcement learning (DRL) has
demonstrated tremendous empirical successes in various domains, including
games, health care, and autonomous driving. Despite these advancements, DRL is
still identified as data-inefficient as effective policies demand vast numbers
of environmental samples. Recently, episodic control (EC)-based model-free DRL
methods enable sample efficiency by recalling past experiences from episodic
memory. However, existing EC-based methods suffer from the limitation of
potential misalignment between the state and reward spaces for neglecting the
utilization of (past) retrieval states with extensive information, which
probably causes inaccurate value estimation and degraded policy performance. To
tackle this issue, we introduce an efficient EC-based DRL framework with
expanded state-reward space, where the expanded states used as the input and
the expanded rewards used in the training both contain historical and current
information. To be specific, we reuse the historical states retrieved by EC as
part of the input states and integrate the retrieved MC-returns into the
immediate reward in each interactive transition. As a result, our method is
able to simultaneously achieve the full utilization of retrieval information
and the better evaluation of state values by a Temporal Difference (TD) loss.
Empirical results on challenging Box2d and Mujoco tasks demonstrate the
superiority of our method over a recent sibling method and common baselines.
Further, we also verify our method's effectiveness in alleviating Q-value
overestimation by additional experiments of Q-value comparison.</div><div><a href='http://arxiv.org/abs/2401.10516v1'>2401.10516v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11760v1")'>Reinforcement Learning as a Parsimonious Alternative to Prediction
  Cascades: A Case Study on Image Segmentation</div>
<div id='2402.11760v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T01:17:52Z</div><div>Authors: Bharat Srikishan, Anika Tabassum, Srikanth Allu, Ramakrishnan Kannan, Nikhil Muralidhar</div><div style='padding-top: 10px; width: 80ex'>Deep learning architectures have achieved state-of-the-art (SOTA) performance
on computer vision tasks such as object detection and image segmentation. This
may be attributed to the use of over-parameterized, monolithic deep learning
architectures executed on large datasets. Although such architectures lead to
increased accuracy, this is usually accompanied by a large increase in
computation and memory requirements during inference. While this is a non-issue
in traditional machine learning pipelines, the recent confluence of machine
learning and fields like the Internet of Things has rendered such large
architectures infeasible for execution in low-resource settings. In such
settings, previous efforts have proposed decision cascades where inputs are
passed through models of increasing complexity until desired performance is
achieved. However, we argue that cascaded prediction leads to increased
computational cost due to wasteful intermediate computations. To address this,
we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a
non-cascading, cost-aware learning pipeline as an alternative to cascaded
architectures. Through experimental evaluation on real-world and standard
datasets, we demonstrate that PaSeR achieves better accuracy while minimizing
computational cost relative to cascaded models. Further, we introduce a new
metric IoU/GigaFlop to evaluate the balance between cost and performance. On
the real-world task of battery material phase segmentation, PaSeR yields a
minimum performance improvement of 174% on the IoU/GigaFlop metric with respect
to baselines. We also demonstrate PaSeR's adaptability to complementary models
trained on a noisy MNIST dataset, where it achieved a minimum performance
improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are
available at https://github.com/scailab/paser .</div><div><a href='http://arxiv.org/abs/2402.11760v1'>2402.11760v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04809v1")'>Investigation of the Impact of Synthetic Training Data in the Industrial
  Application of Terminal Strip Object Detection</div>
<div id='2403.04809v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T18:33:27Z</div><div>Authors: Nico Baumgart, Markus Lange-Hegermann, Mike Mücke</div><div style='padding-top: 10px; width: 80ex'>In industrial manufacturing, numerous tasks of visually inspecting or
detecting specific objects exist that are currently performed manually or by
classical image processing methods. Therefore, introducing recent deep learning
models to industrial environments holds the potential to increase productivity
and enable new applications. However, gathering and labeling sufficient data is
often intractable, complicating the implementation of such projects. Hence,
image synthesis methods are commonly used to generate synthetic training data
from 3D models and annotate them automatically, although it results in a
sim-to-real domain gap. In this paper, we investigate the sim-to-real
generalization performance of standard object detectors on the complex
industrial application of terminal strip object detection. Combining domain
randomization and domain knowledge, we created an image synthesis pipeline for
automatically generating the training data. Moreover, we manually annotated 300
real images of terminal strips for the evaluation. The results show the
cruciality of the objects of interest to have the same scale in either domain.
Nevertheless, under optimized scaling conditions, the sim-to-real performance
difference in mean average precision amounts to 2.69 % for RetinaNet and 0.98 %
for Faster R-CNN, qualifying this approach for industrial requirements.</div><div><a href='http://arxiv.org/abs/2403.04809v1'>2403.04809v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07246v1")'>Towards Generalized Inverse Reinforcement Learning</div>
<div id='2402.07246v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T17:10:31Z</div><div>Authors: Chaosheng Dong, Yijia Wang</div><div style='padding-top: 10px; width: 80ex'>This paper studies generalized inverse reinforcement learning (GIRL) in
Markov decision processes (MDPs), that is, the problem of learning the basic
components of an MDP given observed behavior (policy) that might not be
optimal. These components include not only the reward function and transition
probability matrices, but also the action space and state space that are not
exactly known but are known to belong to given uncertainty sets. We address two
key challenges in GIRL: first, the need to quantify the discrepancy between the
observed policy and the underlying optimal policy; second, the difficulty of
mathematically characterizing the underlying optimal policy when the basic
components of an MDP are unobservable or partially observable. Then, we propose
the mathematical formulation for GIRL and develop a fast heuristic algorithm.
Numerical results on both finite and infinite state problems show the merit of
our formulation and algorithm.</div><div><a href='http://arxiv.org/abs/2402.07246v1'>2402.07246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12508v1")'>On the Stochastic (Variance-Reduced) Proximal Gradient Method for
  Regularized Expected Reward Optimization</div>
<div id='2401.12508v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T06:01:29Z</div><div>Authors: Ling Liang, Haizhao Yang</div><div style='padding-top: 10px; width: 80ex'>We consider a regularized expected reward optimization problem in the
non-oblivious setting that covers many existing problems in reinforcement
learning (RL). In order to solve such an optimization problem, we apply and
analyze the classical stochastic proximal gradient method. In particular, the
method has shown to admit an $O(\epsilon^{-4})$ sample complexity to an
$\epsilon$-stationary point, under standard conditions. Since the variance of
the classical stochastic gradient estimator is typically large which slows down
the convergence, we also apply an efficient stochastic variance-reduce proximal
gradient method with an importance sampling based ProbAbilistic Gradient
Estimator (PAGE). To the best of our knowledge, the application of this method
represents a novel approach in addressing the general regularized reward
optimization problem. Our analysis shows that the sample complexity can be
improved from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$ under additional
conditions. Our results on the stochastic (variance-reduced) proximal gradient
method match the sample complexity of their most competitive counterparts under
similar settings in the RL literature.</div><div><a href='http://arxiv.org/abs/2401.12508v1'>2401.12508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11925v1")'>Global Optimality without Mixing Time Oracles in Average-reward RL via
  Multi-level Actor-Critic</div>
<div id='2403.11925v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:23:47Z</div><div>Authors: Bhrij Patel, Wesley A. Suttle, Alec Koppel, Vaneet Aggarwal, Brian M. Sadler, Amrit Singh Bedi, Dinesh Manocha</div><div style='padding-top: 10px; width: 80ex'>In the context of average-reward reinforcement learning, the requirement for
oracle knowledge of the mixing time, a measure of the duration a Markov chain
under a fixed policy needs to achieve its stationary distribution-poses a
significant challenge for the global convergence of policy gradient methods.
This requirement is particularly problematic due to the difficulty and expense
of estimating mixing time in environments with large state spaces, leading to
the necessity of impractically long trajectories for effective gradient
estimation in practical applications. To address this limitation, we consider
the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level
Monte Carlo (MLMC) gradient estimator. With our approach, we effectively
alleviate the dependency on mixing time knowledge, a first for average-reward
MDPs global convergence. Furthermore, our approach exhibits the
tightest-available dependence of $\mathcal{O}\left( \sqrt{\tau_{mix}} \right)$
relative to prior work. With a 2D gridworld goal-reaching navigation
experiment, we demonstrate that MAC achieves higher reward than a previous
PG-based method for average reward, Parameterized Policy Gradient with
Advantage Estimation (PPGAE), especially in cases with relatively small
training sample budget restricting trajectory length.</div><div><a href='http://arxiv.org/abs/2403.11925v1'>2403.11925v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04154v1")'>Stabilizing Policy Gradients for Stochastic Differential Equations via
  Consistency with Perturbation Process</div>
<div id='2403.04154v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T02:24:45Z</div><div>Authors: Xiangxin Zhou, Liang Wang, Yichi Zhou</div><div style='padding-top: 10px; width: 80ex'>Considering generating samples with high rewards, we focus on optimizing deep
neural networks parameterized stochastic differential equations (SDEs), the
advanced generative models with high expressiveness, with policy gradient, the
leading algorithm in reinforcement learning. Nevertheless, when applying policy
gradients to SDEs, since the policy gradient is estimated on a finite set of
trajectories, it can be ill-defined, and the policy behavior in data-scarce
regions may be uncontrolled. This challenge compromises the stability of policy
gradients and negatively impacts sample complexity. To address these issues, we
propose constraining the SDE to be consistent with its associated perturbation
process. Since the perturbation process covers the entire space and is easy to
sample, we can mitigate the aforementioned problems. Our framework offers a
general approach allowing for a versatile selection of policy gradient methods
to effectively and efficiently train SDEs. We evaluate our algorithm on the
task of structure-based drug design and optimize the binding affinity of
generated ligand molecules. Our method achieves the best Vina score -9.07 on
the CrossDocked2020 dataset.</div><div><a href='http://arxiv.org/abs/2403.04154v1'>2403.04154v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.15301v1")'>Planning with a Learned Policy Basis to Optimally Solve Complex Tasks</div>
<div id='2403.15301v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T15:51:39Z</div><div>Authors: Guillermo Infante, David Kuric, Anders Jonsson, Vicenç Gómez, Herke van Hoof</div><div style='padding-top: 10px; width: 80ex'>Conventional reinforcement learning (RL) methods can successfully solve a
wide range of sequential decision problems. However, learning policies that can
generalize predictably across multiple tasks in a setting with non-Markovian
reward specifications is a challenging problem. We propose to use successor
features to learn a policy basis so that each (sub)policy in it solves a
well-defined subproblem. In a task described by a finite state automaton (FSA)
that involves the same set of subproblems, the combination of these
(sub)policies can then be used to generate an optimal solution without
additional learning. In contrast to other methods that combine (sub)policies
via planning, our method asymptotically attains global optimality, even in
stochastic environments.</div><div><a href='http://arxiv.org/abs/2403.15301v1'>2403.15301v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03197v2")'>Decision Making in Non-Stationary Environments with Policy-Augmented
  Search</div>
<div id='2401.03197v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T11:51:50Z</div><div>Authors: Ava Pettet, Yunuo Zhang, Baiting Luo, Kyle Wray, Hendrik Baier, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay</div><div style='padding-top: 10px; width: 80ex'>Sequential decision-making under uncertainty is present in many important
problems. Two popular approaches for tackling such problems are reinforcement
learning and online search (e.g., Monte Carlo tree search). While the former
learns a policy by interacting with the environment (typically done before
execution), the latter uses a generative model of the environment to sample
promising action trajectories at decision time. Decision-making is particularly
challenging in non-stationary environments, where the environment in which an
agent operates can change over time. Both approaches have shortcomings in such
settings -- on the one hand, policies learned before execution become stale
when the environment changes and relearning takes both time and computational
effort. Online search, on the other hand, can return sub-optimal actions when
there are limitations on allowed runtime. In this paper, we introduce
\textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines
action-value estimates from an out-of-date policy with an online search using
an up-to-date model of the environment. We prove theoretical results showing
conditions under which PA-MCTS selects the one-step optimal action and also
bound the error accrued while following PA-MCTS as a policy. We compare and
contrast our approach with AlphaZero, another hybrid planning approach, and
Deep Q Learning on several OpenAI Gym environments. Through extensive
experiments, we show that under non-stationary settings with limited time
constraints, PA-MCTS outperforms these baselines.</div><div><a href='http://arxiv.org/abs/2401.03197v2'>2401.03197v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15392v1")'>Offline Inverse RL: New Solution Concepts and Provably Efficient
  Algorithms</div>
<div id='2402.15392v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:49:46Z</div><div>Authors: Filippo Lazzati, Mirco Mutti, Alberto Maria Metelli</div><div style='padding-top: 10px; width: 80ex'>Inverse reinforcement learning (IRL) aims to recover the reward function of
an expert agent from demonstrations of behavior. It is well known that the IRL
problem is fundamentally ill-posed, i.e., many reward functions can explain the
demonstrations. For this reason, IRL has been recently reframed in terms of
estimating the feasible reward set, thus, postponing the selection of a single
reward. However, so far, the available formulations and algorithmic solutions
have been proposed and analyzed mainly for the online setting, where the
learner can interact with the environment and query the expert at will. This is
clearly unrealistic in most practical applications, where the availability of
an offline dataset is a much more common scenario. In this paper, we introduce
a novel notion of feasible reward set capturing the opportunities and
limitations of the offline setting and we analyze the complexity of its
estimation. This requires the introduction an original learning framework that
copes with the intrinsic difficulty of the setting, for which the data coverage
is not under control. Then, we propose two computationally and statistically
efficient algorithms, IRLO and PIRLO, for addressing the problem. In
particular, the latter adopts a specific form of pessimism to enforce the novel
desirable property of inclusion monotonicity of the delivered feasible set.
With this work, we aim to provide a panorama of the challenges of the offline
IRL problem and how they can be fruitfully addressed.</div><div><a href='http://arxiv.org/abs/2402.15392v1'>2402.15392v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03282v1")'>A Framework for Partially Observed Reward-States in RLHF</div>
<div id='2402.03282v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:38:55Z</div><div>Authors: Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, Ambuj Tewari</div><div style='padding-top: 10px; width: 80ex'>The study of reinforcement learning from human feedback (RLHF) has gained
prominence in recent years due to its role in the development of LLMs.
Neuroscience research shows that human responses to stimuli are known to depend
on partially-observed "internal states." Unfortunately current models of RLHF
do not take take this into consideration. Moreover most RLHF models do not
account for intermediate feedback, which is gaining importance in empirical
work and can help improve both sample complexity and alignment. To address
these limitations, we model RLHF as reinforcement learning with partially
observed reward-states (PORRL). We show reductions from the the two dominant
forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For
cardinal feedback, we develop generic statistically efficient algorithms and
instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we
show that a naive reduction to cardinal feedback fails to achieve sublinear
dueling regret. We then present the first explicit reduction that converts
guarantees for cardinal regret to dueling regret. We show that our models and
guarantees in both settings generalize and extend existing ones. Finally, we
identify a recursive structure on our model that could improve the statistical
and computational tractability of PORRL, giving examples from past work on RLHF
as well as learning perfect reward machines, which PORRL subsumes.</div><div><a href='http://arxiv.org/abs/2402.03282v1'>2402.03282v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17747v2")'>When Your AIs Deceive You: Challenges with Partial Observability of
  Human Evaluators in Reward Learning</div>
<div id='2402.17747v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:32:11Z</div><div>Authors: Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, Scott Emmons</div><div style='padding-top: 10px; width: 80ex'>Past analyses of reinforcement learning from human feedback (RLHF) assume
that the human fully observes the environment. What happens when human feedback
is based only on partial observations? We formally define two failure cases:
deception and overjustification. Modeling the human as Boltzmann-rational
w.r.t. a belief over trajectories, we prove conditions under which RLHF is
guaranteed to result in policies that deceptively inflate their performance,
overjustify their behavior to make an impression, or both. To help address
these issues, we mathematically characterize how partial observability of the
environment translates into (lack of) ambiguity in the learned return function.
In some cases, accounting for partial observability makes it theoretically
possible to recover the return function and thus the optimal policy, while in
other cases, there is irreducible ambiguity. We caution against blindly
applying RLHF in partially observable settings and propose research directions
to help tackle these challenges.</div><div><a href='http://arxiv.org/abs/2402.17747v2'>2402.17747v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05015v2")'>An Information Theoretic Approach to Interaction-Grounded Learning</div>
<div id='2401.05015v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T09:03:52Z</div><div>Authors: Xiaoyan Hu, Farzan Farnia, Ho-fung Leung</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) problems where the learner attempts to infer an
unobserved reward from some feedback variables have been studied in several
recent papers. The setting of Interaction-Grounded Learning (IGL) is an example
of such feedback-based RL tasks where the learner optimizes the return by
inferring latent binary rewards from the interaction with the environment. In
the IGL setting, a relevant assumption used in the RL literature is that the
feedback variable $Y$ is conditionally independent of the context-action
$(X,A)$ given the latent reward $R$. In this work, we propose Variational
Information-based IGL (VI-IGL) as an information-theoretic method to enforce
the conditional independence assumption in the IGL-based RL problem. The VI-IGL
framework learns a reward decoder using an information-based objective based on
the conditional mutual information (MI) between $(X,A)$ and $Y$. To estimate
and optimize the information-based terms for the continuous random variables in
the RL problem, VI-IGL leverages the variational representation of mutual
information to obtain a min-max optimization problem. Also, we extend the
VI-IGL framework to general $f$-Information measures leading to the generalized
$f$-VI-IGL framework for the IGL-based RL problems. We present numerical
results on several reinforcement learning settings indicating an improved
performance compared to the existing IGL-based RL algorithm.</div><div><a href='http://arxiv.org/abs/2401.05015v2'>2401.05015v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14811v1")'>On the Limitations of Markovian Rewards to Express Multi-Objective,
  Risk-Sensitive, and Modal Tasks</div>
<div id='2401.14811v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T12:18:29Z</div><div>Authors: Joar Skalse, Alessandro Abate</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the expressivity of scalar, Markovian reward
functions in Reinforcement Learning (RL), and identify several limitations to
what they can express. Specifically, we look at three classes of RL tasks;
multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive
necessary and sufficient conditions that describe when a problem in this class
can be expressed using a scalar, Markovian reward. Moreover, we find that
scalar, Markovian rewards are unable to express most of the instances in each
of these three classes. We thereby contribute to a more complete understanding
of what standard reward functions can and cannot express. In addition to this,
we also call attention to modal problems as a new class of problems, since they
have so far not been given any systematic treatment in the RL literature. We
also briefly outline some approaches for solving some of the problems we
discuss, by means of bespoke RL algorithms.</div><div><a href='http://arxiv.org/abs/2401.14811v1'>2401.14811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15703v1")'>Is Offline Decision Making Possible with Only Few Samples? Reliable
  Decisions in Data-Starved Bandits via Trust Region Enhancement</div>
<div id='2402.15703v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T03:41:09Z</div><div>Authors: Ruiqi Zhang, Yuexiang Zhai, Andrea Zanette</div><div style='padding-top: 10px; width: 80ex'>What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from
a dataset that contains just a single sample for each arm? Surprisingly, in
this work, we demonstrate that even in such a data-starved setting it may still
be possible to find a policy competitive with the optimal one. This paves the
way to reliable decision-making in settings where critical decisions must be
made by relying only on a handful of samples.
  Our analysis reveals that \emph{stochastic policies can be substantially
better} than deterministic ones for offline decision-making. Focusing on
offline multi-armed bandits, we design an algorithm called Trust Region of
Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different
from the predominant value-based lower confidence bound approach. Its design is
enabled by localization laws, critical radii, and relative pessimism. We prove
that its sample complexity is comparable to that of LCB on minimax problems
while being substantially lower on problems with very few samples.
  Finally, we consider an application to offline reinforcement learning in the
special case where the logging policies are known.</div><div><a href='http://arxiv.org/abs/2402.15703v1'>2402.15703v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07391v1")'>Replicability is Asymptotically Free in Multi-armed Bandits</div>
<div id='2402.07391v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T03:31:34Z</div><div>Authors: Junpei Komiyama, Shinji Ito, Yuichi Yoshida, Souta Koshino</div><div style='padding-top: 10px; width: 80ex'>This work is motivated by the growing demand for reproducible machine
learning. We study the stochastic multi-armed bandit problem. In particular, we
consider a replicable algorithm that ensures, with high probability, that the
algorithm's sequence of actions is not affected by the randomness inherent in
the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times
more regret than nonreplicable algorithms, where $\rho$ is the level of
nonreplication. However, we demonstrate that this additional cost is
unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$,
provided that the magnitude of the confidence bounds is chosen carefully. We
introduce an explore-then-commit algorithm that draws arms uniformly before
committing to a single arm. Additionally, we examine a successive elimination
algorithm that eliminates suboptimal arms at the end of each phase. To ensure
the replicability of these algorithms, we incorporate randomness into their
decision-making processes. We extend the use of successive elimination to the
linear bandit problem as well. For the analysis of these algorithms, we propose
a principled approach to limiting the probability of nonreplication. This
approach elucidates the steps that existing research has implicitly followed.
Furthermore, we derive the first lower bound for the two-armed replicable
bandit problem, which implies the optimality of the proposed algorithms up to a
$\log\log T$ factor for the two-armed case.</div><div><a href='http://arxiv.org/abs/2402.07391v1'>2402.07391v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14664v1")'>Bayesian Off-Policy Evaluation and Learning for Large Action Spaces</div>
<div id='2402.14664v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:09:45Z</div><div>Authors: Imad Aouali, Victor-Emmanuel Brunel, David Rohde, Anna Korba</div><div style='padding-top: 10px; width: 80ex'>In interactive systems, actions are often correlated, presenting an
opportunity for more sample-efficient off-policy evaluation (OPE) and learning
(OPL) in large action spaces. We introduce a unified Bayesian framework to
capture these correlations through structured and informative priors. In this
framework, we propose sDM, a generic Bayesian approach designed for OPE and
OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM
leverages action correlations without compromising computational efficiency.
Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics
that assess the average performance of algorithms across multiple problem
instances, deviating from the conventional worst-case assessments. We analyze
sDM in OPE and OPL, highlighting the benefits of leveraging action
correlations. Empirical evidence showcases the strong performance of sDM.</div><div><a href='http://arxiv.org/abs/2402.14664v1'>2402.14664v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07314v1")'>A Theoretical Analysis of Nash Learning from Human Feedback under
  General KL-Regularized Preference</div>
<div id='2402.07314v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T21:44:21Z</div><div>Authors: Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) learns from the preference
signal provided by a probabilistic preference model, which takes a prompt and
two responses as input, and produces a score indicating the preference of one
response against another. So far, the most popular RLHF paradigm is
reward-based, which starts with an initial step of reward modeling, and the
constructed reward is then used to provide a reward signal for the subsequent
reward optimization stage. However, the existence of a reward function is a
strong assumption and the reward-based RLHF is limited in expressivity and
cannot capture the real-world complicated human preference.
  In this work, we provide theoretical insights for a recently proposed
learning paradigm, Nash learning from human feedback (NLHF), which considered a
general preference model and formulated the alignment process as a game between
two competitive LLMs. The learning objective is to find a policy that
consistently generates responses preferred over any competing policy while
staying close to the initial model. The objective is defined as the Nash
equilibrium (NE) of the KL-regularized preference model. We aim to make the
first attempt to study the theoretical learnability of the KL-regularized NLHF
by considering both offline and online settings. For the offline learning from
a pre-collected dataset, we propose algorithms that are efficient under
suitable coverage conditions of the dataset. For batch online learning from
iterative interactions with a preference oracle, our proposed algorithm enjoys
a finite sample guarantee under the structural condition of the underlying
preference model. Our results connect the new NLHF paradigm with traditional RL
theory, and validate the potential of reward-model-free learning under general
preference.</div><div><a href='http://arxiv.org/abs/2402.07314v1'>2402.07314v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10160v1")'>Online Policy Learning from Offline Preferences</div>
<div id='2403.10160v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T10:11:26Z</div><div>Authors: Guoxi Zhang, Han Bao, Hisashi Kashima</div><div style='padding-top: 10px; width: 80ex'>In preference-based reinforcement learning (PbRL), a reward function is
learned from a type of human feedback called preference. To expedite preference
collection, recent works have leveraged \emph{offline preferences}, which are
preferences collected for some offline data. In this scenario, the learned
reward function is fitted on the offline data. If a learning agent exhibits
behaviors that do not overlap with the offline data, the learned reward
function may encounter generalizability issues. To address this problem, the
present study introduces a framework that consolidates offline preferences and
\emph{virtual preferences} for PbRL, which are comparisons between the agent's
behaviors and the offline data. Critically, the reward function can track the
agent's behaviors using the virtual preferences, thereby offering well-aligned
guidance to the agent. Through experiments on continuous control tasks, this
study demonstrates the effectiveness of incorporating the virtual preferences
in PbRL.</div><div><a href='http://arxiv.org/abs/2403.10160v1'>2403.10160v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17257v2")'>RIME: Robust Preference-based Reinforcement Learning with Noisy
  Preferences</div>
<div id='2402.17257v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T07:03:25Z</div><div>Authors: Jie Cheng, Gang Xiong, Xingyuan Dai, Qinghai Miao, Yisheng Lv, Fei-Yue Wang</div><div style='padding-top: 10px; width: 80ex'>Preference-based Reinforcement Learning (PbRL) avoids the need for reward
engineering by harnessing human preferences as the reward signal. However,
current PbRL algorithms over-reliance on high-quality feedback from domain
experts, which results in a lack of robustness. In this paper, we present RIME,
a robust PbRL algorithm for effective reward learning from noisy preferences.
Our method incorporates a sample selection-based discriminator to dynamically
filter denoised preferences for robust training. To mitigate the accumulated
error caused by incorrect selection, we propose to warm start the reward model,
which additionally bridges the performance gap during transition from
pre-training to online training in PbRL. Our experiments on robotic
manipulation and locomotion tasks demonstrate that RIME significantly enhances
the robustness of the current state-of-the-art PbRL method. Ablation studies
further demonstrate that the warm start is crucial for both robustness and
feedback-efficiency in limited-feedback cases.</div><div><a href='http://arxiv.org/abs/2402.17257v2'>2402.17257v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17975v1")'>Sample-Efficient Preference-based Reinforcement Learning with Dynamics
  Aware Rewards</div>
<div id='2402.17975v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T01:41:34Z</div><div>Authors: Katherine Metcalf, Miguel Sarabia, Natalie Mackraz, Barry-John Theobald</div><div style='padding-top: 10px; width: 80ex'>Preference-based reinforcement learning (PbRL) aligns a robot behavior with
human preferences via a reward function learned from binary feedback over agent
behaviors. We show that dynamics-aware reward functions improve the sample
efficiency of PbRL by an order of magnitude. In our experiments we iterate
between: (1) learning a dynamics-aware state-action representation (z^{sa}) via
a self-supervised temporal consistency task, and (2) bootstrapping the
preference-based reward function from (z^{sa}), which results in faster policy
learning and better final policy performance. For example, on quadruped-walk,
walker-walk, and cheetah-run, with 50 preference labels we achieve the same
performance as existing approaches with 500 preference labels, and we recover
83\% and 66\% of ground truth reward policy performance versus only 38\% and
21\%. The performance gains demonstrate the benefits of explicitly learning a
dynamics-aware reward model. Repo: \texttt{https://github.com/apple/ml-reed}.</div><div><a href='http://arxiv.org/abs/2402.17975v1'>2402.17975v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15567v1")'>Foundation Policies with Hilbert Representations</div>
<div id='2402.15567v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T19:09:10Z</div><div>Authors: Seohong Park, Tobias Kreiman, Sergey Levine</div><div style='padding-top: 10px; width: 80ex'>Unsupervised and self-supervised objectives, such as next token prediction,
have enabled pre-training generalist models from large amounts of unlabeled
data. In reinforcement learning (RL), however, finding a truly general and
scalable unsupervised pre-training objective for generalist policies from
offline data remains a major open question. While a number of methods have been
proposed to enable generic self-supervised RL, based on principles such as
goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such
methods remain limited in terms of either the diversity of the discovered
behaviors, the need for high-quality demonstration data, or the lack of a clear
prompting or adaptation mechanism for downstream tasks. In this work, we
propose a novel unsupervised framework to pre-train generalist policies that
capture diverse, optimal, long-horizon behaviors from unlabeled offline data
such that they can be quickly adapted to any arbitrary new tasks in a zero-shot
manner. Our key insight is to learn a structured representation that preserves
the temporal structure of the underlying environment, and then to span this
learned latent space with directional movements, which enables various
zero-shot policy "prompting" schemes for downstream tasks. Through our
experiments on simulated robotic locomotion and manipulation benchmarks, we
show that our unsupervised policies can solve goal-conditioned and general RL
tasks in a zero-shot fashion, even often outperforming prior methods designed
specifically for each setting. Our code and videos are available at
https://seohong.me/projects/hilp/</div><div><a href='http://arxiv.org/abs/2402.15567v1'>2402.15567v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17135v1")'>Unsupervised Zero-Shot Reinforcement Learning via Functional Reward
  Encodings</div>
<div id='2402.17135v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T01:59:02Z</div><div>Authors: Kevin Frans, Seohong Park, Pieter Abbeel, Sergey Levine</div><div style='padding-top: 10px; width: 80ex'>Can we pre-train a generalist agent from a large amount of unlabeled offline
trajectories such that it can be immediately adapted to any new downstream
tasks in a zero-shot manner? In this work, we present a functional reward
encoding (FRE) as a general, scalable solution to this zero-shot RL problem.
Our main idea is to learn functional representations of any arbitrary tasks by
encoding their state-reward samples using a transformer-based variational
auto-encoder. This functional encoding not only enables the pre-training of an
agent from a wide diversity of general unsupervised reward functions, but also
provides a way to solve any new downstream tasks in a zero-shot manner, given a
small number of reward-annotated samples. We empirically show that FRE agents
trained on diverse random unsupervised reward functions can generalize to solve
novel tasks in a range of simulated robotic benchmarks, often outperforming
previous zero-shot RL and offline RL methods. Code for this project is provided
at: https://github.com/kvfrans/fre</div><div><a href='http://arxiv.org/abs/2402.17135v1'>2402.17135v1</a></div>
</div></div>
    <div><a href="arxiv_9.html">Prev (9)</a></div>
    <div><a href="arxiv_11.html">Next (11)</a></div>
    