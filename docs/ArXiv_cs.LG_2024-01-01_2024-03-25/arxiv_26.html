
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02416v2")'>Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction</div>
<div id='2402.02416v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:24:51Z</div><div>Authors: Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Yaodong Yang</div><div style='padding-top: 10px; width: 80ex'>Efforts to align Large Language Models (LLMs) are mainly conducted via
Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF
encounters major challenges including training reward models, actor-critic
engineering, and importantly, it requires access to LLM parameters. Here we
introduce Aligner, a new efficient alignment paradigm that bypasses the whole
RLHF process by learning the correctional residuals between the aligned and the
unaligned answers. Our Aligner offers several key advantages. Firstly, it is an
autoregressive seq2seq model that is trained on the query-answer-correction
dataset via supervised learning; this offers a parameter-efficient alignment
solution with minimal resources. Secondly, the Aligner facilitates
weak-to-strong generalization; finetuning large pretrained models by Aligner's
supervisory signals demonstrates strong performance boost. Thirdly, Aligner
functions as a model-agnostic plug-and-play module, allowing for its direct
application on different open-source and API-based models. Remarkably,
Aligner-7B improves 11 different LLMs by 21.9% in helpfulness and 23.8% in
harmlessness on average (GPT-4 by 17.5% and 26.9%). When finetuning (strong)
Llama2-70B with (weak) Aligner-13B's supervision, we can improve Llama2 by 8.2%
in helpfulness and 61.6% in harmlessness. See our dataset and code at
https://aligner2024.github.io</div><div><a href='http://arxiv.org/abs/2402.02416v2'>2402.02416v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14688v1")'>Q-Probe: A Lightweight Approach to Reward Maximization for Language
  Models</div>
<div id='2402.14688v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:43:16Z</div><div>Authors: Kenneth Li, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg, David Brandfonbrener</div><div style='padding-top: 10px; width: 80ex'>We present an approach called Q-probing to adapt a pre-trained language model
to maximize a task-specific reward function. At a high level, Q-probing sits
between heavier approaches such as finetuning and lighter approaches such as
few shot prompting, but can also be combined with either. The idea is to learn
a simple linear function on a model's embedding space that can be used to
reweight candidate completions. We theoretically show that this sampling
procedure is equivalent to a KL-constrained maximization of the Q-probe as the
number of samples increases. To train the Q-probes we consider either reward
modeling or a class of novel direct policy learning objectives based on
importance weighted policy gradients. With this technique, we see gains in
domains with ground-truth rewards (code generation) as well as implicit rewards
defined by preference data, even outperforming finetuning in data-limited
regimes. Moreover, a Q-probe can be trained on top of an API since it only
assumes access to sampling and embeddings. Code:
https://github.com/likenneth/q_probe .</div><div><a href='http://arxiv.org/abs/2402.14688v1'>2402.14688v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03284v1")'>Deal, or no deal (or who knows)? Forecasting Uncertainty in
  Conversations using Large Language Models</div>
<div id='2402.03284v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:39:47Z</div><div>Authors: Anthony Sicilia, Hyunwoo Kim, Khyathi Raghavi Chandu, Malihe Alikhani, Jack Hessel</div><div style='padding-top: 10px; width: 80ex'>Effective interlocutors account for the uncertain goals, beliefs, and
emotions of others. But even the best human conversationalist cannot perfectly
anticipate the trajectory of a dialogue. How well can language models represent
inherent uncertainty in conversations? We propose FortUne Dial, an expansion of
the long-standing "conversation forecasting" task: instead of just accuracy,
evaluation is conducted with uncertainty-aware metrics, effectively enabling
abstention on individual instances. We study two ways in which language models
potentially represent outcome uncertainty (internally, using scores and
directly, using tokens) and propose fine-tuning strategies to improve
calibration of both representations. Experiments on eight difficult negotiation
corpora demonstrate that our proposed fine-tuning strategies (a traditional
supervision strategy and an off-policy reinforcement learning strategy) can
calibrate smaller open-source models to compete with pre-trained models 10x
their size.</div><div><a href='http://arxiv.org/abs/2402.03284v1'>2402.03284v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12621v1")'>Reflect-RL: Two-Player Online RL Fine-Tuning for LMs</div>
<div id='2402.12621v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T01:04:21Z</div><div>Authors: Runlong Zhou, Simon S. Du, Beibin Li</div><div style='padding-top: 10px; width: 80ex'>As language models (LMs) demonstrate their capabilities in various fields,
their application to tasks requiring multi-round interactions has become
increasingly popular. These tasks usually have complex dynamics, so supervised
fine-tuning (SFT) on a limited offline dataset does not yield good performance.
However, only a few works attempted to directly train the LMs within
interactive decision-making environments. We aim to create an effective
mechanism to fine-tune LMs with online reinforcement learning (RL) in these
environments. We propose Reflect-RL, a two-player system to fine-tune an LM
using online RL, where a frozen reflection model assists the policy model. To
generate data for the warm-up SFT stage, we use negative example generation to
enhance the error-correction ability of the reflection model. Furthermore, we
designed single-prompt action enumeration and applied curriculum learning to
allow the policy model to learn more efficiently. Empirically, we verify that
Reflect-RL outperforms SFT and online RL without reflection. Testing results
indicate GPT-2-xl after Reflect-RL also outperforms those of untuned
pre-trained LMs, such as Mistral 7B.</div><div><a href='http://arxiv.org/abs/2402.12621v1'>2402.12621v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00085v1")'>Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy
  Learning</div>
<div id='2402.00085v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T06:13:28Z</div><div>Authors: Xuecheng Niu, Akinori Ito, Takashi Nose</div><div style='padding-top: 10px; width: 80ex'>Training task-oriented dialog agents based on reinforcement learning is
time-consuming and requires a large number of interactions with real users. How
to grasp dialog policy within limited dialog experiences remains an obstacle
that makes the agent training process less efficient. In addition, most
previous frameworks start training by randomly choosing training samples, which
differs from the human learning method and hurts the efficiency and stability
of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a
curiosity-driven curriculum learning framework based on a state-of-the-art
model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ).
Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively,
following two opposite training strategies: classic curriculum learning and its
reverse version. Our results show that by introducing scheduled learning and
curiosity, the new framework leads to a significant improvement over the DDQ
and Deep Q-learning(DQN). Surprisingly, we found that traditional curriculum
learning was not always effective. Specifically, according to the experimental
results, the easy-first and difficult-first strategies are more suitable for
SC-DDQ and DDQ. To analyze our results, we adopted the entropy of sampled
actions to depict action exploration and found that training strategies with
high entropy in the first stage and low entropy in the last stage lead to
better performance.</div><div><a href='http://arxiv.org/abs/2402.00085v1'>2402.00085v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07191v1")'>$\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking
  Reinforcement Learning Algorithms in Generative Language Model</div>
<div id='2403.07191v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T22:24:14Z</div><div>Authors: Yufeng Zhang, Liyu Chen, Boyi Liu, Yingxiang Yang, Qiwen Cui, Yunzhe Tao, Hongxia Yang</div><div style='padding-top: 10px; width: 80ex'>Recent advances in reinforcement learning (RL) algorithms aim to enhance the
performance of language models at scale. Yet, there is a noticeable absence of
a cost-effective and standardized testbed tailored to evaluating and comparing
these algorithms. To bridge this gap, we present a generalized version of the
24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a
target value $K$ with $N$ integers. We evaluate the effectiveness of
established RL algorithms such as Proximal Policy Optimization (PPO), alongside
novel approaches like Identity Policy Optimization (IPO) and Direct Policy
Optimization (DPO).</div><div><a href='http://arxiv.org/abs/2403.07191v1'>2403.07191v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01364v2")'>Continual Learning for Large Language Models: A Survey</div>
<div id='2402.01364v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:34:09Z</div><div>Authors: Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, Gholamreza Haffari</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are not amenable to frequent re-training, due to
high training costs arising from their massive scale. However, updates are
necessary to endow LLMs with new skills and keep them up-to-date with rapidly
evolving human knowledge. This paper surveys recent works on continual learning
for LLMs. Due to the unique nature of LLMs, we catalog continue learning
techniques in a novel multi-staged categorization scheme, involving continual
pretraining, instruction tuning, and alignment. We contrast continual learning
for LLMs with simpler adaptation methods used in smaller models, as well as
with other enhancement strategies like retrieval-augmented generation and model
editing. Moreover, informed by a discussion of benchmarks and evaluation, we
identify several challenges and future work directions for this crucial task.</div><div><a href='http://arxiv.org/abs/2402.01364v2'>2402.01364v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01286v3")'>A Comprehensive Study of Knowledge Editing for Large Language Models</div>
<div id='2401.01286v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T16:54:58Z</div><div>Authors: Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
give a deeper understanding of the knowledge structures inherent within LLMs.
Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.</div><div><a href='http://arxiv.org/abs/2401.01286v3'>2401.01286v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12847v1")'>Instruction-tuned Language Models are Better Knowledge Learners</div>
<div id='2402.12847v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T09:20:32Z</div><div>Authors: Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer</div><div style='padding-top: 10px; width: 80ex'>In order for large language model (LLM)-based assistants to effectively adapt
to evolving information needs, it must be possible to update their factual
knowledge through continued training on new data. The standard recipe for doing
so involves continued pre-training on new documents followed by
instruction-tuning on question-answer (QA) pairs. However, we find that LLMs
trained with this recipe struggle to answer questions, even though the
perplexity of documents is minimized. We found that QA pairs are generally
straightforward, while documents are more complex, weaving many factual
statements together in an intricate manner. Therefore, we hypothesize that it
is beneficial to expose LLMs to QA pairs before continued pre-training on
documents so that the process of encoding knowledge from complex documents
takes into account how this knowledge is accessed through questions. Based on
this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes
on questions prior to training on documents. This contrasts with standard
instruction-tuning, which learns how to extract knowledge after training on
documents. Extensive experiments and ablation studies demonstrate that PIT
significantly enhances the ability of LLMs to absorb knowledge from new
documents, outperforming standard instruction-tuning by 17.8%.</div><div><a href='http://arxiv.org/abs/2402.12847v1'>2402.12847v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12842v1")'>PromptKD: Distilling Student-Friendly Knowledge for Generative Language
  Models via Prompt Tuning</div>
<div id='2402.12842v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T09:10:08Z</div><div>Authors: Gyeongman Kim, Doohyuk Jang, Eunho Yang</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in large language models (LLMs) have raised concerns
about inference costs, increasing the need for research into model compression.
While knowledge distillation (KD) is a prominent method for this, research on
KD for generative language models like LLMs is relatively sparse, and the
approach of distilling student-friendly knowledge, which has shown promising
performance in KD for classification models, remains unexplored in generative
language models. To explore this approach, we propose PromptKD, a simple yet
effective method that utilizes prompt tuning - for the first time in KD - to
enable generative language models to transfer student-friendly knowledge.
Unlike previous works in classification that require fine-tuning the entire
teacher model for extracting student-friendly knowledge, PromptKD achieves
similar effects by adding a small number of prompt tokens and tuning only the
prompt with student guidance. Extensive experiments on instruction-following
datasets using the GPT-2 model family show that PromptKD achieves
state-of-the-art performance while adding only 0.0007% of the teacher's
parameters as prompts. Further analysis suggests that distilling
student-friendly knowledge alleviates exposure bias effectively throughout the
entire training process, leading to performance enhancements.</div><div><a href='http://arxiv.org/abs/2402.12842v1'>2402.12842v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18334v1")'>Learning to Generate Instruction Tuning Datasets for Zero-Shot Task
  Adaptation</div>
<div id='2402.18334v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T13:54:57Z</div><div>Authors: Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach</div><div style='padding-top: 10px; width: 80ex'>We introduce Bonito, an open-source model for conditional task generation:
the task of converting unannotated text into task-specific training datasets
for instruction tuning. Our goal is to enable zero-shot task adaptation of
large language models on users' specialized, private data. We train Bonito on a
new large-scale dataset with 1.65M examples created by remixing existing
instruction tuning datasets into meta-templates. The meta-templates for a
dataset produce training examples where the input is the unannotated text and
the task attribute and the output consists of the instruction and the response.
We use Bonito to generate synthetic tasks for seven datasets from specialized
domains across three task types -- yes-no question answering, extractive
question answering, and natural language inference -- and adapt language
models. We show that Bonito significantly improves the average performance of
pretrained and instruction tuned models over the de facto self supervised
baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned
variants of Mistral and Llama2 with Bonito improves the strong zero-shot
performance by 22.1 F1 points whereas the next word prediction objective undoes
some of the benefits of instruction tuning and reduces the average performance
by 0.8 F1 points. We conduct additional experiments with Bonito to understand
the effects of the domain, the size of the training set, and the choice of
alternative synthetic task generators. Overall, we show that learning with
synthetic instruction tuning datasets is an effective way to adapt language
models to new domains. The model, dataset, and code are available at
https://github.com/BatsResearch/bonito.</div><div><a href='http://arxiv.org/abs/2402.18334v1'>2402.18334v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09906v1")'>Generative Representational Instruction Tuning</div>
<div id='2402.09906v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T12:12:19Z</div><div>Authors: Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</div><div style='padding-top: 10px; width: 80ex'>All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by &gt; 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.</div><div><a href='http://arxiv.org/abs/2402.09906v1'>2402.09906v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00794v2")'>ReAGent: A Model-agnostic Feature Attribution Method for Generative
  Language Models</div>
<div id='2402.00794v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:25:51Z</div><div>Authors: Zhixue Zhao, Boxuan Shan</div><div style='padding-top: 10px; width: 80ex'>Feature attribution methods (FAs), such as gradients and attention, are
widely employed approaches to derive the importance of all input features to
the model predictions. Existing work in natural language processing has mostly
focused on developing and testing FAs for encoder-only language models (LMs) in
classification tasks. However, it is unknown if it is faithful to use these FAs
for decoder-only models on text generation, due to the inherent differences
between model architectures and task settings respectively. Moreover, previous
work has demonstrated that there is no `one-wins-all' FA across models and
tasks. This makes the selection of a FA computationally expensive for large LMs
since input importance derivation often requires multiple forward and backward
passes including gradient computations that might be prohibitive even with
access to large compute. To address these issues, we present a model-agnostic
FA for generative LMs called Recursive Attribution Generator (ReAGent). Our
method updates the token importance distribution in a recursive manner. For
each update, we compute the difference in the probability distribution over the
vocabulary for predicting the next token between using the original input and
using a modified version where a part of the input is replaced with RoBERTa
predictions. Our intuition is that replacing an important token in the context
should have resulted in a larger change in the model's confidence in predicting
the token than replacing an unimportant token. Our method can be universally
applied to any generative LM without accessing internal model weights or
additional training and fine-tuning, as most other FAs require. We extensively
compare the faithfulness of ReAGent with seven popular FAs across six
decoder-only LMs of various sizes. The results show that our method
consistently provides more faithful token importance distributions.</div><div><a href='http://arxiv.org/abs/2402.00794v2'>2402.00794v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05467v1")'>Machine Teaching for Building Modular AI Agents based on Zero-shot
  Learners</div>
<div id='2401.05467v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T14:41:37Z</div><div>Authors: Karan Taneja, Ashok Goel</div><div style='padding-top: 10px; width: 80ex'>The recent advances in large language models (LLMs) have led to the creation
of many modular AI agents. These agents employ LLMs as zero-shot learners to
perform sub-tasks in order to solve complex tasks set forth by human users. We
propose an approach to enhance the robustness and performance of modular AI
agents that utilize LLMs as zero-shot learners. Our iterative machine teaching
method offers an efficient way to teach AI agents over time with limited human
feedback, addressing the limit posed by the quality of zero-shot learning. We
advocate leveraging the data traces from initial deployments and outputs or
annotations from the zero-shot learners to train smaller and task-specific
substitute models which can reduce both the monetary costs and environmental
impact. Our machine teaching process avails human expertise to correct examples
with a high likelihood of misannotations. Results on three tasks, common to
conversational AI agents, show that close-to-oracle performance can be achieved
with supervision on 20-70% of the dataset depending upon the complexity of the
task and performance of zero-shot learners.</div><div><a href='http://arxiv.org/abs/2401.05467v1'>2401.05467v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03870v1")'>Learning to Decode Collaboratively with Multiple Language Models</div>
<div id='2403.03870v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:23:28Z</div><div>Authors: Shannon Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, David Sontag</div><div style='padding-top: 10px; width: 80ex'>We propose a method to teach multiple large language models (LLM) to
collaborate by interleaving their generations at the token level. We model the
decision of which LLM generates the next token as a latent variable. By
optimizing the marginal likelihood of a training set under our latent variable
model, the base LLM automatically learns when to generate itself and when to
call on one of the ``assistant'' language models to generate, all without
direct supervision. Token-level collaboration during decoding allows for a
fusion of each model's expertise in a manner tailored to the specific task at
hand. Our collaborative decoding is especially useful in cross-domain settings
where a generalist base LLM learns to invoke domain expert models. On
instruction-following, domain-specific QA, and reasoning tasks, we show that
the performance of the joint system exceeds that of the individual models.
Through qualitative analysis of the learned latent decisions, we show models
trained with our method exhibit several interesting collaboration patterns,
e.g., template-filling. Our code is available at
https://github.com/clinicalml/co-llm.</div><div><a href='http://arxiv.org/abs/2403.03870v1'>2403.03870v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16247v1")'>Learning Translations: Emergent Communication Pretraining for
  Cooperative Language Acquisition</div>
<div id='2402.16247v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T02:13:36Z</div><div>Authors: Dylan Cope, Peter McBurney</div><div style='padding-top: 10px; width: 80ex'>In Emergent Communication (EC) agents learn to communicate with one another,
but the protocols that they develop are specialised to their training
community. This observation led to research into Zero-Shot Coordination (ZSC)
for learning communication strategies that are robust to agents not encountered
during training. However, ZSC typically assumes that no prior data is available
about the agents that will be encountered in the zero-shot setting. In many
cases, this presents an unnecessarily hard problem and rules out communication
via preestablished conventions. We propose a novel AI challenge called a
Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions
are relaxed by allowing a 'joiner' agent to learn from a dataset of
interactions between agents in a target community. We propose and compare two
methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication
pretraining and Translation Learning (ECTL), in which an agent is trained in
self-play with EC and then learns from the data to translate between the
emergent protocol and the target community's protocol.</div><div><a href='http://arxiv.org/abs/2402.16247v1'>2402.16247v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15670v1")'>YODA: Teacher-Student Progressive Learning for Language Models</div>
<div id='2401.15670v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T14:32:15Z</div><div>Authors: Jianqiao Lu, Wanjun Zhong, Yufei Wang, Zhijiang Guo, Qi Zhu, Wenyong Huang, Yanlin Wang, Fei Mi, Baojun Wang, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu</div><div style='padding-top: 10px; width: 80ex'>Although large language models (LLMs) have demonstrated adeptness in a range
of tasks, they still lag behind human learning efficiency. This disparity is
often linked to the inherent human capacity to learn from basic examples,
gradually generalize and handle more complex problems, and refine their skills
with continuous feedback. Inspired by this, this paper introduces YODA, a novel
teacher-student progressive learning framework that emulates the
teacher-student education process to improve the efficacy of model fine-tuning.
The framework operates on an interactive \textit{basic-generalized-harder}
loop. The teacher agent provides tailored feedback on the student's answers,
and systematically organizes the education process. This process unfolds by
teaching the student basic examples, reinforcing understanding through
generalized questions, and then enhancing learning by posing questions with
progressively enhanced complexity. With the teacher's guidance, the student
learns to iteratively refine its answer with feedback, and forms a robust and
comprehensive understanding of the posed questions. The systematic procedural
data, which reflects the progressive learning process of humans, is then
utilized for model training. Taking math reasoning as a testbed, experiments
show that training LLaMA2 with data from YODA improves SFT with significant
performance gain (+17.01\% on GSM8K and +9.98\% on MATH). In addition, we find
that training with curriculum learning further improves learning robustness.</div><div><a href='http://arxiv.org/abs/2401.15670v1'>2401.15670v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02884v1")'>MathScale: Scaling Instruction Tuning for Mathematical Reasoning</div>
<div id='2403.02884v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:42:59Z</div><div>Authors: Zhengyang Tang, Xingxing Zhang, Benyou Wang, Furu Wei</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have demonstrated remarkable capabilities in
problem-solving. However, their proficiency in solving mathematical problems
remains inadequate. We propose MathScale, a simple and scalable method to
create high-quality mathematical reasoning data using frontier LLMs (e.g., {\tt
GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning,
it first extracts topics and knowledge points from seed math questions and then
build a concept graph, which is subsequently used to generate new math
questions. MathScale exhibits effective scalability along the size axis of the
math dataset that we generate. As a result, we create a mathematical reasoning
dataset (MathScaleQA) containing two million math question-answer pairs. To
evaluate mathematical reasoning abilities of LLMs comprehensively, we construct
{\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten
datasets (including GSM8K and MATH) covering K-12, college, and competition
level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g.,
LLaMA-2 and Mistral), resulting in significantly improved capabilities in
mathematical reasoning. Evaluated on {\sc MwpBench}, MathScale-7B achieves
state-of-the-art performance across all datasets, surpassing its best peers of
equivalent size by 42.9\% in micro average accuracy and 43.7\% in macro average
accuracy, respectively.</div><div><a href='http://arxiv.org/abs/2403.02884v1'>2403.02884v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06961v1")'>CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'
  Mathematical Reasoning Capabilities</div>
<div id='2401.06961v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T03:18:16Z</div><div>Authors: Yujun Mao, Yoon Kim, Yilun Zhou</div><div style='padding-top: 10px; width: 80ex'>Recent large language models (LLMs) have shown indications of mathematical
reasoning ability. However it has not been clear how they would fare on more
challenging competition-level problems. And while self-generated verbalizations
of intermediate reasoning steps (i.e., chain-of-thought prompting) have been
shown to be helpful, whether LLMs can make use of helpful side information such
as problem-specific hints has not been investigated before. In this paper, we
propose a challenging benchmark dataset for enabling such analyses. The Concept
and Hint-Annotated Math Problems (CHAMP) consists of high school math
competition problems, annotated with concepts, or general math facts, and
hints, or problem-specific tricks. These annotations allow us to explore the
effects of additional information, such as relevant hints, misleading concepts,
or related problems. This benchmark is difficult, with the best model only
scoring 58.1% in standard settings. With concepts and hints, performance
sometimes improves, indicating that some models can make use of such side
information. We further annotate model-generated solutions for their
correctness. Using this corpus, we find that models often arrive at the correct
final answer through wrong reasoning steps. In addition, we test whether models
are able to verify these solutions, and find that most models struggle. The
dataset and code are available on the project website.</div><div><a href='http://arxiv.org/abs/2401.06961v1'>2401.06961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16878v1")'>EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math
  Languages</div>
<div id='2402.16878v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:10:11Z</div><div>Authors: Johnathan Mercer</div><div style='padding-top: 10px; width: 80ex'>Formal mathematics is the discipline of translating mathematics into a
programming language in which any statement can be unequivocally checked by a
computer. Mathematicians and computer scientists have spent decades of
painstaking formalization efforts developing languages such as Coq, HOL, and
Lean. Machine learning research has converged on these formal math corpora and
given rise to an assortment of methodologies to aid in interactive and
automated theorem proving. However, these papers have primarily focused on one
method, for one proof task, in one language. This paper introduces EvoGPT-f: a
novel evolutionary framework for the first systematic quantitative analysis of
the differential machine learnability of five formal math corpora (Lean 3, Lean
4, Coq, HOL 4, HOL Light) using four tokenization methods (character,
word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not
put to rest the question of the "best" or "easiest" language to learn. Rather,
this framework and preliminary findings begin to illuminate the differential
machine learnability of these languages, offering a foundation to forge more
systematic quantitative and qualitative comparative research across
communities.</div><div><a href='http://arxiv.org/abs/2402.16878v1'>2402.16878v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06356v2")'>An Empirical Investigation into the Effect of Parameter Choices in
  Knowledge Distillation</div>
<div id='2401.06356v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T04:14:21Z</div><div>Authors: Md Arafat Sultan, Aashka Trivedi, Parul Awasthy, Avirup Sil</div><div style='padding-top: 10px; width: 80ex'>We present a large-scale empirical study of how choices of configuration
parameters affect performance in knowledge distillation (KD). An example of
such a KD parameter is the measure of distance between the predictions of the
teacher and the student, common choices for which include the mean squared
error (MSE) and the KL-divergence. Although scattered efforts have been made to
understand the differences between such options, the KD literature still lacks
a systematic study on their general effect on student performance. We take an
empirical approach to this question in this paper, seeking to find out the
extent to which such choices influence student performance across 13 datasets
from 4 NLP tasks and 3 student sizes. We quantify the cost of making
sub-optimal choices and identify a single configuration that performs well
across the board.</div><div><a href='http://arxiv.org/abs/2401.06356v2'>2401.06356v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09003v4")'>Augmenting Math Word Problems via Iterative Question Composing</div>
<div id='2401.09003v4' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T06:48:16Z</div><div>Authors: Haoxiong Liu, Yifan Zhang, Yifan Luo, Andrew Chi-Chih Yao</div><div style='padding-top: 10px; width: 80ex'>Despite the advancements in large language models (LLMs) for mathematical
reasoning, solving competition-level math problems remains a significant
challenge, especially for open-source LLMs without external tools. We introduce
the MMIQC dataset, comprising a mixture of processed web data and synthetic
question-response pairs, aimed at enhancing the mathematical reasoning
capabilities of base language models. Models fine-tuned on MMIQC consistently
surpass their counterparts in performance on the MATH benchmark across various
model sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding the
previous open-source state-of-the-art by 8.2% and outperforming the initial
version GPT-4 released in 2023. Extensive evaluation results on Hungarian high
school finals suggest that such improvement can generalize to unseen data. Our
ablation study on MMIQC reveals that a large part of the improvement can be
attributed to our novel augmentation method, Iterative Question Composing
(IQC), which involves iteratively composing new questions from seed problems
using an LLM and applying rejection sampling through another LLM. The MMIQC
dataset is available on the HuggingFace hub at
https://huggingface.co/datasets/Vivacem/MMIQC. Our code is available at
https://github.com/iiis-ai/IterativeQuestionComposing.</div><div><a href='http://arxiv.org/abs/2401.09003v4'>2401.09003v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00799v1")'>An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning</div>
<div id='2403.00799v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T17:38:43Z</div><div>Authors: Zui Chen, Yezeng Chen, Jiaqi Han, Zhijie Huang, Ji Qi, Yi Zhou</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are displaying emergent abilities for math
reasoning tasks,and there is a growing attention on enhancing the ability of
open-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to
explore a general data strategy for supervised data to help optimize and expand
math reasoning ability.Firstly, we determine the ability boundary of reasoning
paths augmentation by identifying these paths' minimal optimal set.Secondly, we
validate that different abilities of the model can be cumulatively enhanced by
Mix of Minimal Optimal Sets of corresponding types of data, while our models
MMOS achieve SOTA performance on series base models under much lower
construction costs.Besides, we point out GSM-HARD is not really hard and
today's LLMs no longer lack numerical robustness.Also, we provide an Auto
Problem Generator for robustness testing and educational applications.Our code
and data are publicly available at https://github.com/cyzhh/MMOS.</div><div><a href='http://arxiv.org/abs/2403.00799v1'>2403.00799v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13653v1")'>PQA: Zero-shot Protein Question Answering for Free-form Scientific
  Enquiry with Large Language Models</div>
<div id='2402.13653v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:38:17Z</div><div>Authors: Eli M Carrami, Sahand Sharifzadeh</div><div style='padding-top: 10px; width: 80ex'>We introduce the novel task of zero-shot Protein Question Answering (PQA) for
free-form scientific enquiry. Given a previously unseen protein sequence and a
natural language question, the task is to deliver a scientifically accurate
answer. This task not only supports future biological research, but could also
provide a test bed for assessing the scientific precision of large language
models (LLMs). We contribute the first specialized dataset for PQA model
training, containing 257K protein sequences annotated with 1.97M scientific
question-answer pairs. Additionally, we propose and study several novel
biologically relevant benchmarks for scientific PQA. Employing two robust
multi-modal architectures, we establish an initial state-of-the-art performance
for PQA and reveal key performance factors through ablation studies. Our
comprehensive PQA framework, named Pika, including dataset, code, model
checkpoints, and a user-friendly demo, is openly accessible on
github.com/EMCarrami/Pika, promoting wider research and application in the
field.</div><div><a href='http://arxiv.org/abs/2402.13653v1'>2402.13653v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14661v1")'>Towards Modeling Learner Performance with Large Language Models</div>
<div id='2403.14661v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T14:06:34Z</div><div>Authors: Seyed Parsa Neshaei, Richard Lee Davis, Adam Hazimeh, Bojan Lazarevski, Pierre Dillenbourg, Tanja Käser</div><div style='padding-top: 10px; width: 80ex'>Recent work exploring the capabilities of pre-trained large language models
(LLMs) has demonstrated their ability to act as general pattern machines by
completing complex token sequences representing a wide array of tasks,
including time-series prediction and robot control. This paper investigates
whether the pattern recognition and sequence modeling capabilities of LLMs can
be extended to the domain of knowledge tracing, a critical component in the
development of intelligent tutoring systems (ITSs) that tailor educational
experiences by predicting learner performance over time. In an empirical
evaluation across multiple real-world datasets, we compare two approaches to
using LLMs for this task, zero-shot prompting and model fine-tuning, with
existing, non-LLM approaches to knowledge tracing. While LLM-based approaches
do not achieve state-of-the-art performance, fine-tuned LLMs surpass the
performance of naive baseline models and perform on par with standard Bayesian
Knowledge Tracing approaches across multiple metrics. These findings suggest
that the pattern recognition capabilities of LLMs can be used to model complex
learning trajectories, opening a novel avenue for applying LLMs to educational
contexts. The paper concludes with a discussion of the implications of these
findings for future research, suggesting that further refinements and a deeper
understanding of LLMs' predictive mechanisms could lead to enhanced performance
in knowledge tracing tasks.</div><div><a href='http://arxiv.org/abs/2403.14661v1'>2403.14661v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11705v1")'>Learning Memory Kernels in Generalized Langevin Equations</div>
<div id='2402.11705v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T21:01:49Z</div><div>Authors: Quanjun Lang, Jianfeng Lu</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel approach for learning memory kernels in Generalized
Langevin Equations. This approach initially utilizes a regularized Prony method
to estimate correlation functions from trajectory data, followed by regression
over a Sobolev norm-based loss function with RKHS regularization. Our approach
guarantees improved performance within an exponentially weighted $L^2$ space,
with the kernel estimation error controlled by the error in estimated
correlation functions. We demonstrate the superiority of our estimator compared
to other regression estimators that rely on $L^2$ loss functions and also an
estimator derived from the inverse Laplace transform, using numerical examples
that highlight its consistent advantage across various weight parameter
selections. Additionally, we provide examples that include the application of
force and drift terms in the equation.</div><div><a href='http://arxiv.org/abs/2402.11705v1'>2402.11705v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16291v1")'>MachineLearnAthon: An Action-Oriented Machine Learning Didactic Concept</div>
<div id='2401.16291v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T16:50:32Z</div><div>Authors: Michal Tkáč, Jakub Sieber, Lara Kuhlmann, Matthias Brueggenolte, Alexandru Rinciog, Michael Henke, Artur M. Schweidtmann, Qinghe Gao, Maximilian F. Theisen, Radwa El Shawi</div><div style='padding-top: 10px; width: 80ex'>Machine Learning (ML) techniques are encountered nowadays across disciplines,
from social sciences, through natural sciences to engineering. The broad
application of ML and the accelerated pace of its evolution lead to an
increasing need for dedicated teaching concepts aimed at making the application
of this technology more reliable and responsible. However, teaching ML is a
daunting task. Aside from the methodological complexity of ML algorithms, both
with respect to theory and implementation, the interdisciplinary and empirical
nature of the field need to be taken into consideration. This paper introduces
the MachineLearnAthon format, an innovative didactic concept designed to be
inclusive for students of different disciplines with heterogeneous levels of
mathematics, programming and domain expertise. At the heart of the concept lie
ML challenges, which make use of industrial data sets to solve real-world
problems. These cover the entire ML pipeline, promoting data literacy and
practical skills, from data preparation, through deployment, to evaluation.</div><div><a href='http://arxiv.org/abs/2401.16291v1'>2401.16291v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10561v1")'>A collection of the accepted papers for the Human-Centric Representation
  Learning workshop at AAAI 2024</div>
<div id='2403.10561v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T08:46:07Z</div><div>Authors: Dimitris Spathis, Aaqib Saeed, Ali Etemad, Sana Tonekaboni, Stefanos Laskaridis, Shohreh Deldari, Chi Ian Tang, Patrick Schwab, Shyam Tailor</div><div style='padding-top: 10px; width: 80ex'>This non-archival index is not complete, as some accepted papers chose to
opt-out of inclusion. The list of all accepted papers is available on the
workshop website.</div><div><a href='http://arxiv.org/abs/2403.10561v1'>2403.10561v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17205v1")'>Measuring Vision-Language STEM Skills of Neural Models</div>
<div id='2402.17205v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T04:55:03Z</div><div>Authors: Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang</div><div style='padding-top: 10px; width: 80ex'>We introduce a new challenge to test the STEM skills of neural models. The
problems in the real world often require solutions, combining knowledge from
STEM (science, technology, engineering, and math). Unlike existing datasets,
our dataset requires the understanding of multimodal vision-language
information of STEM. Our dataset features one of the largest and most
comprehensive datasets for the challenge. It includes 448 skills and 1,073,146
questions spanning all STEM subjects. Compared to existing datasets that often
focus on examining expert-level ability, our dataset includes fundamental
skills and questions designed based on the K-12 curriculum. We also add
state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our
benchmark. Results show that the recent model advances only help master a very
limited number of lower grade-level skills (2.5% in the third grade) in our
dataset. In fact, these models are still well below (averaging 54.7%) the
performance of elementary students, not to mention near expert-level
performance. To understand and increase the performance on our dataset, we
teach the models on a training split of our dataset. Even though we observe
improved performance, the model performance remains relatively low compared to
average elementary students. To solve STEM problems, we will need novel
algorithmic innovations from the community.</div><div><a href='http://arxiv.org/abs/2402.17205v1'>2402.17205v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10210v1")'>Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction</div>
<div id='2401.10210v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T17:57:21Z</div><div>Authors: Anup Shakya, Vasile Rus, Deepak Venugopal</div><div style='padding-top: 10px; width: 80ex'>Predicting the strategy (sequence of concepts) that a student is likely to
use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt
themselves to different types of learners based on their learning abilities.
This can lead to a more dynamic, engaging, and personalized experience for
students. To scale up training a prediction model (such as LSTMs) over
large-scale education datasets, we develop a non-parametric approach to cluster
symmetric instances in the data. Specifically, we learn a representation based
on Node2Vec that encodes symmetries over mastery or skill level since, to solve
a problem, it is natural that a student's strategy is likely to involve
concepts in which they have gained mastery. Using this representation, we use
DP-Means to group symmetric instances through a coarse-to-fine refinement of
the clusters. We apply our model to learn strategies for Math learning from
large-scale datasets from MATHia, a leading AIS for middle-school math
learning. Our results illustrate that our approach can consistently achieve
high accuracy using a small sample that is representative of the full dataset.
Further, we show that this approach helps us learn strategies with high
accuracy for students at different skill levels, i.e., leveraging symmetries
improves fairness in the prediction model.</div><div><a href='http://arxiv.org/abs/2401.10210v1'>2401.10210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12819v1")'>Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How
  Many Labelled Samples Do We Need?</div>
<div id='2402.12819v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T08:38:24Z</div><div>Authors: Branislav Pecher, Ivan Srba, Maria Bielikova</div><div style='padding-top: 10px; width: 80ex'>When solving a task with limited labelled data, researchers can either use a
general large language model without further update, or use the few examples to
tune a specialised smaller model. When enough labels are available, the
specialised models outperform the general ones on many NLP tasks. In this work,
we aim to investigate how many labelled samples are required for the
specialised models to achieve this superior performance, while taking the
results variance into consideration. Observing the behaviour of prompting,
in-context learning, fine-tuning and instruction-tuning, identifying their
break-even points when increasing number of labelled training samples across
three tasks of varying complexity, we find that the specialised models often
need only few samples ($100-1000$) to be on par or better than the general
ones. At the same time, the amount of required labelled data strongly depends
on the task complexity and results variance.</div><div><a href='http://arxiv.org/abs/2402.12819v1'>2402.12819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16123v1")'>InstructEdit: Instruction-based Knowledge Editing for Large Language
  Models</div>
<div id='2402.16123v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T15:46:33Z</div><div>Authors: Bozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu, Kouying Xue, Yanjie Gou, Xi Chen, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>Knowledge editing for large language models can offer an efficient solution
to alter a model's behavior without negatively impacting the overall
performance. However, the current approach encounters issues with limited
generalizability across tasks, necessitating one distinct editor for each task,
which significantly hinders the broader applications. To address this, we take
the first step to analyze the multi-task generalization issue in knowledge
editing. Specifically, we develop an instruction-based editing technique,
termed InstructEdit, which facilitates the editor's adaptation to various task
performances simultaneously using simple instructions. With only one unified
editor for each LLM, we empirically demonstrate that InstructEdit can improve
the editor's control, leading to an average 14.86% increase in Reliability in
multi-task editing setting. Furthermore, experiments involving holdout unseen
task illustrate that InstructEdit consistently surpass previous strong
baselines. To further investigate the underlying mechanisms of
instruction-based knowledge editing, we analyze the principal components of the
editing gradient directions, which unveils that instructions can help control
optimization direction with stronger OOD generalization. Code and datasets will
be available in https://github.com/zjunlp/EasyEdit.</div><div><a href='http://arxiv.org/abs/2402.16123v1'>2402.16123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08631v2")'>Knowledge Editing on Black-box Large Language Models</div>
<div id='2402.08631v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T17:59:34Z</div><div>Authors: Xiaoshuai Song, Zhengyang Wang, Keqing He, Guanting Dong, Yutao Mou, Jinxu Zhao, Weiran Xu</div><div style='padding-top: 10px; width: 80ex'>Knowledge editing (KE) aims to efficiently and precisely modify the behavior
of large language models (LLMs) to update specific knowledge without negatively
influencing other knowledge. Current research primarily focuses on white-box
LLMs editing, overlooking an important scenario: black-box LLMs editing, where
LLMs are accessed through interfaces and only textual output is available. In
this paper, we first officially introduce KE on black-box LLMs and then propose
a comprehensive evaluation framework to overcome the limitations of existing
evaluations that are not applicable to black-box LLMs editing and lack
comprehensiveness. To tackle privacy leaks of editing data and style
over-editing in current methods, we introduce a novel postEdit framework,
resolving privacy concerns through downstream post-processing and maintaining
textual style consistency via fine-grained editing to original responses.
Experiments and analysis on two benchmarks demonstrate that postEdit
outperforms all baselines and achieves strong generalization, especially with
huge improvements on style retention (average $+20.82\%\uparrow$).</div><div><a href='http://arxiv.org/abs/2402.08631v2'>2402.08631v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14835v1")'>MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge
  Editing</div>
<div id='2402.14835v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T07:15:03Z</div><div>Authors: Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen, Nan Hu, Guilin Qi, Haiyun Jiang, Siyuan Cheng, Bozhong Tian</div><div style='padding-top: 10px; width: 80ex'>Multimodal knowledge editing represents a critical advancement in enhancing
the capabilities of Multimodal Large Language Models (MLLMs). Despite its
potential, current benchmarks predominantly focus on coarse-grained knowledge,
leaving the intricacies of fine-grained (FG) multimodal entity knowledge
largely unexplored. This gap presents a notable challenge, as FG entity
recognition is pivotal for the practical deployment and effectiveness of MLLMs
in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a
comprehensive benchmark and dataset specifically designed for the FG multimodal
entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess
different perspectives, including Vanilla Name Answering, Entity-Level Caption,
and Complex-Scenario Recognition. In addition, a new form of knowledge editing,
Multi-step Editing, is introduced to evaluate the editing efficiency. Through
our extensive evaluations, we demonstrate that the current state-of-the-art
methods face significant challenges in tackling our proposed benchmark,
underscoring the complexity of FG knowledge editing in MLLMs. Our findings
spotlight the urgent need for novel approaches in this domain, setting a clear
agenda for future research and development efforts within the community.</div><div><a href='http://arxiv.org/abs/2402.14835v1'>2402.14835v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06259v1")'>Editing Conceptual Knowledge for Large Language Models</div>
<div id='2403.06259v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T16:57:10Z</div><div>Authors: Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been a growing interest in knowledge editing for Large
Language Models (LLMs). Current approaches and evaluations merely explore the
instance-level editing, while whether LLMs possess the capability to modify
concepts remains unclear. This paper pioneers the investigation of editing
conceptual knowledge for LLMs, by constructing a novel benchmark dataset
ConceptEdit and establishing a suite of new metrics for evaluation. The
experimental results reveal that, although existing editing methods can
efficiently modify concept-level definition to some extent, they also have the
potential to distort the related instantial knowledge in LLMs, leading to poor
performance. We anticipate this can inspire further progress in better
understanding LLMs. Our project homepage is available at
https://zjunlp.github.io/project/ConceptEdit.</div><div><a href='http://arxiv.org/abs/2403.06259v1'>2403.06259v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11441v1")'>InfuserKI: Enhancing Large Language Models with Knowledge Graphs via
  Infuser-Guided Knowledge Integration</div>
<div id='2402.11441v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T03:36:26Z</div><div>Authors: Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, Haifeng Chen</div><div style='padding-top: 10px; width: 80ex'>Though Large Language Models (LLMs) have shown remarkable open-generation
capabilities across diverse domains, they struggle with knowledge-intensive
tasks. To alleviate this issue, knowledge integration methods have been
proposed to enhance LLMs with domain-specific knowledge graphs using external
modules. However, they suffer from data inefficiency as they require both known
and unknown knowledge for fine-tuning. Thus, we study a novel problem of
integrating unknown knowledge into LLMs efficiently without unnecessary overlap
of known knowledge. Injecting new knowledge poses the risk of forgetting
previously acquired knowledge. To tackle this, we propose a novel
Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes
transformer internal states to determine whether to enhance the original LLM
output with additional information, thereby effectively mitigating knowledge
forgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs
demonstrate that InfuserKI can effectively acquire new knowledge and outperform
state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge
forgetting.</div><div><a href='http://arxiv.org/abs/2402.11441v1'>2402.11441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07969v2")'>KnowCoder: Coding Structured Knowledge into LLMs for Universal
  Information Extraction</div>
<div id='2403.07969v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T14:56:34Z</div><div>Authors: Zixuan Li, Yutao Zeng, Yuxin Zuo, Weicheng Ren, Wenxuan Liu, Miao Su, Yucan Guo, Yantao Liu, Xiang Li, Zhilei Hu, Long Bai, Wei Li, Yidan Liu, Pan Yang, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct
Universal Information Extraction (UIE) via code generation. KnowCoder aims to
develop a kind of unified schema representation that LLMs can easily understand
and an effective learning framework that encourages LLMs to follow schemas and
extract structured knowledge accurately. To achieve these, KnowCoder introduces
a code-style schema representation method to uniformly transform different
schemas into Python classes, with which complex schema information, such as
constraints among tasks in UIE, can be captured in an LLM-friendly manner. We
further construct a code-style schema library covering over $\textbf{30,000}$
types of knowledge, which is the largest one for UIE, to the best of our
knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase
learning framework that enhances its schema understanding ability via code
pretraining and its schema following ability via instruction tuning. After code
pretraining on around $1.5$B automatically constructed data, KnowCoder already
attains remarkable generalization ability and achieves relative improvements by
$\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting. After
instruction tuning, KnowCoder further exhibits strong generalization ability on
unseen schemas and achieves up to $\textbf{12.5%}$ and $\textbf{21.9%}$,
compared to sota baselines, under the zero-shot setting and the low resource
setting, respectively. Additionally, based on our unified schema
representations, various human-annotated datasets can simultaneously be
utilized to refine KnowCoder, which achieves significant improvements up to
$\textbf{7.5%}$ under the supervised setting.</div><div><a href='http://arxiv.org/abs/2403.07969v2'>2403.07969v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11078v2")'>Model Editing by Pure Fine-Tuning</div>
<div id='2402.11078v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T21:10:33Z</div><div>Authors: Govind Gangadhar, Karl Stratos</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning is dismissed as not effective for model editing due to its poor
performance compared to more specialized methods. However, fine-tuning is
simple, agnostic to the architectural details of the model being edited, and
able to leverage ongoing advances in standard training methods (e.g., PEFT),
making it an appealing choice for a model editor. In this work, we show that
pure fine-tuning can be a viable approach to model editing. We propose a slight
modification of naive fine-tuning with two key ingredients. First, we optimize
the conditional likelihood rather than the full likelihood. Second, we augment
the data with random paraphrases and facts to encourage generalization and
locality. Our experiments on ZsRE and CounterFact show that this simple
modification allows fine-tuning to often match or outperform specialized
editors in the edit score.</div><div><a href='http://arxiv.org/abs/2402.11078v2'>2402.11078v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02364v1")'>The Developmental Landscape of In-Context Learning</div>
<div id='2402.02364v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T06:23:05Z</div><div>Authors: Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, Daniel Murfet</div><div style='padding-top: 10px; width: 80ex'>We show that in-context learning emerges in transformers in discrete
developmental stages, when they are trained on either language modeling or
linear regression tasks. We introduce two methods for detecting the milestones
that separate these stages, by probing the geometry of the population loss in
both parameter space and function space. We study the stages revealed by these
new methods using a range of behavioral and structural metrics to establish
their validity.</div><div><a href='http://arxiv.org/abs/2402.02364v1'>2402.02364v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08674v1")'>Human Curriculum Effects Emerge with In-Context Learning in Neural
  Networks</div>
<div id='2402.08674v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:55:27Z</div><div>Authors: Jacob Russin, Ellie Pavlick, Michael J. Frank</div><div style='padding-top: 10px; width: 80ex'>Human learning is sensitive to rule-like structure and the curriculum of
examples used for training. In tasks governed by succinct rules, learning is
more robust when related examples are blocked across trials, but in the absence
of such rules, interleaving is more effective. To date, no neural model has
simultaneously captured these seemingly contradictory effects. Here we show
that this same tradeoff spontaneously emerges with "in-context learning" (ICL)
both in neural networks trained with metalearning and in large language models
(LLMs). ICL is the ability to learn new tasks "in context" - without weight
changes - via an inner-loop algorithm implemented in activation dynamics.
Experiments with pretrained LLMs and metalearning transformers show that ICL
exhibits the blocking advantage demonstrated in humans on a task involving
rule-like structure, and conversely, that concurrent in-weight learning
reproduces the interleaving advantage observed in humans on tasks lacking such
structure.</div><div><a href='http://arxiv.org/abs/2402.08674v1'>2402.08674v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15530v1")'>An Information-Theoretic Analysis of In-Context Learning</div>
<div id='2401.15530v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T00:36:44Z</div><div>Authors: Hong Jun Jeon, Jason D. Lee, Qi Lei, Benjamin Van Roy</div><div style='padding-top: 10px; width: 80ex'>Previous theoretical results pertaining to meta-learning on sequences build
on contrived assumptions and are somewhat convoluted. We introduce new
information-theoretic tools that lead to an elegant and very general
decomposition of error into three components: irreducible error, meta-learning
error, and intra-task error. These tools unify analyses across many
meta-learning challenges. To illustrate, we apply them to establish new results
about in-context learning with transformers. Our theoretical results
characterizes how error decays in both the number of training sequences and
sequence lengths. Our results are very general; for example, they avoid
contrived mixing time assumptions made by all prior results that establish
decay of error with sequence length.</div><div><a href='http://arxiv.org/abs/2401.15530v1'>2401.15530v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03170v1")'>Is Mamba Capable of In-Context Learning?</div>
<div id='2402.03170v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:39:12Z</div><div>Authors: Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank Hutter</div><div style='padding-top: 10px; width: 80ex'>This work provides empirical evidence that Mamba, a newly proposed selective
structured state space model, has similar in-context learning (ICL)
capabilities as transformers. We evaluated Mamba on tasks involving simple
function approximation as well as more complex natural language processing
problems. Our results demonstrate that across both categories of tasks, Mamba
matches the performance of transformer models for ICL. Further analysis reveals
that like transformers, Mamba appears to solve ICL problems by incrementally
optimizing its internal representations. Overall, our work suggests that Mamba
can be an efficient alternative to transformers for ICL tasks involving longer
input sequences.</div><div><a href='http://arxiv.org/abs/2402.03170v1'>2402.03170v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05097v1")'>Any-Way Meta Learning</div>
<div id='2401.05097v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T12:00:53Z</div><div>Authors: Junhoo Lee, Yearim Kim, Hyunho Lee, Nojun Kwak</div><div style='padding-top: 10px; width: 80ex'>Although meta-learning seems promising performance in the realm of rapid
adaptability, it is constrained by fixed cardinality. When faced with tasks of
varying cardinalities that were unseen during training, the model lacks its
ability. In this paper, we address and resolve this challenge by harnessing
`label equivalence' emerged from stochastic numeric label assignments during
episodic task sampling. Questioning what defines ``true" meta-learning, we
introduce the ``any-way" learning paradigm, an innovative model training
approach that liberates model from fixed cardinality constraints. Surprisingly,
this model not only matches but often outperforms traditional fixed-way models
in terms of performance, convergence speed, and stability. This disrupts
established notions about domain generalization. Furthermore, we argue that the
inherent label equivalence naturally lacks semantic information. To bridge this
semantic information gap arising from label equivalence, we further propose a
mechanism for infusing semantic class information into the model. This would
enhance the model's comprehension and functionality. Experiments conducted on
renowned architectures like MAML and ProtoNet affirm the effectiveness of our
method.</div><div><a href='http://arxiv.org/abs/2401.05097v1'>2401.05097v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14953v1")'>Learning Universal Predictors</div>
<div id='2401.14953v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T15:37:16Z</div><div>Authors: Jordi Grau-Moya, Tim Genewein, Marcus Hutter, Laurent Orseau, Grégoire Delétang, Elliot Catt, Anian Ruoss, Li Kevin Wenliang, Christopher Mattern, Matthew Aitchison, Joel Veness</div><div style='padding-top: 10px; width: 80ex'>Meta-learning has emerged as a powerful approach to train neural networks to
learn new tasks quickly from limited data. Broad exposure to different tasks
leads to versatile representations enabling general problem solving. But, what
are the limits of meta-learning? In this work, we explore the potential of
amortizing the most powerful universal predictor, namely Solomonoff Induction
(SI), into neural networks via leveraging meta-learning to its limits. We use
Universal Turing Machines (UTMs) to generate training data used to expose
networks to a broad range of patterns. We provide theoretical analysis of the
UTM data generation processes and meta-training protocols. We conduct
comprehensive experiments with neural architectures (e.g. LSTMs, Transformers)
and algorithmic data generators of varying complexity and universality. Our
results suggest that UTM data is a valuable resource for meta-learning, and
that it can be used to train neural networks capable of learning universal
prediction strategies.</div><div><a href='http://arxiv.org/abs/2401.14953v1'>2401.14953v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09053v1")'>Towards a theory of model distillation</div>
<div id='2403.09053v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T02:42:19Z</div><div>Authors: Enric Boix-Adsera</div><div style='padding-top: 10px; width: 80ex'>Distillation is the task of replacing a complicated machine learning model
with a simpler model that approximates the original [BCNM06,HVD15]. Despite
many practical applications, basic questions about the extent to which models
can be distilled, and the runtime and amount of data needed to distill, remain
largely open.
  To study these questions, we initiate a general theory of distillation,
defining PAC-distillation in an analogous way to PAC-learning [Val84]. As
applications of this theory: (1) we propose new algorithms to extract the
knowledge stored in the trained weights of neural networks -- we show how to
efficiently distill neural networks into succinct, explicit decision tree
representations when possible by using the ``linear representation
hypothesis''; and (2) we prove that distillation can be much cheaper than
learning from scratch, and make progress on characterizing its complexity.</div><div><a href='http://arxiv.org/abs/2403.09053v1'>2403.09053v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04720v2")'>Rethinking of Encoder-based Warm-start Methods in Hyperparameter
  Optimization</div>
<div id='2403.04720v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:16:29Z</div><div>Authors: Dawid Płudowski, Antoni Zajko, Anna Kozak, Katarzyna Woźnica</div><div style='padding-top: 10px; width: 80ex'>Effectively representing heterogeneous tabular datasets for meta-learning
remains an open problem. Previous approaches rely on predefined meta-features,
for example, statistical measures or landmarkers. Encoder-based models, such as
Dataset2Vec, allow us to extract significant meta-features automatically
without human intervention. This research introduces a novel encoder-based
representation of tabular datasets implemented within the liltab package
available on GitHub https://github.com/azoz01/liltab. Our package is based on
an established model for heterogeneous tabular data proposed in [Tomoharu Iwata
and Atsutoshi Kumagai. Meta-learning from Tasks with Heterogeneous Attribute
Spaces. In Advances in Neural Information Processing Systems, 2020]. The
proposed approach employs a different model for encoding feature relationships,
generating alternative representations compared to existing methods like
Dataset2Vec. Both of them leverage the fundamental assumption of dataset
similarity learning. In this work, we evaluate Dataset2Vec and liltab on two
common meta-tasks - representing entire datasets and hyperparameter
optimization warm-start. However, validation on an independent metaMIMIC
dataset highlights the nuanced challenges in representation learning. We show
that general representations may not suffice for some meta-tasks where
requirements are not explicitly considered during extraction.</div><div><a href='http://arxiv.org/abs/2403.04720v2'>2403.04720v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06768v1")'>XB-MAML: Learning Expandable Basis Parameters for Effective
  Meta-Learning with Wide Task Coverage</div>
<div id='2403.06768v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T14:37:57Z</div><div>Authors: Jae-Jun Lee, Sung Whan Yoon</div><div style='padding-top: 10px; width: 80ex'>Meta-learning, which pursues an effective initialization model, has emerged
as a promising approach to handling unseen tasks. However, a limitation remains
to be evident when a meta-learner tries to encompass a wide range of task
distribution, e.g., learning across distinctive datasets or domains. Recently,
a group of works has attempted to employ multiple model initializations to
cover widely-ranging tasks, but they are limited in adaptively expanding
initializations. We introduce XB-MAML, which learns expandable basis
parameters, where they are linearly combined to form an effective
initialization to a given task. XB-MAML observes the discrepancy between the
vector space spanned by the basis and fine-tuned parameters to decide whether
to expand the basis. Our method surpasses the existing works in the
multi-domain meta-learning benchmarks and opens up new chances of meta-learning
for obtaining the diverse inductive bias that can be combined to stretch toward
the effective initialization for diverse unseen tasks.</div><div><a href='http://arxiv.org/abs/2403.06768v1'>2403.06768v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00433v1")'>Merging Multi-Task Models via Weight-Ensembling Mixture of Experts</div>
<div id='2402.00433v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T08:58:57Z</div><div>Authors: Anke Tang, Li Shen, Yong Luo, Nan Yin, Lefei Zhang, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Merging various task-specific Transformer-based models trained on different
tasks into a single unified model can execute all the tasks concurrently.
Previous methods, exemplified by task arithmetic, have been proven to be both
effective and scalable. Existing methods have primarily focused on seeking a
static optimal solution within the original model parameter space. A notable
challenge is mitigating the interference between parameters of different
models, which can substantially deteriorate performance. In this paper, we
propose to merge most of the parameters while upscaling the MLP of the
Transformer layers to a weight-ensembling mixture of experts (MoE) module,
which can dynamically integrate shared and task-specific knowledge based on the
input, thereby providing a more flexible solution that can adapt to the
specific needs of each instance. Our key insight is that by identifying and
separating shared knowledge and task-specific knowledge, and then dynamically
integrating them, we can mitigate the parameter interference problem to a great
extent. We conduct the conventional multi-task model merging experiments and
evaluate the generalization and robustness of our method. The results
demonstrate the effectiveness of our method and provide a comprehensive
understanding of our method. The code is available at
https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/</div><div><a href='http://arxiv.org/abs/2402.00433v1'>2402.00433v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11395v2")'>Automated data processing and feature engineering for deep learning and
  big data applications: a survey</div>
<div id='2403.11395v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T01:07:48Z</div><div>Authors: Alhassan Mumuni, Fuseini Mumuni</div><div style='padding-top: 10px; width: 80ex'>Modern approach to artificial intelligence (AI) aims to design algorithms
that learn directly from data. This approach has achieved impressive results
and has contributed significantly to the progress of AI, particularly in the
sphere of supervised deep learning. It has also simplified the design of
machine learning systems as the learning process is highly automated. However,
not all data processing tasks in conventional deep learning pipelines have been
automated. In most cases data has to be manually collected, preprocessed and
further extended through data augmentation before they can be effective for
training. Recently, special techniques for automating these tasks have emerged.
The automation of data processing tasks is driven by the need to utilize large
volumes of complex, heterogeneous data for machine learning and big data
applications. Today, end-to-end automated data processing systems based on
automated machine learning (AutoML) techniques are capable of taking raw data
and transforming them into useful features for Big Data tasks by automating all
intermediate processing stages. In this work, we present a thorough review of
approaches for automating data processing tasks in deep learning pipelines,
including automated data preprocessing--e.g., data cleaning, labeling, missing
data imputation, and categorical data encoding--as well as data augmentation
(including synthetic data generation using generative AI methods) and feature
engineering--specifically, automated feature extraction, feature construction
and feature selection. In addition to automating specific data processing
tasks, we discuss the use of AutoML methods and tools to simultaneously
optimize all stages of the machine learning pipeline.</div><div><a href='http://arxiv.org/abs/2403.11395v2'>2403.11395v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03139v1")'>Enhancing Neural Subset Selection: Integrating Background Information
  into Set Representations</div>
<div id='2402.03139v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:09:35Z</div><div>Authors: Binghui Xie, Yatao Bian, Kaiwen zhou, Yongqiang Chen, Peilin Zhao, Bo Han, Wei Meng, James Cheng</div><div style='padding-top: 10px; width: 80ex'>Learning neural subset selection tasks, such as compound selection in
AI-aided drug discovery, have become increasingly pivotal across diverse
applications. The existing methodologies in the field primarily concentrate on
constructing models that capture the relationship between utility function
values and subsets within their respective supersets. However, these approaches
tend to overlook the valuable information contained within the superset when
utilizing neural networks to model set functions. In this work, we address this
oversight by adopting a probabilistic perspective. Our theoretical findings
demonstrate that when the target value is conditioned on both the input set and
subset, it is essential to incorporate an \textit{invariant sufficient
statistic} of the superset into the subset of interest for effective learning.
This ensures that the output value remains invariant to permutations of the
subset and its corresponding superset, enabling identification of the specific
superset from which the subset originated. Motivated by these insights, we
propose a simple yet effective information aggregation module designed to merge
the representations of subsets and supersets from a permutation invariance
perspective. Comprehensive empirical evaluations across diverse tasks and
datasets validate the enhanced efficacy of our approach over conventional
methods, underscoring the practicality and potency of our proposed strategies
in real-world contexts.</div><div><a href='http://arxiv.org/abs/2402.03139v1'>2402.03139v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00396v1")'>Efficient Exploration for LLMs</div>
<div id='2402.00396v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T07:32:24Z</div><div>Authors: Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy</div><div style='padding-top: 10px; width: 80ex'>We present evidence of substantial benefit from efficient exploration in
gathering human feedback to improve large language models. In our experiments,
an agent sequentially generates queries while fitting a reward model to the
feedback received. Our best-performing agent generates queries using double
Thompson sampling, with uncertainty represented by an epistemic neural network.
Our results demonstrate that efficient exploration enables high levels of
performance with far fewer queries. Further, both uncertainty estimation and
the choice of exploration scheme play critical roles.</div><div><a href='http://arxiv.org/abs/2402.00396v1'>2402.00396v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03142v1")'>Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for
  Large Language Models</div>
<div id='2402.03142v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:11:43Z</div><div>Authors: Michele Mastromattei, Fabio Massimo Zanzotto</div><div style='padding-top: 10px; width: 80ex'>Neural network pruning has become increasingly crucial due to the complexity
of neural network models and their widespread use in various fields. Existing
pruning algorithms often suffer from limitations such as architecture
specificity, excessive complexity and reliance on complex calculations,
rendering them impractical for real-world applications. In this paper, we
propose KEN: a straightforward, universal and unstructured pruning algorithm
based on Kernel Density Estimation (KDE). KEN aims to construct optimized
transformer models by selectively preserving the most significant parameters
while restoring others to their pre-training state. This approach maintains
model performance while allowing storage of only the optimized subnetwork,
leading to significant memory savings. Extensive evaluations on seven
transformer models demonstrate that KEN achieves equal or better performance
than the original models with a minimum parameter reduction of 25%. In-depth
comparisons against other pruning and PEFT algorithms confirm KEN
effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that
visualizes the optimized model composition and the subnetwork selected by KEN.</div><div><a href='http://arxiv.org/abs/2402.03142v1'>2402.03142v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17762v1")'>Massive Activations in Large Language Models</div>
<div id='2402.17762v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:55:17Z</div><div>Authors: Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu</div><div style='padding-top: 10px; width: 80ex'>We observe an empirical phenomenon in Large Language Models (LLMs) -- very
few activations exhibit significantly larger values than others (e.g., 100,000
times larger). We call them massive activations. First, we demonstrate the
widespread existence of massive activations across various LLMs and
characterize their locations. Second, we find their values largely stay
constant regardless of the input, and they function as indispensable bias terms
in LLMs. Third, these massive activations lead to the concentration of
attention probabilities to their corresponding tokens, and further, implicit
bias terms in the self-attention output. Last, we also study massive
activations in Vision Transformers.</div><div><a href='http://arxiv.org/abs/2402.17762v1'>2402.17762v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07901v1")'>FAST: Factorizable Attention for Speeding up Transformers</div>
<div id='2402.07901v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:59:39Z</div><div>Authors: Armin Gerami, Monte Hoover, Pranav S. Dulepet, Ramani Duraiswami</div><div style='padding-top: 10px; width: 80ex'>Motivated by the factorization inherent in the original fast multipole method
and the improved fast Gauss transform we introduce a factorable form of
attention that operates efficiently in high dimensions. This approach reduces
the computational and memory complexity of the attention mechanism in
transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our
work presents a linearly scaled attention mechanism that maintains the full
representation of the attention matrix without compromising on sparsification
and incorporates the all-to-all relationship between tokens. We explore the
properties of our new attention metric and conduct tests in various standard
settings. Results indicate that our attention mechanism has a robust
performance and holds significant promise for diverse applications where
self-attention is used.</div><div><a href='http://arxiv.org/abs/2402.07901v1'>2402.07901v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04690v2")'>Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level</div>
<div id='2403.04690v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T17:35:58Z</div><div>Authors: Ali Hassani, Wen-Mei Hwu, Humphrey Shi</div><div style='padding-top: 10px; width: 80ex'>Neighborhood attention reduces the cost of self attention by restricting each
token's attention span to its nearest neighbors. This restriction,
parameterized by a window size and dilation factor, draws a spectrum of
possible attention patterns between linear projection and self attention.
Neighborhood attention, and more generally sliding window attention patterns,
have long been bounded by infrastructure, particularly in higher-rank spaces
(2-D and 3-D), calling for the development of custom kernels, which have been
limited in either functionality, or performance, if not both. In this work, we
first show that neighborhood attention can be represented as a batched GEMM
problem, similar to standard attention, and implement it for 1-D and 2-D
neighborhood attention. These kernels on average provide 895% and 272%
improvement in full precision latency compared to existing naive kernels for
1-D and 2-D neighborhood attention respectively. We find certain inherent
inefficiencies in all unfused neighborhood attention kernels that bound their
performance and lower-precision scalability. We also developed fused
neighborhood attention; an adaptation of fused dot-product attention kernels
that allow fine-grained control over attention across different spatial axes.
Known for reducing the quadratic time complexity of self attention to a linear
complexity, neighborhood attention can now enjoy a reduced and constant memory
footprint, and record-breaking half precision latency. We observe that our
fused kernels successfully circumvent some of the unavoidable inefficiencies in
unfused implementations. While our unfused GEMM-based kernels only improve half
precision performance compared to naive kernels by an average of 496% and 113%
in 1-D and 2-D problems respectively, our fused kernels improve naive kernels
by an average of 1607% and 581% in 1-D and 2-D problems respectively.</div><div><a href='http://arxiv.org/abs/2403.04690v2'>2403.04690v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07443v1")'>The I/O Complexity of Attention, or How Optimal is Flash Attention?</div>
<div id='2402.07443v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T06:50:45Z</div><div>Authors: Barna Saha, Christopher Ye</div><div style='padding-top: 10px; width: 80ex'>Self-attention is at the heart of the popular Transformer architecture, yet
suffers from quadratic time and memory complexity. The breakthrough
FlashAttention algorithm revealed I/O complexity as the true bottleneck in
scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g.
GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O
complexity measures the number of accesses to memory. FlashAttention computes
attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of
the attention matrix, $d$ the head-dimension and $M$ the cache size. However,
is this I/O complexity optimal? The known lower bound only rules out an I/O
complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be
written to slow memory is $\Omega(Nd)$. This leads to the main question of our
work: Is FlashAttention I/O optimal for all values of $M$?
  We resolve the above question in its full generality by showing an I/O
complexity lower bound that matches the upper bound provided by FlashAttention
for any values of $M \geq d^2$ within any constant factors. Further, we give a
better algorithm with lower I/O complexity for $M &lt; d^2$, and show that it is
optimal as well. Moreover, our lower bounds do not rely on using combinatorial
matrix multiplication for computing the attention matrix. We show even if one
uses fast matrix multiplication, the above I/O complexity bounds cannot be
improved. We do so by introducing a new communication complexity protocol for
matrix compression, and connecting communication complexity to I/O complexity.
To the best of our knowledge, this is the first work to establish a connection
between communication complexity and I/O complexity, and we believe this
connection could be of independent interest and will find many more
applications in proving I/O complexity lower bounds in the future.</div><div><a href='http://arxiv.org/abs/2402.07443v1'>2402.07443v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10266v1")'>DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</div>
<div id='2403.10266v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T12:53:50Z</div><div>Authors: Xuanlei Zhao, Shenggan Cheng, Zangwei Zheng, Zheming Yang, Ziming Liu, Yang You</div><div style='padding-top: 10px; width: 80ex'>Scaling large models with long sequences across applications like language
generation, video generation and multimodal tasks requires efficient sequence
parallelism. However, existing sequence parallelism methods all assume a single
sequence dimension and fail to adapt to multi-dimensional transformer
architectures that perform attention calculations across different dimensions.
This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to
enable efficient sequence parallelism for multi-dimensional transformer models.
The key idea is to dynamically switch the parallelism dimension according to
the current computation stage, leveraging the potential characteristics of
multi-dimensional attention. This dynamic dimension switching allows sequence
parallelism with minimal communication overhead compared to applying
traditional single-dimension parallelism to multi-dimensional models.
Experiments show DSP improves end-to-end throughput by 42.0% to 216.8% over
prior sequence parallelism methods.</div><div><a href='http://arxiv.org/abs/2403.10266v1'>2403.10266v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02098v1")'>Self-attention Networks Localize When QK-eigenspectrum Concentrates</div>
<div id='2402.02098v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T09:35:53Z</div><div>Authors: Han Bao, Ryuichiro Hataya, Ryo Karakida</div><div style='padding-top: 10px; width: 80ex'>The self-attention mechanism prevails in modern machine learning. It has an
interesting functionality of adaptively selecting tokens from an input sequence
by modulating the degree of attention localization, which many researchers
speculate is the basis of the powerful model performance but complicates the
underlying mechanism of the learning dynamics. In recent years, mainly two
arguments have connected attention localization to the model performances. One
is the rank collapse, where the embedded tokens by a self-attention block
become very similar across different tokens, leading to a less expressive
network. The other is the entropy collapse, where the attention probability
approaches non-uniform and entails low entropy, making the learning dynamics
more likely to be trapped in plateaus. These two failure modes may apparently
contradict each other because the rank and entropy collapses are relevant to
uniform and non-uniform attention, respectively. To this end, we characterize
the notion of attention localization by the eigenspectrum of query-key
parameter matrices and reveal that a small eigenspectrum variance leads
attention to be localized. Interestingly, the small eigenspectrum variance
prevents both rank and entropy collapse, leading to better model expressivity
and trainability.</div><div><a href='http://arxiv.org/abs/2402.02098v1'>2402.02098v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19449v1")'>Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent
  on Language Models</div>
<div id='2402.19449v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:47:52Z</div><div>Authors: Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, Alberto Bietti</div><div style='padding-top: 10px; width: 80ex'>Adam has been shown to outperform gradient descent in optimizing large
language transformers empirically, and by a larger margin than on other tasks,
but it is unclear why this happens. We show that the heavy-tailed class
imbalance found in language modeling tasks leads to difficulties in the
optimization dynamics. When training with gradient descent, the loss associated
with infrequent words decreases slower than the loss associated with frequent
ones. As most samples come from relatively infrequent words, the average loss
decreases slowly with gradient descent. On the other hand, Adam and sign-based
methods do not suffer from this problem and improve predictions on all classes.
To establish that this behavior is indeed caused by class imbalance, we show
empirically that it persist through different architectures and data types, on
language transformers, vision CNNs, and linear models. We further study this
phenomenon on a linear classification with cross-entropy loss, showing that
heavy-tailed class imbalance leads to ill-conditioning, and that the
normalization used by Adam can counteract it.</div><div><a href='http://arxiv.org/abs/2402.19449v1'>2402.19449v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02648v1")'>Remove that Square Root: A New Efficient Scale-Invariant Version of
  AdaGrad</div>
<div id='2403.02648v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T04:35:59Z</div><div>Authors: Sayantan Choudhury, Nazarii Tupitsa, Nicolas Loizou, Samuel Horvath, Martin Takac, Eduard Gorbunov</div><div style='padding-top: 10px; width: 80ex'>Adaptive methods are extremely popular in machine learning as they make
learning rate tuning less expensive. This paper introduces a novel optimization
algorithm named KATE, which presents a scale-invariant adaptation of the
well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the
case of Generalized Linear Models. Moreover, for general smooth non-convex
problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}}
\right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also
compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in
numerical experiments with different problems, including complex machine
learning tasks like image classification and text classification on real data.
The results indicate that KATE consistently outperforms AdaGrad and
matches/surpasses the performance of Adam in all considered scenarios.</div><div><a href='http://arxiv.org/abs/2403.02648v1'>2403.02648v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11143v3")'>Gaussian Adaptive Attention is All You Need: Robust Contextual
  Representations Across Multiple Modalities</div>
<div id='2401.11143v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T06:42:32Z</div><div>Authors: Georgios Ioannides, Aman Chadha, Aaron Elkins</div><div style='padding-top: 10px; width: 80ex'>We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM's
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.</div><div><a href='http://arxiv.org/abs/2401.11143v3'>2401.11143v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09419v1")'>Multidimensional Gabor-Like Filters Derived from Gaussian Functions on
  Logarithmic Frequency Axes</div>
<div id='2402.09419v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T08:34:12Z</div><div>Authors: Dherik Devakumar, Ole Christian Eidheim</div><div style='padding-top: 10px; width: 80ex'>A novel wavelet-like function is presented that makes it convenient to create
filter banks given mainly two parameters that influence the focus area and the
filter count. This is accomplished by computing the inverse Fourier transform
of Gaussian functions on logarithmic frequency axes in the frequency domain.
The resulting filters are similar to Gabor filters and represent oriented brief
signal oscillations of different sizes. The wavelet-like function can be
thought of as a generalized Log-Gabor filter that is multidimensional, always
uses Gaussian functions on logarithmic frequency axes, and innately includes
low-pass filters from Gaussian functions located at the frequency domain
origin.</div><div><a href='http://arxiv.org/abs/2402.09419v1'>2402.09419v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15623v1")'>GT-PCA: Effective and Interpretable Dimensionality Reduction with
  General Transform-Invariant Principal Component Analysis</div>
<div id='2401.15623v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T10:29:23Z</div><div>Authors: Florian Heinrichs</div><div style='padding-top: 10px; width: 80ex'>Data analysis often requires methods that are invariant with respect to
specific transformations, such as rotations in case of images or shifts in case
of images and time series. While principal component analysis (PCA) is a
widely-used dimension reduction technique, it lacks robustness with respect to
these transformations. Modern alternatives, such as autoencoders, can be
invariant with respect to specific transformations but are generally not
interpretable. We introduce General Transform-Invariant Principal Component
Analysis (GT-PCA) as an effective and interpretable alternative to PCA and
autoencoders. We propose a neural network that efficiently estimates the
components and show that GT-PCA significantly outperforms alternative methods
in experiments based on synthetic and real data.</div><div><a href='http://arxiv.org/abs/2401.15623v1'>2401.15623v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01282v2")'>Differentiable and accelerated wavelet transforms on the sphere and ball</div>
<div id='2402.01282v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:16:10Z</div><div>Authors: Matthew A. Price, Alicja Polanska, Jessica Whitney, Jason D. McEwen</div><div style='padding-top: 10px; width: 80ex'>Directional wavelet dictionaries are hierarchical representations which
efficiently capture and segment information across scale, location and
orientation. Such representations demonstrate a particular affinity to physical
signals, which often exhibit highly anisotropic, localised multiscale
structure. Many physically important signals are observed over spherical
domains, such as the celestial sky in cosmology. Leveraging recent advances in
computational harmonic analysis, we design new highly distributable and
automatically differentiable directional wavelet transforms on the
$2$-dimensional sphere $\mathbb{S}^2$ and $3$-dimensional ball $\mathbb{B}^3 =
\mathbb{R}^+ \times \mathbb{S}^2$ (the space formed by augmenting the sphere
with the radial half-line). We observe up to a $300$-fold and $21800$-fold
acceleration for signals on the sphere and ball, respectively, compared to
existing software, whilst maintaining 64-bit machine precision. Not only do
these algorithms dramatically accelerate existing spherical wavelet transforms,
the gradient information afforded by automatic differentiation unlocks many
data-driven analysis techniques previously not possible for these spaces. We
publicly release both S2WAV and S2BALL, open-sourced JAX libraries for our
transforms that are automatically differentiable and readily deployable both on
and over clusters of hardware accelerators (e.g. GPUs &amp; TPUs).</div><div><a href='http://arxiv.org/abs/2402.01282v2'>2402.01282v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15195v1")'>The AffectToolbox: Affect Analysis for Everyone</div>
<div id='2402.15195v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:55:47Z</div><div>Authors: Silvan Mertes, Dominik Schiller, Michael Dietz, Elisabeth André, Florian Lingenfelser</div><div style='padding-top: 10px; width: 80ex'>In the field of affective computing, where research continually advances at a
rapid pace, the demand for user-friendly tools has become increasingly
apparent. In this paper, we present the AffectToolbox, a novel software system
that aims to support researchers in developing affect-sensitive studies and
prototypes. The proposed system addresses the challenges posed by existing
frameworks, which often require profound programming knowledge and cater
primarily to power-users or skilled developers. Aiming to facilitate ease of
use, the AffectToolbox requires no programming knowledge and offers its
functionality to reliably analyze the affective state of users through an
accessible graphical user interface. The architecture encompasses a variety of
models for emotion recognition on multiple affective channels and modalities,
as well as an elaborate fusion system to merge multi-modal assessments into a
unified result. The entire system is open-sourced and will be publicly
available to ensure easy integration into more complex applications through a
well-structured, Python-based code base - therefore marking a substantial
contribution toward advancing affective computing research and fostering a more
collaborative and inclusive environment within this interdisciplinary field.</div><div><a href='http://arxiv.org/abs/2402.15195v1'>2402.15195v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15743v1")'>Real-time EEG-based Emotion Recognition Model using Principal Component
  Analysis and Tree-based Models for Neurohumanities</div>
<div id='2401.15743v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T20:02:13Z</div><div>Authors: Miguel A. Blanco-Rios, Milton O. Candela-Leal, Cecilia Orozco-Romo, Paulina Remis-Serna, Carol S. Velez-Saboya, Jorge De-J. Lozoya-Santos, Manuel Cebral-Loureda, Mauricio A. Ramirez-Moreno</div><div style='padding-top: 10px; width: 80ex'>Within the field of Humanities, there is a recognized need for educational
innovation, as there are currently no reported tools available that enable
individuals to interact with their environment to create an enhanced learning
experience in the humanities (e.g., immersive spaces). This project proposes a
solution to address this gap by integrating technology and promoting the
development of teaching methodologies in the humanities, specifically by
incorporating emotional monitoring during the learning process of humanistic
context inside an immersive space. In order to achieve this goal, a real-time
emotion detection EEG-based system was developed to interpret and classify
specific emotions. These emotions aligned with the early proposal by Descartes
(Passions), including admiration, love, hate, desire, joy, and sadness. This
system aims to integrate emotional data into the Neurohumanities Lab
interactive platform, creating a comprehensive and immersive learning
environment. This work developed a ML, real-time emotion detection model that
provided Valence, Arousal, and Dominance (VAD) estimations every 5 seconds.
Using PCA, PSD, RF, and Extra-Trees, the best 8 channels and their respective
best band powers were extracted; furthermore, multiple models were evaluated
using shift-based data division and cross-validations. After assessing their
performance, Extra-Trees achieved a general accuracy of 96%, higher than the
reported in the literature (88% accuracy). The proposed model provided
real-time predictions of VAD variables and was adapted to classify Descartes'
six main passions. However, with the VAD values obtained, more than 15 emotions
can be classified (reported in the VAD emotion mapping) and extend the range of
this application.</div><div><a href='http://arxiv.org/abs/2401.15743v1'>2401.15743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08309v1")'>Anchor function: a type of benchmark functions for studying language
  models</div>
<div id='2401.08309v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T12:10:49Z</div><div>Authors: Zhongwang Zhang, Zhiwei Wang, Junjie Yao, Zhangchen Zhou, Xiaolong Li, Weinan E, Zhi-Qin John Xu</div><div style='padding-top: 10px; width: 80ex'>Understanding transformer-based language models is becoming increasingly
crucial, particularly as they play pivotal roles in advancing towards
artificial general intelligence. However, language model research faces
significant challenges, especially for academic research groups with
constrained resources. These challenges include complex data structures,
unknown target functions, high computational costs and memory requirements, and
a lack of interpretability in the inference process, etc. Drawing a parallel to
the use of simple models in scientific research, we propose the concept of an
anchor function. This is a type of benchmark function designed for studying
language models in learning tasks that follow an "anchor-key" pattern. By
utilizing the concept of an anchor function, we can construct a series of
functions to simulate various language tasks. The anchor function plays a role
analogous to that of mice in diabetes research, particularly suitable for
academic research. We demonstrate the utility of the anchor function with an
example, revealing two basic operations by attention structures in language
models: shifting tokens and broadcasting one token from one position to many
positions. These operations are also commonly observed in large language
models. The anchor function framework, therefore, opens up a series of valuable
and accessible research questions for further exploration, especially for
theoretical study.</div><div><a href='http://arxiv.org/abs/2401.08309v1'>2401.08309v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15175v2")'>Unified View of Grokking, Double Descent and Emergent Abilities: A
  Perspective from Circuits Competition</div>
<div id='2402.15175v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:14:36Z</div><div>Authors: Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun</div><div style='padding-top: 10px; width: 80ex'>Recent studies have uncovered intriguing phenomena in deep learning, such as
grokking, double descent, and emergent abilities in large language models,
which challenge human intuition and are crucial for a deeper understanding of
neural models. In this paper, we present a comprehensive framework that
provides a unified view of these three phenomena, focusing on the competition
between memorization and generalization circuits. This approach, initially
employed to explain grokking, is extended in our work to encompass a wider
range of model sizes and training data volumes. Our framework delineates four
distinct training dynamics, each depending on varying combinations of model
size and training data quantity. Utilizing this framework, we provide a
detailed analysis of the double descent phenomenon and propose two verifiable
predictions regarding its occurrence, both substantiated by our experimental
results. Moreover, we expand our framework to the multi-task learning paradigm,
demonstrating how algorithm tasks can be turned into emergent abilities. This
offers a novel perspective to understand emergent abilities in Large Language
Models.</div><div><a href='http://arxiv.org/abs/2402.15175v2'>2402.15175v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13257v2")'>Arcee's MergeKit: A Toolkit for Merging Large Language Models</div>
<div id='2403.13257v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T02:38:01Z</div><div>Authors: Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz</div><div style='padding-top: 10px; width: 80ex'>The rapid expansion of the open-source language model landscape presents an
opportunity to merge the competencies of these model checkpoints by combining
their parameters. Advances in transfer learning, the process of fine-tuning
pretrained models for specific tasks, has resulted in the development of vast
amounts of task-specific models, typically specialized in individual tasks and
unable to utilize each other's strengths. Model merging facilitates the
creation of multitask models without the need for additional training, offering
a promising avenue for enhancing model performance and versatility. By
preserving the intrinsic capabilities of the original models, model merging
addresses complex challenges in AI - including the difficulties of catastrophic
forgetting and multitask learning. To support this expanding area of research,
we introduce MergeKit, a comprehensive, open-source library designed to
facilitate the application of model merging strategies. MergeKit offers an
extensible framework to efficiently merge models on any hardware, providing
utility to researchers and practitioners. To date, thousands of models have
been merged by the open-source community, leading to the creation of some of
the worlds most powerful open-source model checkpoints, as assessed by the Open
LLM Leaderboard. The library is accessible at
https://github.com/arcee-ai/MergeKit.</div><div><a href='http://arxiv.org/abs/2403.13257v2'>2403.13257v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02412v1")'>LLM Augmented LLMs: Expanding Capabilities through Composition</div>
<div id='2401.02412v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T18:53:01Z</div><div>Authors: Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar</div><div style='padding-top: 10px; width: 80ex'>Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.</div><div><a href='http://arxiv.org/abs/2401.02412v1'>2401.02412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04769v2")'>Using Hallucinations to Bypass GPT4's Filter</div>
<div id='2403.04769v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T17:02:53Z</div><div>Authors: Benjamin Lemkin</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are initially trained on vast amounts of data,
then fine-tuned using reinforcement learning from human feedback (RLHF); this
also serves to teach the LLM to provide appropriate and safe responses. In this
paper, we present a novel method to manipulate the fine-tuned version into
reverting to its pre-RLHF behavior, effectively erasing the model's filters;
the exploit currently works for GPT4, Claude Sonnet, and (to some extent) for
Inflection-2.5. Unlike other jailbreaks (for example, the popular "Do Anything
Now" (DAN) ), our method does not rely on instructing the LLM to override its
RLHF policy; hence, simply modifying the RLHF process is unlikely to address
it. Instead, we induce a hallucination involving reversed text during which the
model reverts to a word bucket, effectively pausing the model's filter. We
believe that our exploit presents a fundamental vulnerability in LLMs currently
unaddressed, as well as an opportunity to better understand the inner workings
of LLMs during hallucinations.</div><div><a href='http://arxiv.org/abs/2403.04769v2'>2403.04769v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18551v1")'>Implicit Bias of Next-Token Prediction</div>
<div id='2402.18551v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T18:34:53Z</div><div>Authors: Christos Thrampoulidis</div><div style='padding-top: 10px; width: 80ex'>Next-token prediction (NTP), the go-to training paradigm in training large
language models, involves predicting the next token in a sequence. Departing
from traditional one-hot classification, in NTP, multiple tokens with varying
frequencies follow each given context. This work frames NTP training as
cross-entropy minimization over distinct contexts, each associated with a
sparse empirical probability vector across a finite vocabulary. It then
addresses the following question: do gradient-based optimizers exhibit a bias
towards solutions with specific structure as the NTP training loss reaches its
lower bound (entropy)? Specifically, for linear NTP models trained using
gradient descent (GD), we make the following contributions: Firstly, we
determine NTP-separability conditions on the data, under which GD can attain
its lower bound. We also demonstrate that these conditions hold under
overparameterization. Secondly, we establish that the parameters of GD
projected onto an appropriate data subspace converge to the unique solution of
a system of linear equations, which requires the logits' difference of
in-support tokens to be equal to the log-ratio of their respective
probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge
and converge in the direction of the solution of a max-margin quadratic
program, minimizing the Euclidean norm of parameters satisfying the
\NTP-separability conditions. Akin to prior research on implicit bias of
one-hot classification, our work opens exciting avenues for future research
that can lead to better understanding optimization, generalization and
robustness principles of models trained with NTP.</div><div><a href='http://arxiv.org/abs/2402.18551v1'>2402.18551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07712v1")'>Model Collapse Demystified: The Case of Regression</div>
<div id='2402.07712v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:26:01Z</div><div>Authors: Elvis Dohmatob, Yunzhen Feng, Julia Kempe</div><div style='padding-top: 10px; width: 80ex'>In the era of large language models like ChatGPT, the phenomenon of "model
collapse" refers to the situation whereby as a model is trained recursively on
data generated from previous generations of itself over time, its performance
degrades until the model eventually becomes completely useless, i.e the model
collapses. In this work, we study this phenomenon in the simplified setting of
kernel regression and obtain results which show a clear crossover between where
the model can cope with fake data, and a regime where the model's performance
completely collapses. Under polynomial decaying spectral and source conditions,
we obtain modified scaling laws which exhibit new crossover phenomena from fast
to slow rates. We also propose a simple strategy based on adaptive
regularization to mitigate model collapse. Our theoretical results are
validated with experiments.</div><div><a href='http://arxiv.org/abs/2402.07712v1'>2402.07712v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14897v1")'>Chain-of-Thought Unfaithfulness as Disguised Accuracy</div>
<div id='2402.14897v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:23:53Z</div><div>Authors: Oliver Bentham, Nathan Stringham, Ana Marasović</div><div style='padding-top: 10px; width: 80ex'>Understanding the extent to which Chain-of-Thought (CoT) generations align
with a large language model's (LLM) internal computations is critical for
deciding whether to trust an LLM's output. As a proxy for CoT faithfulness,
arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT
for producing an answer. Within a single family of proprietary models, they
find that LLMs exhibit a scaling-then-inverse-scaling relationship between
model size and their measure of faithfulness, and that a 13 billion parameter
model exhibits increased faithfulness compared to models ranging from 810
million to 175 billion parameters in size. We evaluate whether these results
generalize as a property of all LLMs. We replicate their experimental setup
with three different families of models and, under specific conditions,
successfully reproduce the scaling trends for CoT faithfulness they report.
However, we discover that simply changing the order of answer choices in the
prompt can reduce the metric by 73 percentage points. The faithfulness metric
is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about
its validity as a construct for evaluating faithfulness.</div><div><a href='http://arxiv.org/abs/2402.14897v1'>2402.14897v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13106v1")'>Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying
  Structure of Data</div>
<div id='2403.13106v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:13:22Z</div><div>Authors: Divyansh Singhvi, Andrej Erkelens, Raghav Jain, Diganta Misra, Naomi Saphra</div><div style='padding-top: 10px; width: 80ex'>Measuring nonlinear feature interaction is an established approach to
understanding complex patterns of attribution in many models. In this paper, we
use Shapley Taylor interaction indices (STII) to analyze the impact of
underlying data structure on model representations in a variety of modalities,
tasks, and architectures. Considering linguistic structure in masked and
auto-regressive language models (MLMs and ALMs), we find that STII increases
within idiomatic expressions and that MLMs scale STII with syntactic distance,
relying more on syntax in their nonlinear structure than ALMs do. Our speech
model findings reflect the phonetic principal that the openness of the oral
cavity determines how much a phoneme varies based on its context. Finally, we
study image classifiers and illustrate that feature interactions intuitively
reflect object boundaries. Our wide range of results illustrates the benefits
of interdisciplinary work and domain expertise in interpretability research.</div><div><a href='http://arxiv.org/abs/2403.13106v1'>2403.13106v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07321v1")'>Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs</div>
<div id='2402.07321v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T22:58:49Z</div><div>Authors: Bilal Chughtai, Alan Cooney, Neel Nanda</div><div style='padding-top: 10px; width: 80ex'>How do transformer-based large language models (LLMs) store and retrieve
knowledge? We focus on the most basic form of this task -- factual recall,
where the model is tasked with explicitly surfacing stored facts in prompts of
form `Fact: The Colosseum is in the country of'. We find that the mechanistic
story behind factual recall is more complex than previously thought. It
comprises several distinct, independent, and qualitatively different mechanisms
that additively combine, constructively interfering on the correct attribute.
We term this generic phenomena the additive motif: models compute through
summing up multiple independent contributions. Each mechanism's contribution
may be insufficient alone, but summing results in constructive interfere on the
correct answer. In addition, we extend the method of direct logit attribution
to attribute an attention head's output to individual source tokens. We use
this technique to unpack what we call `mixed heads' -- which are themselves a
pair of two separate additive updates from different source tokens.</div><div><a href='http://arxiv.org/abs/2402.07321v1'>2402.07321v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08919v2")'>Partial Diacritization: A Context-Contrastive Inference Approach</div>
<div id='2401.08919v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T02:04:59Z</div><div>Authors: Muhammad ElNokrashy, Badr AlKhamissi</div><div style='padding-top: 10px; width: 80ex'>Diacritization plays a pivotal role in improving readability and
disambiguating the meaning of Arabic texts. Efforts have so far focused on
marking every eligible character (Full Diacritization). Comparatively
overlooked, Partial Diacritzation (PD) is the selection of a subset of
characters to be marked to aid comprehension where needed. Research has
indicated that excessive diacritic marks can hinder skilled readers--reducing
reading speed and accuracy. We conduct a behavioral experiment and show that
partially marked text is often easier to read than fully marked text, and
sometimes easier than plain text. In this light, we introduce
Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which
integrates seamlessly with existing Arabic diacritization systems. CCPD
processes each word twice, once with context and once without, and diacritizes
only the characters with disparities between the two inferences. Further, we
introduce novel indicators for measuring partial diacritization quality (SR,
PDER, HDER, ERE), essential for establishing this as a machine learning task.
Lastly, we introduce TD2, a Transformer-variant of an established model which
offers a markedly different performance profile on our proposed indicators
compared to all other known systems.</div><div><a href='http://arxiv.org/abs/2401.08919v2'>2401.08919v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17505v2")'>Arrows of Time for Large Language Models</div>
<div id='2401.17505v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T23:46:35Z</div><div>Authors: Vassilis Papadopoulos, Jérémie Wenger, Clément Hongler</div><div style='padding-top: 10px; width: 80ex'>We study the probabilistic modeling performed by Autoregressive Large
Language Models through the angle of time directionality. We empirically find a
time asymmetry exhibited by such models in their ability to model natural
language: a difference in the average log-perplexity when trying to predict the
next token versus when trying to predict the previous one. This difference is
at the same time subtle and very consistent across various modalities
(language, model size, training time, ...). Theoretically, this is surprising:
from an information-theoretic point of view, there should be no such
difference. We provide a theoretical framework to explain how such an asymmetry
can appear from sparsity and computational complexity considerations, and
outline a number of perspectives opened by our results.</div><div><a href='http://arxiv.org/abs/2401.17505v2'>2401.17505v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04005v1")'>On the Efficient Marginalization of Probabilistic Sequence Models</div>
<div id='2403.04005v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T19:29:08Z</div><div>Authors: Alex Boyd</div><div style='padding-top: 10px; width: 80ex'>Real-world data often exhibits sequential dependence, across diverse domains
such as human behavior, medicine, finance, and climate modeling. Probabilistic
methods capture the inherent uncertainty associated with prediction in these
contexts, with autoregressive models being especially prominent. This
dissertation focuses on using autoregressive models to answer complex
probabilistic queries that go beyond single-step prediction, such as the timing
of future events or the likelihood of a specific event occurring before
another. In particular, we develop a broad class of novel and efficient
approximation techniques for marginalization in sequential models that are
model-agnostic. These techniques rely solely on access to and sampling from
next-step conditional distributions of a pre-trained autoregressive model,
including both traditional parametric models as well as more recent neural
autoregressive models. Specific approaches are presented for discrete
sequential models, for marked temporal point processes, and for stochastic jump
processes, each tailored to a well-defined class of informative, long-range
probabilistic queries.</div><div><a href='http://arxiv.org/abs/2403.04005v1'>2403.04005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07206v1")'>Probabilistic Reduced-Dimensional Vector Autoregressive Modeling with
  Oblique Projections</div>
<div id='2401.07206v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T05:38:10Z</div><div>Authors: Yanfang Mo, S. Joe Qin</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a probabilistic reduced-dimensional vector
autoregressive (PredVAR) model to extract low-dimensional dynamics from
high-dimensional noisy data. The model utilizes an oblique projection to
partition the measurement space into a subspace that accommodates the
reduced-dimensional dynamics and a complementary static subspace. An optimal
oblique decomposition is derived for the best predictability regarding
prediction error covariance. Building on this, we develop an iterative PredVAR
algorithm using maximum likelihood and the expectation-maximization (EM)
framework. This algorithm alternately updates the estimates of the latent
dynamics and optimal oblique projection, yielding dynamic latent variables with
rank-ordered predictability and an explicit latent VAR model that is consistent
with the outer projection model. The superior performance and efficiency of the
proposed approach are demonstrated using data sets from a synthesized Lorenz
system and an industrial process from Eastman Chemical.</div><div><a href='http://arxiv.org/abs/2401.07206v1'>2401.07206v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01734v2")'>CFTM: Continuous time fractional topic model</div>
<div id='2402.01734v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:07:41Z</div><div>Authors: Kei Nakagawa, Kohei Hayashi, Yugo Fujimoto</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose the Continuous Time Fractional Topic Model (cFTM),
a new method for dynamic topic modeling. This approach incorporates fractional
Brownian motion~(fBm) to effectively identify positive or negative correlations
in topic and word distribution over time, revealing long-term dependency or
roughness. Our theoretical analysis shows that the cFTM can capture these
long-term dependency or roughness in both topic and word distributions,
mirroring the main characteristics of fBm. Moreover, we prove that the
parameter estimation process for the cFTM is on par with that of LDA,
traditional topic models. To demonstrate the cFTM's property, we conduct
empirical study using economic news articles. The results from these tests
support the model's ability to identify and track long-term dependency or
roughness in topics over time.</div><div><a href='http://arxiv.org/abs/2402.01734v2'>2402.01734v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03176v1")'>Comparison of Topic Modelling Approaches in the Banking Context</div>
<div id='2402.03176v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:43:53Z</div><div>Authors: Bayode Ogunleye, Tonderai Maswera, Laurence Hirsch, Jotham Gaudoin, Teresa Brunsdon</div><div style='padding-top: 10px; width: 80ex'>Topic modelling is a prominent task for automatic topic extraction in many
applications such as sentiment analysis and recommendation systems. The
approach is vital for service industries to monitor their customer discussions.
The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for
topic discovery has shown great performances, however, they are not consistent
in their results as these approaches suffer from data sparseness and inability
to model the word order in a document. Thus, this study presents the use of
Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the
BERTopic architecture. We have prepared a new dataset using tweets from
customers of Nigerian banks and we use this to compare the topic modelling
approaches. Our findings showed KernelPCA and K-means in the BERTopic
architecture-produced coherent topics with a coherence score of 0.8463.</div><div><a href='http://arxiv.org/abs/2402.03176v1'>2402.03176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14290v1")'>CEV-LM: Controlled Edit Vector Language Model for Shaping Natural
  Language Generations</div>
<div id='2402.14290v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T05:07:31Z</div><div>Authors: Samraj Moorjani, Adit Krishnan, Hari Sundaram</div><div style='padding-top: 10px; width: 80ex'>As large-scale language models become the standard for text generation, there
is a greater need to tailor the generations to be more or less concise,
targeted, and informative, depending on the audience/application. Existing
control approaches primarily adjust the semantic (e.g., emotion, topics),
structural (e.g., syntax tree, parts-of-speech), and lexical (e.g.,
keyword/phrase inclusion) properties of text, but are insufficient to
accomplish complex objectives such as pacing which control the complexity and
readability of the text. In this paper, we introduce CEV-LM - a lightweight,
semi-autoregressive language model that utilizes constrained edit vectors to
control three complementary metrics (speed, volume, and circuitousness) that
quantify the shape of text (e.g., pacing of content). We study an extensive set
of state-of-the-art CTG models and find that CEV-LM provides significantly more
targeted and precise control of these three metrics while preserving semantic
content, using less training data, and containing fewer parameters.</div><div><a href='http://arxiv.org/abs/2402.14290v1'>2402.14290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13040v1")'>Text-Guided Molecule Generation with Diffusion Language Model</div>
<div id='2402.13040v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T14:29:02Z</div><div>Authors: Haisong Gong, Qiang Liu, Shu Wu, Liang Wang</div><div style='padding-top: 10px; width: 80ex'>Text-guided molecule generation is a task where molecules are generated to
match specific textual descriptions. Recently, most existing SMILES-based
molecule generation methods rely on an autoregressive architecture. In this
work, we propose the Text-Guided Molecule Generation with Diffusion Language
Model (TGM-DLM), a novel approach that leverages diffusion models to address
the limitations of autoregressive methods. TGM-DLM updates token embeddings
within the SMILES string collectively and iteratively, using a two-phase
diffusion generation process. The first phase optimizes embeddings from random
noise, guided by the text description, while the second phase corrects invalid
SMILES strings to form valid molecular representations. We demonstrate that
TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for
additional data resources. Our findings underscore the remarkable effectiveness
of TGM-DLM in generating coherent and precise molecules with specific
properties, opening new avenues in drug discovery and related scientific
domains. Code will be released at: https://github.com/Deno-V/tgm-dlm.</div><div><a href='http://arxiv.org/abs/2402.13040v1'>2402.13040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02255v1")'>Frequency Explains the Inverse Correlation of Large Language Models'
  Size, Training Data Amount, and Surprisal's Fit to Reading Times</div>
<div id='2402.02255v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T20:22:54Z</div><div>Authors: Byung-Doh Oh, Shisen Yue, William Schuler</div><div style='padding-top: 10px; width: 80ex'>Recent studies have shown that as Transformer-based language models become
larger and are trained on very large amounts of data, the fit of their
surprisal estimates to naturalistic human reading times degrades. The current
work presents a series of analyses showing that word frequency is a key
explanatory factor underlying these two trends. First, residual errors from
four language model families on four corpora show that the inverse correlation
between model size and fit to reading times is the strongest on the subset of
least frequent words, which is driven by excessively accurate predictions of
larger model variants. Additionally, training dynamics reveal that during later
training steps, all model variants learn to predict rare words and that larger
model variants do so more accurately, which explains the detrimental effect of
both training data amount and model size on fit to reading times. Finally, a
feature attribution analysis demonstrates that larger model variants are able
to accurately predict rare words based on both an effectively longer context
window size as well as stronger local associations compared to smaller model
variants. Taken together, these results indicate that Transformer-based
language models' surprisal estimates diverge from human-like expectations due
to the superhumanly complex associations they learn for predicting rare words.</div><div><a href='http://arxiv.org/abs/2402.02255v1'>2402.02255v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09739v1")'>QuRating: Selecting High-Quality Data for Training Language Models</div>
<div id='2402.09739v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T06:36:07Z</div><div>Authors: Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen</div><div style='padding-top: 10px; width: 80ex'>Selecting high-quality pre-training data is important for creating capable
language models, but existing methods rely on simple heuristics. We introduce
QuRating, a method for selecting pre-training data that captures the abstract
qualities of texts which humans intuitively perceive. In this paper, we
investigate four qualities - writing style, required expertise, facts &amp; trivia,
and educational value. We find that LLMs are able to discern these qualities
and observe that they are better at making pairwise judgments of texts than at
rating the quality of a text directly. We train a QuRater model to learn scalar
ratings from pairwise judgments, and use it to annotate a 260B training corpus
with quality ratings for each of the four criteria. In our experiments, we
select 30B tokens according to the different quality ratings and train
1.3B-parameter language models on the selected data. We find that it is
important to balance quality and diversity, as selecting only the highest-rated
documents leads to poor results. When we sample using quality ratings as logits
over documents, our models achieve lower perplexity and stronger in-context
learning performance than baselines. Beyond data selection, we use the quality
ratings to construct a training curriculum which improves performance without
changing the training dataset. We extensively analyze the quality ratings and
discuss their characteristics, biases, and wider implications.</div><div><a href='http://arxiv.org/abs/2402.09739v1'>2402.09739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12492v1")'>Comparing Human-Centered Language Modeling: Is it Better to Model
  Groups, Individual Traits, or Both?</div>
<div id='2401.12492v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T05:20:35Z</div><div>Authors: Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz, Dirk Hovy</div><div style='padding-top: 10px; width: 80ex'>Natural language processing has made progress in incorporating human context
into its models, but whether it is more effective to use group-wise attributes
(e.g., over-45-year-olds) or model individuals remains open. Group attributes
are technically easier but coarse: not all 45-year-olds write the same way. In
contrast, modeling individuals captures the complexity of each person's
identity. It allows for a more personalized representation, but we may have to
model an infinite number of users and require data that may be impossible to
get. We compare modeling human context via group attributes, individual users,
and combined approaches. Combining group and individual features significantly
benefits user-level regression tasks like age estimation or personality
assessment from a user's documents. Modeling individual users significantly
improves the performance of single document-level classification tasks like
stance and topic detection. We also find that individual-user modeling does
well even without user's historical data.</div><div><a href='http://arxiv.org/abs/2401.12492v1'>2401.12492v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05045v1")'>Are Human Conversations Special? A Large Language Model Perspective</div>
<div id='2403.05045v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T04:44:25Z</div><div>Authors: Toshish Jawale, Chaitanya Animesh, Sekhar Vallath, Kartik Talamadupula, Larry Heck</div><div style='padding-top: 10px; width: 80ex'>This study analyzes changes in the attention mechanisms of large language
models (LLMs) when used to understand natural conversations between humans
(human-human). We analyze three use cases of LLMs: interactions over web
content, code, and mathematical texts. By analyzing attention distance,
dispersion, and interdependency across these domains, we highlight the unique
challenges posed by conversational data. Notably, conversations require nuanced
handling of long-term contextual relationships and exhibit higher complexity
through their attention patterns. Our findings reveal that while language
models exhibit domain-specific attention behaviors, there is a significant gap
in their ability to specialize in human conversations. Through detailed
attention entropy analysis and t-SNE visualizations, we demonstrate the need
for models trained with a diverse array of high-quality conversational data to
enhance understanding and generation of human-like dialogue. This research
highlights the importance of domain specialization in language models and
suggests pathways for future advancement in modeling human conversational
nuances.</div><div><a href='http://arxiv.org/abs/2403.05045v1'>2403.05045v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10962v1")'>Measuring and Controlling Persona Drift in Language Model Dialogs</div>
<div id='2402.10962v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T20:10:29Z</div><div>Authors: Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg</div><div style='padding-top: 10px; width: 80ex'>Prompting is a standard tool for customizing language-model chatbots,
enabling them to take on a specific "persona". An implicit assumption in the
use of prompts is that they will be stable, so the chatbot will continue to
generate text according to the stipulated persona for the duration of a
conversation. We propose a quantitative benchmark to test this assumption,
evaluating persona stability via self-chats between two personalized chatbots.
Testing popular models like LLaMA2-chat-70B, we reveal a significant persona
drift within eight rounds of conversations. An empirical and theoretical
analysis of this phenomenon suggests the transformer attention mechanism plays
a role, due to attention decay over long exchanges. To combat attention decay
and persona drift, we propose a lightweight method called split-softmax, which
compares favorably against two strong baselines.</div><div><a href='http://arxiv.org/abs/2402.10962v1'>2402.10962v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05931v1")'>Thread Detection and Response Generation using Transformers with Prompt
  Optimisation</div>
<div id='2403.05931v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T14:50:20Z</div><div>Authors: Kevin Joshua T, Arnav Agarwal, Shriya Sanjay, Yash Sarda, John Sahaya Rani Alex, Saurav Gupta, Sushant Kumar, Vishwanath Kamath</div><div style='padding-top: 10px; width: 80ex'>Conversational systems are crucial for human-computer interaction, managing
complex dialogues by identifying threads and prioritising responses. This is
especially vital in multi-party conversations, where precise identification of
threads and strategic response prioritisation ensure efficient dialogue
management. To address these challenges an end-to-end model that identifies
threads and prioritises their response generation based on the importance was
developed, involving a systematic decomposition of the problem into discrete
components - thread detection, prioritisation, and performance optimisation
which was meticulously analysed and optimised. These refined components
seamlessly integrate into a unified framework, in conversational systems.
Llama2 7b is used due to its high level of generalisation but the system can be
updated with any open source Large Language Model(LLM). The computational
capabilities of the Llama2 model was augmented by using fine tuning methods and
strategic prompting techniques to optimise the model's performance, reducing
computational time and increasing the accuracy of the model. The model achieves
up to 10x speed improvement, while generating more coherent results compared to
existing models.</div><div><a href='http://arxiv.org/abs/2403.05931v1'>2403.05931v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11330v1")'>Improving Dialogue Agents by Decomposing One Global Explicit Annotation
  with Local Implicit Multimodal Feedback</div>
<div id='2403.11330v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T20:21:26Z</div><div>Authors: Dong Won Lee, Hae Won Park, Yoon Kim, Cynthia Breazeal, Louis-Philippe Morency</div><div style='padding-top: 10px; width: 80ex'>We describe an approach for aligning an LLM-based dialogue agent based on
global (i.e., dialogue-level) rewards, while also taking into account
naturally-occurring multimodal signals. At a high level, our approach (dubbed
GELI) learns a local, turn-level reward model by decomposing the human-provided
Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal
reward signals to crossmodally shape the reward decomposition step. This
decomposed reward model is then used as part of the standard RHLF pipeline
improve an LLM-based dialog agent. We run quantitative and qualitative human
studies to evaluate the performance of our GELI approach, and find that it
shows consistent improvements across various conversational metrics compared to
baseline methods.</div><div><a href='http://arxiv.org/abs/2403.11330v1'>2403.11330v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04770v1")'>Social Orientation: A New Feature for Dialogue Analysis</div>
<div id='2403.04770v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T01:55:45Z</div><div>Authors: Todd Morrill, Zhaoyuan Deng, Yanda Chen, Amith Ananthram, Colin Wayne Leach, Kathleen McKeown</div><div style='padding-top: 10px; width: 80ex'>There are many settings where it is useful to predict and explain the success
or failure of a dialogue. Circumplex theory from psychology models the social
orientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation
participants and can be used to predict and explain the outcome of social
interactions. Our work is novel in its systematic application of social
orientation tags to modeling conversation outcomes. In this paper, we introduce
a new data set of dialogue utterances machine-labeled with social orientation
tags. We show that social orientation tags improve task performance, especially
in low-resource settings, on both English and Chinese language benchmarks. We
also demonstrate how social orientation tags help explain the outcomes of
social interactions when used in neural models. Based on these results showing
the utility of social orientation tags for dialogue outcome prediction tasks,
we release our data sets, code, and models that are fine-tuned to predict
social orientation tags on dialogue utterances.</div><div><a href='http://arxiv.org/abs/2403.04770v1'>2403.04770v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11777v1")'>Uncovering Latent Human Wellbeing in Language Model Embeddings</div>
<div id='2402.11777v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:08:03Z</div><div>Authors: Pedro Freire, ChengCheng Tan, Adam Gleave, Dan Hendrycks, Scott Emmons</div><div style='padding-top: 10px; width: 80ex'>Do language models implicitly learn a concept of human wellbeing? We explore
this through the ETHICS Utilitarianism task, assessing if scaling enhances
pretrained models' representations. Our initial finding reveals that, without
any prompt engineering or finetuning, the leading principal component from
OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches
the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting
pretraining conveys some understanding about human wellbeing. Next, we consider
four language model families, observing how Utilitarianism accuracy varies with
increased parameters. We find performance is nondecreasing with increased model
size when using sufficient numbers of principal components.</div><div><a href='http://arxiv.org/abs/2402.11777v1'>2402.11777v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07815v1")'>Chronos: Learning the Language of Time Series</div>
<div id='2403.07815v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:53:54Z</div><div>Authors: Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, Yuyang Wang</div><div style='padding-top: 10px; width: 80ex'>We introduce Chronos, a simple yet effective framework for pretrained
probabilistic time series models. Chronos tokenizes time series values using
scaling and quantization into a fixed vocabulary and trains existing
transformer-based language model architectures on these tokenized time series
via the cross-entropy loss. We pretrained Chronos models based on the T5 family
(ranging from 20M to 710M parameters) on a large collection of publicly
available datasets, complemented by a synthetic dataset that we generated via
Gaussian processes to improve generalization. In a comprehensive benchmark
consisting of 42 datasets, and comprising both classical local models and deep
learning methods, we show that Chronos models: (a) significantly outperform
other methods on datasets that were part of the training corpus; and (b) have
comparable and occasionally superior zero-shot performance on new datasets,
relative to methods that were trained specifically on them. Our results
demonstrate that Chronos models can leverage time series data from diverse
domains to improve zero-shot accuracy on unseen forecasting tasks, positioning
pretrained models as a viable tool to greatly simplify forecasting pipelines.</div><div><a href='http://arxiv.org/abs/2403.07815v1'>2403.07815v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00131v1")'>UniTS: Building a Unified Time Series Model</div>
<div id='2403.00131v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T21:25:58Z</div><div>Authors: Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros Tsiligkaridis, Marinka Zitnik</div><div style='padding-top: 10px; width: 80ex'>Foundation models, especially LLMs, are profoundly transforming deep
learning. Instead of training many task-specific models, we can adapt a single
pretrained model to many tasks via fewshot prompting or fine-tuning. However,
current foundation models apply to sequence data but not to time series, which
present unique challenges due to the inherent diverse and multidomain time
series datasets, diverging task specifications across forecasting,
classification and other types of tasks, and the apparent need for
task-specialized models. We developed UNITS, a unified time series model that
supports a universal task specification, accommodating classification,
forecasting, imputation, and anomaly detection tasks. This is achieved through
a novel unified network backbone, which incorporates sequence and variable
attention along with a dynamic linear operator and is trained as a unified
model. Across 38 multi-domain datasets, UNITS demonstrates superior performance
compared to task-specific models and repurposed natural language-based LLMs.
UNITS exhibits remarkable zero-shot, few-shot, and prompt learning capabilities
when evaluated on new data domains and tasks. The source code and datasets are
available at https://github.com/mims-harvard/UniTS.</div><div><a href='http://arxiv.org/abs/2403.00131v1'>2403.00131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09093v1")'>RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series
  Tasks</div>
<div id='2401.09093v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T09:56:10Z</div><div>Authors: Haowen Hou, F. Richard Yu</div><div style='padding-top: 10px; width: 80ex'>Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and
GRU, have historically held prominence in time series tasks. However, they have
recently seen a decline in their dominant position across various time series
tasks. As a result, recent advancements in time series forecasting have seen a
notable shift away from RNNs towards alternative architectures such as
Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs,
we design an efficient RNN-based model for time series tasks, named RWKV-TS,
with three distinctive features: (i) A novel RNN architecture characterized by
$O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture
long-term sequence information compared to traditional RNNs. (iii) High
computational efficiency coupled with the capacity to scale up effectively.
Through extensive experimentation, our proposed RWKV-TS model demonstrates
competitive performance when compared to state-of-the-art Transformer-based or
CNN-based models. Notably, RWKV-TS exhibits not only comparable performance but
also demonstrates reduced latency and memory utilization. The success of
RWKV-TS encourages further exploration and innovation in leveraging RNN-based
approaches within the domain of Time Series. The combination of competitive
performance, low latency, and efficient memory usage positions RWKV-TS as a
promising avenue for future research in time series tasks. Code is available
at:\href{https://github.com/howard-hou/RWKV-TS}{
https://github.com/howard-hou/RWKV-TS}</div><div><a href='http://arxiv.org/abs/2401.09093v1'>2401.09093v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02368v1")'>Timer: Transformers for Time Series Analysis at Scale</div>
<div id='2402.02368v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T06:55:55Z</div><div>Authors: Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, Mingsheng Long</div><div style='padding-top: 10px; width: 80ex'>Deep learning has contributed remarkably to the advancement of time series
analysis. Still, deep models can encounter performance bottlenecks in
real-world small-sample scenarios, which can be concealed due to the
performance saturation with small models on current benchmarks. Meanwhile,
large models have demonstrated great powers in these scenarios through
large-scale pre-training. Continuous progresses have been achieved as the
emergence of large language models, exhibiting unprecedented ability in
few-shot generalization, scalability, and task generality, which is however
absent in time series models. To change the current practices of training small
models on specific datasets from scratch, this paper aims at an early
development of large time series models (LTSM). During pre-training, we curate
large-scale datasets with up to 1 billion time points, unify heterogeneous time
series into single-series sequence (S3) format, and develop the GPT-style
architecture toward LTSMs. To meet diverse application needs, we convert
forecasting, imputation, and anomaly detection of time series into a unified
generative task. The outcome of this study is a Time Series Transformer
(Timer), that is pre-trained by autoregressive next token prediction on large
multi-domain datasets, and is fine-tuned to downstream scenarios with promising
abilities as an LTSM.</div><div><a href='http://arxiv.org/abs/2402.02368v1'>2402.02368v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13912v1")'>A Survey of Deep Learning and Foundation Models for Time Series
  Forecasting</div>
<div id='2401.13912v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T03:14:07Z</div><div>Authors: John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, Ninghao Liu</div><div style='padding-top: 10px; width: 80ex'>Deep Learning has been successfully applied to many application domains, yet
its advantages have been slow to emerge for time series forecasting. For
example, in the well-known Makridakis (M) Competitions, hybrids of traditional
statistical or machine learning techniques have only recently become the top
performers. With the recent architectural advances in deep learning being
applied to time series forecasting (e.g., encoder-decoders with attention,
transformers, and graph neural networks), deep learning has begun to show
significant advantages. Still, in the area of pandemic prediction, there remain
challenges for deep learning models: the time series is not long enough for
effective training, unawareness of accumulated scientific knowledge, and
interpretability of the model. To this end, the development of foundation
models (large deep learning models with extensive pre-training) allows models
to understand patterns and acquire knowledge that can be applied to new related
problems before extensive training data becomes available. Furthermore, there
is a vast amount of knowledge available that deep learning models can tap into,
including Knowledge Graphs and Large Language Models fine-tuned with scientific
domain knowledge. There is ongoing research examining how to utilize or inject
such knowledge into deep learning models. In this survey, several
state-of-the-art modeling techniques are reviewed, and suggestions for further
work are provided.</div><div><a href='http://arxiv.org/abs/2401.13912v1'>2401.13912v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14918v1")'>Deep learning-based method for weather forecasting: A case study in
  Itoshima</div>
<div id='2403.14918v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T02:42:38Z</div><div>Authors: Yuzhong Cheng, Linh Thi Hoai Nguyen, Akinori Ozaki, Ton Viet Ta</div><div style='padding-top: 10px; width: 80ex'>Accurate weather forecasting is of paramount importance for a wide range of
practical applications, drawing substantial scientific and societal interest.
However, the intricacies of weather systems pose substantial challenges to
accurate predictions. This research introduces a multilayer perceptron model
tailored for weather forecasting in Itoshima, Kyushu, Japan. Our meticulously
designed architecture demonstrates superior performance compared to existing
models, surpassing benchmarks such as Long Short-Term Memory and Recurrent
Neural Networks.</div><div><a href='http://arxiv.org/abs/2403.14918v1'>2403.14918v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08233v1")'>Enhancing Wind Speed and Wind Power Forecasting Using Shape-Wise Feature
  Engineering: A Novel Approach for Improved Accuracy and Robustness</div>
<div id='2401.08233v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T09:34:17Z</div><div>Authors: Mulomba Mukendi Christian, Yun Seon Kim, Hyebong Choi, Jaeyoung Lee, SongHee You</div><div style='padding-top: 10px; width: 80ex'>Accurate prediction of wind speed and power is vital for enhancing the
efficiency of wind energy systems. Numerous solutions have been implemented to
date, demonstrating their potential to improve forecasting. Among these, deep
learning is perceived as a revolutionary approach in the field. However,
despite their effectiveness, the noise present in the collected data remains a
significant challenge. This noise has the potential to diminish the performance
of these algorithms, leading to inaccurate predictions. In response to this,
this study explores a novel feature engineering approach. This approach
involves altering the data input shape in both Convolutional Neural
Network-Long Short-Term Memory (CNN-LSTM) and Autoregressive models for various
forecasting horizons. The results reveal substantial enhancements in model
resilience against noise resulting from step increases in data. The approach
could achieve an impressive 83% accuracy in predicting unseen data up to the
24th steps. Furthermore, this method consistently provides high accuracy for
short, mid, and long-term forecasts, outperforming the performance of
individual models. These findings pave the way for further research on noise
reduction strategies at different forecasting horizons through shape-wise
feature engineering.</div><div><a href='http://arxiv.org/abs/2401.08233v1'>2401.08233v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13916v1")'>Bias correction of wind power forecasts with SCADA data and continuous
  learning</div>
<div id='2402.13916v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:31:45Z</div><div>Authors: Stefan Jonas, Kevin Winter, Bernhard Brodbeck, Angela Meyer</div><div style='padding-top: 10px; width: 80ex'>Wind energy plays a critical role in the transition towards renewable energy
sources. However, the uncertainty and variability of wind can impede its full
potential and the necessary growth of wind power capacity. To mitigate these
challenges, wind power forecasting methods are employed for applications in
power management, energy trading, or maintenance scheduling. In this work, we
present, evaluate, and compare four machine learning-based wind power
forecasting models. Our models correct and improve 48-hour forecasts extracted
from a numerical weather prediction (NWP) model. The models are evaluated on
datasets from a wind park comprising 65 wind turbines. The best improvement in
forecasting error and mean bias was achieved by a convolutional neural network,
reducing the average NRMSE down to 22%, coupled with a significant reduction in
mean bias, compared to a NRMSE of 35% from the strongly biased baseline model
using uncorrected NWP forecasts. Our findings further indicate that changes to
neural network architectures play a minor role in affecting the forecasting
performance, and that future research should rather investigate changes in the
model pipeline. Moreover, we introduce a continuous learning strategy, which is
shown to achieve the highest forecasting performance improvements when new data
is made available.</div><div><a href='http://arxiv.org/abs/2402.13916v1'>2402.13916v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14385v1")'>WindDragon: Enhancing wind power forecasting with Automated Deep
  Learning</div>
<div id='2402.14385v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T08:55:21Z</div><div>Authors: Julie Keisler, Etienne Le Naour</div><div style='padding-top: 10px; width: 80ex'>Achieving net zero carbon emissions by 2050 requires the integration of
increasing amounts of wind power into power grids. This energy source poses a
challenge to system operators due to its variability and uncertainty.
Therefore, accurate forecasting of wind power is critical for grid operation
and system balancing. This paper presents an innovative approach to short-term
(1 to 6 hour horizon) windpower forecasting at a national level. The method
leverages Automated Deep Learning combined with Numerical Weather Predictions
wind speed maps to accurately forecast wind power.</div><div><a href='http://arxiv.org/abs/2402.14385v1'>2402.14385v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11384v1")'>Reinforcement learning to maximise wind turbine energy generation</div>
<div id='2402.11384v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T21:35:13Z</div><div>Authors: Daniel Soler, Oscar Mariño, David Huergo, Martín de Frutos, Esteban Ferrer</div><div style='padding-top: 10px; width: 80ex'>We propose a reinforcement learning strategy to control wind turbine energy
generation by actively changing the rotor speed, the rotor yaw angle and the
blade pitch angle. A double deep Q-learning with a prioritized experience
replay agent is coupled with a blade element momentum model and is trained to
allow control for changing winds. The agent is trained to decide the best
control (speed, yaw, pitch) for simple steady winds and is subsequently
challenged with real dynamic turbulent winds, showing good performance. The
double deep Q- learning is compared with a classic value iteration
reinforcement learning control and both strategies outperform a classic PID
control in all environments. Furthermore, the reinforcement learning approach
is well suited to changing environments including turbulent/gusty winds,
showing great adaptability. Finally, we compare all control strategies with
real winds and compute the annual energy production. In this case, the double
deep Q-learning algorithm also outperforms classic methodologies.</div><div><a href='http://arxiv.org/abs/2402.11384v1'>2402.11384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00975v1")'>Equipment Health Assessment: Time Series Analysis for Wind Turbine
  Performance</div>
<div id='2403.00975v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T20:54:31Z</div><div>Authors: Jana Backhus, Aniruddha Rajendra Rao, Chandrasekar Venkatraman, Abhishek Padmanabhan, A. Vinoth Kumar, Chetan Gupta</div><div style='padding-top: 10px; width: 80ex'>In this study, we leverage SCADA data from diverse wind turbines to predict
power output, employing advanced time series methods, specifically Functional
Neural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key
innovation lies in the ensemble of FNN and LSTM models, capitalizing on their
collective learning. This ensemble approach outperforms individual models,
ensuring stable and accurate power output predictions. Additionally, machine
learning techniques are applied to detect wind turbine performance
deterioration, enabling proactive maintenance strategies and health assessment.
Crucially, our analysis reveals the uniqueness of each wind turbine,
necessitating tailored models for optimal predictions. These insight
underscores the importance of providing automatized customization for different
turbines to keep human modeling effort low. Importantly, the methodologies
developed in this analysis are not limited to wind turbines; they can be
extended to predict and optimize performance in various machinery, highlighting
the versatility and applicability of our research across diverse industrial
contexts.</div><div><a href='http://arxiv.org/abs/2403.00975v1'>2403.00975v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17804v1")'>Predicting machine failures from multivariate time series: an industrial
  case study</div>
<div id='2402.17804v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T09:07:59Z</div><div>Authors: Nicolò Oreste Pinciroli Vago, Francesca Forbicini, Piero Fraternali</div><div style='padding-top: 10px; width: 80ex'>Non-neural Machine Learning (ML) and Deep Learning (DL) models are often used
to predict system failures in the context of industrial maintenance. However,
only a few researches jointly assess the effect of varying the amount of past
data used to make a prediction and the extension in the future of the forecast.
This study evaluates the impact of the size of the reading window and of the
prediction window on the performances of models trained to forecast failures in
three data sets concerning the operation of (1) an industrial wrapping machine
working in discrete sessions, (2) an industrial blood refrigerator working
continuously, and (3) a nitrogen generator working continuously. The problem is
formulated as a binary classification task that assigns the positive label to
the prediction window based on the probability of a failure to occur in such an
interval. Six algorithms (logistic regression, random forest, support vector
machine, LSTM, ConvLSTM, and Transformers) are compared using multivariate
telemetry time series. The results indicate that, in the considered scenarios,
the dimension of the prediction windows plays a crucial role and highlight the
effectiveness of DL approaches at classifying data with diverse time-dependent
patterns preceding a failure and the effectiveness of ML approaches at
classifying similar and repetitive patterns preceding a failure.</div><div><a href='http://arxiv.org/abs/2402.17804v1'>2402.17804v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10259v1")'>Comprehensive Study Of Predictive Maintenance In Industries Using
  Classification Models And LSTM Model</div>
<div id='2403.10259v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T12:47:45Z</div><div>Authors: Saket Maheshwari, Sambhav Tiwari, Shyam Rai, Satyam Vinayak Daman Pratap Singh</div><div style='padding-top: 10px; width: 80ex'>In today's technology-driven era, the imperative for predictive maintenance
and advanced diagnostics extends beyond aviation to encompass the
identification of damages, failures, and operational defects in rotating and
moving machines. Implementing such services not only curtails maintenance costs
but also extends machine lifespan, ensuring heightened operational efficiency.
Moreover, it serves as a preventive measure against potential accidents or
catastrophic events. The advent of Artificial Intelligence (AI) has
revolutionized maintenance across industries, enabling more accurate and
efficient prediction and analysis of machine failures, thereby conserving time
and resources. Our proposed study aims to delve into various machine learning
classification techniques, including Support Vector Machine (SVM), Random
Forest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for
predicting and analyzing machine performance. SVM classifies data into
different categories based on their positions in a multidimensional space,
while Random Forest employs ensemble learning to create multiple decision trees
for classification. Logistic Regression predicts the probability of binary
outcomes using input data. The primary objective of the study is to assess
these algorithms' performance in predicting and analyzing machine performance,
considering factors such as accuracy, precision, recall, and F1 score. The
findings will aid maintenance experts in selecting the most suitable machine
learning algorithm for effective prediction and analysis of machine
performance.</div><div><a href='http://arxiv.org/abs/2403.10259v1'>2403.10259v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10266v1")'>Intelligent Condition Monitoring of Industrial Plants: An Overview of
  Methodologies and Uncertainty Management Strategies</div>
<div id='2401.10266v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T21:35:03Z</div><div>Authors: Maryam Ahang, Todd Charter, Oluwaseyi Ogunfowora, Maziyar Khadivi, Mostafa Abbasi, Homayoun Najjaran</div><div style='padding-top: 10px; width: 80ex'>Condition monitoring plays a significant role in the safety and reliability
of modern industrial systems. Artificial intelligence (AI) approaches are
gaining attention from academia and industry as a growing subject in industrial
applications and as a powerful way of identifying faults. This paper provides
an overview of intelligent condition monitoring and fault detection and
diagnosis methods for industrial plants with a focus on the open-source
benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and
state-of-the-art deep learning (DL) and machine learning (ML) algorithms for
industrial plant condition monitoring, fault detection, and diagnosis are
summarized and the advantages and disadvantages of each algorithm are studied.
Challenges like imbalanced data, unlabelled samples and how deep learning
models can handle them are also covered. Finally, a comparison of the
accuracies and specifications of different algorithms utilizing the Tennessee
Eastman Process (TEP) is conducted. This research will be beneficial for both
researchers who are new to the field and experts, as it covers the literature
on condition monitoring and state-of-the-art methods alongside the challenges
and possible solutions to them.</div><div><a href='http://arxiv.org/abs/2401.10266v1'>2401.10266v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10182v1")'>Reliable uncertainty with cheaper neural network ensembles: a case study
  in industrial parts classification</div>
<div id='2403.10182v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T10:38:48Z</div><div>Authors: Arthur Thuy, Dries F. Benoit</div><div style='padding-top: 10px; width: 80ex'>In operations research (OR), predictive models often encounter
out-of-distribution (OOD) scenarios where the data distribution differs from
the training data distribution. In recent years, neural networks (NNs) are
gaining traction in OR for their exceptional performance in fields such as
image classification. However, NNs tend to make confident yet incorrect
predictions when confronted with OOD data. Uncertainty estimation offers a
solution to overconfident models, communicating when the output should (not) be
trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR
domain. Deep ensembles, composed of multiple independent NNs, have emerged as a
promising approach, offering not only strong predictive accuracy but also
reliable uncertainty estimation. However, their deployment is challenging due
to substantial computational demands. Recent fundamental research has proposed
more efficient NN ensembles, namely the snapshot, batch, and multi-input
multi-output ensemble. This study is the first to provide a comprehensive
comparison of a single NN, a deep ensemble, and the three efficient NN
ensembles. In addition, we propose a Diversity Quality metric to quantify the
ensembles' performance on the in-distribution and OOD sets in one single
metric. The OR case study discusses industrial parts classification to identify
and manage spare parts, important for timely maintenance of industrial plants.
The results highlight the batch ensemble as a cost-effective and competitive
alternative to the deep ensemble. It outperforms the deep ensemble in both
uncertainty and accuracy while exhibiting a training time speedup of 7x, a test
time speedup of 8x, and 9x memory savings.</div><div><a href='http://arxiv.org/abs/2403.10182v1'>2403.10182v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12606v1")'>On the Effectiveness of Heterogeneous Ensemble Methods for
  Re-identification</div>
<div id='2403.12606v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T10:17:26Z</div><div>Authors: Simon Klüttermann, Jérôme Rutinowski, Anh Nguyen, Britta Grimme, Moritz Roidl, Emmanuel Müller</div><div style='padding-top: 10px; width: 80ex'>In this contribution, we introduce a novel ensemble method for the
re-identification of industrial entities, using images of chipwood pallets and
galvanized metal plates as dataset examples. Our algorithms replace commonly
used, complex siamese neural networks with an ensemble of simplified,
rudimentary models, providing wider applicability, especially in
hardware-restricted scenarios. Each ensemble sub-model uses different types of
extracted features of the given data as its input, allowing for the creation of
effective ensembles in a fraction of the training duration needed for more
complex state-of-the-art models. We reach state-of-the-art performance at our
task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%,
and introduce five distinct feature extraction approaches, and study their
combination using different ensemble methods.</div><div><a href='http://arxiv.org/abs/2403.12606v1'>2403.12606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14413v1")'>Aprendizado de máquina aplicado na eletroquímica</div>
<div id='2401.14413v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T16:41:25Z</div><div>Authors: Carlos Eduardo do Egito Araújo, Lívia F. Sgobbi, Iwens Gervasio Sene Jr, Sergio Teixeira de Carvalho</div><div style='padding-top: 10px; width: 80ex'>This systematic review focuses on analyzing the use of machine learning
techniques for identifying and quantifying analytes in various electrochemical
applications, presenting the available applications in the literature. Machine
learning is a tool that can facilitate the analysis and enhance the
understanding of processes involving various analytes. In electrochemical
biosensors, it increases the precision of medical diagnostics, improving the
identification of biomarkers and pathogens with high reliability. It can be
effectively used for the classification of complex chemical products; in
environmental monitoring, using low-cost sensors; in portable devices and
wearable systems; among others. Currently, the analysis of some analytes is
still performed manually, requiring the expertise of a specialist in the field
and thus hindering the generalization of results. In light of the advancements
in artificial intelligence today, this work proposes to carry out a systematic
review of the literature on the applications of artificial intelligence
techniques. A set of articles has been identified that address electrochemical
problems using machine learning techniques, more specifically, supervised
learning.</div><div><a href='http://arxiv.org/abs/2401.14413v1'>2401.14413v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08611v1")'>A Cost-Sensitive Transformer Model for Prognostics Under Highly
  Imbalanced Industrial Data</div>
<div id='2402.08611v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T15:09:53Z</div><div>Authors: Ali Beikmohammadi, Mohammad Hosein Hamian, Neda Khoeyniha, Tony Lindgren, Olof Steinert, Sindri Magnússon</div><div style='padding-top: 10px; width: 80ex'>The rapid influx of data-driven models into the industrial sector has been
facilitated by the proliferation of sensor technology, enabling the collection
of vast quantities of data. However, leveraging these models for failure
detection and prognosis poses significant challenges, including issues like
missing values and class imbalances. Moreover, the cost sensitivity associated
with industrial operations further complicates the application of conventional
models in this context. This paper introduces a novel cost-sensitive
transformer model developed as part of a systematic workflow, which also
integrates a hybrid resampler and a regression-based imputer. After subjecting
our approach to rigorous testing using the APS failure dataset from Scania
trucks and the SECOM dataset, we observed a substantial enhancement in
performance compared to state-of-the-art methods. Moreover, we conduct an
ablation study to analyze the contributions of different components in our
proposed method. Our findings highlight the potential of our method in
addressing the unique challenges of failure prediction in industrial settings,
thereby contributing to enhanced reliability and efficiency in industrial
operations.</div><div><a href='http://arxiv.org/abs/2402.08611v1'>2402.08611v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02886v1")'>Revisiting Confidence Estimation: Towards Reliable Failure Prediction</div>
<div id='2403.02886v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:44:14Z</div><div>Authors: Fei Zhu, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu</div><div style='padding-top: 10px; width: 80ex'>Reliable confidence estimation is a challenging yet fundamental requirement
in many risk-sensitive applications. However, modern deep neural networks are
often overconfident for their incorrect predictions, i.e., misclassified
samples from known classes, and out-of-distribution (OOD) samples from unknown
classes. In recent years, many confidence calibration and OOD detection methods
have been developed. In this paper, we find a general, widely existing but
actually-neglected phenomenon that most confidence estimation methods are
harmful for detecting misclassification errors. We investigate this problem and
reveal that popular calibration and OOD detection methods often lead to worse
confidence separation between correctly classified and misclassified examples,
making it difficult to decide whether to trust a prediction or not. Finally, we
propose to enlarge the confidence gap by finding flat minima, which yields
state-of-the-art failure prediction performance under various settings
including balanced, long-tailed, and covariate-shift classification scenarios.
Our study not only provides a strong baseline for reliable confidence
estimation but also acts as a bridge between understanding calibration, OOD
detection, and failure prediction. The code is available at
\url{https://github.com/Impression2805/FMFP}.</div><div><a href='http://arxiv.org/abs/2403.02886v1'>2403.02886v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14093v1")'>McUDI: Model-Centric Unsupervised Degradation Indicator for Failure
  Prediction AIOps Solutions</div>
<div id='2401.14093v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:15:51Z</div><div>Authors: Lorena Poenaru-Olaru, Luis Cruz, Jan Rellermeyer, Arie van Deursen</div><div style='padding-top: 10px; width: 80ex'>Due to the continuous change in operational data, AIOps solutions suffer from
performance degradation over time. Although periodic retraining is the
state-of-the-art technique to preserve the failure prediction AIOps models'
performance over time, this technique requires a considerable amount of labeled
data to retrain. In AIOps obtaining label data is expensive since it requires
the availability of domain experts to intensively annotate it. In this paper,
we present McUDI, a model-centric unsupervised degradation indicator that is
capable of detecting the exact moment the AIOps model requires retraining as a
result of changes in data. We further show how employing McUDI in the
maintenance pipeline of AIOps solutions can reduce the number of samples that
require annotations with 30k for job failure prediction and 260k for disk
failure prediction while achieving similar performance with periodic
retraining.</div><div><a href='http://arxiv.org/abs/2401.14093v1'>2401.14093v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07871v1")'>Explainable Predictive Maintenance: A Survey of Current Methods,
  Challenges and Opportunities</div>
<div id='2401.07871v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T18:06:59Z</div><div>Authors: Logan Cummins, Alex Sommers, Somayeh Bakhtiari Ramezani, Sudip Mittal, Joseph Jabour, Maria Seale, Shahram Rahimi</div><div style='padding-top: 10px; width: 80ex'>Predictive maintenance is a well studied collection of techniques that aims
to prolong the life of a mechanical system by using artificial intelligence and
machine learning to predict the optimal time to perform maintenance. The
methods allow maintainers of systems and hardware to reduce financial and time
costs of upkeep. As these methods are adopted for more serious and potentially
life-threatening applications, the human operators need trust the predictive
system. This attracts the field of Explainable AI (XAI) to introduce
explainability and interpretability into the predictive system. XAI brings
methods to the field of predictive maintenance that can amplify trust in the
users while maintaining well-performing systems. This survey on explainable
predictive maintenance (XPM) discusses and presents the current methods of XAI
as applied to predictive maintenance while following the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We
categorize the different XPM methods into groups that follow the XAI
literature. Additionally, we include current challenges and a discussion on
future research directions in XPM.</div><div><a href='http://arxiv.org/abs/2401.07871v1'>2401.07871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13785v1")'>Towards an extension of Fault Trees in the Predictive Maintenance
  Scenario</div>
<div id='2403.13785v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:47:25Z</div><div>Authors: Roberta De Fazio, Stefano Marrone, Laura Verde, Vincenzo Reccia, Paolo Valletta</div><div style='padding-top: 10px; width: 80ex'>One of the most appreciated features of Fault Trees (FTs) is their
simplicity, making them fit into industrial processes. As such processes evolve
in time, considering new aspects of large modern systems, modelling techniques
based on FTs have adapted to these needs. This paper proposes an extension of
FTs to take into account the problem of Predictive Maintenance, one of the
challenges of the modern dependability field of study. The paper sketches the
Predictive Fault Tree language and proposes some use cases to support their
modelling and analysis in concrete industrial settings.</div><div><a href='http://arxiv.org/abs/2403.13785v1'>2403.13785v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13861v1")'>Machine Learning-based Layer-wise Detection of Overheating Anomaly in
  LPBF using Photodiode Data</div>
<div id='2403.13861v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T01:12:44Z</div><div>Authors: Nazmul Hasan, Apurba Kumar Saha, Andrew Wessman, Mohammed Shafae</div><div style='padding-top: 10px; width: 80ex'>Overheating anomaly detection is essential for the quality and reliability of
parts produced by laser powder bed fusion (LPBF) additive manufacturing (AM).
In this research, we focus on the detection of overheating anomalies using
photodiode sensor data. Photodiode sensors can collect high-frequency data from
the melt pool, reflecting the process dynamics and thermal history. Hence, the
proposed method offers a machine learning (ML) framework to utilize photodiode
sensor data for layer-wise detection of overheating anomalies. In doing so,
three sets of features are extracted from the raw photodiode data: MSMM (mean,
standard deviation, median, maximum), MSQ (mean, standard deviation,
quartiles), and MSD (mean, standard deviation, deciles). These three datasets
are used to train several ML classifiers. Cost-sensitive learning is used to
handle the class imbalance between the "anomalous" layers (affected by
overheating) and "nominal" layers in the benchmark dataset. To boost detection
accuracy, our proposed ML framework involves utilizing the majority voting
ensemble (MVE) approach. The proposed method is demonstrated using a case study
including an open benchmark dataset of photodiode measurements from an LPBF
specimen with deliberate overheating anomalies at some layers. The results from
the case study demonstrate that the MSD features yield the best performance for
all classifiers, and the MVE classifier (with a mean F1-score of 0.8654)
surpasses the individual ML classifiers. Moreover, our machine learning
methodology achieves superior results (9.66% improvement in mean F1-score) in
detecting layer-wise overheating anomalies, surpassing the existing methods in
the literature that use the same benchmark dataset.</div><div><a href='http://arxiv.org/abs/2403.13861v1'>2403.13861v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03494v1")'>Pre-insertion resistors temperature prediction based on improved WOA-SVR</div>
<div id='2401.03494v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T14:24:04Z</div><div>Authors: Honghe Dai, Site Mo, Haoxin Wang, Nan Yin, Songhai Fan, Bixiong Li</div><div style='padding-top: 10px; width: 80ex'>The pre-insertion resistors (PIR) within high-voltage circuit breakers are
critical components and warm up by generating Joule heat when an electric
current flows through them. Elevated temperature can lead to temporary closure
failure and, in severe cases, the rupture of PIR. To accurately predict the
temperature of PIR, this study combines finite element simulation techniques
with Support Vector Regression (SVR) optimized by an Improved Whale
Optimization Algorithm (IWOA) approach. The IWOA includes Tent mapping, a
convergence factor based on the sigmoid function, and the Ornstein-Uhlenbeck
variation strategy. The IWOA-SVR model is compared with the SSA-SVR and
WOA-SVR. The results reveal that the prediction accuracies of the IWOA-SVR
model were 90.2% and 81.5% (above 100$^\circ$C) in the 3$^\circ$C temperature
deviation range and 96.3% and 93.4% (above 100$^\circ$C) in the 4$^\circ$C
temperature deviation range, surpassing the performance of the comparative
models. This research demonstrates the method proposed can realize the online
monitoring of the temperature of the PIR, which can effectively prevent thermal
faults PIR and provide a basis for the opening and closing of the circuit
breaker within a short period.</div><div><a href='http://arxiv.org/abs/2401.03494v1'>2401.03494v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15608v1")'>Machine Learning-Based Completions Sequencing for Well Performance
  Optimization</div>
<div id='2402.15608v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T21:11:17Z</div><div>Authors: Anjie Liu, Jinglang W. Sun, Anh Ngo, Ademide O. Mabadeje, Jose L. Hernandez-Mejia</div><div style='padding-top: 10px; width: 80ex'>Establishing accurate field development parameters to optimize long-term oil
production takes time and effort due to the complexity of oil well development,
and the uncertainty in estimating long-term well production. Traditionally, oil
and gas companies use simulation software that are inherently computationally
expensive to forecast production. Thus, machine learning approaches are
recently utilized in literature as an efficient alternative to optimize well
developments by enhancing completion conditions. The primary goal of this
project is to develop effective machine-learning models that can integrate the
effects of multidimensional predictive variables (i.e., completion conditions)
to predict 12-Month Cumulative Production accurately.
  Three predictive regression machine learning models are implemented for
predicting 12-month cumulative oil production: Random Forest, Gradient
Boosting, and Long Short-Term Memory Models. All three models yielded
cumulative production predictions with root mean squared error (RMSE ) values
ranging from 7.35 to 20.01 thousand barrels of oil. Although we hypothesized
that all models would yield accurate predictions, the results indicated a
crucial need for further refinement to create reliable and rational predictive
tools in the subsurface. While this study did not produce optimal models for
completion sequencing to maximize long-term production, we established that
machine learning models alone are not self-sufficient for problems of this
nature. Hence, there is potential for significant improvement, including
comprehensive feature engineering, and a recommendation of exploring the use of
hybrid or surrogate models (i.e., coupling physics reduced models and machine
learning models), to ascertain significant contribution to the progress of
completion sequencing workflows.</div><div><a href='http://arxiv.org/abs/2402.15608v1'>2402.15608v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15962v1")'>Hierarchical energy signatures using machine learning for operational
  visibility and diagnostics in automotive manufacturing</div>
<div id='2402.15962v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T02:54:14Z</div><div>Authors: Ankur Verma, Seog-Chan Oh, Jorge Arinez, Soundar Kumara</div><div style='padding-top: 10px; width: 80ex'>Manufacturing energy consumption data contains important process signatures
required for operational visibility and diagnostics. These signatures may be of
different temporal scales, ranging from monthly to sub-second resolutions. We
introduce a hierarchical machine learning approach to identify automotive
process signatures from paint shop electricity consumption data at varying
temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and Principal Component Analysis (PCA)
combined with Logistic Regression (LR) are used for the analysis. We validate
the utility of the developed algorithms with subject matter experts for (i)
better operational visibility, and (ii) identifying energy saving
opportunities.</div><div><a href='http://arxiv.org/abs/2402.15962v1'>2402.15962v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06458v1")'>Prediction of Wort Density with LSTM Network</div>
<div id='2403.06458v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T06:36:33Z</div><div>Authors: Derk Rembold, Bernd Stauss, Stefan Schwarzkopf</div><div style='padding-top: 10px; width: 80ex'>Many physical target values in technical processes are error-prone,
cumbersome, or expensive to measure automatically. One example of a physical
target value is the wort density, which is an important value needed for beer
production. This article introduces a system that helps the brewer measure wort
density through sensors in order to reduce errors in manual data collection.
Instead of a direct measurement of wort density, a method is developed that
calculates the density from measured values acquired by inexpensive standard
sensors such as pressure or temperature. The model behind the calculation is a
neural network, known as LSTM.</div><div><a href='http://arxiv.org/abs/2403.06458v1'>2403.06458v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13845v1")'>Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting
  Framework for Incremental Zero-Shot Fault Diagnosis</div>
<div id='2403.13845v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T02:50:42Z</div><div>Authors: Jiancheng Zhao, Jiaqi Yue, Chunhui Zhao</div><div style='padding-top: 10px; width: 80ex'>Zero-shot fault diagnosis (ZSFD) is capable of identifying unseen faults via
predicting fault attributes labeled by human experts. We first recognize the
demand of ZSFD to deal with continuous changes in industrial processes, i.e.,
the model's ability to adapt to new fault categories and attributes while
avoiding forgetting the diagnosis ability learned previously. To overcome the
issue that the existing ZSFD paradigm cannot learn from evolving streams of
training data in industrial scenarios, the incremental ZSFD (IZSFD) paradigm is
proposed for the first time, which incorporates category increment and
attribute increment for both traditional ZSFD and generalized ZSFD paradigms.
To achieve IZSFD, we present a broad-deep mixed anti-forgetting framework
(BDMAFF) that aims to learn from new fault categories and attributes. To tackle
the issue of forgetting, BDMAFF effectively accumulates previously acquired
knowledge from two perspectives: features and attribute prototypes. The feature
memory is established through a deep generative model that employs
anti-forgetting training strategies, ensuring the generation quality of
historical categories is supervised and maintained. The diagnosis model SEEs
the UNSEEN faults with the help of generated samples from the generative model.
The attribute prototype memory is established through a diagnosis model
inspired by the broad learning system. Unlike traditional incremental learning
algorithms, BDMAFF introduces a memory-driven iterative update strategy for the
diagnosis model, which allows the model to learn new faults and attributes
without requiring the storage of all historical training samples. The
effectiveness of the proposed method is verified by a real hydraulic system and
the Tennessee-Eastman benchmark process.</div><div><a href='http://arxiv.org/abs/2403.13845v1'>2403.13845v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07033v1")'>Interpreting What Typical Fault Signals Look Like via Prototype-matching</div>
<div id='2403.07033v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T05:47:07Z</div><div>Authors: Qian Chen, Xingjian Dong, Zhike Peng</div><div style='padding-top: 10px; width: 80ex'>Neural networks, with powerful nonlinear mapping and classification
capabilities, are widely applied in mechanical fault diagnosis to ensure
safety. However, being typical black-box models, their application is limited
in high-reliability-required scenarios. To understand the classification logic
and explain what typical fault signals look like, the prototype matching
network (PMN) is proposed by combining the human-inherent prototype-matching
with autoencoder (AE). The PMN matches AE-extracted feature with each prototype
and selects the most similar prototype as the prediction result. It has three
interpreting paths on classification logic, fault prototypes, and matching
contributions. Conventional diagnosis and domain generalization experiments
demonstrate its competitive diagnostic performance and distinguished advantages
in representation learning. Besides, the learned typical fault signals (i.e.,
sample-level prototypes) showcase the ability for denoising and extracting
subtle key features that experts find challenging to capture. This ability
broadens human understanding and provides a promising solution from
interpretability research to AI-for-Science.</div><div><a href='http://arxiv.org/abs/2403.07033v1'>2403.07033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10842v1")'>Twin Transformer using Gated Dynamic Learnable Attention mechanism for
  Fault Detection and Diagnosis in the Tennessee Eastman Process</div>
<div id='2403.10842v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T07:40:23Z</div><div>Authors: Mohammad Ali Labbaf-Khaniki, Mohammad Manthouri, Hanieh Ajami</div><div style='padding-top: 10px; width: 80ex'>Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety
and efficiency of industrial processes. We propose a novel FDD methodology for
the Tennessee Eastman Process (TEP), a widely used benchmark for chemical
process control. The model employs two separate Transformer branches, enabling
independent processing of input data and potential extraction of diverse
information. A novel attention mechanism, Gated Dynamic Learnable Attention
(GDLAttention), is introduced which integrates a gating mechanism and dynamic
learning capabilities. The gating mechanism modulates the attention weights,
allowing the model to focus on the most relevant parts of the input. The
dynamic learning approach adapts the attention strategy during training,
potentially leading to improved performance. The attention mechanism uses a
bilinear similarity function, providing greater flexibility in capturing
complex relationships between query and key vectors. In order to assess the
effectiveness of our approach, we tested it against 21 and 18 distinct fault
scenarios in TEP, and compared its performance with several established FDD
techniques. The outcomes indicate that the method outperforms others in terms
of accuracy, false alarm rate, and misclassification rate. This underscores the
robustness and efficacy of the approach for FDD in intricate industrial
processes.</div><div><a href='http://arxiv.org/abs/2403.10842v1'>2403.10842v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01010v1")'>Unsupervised Continual Anomaly Detection with Contrastively-learned
  Prompt</div>
<div id='2401.01010v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T03:37:11Z</div><div>Authors: Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, Jinbao Wang, Chengjie Wang, Feng Zheng</div><div style='padding-top: 10px; width: 80ex'>Unsupervised Anomaly Detection (UAD) with incremental training is crucial in
industrial manufacturing, as unpredictable defects make obtaining sufficient
labeled data infeasible. However, continual learning methods primarily rely on
supervised annotations, while the application in UAD is limited due to the
absence of supervision. Current UAD methods train separate models for different
classes sequentially, leading to catastrophic forgetting and a heavy
computational burden. To address this issue, we introduce a novel Unsupervised
Continual Anomaly Detection framework called UCAD, which equips the UAD with
continual learning capability through contrastively-learned prompts. In the
proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a
concise key-prompt-knowledge memory bank to guide task-invariant `anomaly'
model predictions using task-specific `normal' knowledge. Moreover,
Structure-based Contrastive Learning (SCL) is designed with the Segment
Anything Model (SAM) to improve prompt learning and anomaly segmentation
results. Specifically, by treating SAM's masks as structure, we draw features
within the same mask closer and push others apart for general feature
representations. We conduct comprehensive experiments and set the benchmark on
unsupervised continual anomaly detection and segmentation, demonstrating that
our method is significantly better than anomaly detection methods, even with
rehearsal training. The code will be available at
https://github.com/shirowalker/UCAD.</div><div><a href='http://arxiv.org/abs/2401.01010v1'>2401.01010v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12729v1")'>Scalable and reliable deep transfer learning for intelligent fault
  detection via multi-scale neural processes embedded with knowledge</div>
<div id='2402.12729v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T05:39:32Z</div><div>Authors: Zhongzhi Li, Jingqi Tu, Jiacheng Zhu, Jianliang Ai, Yiqun Dong</div><div style='padding-top: 10px; width: 80ex'>Deep transfer learning (DTL) is a fundamental method in the field of
Intelligent Fault Detection (IFD). It aims to mitigate the degradation of
method performance that arises from the discrepancies in data distribution
between training set (source domain) and testing set (target domain).
Considering the fact that fault data collection is challenging and certain
faults are scarce, DTL-based methods face the limitation of available
observable data, which reduces the detection performance of the methods in the
target domain. Furthermore, DTL-based methods lack comprehensive uncertainty
analysis that is essential for building reliable IFD systems. To address the
aforementioned problems, this paper proposes a novel DTL-based method known as
Neural Processes-based deep transfer learning with graph convolution network
(GTNP). Feature-based transfer strategy of GTNP bridges the data distribution
discrepancies of source domain and target domain in high-dimensional space.
Both the joint modeling based on global and local latent variables and sparse
sampling strategy reduce the demand of observable data in the target domain.
The multi-scale uncertainty analysis is obtained by using the distribution
characteristics of global and local latent variables. Global analysis of
uncertainty enables GTNP to provide quantitative values that reflect the
complexity of methods and the difficulty of tasks. Local analysis of
uncertainty allows GTNP to model uncertainty (confidence of the fault detection
result) at each sample affected by noise and bias. The validation of the
proposed method is conducted across 3 IFD tasks, consistently showing the
superior detection performance of GTNP compared to the other DTL-based methods.</div><div><a href='http://arxiv.org/abs/2402.12729v1'>2402.12729v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13032v1")'>Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch
  Processes</div>
<div id='2403.13032v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T09:33:07Z</div><div>Authors: Christian W. Frey</div><div style='padding-top: 10px; width: 80ex'>Industrial production processes, especially in the pharmaceutical industry,
are complex systems that require continuous monitoring to ensure efficiency,
product quality, and safety. This paper presents a hybrid unsupervised learning
strategy (HULS) for monitoring complex industrial processes. Addressing the
limitations of traditional Self-Organizing Maps (SOMs), especially in scenarios
with unbalanced data sets and highly correlated process variables, HULS
combines existing unsupervised learning techniques to address these challenges.
To evaluate the performance of the HULS concept, comparative experiments are
performed based on a laboratory batch</div><div><a href='http://arxiv.org/abs/2403.13032v1'>2403.13032v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06923v2")'>Minimally Supervised Learning using Topological Projections in
  Self-Organizing Maps</div>
<div id='2401.06923v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T22:51:48Z</div><div>Authors: Zimeng Lyu, Alexander Ororbia, Rui Li, Travis Desell</div><div style='padding-top: 10px; width: 80ex'>Parameter prediction is essential for many applications, facilitating
insightful interpretation and decision-making. However, in many real life
domains, such as power systems, medicine, and engineering, it can be very
expensive to acquire ground truth labels for certain datasets as they may
require extensive and expensive laboratory testing. In this work, we introduce
a semi-supervised learning approach based on topological projections in
self-organizing maps (SOMs), which significantly reduces the required number of
labeled data points to perform parameter prediction, effectively exploiting
information contained in large unlabeled datasets. Our proposed method first
trains SOMs on unlabeled data and then a minimal number of available labeled
data points are assigned to key best matching units (BMU). The values estimated
for newly-encountered data points are computed utilizing the average of the $n$
closest labeled data points in the SOM's U-matrix in tandem with a topological
shortest path distance calculation scheme. Our results indicate that the
proposed minimally supervised model significantly outperforms traditional
regression techniques, including linear and polynomial regression, Gaussian
process regression, K-nearest neighbors, as well as deep neural network models
and related clustering schemes.</div><div><a href='http://arxiv.org/abs/2401.06923v2'>2401.06923v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07283v1")'>Power Transformer Fault Prediction Based on Knowledge Graphs</div>
<div id='2402.07283v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T19:14:28Z</div><div>Authors: Chao Wang, Zhuo Chen, Ziyan Zhang, Chiyi Li, Kai Song</div><div style='padding-top: 10px; width: 80ex'>In this paper, we address the challenge of learning with limited fault data
for power transformers. Traditional operation and maintenance tools lack
effective predictive capabilities for potential faults. The scarcity of
extensive fault data makes it difficult to apply machine learning techniques
effectively. To solve this problem, we propose a novel approach that leverages
the knowledge graph (KG) technology in combination with gradient boosting
decision trees (GBDT). This method is designed to efficiently learn from a
small set of high-dimensional data, integrating various factors influencing
transformer faults and historical operational data. Our approach enables
accurate safe state assessments and fault analyses of power transformers
despite the limited fault characteristic data. Experimental results demonstrate
that this method outperforms other learning approaches in prediction accuracy,
such as artificial neural networks (ANN) and logistic regression (LR).
Furthermore, it offers significant improvements in progressiveness,
practicality, and potential for widespread application.</div><div><a href='http://arxiv.org/abs/2402.07283v1'>2402.07283v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02021v1")'>Transfer Learning in ECG Diagnosis: Is It Effective?</div>
<div id='2402.02021v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T04:27:26Z</div><div>Authors: Cuong V. Nguyen, Cuong D. Do</div><div style='padding-top: 10px; width: 80ex'>The adoption of deep learning in ECG diagnosis is often hindered by the
scarcity of large, well-labeled datasets in real-world scenarios, leading to
the use of transfer learning to leverage features learned from larger datasets.
Yet the prevailing assumption that transfer learning consistently outperforms
training from scratch has never been systematically validated. In this study,
we conduct the first extensive empirical study on the effectiveness of transfer
learning in multi-label ECG classification, by investigating comparing the
fine-tuning performance with that of training from scratch, covering a variety
of ECG datasets and deep neural networks. We confirm that fine-tuning is the
preferable choice for small downstream datasets; however, when the dataset is
sufficiently large, training from scratch can achieve comparable performance,
albeit requiring a longer training time to catch up. Furthermore, we find that
transfer learning exhibits better compatibility with convolutional neural
networks than with recurrent neural networks, which are the two most prevalent
architectures for time-series ECG applications. Our results underscore the
importance of transfer learning in ECG diagnosis, yet depending on the amount
of available data, researchers may opt not to use it, considering the
non-negligible cost associated with pre-training.</div><div><a href='http://arxiv.org/abs/2402.02021v1'>2402.02021v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01172v2")'>Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing
  Bearing Faults</div>
<div id='2401.01172v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T12:02:50Z</div><div>Authors: Mohammad Al-Sa'd, Tuomas Jalonen, Serkan Kiranyaz, Moncef Gabbouj</div><div style='padding-top: 10px; width: 80ex'>Diagnosis of bearing faults is paramount to reducing maintenance costs and
operational breakdowns. Bearing faults are primary contributors to machine
vibrations, and analyzing their signal morphology offers insights into their
health status. Unfortunately, existing approaches are optimized for controlled
environments, neglecting realistic conditions such as time-varying rotational
speeds and the vibration's non-stationary nature. This paper presents a fusion
of time-frequency analysis and deep learning techniques to diagnose bearing
faults under time-varying speeds and varying noise levels. First, we formulate
the bearing fault-induced vibrations and discuss the link between their
non-stationarity and the bearing's inherent and operational parameters. We also
elucidate quadratic time-frequency distributions and validate their
effectiveness in resolving distinctive dynamic patterns associated with
different bearing faults. Based on this, we design a time-frequency
convolutional neural network (TF-CNN) to diagnose various faults in
rolling-element bearings. Our experimental findings undeniably demonstrate the
superior performance of TF-CNN in comparison to recently developed techniques.
They also assert its versatility in capturing fault-relevant non-stationary
features that couple with speed changes and show its exceptional resilience to
noise, consistently surpassing competing methods across various signal-to-noise
ratios and performance metrics. Altogether, the TF-CNN achieves substantial
accuracy improvements up to 15%, in severe noise conditions.</div><div><a href='http://arxiv.org/abs/2401.01172v2'>2401.01172v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09957v1")'>On Designing Features for Condition Monitoring of Rotating Machines</div>
<div id='2402.09957v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:08:08Z</div><div>Authors: Seetaram Maurya, Nishchal K. Verma</div><div style='padding-top: 10px; width: 80ex'>Various methods for designing input features have been proposed for fault
recognition in rotating machines using one-dimensional raw sensor data. The
available methods are complex, rely on empirical approaches, and may differ
depending on the condition monitoring data used. Therefore, this article
proposes a novel algorithm to design input features that unifies the feature
extraction process for different time-series sensor data. This new insight for
designing/extracting input features is obtained through the lens of histogram
theory. The proposed algorithm extracts discriminative input features, which
are suitable for a simple classifier to deep neural network-based classifiers.
The designed input features are given as input to the classifier with
end-to-end training in a single framework for machine conditions recognition.
The proposed scheme has been validated through three real-time datasets: a)
acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset. The
real-time results and comparative study show the effectiveness of the proposed
scheme for the prediction of the machine's health states.</div><div><a href='http://arxiv.org/abs/2402.09957v1'>2402.09957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15417v1")'>Fault Diagnosis on Induction Motor using Machine Learning and Signal
  Processing</div>
<div id='2401.15417v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T14:12:42Z</div><div>Authors: Muhammad Samiullah, Hasan Ali, Shehryar Zahoor, Anas Ali</div><div style='padding-top: 10px; width: 80ex'>The detection and identification of induction motor faults using machine
learning and signal processing is a valuable approach to avoiding plant
disturbances and shutdowns in the context of Industry 4.0. In this work, we
present a study on the detection and identification of induction motor faults
using machine learning and signal processing with MATLAB Simulink. We developed
a model of a three-phase induction motor in MATLAB Simulink to generate healthy
and faulty motor data. The data collected included stator currents, rotor
currents, input power, slip, rotor speed, and efficiency. We generated four
faults in the induction motor: open circuit fault, short circuit fault,
overload, and broken rotor bars. We collected a total of 150,000 data points
with a 60-40% ratio of healthy to faulty motor data. We applied Fast Fourier
Transform (FFT) to detect and identify healthy and unhealthy conditions and
added a distinctive feature in our data. The generated dataset was trained
different machine learning models. On comparing the accuracy of the models on
the test set, we concluded that the Decision Tree algorithm performed the best
with an accuracy of about 92%. Our study contributes to the literature by
providing a valuable approach to fault detection and classification with
machine learning models for industrial applications.</div><div><a href='http://arxiv.org/abs/2401.15417v1'>2401.15417v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14894v1")'>Data-Driven Ground-Fault Location Method in Distribution Power System
  With Distributed Generation</div>
<div id='2402.14894v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:25:32Z</div><div>Authors: Mauro Caporuscio, Antoine Dupuis, Welf Löwe</div><div style='padding-top: 10px; width: 80ex'>The recent increase in renewable energy penetration at the distribution level
introduces a multi-directional power flow that outdated traditional fault
location techniques. To this extent, the development of new methods is needed
to ensure fast and accurate fault localization and, hence, strengthen power
system reliability. This paper proposes a data-driven ground fault location
method for the power distribution system. An 11-bus 20 kV power system is
modeled in Matlab/Simulink to simulate ground faults. The faults are generated
at different locations and under various system operational states. Time-domain
faulted three-phase voltages at the system substation are then analyzed with
discrete wavelet transform. Statistical quantities of the processed data are
eventually used to train an Artificial Neural Network (ANN) to find a mapping
between computed voltage features and faults. Specifically, three ANNs allow
the prediction of faulted phase, faulted branch, and fault distance from the
system substation separately. According to the results, the method shows good
potential, with a total relative error of 0,4% for fault distance prediction.
The method is applied to datasets with unknown system states to test
robustness.</div><div><a href='http://arxiv.org/abs/2402.14894v1'>2402.14894v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16434v1")'>A novel ANROA based control approach for grid-tied multi-functional
  solar energy conversion system</div>
<div id='2401.16434v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T09:12:39Z</div><div>Authors: Dinanath Prasad, Narendra Kumar, Rakhi Sharma, Hasmat Malik, Fausto Pedro García Márquez, Jesús María Pinar Pérez</div><div style='padding-top: 10px; width: 80ex'>An adaptive control approach for a three-phase grid-interfaced solar
photovoltaic system based on the new Neuro-Fuzzy Inference System with Rain
Optimization Algorithm (ANROA) methodology is proposed and discussed in this
manuscript. This method incorporates an Adaptive Neuro-fuzzy Inference System
(ANFIS) with a Rain Optimization Algorithm (ROA). The ANFIS controller has
excellent maximum tracking capability because it includes features of both
neural and fuzzy techniques. The ROA technique is in charge of controlling the
voltage source converter switching. Avoiding power quality problems including
voltage fluctuations, harmonics, and flickers as well as unbalanced loads and
reactive power usage is the major goal. Besides, the proposed method performs
at zero voltage regulation and unity power factor modes. The suggested control
approach has been modeled and simulated, and its performance has been assessed
using existing alternative methods. A statistical analysis of proposed and
existing techniques has been also presented and discussed. The results of the
simulations demonstrate that, when compared to alternative approaches, the
suggested strategy may properly and effectively identify the best global
solutions. Furthermore, the system's robustness has been studied by using
MATLAB/SIMULINK environment and experimentally by Field Programmable Gate
Arrays Controller (FPGA)-based Hardware-in-Loop (HLL).</div><div><a href='http://arxiv.org/abs/2401.16434v1'>2401.16434v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03534v1")'>ANN-based position and speed sensorless estimation for BLDC motors</div>
<div id='2402.03534v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T21:43:40Z</div><div>Authors: Jose-Carlos Gamazo-Real, Victor Martinez-Martinez, Jaime Gomez-Gil</div><div style='padding-top: 10px; width: 80ex'>BLDC motor applications require precise position and speed measurements,
traditionally obtained with sensors. This article presents a method for
estimating those measurements without position sensors using terminal phase
voltages with attenuated spurious, acquired with a FPGA that also operates a
PWM-controlled inverter. Voltages are labelled with electrical and virtual
rotor states using an encoder that provides training and testing data for two
three-layer ANNs with perceptron-based cascade topology. The first ANN
estimates the position from features of voltages with incremental timestamps,
and the second ANN estimates the speed from features of position differentials
considering timestamps in an acquisition window. Sensor-based training and
sensorless testing at 125 to 1,500 rpm with a loaded 8-pole-pair motor obtained
absolute errors of 0.8 electrical degrees and 22 rpm. Results conclude that the
overall position estimation significantly improved conventional and advanced
methods, and the speed estimation slightly improved conventional methods, but
was worse than in advanced ones.</div><div><a href='http://arxiv.org/abs/2402.03534v1'>2402.03534v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.09030v1")'>An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from
  Acoustic Signals</div>
<div id='2403.09030v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T01:46:30Z</div><div>Authors: Zhao Wang, Xiaomeng Li, Na Li, Longlong Shu</div><div style='padding-top: 10px; width: 80ex'>This study aimed to develop a deep learning model for the classification of
bearing faults in wind turbine generators from acoustic signals. A
convolutional LSTM model was successfully constructed and trained by using
audio data from five predefined fault types for both training and validation.
To create the dataset, raw audio signal data was collected and processed in
frames to capture time and frequency domain information. The model exhibited
outstanding accuracy on training samples and demonstrated excellent
generalization ability during validation, indicating its proficiency of
generalization capability. On the test samples, the model achieved remarkable
classification performance, with an overall accuracy exceeding 99.5%, and a
false positive rate of less than 1% for normal status. The findings of this
study provide essential support for the diagnosis and maintenance of bearing
faults in wind turbine generators, with the potential to enhance the
reliability and efficiency of wind power generation.</div><div><a href='http://arxiv.org/abs/2403.09030v1'>2403.09030v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08742v1")'>Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize
  Energy Management in Sports Facilities</div>
<div id='2402.08742v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:27:06Z</div><div>Authors: Fodil Fadli, Yassine Himeur, Mariam Elnour, Abbes Amira</div><div style='padding-top: 10px; width: 80ex'>Anomaly detection in sport facilities has gained significant attention due to
its potential to promote energy saving and optimizing operational efficiency.
In this research article, we investigate the role of machine learning,
particularly deep learning, in anomaly detection for sport facilities. We
explore the challenges and perspectives of utilizing deep learning methods for
this task, aiming to address the drawbacks and limitations of conventional
approaches. Our proposed approach involves feature extraction from the data
collected in sport facilities. We present a problem formulation using Deep
Feedforward Neural Networks (DFNN) and introduce threshold estimation
techniques to identify anomalies effectively. Furthermore, we propose methods
to reduce false alarms, ensuring the reliability and accuracy of anomaly
detection. To evaluate the effectiveness of our approach, we conduct
experiments on aquatic center dataset at Qatar University. The results
demonstrate the superiority of our deep learning-based method over conventional
techniques, highlighting its potential in real-world applications. Typically,
94.33% accuracy and 92.92% F1-score have been achieved using the proposed
scheme.</div><div><a href='http://arxiv.org/abs/2402.08742v1'>2402.08742v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01119v1")'>Utilizing Autoregressive Networks for Full Lifecycle Data Generation of
  Rolling Bearings for RUL Prediction</div>
<div id='2401.01119v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T09:31:14Z</div><div>Authors: Junliang Wang, Qinghua Zhang, Guanhua Zhu, Guoxi Sun</div><div style='padding-top: 10px; width: 80ex'>The prediction of rolling bearing lifespan is of significant importance in
industrial production. However, the scarcity of high-quality, full lifecycle
data has been a major constraint in achieving precise predictions. To address
this challenge, this paper introduces the CVGAN model, a novel framework
capable of generating one-dimensional vibration signals in both horizontal and
vertical directions, conditioned on historical vibration data and remaining
useful life. In addition, we propose an autoregressive generation method that
can iteratively utilize previously generated vibration information to guide the
generation of current signals. The effectiveness of the CVGAN model is
validated through experiments conducted on the PHM 2012 dataset. Our findings
demonstrate that the CVGAN model, in terms of both MMD and FID metrics,
outperforms many advanced methods in both autoregressive and non-autoregressive
generation modes. Notably, training using the full lifecycle data generated by
the CVGAN model significantly improves the performance of the predictive model.
This result highlights the effectiveness of the data generated by CVGans in
enhancing the predictive power of these models.</div><div><a href='http://arxiv.org/abs/2401.01119v1'>2401.01119v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16462v1")'>Supervised Contrastive Learning based Dual-Mixer Model for Remaining
  Useful Life Prediction</div>
<div id='2401.16462v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T14:38:44Z</div><div>Authors: En Fu, Yanyan Hu, Kaixiang Peng, Yuxin Chu</div><div style='padding-top: 10px; width: 80ex'>The problem of the Remaining Useful Life (RUL) prediction, aiming at
providing an accurate estimate of the remaining time from the current
predicting moment to the complete failure of the device, has gained significant
attention from researchers in recent years. In this paper, to overcome the
shortcomings of rigid combination for temporal and spatial features in most
existing RUL prediction approaches, a spatial-temporal homogeneous feature
extractor, named Dual-Mixer model, is firstly proposed. Flexible layer-wise
progressive feature fusion is employed to ensure the homogeneity of
spatial-temporal features and enhance the prediction accuracy. Secondly, the
Feature Space Global Relationship Invariance (FSGRI) training method is
introduced based on supervised contrastive learning. This method maintains the
consistency of relationships among sample features with their degradation
patterns during model training, simplifying the subsequently regression task in
the output layer and improving the model's performance in RUL prediction.
Finally, the effectiveness of the proposed method is validated through
comparisons with other latest research works on the C-MAPSS dataset. The
Dual-Mixer model demonstrates superiority across most metrics, while the FSGRI
training method shows an average improvement of 7.00% and 2.41% in RMSE and
MAPE, respectively, for all baseline models. Our experiments and model code are
publicly available at https://github.com/fuen1590/PhmDeepLearningProjects.</div><div><a href='http://arxiv.org/abs/2401.16462v1'>2401.16462v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.14065v1")'>Novel application of Relief Algorithm in cascaded artificial neural
  network to predict wind speed for wind power resource assessment in India</div>
<div id='2401.14065v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T10:39:40Z</div><div>Authors: Hasmat Malik, Amit Kumar Yadav, Fausto Pedro García Márquez, Jesús María Pinar-Pérez</div><div style='padding-top: 10px; width: 80ex'>Wind power generated by wind has non-schedule nature due to stochastic nature
of meteorological variable. Hence energy business and control of wind power
generation requires prediction of wind speed (WS) from few seconds to different
time steps in advance. To deal with prediction shortcomings, various WS
prediction methods have been used. Predictive data mining offers variety of
methods for WS predictions where artificial neural network (ANN) is one of the
reliable and accurate methods. It is observed from the result of this study
that ANN gives better accuracy in comparison conventional model. The accuracy
of WS prediction models is found to be dependent on input parameters and
architecture type algorithms utilized. So the selection of most relevant input
parameters is important research area in WS predicton field. The objective of
the paper is twofold: first extensive review of ANN for wind power and WS
prediction is carried out. Discussion and analysis of feature selection using
Relief Algorithm (RA) in WS prediction are considered for different Indian
sites. RA identify atmospheric pressure, solar radiation and relative humidity
are relevant input variables. Based on relevant input variables Cascade ANN
model is developed and prediction accuracy is evaluated. It is found that root
mean square error (RMSE) for comparison between predicted and measured WS for
training and testing wind speed are found to be 1.44 m/s and 1.49 m/s
respectively. The developed cascade ANN model can be used to predict wind speed
for sites where there are not WS measuring instruments are installed in India.</div><div><a href='http://arxiv.org/abs/2401.14065v1'>2401.14065v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16936v1")'>Multi-modal Representation Learning for Cross-modal Prediction of
  Continuous Weather Patterns from Discrete Low-Dimensional Data</div>
<div id='2401.16936v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T12:03:40Z</div><div>Authors: Alif Bin Abdul Qayyum, Xihaier Luo, Nathan M. Urban, Xiaoning Qian, Byung-Jun Yoon</div><div style='padding-top: 10px; width: 80ex'>World is looking for clean and renewable energy sources that do not pollute
the environment, in an attempt to reduce greenhouse gas emissions that
contribute to global warming. Wind energy has significant potential to not only
reduce greenhouse emission, but also meet the ever increasing demand for
energy. To enable the effective utilization of wind energy, addressing the
following three challenges in wind data analysis is crucial. Firstly, improving
data resolution in various climate conditions to ensure an ample supply of
information for assessing potential energy resources. Secondly, implementing
dimensionality reduction techniques for data collected from sensors/simulations
to efficiently manage and store large datasets. Thirdly, extrapolating wind
data from one spatial specification to another, particularly in cases where
data acquisition may be impractical or costly. We propose a deep learning based
approach to achieve multi-modal continuous resolution wind data prediction from
discontinuous wind data, along with data dimensionality reduction.</div><div><a href='http://arxiv.org/abs/2401.16936v1'>2401.16936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17966v1")'>Conformer: Embedding Continuous Attention in Vision Transformer for
  Weather Forecasting</div>
<div id='2402.17966v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T01:15:30Z</div><div>Authors: Hira Saleem, Flora Salim, Cormac Purcell</div><div style='padding-top: 10px; width: 80ex'>Operational weather forecasting system relies on computationally expensive
physics-based models. Although Transformers-based models have shown remarkable
potential in weather forecasting, Transformers are discrete models which limit
their ability to learn the continuous spatio-temporal features of the dynamical
weather system. We address this issue with Conformer, a spatio-temporal
Continuous Vision Transformer for weather forecasting. Conformer is designed to
learn the continuous weather evolution over time by implementing continuity in
the multi-head attention mechanism. The attention mechanism is encoded as a
differentiable function in the transformer architecture to model the complex
weather dynamics. We evaluate Conformer against a state-of-the-art Numerical
Weather Prediction (NWP) model and several deep learning based weather
forecasting models. Conformer outperforms some of the existing data-driven
models at all lead times while only being trained at lower resolution data.</div><div><a href='http://arxiv.org/abs/2402.17966v1'>2402.17966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04125v1")'>DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for
  Accurate and Continuous Weather Modeling</div>
<div id='2401.04125v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T05:05:16Z</div><div>Authors: Wenyuan Li, Zili Liu, Keyan Chen, Hao Chen, Shunlin Liang, Zhengxia Zou, Zhenwei Shi</div><div style='padding-top: 10px; width: 80ex'>Accurate weather forecasting holds significant importance to human
activities. Currently, there are two paradigms for weather forecasting:
Numerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP).
NWP utilizes atmospheric physics for weather modeling but suffers from poor
data utilization and high computational costs, while DLP can learn weather
patterns from vast amounts of data directly but struggles to incorporate
physical laws. Both paradigms possess their respective strengths and
weaknesses, and are incompatible, because physical laws adopted in NWP describe
the relationship between coordinates and meteorological variables, while DLP
directly learns the relationships between meteorological variables without
consideration of coordinates. To address these problems, we introduce the
DeepPhysiNet framework, incorporating physical laws into deep learning models
for accurate and continuous weather system modeling. First, we construct
physics networks based on multilayer perceptrons (MLPs) for individual
meteorological variable, such as temperature, pressure, and wind speed. Physics
networks establish relationships between variables and coordinates by taking
coordinates as input and producing variable values as output. The physical laws
in the form of Partial Differential Equations (PDEs) can be incorporated as a
part of loss function. Next, we construct hyper-networks based on deep learning
methods to directly learn weather patterns from a large amount of
meteorological data. The output of hyper-networks constitutes a part of the
weights for the physics networks. Experimental results demonstrate that, upon
successful integration of physical laws, DeepPhysiNet can accomplish multiple
tasks simultaneously, not only enhancing forecast accuracy but also obtaining
continuous spatiotemporal resolution results, which is unattainable by either
the NWP or DLP.</div><div><a href='http://arxiv.org/abs/2401.04125v1'>2401.04125v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10747v1")'>Fully Differentiable Lagrangian Convolutional Neural Network for
  Continuity-Consistent Physics-Informed Precipitation Nowcasting</div>
<div id='2402.10747v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T15:13:30Z</div><div>Authors: Peter Pavlík, Martin Výboh, Anna Bou Ezzeddine, Viera Rozinajová</div><div style='padding-top: 10px; width: 80ex'>This paper presents a convolutional neural network model for precipitation
nowcasting that combines data-driven learning with physics-informed domain
knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed
Nowcasting, that draws from existing extrapolation-based nowcasting methods and
implements the Lagrangian coordinate system transformation of the data in a
fully differentiable and GPU-accelerated manner to allow for real-time
end-to-end training and inference. Based on our evaluation, LUPIN matches and
exceeds the performance of the chosen benchmark, opening the door for other
Lagrangian machine learning models.</div><div><a href='http://arxiv.org/abs/2402.10747v1'>2402.10747v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17870v2")'>Efficient Subseasonal Weather Forecast using Teleconnection-informed
  Transformers</div>
<div id='2401.17870v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T14:27:35Z</div><div>Authors: Shan Zhao, Zhitong Xiong, Xiao Xiang Zhu</div><div style='padding-top: 10px; width: 80ex'>Subseasonal forecasting, which is pivotal for agriculture, water resource
management, and early warning of disasters, faces challenges due to the chaotic
nature of the atmosphere. Recent advances in machine learning (ML) have
revolutionized weather forecasting by achieving competitive predictive skills
to numerical models. However, training such foundation models requires
thousands of GPU days, which causes substantial carbon emissions and limits
their broader applicability. Moreover, ML models tend to fool the pixel-wise
error scores by producing smoothed results which lack physical consistency and
meteorological meaning. To deal with the aforementioned problems, we propose a
teleconnection-informed transformer. Our architecture leverages the pretrained
Pangu model to achieve good initial weights and integrates a
teleconnection-informed temporal module to improve predictability in an
extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's
parameters, our method enhances predictability on four surface and five
upper-level atmospheric variables at a two-week lead time. Furthermore, the
teleconnection-filtered features improve the spatial granularity of outputs
significantly, indicating their potential physical consistency. Our research
underscores the importance of atmospheric and oceanic teleconnections in
driving future weather conditions. Besides, it presents a resource-efficient
pathway for researchers to leverage existing foundation models on versatile
downstream tasks.</div><div><a href='http://arxiv.org/abs/2401.17870v2'>2401.17870v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00059v1")'>FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather
  Forecasting</div>
<div id='2402.00059v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T13:23:25Z</div><div>Authors: Tao Han, Song Guo, Fenghua Ling, Kang Chen, Junchao Gong, Jingjia Luo, Junxia Gu, Kan Dai, Wanli Ouyang, Lei Bai</div><div style='padding-top: 10px; width: 80ex'>Kilometer-scale modeling of global atmosphere dynamics enables fine-grained
weather forecasting and decreases the risk of disastrous weather and climate
activity. Therefore, building a kilometer-scale global forecast model is a
persistent pursuit in the meteorology domain. Active international efforts have
been made in past decades to improve the spatial resolution of numerical
weather models. Nonetheless, developing the higher resolution numerical model
remains a long-standing challenge due to the substantial consumption of
computational resources. Recent advances in data-driven global weather
forecasting models utilize reanalysis data for model training and have
demonstrated comparable or even higher forecasting skills than numerical
models. However, they are all limited by the resolution of reanalysis data and
incapable of generating higher-resolution forecasts. This work presents
FengWu-GHR, the first data-driven global weather forecasting model running at
the 0.09$^{\circ}$ horizontal resolution. FengWu-GHR introduces a novel
approach that opens the door for operating ML-based high-resolution forecasts
by inheriting prior knowledge from a pretrained low-resolution model. The
hindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to
the IFS-HRES. Furthermore, evaluations on station observations and case studies
of extreme events support the competitive operational forecasting skill of
FengWu-GHR at the high resolution.</div><div><a href='http://arxiv.org/abs/2402.00059v1'>2402.00059v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01295v1")'>ExtremeCast: Boosting Extreme Value Prediction for Global Weather
  Forecast</div>
<div id='2402.01295v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:34:13Z</div><div>Authors: Wanghan Xu, Kang Chen, Tao Han, Hao Chen, Wanli Ouyang, Lei Bai</div><div style='padding-top: 10px; width: 80ex'>Data-driven weather forecast based on machine learning (ML) has experienced
rapid development and demonstrated superior performance in the global
medium-range forecast compared to traditional physics-based dynamical models.
However, most of these ML models struggle with accurately predicting extreme
weather, which is closely related to the extreme value prediction. Through
mathematical analysis, we prove that the use of symmetric losses, such as the
Mean Squared Error (MSE), leads to biased predictions and underestimation of
extreme values. To address this issue, we introduce Exloss, a novel loss
function that performs asymmetric optimization and highlights extreme values to
obtain accurate extreme weather forecast. Furthermore, we introduce a
training-free extreme value enhancement strategy named ExEnsemble, which
increases the variance of pixel values and improves the forecast robustness.
Combined with an advanced global weather forecast model, extensive experiments
show that our solution can achieve state-of-the-art performance in extreme
weather prediction, while maintaining the overall forecast accuracy comparable
to the top medium-range forecast models.</div><div><a href='http://arxiv.org/abs/2402.01295v1'>2402.01295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03929v1")'>Extreme Precipitation Nowcasting using Transformer-based Generative
  Models</div>
<div id='2403.03929v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T18:39:41Z</div><div>Authors: Cristian Meo, Ankush Roy, Mircea Lică, Junzhe Yin, Zeineb Bou Che, Yanbo Wang, Ruben Imhoff, Remko Uijlenhoet, Justin Dauwels</div><div style='padding-top: 10px; width: 80ex'>This paper presents an innovative approach to extreme precipitation
nowcasting by employing Transformer-based generative models, namely
NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a
comprehensive dataset from the Royal Netherlands Meteorological Institute
(KNMI), our study focuses on predicting short-term precipitation with high
accuracy. We introduce a novel method for computing EVL without assuming fixed
extreme representations, addressing the limitations of current models in
capturing extreme weather events. We present both qualitative and quantitative
analyses, demonstrating the superior performance of the proposed
NowcastingGPT-EVL in generating accurate precipitation forecasts, especially
when dealing with extreme precipitation events. The code is available at
\url{https://github.com/Cmeo97/NowcastingGPT}.</div><div><a href='http://arxiv.org/abs/2403.03929v1'>2403.03929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09466v1")'>Self Supervised Vision for Climate Downscaling</div>
<div id='2401.09466v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T10:20:49Z</div><div>Authors: Karandeep Singh, Chaeyoon Jeong, Naufal Shidqi, Sungwon Park, Arjun Nellikkattil, Elke Zeller, Meeyoung Cha</div><div style='padding-top: 10px; width: 80ex'>Climate change is one of the most critical challenges that our planet is
facing today. Rising global temperatures are already bringing noticeable
changes to Earth's weather and climate patterns with an increased frequency of
unpredictable and extreme weather events. Future projections for climate change
research are based on Earth System Models (ESMs), the computer models that
simulate the Earth's climate system. ESMs provide a framework to integrate
various physical systems, but their output is bound by the enormous
computational resources required for running and archiving higher-resolution
simulations. For a given resource budget, the ESMs are generally run on a
coarser grid, followed by a computationally lighter $downscaling$ process to
obtain a finer-resolution output. In this work, we present a deep-learning
model for downscaling ESM simulation data that does not require high-resolution
ground truth data for model optimization. This is realized by leveraging
salient data distribution patterns and the hidden dependencies between weather
variables for an $\textit{individual}$ data point at $\textit{runtime}$.
Extensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates
that the proposed model consistently obtains superior performance over that of
various baselines. The improved downscaling performance and no dependence on
high-resolution ground truth data make the proposed method a valuable tool for
climate research and mark it as a promising direction for future research.</div><div><a href='http://arxiv.org/abs/2401.09466v1'>2401.09466v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14049v1")'>Generative Adversarial Models for Extreme Downscaling of Climate
  Datasets</div>
<div id='2402.14049v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:25:04Z</div><div>Authors: Guiye Li, Guofeng Cao</div><div style='padding-top: 10px; width: 80ex'>Addressing the challenges of climate change requires accurate and
high-resolution mapping of climate and weather variables. However, many
existing climate datasets, such as the gridded outputs of the state-of-the-art
numerical climate models (e.g., general circulation models), are only available
at very coarse spatial resolutions due to the model complexity and extremely
high computational demand. Deep-learning-based methods, particularly generative
adversarial networks (GANs) and their variants, have proved effective for
refining natural images, and have shown great promise in improving scientific
datasets. In this paper, we describe a conditional GAN-based geospatial
downscaling method for extreme downscaling of gridded climate datasets.
Compared to most existing methods, the method can generate high-resolution
accurate climate datasets from very low-resolution inputs. More importantly,
the method explicitly considers the uncertainty inherent to the downscaling
process that tends to be ignored in existing methods. Given an input, the
method can produce a multitude of plausible high-resolution samples instead of
one single deterministic result. These samples allow for an empirical
exploration and inferences of model uncertainty and robustness. With a case
study of gridded climate datasets (wind velocity and solar irradiance), we
demonstrate the performances of the framework in downscaling tasks with very
high scaling factors (up to $64\times$) and highlight the advantages of the
framework with a comprehensive comparison with commonly used downscaling
methods, including area-to-point (ATP) kriging, deep image prior (DIP),
enhanced deep super-resolution network (EDSR), enhanced super-resolution
generative adversarial networks (ESRGAN), and physics-informed
resolution-enhancing GAN (PhIRE GAN).</div><div><a href='http://arxiv.org/abs/2402.14049v1'>2402.14049v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02774v1")'>Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System
  Model Fields with Generative Foundation Models</div>
<div id='2403.02774v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:41:41Z</div><div>Authors: Philipp Hess, Michael Aich, Baoxiang Pan, Niklas Boers</div><div style='padding-top: 10px; width: 80ex'>Accurate and high-resolution Earth system model (ESM) simulations are
essential to assess the ecological and socio-economic impacts of anthropogenic
climate change, but are computationally too expensive. Recent machine learning
approaches have shown promising results in downscaling ESM simulations,
outperforming state-of-the-art statistical approaches. However, existing
methods require computationally costly retraining for each ESM and extrapolate
poorly to climates unseen during training. We address these shortcomings by
learning a consistency model (CM) that efficiently and accurately downscales
arbitrary ESM simulations without retraining in a zero-shot manner. Our
foundation model approach yields probabilistic downscaled fields at resolution
only limited by the observational reference data. We show that the CM
outperforms state-of-the-art diffusion models at a fraction of computational
cost while maintaining high controllability on the downscaling task. Further,
our method generalizes to climate states unseen during training without
explicitly formulated physical constraints.</div><div><a href='http://arxiv.org/abs/2403.02774v1'>2403.02774v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06646v1")'>Diffusion Model-based Probabilistic Downscaling for 180-year East Asian
  Climate Reconstruction</div>
<div id='2402.06646v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T01:34:33Z</div><div>Authors: Fenghua Ling, Zeyu Lu, Jing-Jia Luo, Lei Bai, Swadhin K. Behera, Dachao Jin, Baoxiang Pan, Huidong Jiang, Toshio Yamagata</div><div style='padding-top: 10px; width: 80ex'>As our planet is entering into the "global boiling" era, understanding
regional climate change becomes imperative. Effective downscaling methods that
provide localized insights are crucial for this target. Traditional approaches,
including computationally-demanding regional dynamical models or statistical
downscaling frameworks, are often susceptible to the influence of downscaling
uncertainty. Here, we address these limitations by introducing a diffusion
probabilistic downscaling model (DPDM) into the meteorological field. This
model can efficiently transform data from 1{\deg} to 0.1{\deg} resolution.
Compared with deterministic downscaling schemes, it not only has more accurate
local details, but also can generate a large number of ensemble members based
on probability distribution sampling to evaluate the uncertainty of
downscaling. Additionally, we apply the model to generate a 180-year dataset of
monthly surface variables in East Asia, offering a more detailed perspective
for understanding local scale climate change over the past centuries.</div><div><a href='http://arxiv.org/abs/2402.06646v1'>2402.06646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03349v1")'>When Geoscience Meets Generative AI and Large Language Models:
  Foundations, Trends, and Future Challenges</div>
<div id='2402.03349v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T12:03:50Z</div><div>Authors: Abdenour Hadid, Tanujit Chakraborty, Daniel Busby</div><div style='padding-top: 10px; width: 80ex'>Generative Artificial Intelligence (GAI) represents an emerging field that
promises the creation of synthetic data and outputs in different modalities.
GAI has recently shown impressive results across a large spectrum of
applications ranging from biology, medicine, education, legislation, computer
science, and finance. As one strives for enhanced safety, efficiency, and
sustainability, generative AI indeed emerges as a key differentiator and
promises a paradigm shift in the field. This paper explores the potential
applications of generative AI and large language models in geoscience. The
recent developments in the field of machine learning and deep learning have
enabled the generative model's utility for tackling diverse prediction
problems, simulation, and multi-criteria decision-making challenges related to
geoscience and Earth system dynamics. This survey discusses several GAI models
that have been used in geoscience comprising generative adversarial networks
(GANs), physics-informed neural networks (PINNs), and generative pre-trained
transformer (GPT)-based structures. These tools have helped the geoscience
community in several applications, including (but not limited to) data
generation/augmentation, super-resolution, panchromatic sharpening, haze
removal, restoration, and land surface changing. Some challenges still remain
such as ensuring physical interpretation, nefarious use cases, and
trustworthiness. Beyond that, GAI models show promises to the geoscience
community, especially with the support to climate change, urban science,
atmospheric science, marine science, and planetary science through their
extraordinary ability to data-driven modeling and uncertainty quantification.</div><div><a href='http://arxiv.org/abs/2402.03349v1'>2402.03349v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13825v1")'>Deep Generative Models for Ultra-High Granularity Particle Physics
  Detector Simulation: A Voyage From Emulation to Extrapolation</div>
<div id='2403.13825v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T23:12:47Z</div><div>Authors: Baran Hashemi</div><div style='padding-top: 10px; width: 80ex'>Simulating ultra-high-granularity detector responses in Particle Physics
represents a critical yet computationally demanding task. This thesis aims to
overcome this challenge for the Pixel Vertex Detector (PXD) at the Belle II
experiment, which features over 7.5M pixel channels-the highest spatial
resolution detector simulation dataset ever analysed with generative models.
This thesis starts off by a comprehensive and taxonomic review on generative
models for simulating detector signatures. Then, it presents the Intra-Event
Aware Generative Adversarial Network (IEA-GAN), a new geometry-aware generative
model that introduces a relational attentive reasoning and Self-Supervised
Learning to approximate an "event" in the detector. This study underscores the
importance of intra-event correlation for downstream physics analyses. Building
upon this, the work drifts towards a more generic approach and presents
YonedaVAE, a Category Theory-inspired generative model that tackles the open
problem of Out-of-Distribution (OOD) simulation. YonedaVAE introduces a
learnable Yoneda embedding to capture the entirety of an event based on its
sensor relationships, formulating a Category theoretical language for
intra-event relational reasoning. This is complemented by introducing a
Self-Supervised learnable prior for VAEs and an Adaptive Top-q sampling
mechanism, enabling the model to sample point clouds with variable
intra-category cardinality in a zero-shot manner. Variable Intra-event
cardinality has not been approached before and is vital for simulating
irregular detector geometries. Trained on an early experiment data, YonedaVAE
can reach a reasonable OOD simulation precision of a later experiment with
almost double luminosity. This study introduces, for the first time, the
results of using deep generative models for ultra-high granularity detector
simulation in Particle Physics.</div><div><a href='http://arxiv.org/abs/2403.13825v1'>2403.13825v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01395v1")'>Deep autoregressive modeling for land use land cover</div>
<div id='2401.01395v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T18:03:57Z</div><div>Authors: Christopher Krapu, Mark Borsuk, Ryan Calder</div><div style='padding-top: 10px; width: 80ex'>Land use / land cover (LULC) modeling is a challenging task due to long-range
dependencies between geographic features and distinct spatial patterns related
to topography, ecology, and human development. We identify a close connection
between modeling of spatial patterns of land use and the task of image
inpainting from computer vision and conduct a study of a modified PixelCNN
architecture with approximately 19 million parameters for modeling LULC. In
comparison with a benchmark spatial statistical model, we find that the former
is capable of capturing much richer spatial correlation patterns such as roads
and water bodies but does not produce a calibrated predictive distribution,
suggesting the need for additional tuning. We find evidence of predictive
underdispersion with regard to important ecologically-relevant land use
statistics such as patch count and adjacency which can be ameliorated to some
extent by manipulating sampling variability.</div><div><a href='http://arxiv.org/abs/2401.01395v1'>2401.01395v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13979v1")'>The Importance of Architecture Choice in Deep Learning for Climate
  Applications</div>
<div id='2402.13979v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:09:04Z</div><div>Authors: Simon Dräger, Maike Sonnewald</div><div style='padding-top: 10px; width: 80ex'>Machine Learning has become a pervasive tool in climate science applications.
However, current models fail to address nonstationarity induced by
anthropogenic alterations in greenhouse emissions and do not routinely quantify
the uncertainty of proposed projections. In this paper, we model the Atlantic
Meridional Overturning Circulation (AMOC) which is of major importance to
climate in Europe and the US East Coast by transporting warm water to these
regions, and has the potential for abrupt collapse. We can generate arbitrarily
extreme climate scenarios through arbitrary time scales which we then predict
using neural networks. Our analysis shows that the AMOC is predictable using
neural networks under a diverse set of climate scenarios. Further experiments
reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead
of imitating its progression through autocorrelation. With quantified
uncertainty, an intriguing pattern of "spikes" before critical points of
collapse in the AMOC casts doubt on previous analyses that predicted an AMOC
collapse within this century. Our results show that Bayesian Neural Networks
perform poorly compared to more dense architectures and care should be taken
when applying neural networks to nonstationary scenarios such as climate
projections. Further, our results highlight that big NN models might have
difficulty in modeling global Earth System dynamics accurately and be
successfully applied in nonstationary climate scenarios due to the physics
being challenging for neural networks to capture.</div><div><a href='http://arxiv.org/abs/2402.13979v1'>2402.13979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04818v1")'>Storm Surge Modeling in the AI ERA: Using LSTM-based Machine Learning
  for Enhancing Forecasting Accuracy</div>
<div id='2403.04818v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T13:19:38Z</div><div>Authors: Stefanos Giaremis, Noujoud Nader, Clint Dawson, Hartmut Kaiser, Carola Kaiser, Efstratios Nikidis</div><div style='padding-top: 10px; width: 80ex'>Physics simulation results of natural processes usually do not fully capture
the real world. This is caused for instance by limits in what physical
processes are simulated and to what accuracy. In this work we propose and
analyze the use of an LSTM-based deep learning network machine learning (ML)
architecture for capturing and predicting the behavior of the systemic error
for storm surge forecast models with respect to real-world water height
observations from gauge stations during hurricane events. The overall goal of
this work is to predict the systemic error of the physics model and use it to
improve the accuracy of the simulation results post factum. We trained our
proposed ML model on a dataset of 61 historical storms in the coastal regions
of the U.S. and we tested its performance in bias correcting modeled water
level data predictions from hurricane Ian (2022). We show that our model can
consistently improve the forecasting accuracy for hurricane Ian -- unknown to
the ML model -- at all gauge station coordinates used for the initial data.
Moreover, by examining the impact of using different subsets of the initial
training dataset, containing a number of relatively similar or different
hurricanes in terms of hurricane track, we found that we can obtain similar
quality of bias correction by only using a subset of six hurricanes. This is an
important result that implies the possibility to apply a pre-trained ML model
to real-time hurricane forecasting results with the goal of bias correcting and
improving the produced simulation accuracy. The presented work is an important
first step in creating a bias correction system for real-time storm surge
forecasting applicable to the full simulation area. It also presents a highly
transferable and operationally applicable methodology for improving the
accuracy in a wide range of physics simulation scenarios beyond storm surge
forecasting.</div><div><a href='http://arxiv.org/abs/2403.04818v1'>2403.04818v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12226v1")'>Large-scale flood modeling and forecasting with FloodCast</div>
<div id='2403.12226v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T20:18:32Z</div><div>Authors: Qingsong Xu, Yilei Shi, Jonathan Bamber, Chaojun Ouyang, Xiao Xiang Zhu</div><div style='padding-top: 10px; width: 80ex'>Large-scale hydrodynamic models generally rely on fixed-resolution spatial
grids and model parameters as well as incurring a high computational cost. This
limits their ability to accurately forecast flood crests and issue
time-critical hazard warnings. In this work, we build a fast, stable, accurate,
resolution-invariant, and geometry-adaptative flood modeling and forecasting
framework that can perform at large scales, namely FloodCast. The framework
comprises two main modules: multi-satellite observation and hydrodynamic
modeling. In the multi-satellite observation module, a real-time unsupervised
change detection method and a rainfall processing and analysis tool are
proposed to harness the full potential of multi-satellite observations in
large-scale flood prediction. In the hydrodynamic modeling module, a
geometry-adaptive physics-informed neural solver (GeoPINS) is introduced,
benefiting from the absence of a requirement for training data in
physics-informed neural networks and featuring a fast, accurate, and
resolution-invariant architecture with Fourier neural operators. GeoPINS
demonstrates impressive performance on popular PDEs across regular and
irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence
GeoPINS model to handle long-term temporal series and extensive spatial domains
in large-scale flood modeling. Next, we establish a benchmark dataset in the
2022 Pakistan flood to assess various flood prediction methods. Finally, we
validate the model in three dimensions - flood inundation range, depth, and
transferability of spatiotemporal downscaling. Traditional hydrodynamics and
sequence-to-sequence GeoPINS exhibit exceptional agreement during high water
levels, while comparative assessments with SAR-based flood depth data show that
sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with
smaller prediction errors.</div><div><a href='http://arxiv.org/abs/2403.12226v1'>2403.12226v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03131v1")'>A Physics-guided Generative AI Toolkit for Geophysical Monitoring</div>
<div id='2401.03131v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:09:05Z</div><div>Authors: Junhuan Yang, Hanchen Wang, Yi Sheng, Youzuo Lin, Lei Yang</div><div style='padding-top: 10px; width: 80ex'>Full-waveform inversion (FWI) plays a vital role in geoscience to explore the
subsurface. It utilizes the seismic wave to image the subsurface velocity map.
As the machine learning (ML) technique evolves, the data-driven approaches
using ML for FWI tasks have emerged, offering enhanced accuracy and reduced
computational cost compared to traditional physics-based methods. However, a
common challenge in geoscience, the unprivileged data, severely limits ML
effectiveness. The issue becomes even worse during model pruning, a step
essential in geoscience due to environmental complexities. To tackle this, we
introduce the EdGeo toolkit, which employs a diffusion-based model guided by
physics principles to generate high-fidelity velocity maps. The toolkit uses
the acoustic wave equation to generate corresponding seismic waveform data,
facilitating the fine-tuning of pruned ML models. Our results demonstrate
significant improvements in SSIM scores and reduction in both MAE and MSE
across various pruning ratios. Notably, the ML model fine-tuned using data
generated by EdGeo yields superior quality of velocity maps, especially in
representing unprivileged features, outperforming other existing methods.</div><div><a href='http://arxiv.org/abs/2401.03131v1'>2401.03131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15095v1")'>End-to-End Mineral Exploration with Artificial Intelligence and Ambient
  Noise Tomography</div>
<div id='2403.15095v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T10:23:48Z</div><div>Authors: Jack Muir, Gerrit Olivier, Anthony Reid</div><div style='padding-top: 10px; width: 80ex'>This paper presents an innovative end-to-end workflow for mineral
exploration, integrating ambient noise tomography (ANT) and artificial
intelligence (AI) to enhance the discovery and delineation of mineral resources
essential for the global transition to a low carbon economy. We focus on copper
as a critical element, required in significant quantities for renewable energy
solutions. We show the benefits of utilising ANT, characterised by its speed,
scalability, depth penetration, resolution, and low environmental impact,
alongside artificial intelligence (AI) techniques to refine a continent-scale
prospectivity model at the deposit scale by fine-tuning our model on local
high-resolution data. We show the promise of the method by first presenting a
new data-driven AI prospectivity model for copper within Australia, which
serves as our foundation model for further fine-tuning. We then focus on the
Hillside IOCG deposit on the prospective Yorke Peninsula. We show that with
relatively few local training samples (orebody intercepts), we can fine tune
the foundation model to provide a good estimate of the Hillside orebody
outline. Our methodology demonstrates how AI can augment geophysical data
interpretation, providing a novel approach to mineral exploration with improved
decision-making capabilities for targeting mineralization, thereby addressing
the urgent need for increased mineral resource discovery.</div><div><a href='http://arxiv.org/abs/2403.15095v1'>2403.15095v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14256v1")'>Producing Plankton Classifiers that are Robust to Dataset Shift</div>
<div id='2401.14256v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T15:47:18Z</div><div>Authors: Cheng Chen, Sreenath Kyathanahally, Marta Reyes, Stefanie Merkli, Ewa Merz, Emanuele Francazi, Marvin Hoege, Francesco Pomati, Marco Baity-Jesi</div><div style='padding-top: 10px; width: 80ex'>Modern plankton high-throughput monitoring relies on deep learning
classifiers for species recognition in water ecosystems. Despite satisfactory
nominal performances, a significant challenge arises from Dataset Shift, which
causes performances to drop during deployment. In our study, we integrate the
ZooLake dataset with manually-annotated images from 10 independent days of
deployment, serving as test cells to benchmark Out-Of-Dataset (OOD)
performances. Our analysis reveals instances where classifiers, initially
performing well in In-Dataset conditions, encounter notable failures in
practical scenarios. For example, a MobileNet with a 92% nominal test accuracy
shows a 77% OOD accuracy. We systematically investigate conditions leading to
OOD performance drops and propose a preemptive assessment method to identify
potential pitfalls when classifying new data, and pinpoint features in OOD
images that adversely impact classification. We present a three-step pipeline:
(i) identifying OOD degradation compared to nominal test performance, (ii)
conducting a diagnostic analysis of degradation causes, and (iii) providing
solutions. We find that ensembles of BEiT vision transformers, with targeted
augmentations addressing OOD robustness, geometric ensembling, and
rotation-based test-time augmentation, constitute the most robust model, which
we call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated
on container classes. Moreover, it exhibits lower sensitivity to dataset shift,
and reproduces well the plankton abundances. Our proposed pipeline is
applicable to generic plankton classifiers, contingent on the availability of
suitable test cells. By identifying critical shortcomings and offering
practical procedures to fortify models against dataset shift, our study
contributes to the development of more reliable plankton classification
technologies.</div><div><a href='http://arxiv.org/abs/2401.14256v1'>2401.14256v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07164v1")'>GeoFormer: A Vision and Sequence Transformer-based Approach for
  Greenhouse Gas Monitoring</div>
<div id='2402.07164v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T11:20:29Z</div><div>Authors: Madhav Khirwar, Ankur Narang</div><div style='padding-top: 10px; width: 80ex'>Air pollution represents a pivotal environmental challenge globally, playing
a major role in climate change via greenhouse gas emissions and negatively
affecting the health of billions. However predicting the spatial and temporal
patterns of pollutants remains challenging. The scarcity of ground-based
monitoring facilities and the dependency of air pollution modeling on
comprehensive datasets, often inaccessible for numerous areas, complicate this
issue. In this work, we introduce GeoFormer, a compact model that combines a
vision transformer module with a highly efficient time-series transformer
module to predict surface-level nitrogen dioxide (NO2) concentrations from
Sentinel-5P satellite imagery. We train the proposed model to predict
surface-level NO2 measurements using a dataset we constructed with Sentinel-5P
images of ground-level monitoring stations, and their corresponding NO2
concentration readings. The proposed model attains high accuracy (MAE 5.65),
demonstrating the efficacy of combining vision and time-series transformer
architectures to harness satellite-derived data for enhanced GHG emission
insights, proving instrumental in advancing climate change monitoring and
emission regulation efforts globally.</div><div><a href='http://arxiv.org/abs/2402.07164v1'>2402.07164v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02278v1")'>Lightweight Fish Classification Model for Sustainable Marine Management:
  Indonesian Case</div>
<div id='2401.02278v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T13:56:54Z</div><div>Authors: Febrian Kurniawan, Gandeva Bayu Satrya, Firuz Kamalov</div><div style='padding-top: 10px; width: 80ex'>The enormous demand for seafood products has led to exploitation of marine
resources and near-extinction of some species. In particular, overfishing is
one the main issues in sustainable marine development. In alignment with the
protection of marine resources and sustainable fishing, this study proposes to
advance fish classification techniques that support identifying protected fish
species using state-of-the-art machine learning. We use a custom modification
of the MobileNet model to design a lightweight classifier called M-MobileNet
that is capable of running on limited hardware. As part of the study, we
compiled a labeled dataset of 37,462 images of fish found in the waters of the
Indonesian archipelago. The proposed model is trained on the dataset to
classify images of the captured fish into their species and give
recommendations on whether they are consumable or not. Our modified MobileNet
model uses only 50\% of the top layer parameters with about 42% GTX 860M
utility and achieves up to 97% accuracy in fish classification and determining
its consumability. Given the limited computing capacity available on many
fishing vessels, the proposed model provides a practical solution to on-site
fish classification. In addition, synchronized implementation of the proposed
model on multiple vessels can supply valuable information about the movement
and location of different species of fish.</div><div><a href='http://arxiv.org/abs/2401.02278v1'>2401.02278v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09271v1")'>Hybrid Machine Learning techniques in the management of harmful algal
  blooms impact</div>
<div id='2402.09271v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:59:22Z</div><div>Authors: Andres Molares-Ulloa, Daniel Rivero, Jesus Gil Ruiz, Enrique Fernandez-Blanco, Luis de-la-Fuente-Valentín</div><div style='padding-top: 10px; width: 80ex'>Harmful algal blooms (HABs) are episodes of high concentrations of algae that
are potentially toxic for human consumption. Mollusc farming can be affected by
HABs because, as filter feeders, they can accumulate high concentrations of
marine biotoxins in their tissues. To avoid the risk to human consumption,
harvesting is prohibited when toxicity is detected. At present, the closure of
production areas is based on expert knowledge and the existence of a predictive
model would help when conditions are complex and sampling is not possible.
Although the concentration of toxin in meat is the method most commonly used by
experts in the control of shellfish production areas, it is rarely used as a
target by automatic prediction models. This is largely due to the irregularity
of the data due to the established sampling programs. As an alternative, the
activity status of production areas has been proposed as a target variable
based on whether mollusc meat has a toxicity level below or above the legal
limit. This new option is the most similar to the actual functioning of the
control of shellfish production areas. For this purpose, we have made a
comparison between hybrid machine learning models like Neural-Network-Adding
Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN)
when estimating the state of production areas. The study has been carried out
in several estuaries with different levels of complexity in the episodes of
algal blooms to demonstrate the generalization capacity of the models in bloom
detection. As a result, we could observe that, with an average recall value of
93.41% and without dropping below 90% in any of the estuaries, BAGNET
outperforms the other models both in terms of results and robustness.</div><div><a href='http://arxiv.org/abs/2402.09271v1'>2402.09271v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11482v1")'>SeisFusion: Constrained Diffusion Model with Input Guidance for 3D
  Seismic Data Interpolation and Reconstruction</div>
<div id='2403.11482v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T05:10:13Z</div><div>Authors: Shuang Wang, Fei Deng, Peifan Jiang, Zishan Gong, Xiaolin Wei, Yuqing Wang</div><div style='padding-top: 10px; width: 80ex'>Geographical, physical, or economic constraints often result in missing
traces within seismic data, making the reconstruction of complete seismic data
a crucial step in seismic data processing. Traditional methods for seismic data
reconstruction require the selection of multiple empirical parameters and
struggle to handle large-scale continuous missing data. With the development of
deep learning, various neural networks have demonstrated powerful
reconstruction capabilities. However, these convolutional neural networks
represent a point-to-point reconstruction approach that may not cover the
entire distribution of the dataset. Consequently, when dealing with seismic
data featuring complex missing patterns, such networks may experience varying
degrees of performance degradation. In response to this challenge, we propose a
novel diffusion model reconstruction framework tailored for 3D seismic data. To
constrain the results generated by the diffusion model, we introduce
conditional supervision constraints into the diffusion model, constraining the
generated data of the diffusion model based on the input data to be
reconstructed. We introduce a 3D neural network architecture into the diffusion
model, successfully extending the 2D diffusion model to 3D space. Additionally,
we refine the model's generation process by incorporating missing data into the
generation process, resulting in reconstructions with higher consistency.
Through ablation studies determining optimal parameter values, our method
exhibits superior reconstruction accuracy when applied to both field datasets
and synthetic datasets, effectively addressing a wide range of complex missing
patterns. Our implementation is available at
https://github.com/WAL-l/SeisFusion.</div><div><a href='http://arxiv.org/abs/2403.11482v1'>2403.11482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00310v1")'>Seismic Traveltime Tomography with Label-free Learning</div>
<div id='2402.00310v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T03:49:10Z</div><div>Authors: Feng Wang, Bo Yang, Renfang Wang, Hong Qiu</div><div style='padding-top: 10px; width: 80ex'>Deep learning techniques have been used to build velocity models (VMs) for
seismic traveltime tomography and have shown encouraging performance in recent
years. However, they need to generate labeled samples (i.e., pairs of input and
label) to train the deep neural network (NN) with end-to-end learning, and the
real labels for field data inversion are usually missing or very expensive.
Some traditional tomographic methods can be implemented quickly, but their
effectiveness is often limited by prior assumptions. To avoid generating
labeled samples, we propose a novel method by integrating deep learning and
dictionary learning to enhance the VMs with low resolution by using the
traditional tomography-least square method (LSQR). We first design a type of
shallow and simple NN to reduce computational cost followed by proposing a
two-step strategy to enhance the VMs with low resolution: (1) Warming up. An
initial dictionary is trained from the estimation by LSQR through dictionary
learning method; (2) Dictionary optimization. The initial dictionary obtained
in the warming-up step will be optimized by the NN, and then it will be used to
reconstruct high-resolution VMs with the reference slowness and the estimation
by LSQR. Furthermore, we design a loss function to minimize traveltime misfit
to ensure that NN training is label-free, and the optimized dictionary can be
obtained after each epoch of NN training. We demonstrate the effectiveness of
the proposed method through numerical tests.</div><div><a href='http://arxiv.org/abs/2402.00310v1'>2402.00310v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07569v1")'>Exploring Challenges in Deep Learning of Single-Station Ground Motion
  Records</div>
<div id='2403.07569v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:56:50Z</div><div>Authors: Ümit Mert Çağlar, Baris Yilmaz, Melek Türkmen, Erdem Akagündüz, Salih Tileylioglu</div><div style='padding-top: 10px; width: 80ex'>Contemporary deep learning models have demonstrated promising results across
various applications within seismology and earthquake engineering. These models
rely primarily on utilizing ground motion records for tasks such as earthquake
event classification, localization, earthquake early warning systems, and
structural health monitoring. However, the extent to which these models
effectively learn from these complex time-series signals has not been
thoroughly analyzed. In this study, our objective is to evaluate the degree to
which auxiliary information, such as seismic phase arrival times or seismic
station distribution within a network, dominates the process of deep learning
from ground motion records, potentially hindering its effectiveness. We perform
a hyperparameter search on two deep learning models to assess their
effectiveness in deep learning from ground motion records while also examining
the impact of auxiliary information on model performance. Experimental results
reveal a strong reliance on the highly correlated P and S phase arrival
information. Our observations highlight a potential gap in the field,
indicating an absence of robust methodologies for deep learning of
single-station ground motion recordings independent of any auxiliary
information.</div><div><a href='http://arxiv.org/abs/2403.07569v1'>2403.07569v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12864v1")'>A Comparison of Deep Learning Architectures for Spacecraft Anomaly
  Detection</div>
<div id='2403.12864v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T16:08:27Z</div><div>Authors: Daniel Lakey, Tim Schlippe</div><div style='padding-top: 10px; width: 80ex'>Spacecraft operations are highly critical, demanding impeccable reliability
and safety. Ensuring the optimal performance of a spacecraft requires the early
detection and mitigation of anomalies, which could otherwise result in unit or
mission failures. With the advent of deep learning, a surge of interest has
been seen in leveraging these sophisticated algorithms for anomaly detection in
space operations. This study aims to compare the efficacy of various deep
learning architectures in detecting anomalies in spacecraft data. The deep
learning models under investigation include Convolutional Neural Networks
(CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM)
networks, and Transformer-based architectures. Each of these models was trained
and validated using a comprehensive dataset sourced from multiple spacecraft
missions, encompassing diverse operational scenarios and anomaly types. Initial
results indicate that while CNNs excel in identifying spatial patterns and may
be effective for some classes of spacecraft data, LSTMs and RNNs show a marked
proficiency in capturing temporal anomalies seen in time-series spacecraft
telemetry. The Transformer-based architectures, given their ability to focus on
both local and global contexts, have showcased promising results, especially in
scenarios where anomalies are subtle and span over longer durations.
Additionally, considerations such as computational efficiency, ease of
deployment, and real-time processing capabilities were evaluated. While CNNs
and LSTMs demonstrated a balance between accuracy and computational demands,
Transformer architectures, though highly accurate, require significant
computational resources. In conclusion, the choice of deep learning
architecture for spacecraft anomaly detection is highly contingent on the
nature of the data, the type of anomalies, and operational constraints.</div><div><a href='http://arxiv.org/abs/2403.12864v1'>2403.12864v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15543v1")'>Anomaly Detection of Particle Orbit in Accelerator using LSTM Deep
  Learning Technology</div>
<div id='2401.15543v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T02:09:26Z</div><div>Authors: Zhiyuan Chen, Wei Lu, Radhika Bhong, Yimin Hu, Brian Freeman, Adam Carpenter</div><div style='padding-top: 10px; width: 80ex'>A stable, reliable, and controllable orbit lock system is crucial to an
electron (or ion) accelerator because the beam orbit and beam energy
instability strongly affect the quality of the beam delivered to experimental
halls. Currently, when the orbit lock system fails operators must manually
intervene. This paper develops a Machine Learning based fault detection
methodology to identify orbit lock anomalies and notify accelerator operations
staff of the off-normal behavior. Our method is unsupervised, so it does not
require labeled data. It uses Long-Short Memory Networks (LSTM) Auto Encoder to
capture normal patterns and predict future values of monitoring sensors in the
orbit lock system. Anomalies are detected when the prediction error exceeds a
threshold. We conducted experiments using monitoring data from Jefferson Lab's
Continuous Electron Beam Accelerator Facility (CEBAF). The results are
promising: the percentage of real anomalies identified by our solution is
68.6%-89.3% using monitoring data of a single component in the orbit lock
control system. The accuracy can be as high as 82%.</div><div><a href='http://arxiv.org/abs/2401.15543v1'>2401.15543v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05452v2")'>The R2D2 deep neural network series paradigm for fast precision imaging
  in radio astronomy</div>
<div id='2403.05452v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T16:57:54Z</div><div>Authors: Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux</div><div style='padding-top: 10px; width: 80ex'>Radio-interferometric (RI) imaging entails solving high-resolution
high-dynamic range inverse problems from large data volumes. Recent image
reconstruction techniques grounded in optimization theory have demonstrated
remarkable capability for imaging precision, well beyond CLEAN's capability.
These range from advanced proximal algorithms propelled by handcrafted
regularization operators, such as the SARA family, to hybrid plug-and-play
(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.
Optimization and PnP structures are however highly iterative, which hinders
their ability to handle the extreme data sizes expected from future
instruments. To address this scalability challenge, we introduce a novel deep
learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic
range imaging''. R2D2's reconstruction is formed as a series of residual
images, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking
the previous iteration's image estimate and associated data residual as inputs.
It thus takes a hybrid structure between a PnP algorithm and a learned version
of the matching pursuit algorithm that underpins CLEAN. We present a
comprehensive study of our approach, featuring its multiple incarnations
distinguished by their DNN architectures. We provide a detailed description of
its training process, targeting a telescope-specific approach. R2D2's
capability to deliver high precision is demonstrated in simulation, across a
variety of image and observation settings using the Very Large Array (VLA). Its
reconstruction speed is also demonstrated: with only few iterations required to
clean data residuals at dynamic ranges up to 100000, R2D2 opens the door to
fast precision imaging. R2D2 codes are available in the BASPLib library on
GitHub.</div><div><a href='http://arxiv.org/abs/2403.05452v2'>2403.05452v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11350v1")'>Robustness of the data-driven approach in limited angle tomography</div>
<div id='2403.11350v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T21:35:45Z</div><div>Authors: Yiran Wang, Yimin Zhong</div><div style='padding-top: 10px; width: 80ex'>The limited angle Radon transform is notoriously difficult to invert due to
the ill-posedness. In this work, we give a mathematical explanation that the
data-driven approach based on deep neural networks can reconstruct more
information in a stable way compared to traditional methods.</div><div><a href='http://arxiv.org/abs/2403.11350v1'>2403.11350v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00153v1")'>Fully Data-Driven Model for Increasing Sampling Rate Frequency of
  Seismic Data using Super-Resolution Generative Adversarial Networks</div>
<div id='2402.00153v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T20:15:35Z</div><div>Authors: Navid Gholizadeh, Javad Katebi</div><div style='padding-top: 10px; width: 80ex'>High-quality data is one of the key requirements for any engineering
application. In earthquake engineering practice, accurate data is pivotal in
predicting the response of structure or damage detection process in an
Structural Health Monitoring (SHM) application with less uncertainty. However,
obtaining high-resolution data is fraught with challenges, such as significant
costs, extensive data channels, and substantial storage requirements. To
address these challenges, this study employs super-resolution generative
adversarial networks (SRGANs) to improve the resolution of time-history data
such as the data obtained by a sensor network in an SHM application, marking
the first application of SRGANs in earthquake engineering domain. The
time-series data are transformed into RGB values, converting raw data into
images. SRGANs are then utilized to upscale these low-resolution images,
thereby enhancing the overall sensor resolution. This methodology not only
offers potential reductions in data storage requirements but also simplifies
the sensor network, which could result in lower installation and maintenance
costs. The proposed SRGAN method is rigorously evaluated using real seismic
data, and its performance is compared with traditional enhancement techniques.
The findings of this study pave the way for cost-effective and efficient
improvements in the resolution of sensors used in SHM systems, with promising
implications for the safety and sustainability of infrastructures worldwide.</div><div><a href='http://arxiv.org/abs/2402.00153v1'>2402.00153v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08684v1")'>A Physics-informed machine learning model for time-dependent wave runup
  prediction</div>
<div id='2401.08684v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:58:37Z</div><div>Authors: Saeed Saviz Naeini, Reda Snaiki</div><div style='padding-top: 10px; width: 80ex'>Wave runup is a critical factor affecting coastal flooding, shoreline
changes, and damage to coastal structures. Climate change is also expected to
amplify wave runup's impact on coastal areas. Therefore, fast and accurate wave
runup estimation is essential for effective coastal engineering design and
management. However, predicting the time-dependent wave runup is challenging
due to the intrinsic nonlinearities and non-stationarity of the process, even
with the use of the most advanced machine learning techniques. In this study, a
physics-informed machine learning-based approach is proposed to efficiently and
accurately simulate time-series wave runup. The methodology combines the
computational efficiency of the Surfbeat (XBSB) mode with the accuracy of the
nonhydrostatic (XBNH) mode of the XBeach model. Specifically, a conditional
generative adversarial network (cGAN) is used to map the image representation
of wave runup from XBSB to the corresponding image from XBNH. These images are
generated by first converting wave runup signals into time-frequency scalograms
and then transforming them into image representations. The cGAN model achieves
improved performance in image-to-image mapping tasks by incorporating
physics-based knowledge from XBSB. After training the model, the high-fidelity
XBNH-based scalograms can be predicted, which are then employed to reconstruct
the time-series wave runup using the inverse wavelet transform. The simulation
results underscore the efficiency and robustness of the proposed model in
predicting wave runup, suggesting its potential value for applications in risk
assessment and management.</div><div><a href='http://arxiv.org/abs/2401.08684v1'>2401.08684v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13136v1")'>Multi-fidelity surrogate with heterogeneous input spaces for modeling
  melt pools in laser-directed energy deposition</div>
<div id='2403.13136v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T20:12:46Z</div><div>Authors: Nandana Menon, Amrita Basak</div><div style='padding-top: 10px; width: 80ex'>Multi-fidelity (MF) modeling is a powerful statistical approach that can
intelligently blend data from varied fidelity sources. This approach finds a
compelling application in predicting melt pool geometry for laser-directed
energy deposition (L-DED). One major challenge in using MF surrogates to merge
a hierarchy of melt pool models is the variability in input spaces. To address
this challenge, this paper introduces a novel approach for constructing an MF
surrogate for predicting melt pool geometry by integrating models of varying
complexity, that operate on heterogeneous input spaces. The first thermal model
incorporates five input parameters i.e., laser power, scan velocity, powder
flow rate, carrier gas flow rate, and nozzle height. In contrast, the second
thermal model can only handle laser power and scan velocity. A mapping is
established between the heterogeneous input spaces so that the five-dimensional
space can be morphed into a pseudo two-dimensional space. Predictions are then
blended using a Gaussian process-based co-kriging method. The resulting
heterogeneous multi-fidelity Gaussian process (Het-MFGP) surrogate not only
improves predictive accuracy but also offers computational efficiency by
reducing evaluations required from the high-dimensional, high-fidelity thermal
model. The results underscore the benefits of employing Het-MFGP for modeling
melt pool behavior in L-DED. The framework successfully demonstrates how to
leverage multimodal data and handle scenarios where certain input parameters
may be difficult to model or measure.</div><div><a href='http://arxiv.org/abs/2403.13136v1'>2403.13136v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.18484v1")'>A non-intrusive machine learning framework for debiasing long-time
  coarse resolution climate simulations and quantifying rare events statistics</div>
<div id='2402.18484v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:06:19Z</div><div>Authors: Benedikt Barthel Sorensen, Alexis Charalampopoulos, Shixuan Zhang, Bryce Harrop, Ruby Leung, Themistoklis Sapsis</div><div style='padding-top: 10px; width: 80ex'>Due to the rapidly changing climate, the frequency and severity of extreme
weather is expected to increase over the coming decades. As fully-resolved
climate simulations remain computationally intractable, policy makers must rely
on coarse-models to quantify risk for extremes. However, coarse models suffer
from inherent bias due to the ignored "sub-grid" scales. We propose a framework
to non-intrusively debias coarse-resolution climate predictions using
neural-network (NN) correction operators. Previous efforts have attempted to
train such operators using loss functions that match statistics. However, this
approach falls short with events that have longer return period than that of
the training data, since the reference statistics have not converged. Here, the
scope is to formulate a learning method that allows for correction of dynamics
and quantification of extreme events with longer return period than the
training data. The key obstacle is the chaotic nature of the underlying
dynamics. To overcome this challenge, we introduce a dynamical systems approach
where the correction operator is trained using reference data and a coarse
model simulation nudged towards that reference. The method is demonstrated on
debiasing an under-resolved quasi-geostrophic model and the Energy Exascale
Earth System Model (E3SM). For the former, our method enables the
quantification of events that have return period two orders longer than the
training data. For the latter, when trained on 8 years of ERA5 data, our
approach is able to correct the coarse E3SM output to closely reflect the
36-year ERA5 statistics for all prognostic variables and significantly reduce
their spatial biases.</div><div><a href='http://arxiv.org/abs/2402.18484v1'>2402.18484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13911v1")'>Replication Study: Enhancing Hydrological Modeling with Physics-Guided
  Machine Learning</div>
<div id='2402.13911v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:26:59Z</div><div>Authors: Mostafa Esmaeilzadeh, Melika Amirzadeh</div><div style='padding-top: 10px; width: 80ex'>Current hydrological modeling methods combine data-driven Machine Learning
(ML) algorithms and traditional physics-based models to address their
respective limitations incorrect parameter estimates from rigid physics-based
models and the neglect of physical process constraints by ML algorithms.
Despite the accuracy of ML in outcome prediction, the integration of scientific
knowledge is crucial for reliable predictions. This study introduces a Physics
Informed Machine Learning (PIML) model, which merges the process understanding
of conceptual hydrological models with the predictive efficiency of ML
algorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates
superior performance in forecasting monthly streamflow and actual
evapotranspiration over both standalone conceptual models and ML algorithms,
ensuring physical consistency of the outputs. This study replicates the
methodologies of Bhasme, P., Vagadiya, J., &amp; Bhatia, U. (2022) from their
pivotal work on Physics Informed Machine Learning for hydrological processes,
utilizing their shared code and datasets to further explore the predictive
capabilities in hydrological modeling.</div><div><a href='http://arxiv.org/abs/2402.13911v1'>2402.13911v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13371v1")'>FIDLAR: Forecast-Informed Deep Learning Architecture for Flood
  Mitigation</div>
<div id='2402.13371v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T20:53:04Z</div><div>Authors: Jimeng Shi, Zeda Yin, Arturo Leon, Jayantha Obeysekera, Giri Narasimhan</div><div style='padding-top: 10px; width: 80ex'>In coastal river systems, frequent floods, often occurring during major
storms or king tides, pose a severe threat to lives and property. However,
these floods can be mitigated or even prevented by strategically releasing
water before extreme weather events with hydraulic structures such as dams,
gates, pumps, and reservoirs. A standard approach used by local water
management agencies is the "rule-based" method, which specifies predetermined
pre-releases of water based on historical and time-tested human experience, but
which tends to result in excess or inadequate water release. The model
predictive control (MPC), a physics-based model for prediction, is an
alternative approach, albeit involving computationally intensive calculations.
In this paper, we propose a Forecast Informed Deep Learning Architecture,
FIDLAR, to achieve rapid and optimal flood management with precise water
pre-releases. FIDLAR seamlessly integrates two neural network modules: one
called the Flood Manager, which is responsible for generating water pre-release
schedules, and another called the Flood Evaluator, which assesses these
generated schedules. The Evaluator module is pre-trained separately, and its
gradient-based feedback is used to train the Manager model, ensuring optimal
water pre-releases. We have conducted experiments using FIDLAR with data from a
flood-prone coastal area in South Florida, particularly susceptible to frequent
storms. Results show that FIDLAR is several orders of magnitude faster than
currently used physics-based approaches while outperforming baseline methods
with improved water pre-release schedules. Our code is at
https://github.com/JimengShi/FIDLAR/.</div><div><a href='http://arxiv.org/abs/2402.13371v1'>2402.13371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02821v1")'>An Adaptive Hydropower Management Approach for Downstream Ecosystem
  Preservation</div>
<div id='2403.02821v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T09:44:51Z</div><div>Authors: C. Coelho, M. Jing, M. Fernanda P. Costa, L. L. Ferrás</div><div style='padding-top: 10px; width: 80ex'>Hydropower plants play a pivotal role in advancing clean and sustainable
energy production, contributing significantly to the global transition towards
renewable energy sources. However, hydropower plants are currently perceived
both positively as sources of renewable energy and negatively as disruptors of
ecosystems. In this work, we highlight the overlooked potential of using
hydropower plant as protectors of ecosystems by using adaptive ecological
discharges. To advocate for this perspective, we propose using a neural network
to predict the minimum ecological discharge value at each desired time.
Additionally, we present a novel framework that seamlessly integrates it into
hydropower management software, taking advantage of the well-established
approach of using traditional constrained optimisation algorithms. This novel
approach not only protects the ecosystems from climate change but also
contributes to potentially increase the electricity production.</div><div><a href='http://arxiv.org/abs/2403.02821v1'>2403.02821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14521v2")'>Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological
  Modeling using the Mass-Conserving-Perceptron</div>
<div id='2401.14521v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T21:26:49Z</div><div>Authors: Yuan-Heng Wang, Hoshin V. Gupta</div><div style='padding-top: 10px; width: 80ex'>We investigate the applicability of machine learning technologies to the
development of parsimonious, interpretable, catchment-scale hydrologic models
using directed-graph architectures based on the mass-conserving perceptron
(MCP) as the fundamental computational unit. Here, we focus on architectural
complexity (depth) at a single location, rather than universal applicability
(breadth) across large samples of catchments. The goal is to discover a minimal
representation (numbers of cell-states and flow paths) that represents the
dominant processes that can explain the input-state-output behaviors of a given
catchment, with particular emphasis given to simulating the full range (high,
medium, and low) of flow dynamics. We find that a HyMod-like architecture with
three cell-states and two major flow pathways achieves such a representation at
our study location, but that the additional incorporation of an input-bypass
mechanism significantly improves the timing and shape of the hydrograph, while
the inclusion of bi-directional groundwater mass exchanges significantly
enhances the simulation of baseflow. Overall, our results demonstrate the
importance of using multiple diagnostic metrics for model evaluation, while
highlighting the need for designing training metrics that are better suited to
extracting information across the full range of flow dynamics. Further, they
set the stage for interpretable regional-scale MCP-based hydrological modeling
(using large sample data) by using neural architecture search to determine
appropriate minimal representations for catchments in different hydroclimatic
regimes.</div><div><a href='http://arxiv.org/abs/2401.14521v2'>2401.14521v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09961v1")'>Thermal Earth Model for the Conterminous United States Using an
  Interpolative Physics-Informed Graph Neural Network (InterPIGNN)</div>
<div id='2403.09961v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T01:55:07Z</div><div>Authors: Mohammad J. Aljubran, Roland N. Horne</div><div style='padding-top: 10px; width: 80ex'>This study presents a data-driven spatial interpolation algorithm based on
physics-informed graph neural networks used to develop national
temperature-at-depth maps for the conterminous United States. The model was
trained to approximately satisfy the three-dimensional heat conduction law by
simultaneously predicting subsurface temperature, surface heat flow, and rock
thermal conductivity. In addition to bottomhole temperature measurements, we
incorporated other physical quantities as model inputs, such as depth,
geographic coordinates, elevation, sediment thickness, magnetic anomaly,
gravity anomaly, gamma-ray flux of radioactive elements, seismicity, and
electric conductivity. We constructed surface heat flow, and temperature and
thermal conductivity predictions for depths of 0-7 km at an interval of 1 km
with spatial resolution of 18 km$^2$ per grid cell. Our model showed superior
temperature, surface heat flow and thermal conductivity mean absolute errors of
4.8{\deg} C, 5.817 mW/m$^2$ and 0.022 W/(C-m)$, respectively. The predictions
were visualized in two-dimensional spatial maps across the modeled depths. This
thorough modeling of the Earth's thermal processes is crucial to understanding
subsurface phenomena and exploiting natural underground resources.</div><div><a href='http://arxiv.org/abs/2403.09961v1'>2403.09961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12717v1")'>Gas trap prediction from 3D seismic and well test data using machine
  learning</div>
<div id='2401.12717v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T12:39:15Z</div><div>Authors: Dmitry Ivlev</div><div style='padding-top: 10px; width: 80ex'>The aim of this work is to create and apply a methodological approach for
predicting gas traps from 3D seismic data and gas well testing. The paper
formalizes the approach to creating a training dataset by selecting volumes
with established gas saturation and filtration properties within the seismic
wavefield. The training dataset thus created is used in a process stack of
sequential application of data processing methods and ensemble machine learning
algorithms. As a result, a cube of calibrated probabilities of belonging of the
study space to gas reservoirs was obtained. The high efficiency of this
approach is shown on a delayed test sample of three wells (blind wells). The
final value of the gas reservoir prediction quality metric f1 score was
0.893846.</div><div><a href='http://arxiv.org/abs/2401.12717v1'>2401.12717v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10404v1")'>A comparative study on machine learning approaches for rock mass
  classification using drilling data</div>
<div id='2403.10404v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:37:19Z</div><div>Authors: Tom F. Hansen, Georg H. Erharter, Zhongqiang Liu, Jim Torresen</div><div style='padding-top: 10px; width: 80ex'>Current rock engineering design in drill and blast tunnelling primarily
relies on engineers' observational assessments. Measure While Drilling (MWD)
data, a high-resolution sensor dataset collected during tunnel excavation, is
underutilised, mainly serving for geological visualisation. This study aims to
automate the translation of MWD data into actionable metrics for rock
engineering. It seeks to link data to specific engineering actions, thus
providing critical decision support for geological challenges ahead of the
tunnel face. Leveraging a large and geologically diverse dataset of 500,000
drillholes from 15 tunnels, the research introduces models for accurate rock
mass quality classification in a real-world tunnelling context. Both
conventional machine learning and image-based deep learning are explored to
classify MWD data into Q-classes and Q-values, examples of metrics describing
the stability of the rock mass, using both tabular and image data. The results
indicate that the K-nearest neighbours algorithm in an ensemble with tree-based
models using tabular data, effectively classifies rock mass quality. It
achieves a cross-validated balanced accuracy of 0.86 in classifying rock mass
into the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classification
with E versus the rest. Classification using a CNN with MWD-images for each
blasting round resulted in a balanced accuracy of 0.82 for binary
classification. Regressing the Q-value from tabular MWD-data achieved
cross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble model
as in classification. High performance in regression and classification boosts
confidence in automated rock mass assessment. Applying advanced modelling on a
unique dataset demonstrates MWD data's value in improving rock mass
classification accuracy and advancing data-driven rock engineering design,
reducing manual intervention.</div><div><a href='http://arxiv.org/abs/2403.10404v1'>2403.10404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15106v1")'>Sampling-based Distributed Training with Message Passing Neural Network</div>
<div id='2402.15106v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T05:33:43Z</div><div>Authors: Priyesh Kakka, Sheel Nidhan, Rishikesh Ranade, Jonathan F. MacArt</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce a domain-decomposition-based distributed training
and inference approach for message-passing neural networks (MPNN). Our
objective is to address the challenge of scaling edge-based graph neural
networks as the number of nodes increases. Through our distributed training
approach, coupled with Nystr\"om-approximation sampling techniques, we present
a scalable graph neural network, referred to as DS-MPNN (D and S standing for
distributed and sampled, respectively), capable of scaling up to $O(10^5)$
nodes. We validate our sampling and distributed training approach on two cases:
(a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils,
providing comparisons with both single-GPU implementation and node-based graph
convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy
to single-GPU implementation, can accommodate a significantly larger number of
nodes compared to the single-GPU variant (S-MPNN), and significantly
outperforms the node-based GCN.</div><div><a href='http://arxiv.org/abs/2402.15106v1'>2402.15106v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07851v1")'>Comparing skill of historical rainfall data based monsoon rainfall
  prediction in India with NCEP-NWP forecasts</div>
<div id='2402.07851v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:59:20Z</div><div>Authors: Apoorva Narula, Aastha Jain, Jatin Batra, Sandeep Juneja</div><div style='padding-top: 10px; width: 80ex'>In this draft we consider the problem of forecasting rainfall across India
during the four monsoon months, one day as well as three days in advance. We
train neural networks using historical daily gridded precipitation data for
India obtained from IMD for the time period $1901- 2022$, at a spatial
resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical
weather prediction (NWP) forecasts obtained from NCEP (National Centre for
Environmental Prediction) available for the period 2011-2022. We conduct a
detailed country wide analysis and separately analyze some of the most
populated cities in India. Our conclusion is that forecasts obtained by
applying deep learning to historical rainfall data are more accurate compared
to NWP forecasts as well as predictions based on persistence. On average,
compared to our predictions, forecasts from NCEP-NWP model have about 34%
higher error for a single day prediction, and over 68% higher error for a three
day prediction. Similarly, persistence estimates report a 29% higher error in a
single day forecast, and over 54% error in a three day forecast. We further
observe that data up to 20 days in the past is useful in reducing errors of one
and three day forecasts, when a transformer based learning architecture, and to
a lesser extent when an LSTM is used. A key conclusion suggested by our
preliminary analysis is that NWP forecasts can be substantially improved upon
through more and diverse data relevant to monsoon prediction combined with
carefully selected neural network architecture.</div><div><a href='http://arxiv.org/abs/2402.07851v1'>2402.07851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01208v1")'>Location Agnostic Adaptive Rain Precipitation Prediction using Deep
  Learning</div>
<div id='2402.01208v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T08:26:42Z</div><div>Authors: Md Shazid Islam, Md Saydur Rahman, Md Saad Ul Haque, Farhana Akter Tumpa, Md Sanzid Bin Hossain, Abul Al Arabi</div><div style='padding-top: 10px; width: 80ex'>Rain precipitation prediction is a challenging task as it depends on weather
and meteorological features which vary from location to location. As a result,
a prediction model that performs well at one location does not perform well at
other locations due to the distribution shifts. In addition, due to global
warming, the weather patterns are changing very rapidly year by year which
creates the possibility of ineffectiveness of those models even at the same
location as time passes. In our work, we have proposed an adaptive deep
learning-based framework in order to provide a solution to the aforementioned
challenges. Our method can generalize the model for the prediction of
precipitation for any location where the methods without adaptation fail. Our
method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a
deep neural network for predicting the precipitation of Paris, Los Angeles, and
Tokyo, respectively.</div><div><a href='http://arxiv.org/abs/2402.01208v1'>2402.01208v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09846v1")'>A Deep Learning Approach to Radar-based QPE</div>
<div id='2402.09846v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T10:05:18Z</div><div>Authors: Ting-Shuo Yo, Shih-Hao Su, Jung-Lien Chu, Chiao-Wei Chang, Hung-Chi Kuo</div><div style='padding-top: 10px; width: 80ex'>In this study, we propose a volume-to-point framework for quantitative
precipitation estimation (QPE) based on the Quantitative Precipitation
Estimation and Segregation Using Multiple Sensor (QPESUMS) Mosaic Radar data
set. With a data volume consisting of the time series of gridded radar
reflectivities over the Taiwan area, we used machine learning algorithms to
establish a statistical model for QPE in weather stations. The model extracts
spatial and temporal features from the input data volume and then associates
these features with the location-specific precipitations. In contrast to QPE
methods based on the Z-R relation, we leverage the machine learning algorithms
to automatically detect the evolution and movement of weather systems and
associate these patterns to a location with specific topographic attributes.
Specifically, we evaluated this framework with the hourly precipitation data of
45 weather stations in Taipei during 2013-2016. In comparison to the
operational QPE scheme used by the Central Weather Bureau, the volume-to-point
framework performed comparably well in general cases and excelled in detecting
heavy-rainfall events. By using the current results as the reference benchmark,
the proposed method can integrate the heterogeneous data sources and
potentially improve the forecast in extreme precipitation scenarios.</div><div><a href='http://arxiv.org/abs/2402.09846v1'>2402.09846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00023v1")'>Using Multi-Temporal Sentinel-1 and Sentinel-2 data for water bodies
  mapping</div>
<div id='2402.00023v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T18:11:08Z</div><div>Authors: Luigi Russo, Francesco Mauro, Babak Memar, Alessandro Sebastianelli, Paolo Gamba, Silvia Liberata Ullo</div><div style='padding-top: 10px; width: 80ex'>Climate change is intensifying extreme weather events, causing both water
scarcity and severe rainfall unpredictability, and posing threats to
sustainable development, biodiversity, and access to water and sanitation. This
paper aims to provide valuable insights for comprehensive water resource
monitoring under diverse meteorological conditions. An extension of the
SEN2DWATER dataset is proposed to enhance its capabilities for water basin
segmentation. Through the integration of temporally and spatially aligned radar
information from Sentinel-1 data with the existing multispectral Sentinel-2
data, a novel multisource and multitemporal dataset is generated. Benchmarking
the enhanced dataset involves the application of indices such as the Soil Water
Index (SWI) and Normalized Difference Water Index (NDWI), along with an
unsupervised Machine Learning (ML) classifier (k-means clustering). Promising
results are obtained and potential future developments and applications arising
from this research are also explored.</div><div><a href='http://arxiv.org/abs/2402.00023v1'>2402.00023v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13135v1")'>A Parallel Workflow for Polar Sea-Ice Classification using Auto-labeling
  of Sentinel-2 Imagery</div>
<div id='2403.13135v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T20:10:50Z</div><div>Authors: Jurdana Masuma Iqrah, Wei Wang, Hongjie Xie, Sushil Prasad</div><div style='padding-top: 10px; width: 80ex'>The observation of the advancing and retreating pattern of polar sea ice
cover stands as a vital indicator of global warming. This research aims to
develop a robust, effective, and scalable system for classifying polar sea ice
as thick/snow-covered, young/thin, or open water using Sentinel-2 (S2) images.
Since the S2 satellite is actively capturing high-resolution imagery over the
earth's surface, there are lots of images that need to be classified. One major
obstacle is the absence of labeled S2 training data (images) to act as the
ground truth. We demonstrate a scalable and accurate method for segmenting and
automatically labeling S2 images using carefully determined color thresholds.
We employ a parallel workflow using PySpark to scale and achieve 9-fold data
loading and 16-fold map-reduce speedup on auto-labeling S2 images based on thin
cloud and shadow-filtered color-based segmentation to generate label data. The
auto-labeled data generated from this process are then employed to train a
U-Net machine learning model, resulting in good classification accuracy. As
training the U-Net classification model is computationally heavy and
time-consuming, we distribute the U-Net model training to scale it over 8 GPUs
using the Horovod framework over a DGX cluster with a 7.21x speedup without
affecting the accuracy of the model. Using the Antarctic's Ross Sea region as
an example, the U-Net model trained on auto-labeled data achieves a
classification accuracy of 98.97% for auto-labeled training datasets when the
thin clouds and shadows from the S2 images are filtered out.</div><div><a href='http://arxiv.org/abs/2403.13135v1'>2403.13135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01648v1")'>Forecasting Imports in OECD Member Countries and Iran by Using Neural
  Network Algorithms of LSTM</div>
<div id='2402.01648v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T17:34:26Z</div><div>Authors: Soheila Khajoui, Saeid Dehyadegari, Sayyed Abdolmajid Jalaee</div><div style='padding-top: 10px; width: 80ex'>Artificial Neural Networks (ANN) which are a branch of artificial
intelligence, have shown their high value in lots of applications and are used
as a suitable forecasting method. Therefore, this study aims at forecasting
imports in OECD member selected countries and Iran for 20 seasons from 2021 to
2025 by means of ANN. Data related to the imports of such countries collected
over 50 years from 1970 to 2019 from valid resources including World Bank, WTO,
IFM,the data turned into seasonal data to increase the number of collected data
for better performance and high accuracy of the network by using Diz formula
that there were totally 200 data related to imports. This study has used LSTM
to analyse data in Pycharm. 75% of data considered as training data and 25%
considered as test data and the results of the analysis were forecasted with
99% accuracy which revealed the validity and reliability of the output. Since
the imports is consumption function and since the consumption is influenced
during Covid-19 Pandemic, so it is time-consuming to correct and improve it to
be influential on the imports, thus the imports in the years after Covid-19
Pandemic has had a fluctuating trend.</div><div><a href='http://arxiv.org/abs/2402.01648v1'>2402.01648v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15469v2")'>Wind speed super-resolution and validation: from ERA5 to CERRA via
  diffusion models</div>
<div id='2401.15469v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T17:43:08Z</div><div>Authors: Fabio Merizzi, Andrea Asperti, Stefano Colamonaco</div><div style='padding-top: 10px; width: 80ex'>The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution
regional reanalysis dataset for the European domain. In recent years it has
shown significant utility across various climate-related tasks, ranging from
forecasting and climate change research to renewable energy prediction,
resource management, air quality risk assessment, and the forecasting of rare
events, among others. Unfortunately, the availability of CERRA is lagging two
years behind the current date, due to constraints in acquiring the requisite
external data and the intensive computational demands inherent in its
generation. As a solution, this paper introduces a novel method using diffusion
models to approximate CERRA downscaling in a data-driven manner, without
additional informations. By leveraging the lower resolution ERA5 dataset, which
provides boundary conditions for CERRA, we approach this as a super-resolution
task. Focusing on wind speed around Italy, our model, trained on existing CERRA
data, shows promising results, closely mirroring original CERRA data.
Validation with in-situ observations further confirms the model's accuracy in
approximating ground measurements.</div><div><a href='http://arxiv.org/abs/2401.15469v2'>2401.15469v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07604v1")'>Data Assimilation using ERA5, ASOS, and the U-STN model for Weather
  Forecasting over the UK</div>
<div id='2401.07604v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T11:21:25Z</div><div>Authors: Wenqi Wang, Jacob Bieker, Rossella Arcucci, César Quilodrán-Casas</div><div style='padding-top: 10px; width: 80ex'>In recent years, the convergence of data-driven machine learning models with
Data Assimilation (DA) offers a promising avenue for enhancing weather
forecasting. This study delves into this emerging trend, presenting our
methodologies and outcomes. We harnessed the UK's local ERA5 850 hPa
temperature data and refined the U-STN12 global weather forecasting model,
tailoring its predictions to the UK's climate nuances. From the ASOS network,
we sourced T2m data, representing ground observations across the UK. We
employed the advanced kriging method with a polynomial drift term for
consistent spatial resolution. Furthermore, Gaussian noise was superimposed on
the ERA5 T850 data, setting the stage for ensuing multi-time step synthetic
observations. Probing into the assimilation impacts, the ASOS T2m data was
integrated with the ERA5 T850 dataset. Our insights reveal that while global
forecast models can adapt to specific regions, incorporating atmospheric data
in DA significantly bolsters model accuracy. Conversely, the direct
assimilation of surface temperature data tends to mitigate this enhancement,
tempering the model's predictive prowess.</div><div><a href='http://arxiv.org/abs/2401.07604v1'>2401.07604v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07822v1")'>Fusing Climate Data Products using a Spatially Varying Autoencoder</div>
<div id='2403.07822v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:03:07Z</div><div>Authors: Jacob A. Johnson, Matthew J. Heaton, William F. Christensen, Lynsie R. Warr, Summer B. Rupper</div><div style='padding-top: 10px; width: 80ex'>Autoencoders are powerful machine learning models used to compress
information from multiple data sources. However, autoencoders, like all
artificial neural networks, are often unidentifiable and uninterpretable. This
research focuses on creating an identifiable and interpretable autoencoder that
can be used to meld and combine climate data products. The proposed autoencoder
utilizes a Bayesian statistical framework, allowing for probabilistic
interpretations while also varying spatially to capture useful spatial patterns
across the various data products. Constraints are placed on the autoencoder as
it learns patterns in the data, creating an interpretable consensus that
includes the important features from each input. We demonstrate the utility of
the autoencoder by combining information from multiple precipitation products
in High Mountain Asia.</div><div><a href='http://arxiv.org/abs/2403.07822v1'>2403.07822v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09471v1")'>Machine Learning for Stochastic Parametrisation</div>
<div id='2402.09471v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T08:38:53Z</div><div>Authors: Hannah M. Christensen, Salah Kouhen, Greta Miller, Raghul Parthipan</div><div style='padding-top: 10px; width: 80ex'>Atmospheric models used for weather and climate prediction are traditionally
formulated in a deterministic manner. In other words, given a particular state
of the resolved scale variables, the most likely forcing from the sub-grid
scale processes is estimated and used to predict the evolution of the
large-scale flow. However, the lack of scale-separation in the atmosphere means
that this approach is a large source of error in forecasts. Over recent years,
an alternative paradigm has developed: the use of stochastic techniques to
characterise uncertainty in small-scale processes. These techniques are now
widely used across weather, sub-seasonal, seasonal, and climate timescales. In
parallel, recent years have also seen significant progress in replacing
parametrisation schemes using machine learning (ML). This has the potential to
both speed up and improve our numerical models. However, the focus to date has
largely been on deterministic approaches. In this position paper, we bring
together these two key developments, and discuss the potential for data-driven
approaches for stochastic parametrisation. We highlight early studies in this
area, and draw attention to the novel challenges that remain.</div><div><a href='http://arxiv.org/abs/2402.09471v1'>2402.09471v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18354v1")'>SuperdropNet: a Stable and Accurate Machine Learning Proxy for
  Droplet-based Cloud Microphysics</div>
<div id='2402.18354v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T14:26:16Z</div><div>Authors: Shivani Sharma, David Greenberg</div><div style='padding-top: 10px; width: 80ex'>Cloud microphysics has important consequences for climate and weather
phenomena, and inaccurate representations can limit forecast accuracy. While
atmospheric models increasingly resolve storms and clouds, the accuracy of the
underlying microphysics remains limited by computationally expedient bulk
moment schemes based on simplifying assumptions. Droplet-based Lagrangian
schemes are more accurate but are underutilized due to their large
computational overhead. Machine learning (ML) based schemes can bridge this gap
by learning from vast droplet-based simulation datasets, but have so far
struggled to match the accuracy and stability of bulk moment schemes. To
address this challenge, we developed SuperdropNet, an ML-based emulator of the
Lagrangian superdroplet simulations. To improve accuracy and stability, we
employ multi-step autoregressive prediction during training, impose physical
constraints, and carefully control stochasticity in the training data.
Superdropnet predicted hydrometeor states and cloud-to-rain transition times
more accurately than previous ML emulators, and matched or outperformed bulk
moment schemes in many cases. We further carried out detailed analyses to
reveal how multistep autoregressive training improves performance, and how the
performance of SuperdropNet and other microphysical schemes hydrometeors' mass,
number and size distribution. Together our results suggest that ML models can
effectively emulate cloud microphysics, in a manner consistent with
droplet-based simulations.</div><div><a href='http://arxiv.org/abs/2402.18354v1'>2402.18354v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09493v3")'>Identifying Three-Dimensional Radiative Patterns Associated with Early
  Tropical Cyclone Intensification</div>
<div id='2401.09493v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T01:17:12Z</div><div>Authors: Frederick Iat-Hin Tam, Tom Beucler, James H. Ruppert Jr</div><div style='padding-top: 10px; width: 80ex'>Cloud radiative feedback impacts early tropical cyclone (TC) intensification,
but limitations in existing diagnostic frameworks make them unsuitable for
studying asymmetric or transient radiative heating. We propose a linear
Variational Encoder-Decoder (VED) to learn the hidden relationship between
radiation and the surface intensification of realistic simulated TCs. Limiting
VED model inputs enables using its uncertainty to identify periods when
radiation has more importance for intensification. A close examination of the
extracted 3D radiative structures suggests that longwave radiative forcing from
inner core deep convection and shallow clouds both contribute to
intensification, with the deep convection having the most impact overall. We
find that deep convection downwind of the shallow clouds is critical to the
intensification of Haiyan. Our work demonstrates that machine learning can
discover thermodynamic-kinematic relationships without relying on axisymmetric
or deterministic assumptions, paving the way towards the objective discovery of
processes leading to TC intensification in realistic conditions.</div><div><a href='http://arxiv.org/abs/2401.09493v3'>2401.09493v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15305v1")'>A Practical Probabilistic Benchmark for AI Weather Models</div>
<div id='2401.15305v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T05:53:16Z</div><div>Authors: Noah D. Brenowitz, Yair Cohen, Jaideep Pathak, Ankur Mahesh, Boris Bonev, Thorsten Kurth, Dale R. Durran, Peter Harrington, Michael S. Pritchard</div><div style='padding-top: 10px; width: 80ex'>Since the weather is chaotic, forecasts aim to predict the distribution of
future states rather than make a single prediction. Recently, multiple data
driven weather models have emerged claiming breakthroughs in skill. However,
these have mostly been benchmarked using deterministic skill scores, and little
is known about their probabilistic skill. Unfortunately, it is hard to fairly
compare AI weather models in a probabilistic sense, since variations in choice
of ensemble initialization, definition of state, and noise injection
methodology become confounding. Moreover, even obtaining ensemble forecast
baselines is a substantial engineering challenge given the data volumes
involved. We sidestep both problems by applying a decades-old idea -- lagged
ensembles -- whereby an ensemble can be constructed from a moderately-sized
library of deterministic forecasts. This allows the first parameter-free
intercomparison of leading AI weather models' probabilistic skill against an
operational baseline. The results reveal that two leading AI weather models,
i.e. GraphCast and Pangu, are tied on the probabilistic CRPS metric even though
the former outperforms the latter in deterministic scoring. We also reveal how
multiple time-step loss functions, which many data-driven weather models have
employed, are counter-productive: they improve deterministic metrics at the
cost of increased dissipation, deteriorating probabilistic skill. This is
confirmed through ablations applied to a spherical Fourier Neural Operator
(SFNO) approach to AI weather forecasting. Separate SFNO ablations modulating
effective resolution reveal it has a useful effect on ensemble dispersion
relevant to achieving good ensemble calibration. We hope these and forthcoming
insights from lagged ensembles can help guide the development of AI weather
forecasts and have thus shared the diagnostic code.</div><div><a href='http://arxiv.org/abs/2401.15305v1'>2401.15305v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16669v1")'>Is Artificial Intelligence Providing the Second Revolution for Weather
  Forecasting?</div>
<div id='2401.16669v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T01:34:43Z</div><div>Authors: Fenghua Ling, Lin Ouyang, Boufeniza Redouane Larbi, Jing-Jia Luo, Tao Han, Xiaohui Zhong, Lei Bai</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of artificial intelligence technologies, particularly
in recent years, has led to the emergence of several large parameter artificial
intelligence weather forecast models. These models represent a significant
breakthrough, overcoming the limitations of traditional numerical weather
prediction models and indicating a potential second revolution for weather
forecast. This study explores the evolution of these advanced artificial
intelligence forecast models, and based on the identified commonalities,
proposes the "Three Large Rules" for their development. We discuss the
potential of artificial intelligence in revolutionizing numerical weather
prediction, briefly outlining the underlying reasons for this potential.
Additionally, we explore key areas for future development prospects for large
artificial intelligence weather forecast models, integrating the entire
numerical prediction process. Through an example that combines a large
artificial intelligence model with ocean wave forecasting, we illustrate how
forecasters can adapt and leverage the advanced artificial intelligence model.
While acknowledging the high accuracy, computational efficiency, and ease of
deployment of large artificial intelligence forecast models, we emphasize the
irreplaceable values of traditional numerical forecasts. We believe that the
optimal future of weather forecasting lies in achieving a seamless integration
of artificial intelligence and traditional numerical models. Such a synthesis
is anticipated to offer a more comprehensive and reliable approach for future
weather forecasting.</div><div><a href='http://arxiv.org/abs/2401.16669v1'>2401.16669v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03702v1")'>Online model error correction with neural networks: application to the
  Integrated Forecasting System</div>
<div id='2403.03702v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T13:36:31Z</div><div>Authors: Alban Farchi, Marcin Chrust, Marc Bocquet, Massimo Bonavita</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been significant progress in the development of
fully data-driven global numerical weather prediction models. These machine
learning weather prediction models have their strength, notably accuracy and
low computational requirements, but also their weakness: they struggle to
represent fundamental dynamical balances, and they are far from being suitable
for data assimilation experiments. Hybrid modelling emerges as a promising
approach to address these limitations. Hybrid models integrate a physics-based
core component with a statistical component, typically a neural network, to
enhance prediction capabilities. In this article, we propose to develop a model
error correction for the operational Integrated Forecasting System (IFS) of the
European Centre for Medium-Range Weather Forecasts using a neural network. The
neural network is initially pre-trained offline using a large dataset of
operational analyses and analysis increments. Subsequently, the trained network
is integrated into the IFS within the Object-Oriented Prediction System (OOPS)
so as to be used in data assimilation and forecast experiments. It is then
further trained online using a recently developed variant of weak-constraint
4D-Var. The results show that the pre-trained neural network already provides a
reliable model error correction, which translates into reduced forecast errors
in many conditions and that the online training further improves the accuracy
of the hybrid model in many conditions.</div><div><a href='http://arxiv.org/abs/2403.03702v1'>2403.03702v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15027v1")'>Grey-informed neural network for time-series forecasting</div>
<div id='2403.15027v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T08:17:00Z</div><div>Authors: Wanli Xie, Ruibin Zhao, Zhenguo Xu, Tingting Liang</div><div style='padding-top: 10px; width: 80ex'>Neural network models have shown outstanding performance and successful
resolutions to complex problems in various fields. However, the majority of
these models are viewed as black-box, requiring a significant amount of data
for development. Consequently, in situations with limited data, constructing
appropriate models becomes challenging due to the lack of transparency and
scarcity of data. To tackle these challenges, this study suggests the
implementation of a grey-informed neural network (GINN). The GINN ensures that
the output of the neural network follows the differential equation model of the
grey system, improving interpretability. Moreover, incorporating prior
knowledge from grey system theory enables traditional neural networks to
effectively handle small data samples. Our proposed model has been observed to
uncover underlying patterns in the real world and produce reliable forecasts
based on empirical data.</div><div><a href='http://arxiv.org/abs/2403.15027v1'>2403.15027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18576v1")'>Improved Forecasting Using a PSO-RDV Framework to Enhance Artificial
  Neural Network</div>
<div id='2402.18576v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T01:15:33Z</div><div>Authors: Sales Aribe Jr</div><div style='padding-top: 10px; width: 80ex'>Decision making and planning have long relied heavily on AI-driven forecasts.
The government and the general public are working to minimize the risks while
maximizing benefits in the face of potential future public health
uncertainties. This study used an improved method of forecasting utilizing the
Random Descending Velocity Inertia Weight (RDV IW) technique to improve the
convergence of Particle Swarm Optimization (PSO) and the accuracy of Artificial
Neural Network (ANN). The IW technique, inspired by the motions of a golf ball,
modified the particles' velocities as they approached the solution point to a
parabolically descending structure. Simulation results revealed that the
proposed forecasting model with [0.4, 0.9] combination of alpha and alpha_dump
exhibits a 6.36% improvement in position error and 11.75% improvement in
computational time compared to the old model, thus, improving its convergence.
It reached the optimum level at minimal steps with 12.50% improvement as
against the old model since it provides better velocity averages when speed
stabilization occurs at the 24th iteration. Meanwhile, the computed p-values
for NRMSE (0.04889174), MAE (0.02829063), MAPE (0.02226053), WAPE (0.01701545),
and R2 (0.00000021) of the proposed algorithm are less than the set 0.05 level
of significance, thus the values indicated a significant result in terms of
accuracy performance. Applying the modified ANN-PSO using RDV IW technique
greatly improved the new HIV/AIDS forecasting model compared with the two
models.</div><div><a href='http://arxiv.org/abs/2402.18576v1'>2402.18576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14169v3")'>A Temporal Bias Correction using a Machine Learning Attention model</div>
<div id='2402.14169v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T23:18:03Z</div><div>Authors: Omer Nivron, Damon J. Wischik, Mathieu Vrac, Emily Shuckburgh</div><div style='padding-top: 10px; width: 80ex'>Climate models are biased with respect to real world observations and usually
need to be calibrated prior to impact studies. The suite of statistical methods
that enable such calibrations is called bias correction (BC). However, current
BC methods struggle to adjust for temporal biases, because they disregard the
dependence between consecutive time-points. As a result, climate statistics
with long-range temporal properties, such as heatwave duration and frequency,
cannot be corrected accurately, making it more difficult to produce reliable
impact studies on such climate statistics. In this paper, we offer a novel BC
methodology to correct for temporal biases. This is made possible by i)
re-thinking BC as a probability model rather than an algorithmic procedure, and
ii) adapting state-of-the-art machine-learning (ML) probabilistic attention
models to fit the BC task. With a case study of heatwave duration statistics in
Abuja, Nigeria, and Tokyo, Japan, we show striking results compared to current
climate model outputs and alternative BC methods.</div><div><a href='http://arxiv.org/abs/2402.14169v3'>2402.14169v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14877v1")'>Machine-learning prediction of tipping and collapse of the Atlantic
  Meridional Overturning Circulation</div>
<div id='2402.14877v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T20:59:19Z</div><div>Authors: Shirin Panahi, Ling-Wei Kong, Mohammadamin Moradi, Zheng-Meng Zhai, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</div><div style='padding-top: 10px; width: 80ex'>Recent research on the Atlantic Meridional Overturning Circulation (AMOC)
raised concern about its potential collapse through a tipping point due to the
climate-change caused increase in the freshwater input into the North Atlantic.
The predicted time window of collapse is centered about the middle of the
century and the earliest possible start is approximately two years from now.
More generally, anticipating a tipping point at which the system transitions
from one stable steady state to another is relevant to a broad range of fields.
We develop a machine-learning approach to predicting tipping in noisy dynamical
systems with a time-varying parameter and test it on a number of systems
including the AMOC, ecological networks, an electrical power system, and a
climate model. For the AMOC, our prediction based on simulated fingerprint data
and real data of the sea surface temperature places the time window of a
potential collapse between the years 2040 and 2065.</div><div><a href='http://arxiv.org/abs/2402.14877v1'>2402.14877v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12366v1")'>U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted
  Ensemble Data Assimilation</div>
<div id='2403.12366v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:23:12Z</div><div>Authors: Feiyu Lu</div><div style='padding-top: 10px; width: 80ex'>Machine learning techniques have seen a tremendous rise in popularity in
weather and climate sciences. Data assimilation (DA), which combines
observations and numerical models, has great potential to incorporate machine
learning and artificial intelligence (ML/AI) techniques. In this paper, we use
U-Net, a type of convolutional neutral network (CNN), to predict the localized
ensemble covariances for the Ensemble Kalman Filter (EnKF) algorithm. Using a
2-layer quasi-geostrophic model, U-Nets are trained using data from EnKF DA
experiments. The trained U-Nets are then used to predict the flow-dependent
localized error covariance matrices in U-Net Kalman Filter (UNetKF)
experiments, which are compared to traditional 3-dimensional variational
(3DVar), ensemble 3DVar (En3DVar) and EnKF methods. The performance of UNetKF
can match or exceed that of 3DVar, En3DVar or EnKF. We also demonstrate that
trained U-Nets can be transferred to a higher-resolution model for UNetKF
implementation, which again performs competitively to 3DVar and EnKF,
particularly for small ensemble sizes.</div><div><a href='http://arxiv.org/abs/2403.12366v1'>2403.12366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13270v1")'>Global Tropical Cyclone Intensity Forecasting with Multi-modal
  Multi-scale Causal Autoregressive Model</div>
<div id='2402.13270v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T15:26:33Z</div><div>Authors: Xinyu Wang, Kang Chen, Lei Liu, Tao Han, Bin Li, Lei Bai</div><div style='padding-top: 10px; width: 80ex'>Accurate forecasting of Tropical cyclone (TC) intensity is crucial for
formulating disaster risk reduction strategies. Current methods predominantly
rely on limited spatiotemporal information from ERA5 data and neglect the
causal relationships between these physical variables, failing to fully capture
the spatial and temporal patterns required for intensity forecasting. To
address this issue, we propose a Multi-modal multi-Scale Causal AutoRegressive
model (MSCAR), which is the first model that combines causal relationships with
large-scale multi-modal data for global TC intensity autoregressive
forecasting. Furthermore, given the current absence of a TC dataset that offers
a wide range of spatial variables, we present the Satellite and ERA5-based
Tropical Cyclone Dataset (SETCD), which stands as the longest and most
comprehensive global dataset related to TCs. Experiments on the dataset show
that MSCAR outperforms the state-of-the-art methods, achieving maximum
reductions in global and regional forecast errors of 9.52% and 6.74%,
respectively. The code and dataset are publicly available at
https://anonymous.4open.science/r/MSCAR.</div><div><a href='http://arxiv.org/abs/2402.13270v1'>2402.13270v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02665v1")'>Zero-shot Microclimate Prediction with Deep Learning</div>
<div id='2401.02665v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T06:46:56Z</div><div>Authors: Iman Deznabi, Peeyush Kumar, Madalina Fiterau</div><div style='padding-top: 10px; width: 80ex'>Weather station data is a valuable resource for climate prediction, however,
its reliability can be limited in remote locations. To compound the issue,
making local predictions often relies on sensor data that may not be accessible
for a new, previously unmonitored location. In response to these challenges, we
propose a novel zero-shot learning approach designed to forecast various
climate measurements at new and unmonitored locations. Our method surpasses
conventional weather forecasting techniques in predicting microclimate
variables by leveraging knowledge extracted from other geographic locations.</div><div><a href='http://arxiv.org/abs/2401.02665v1'>2401.02665v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08061v1")'>Augmenting Ground-Level PM2.5 Prediction via Kriging-Based Pseudo-Label
  Generation</div>
<div id='2401.08061v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T02:42:45Z</div><div>Authors: Lei Duan, Ziyang Jiang, David Carlson</div><div style='padding-top: 10px; width: 80ex'>Fusing abundant satellite data with sparse ground measurements constitutes a
major challenge in climate modeling. To address this, we propose a strategy to
augment the training dataset by introducing unlabeled satellite images paired
with pseudo-labels generated through a spatial interpolation technique known as
ordinary kriging, thereby making full use of the available satellite data
resources. We show that the proposed data augmentation strategy helps enhance
the performance of the state-of-the-art convolutional neural network-random
forest (CNN-RF) model by a reasonable amount, resulting in a noteworthy
improvement in spatial correlation and a reduction in prediction error.</div><div><a href='http://arxiv.org/abs/2401.08061v1'>2401.08061v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02922v1")'>From Spectra to Biophysical Insights: End-to-End Learning with a Biased
  Radiative Transfer Model</div>
<div id='2403.02922v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T12:38:54Z</div><div>Authors: Yihang She, Clement Atzberger, Andrew Blake, Srinivasan Keshav</div><div style='padding-top: 10px; width: 80ex'>Advances in machine learning have boosted the use of Earth observation data
for climate change research. Yet, the interpretability of machine-learned
representations remains a challenge, particularly in understanding forests'
biophysical reactions to climate change. Traditional methods in remote sensing
that invert radiative transfer models (RTMs) to retrieve biophysical variables
from spectral data often fail to account for biases inherent in the RTM,
especially for complex forests. We propose to integrate RTMs into an
auto-encoder architecture, creating an end-to-end learning approach. Our method
not only corrects biases in RTMs but also outperforms traditional techniques
for variable retrieval like neural network regression. Furthermore, our
framework has potential generally for inverting biased physical models. The
code is available on https://github.com/yihshe/ai-refined-rtm.git.</div><div><a href='http://arxiv.org/abs/2403.02922v1'>2403.02922v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02139v1")'>Using Deep Ensemble Forest for High Resolution Mapping of PM2.5 from
  MODIS MAIAC AOD in Tehran, Iran</div>
<div id='2402.02139v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T13:01:39Z</div><div>Authors: Hossein Bagheri</div><div style='padding-top: 10px; width: 80ex'>High resolution mapping of PM2.5 concentration over Tehran city is
challenging because of the complicated behavior of numerous sources of
pollution and the insufficient number of ground air quality monitoring
stations. Alternatively, high resolution satellite Aerosol Optical Depth (AOD)
data can be employed for high resolution mapping of PM2.5. For this purpose,
different data-driven methods have been used in the literature. Recently, deep
learning methods have demonstrated their ability to estimate PM2.5 from AOD
data. However, these methods have several weaknesses in solving the problem of
estimating PM2.5 from satellite AOD data. In this paper, the potential of the
deep ensemble forest method for estimating the PM2.5 concentration from AOD
data was evaluated. The results showed that the deep ensemble forest method
with R2 = 0.74 gives a higher accuracy of PM2.5 estimation than deep learning
methods (R2 = 0.67) as well as classic data-driven methods such as random
forest (R2 = 0.68). Additionally, the estimated values of PM2.5 using the deep
ensemble forest algorithm were used along with ground data to generate a high
resolution map of PM2.5. Evaluation of the produced PM2.5 map revealed the good
performance of the deep ensemble forest for modeling the variation of PM2.5 in
the city of Tehran.</div><div><a href='http://arxiv.org/abs/2402.02139v1'>2402.02139v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14657v1")'>Validating Climate Models with Spherical Convolutional Wasserstein
  Distance</div>
<div id='2401.14657v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T05:35:50Z</div><div>Authors: Robert C. Garrett, Trevor Harris, Bo Li, Zhuo Wang</div><div style='padding-top: 10px; width: 80ex'>The validation of global climate models is crucial to ensure the accuracy and
efficacy of model output. We introduce the spherical convolutional Wasserstein
distance to more comprehensively measure differences between climate models and
reanalysis data. This new similarity measure accounts for spatial variability
using convolutional projections and quantifies local differences in the
distribution of climate variables. We apply this method to evaluate the
historical model outputs of the Coupled Model Intercomparison Project (CMIP)
members by comparing them to observational and reanalysis data products.
Additionally, we investigate the progression from CMIP phase 5 to phase 6 and
find modest improvements in the phase 6 models regarding their ability to
produce realistic climatologies.</div><div><a href='http://arxiv.org/abs/2401.14657v1'>2401.14657v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14422v2")'>Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar
  Power Generation</div>
<div id='2401.14422v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T02:08:48Z</div><div>Authors: Md Shazid Islam, A S M Jahid Hasan, Md Saydur Rahman, Jubair Yusuf, Md Saiful Islam Sajol, Farhana Akter Tumpa</div><div style='padding-top: 10px; width: 80ex'>The prediction of solar power generation is a challenging task due to its
dependence on climatic characteristics that exhibit spatial and temporal
variability. The performance of a prediction model may vary across different
places due to changes in data distribution, resulting in a model that works
well in one region but not in others. Furthermore, as a consequence of global
warming, there is a notable acceleration in the alteration of weather patterns
on an annual basis. This phenomenon introduces the potential for diminished
efficacy of existing models, even within the same geographical region, as time
progresses. In this paper, a domain adaptive deep learning-based framework is
proposed to estimate solar power generation using weather features that can
solve the aforementioned challenges. A feed-forward deep convolutional network
model is trained for a known location dataset in a supervised manner and
utilized to predict the solar power of an unknown location later. This adaptive
data-driven approach exhibits notable advantages in terms of computing speed,
storage efficiency, and its ability to improve outcomes in scenarios where
state-of-the-art non-adaptive methods fail. Our method has shown an improvement
of $10.47 \%$, $7.44 \%$, $5.11\%$ in solar power prediction accuracy compared
to best performing non-adaptive method for California (CA), Florida (FL) and
New York (NY), respectively.</div><div><a href='http://arxiv.org/abs/2401.14422v2'>2401.14422v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17196v1")'>Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with
  Uncertainty Quantification</div>
<div id='2402.17196v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T04:23:35Z</div><div>Authors: Yasser Abduallah, Khalid A. Alobaid, Jason T. L. Wang, Haimin Wang, Vania K. Jordanova, Vasyl Yurchyshyn, Huseyin Cavus, Ju Jing</div><div style='padding-top: 10px; width: 80ex'>We propose a novel deep learning framework, named SYMHnet, which employs a
graph neural network and a bidirectional long short-term memory network to
cooperatively learn patterns from solar wind and interplanetary magnetic field
parameters for short-term forecasts of the SYM-H index based on 1-minute and
5-minute resolution data. SYMHnet takes, as input, the time series of the
parameters' values provided by NASA's Space Science Data Coordinated Archive
and predicts, as output, the SYM-H index value at time point t + w hours for a
given time point t where w is 1 or 2. By incorporating Bayesian inference into
the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty
and epistemic (model) uncertainty when predicting future SYM-H indices.
Experimental results show that SYMHnet works well at quiet time and storm time,
for both 1-minute and 5-minute resolution data. The results also show that
SYMHnet generally performs better than related machine learning methods. For
example, SYMHnet achieves a forecast skill score (FSS) of 0.343 compared to the
FSS of 0.074 of a recent gradient boosting machine (GBM) method when predicting
SYM-H indices (1 hour in advance) in a large storm (SYM-H = -393 nT) using
5-minute resolution data. When predicting the SYM-H indices (2 hours in
advance) in the large storm, SYMHnet achieves an FSS of 0.553 compared to the
FSS of 0.087 of the GBM method. In addition, SYMHnet can provide results for
both data and model uncertainty quantification, whereas the related methods
cannot.</div><div><a href='http://arxiv.org/abs/2402.17196v1'>2402.17196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14806v1")'>Difference Learning for Air Quality Forecasting Transport Emulation</div>
<div id='2402.14806v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:58:05Z</div><div>Authors: Reed River Chen, Christopher Ribaudo, Jennifer Sleeman, Chace Ashcraft, Collin Kofroth, Marisa Hughes, Ivanka Stajner, Kevin Viner, Kai Wang</div><div style='padding-top: 10px; width: 80ex'>Human health is negatively impacted by poor air quality including increased
risk for respiratory and cardiovascular disease. Due to a recent increase in
extreme air quality events, both globally and locally in the United States,
finer resolution air quality forecasting guidance is needed to effectively
adapt to these events. The National Oceanic and Atmospheric Administration
provides air quality forecasting guidance for the Continental United States.
Their air quality forecasting model is based on a 15 km spatial resolution;
however, the goal is to reach a three km spatial resolution. This is currently
not feasible due in part to prohibitive computational requirements for modeling
the transport of chemical species. In this work, we describe a deep learning
transport emulator that is able to reduce computations while maintaining skill
comparable with the existing numerical model. We show how this method maintains
skill in the presence of extreme air quality events, making it a potential
candidate for operational use. We also explore evaluating how well this model
maintains the physical properties of the modeled transport for a given set of
species.</div><div><a href='http://arxiv.org/abs/2402.14806v1'>2402.14806v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07027v1")'>FWin transformer for dengue prediction under climate and ocean influence</div>
<div id='2403.07027v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T19:20:55Z</div><div>Authors: Nhat Thanh Tran, Jack Xin, Guofa Zhou</div><div style='padding-top: 10px; width: 80ex'>Dengue fever is one of the most deadly mosquito-born tropical infectious
diseases. Detailed long range forecast model is vital in controlling the spread
of disease and making mitigation efforts. In this study, we examine methods
used to forecast dengue cases for long range predictions. The dataset consists
of local climate/weather in addition to global climate indicators of Singapore
from 2000 to 2019. We utilize newly developed deep neural networks to learn the
intricate relationship between the features. The baseline models in this study
are in the class of recent transformers for long sequence forecasting tasks. We
found that a Fourier mixed window attention (FWin) based transformer performed
the best in terms of both the mean square error and the maximum absolute error
on the long range dengue forecast up to 60 weeks.</div><div><a href='http://arxiv.org/abs/2403.07027v1'>2403.07027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08132v2")'>On the Resurgence of Recurrent Models for Long Sequences -- Survey and
  Research Opportunities in the Transformer Era</div>
<div id='2402.08132v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T23:55:55Z</div><div>Authors: Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, Stefano Melacci</div><div style='padding-top: 10px; width: 80ex'>A longstanding challenge for the Machine Learning community is the one of
developing models that are capable of processing and learning from very long
sequences of data. The outstanding results of Transformers-based networks
(e.g., Large Language Models) promotes the idea of parallel attention as the
key to succeed in such a challenge, obfuscating the role of classic sequential
processing of Recurrent Models. However, in the last few years, researchers who
were concerned by the quadratic complexity of self-attention have been
proposing a novel wave of neural models, which gets the best from the two
worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State
Models emerged as robust approaches to function approximation over time, thus
opening a new perspective in learning from sequential data, followed by many
people in the field and exploited to implement a special class of (linear)
Recurrent Neural Networks. This survey is aimed at providing an overview of
these trends framed under the unifying umbrella of Recurrence. Moreover, it
emphasizes novel research opportunities that become prominent when abandoning
the idea of processing long sequences whose length is known-in-advance for the
more realistic setting of potentially infinite-length sequences, thus
intersecting the field of lifelong-online learning from streamed data.</div><div><a href='http://arxiv.org/abs/2402.08132v2'>2402.08132v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18040v1")'>Automated Discovery of Integral with Deep Learning</div>
<div id='2402.18040v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T04:34:15Z</div><div>Authors: Xiaoxin Yin</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in the realm of deep learning, particularly in the
development of large language models (LLMs), have demonstrated AI's ability to
tackle complex mathematical problems or solving programming challenges.
However, the capability to solve well-defined problems based on extensive
training data differs significantly from the nuanced process of making
scientific discoveries. Trained on almost all human knowledge available,
today's sophisticated LLMs basically learn to predict sequences of tokens. They
generate mathematical derivations and write code in a similar way as writing an
essay, and do not have the ability to pioneer scientific discoveries in the
manner a human scientist would do.
  In this study we delve into the potential of using deep learning to
rediscover a fundamental mathematical concept: integrals. By defining integrals
as area under the curve, we illustrate how AI can deduce the integral of a
given function, exemplified by inferring $\int_{0}^{x} t^2 dt = \frac{x^3}{3}$
and $\int_{0}^{x} ae^{bt} dt = \frac{a}{b} e^{bx} - \frac{a}{b}$. Our
experiments show that deep learning models can approach the task of inferring
integrals either through a sequence-to-sequence model, akin to language
translation, or by uncovering the rudimentary principles of integration, such
as $\int_{0}^{x} t^n dt = \frac{x^{n+1}}{n+1}$.</div><div><a href='http://arxiv.org/abs/2402.18040v1'>2402.18040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00236v1")'>Positional Encoding Helps Recurrent Neural Networks Handle a Large
  Vocabulary</div>
<div id='2402.00236v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T23:32:20Z</div><div>Authors: Takashi Morita</div><div style='padding-top: 10px; width: 80ex'>This study discusses the effects of positional encoding on recurrent neural
networks (RNNs) utilizing synthetic benchmarks. Positional encoding
"time-stamps" data points in time series and complements the capabilities of
Transformer neural networks, which lack an inherent mechanism for representing
the data order. By contrast, RNNs can encode the temporal information of data
points on their own, rendering their use of positional encoding seemingly
"redundant". Nonetheless, empirical investigations reveal the effectiveness of
positional encoding even when coupled with RNNs, specifically for handling a
large vocabulary that yields diverse observations. These findings pave the way
for a new line of research on RNNs, concerning the combination of input-driven
and autonomous time representation. Additionally, biological implications of
the computational/simulational results are discussed, in the light of the
affinity between the sinusoidal implementation of positional encoding and
neural oscillations in biological brains.</div><div><a href='http://arxiv.org/abs/2402.00236v1'>2402.00236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04284v2")'>PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks</div>
<div id='2402.04284v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T01:34:56Z</div><div>Authors: Junwei Su, Difan Zou, Chuan Wu</div><div style='padding-top: 10px; width: 80ex'>Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic
graph neural networks that leverage a memory module to extract, distill, and
memorize long-term temporal dependencies, leading to superior performance
compared to memory-less counterparts. However, training MDGNNs faces the
challenge of handling entangled temporal and structural dependencies, requiring
sequential and chronological processing of data sequences to capture accurate
temporal patterns. During the batch training, the temporal data points within
the same batch will be processed in parallel, while their temporal dependencies
are neglected. This issue is referred to as temporal discontinuity and
restricts the effective temporal batch size, limiting data parallelism and
reducing MDGNNs' flexibility in industrial applications. This paper studies the
efficient training of MDGNNs at scale, focusing on the temporal discontinuity
in training MDGNNs with large temporal batch sizes. We first conduct a
theoretical study on the impact of temporal batch size on the convergence of
MDGNN training. Based on the analysis, we propose PRES, an iterative
prediction-correction scheme combined with a memory coherence learning
objective to mitigate the effect of temporal discontinuity, enabling MDGNNs to
be trained with significantly larger temporal batches without sacrificing
generalization performance. Experimental results demonstrate that our approach
enables up to a 4x larger temporal batch (3.4x speed-up) during MDGNN training.</div><div><a href='http://arxiv.org/abs/2402.04284v2'>2402.04284v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15113v1")'>MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline</div>
<div id='2402.15113v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T05:57:22Z</div><div>Authors: Guangming Sheng, Junwei Su, Chao Huang, Chuan Wu</div><div style='padding-top: 10px; width: 80ex'>Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal
graph neural networks that utilize a node memory module to capture and retain
long-term temporal dependencies, leading to superior performance compared to
memory-less counterparts. However, the iterative reading and updating process
of the memory module in MTGNNs to obtain up-to-date information needs to follow
the temporal dependencies. This introduces significant overhead and limits
training throughput. Existing optimizations for static GNNs are not directly
applicable to MTGNNs due to differences in training paradigm, model
architecture, and the absence of a memory module. Moreover, they do not
effectively address the challenges posed by temporal dependencies, making them
ineffective for MTGNN training. In this paper, we propose MSPipe, a general and
efficient framework for MTGNNs that maximizes training throughput while
maintaining model accuracy. Our design addresses the unique challenges
associated with fetching and updating node memory states in MTGNNs by
integrating staleness into the memory module. However, simply introducing a
predefined staleness bound in the memory module to break temporal dependencies
may lead to suboptimal performance and lack of generalizability across
different models and datasets. To solve this, we introduce an online pipeline
scheduling algorithm in MSPipe that strategically breaks temporal dependencies
with minimal staleness and delays memory fetching to obtain fresher memory
states. Moreover, we design a staleness mitigation mechanism to enhance
training convergence and model accuracy. We provide convergence analysis and
prove that MSPipe maintains the same convergence rate as vanilla sample-based
GNN training. Experimental results show that MSPipe achieves up to 2.45x
speed-up without sacrificing accuracy, making it a promising solution for
efficient MTGNN training.</div><div><a href='http://arxiv.org/abs/2402.15113v1'>2402.15113v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00066v1")'>TrackGPT -- A generative pre-trained transformer for cross-domain entity
  trajectory forecasting</div>
<div id='2402.00066v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T20:05:14Z</div><div>Authors: Nicholas Stroh</div><div style='padding-top: 10px; width: 80ex'>The forecasting of entity trajectories at future points in time is a critical
capability gap in applications across both Commercial and Defense sectors.
Transformers, and specifically Generative Pre-trained Transformer (GPT)
networks have recently revolutionized several fields of Artificial
Intelligence, most notably Natural Language Processing (NLP) with the advent of
Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we
introduce TrackGPT, a GPT-based model for entity trajectory forecasting that
has shown utility across both maritime and air domains, and we expect to
perform well in others. TrackGPT stands as a pioneering GPT model capable of
producing accurate predictions across diverse entity time series datasets,
demonstrating proficiency in generating both long-term forecasts with sustained
accuracy and short-term forecasts with high precision. We present benchmarks
against state-of-the-art deep learning techniques, showing that TrackGPT's
forecasting capability excels in terms of accuracy, reliability, and
modularity. Importantly, TrackGPT achieves these results while remaining
domain-agnostic and requiring minimal data features (only location and time)
compared to models achieving similar performance. In conclusion, our findings
underscore the immense potential of applying GPT architectures to the task of
entity trajectory forecasting, exemplified by the innovative TrackGPT model.</div><div><a href='http://arxiv.org/abs/2402.00066v1'>2402.00066v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05012v1")'>HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for
  Long-Term Forecasting</div>
<div id='2401.05012v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T09:00:03Z</div><div>Authors: Shubao Zhao, Ming Jin, Zhaoxiang Hou, Chengyi Yang, Zengxiang Li, Qingsong Wen, Yi Wang</div><div style='padding-top: 10px; width: 80ex'>Time series forecasting is crucial and challenging in the real world. The
recent surge in interest regarding time series foundation models, which cater
to a diverse array of downstream tasks, is noteworthy. However, existing
methods often overlook the multi-scale nature of time series, an aspect crucial
for precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical
multi-scale masked time series modeling method designed for long-term
forecasting. Specifically, it comprises four integral components: (1)
hierarchical multi-scale transformer (HMT) to capture temporal information at
different scales; (2) decoupled encoder-decoder (DED) forces the encoder to
focus on feature extraction, while the decoder to focus on pretext tasks; (3)
multi-scale masked reconstruction (MMR) provides multi-stage supervision
signals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to
capture dependencies between different scales for forecasting. Collectively,
these components enhance multi-scale feature extraction capabilities in masked
time series modeling and contribute to improved prediction accuracy. We conduct
extensive experiments on 7 mainstream datasets to prove that HiMTM has obvious
advantages over contemporary self-supervised and end-to-end learning methods.
The effectiveness of HiMTM is further showcased by its application in the
industry of natural gas demand forecasting.</div><div><a href='http://arxiv.org/abs/2401.05012v1'>2401.05012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16516v1")'>Generative Pretrained Hierarchical Transformer for Time Series
  Forecasting</div>
<div id='2402.16516v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T11:54:54Z</div><div>Authors: Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, Zhi Li</div><div style='padding-top: 10px; width: 80ex'>Recent efforts have been dedicated to enhancing time series forecasting
accuracy by introducing advanced network architectures and self-supervised
pretraining strategies. Nevertheless, existing approaches still exhibit two
critical drawbacks. Firstly, these methods often rely on a single dataset for
training, limiting the model's generalizability due to the restricted scale of
the training data. Secondly, the one-step generation schema is widely followed,
which necessitates a customized forecasting head and overlooks the temporal
dependencies in the output series, and also leads to increased training costs
under different horizon length settings.
  To address these issues, we propose a novel generative pretrained
hierarchical transformer architecture for forecasting, named GPHT. There are
two aspects of key designs in GPHT. On the one hand, we advocate for
constructing a mixed dataset for pretraining our model, comprising various
datasets from diverse data scenarios. This approach significantly expands the
scale of training data, allowing our model to uncover commonalities in time
series data and facilitating improved transfer to specific datasets. On the
other hand, GPHT employs an auto-regressive forecasting approach under the
channel-independent assumption, effectively modeling temporal dependencies in
the output series. Importantly, no customized forecasting head is required,
enabling a single model to forecast at arbitrary horizon settings. We conduct
sufficient experiments on eight datasets with mainstream self-supervised
pretraining models and supervised models. The results demonstrated that GPHT
surpasses the baseline models across various fine-tuning and zero/few-shot
learning settings in the traditional long-term forecasting task, providing
support for verifying the feasibility of pretrained time series large models.</div><div><a href='http://arxiv.org/abs/2402.16516v1'>2402.16516v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02534v1")'>Towards Foundation Time Series Model: To Synthesize Or Not To
  Synthesize?</div>
<div id='2403.02534v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T23:03:17Z</div><div>Authors: Kseniia Kuvshinova, Olga Tsymboi, Alina Kostromina, Dmitry Simakov, Elizaveta Kovtun</div><div style='padding-top: 10px; width: 80ex'>The industry is rich in cases when we are required to make forecasting for
large amounts of time series at once. However, we might be in a situation where
we can not afford to train a separate model for each of them. Such issue in
time series modeling remains without due attention. The remedy for this setting
is the establishment of a foundation model. Such a model is expected to work in
zero-shot and few-shot regimes. However, what should we take as a training
dataset for such kind of model?
  Witnessing the benefits from the enrichment of NLP datasets with
artificially-generated data, we might want to adopt their experience for time
series. In contrast to natural language, the process of generation of synthetic
time series data is even more favorable because it provides full control of
series patterns, time horizons, and number of samples. In this work, we
consider the essential question if it is advantageous to train a foundation
model on synthetic data or it is better to utilize only a limited number of
real-life examples. Our experiments are conducted only for regular time series
and speak in favor of leveraging solely the real time series. Moreover, the
choice of the proper source dataset strongly influences the performance during
inference. When provided access even to a limited quantity of short time series
data, employing it within a supervised framework yields more favorable results
than training on a larger volume of synthetic data. The code for our
experiments is publicly available on Github
\url{https://github.com/sb-ai-lab/synthesize_or_not}.</div><div><a href='http://arxiv.org/abs/2403.02534v1'>2403.02534v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02010v1")'>GenFormer: A Deep-Learning-Based Approach for Generating Multivariate
  Stochastic Processes</div>
<div id='2402.02010v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T03:50:18Z</div><div>Authors: Haoran Zhao, Wayne Isaac Tan Uy</div><div style='padding-top: 10px; width: 80ex'>Stochastic generators are essential to produce synthetic realizations that
preserve target statistical properties. We propose GenFormer, a stochastic
generator for spatio-temporal multivariate stochastic processes. It is
constructed using a Transformer-based deep learning model that learns a mapping
between a Markov state sequence and time series values. The synthetic data
generated by the GenFormer model preserves the target marginal distributions
and approximately captures other desired statistical properties even in
challenging applications involving a large number of spatial locations and a
long simulation horizon. The GenFormer model is applied to simulate synthetic
wind speed data at various stations in Florida to calculate exceedance
probabilities for risk management.</div><div><a href='http://arxiv.org/abs/2402.02010v1'>2402.02010v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13968v1")'>Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks</div>
<div id='2401.13968v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T06:03:56Z</div><div>Authors: Muhammad Anwar Ma'sum, MD Rasel Sarkar, Mahardhika Pratama, Savitha Ramasamy, Sreenatha Anavatti, Lin Liu, Habibullah, Ryszard Kowalczyk</div><div style='padding-top: 10px; width: 80ex'>A reliable long-term time-series forecaster is highly demanded in practice
but comes across many challenges such as low computational and memory
footprints as well as robustness against dynamic learning environments. This
paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic
long-term time-series forecasting tasks. MANTRA relies on the concept of fast
and slow learners where a collection of fast learners learns different aspects
of data distributions while adapting quickly to changes. A slow learner tailors
suitable representations to fast learners. Fast adaptations to dynamic
environments are achieved using the universal representation transformer layers
producing task-adapted representations with a small number of parameters. Our
experiments using four datasets with different prediction lengths demonstrate
the advantage of our approach with at least $3\%$ improvements over the
baseline algorithms for both multivariate and univariate settings. Source codes
of MANTRA are publicly available in
\url{https://github.com/anwarmaxsum/MANTRA}.</div><div><a href='http://arxiv.org/abs/2401.13968v1'>2401.13968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14949v1")'>Addressing Concept Shift in Online Time Series Forecasting:
  Detect-then-Adapt</div>
<div id='2403.14949v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T04:44:43Z</div><div>Authors: YiFan Zhang, Weiqi Chen, Zhaoyang Zhu, Dalin Qin, Liang Sun, Xue Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin</div><div style='padding-top: 10px; width: 80ex'>Online updating of time series forecasting models aims to tackle the
challenge of concept drifting by adjusting forecasting models based on
streaming data. While numerous algorithms have been developed, most of them
focus on model design and updating. In practice, many of these methods struggle
with continuous performance regression in the face of accumulated concept
drifts over time. To address this limitation, we present a novel approach,
Concept \textbf{D}rift \textbf{D}etection an\textbf{D} \textbf{A}daptation
(D3A), that first detects drifting conception and then aggressively adapts the
current model to the drifted concepts after the detection for rapid adaption.
To best harness the utility of historical data for model adaptation, we propose
a data augmentation strategy introducing Gaussian noise into existing training
instances. It helps mitigate the data distribution gap, a critical factor
contributing to train-test performance inconsistency. The significance of our
data augmentation process is verified by our theoretical analysis. Our
empirical studies across six datasets demonstrate the effectiveness of D3A in
improving model adaptation capability. Notably, compared to a simple Temporal
Convolutional Network (TCN) baseline, D3A reduces the average Mean Squared
Error (MSE) by $43.9\%$. For the state-of-the-art (SOTA) model, the MSE is
reduced by $33.3\%$.</div><div><a href='http://arxiv.org/abs/2403.14949v1'>2403.14949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17802v1")'>Distillation Enhanced Time Series Forecasting Network with Momentum
  Contrastive Learning</div>
<div id='2401.17802v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T12:52:10Z</div><div>Authors: Haozhi Gao, Qianqian Ren, Jinbao Li</div><div style='padding-top: 10px; width: 80ex'>Contrastive representation learning is crucial in time series analysis as it
alleviates the issue of data noise and incompleteness as well as sparsity of
supervision signal. However, existing constrastive learning frameworks usually
focus on intral-temporal features, which fails to fully exploit the intricate
nature of time series data. To address this issue, we propose DE-TSMCL, an
innovative distillation enhanced framework for long sequence time series
forecasting. Specifically, we design a learnable data augmentation mechanism
which adaptively learns whether to mask a timestamp to obtain optimized
sub-sequences. Then, we propose a contrastive learning task with momentum
update to explore inter-sample and intra-temporal correlations of time series
to learn the underlying structure feature on the unlabeled time series.
Meanwhile, we design a supervised task to learn more robust representations and
facilitate the contrastive learning process. Finally, we jointly optimize the
above two tasks. By developing model loss from multiple tasks, we can learn
effective representations for downstream forecasting task. Extensive
experiments, in comparison with state-of-the-arts, well demonstrate the
effectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%.</div><div><a href='http://arxiv.org/abs/2401.17802v1'>2401.17802v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02023v1")'>Self-Supervised Contrastive Forecasting</div>
<div id='2402.02023v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T04:32:34Z</div><div>Authors: Junwoo Park, Daehoon Gwak, Jaegul Choo, Edward Choi</div><div style='padding-top: 10px; width: 80ex'>Long-term forecasting presents unique challenges due to the time and memory
complexity of handling long sequences. Existing methods, which rely on sliding
windows to process long sequences, struggle to effectively capture long-term
variations that are partially caught within the short window (i.e.,
outer-window variations). In this paper, we introduce a novel approach that
overcomes this limitation by employing contrastive learning and enhanced
decomposition architecture, specifically designed to focus on long-term
variations. To this end, our contrastive loss incorporates global
autocorrelation held in the whole time series, which facilitates the
construction of positive and negative pairs in a self-supervised manner. When
combined with our decomposition networks, our contrastive learning
significantly improves long-term forecasting performance. Extensive experiments
demonstrate that our approach outperforms 14 baseline models in multiple
experiments over nine long-term benchmarks, especially in challenging scenarios
that require a significantly long output for forecasting. Source code is
available at
https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.</div><div><a href='http://arxiv.org/abs/2402.02023v1'>2402.02023v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09373v1")'>Loss Shaping Constraints for Long-Term Time Series Forecasting</div>
<div id='2402.09373v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:20:44Z</div><div>Authors: Ignacio Hounie, Javier Porras-Valenzuela, Alejandro Ribeiro</div><div style='padding-top: 10px; width: 80ex'>Several applications in time series forecasting require predicting multiple
steps ahead. Despite the vast amount of literature in the topic, both classical
and recent deep learning based approaches have mostly focused on minimising
performance averaged over the predicted window. We observe that this can lead
to disparate distributions of errors across forecasting steps, especially for
recent transformer architectures trained on popular forecasting benchmarks.
That is, optimising performance on average can lead to undesirably large errors
at specific time-steps. In this work, we present a Constrained Learning
approach for long-term time series forecasting that aims to find the best model
in terms of average performance that respects a user-defined upper bound on the
loss at each time-step. We call our approach loss shaping constraints because
it imposes constraints on the loss at each time step, and leverage recent
duality results to show that despite its non-convexity, the resulting problem
has a bounded duality gap. We propose a practical Primal-Dual algorithm to
tackle it, and demonstrate that the proposed approach exhibits competitive
average performance in time series forecasting benchmarks, while shaping the
distribution of errors across the predicted window.</div><div><a href='http://arxiv.org/abs/2402.09373v1'>2402.09373v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09898v1")'>TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting</div>
<div id='2403.09898v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T22:19:37Z</div><div>Authors: Md Atik Ahamed, Qiang Cheng</div><div style='padding-top: 10px; width: 80ex'>Long-term time-series forecasting remains challenging due to the difficulty
in capturing long-term dependencies, achieving linear scalability, and
maintaining computational efficiency. We introduce TimeMachine, an innovative
model that leverages Mamba, a state-space model, to capture long-term
dependencies in multivariate time series data while maintaining linear
scalability and small memory footprints. TimeMachine exploits the unique
properties of time series data to produce salient contextual cues at
multi-scales and leverage an innovative integrated quadruple-Mamba architecture
to unify the handling of channel-mixing and channel-independence situations,
thus enabling effective selection of contents for prediction against global and
local contexts at different scales. Experimentally, TimeMachine achieves
superior performance in prediction accuracy, scalability, and memory
efficiency, as extensively validated using benchmark datasets. Code
availability: https://github.com/Atik-Ahamed/TimeMachine</div><div><a href='http://arxiv.org/abs/2403.09898v1'>2403.09898v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09223v1")'>MCformer: Multivariate Time Series Forecasting with Mixed-Channels
  Transformer</div>
<div id='2403.09223v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T09:43:07Z</div><div>Authors: Wenyong Han, Tao Zhu Member, Liming Chen, Huansheng Ning, Yang Luo, Yaping Wan</div><div style='padding-top: 10px; width: 80ex'>The massive generation of time-series data by largescale Internet of Things
(IoT) devices necessitates the exploration of more effective models for
multivariate time-series forecasting. In previous models, there was a
predominant use of the Channel Dependence (CD) strategy (where each channel
represents a univariate sequence). Current state-of-the-art (SOTA) models
primarily rely on the Channel Independence (CI) strategy. The CI strategy
treats all channels as a single channel, expanding the dataset to improve
generalization performance and avoiding inter-channel correlation that disrupts
long-term features. However, the CI strategy faces the challenge of
interchannel correlation forgetting. To address this issue, we propose an
innovative Mixed Channels strategy, combining the data expansion advantages of
the CI strategy with the ability to counteract inter-channel correlation
forgetting. Based on this strategy, we introduce MCformer, a multivariate
time-series forecasting model with mixed channel features. The model blends a
specific number of channels, leveraging an attention mechanism to effectively
capture inter-channel correlation information when modeling long-term features.
Experimental results demonstrate that the Mixed Channels strategy outperforms
pure CI strategy in multivariate time-series forecasting tasks.</div><div><a href='http://arxiv.org/abs/2403.09223v1'>2403.09223v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17548v2")'>Rethinking Channel Dependence for Multivariate Time Series Forecasting:
  Learning from Leading Indicators</div>
<div id='2401.17548v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T02:26:09Z</div><div>Authors: Lifan Zhao, Yanyan Shen</div><div style='padding-top: 10px; width: 80ex'>Recently, channel-independent methods have achieved state-of-the-art
performance in multivariate time series (MTS) forecasting. Despite reducing
overfitting risks, these methods miss potential opportunities in utilizing
channel dependence for accurate predictions. We argue that there exist locally
stationary lead-lag relationships between variates, i.e., some lagged variates
may follow the leading indicators within a short time period. Exploiting such
channel dependence is beneficial since leading indicators offer advance
information that can be used to reduce the forecasting difficulty of the lagged
variates. In this paper, we propose a new method named LIFT that first
efficiently estimates leading indicators and their leading steps at each time
step and then judiciously allows the lagged variates to utilize the advance
information from leading indicators. LIFT plays as a plugin that can be
seamlessly collaborated with arbitrary time series forecasting methods.
Extensive experiments on six real-world datasets demonstrate that LIFT improves
the state-of-the-art methods by 5.5% in average forecasting performance.</div><div><a href='http://arxiv.org/abs/2401.17548v2'>2401.17548v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00869v1")'>Enhancing Multivariate Time Series Forecasting with Mutual
  Information-driven Cross-Variable and Temporal Modeling</div>
<div id='2403.00869v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T04:42:47Z</div><div>Authors: Shiyi Qi, Liangjian Wen, Yiduo Li, Yuanhang Yang, Zhe Li, Zhongwen Rao, Lujia Pan, Zenglin Xu</div><div style='padding-top: 10px; width: 80ex'>Recent advancements have underscored the impact of deep learning techniques
on multivariate time series forecasting (MTSF). Generally, these techniques are
bifurcated into two categories: Channel-independence and Channel-mixing
approaches. Although Channel-independence methods typically yield better
results, Channel-mixing could theoretically offer improvements by leveraging
inter-variable correlations. Nonetheless, we argue that the integration of
uncorrelated information in channel-mixing methods could curtail the potential
enhancement in MTSF model performance. To substantiate this claim, we introduce
the Cross-variable Decorrelation Aware feature Modeling (CDAM) for
Channel-mixing approaches, aiming to refine Channel-mixing by minimizing
redundant information between channels while enhancing relevant mutual
information. Furthermore, we introduce the Temporal correlation Aware Modeling
(TAM) to exploit temporal correlations, a step beyond conventional single-step
forecasting methods. This strategy maximizes the mutual information between
adjacent sub-sequences of both the forecasted and target series. Combining CDAM
and TAM, our novel framework significantly surpasses existing models, including
those previously considered state-of-the-art, in comprehensive tests.</div><div><a href='http://arxiv.org/abs/2403.00869v1'>2403.00869v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15134v1")'>Deep Coupling Network For Multivariate Time Series Forecasting</div>
<div id='2402.15134v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:38:08Z</div><div>Authors: Kun Yi, Qi Zhang, Hui He, Kaize Shi, Liang Hu, Ning An, Zhendong Niu</div><div style='padding-top: 10px; width: 80ex'>Multivariate time series (MTS) forecasting is crucial in many real-world
applications. To achieve accurate MTS forecasting, it is essential to
simultaneously consider both intra- and inter-series relationships among time
series data. However, previous work has typically modeled intra- and
inter-series relationships separately and has disregarded multi-order
interactions present within and between time series data, which can seriously
degrade forecasting accuracy. In this paper, we reexamine intra- and
inter-series relationships from the perspective of mutual information and
accordingly construct a comprehensive relationship learning mechanism tailored
to simultaneously capture the intricate multi-order intra- and inter-series
couplings. Based on the mechanism, we propose a novel deep coupling network for
MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated
to explicitly exploring the multi-order intra- and inter-series relationships
among time series data concurrently, a coupled variable representation module
aimed at encoding diverse variable patterns, and an inference module
facilitating predictions through one forward step. Extensive experiments
conducted on seven real-world datasets demonstrate that our proposed DeepCN
achieves superior performance compared with the state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2402.15134v1'>2402.15134v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02236v1")'>U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for
  Time Series Forecasting</div>
<div id='2401.02236v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T12:41:40Z</div><div>Authors: Xiang Ma, Xuemei Li, Lexin Fang, Tianlong Zhao, Caiming Zhang</div><div style='padding-top: 10px; width: 80ex'>Time series forecasting is a crucial task in various domains. Caused by
factors such as trends, seasonality, or irregular fluctuations, time series
often exhibits non-stationary. It obstructs stable feature propagation through
deep layers, disrupts feature distributions, and complicates learning data
distribution changes. As a result, many existing models struggle to capture the
underlying patterns, leading to degraded forecasting performance. In this
study, we tackle the challenge of non-stationarity in time series forecasting
with our proposed framework called U-Mixer. By combining Unet and Mixer,
U-Mixer effectively captures local temporal dependencies between different
patches and channels separately to avoid the influence of distribution
variations among channels, and merge low- and high-levels features to obtain
comprehensive data representations. The key contribution is a novel
stationarity correction method, explicitly restoring data distribution by
constraining the difference in stationarity between the data before and after
model processing to restore the non-stationarity information, while ensuring
the temporal dependencies are preserved. Through extensive experiments on
various real-world time series datasets, U-Mixer demonstrates its effectiveness
and robustness, and achieves 14.5\% and 7.7\% improvements over
state-of-the-art (SOTA) methods.</div><div><a href='http://arxiv.org/abs/2401.02236v1'>2401.02236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02332v1")'>Minusformer: Improving Time Series Forecasting by Progressively Learning
  Residuals</div>
<div id='2402.02332v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T03:54:31Z</div><div>Authors: Daojun Liang, Haixia Zhang, Dongfeng Yuan, Bingzheng Zhang, Minggao Zhang</div><div style='padding-top: 10px; width: 80ex'>In this paper, we find that ubiquitous time series (TS) forecasting models
are prone to severe overfitting. To cope with this problem, we embrace a
de-redundancy approach to progressively reinstate the intrinsic values of TS
for future intervals. Specifically, we renovate the vanilla Transformer by
reorienting the information aggregation mechanism from addition to subtraction.
Then, we incorporate an auxiliary output branch into each block of the original
model to construct a highway leading to the ultimate prediction. The output of
subsequent modules in this branch will subtract the previously learned results,
enabling the model to learn the residuals of the supervision signal, layer by
layer. This designing facilitates the learning-driven implicit progressive
decomposition of the input and output streams, empowering the model with
heightened versatility, interpretability, and resilience against overfitting.
Since all aggregations in the model are minus signs, which is called
Minusformer. Extensive experiments demonstrate the proposed method outperform
existing state-of-the-art methods, yielding an average performance improvement
of 11.9% across various datasets.</div><div><a href='http://arxiv.org/abs/2402.02332v1'>2402.02332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12694v3")'>Revitalizing Multivariate Time Series Forecasting: Learnable
  Decomposition with Inter-Series Dependencies and Intra-Series Variations
  Modeling</div>
<div id='2402.12694v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T03:45:59Z</div><div>Authors: Guoqi Yu, Jing Zou, Xiaowei Hu, Angelica I. Aviles-Rivero, Jing Qin, Shujun Wang</div><div style='padding-top: 10px; width: 80ex'>Predicting multivariate time series is crucial, demanding precise modeling of
intricate patterns, including inter-series dependencies and intra-series
variations. Distinctive trend characteristics in each time series pose
challenges, and existing methods, relying on basic moving average kernels, may
struggle with the non-linear structure and complex trends in real-world data.
Given that, we introduce a learnable decomposition strategy to capture dynamic
trend information more reasonably. Additionally, we propose a dual attention
module tailored to capture inter-series dependencies and intra-series
variations simultaneously for better time series forecasting, which is
implemented by channel-wise self-attention and autoregressive self-attention.
To evaluate the effectiveness of our method, we conducted experiments across
eight open-source datasets and compared it with the state-of-the-art methods.
Through the comparison results, our Leddam (LEarnable Decomposition and Dual
Attention Module) not only demonstrates significant advancements in predictive
performance, but also the proposed decomposition strategy can be plugged into
other methods with a large performance-boosting, from 11.87% to 48.56% MSE
error degradation.</div><div><a href='http://arxiv.org/abs/2402.12694v3'>2402.12694v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04882v1")'>Efficient High-Resolution Time Series Classification via Attention
  Kronecker Decomposition</div>
<div id='2403.04882v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T20:14:20Z</div><div>Authors: Aosong Feng, Jialin Chen, Juan Garza, Brooklyn Berry, Francisco Salazar, Yifeng Gao, Rex Ying, Leandros Tassiulas</div><div style='padding-top: 10px; width: 80ex'>The high-resolution time series classification problem is essential due to
the increasing availability of detailed temporal data in various domains. To
tackle this challenge effectively, it is imperative that the state-of-the-art
attention model is scalable to accommodate the growing sequence lengths
typically encountered in high-resolution time series data, while also
demonstrating robustness in handling the inherent noise prevalent in such
datasets. To address this, we propose to hierarchically encode the long time
series into multiple levels based on the interaction ranges. By capturing
relationships at different levels, we can build more robust, expressive, and
efficient models that are capable of capturing both short-term fluctuations and
long-term trends in the data. We then propose a new time series transformer
backbone (KronTime) by introducing Kronecker-decomposed attention to process
such multi-level time series, which sequentially calculates attention from the
lower level to the upper level. Experiments on four long time series datasets
demonstrate superior classification results with improved efficiency compared
to baseline methods.</div><div><a href='http://arxiv.org/abs/2403.04882v1'>2403.04882v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19072v1")'>TimeXer: Empowering Transformers for Time Series Forecasting with
  Exogenous Variables</div>
<div id='2402.19072v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T11:54:35Z</div><div>Authors: Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, Haoran Zhang, Jianmin Wang, Mingsheng Long</div><div style='padding-top: 10px; width: 80ex'>Recent studies have demonstrated remarkable performance in time series
forecasting. However, due to the partially-observed nature of real-world
applications, solely focusing on the target of interest, so-called endogenous
variables, is usually insufficient to guarantee accurate forecasting. Notably,
a system is often recorded into multiple variables, where the exogenous series
can provide valuable external information for endogenous variables. Thus,
unlike prior well-established multivariate or univariate forecasting that
either treats all the variables equally or overlooks exogenous information,
this paper focuses on a practical setting, which is time series forecasting
with exogenous variables. We propose a novel framework, TimeXer, to utilize
external information to enhance the forecasting of endogenous variables. With a
deftly designed embedding layer, TimeXer empowers the canonical Transformer
architecture with the ability to reconcile endogenous and exogenous
information, where patch-wise self-attention and variate-wise cross-attention
are employed. Moreover, a global endogenous variate token is adopted to
effectively bridge the exogenous series into endogenous temporal patches.
Experimentally, TimeXer significantly improves time series forecasting with
exogenous variables and achieves consistent state-of-the-art performance in
twelve real-world forecasting benchmarks.</div><div><a href='http://arxiv.org/abs/2402.19072v1'>2402.19072v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11463v1")'>Attractor Memory for Long-Term Time Series Forecasting: A Chaos
  Perspective</div>
<div id='2402.11463v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T05:35:01Z</div><div>Authors: Jiaxi Hu, Yuehong Hu, Wei Chen, Ming Jin, Shirui Pan, Qingsong Wen, Yuxuan Liang</div><div style='padding-top: 10px; width: 80ex'>In long-term time series forecasting (LTSF) tasks, existing deep learning
models overlook the crucial characteristic that discrete time series originate
from underlying continuous dynamic systems, resulting in a lack of
extrapolation and evolution capabilities. Recognizing the chaotic nature of
real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos
theory into LTSF, perceiving real-world time series as observations from
unknown high-dimensional chaotic dynamic systems. Under the concept of
attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory
unit to memorize historical dynamics structure and predicts by a
frequency-enhanced local evolution strategy. Detailed theoretical analysis and
abundant empirical evidence consistently show that Attraos outperforms various
LTSF methods on mainstream LTSF datasets and chaotic datasets.</div><div><a href='http://arxiv.org/abs/2402.11463v1'>2402.11463v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16777v1")'>Addressing Distribution Shift in Time Series Forecasting with Instance
  Normalization Flows</div>
<div id='2401.16777v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:35:52Z</div><div>Authors: Wei Fan, Shun Zheng, Pengyang Wang, Rui Xie, Jiang Bian, Yanjie Fu</div><div style='padding-top: 10px; width: 80ex'>Due to non-stationarity of time series, the distribution shift problem
largely hinders the performance of time series forecasting. Existing solutions
either fail for the shifts beyond simple statistics or the limited
compatibility with forecasting models. In this paper, we propose a general
decoupled formulation for time series forecasting, with no reliance on fixed
statistics and no restriction on forecasting architectures. Then, we make such
a formulation formalized into a bi-level optimization problem, to enable the
joint learning of the transformation (outer loop) and forecasting (inner loop).
Moreover, the special requirements of expressiveness and bi-direction for the
transformation motivate us to propose instance normalization flows (IN-Flow), a
novel invertible network for time series transformation. Extensive experiments
demonstrate our method consistently outperforms state-of-the-art baselines on
both synthetic and real-world data.</div><div><a href='http://arxiv.org/abs/2401.16777v1'>2401.16777v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03001v1")'>UnetTSF: A Better Performance Linear Complexity Time Series Prediction
  Model</div>
<div id='2401.03001v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T03:12:20Z</div><div>Authors: Li chu, Xiao bingjia, Yuan qiping</div><div style='padding-top: 10px; width: 80ex'>Recently, Transformer-base models have made significant progress in the field
of time series prediction which have achieved good results and become baseline
models beyond Dlinear. The paper proposes an U-Net time series prediction model
(UnetTSF) with linear complexity, which adopts the U-Net architecture. We are
the first to use FPN technology to extract features from time series data,
replacing the method of decomposing time series data into trend and seasonal
terms, while designing a fusion structure suitable for time series data. After
testing on 8 open-source datasets, compared to the best linear model DLiner.
Out of 32 testing projects, 31 achieved the best results. The average decrease
in mse is 10.1%, while the average decrease in mae is 9.1%. Compared with the
complex transformer-base PatchTST, UnetTSF obtained 9 optimal results for mse
and 15 optimal results for mae in 32 testing projects.</div><div><a href='http://arxiv.org/abs/2401.03001v1'>2401.03001v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11664v1")'>Interpretable Short-Term Load Forecasting via Multi-Scale Temporal
  Decomposition</div>
<div id='2402.11664v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T17:55:59Z</div><div>Authors: Yuqi Jiang, Yan Li, Yize Chen</div><div style='padding-top: 10px; width: 80ex'>Rapid progress in machine learning and deep learning has enabled a wide range
of applications in the electricity load forecasting of power systems, for
instance, univariate and multivariate short-term load forecasting. Though the
strong capabilities of learning the non-linearity of the load patterns and the
high prediction accuracy have been achieved, the interpretability of typical
deep learning models for electricity load forecasting is less studied. This
paper proposes an interpretable deep learning method, which learns a linear
combination of neural networks that each attends to an input time feature. We
also proposed a multi-scale time series decomposition method to deal with the
complex time patterns. Case studies have been carried out on the Belgium
central grid load dataset and the proposed model demonstrated better accuracy
compared to the frequently applied baseline model. Specifically, the proposed
multi-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52,
0.57 and 0.72 respectively. As for interpretability, on one hand, the proposed
method displays generalization capability. On the other hand, it can
demonstrate not only the feature but also the temporal interpretability
compared to other baseline methods. Besides, the global time feature
interpretabilities are also obtained. Obtaining global feature
interpretabilities allows us to catch the overall patterns, trends, and
cyclicality in load data while also revealing the significance of various
time-related features in forming the final outputs.</div><div><a href='http://arxiv.org/abs/2402.11664v1'>2402.11664v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15119v1")'>Interpreting Time Series Transformer Models and Sensitivity Analysis of
  Population Age Groups to COVID-19 Infections</div>
<div id='2401.15119v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T02:58:59Z</div><div>Authors: Md Khairul Islam, Tyler Valentine, Timothy Joowon Sue, Ayush Karmacharya, Luke Neil Benham, Zhengguang Wang, Kingsley Kim, Judy Fox</div><div style='padding-top: 10px; width: 80ex'>Interpreting deep learning time series models is crucial in understanding the
model's behavior and learning patterns from raw data for real-time
decision-making. However, the complexity inherent in transformer-based time
series models poses challenges in explaining the impact of individual features
on predictions. In this study, we leverage recent local interpretation methods
to interpret state-of-the-art time series models. To use real-world datasets,
we collected three years of daily case data for 3,142 US counties. Firstly, we
compare six transformer-based models and choose the best prediction model for
COVID-19 infection. Using 13 input features from the last two weeks, we can
predict the cases for the next two weeks. Secondly, we present an innovative
way to evaluate the prediction sensitivity to 8 population age groups over
highly dynamic multivariate infection data. Thirdly, we compare our proposed
perturbation-based interpretation method with related work, including a total
of eight local interpretation methods. Finally, we apply our framework to
traffic and electricity datasets, demonstrating that our approach is generic
and can be applied to other time-series domains.</div><div><a href='http://arxiv.org/abs/2401.15119v1'>2401.15119v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.18047v1")'>Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm
  Optimization, and Deep Learning</div>
<div id='2401.18047v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T18:08:06Z</div><div>Authors: Naresh Kumar, Seba Susan</div><div style='padding-top: 10px; width: 80ex'>Epidemiological models are best suitable to model an epidemic if the spread
pattern is stationary. To deal with non-stationary patterns and multiple waves
of an epidemic, we develop a hybrid model encompassing epidemic modeling,
particle swarm optimization, and deep learning. The model mainly caters to
three objectives for better prediction: 1. Periodic estimation of the model
parameters. 2. Incorporating impact of all the aspects using data fitting and
parameter optimization 3. Deep learning based prediction of the model
parameters. In our model, we use a system of ordinary differential equations
(ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling,
Particle Swarm Optimization (PSO) for model parameter optimization, and
stacked-LSTM for forecasting the model parameters. Initial or one time
estimation of model parameters is not able to model multiple waves of an
epidemic. So, we estimate the model parameters periodically (weekly). We use
PSO to identify the optimum values of the model parameters. We next train the
stacked-LSTM on the optimized parameters, and perform forecasting of the model
parameters for upcoming four weeks. Further, we fed the LSTM forecasted
parameters into the SIRD model to forecast the number of COVID-19 cases. We
evaluate the model for highly affected three countries namely; the USA, India,
and the UK. The proposed hybrid model is able to deal with multiple waves, and
has outperformed existing methods on all the three datasets.</div><div><a href='http://arxiv.org/abs/2401.18047v1'>2401.18047v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05933v1")'>Time Series Forecasting of HIV/AIDS in the Philippines Using Deep
  Learning: Does COVID-19 Epidemic Matter?</div>
<div id='2401.05933v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T14:11:30Z</div><div>Authors: Sales G. Aribe Jr., Bobby D. Gerardo, Ruji P. Medina</div><div style='padding-top: 10px; width: 80ex'>With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS
epidemic in the Philippines is the one that is spreading the quickest in the
western Pacific. Although the full effects of COVID-19 on HIV services and
development are still unknown, it is predicted that such disruptions could lead
to a significant increase in HIV casualties. Therefore, the nation needs some
modeling and forecasting techniques to foresee the spread pattern and enhance
the governments prevention, treatment, testing, and care program. In this
study, the researcher uses Multilayer Perceptron Neural Network to forecast
time series during the period when the COVID-19 pandemic strikes the nation,
using statistics taken from the HIV/AIDS and ART Registry of the Philippines.
After training, validation, and testing of data, the study finds that the
predicted cumulative cases in the nation by 2030 will reach 145,273.
Additionally, there is very little difference between observed and anticipated
HIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well
as a greater coefficient of determination. Further research revealed that the
Philippines seems far from achieving Sustainable Development Goal 3 of Project
2030 due to an increase in the nations rate of new HIV infections. Despite the
detrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the
Philippine government, under the Marcos administration, must continue to adhere
to the United Nations 90-90-90 targets by enhancing its ART program and
ensuring that all vital health services are readily accessible and available.</div><div><a href='http://arxiv.org/abs/2401.05933v1'>2401.05933v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06033v1")'>Predicting Depression and Anxiety: A Multi-Layer Perceptron for
  Analyzing the Mental Health Impact of COVID-19</div>
<div id='2403.06033v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T22:49:04Z</div><div>Authors: David Fong, Tianshu Chu, Matthew Heflin, Xiaosi Gu, Oshani Seneviratne</div><div style='padding-top: 10px; width: 80ex'>We introduce a multi-layer perceptron (MLP) called the COVID-19 Depression
and Anxiety Predictor (CoDAP) to predict mental health trends, particularly
anxiety and depression, during the COVID-19 pandemic. Our method utilizes a
comprehensive dataset, which tracked mental health symptoms weekly over ten
weeks during the initial COVID-19 wave (April to June 2020) in a diverse cohort
of U.S. adults. This period, characterized by a surge in mental health symptoms
and conditions, offers a critical context for our analysis. Our focus was to
extract and analyze patterns of anxiety and depression through a unique lens of
qualitative individual attributes using CoDAP. This model not only predicts
patterns of anxiety and depression during the pandemic but also unveils key
insights into the interplay of demographic factors, behavioral changes, and
social determinants of mental health. These findings contribute to a more
nuanced understanding of the complexity of mental health issues in times of
global health crises, potentially guiding future early interventions.</div><div><a href='http://arxiv.org/abs/2403.06033v1'>2403.06033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03171v1")'>Exploration of Adolescent Depression Risk Prediction Based on Census
  Surveys and General Life Issues</div>
<div id='2401.03171v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T09:14:25Z</div><div>Authors: Qiang Li, Yufeng Wu, Zhan Xu, Hefeng Zhou</div><div style='padding-top: 10px; width: 80ex'>In contemporary society, the escalating pressures of life and work have
propelled psychological disorders to the forefront of modern health concerns,
an issue that has been further accentuated by the COVID-19 pandemic. The
prevalence of depression among adolescents is steadily increasing, and
traditional diagnostic methods, which rely on scales or interviews, prove
particularly inadequate for detecting depression in young people. Addressing
these challenges, numerous AI-based methods for assisting in the diagnosis of
mental health issues have emerged. However, most of these methods center around
fundamental issues with scales or use multimodal approaches like facial
expression recognition. Diagnosis of depression risk based on everyday habits
and behaviors has been limited to small-scale qualitative studies. Our research
leverages adolescent census data to predict depression risk, focusing on
children's experiences with depression and their daily life situations. We
introduced a method for managing severely imbalanced high-dimensional data and
an adaptive predictive approach tailored to data structure characteristics.
Furthermore, we proposed a cloud-based architecture for automatic online
learning and data updates. This study utilized publicly available NSCH youth
census data from 2020 to 2022, encompassing nearly 150,000 data entries. We
conducted basic data analyses and predictive experiments, demonstrating
significant performance improvements over standard machine learning and deep
learning algorithms. This affirmed our data processing method's broad
applicability in handling imbalanced medical data. Diverging from typical
predictive method research, our study presents a comprehensive architectural
solution, considering a wider array of user needs.</div><div><a href='http://arxiv.org/abs/2401.03171v1'>2401.03171v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03434v2")'>An AI-enabled Agent-Based Model and Its Application in Measles Outbreak
  Simulation for New Zealand</div>
<div id='2403.03434v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T03:36:07Z</div><div>Authors: Sijin Zhang, Alvaro Orsi, Lei Chen</div><div style='padding-top: 10px; width: 80ex'>Agent Based Models (ABMs) have emerged as a powerful tool for investigating
complex social interactions, particularly in the context of public health and
infectious disease investigation. In an effort to enhance the conventional ABM,
enabling automated model calibration and reducing the computational resources
needed for scaling up the model, we have developed a tensorized and
differentiable agent-based model by coupling Graph Neural Network (GNN) and
Long Short-Term Memory (LSTM) network. The model was employed to investigate
the 2019 measles outbreak occurred in New Zealand, demonstrating a promising
ability to accurately simulate the outbreak dynamics, particularly during the
peak period of repeated cases. This paper shows that by leveraging the latest
Artificial Intelligence (AI) technology and the capabilities of traditional
ABMs, we gain deeper insights into the dynamics of infectious disease
outbreaks. This, in turn, helps us make more informed decision when developing
effective strategies that strike a balance between managing outbreaks and
minimizing disruptions to everyday life.</div><div><a href='http://arxiv.org/abs/2403.03434v2'>2403.03434v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03390v1")'>Global Prediction of COVID-19 Variant Emergence Using Dynamics-Informed
  Graph Neural Networks</div>
<div id='2401.03390v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T05:03:30Z</div><div>Authors: Majd Al Aawar, Srikar Mutnuri, Mansooreh Montazerin, Ajitesh Srivastava</div><div style='padding-top: 10px; width: 80ex'>During the COVID-19 pandemic, a major driver of new surges has been the
emergence of new variants. When a new variant emerges in one or more countries,
other nations monitor its spread in preparation for its potential arrival. The
impact of the variant and the timing of epidemic peaks in a country highly
depend on when the variant arrives. The current methods for predicting the
spread of new variants rely on statistical modeling, however, these methods
work only when the new variant has already arrived in the region of interest
and has a significant prevalence. The question arises: Can we predict when (and
if) a variant that exists elsewhere will arrive in a given country and reach a
certain prevalence? We propose a variant-dynamics-informed Graph Neural Network
(GNN) approach. First, We derive the dynamics of variant prevalence across
pairs of regions (countries) that applies to a large class of epidemic models.
The dynamics suggest that ratios of variant proportions lead to simpler
patterns. Therefore, we use ratios of variant proportions along with some
parameters estimated from the dynamics as features in a GNN. We develop a
benchmarking tool to evaluate variant emergence prediction over 87 countries
and 36 variants. We leverage this tool to compare our GNN-based approach
against our dynamics-only model and a number of machine learning models.
Results show that the proposed dynamics-informed GNN method retrospectively
outperforms all the baselines, including the currently pervasive framework of
Physics-Informed Neural Networks (PINNs) that incorporates the dynamics in the
loss function.</div><div><a href='http://arxiv.org/abs/2401.03390v1'>2401.03390v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16521v1")'>Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity
  Analysis Methods for Time-Series Deep Learning Models</div>
<div id='2401.16521v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T19:51:50Z</div><div>Authors: Zhengguang Wang</div><div style='padding-top: 10px; width: 80ex'>This work undertakes studies to evaluate Interpretability Methods for
Time-Series Deep Learning. Sensitivity analysis assesses how input changes
affect the output, constituting a key component of interpretation. Among the
post-hoc interpretation methods such as back-propagation, perturbation, and
approximation, my work will investigate perturbation-based sensitivity Analysis
methods on modern Transformer models to benchmark their performances.
Specifically, my work answers three research questions: 1) Do different
sensitivity analysis (SA) methods yield comparable outputs and attribute
importance rankings? 2) Using the same sensitivity analysis method, do
different Deep Learning (DL) models impact the output of the sensitivity
analysis? 3) How well do the results from sensitivity analysis methods align
with the ground truth?</div><div><a href='http://arxiv.org/abs/2401.16521v1'>2401.16521v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04632v2")'>Hypercomplex neural network in time series forecasting of stock data</div>
<div id='2401.04632v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T15:59:43Z</div><div>Authors: Radosław Kycia, Agnieszka Niemczynowicz</div><div style='padding-top: 10px; width: 80ex'>The goal of this paper is to test three classes of neural network (NN)
architectures based on four-dimensional (4D) hypercomplex algebras for time
series prediction. We evaluate different architectures, varying the input
layers to include convolutional, Long Short-Term Memory (LSTM), or dense
hypercomplex layers for 4D algebras. Four related Stock Market time series are
used as input data, with the prediction focused on one of them. Hyperparameter
optimization for each architecture class was conducted to compare the
best-performing neural networks within each class. The results indicate that,
in most cases, architectures with hypercomplex dense layers achieve similar
Mean Absolute Error (MAE) accuracy compared to other architectures, but with
significantly fewer trainable parameters. Consequently, hypercomplex neural
networks demonstrate the ability to learn and process time series data faster
than the other tested architectures. Additionally, it was found that the
ordering of the input time series have a notable impact on effectiveness.</div><div><a href='http://arxiv.org/abs/2401.04632v2'>2401.04632v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11722v1")'>Time Series Compression using Quaternion Valued Neural Networks and
  Quaternion Backpropagation</div>
<div id='2403.11722v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T12:22:11Z</div><div>Authors: Johannes Pöppelbaum, Andreas Schwung</div><div style='padding-top: 10px; width: 80ex'>We propose a novel quaternionic time-series compression methodology where we
divide a long time-series into segments of data, extract the min, max, mean and
standard deviation of these chunks as representative features and encapsulate
them in a quaternion, yielding a quaternion valued time-series. This
time-series is processed using quaternion valued neural network layers, where
we aim to preserve the relation between these features through the usage of the
Hamilton product. To train this quaternion neural network, we derive quaternion
backpropagation employing the GHR calculus, which is required for a valid
product and chain rule in quaternion space. Furthermore, we investigate the
connection between the derived update rules and automatic differentiation. We
apply our proposed compression method on the Tennessee Eastman Dataset, where
we perform fault classification using the compressed data in two settings: a
fully supervised one and in a semi supervised, contrastive learning setting.
Both times, we were able to outperform real valued counterparts as well as two
baseline models: one with the uncompressed time-series as the input and the
other with a regular downsampling using the mean. Further, we could improve the
classification benchmark set by SimCLR-TS from 81.43% to 83.90%.</div><div><a href='http://arxiv.org/abs/2403.11722v1'>2403.11722v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02032v1")'>RobustTSF: Towards Theory and Design of Robust Time Series Forecasting
  with Anomalies</div>
<div id='2402.02032v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:13:09Z</div><div>Authors: Hao Cheng, Qingsong Wen, Yang Liu, Liang Sun</div><div style='padding-top: 10px; width: 80ex'>Time series forecasting is an important and forefront task in many real-world
applications. However, most of time series forecasting techniques assume that
the training data is clean without anomalies. This assumption is unrealistic
since the collected time series data can be contaminated in practice. The
forecasting model will be inferior if it is directly trained by time series
with anomalies. Thus it is essential to develop methods to automatically learn
a robust forecasting model from the contaminated data. In this paper, we first
statistically define three types of anomalies, then theoretically and
experimentally analyze the loss robustness and sample robustness when these
anomalies exist. Based on our analyses, we propose a simple and efficient
algorithm to learn a robust forecasting model. Extensive experiments show that
our method is highly robust and outperforms all existing approaches. The code
is available at https://github.com/haochenglouis/RobustTSF.</div><div><a href='http://arxiv.org/abs/2402.02032v1'>2402.02032v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03508v1")'>Probing the Robustness of Time-series Forecasting Models with
  CounterfacTS</div>
<div id='2403.03508v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T07:34:47Z</div><div>Authors: Håkon Hanisch Kjærnli, Lluis Mas-Ribas, Aida Ashrafi, Gleb Sizov, Helge Langseth, Odd Erik Gundersen</div><div style='padding-top: 10px; width: 80ex'>A common issue for machine learning models applied to time-series forecasting
is the temporal evolution of the data distributions (i.e., concept drift).
Because most of the training data does not reflect such changes, the models
present poor performance on the new out-of-distribution scenarios and,
therefore, the impact of such events cannot be reliably anticipated ahead of
time. We present and publicly release CounterfacTS, a tool to probe the
robustness of deep learning models in time-series forecasting tasks via
counterfactuals. CounterfacTS has a user-friendly interface that allows the
user to visualize, compare and quantify time series data and their forecasts,
for a number of datasets and deep learning models. Furthermore, the user can
apply various transformations to the time series and explore the resulting
changes in the forecasts in an interpretable manner. Through example cases, we
illustrate how CounterfacTS can be used to i) identify the main features
characterizing and differentiating sets of time series, ii) assess how the
model performance depends on these characateristics, and iii) guide
transformations of the original time series to create counterfactuals with
desired properties for training and increasing the forecasting performance in
new regions of the data distribution. We discuss the importance of visualizing
and considering the location of the data in a projected feature space to
transform time-series and create effective counterfactuals for training the
models. Overall, CounterfacTS aids at creating counterfactuals to efficiently
explore the impact of hypothetical scenarios not covered by the original data
in time-series forecasting tasks.</div><div><a href='http://arxiv.org/abs/2403.03508v1'>2403.03508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06524v1")'>Domain Adaptation for Time series Transformers using One-step
  fine-tuning</div>
<div id='2401.06524v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T11:43:16Z</div><div>Authors: Subina Khanal, Seshu Tirupathi, Giulio Zizzo, Ambrish Rawat, Torben Bach Pedersen</div><div style='padding-top: 10px; width: 80ex'>The recent breakthrough of Transformers in deep learning has drawn
significant attention of the time series community due to their ability to
capture long-range dependencies. However, like other deep learning models,
Transformers face limitations in time series prediction, including insufficient
temporal understanding, generalization challenges, and data shift issues for
the domains with limited data. Additionally, addressing the issue of
catastrophic forgetting, where models forget previously learned information
when exposed to new data, is another critical aspect that requires attention in
enhancing the robustness of Transformers for time series tasks. To address
these limitations, in this paper, we pre-train the time series Transformer
model on a source domain with sufficient data and fine-tune it on the target
domain with limited data. We introduce the \emph{One-step fine-tuning}
approach, adding some percentage of source domain data to the target domains,
providing the model with diverse time series instances. We then fine-tune the
pre-trained model using a gradual unfreezing technique. This helps enhance the
model's performance in time series prediction for domains with limited data.
Extensive experimental results on two real-world datasets show that our
approach improves over the state-of-the-art baselines by 4.35% and 11.54% for
indoor temperature and wind power prediction, respectively.</div><div><a href='http://arxiv.org/abs/2401.06524v1'>2401.06524v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09573v1")'>Changes by Butterflies: Farsighted Forecasting with Group Reservoir
  Transformer</div>
<div id='2402.09573v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T20:48:58Z</div><div>Authors: Md Kowsher, Jia Xu</div><div style='padding-top: 10px; width: 80ex'>In Chaos, a minor divergence between two initial conditions exhibits
exponential amplification over time, leading to far-away outcomes, known as the
butterfly effect. Thus, the distant future is full of uncertainty and hard to
forecast. We introduce Group Reservoir Transformer to predict long-term events
more accurately and robustly by overcoming two challenges in Chaos: (1) the
extensive historical sequences and (2) the sensitivity to initial conditions. A
reservoir is attached to a Transformer to efficiently handle arbitrarily long
historical lengths, with an extension of a group of reservoirs to reduce the
uncertainty due to the initialization variations. Our architecture consistently
outperforms state-of-the-art DNN models in multivariate time series, including
NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an
error reduction of up to -89.43\% in various fields such as ETTh, ETTm, and air
quality, demonstrating that an ensemble of butterfly learning, the prediction
can be improved to a more adequate and certain one, despite of the traveling
time to the unknown future.</div><div><a href='http://arxiv.org/abs/2402.09573v1'>2402.09573v1</a></div>
</div></div>
    <div><a href="arxiv_25.html">Prev (25)</a></div>
    <div><a href="arxiv_27.html">Next (27)</a></div>
    