
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15109v1")'>Machine Unlearning by Suppressing Sample Contribution</div>
<div id='2402.15109v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T05:44:15Z</div><div>Authors: Xinwen Cheng, Zhehao Huang, Xiaolin Huang</div><div style='padding-top: 10px; width: 80ex'>Machine Unlearning (MU) is to forget data from a well-trained model, which is
practically important due to the "right to be forgotten". In this paper, we
start from the fundamental distinction between training data and unseen data on
their contribution to the model: the training data contributes to the final
model while the unseen data does not. We theoretically discover that the input
sensitivity can approximately measure the contribution and practically design
an algorithm, called MU-Mis (machine unlearning via minimizing input
sensitivity), to suppress the contribution of the forgetting data. Experimental
results demonstrate that MU-Mis outperforms state-of-the-art MU methods
significantly. Additionally, MU-Mis aligns more closely with the application of
MU as it does not require the use of remaining data.</div><div><a href='http://arxiv.org/abs/2402.15109v1'>2402.15109v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19308v1")'>Loss-Free Machine Unlearning</div>
<div id='2402.19308v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T16:15:34Z</div><div>Authors: Jack Foster, Stefan Schoepf, Alexandra Brintrup</div><div style='padding-top: 10px; width: 80ex'>We present a machine unlearning approach that is both retraining- and
label-free. Most existing machine unlearning approaches require a model to be
fine-tuned to remove information while preserving performance. This is
computationally expensive and necessitates the storage of the whole dataset for
the lifetime of the model. Retraining-free approaches often utilise Fisher
information, which is derived from the loss and requires labelled data which
may not be available. Thus, we present an extension to the Selective Synaptic
Dampening algorithm, substituting the diagonal of the Fisher information matrix
for the gradient of the l2 norm of the model output to approximate sensitivity.
We evaluate our method in a range of experiments using ResNet18 and Vision
Transformer. Results show our label-free method is competitive with existing
state-of-the-art approaches.</div><div><a href='http://arxiv.org/abs/2402.19308v1'>2402.19308v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16386v1")'>Continual Learning with Pre-Trained Models: A Survey</div>
<div id='2401.16386v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:27:52Z</div><div>Authors: Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan</div><div style='padding-top: 10px; width: 80ex'>Nowadays, real-world applications often face streaming data, which requires
the learning system to absorb new knowledge as data evolves. Continual Learning
(CL) aims to achieve this goal and meanwhile overcome the catastrophic
forgetting of former knowledge when learning new ones. Typical CL methods build
the model from scratch to grow with incoming data. However, the advent of the
pre-trained model (PTM) era has sparked immense research interest, particularly
in leveraging PTMs' robust representational capabilities. This paper presents a
comprehensive survey of the latest advancements in PTM-based CL. We categorize
existing methodologies into three distinct groups, providing a comparative
analysis of their similarities, differences, and respective advantages and
disadvantages. Additionally, we offer an empirical study contrasting various
state-of-the-art methods to highlight concerns regarding fairness in
comparisons. The source code to reproduce these evaluations is available at:
https://github.com/sun-hailong/LAMDA-PILOT</div><div><a href='http://arxiv.org/abs/2401.16386v1'>2401.16386v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01054v1")'>Elastic Multi-Gradient Descent for Parallel Continual Learning</div>
<div id='2401.01054v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T06:26:25Z</div><div>Authors: Fan Lyu, Wei Feng, Yuepan Li, Qing Sun, Fanhua Shang, Liang Wan, Liang Wang</div><div style='padding-top: 10px; width: 80ex'>The goal of Continual Learning (CL) is to continuously learn from new data
streams and accomplish the corresponding tasks. Previously studied CL assumes
that data are given in sequence nose-to-tail for different tasks, thus indeed
belonging to Serial Continual Learning (SCL). This paper studies the novel
paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios,
where a diverse set of tasks is encountered at different time points. PCL
presents challenges due to the training of an unspecified number of tasks with
varying learning progress, leading to the difficulty of guaranteeing effective
model updates for all encountered tasks. In our previous conference work, we
focused on measuring and reducing the discrepancy among gradients in a
multi-objective optimization problem, which, however, may still contain
negative transfers in every model update. To address this issue, in the dynamic
multi-objective optimization problem, we introduce task-specific elastic
factors to adjust the descent direction towards the Pareto front. The proposed
method, called Elastic Multi-Gradient Descent (EMGD), ensures that each update
follows an appropriate Pareto descent direction, minimizing any negative impact
on previously learned tasks. To balance the training between old and new tasks,
we also propose a memory editing mechanism guided by the gradient computed
using EMGD. This editing process updates the stored data points, reducing
interference in the Pareto descent direction from previous tasks. Experiments
on public datasets validate the effectiveness of our EMGD in the PCL setting.</div><div><a href='http://arxiv.org/abs/2401.01054v1'>2401.01054v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05667v1")'>EsaCL: Efficient Continual Learning of Sparse Models</div>
<div id='2401.05667v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T04:59:44Z</div><div>Authors: Weijieying Ren, Vasant G Honavar</div><div style='padding-top: 10px; width: 80ex'>A key challenge in the continual learning setting is to efficiently learn a
sequence of tasks without forgetting how to perform previously learned tasks.
Many existing approaches to this problem work by either retraining the model on
previous tasks or by expanding the model to accommodate new tasks. However,
these approaches typically suffer from increased storage and computational
requirements, a problem that is worsened in the case of sparse models due to
need for expensive re-training after sparsification. To address this challenge,
we propose a new method for efficient continual learning of sparse models
(EsaCL) that can automatically prune redundant parameters without adversely
impacting the model's predictive power, and circumvent the need of retraining.
We conduct a theoretical analysis of loss landscapes with parameter pruning,
and design a directional pruning (SDP) strategy that is informed by the
sharpness of the loss function with respect to the model parameters. SDP
ensures model with minimal loss of predictive accuracy, accelerating the
learning of sparse models at each stage. To accelerate model update, we
introduce an intelligent data selection (IDS) strategy that can identify
critical instances for estimating loss landscape, yielding substantially
improved data efficiency. The results of our experiments show that EsaCL
achieves performance that is competitive with the state-of-the-art methods on
three continual learning benchmarks, while using substantially reduced memory
and computational resources.</div><div><a href='http://arxiv.org/abs/2401.05667v1'>2401.05667v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14679v1")'>Continual Learning by Three-Phase Consolidation</div>
<div id='2403.14679v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T15:31:14Z</div><div>Authors: Davide Maltoni, Lorenzo Pellegrini</div><div style='padding-top: 10px; width: 80ex'>TPC (Three-Phase Consolidation) is here introduced as a simple but effective
approach to continually learn new classes (and/or instances of known classes)
while controlling forgetting of previous knowledge. Each experience (a.k.a.
task) is learned in three phases characterized by different rules and learning
dynamics, aimed at removing the class-bias problem (due to class unbalancing)
and limiting gradient-based corrections to prevent forgetting of
underrepresented classes. Several experiments on complex datasets demonstrate
its accuracy and efficiency advantages over competitive existing approaches.
The algorithm and all the results presented in this paper are fully
reproducible thanks to its publication on the Avalanche open framework for
continual learning.</div><div><a href='http://arxiv.org/abs/2403.14679v1'>2403.14679v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10853v1")'>Just Say the Name: Online Continual Learning with Category Names Only
  via Data Generation</div>
<div id='2403.10853v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T08:28:42Z</div><div>Authors: Minhyuk Seo, Diganta Misra, Seongwon Cho, Minjae Lee, Jonghyun Choi</div><div style='padding-top: 10px; width: 80ex'>In real-world scenarios, extensive manual annotation for continual learning
is impractical due to prohibitive costs. Although prior arts, influenced by
large-scale webly supervised training, suggest leveraging web-scraped data in
continual learning, this poses challenges such as data imbalance, usage
restrictions, and privacy concerns. Addressing the risks of continual webly
supervised training, we present an online continual learning framework -
Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a
set of generators G along with the learner. When encountering new concepts
(i.e., classes), G-NoCL employs the novel sample complexity-guided data
ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to
optimally sample training data from generated data. Through extensive
experimentation, we demonstrate superior performance of DISCOBER in G-NoCL
online CL benchmarks, covering both In-Distribution (ID) and
Out-of-Distribution (OOD) generalization evaluations, compared to naive
generator-ensembling, web-supervised, and manually annotated data.</div><div><a href='http://arxiv.org/abs/2403.10853v1'>2403.10853v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00622v2")'>Federated Class-Incremental Learning with New-Class Augmented
  Self-Distillation</div>
<div id='2401.00622v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T00:54:02Z</div><div>Authors: Zhiyuan Wu, Tianliu He, Sheng Sun, Yuwei Wang, Min Liu, Bo Gao, Xuefeng Jiang</div><div style='padding-top: 10px; width: 80ex'>Federated Learning (FL) enables collaborative model training among
participants while guaranteeing the privacy of raw data. Mainstream FL
methodologies overlook the dynamic nature of real-world data, particularly its
tendency to grow in volume and diversify in classes over time. This oversight
results in FL methods suffering from catastrophic forgetting, where the trained
models inadvertently discard previously learned information upon assimilating
new data. In response to this challenge, we propose a novel Federated
Class-Incremental Learning (FCIL) method, named \underline{Fed}erated
\underline{C}lass-Incremental \underline{L}earning with New-Class
\underline{A}ugmented \underline{S}elf-Di\underline{S}tillation (FedCLASS). The
core of FedCLASS is to enrich the class scores of historical models with new
class scores predicted by current models and utilize the combined knowledge for
self-distillation, enabling a more sufficient and precise knowledge transfer
from historical models to current models. Theoretical analyses demonstrate that
FedCLASS stands on reliable foundations, considering scores of old classes
predicted by historical models as conditional probabilities in the absence of
new classes, and the scores of new classes predicted by current models as the
conditional probabilities of class scores derived from historical models.
Empirical experiments demonstrate the superiority of FedCLASS over four
baseline algorithms in reducing average forgetting rate and boosting global
accuracy.</div><div><a href='http://arxiv.org/abs/2401.00622v2'>2401.00622v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12030v1")'>Expandable Subspace Ensemble for Pre-Trained Model-Based
  Class-Incremental Learning</div>
<div id='2403.12030v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:58:13Z</div><div>Authors: Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, De-Chuan Zhan</div><div style='padding-top: 10px; width: 80ex'>Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Despite the strong performance of
Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new
classes often results in the overwriting of old ones. Excessive modification of
the network causes forgetting, while minimal adjustments lead to an inadequate
fit for new classes. As a result, it is desired to figure out a way of
efficient model updating without harming former knowledge. In this paper, we
propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model
updating without conflict, we train a distinct lightweight adapter module for
each new task, aiming to create task-specific subspaces. These adapters span a
high-dimensional feature space, enabling joint decision-making across multiple
subspaces. As data evolves, the expanding subspaces render the old class
classifiers incompatible with new-stage spaces. Correspondingly, we design a
semantic-guided prototype complement strategy that synthesizes old classes' new
features without using any old class instance. Extensive experiments on seven
benchmark datasets verify EASE's state-of-the-art performance. Code is
available at: https://github.com/sun-hailong/CVPR24-Ease</div><div><a href='http://arxiv.org/abs/2403.12030v1'>2403.12030v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07851v1")'>12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning</div>
<div id='2403.07851v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:43:20Z</div><div>Authors: Yoga Esa Wibowo, Cristian Cioflan, Thorir Mar Ingolfsson, Michael Hersche, Leo Zhao, Abbas Rahimi, Luca Benini</div><div style='padding-top: 10px; width: 80ex'>Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems
to expand their inference capabilities to new classes using only a few labeled
examples, without forgetting the previously learned classes. Classical
backpropagation-based learning and its variants are often unsuitable for
battery-powered, memory-constrained systems at the extreme edge. In this work,
we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a
lightweight model consisting of a pretrained and metalearned feature extractor
and an expandable explicit memory storing the class prototypes. The
architecture is pretrained with a novel feature orthogonality regularization
and metalearned with a multi-margin loss. For learning a new class, our
approach extends the explicit memory with novel class prototypes, while the
remaining architecture is kept frozen. This allows learning previously unseen
classes based on only a few examples with one single pass (hence online).
O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,
achieving state-of-the-art results. Tailored for ultra-low-power platforms, we
implement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online
learning capabilities within just 12 mJ per new class.</div><div><a href='http://arxiv.org/abs/2403.07851v1'>2403.07851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02457v1")'>eCIL-MU: Embedding based Class Incremental Learning and Machine
  Unlearning</div>
<div id='2401.02457v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T07:18:32Z</div><div>Authors: Zhiwei Zuo, Zhuo Tang, Bin Wang, Kenli Li, Anwitaman Datta</div><div style='padding-top: 10px; width: 80ex'>New categories may be introduced over time, or existing categories may need
to be reclassified. Class incremental learning (CIL) is employed for the
gradual acquisition of knowledge about new categories while preserving
information about previously learned ones in such dynamic environments. It
might also be necessary to also eliminate the influence of related categories
on the model to adapt to reclassification. We thus introduce class-level
machine unlearning (MU) within CIL. Typically, MU methods tend to be
time-consuming and can potentially harm the model's performance. A continuous
stream of unlearning requests could lead to catastrophic forgetting. To address
these issues, we propose a non-destructive eCIL-MU framework based on embedding
techniques to map data into vectors and then be stored in vector databases. Our
approach exploits the overlap between CIL and MU tasks for acceleration.
Experiments demonstrate the capability of achieving unlearning effectiveness
and orders of magnitude (upto $\sim 278\times$) of acceleration.</div><div><a href='http://arxiv.org/abs/2401.02457v1'>2401.02457v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08963v1")'>DUEL: Duplicate Elimination on Active Memory for Self-Supervised
  Class-Imbalanced Learning</div>
<div id='2402.08963v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T06:09:36Z</div><div>Authors: Won-Seok Choi, Hyundo Lee, Dong-Sig Han, Junseok Park, Heeyeon Koo, Byoung-Tak Zhang</div><div style='padding-top: 10px; width: 80ex'>Recent machine learning algorithms have been developed using well-curated
datasets, which often require substantial cost and resources. On the other
hand, the direct use of raw data often leads to overfitting towards frequently
occurring class information. To address class imbalances cost-efficiently, we
propose an active data filtering process during self-supervised pre-training in
our novel framework, Duplicate Elimination (DUEL). This framework integrates an
active memory inspired by human working memory and introduces distinctiveness
information, which measures the diversity of the data in the memory, to
optimize both the feature extractor and the memory. The DUEL policy, which
replaces the most duplicated data with new samples, aims to enhance the
distinctiveness information in the memory and thereby mitigate class
imbalances. We validate the effectiveness of the DUEL framework in
class-imbalanced environments, demonstrating its robustness and providing
reliable results in downstream tasks. We also analyze the role of the DUEL
policy in the training process through various metrics and visualizations.</div><div><a href='http://arxiv.org/abs/2402.08963v1'>2402.08963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10253v1")'>Open Continual Feature Selection via Granular-Ball Knowledge Transfer</div>
<div id='2403.10253v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T12:43:03Z</div><div>Authors: Xuemei Cao, Xin Yang, Shuyin Xia, Guoyin Wang, Tianrui Li</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel framework for continual feature selection (CFS)
in data preprocessing, particularly in the context of an open and dynamic
environment where unknown classes may emerge. CFS encounters two primary
challenges: the discovery of unknown knowledge and the transfer of known
knowledge. To this end, the proposed CFS method combines the strengths of
continual learning (CL) with granular-ball computing (GBC), which focuses on
constructing a granular-ball knowledge base to detect unknown classes and
facilitate the transfer of previously learned knowledge for further feature
selection. CFS consists of two stages: initial learning and open learning. The
former aims to establish an initial knowledge base through multi-granularity
representation using granular-balls. The latter utilizes prior granular-ball
knowledge to identify unknowns, updates the knowledge base for granular-ball
knowledge transfer, reinforces old knowledge, and integrates new knowledge.
Subsequently, we devise an optimal feature subset mechanism that incorporates
minimal new features into the existing optimal subset, often yielding superior
results during each period. Extensive experimental results on public benchmark
datasets demonstrate our method's superiority in terms of both effectiveness
and efficiency compared to state-of-the-art feature selection methods.</div><div><a href='http://arxiv.org/abs/2403.10253v1'>2403.10253v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08255v1")'>Distal Interference: Exploring the Limits of Model-Based Continual
  Learning</div>
<div id='2402.08255v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T07:07:37Z</div><div>Authors: Heinrich van Deventer, Anna Sergeevna Bosman</div><div style='padding-top: 10px; width: 80ex'>Continual learning is the sequential learning of different tasks by a machine
learning model. Continual learning is known to be hindered by catastrophic
interference or forgetting, i.e. rapid unlearning of earlier learned tasks when
new tasks are learned. Despite their practical success, artificial neural
networks (ANNs) are prone to catastrophic interference. This study analyses how
gradient descent and overlapping representations between distant input points
lead to distal interference and catastrophic interference. Distal interference
refers to the phenomenon where training a model on a subset of the domain leads
to non-local changes on other subsets of the domain. This study shows that
uniformly trainable models without distal interference must be exponentially
large. A novel antisymmetric bounded exponential layer B-spline ANN
architecture named ABEL-Spline is proposed that can approximate any continuous
function, is uniformly trainable, has polynomial computational complexity, and
provides some guarantees for distal interference. Experiments are presented to
demonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are also
evaluated on benchmark regression problems. It is concluded that the weaker
distal interference guarantees in ABEL-Splines are insufficient for model-only
continual learning. It is conjectured that continual learning with polynomial
complexity models requires augmentation of the training data or algorithm.</div><div><a href='http://arxiv.org/abs/2402.08255v1'>2402.08255v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17401v1")'>Step-size Optimization for Continual Learning</div>
<div id='2401.17401v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T19:35:43Z</div><div>Authors: Thomas Degris, Khurram Javed, Arsalan Sharifnassab, Yuxin Liu, Richard Sutton</div><div style='padding-top: 10px; width: 80ex'>In continual learning, a learner has to keep learning from the data over its
whole life time. A key issue is to decide what knowledge to keep and what
knowledge to let go. In a neural network, this can be implemented by using a
step-size vector to scale how much gradient samples change network weights.
Common algorithms, like RMSProp and Adam, use heuristics, specifically
normalization, to adapt this step-size vector. In this paper, we show that
those heuristics ignore the effect of their adaptation on the overall objective
function, for example by moving the step-size vector away from better step-size
vectors. On the other hand, stochastic meta-gradient descent algorithms, like
IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to
the overall objective function. On simple problems, we show that IDBD is able
to consistently improve step-size vectors, where RMSProp and Adam do not. We
explain the differences between the two approaches and their respective
limitations. We conclude by suggesting that combining both approaches could be
a promising future direction to improve the performance of neural networks in
continual learning.</div><div><a href='http://arxiv.org/abs/2401.17401v1'>2401.17401v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18449v1")'>HOP to the Next Tasks and Domains for Continual Learning in NLP</div>
<div id='2402.18449v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T16:21:02Z</div><div>Authors: Umberto Michieli, Mete Ozay</div><div style='padding-top: 10px; width: 80ex'>Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and
domains) by transferring knowledge acquired on previous problems, whilst
avoiding forgetting of past ones. Different from previous approaches which
focused on CL for one NLP task or domain in a specific use-case, in this paper,
we address a more general CL setting to learn from a sequence of problems in a
unique framework. Our method, HOP, permits to hop across tasks and domains by
addressing the CL problem along three directions: (i) we employ a set of
adapters to generalize a large pre-trained model to unseen problems, (ii) we
compute high-order moments over the distribution of embedded representations to
distinguish independent and correlated statistics across different tasks and
domains, (iii) we process this enriched information with auxiliary heads
specialized for each end problem. Extensive experimental campaign on 4 NLP
applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of
our HOP.</div><div><a href='http://arxiv.org/abs/2402.18449v1'>2402.18449v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12490v1")'>Towards Cross-Domain Continual Learning</div>
<div id='2402.12490v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:54:03Z</div><div>Authors: Marcus de Carvalho, Mahardhika Pratama, Jie Zhang, Chua Haoyan, Edward Yapp</div><div style='padding-top: 10px; width: 80ex'>Continual learning is a process that involves training learning agents to
sequentially master a stream of tasks or classes without revisiting past data.
The challenge lies in leveraging previously acquired knowledge to learn new
tasks efficiently, while avoiding catastrophic forgetting. Existing methods
primarily focus on single domains, restricting their applicability to specific
problems.
  In this work, we introduce a novel approach called Cross-Domain Continual
Learning (CDCL) that addresses the limitations of being limited to single
supervised domains. Our method combines inter- and intra-task cross-attention
mechanisms within a compact convolutional network. This integration enables the
model to maintain alignment with features from previous tasks, thereby delaying
the data drift that may occur between tasks, while performing unsupervised
cross-domain (UDA) between related domains. By leveraging an
intra-task-specific pseudo-labeling method, we ensure accurate input pairs for
both labeled and unlabeled samples, enhancing the learning process. To validate
our approach, we conduct extensive experiments on public UDA datasets,
showcasing its positive performance on cross-domain continual learning
challenges. Additionally, our work introduces incremental ideas that contribute
to the advancement of this field.
  We make our code and models available to encourage further exploration and
reproduction of our results: \url{https://github.com/Ivsucram/CDCL}</div><div><a href='http://arxiv.org/abs/2402.12490v1'>2402.12490v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02628v2")'>Interactive Continual Learning: Fast and Slow Thinking</div>
<div id='2403.02628v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T03:37:28Z</div><div>Authors: Biqing Qi, Xingquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou</div><div style='padding-top: 10px; width: 80ex'>Advanced life forms, sustained by the synergistic interaction of neural
cognitive mechanisms, continually acquire and transfer knowledge throughout
their lifespan. In contrast, contemporary machine learning paradigms exhibit
limitations in emulating the facets of continual learning (CL). Nonetheless,
the emergence of large language models (LLMs) presents promising avenues for
realizing CL via interactions with these models. Drawing on Complementary
Learning System theory, this paper presents a novel Interactive Continual
Learning (ICL) framework, enabled by collaborative interactions among models of
various sizes. Specifically, we assign the ViT model as System1 and multimodal
LLM as System2. To enable the memory module to deduce tasks from class
information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task
Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in
System1 through enhanced geometric representation, we introduce the CL-vMF
mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we
introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI)
strategy to identify hard examples, thus enhancing collaboration between
System1 and System2 for complex reasoning realization. Comprehensive evaluation
of our proposed ICL demonstrates significant resistance to forgetting and
superior performance relative to existing methods. Code is available at
github.com/ICL.</div><div><a href='http://arxiv.org/abs/2403.02628v2'>2403.02628v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13844v1")'>Scheduled Knowledge Acquisition on Lightweight Vector Symbolic
  Architectures for Brain-Computer Interfaces</div>
<div id='2403.13844v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T01:06:29Z</div><div>Authors: Yejia Liu, Shijin Duan, Xiaolin Xu, Shaolei Ren</div><div style='padding-top: 10px; width: 80ex'>Brain-Computer interfaces (BCIs) are typically designed to be lightweight and
responsive in real-time to provide users timely feedback. Classical feature
engineering is computationally efficient but has low accuracy, whereas the
recent neural networks (DNNs) improve accuracy but are computationally
expensive and incur high latency. As a promising alternative, the
low-dimensional computing (LDC) classifier based on vector symbolic
architecture (VSA), achieves small model size yet higher accuracy than
classical feature engineering methods. However, its accuracy still lags behind
that of modern DNNs, making it challenging to process complex brain signals. To
improve the accuracy of a small model, knowledge distillation is a popular
method. However, maintaining a constant level of distillation between the
teacher and student models may not be the best way for a growing student during
its progressive learning stages. In this work, we propose a simple scheduled
knowledge distillation method based on curriculum data order to enable the
student to gradually build knowledge from the teacher model, controlled by an
$\alpha$ scheduler. Meanwhile, we employ the LDC/VSA as the student model to
enhance the on-device inference efficiency for tiny BCI devices that demand low
latency. The empirical results have demonstrated that our approach achieves
better tradeoff between accuracy and hardware efficiency compared to other
methods.</div><div><a href='http://arxiv.org/abs/2403.13844v1'>2403.13844v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09067v1")'>Towards Continual Learning Desiderata via HSIC-Bottleneck
  Orthogonalization and Equiangular Embedding</div>
<div id='2401.09067v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T09:01:29Z</div><div>Authors: Depeng Li, Tianqi Wang, Junwei Chen, Qining Ren, Kenji Kawaguchi, Zhigang Zeng</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks are susceptible to catastrophic forgetting when trained
on sequential tasks. Various continual learning (CL) methods often rely on
exemplar buffers or/and network expansion for balancing model stability and
plasticity, which, however, compromises their practical value due to privacy
and memory concerns. Instead, this paper considers a strict yet realistic
setting, where the training data from previous tasks is unavailable and the
model size remains relatively constant during sequential training. To achieve
such desiderata, we propose a conceptually simple yet effective method that
attributes forgetting to layer-wise parameter overwriting and the resulting
decision boundary distortion. This is achieved by the synergy between two key
components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten
parameter updates mediated by Hilbert-Schmidt independence criterion in an
orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary
adaptation between old and new tasks with predefined basis vectors. Extensive
experiments demonstrate that our method achieves competitive accuracy
performance, even with absolute superiority of zero exemplar buffer and 1.02x
the base model.</div><div><a href='http://arxiv.org/abs/2401.09067v1'>2401.09067v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06725v1")'>Improving Low-Resource Knowledge Tracing Tasks by Supervised
  Pre-training and Importance Mechanism Fine-tuning</div>
<div id='2403.06725v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T13:44:43Z</div><div>Authors: Hengyuan Zhang, Zitao Liu, Shuyan Huang, Chenming Shang, Bojun Zhan, Yong Jiang</div><div style='padding-top: 10px; width: 80ex'>Knowledge tracing (KT) aims to estimate student's knowledge mastery based on
their historical interactions. Recently, the deep learning based KT (DLKT)
approaches have achieved impressive performance in the KT task. These DLKT
models heavily rely on the large number of available student interactions.
However, due to various reasons such as budget constraints and privacy
concerns, observed interactions are very limited in many real-world scenarios,
a.k.a, low-resource KT datasets. Directly training a DLKT model on a
low-resource KT dataset may lead to overfitting and it is difficult to choose
the appropriate deep neural architecture. Therefore, in this paper, we propose
a low-resource KT framework called LoReKT to address above challenges. Inspired
by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn
transferable parameters and representations from rich-resource KT datasets
during the pre-training stage and subsequently facilitate effective adaptation
to low-resource KT datasets. Specifically, we simplify existing sophisticated
DLKT model architectures with purely a stack of transformer decoders. We design
an encoding mechanism to incorporate student interactions from multiple KT data
sources and develop an importance mechanism to prioritize updating parameters
with high importance while constraining less important ones during the
fine-tuning stage. We evaluate LoReKT on six public KT datasets and
experimental results demonstrate the superiority of our approach in terms of
AUC and Accuracy. To encourage reproducible research, we make our data and code
publicly available at https://anonymous.4open.science/r/LoReKT-C619.</div><div><a href='http://arxiv.org/abs/2403.06725v1'>2403.06725v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15304v1")'>KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing</div>
<div id='2403.15304v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T15:54:30Z</div><div>Authors: Yahya Badran, Christine Preisach</div><div style='padding-top: 10px; width: 80ex'>Knowledge Tracing (KT) is concerned with predicting students' future
performance on learning items in intelligent tutoring systems. Learning items
are tagged with skill labels called knowledge concepts (KCs). Many KT models
expand the sequence of item-student interactions into KC-student interactions
by replacing learning items with their constituting KCs. This often results in
a longer sequence length. This approach addresses the issue of sparse
item-student interactions and minimises model parameters. However, two problems
have been identified with such models.
  The first problem is the model's ability to learn correlations between KCs
belonging to the same item, which can result in the leakage of ground truth
labels and hinder performance. This problem can lead to a significant decrease
in performance on datasets with a higher number of KCs per item. The second
problem is that the available benchmark implementations ignore accounting for
changes in sequence length when expanding KCs, leading to different models
being tested with varying sequence lengths but still compared against the same
benchmark.
  To address these problems, we introduce a general masking framework that
mitigates the first problem and enhances the performance of such KT models
while preserving the original model architecture without significant
alterations. Additionally, we introduce KTbench, an open-source benchmark
library designed to ensure the reproducibility of this work while mitigating
the second problem.</div><div><a href='http://arxiv.org/abs/2403.15304v1'>2403.15304v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16832v1")'>Analysis of Knowledge Tracing performance on synthesised student data</div>
<div id='2401.16832v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T09:19:50Z</div><div>Authors: Panagiotis Pagonis, Kai Hartung, Di Wu, Munir Georges, Sören Gröttrup</div><div style='padding-top: 10px; width: 80ex'>Knowledge Tracing (KT) aims to predict the future performance of students by
tracking the development of their knowledge states. Despite all the recent
progress made in this field, the application of KT models in education systems
is still restricted from the data perspectives: 1) limited access to real life
data due to data protection concerns, 2) lack of diversity in public datasets,
3) noises in benchmark datasets such as duplicate records. To resolve these
problems, we simulated student data with three statistical strategies based on
public datasets and tested their performance on two KT baselines. While we
observe only minor performance improvement with additional synthetic data, our
work shows that using only synthetic data for training can lead to similar
performance as real data.</div><div><a href='http://arxiv.org/abs/2401.16832v1'>2401.16832v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07322v1")'>A Question-centric Multi-experts Contrastive Learning Framework for
  Improving the Accuracy and Interpretability of Deep Sequential Knowledge
  Tracing Models</div>
<div id='2403.07322v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T05:15:42Z</div><div>Authors: Hengyuan Zhang, Zitao Liu, Chenming Shang, Dawei Li, Yong Jiang</div><div style='padding-top: 10px; width: 80ex'>Knowledge tracing (KT) plays a crucial role in predicting students' future
performance by analyzing their historical learning processes. Deep neural
networks (DNNs) have shown great potential in solving the KT problem. However,
there still exist some important challenges when applying deep learning
techniques to model the KT process. The first challenge lies in taking the
individual information of the question into modeling. This is crucial because,
despite questions sharing the same knowledge component (KC), students'
knowledge acquisition on homogeneous questions can vary significantly. The
second challenge lies in interpreting the prediction results from existing deep
learning-based KT models. In real-world applications, while it may not be
necessary to have complete transparency and interpretability of the model
parameters, it is crucial to present the model's prediction results in a manner
that teachers find interpretable. This makes teachers accept the rationale
behind the prediction results and utilize them to design teaching activities
and tailored learning strategies for students. However, the inherent black-box
nature of deep learning techniques often poses a hurdle for teachers to fully
embrace the model's prediction results. To address these challenges, we propose
a Question-centric Multi-experts Contrastive Learning framework for KT called
Q-MCKT.</div><div><a href='http://arxiv.org/abs/2403.07322v1'>2403.07322v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13179v1")'>Predictive, scalable and interpretable knowledge tracing on structured
  domains</div>
<div id='2403.13179v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T22:19:29Z</div><div>Authors: Hanqi Zhou, Robert Bamler, Charley M. Wu, Álvaro Tejero-Cantero</div><div style='padding-top: 10px; width: 80ex'>Intelligent tutoring systems optimize the selection and timing of learning
materials to enhance understanding and long-term retention. This requires
estimates of both the learner's progress (''knowledge tracing''; KT), and the
prerequisite structure of the learning domain (''knowledge mapping''). While
recent deep learning models achieve high KT accuracy, they do so at the expense
of the interpretability of psychologically-inspired models. In this work, we
present a solution to this trade-off. PSI-KT is a hierarchical generative
approach that explicitly models how both individual cognitive traits and the
prerequisite structure of knowledge influence learning dynamics, thus achieving
interpretability by design. Moreover, by using scalable Bayesian inference,
PSI-KT targets the real-world need for efficient personalization even with a
growing body of learners and learning histories. Evaluated on three datasets
from online learning platforms, PSI-KT achieves superior multi-step predictive
accuracy and scalable inference in continual-learning settings, all while
providing interpretable representations of learner-specific traits and the
prerequisite structure of knowledge that causally supports learning. In sum,
predictive, scalable and interpretable knowledge tracing with solid knowledge
mapping lays a key foundation for effective personalized learning to make
education accessible to a broad, global audience.</div><div><a href='http://arxiv.org/abs/2403.13179v1'>2403.13179v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01746v1")'>3DG: A Framework for Using Generative AI for Handling Sparse Learner
  Performance Data From Intelligent Tutoring Systems</div>
<div id='2402.01746v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T22:34:01Z</div><div>Authors: Liang Zhang, Jionghao Lin, Conrad Borchers, Meng Cao, Xiangen Hu</div><div style='padding-top: 10px; width: 80ex'>Learning performance data (e.g., quiz scores and attempts) is significant for
understanding learner engagement and knowledge mastery level. However, the
learning performance data collected from Intelligent Tutoring Systems (ITSs)
often suffers from sparsity, impacting the accuracy of learner modeling and
knowledge assessments. To address this, we introduce the 3DG framework
(3-Dimensional tensor for Densification and Generation), a novel approach
combining tensor factorization with advanced generative models, including
Generative Adversarial Network (GAN) and Generative Pre-trained Transformer
(GPT), for enhanced data imputation and augmentation. The framework operates by
first representing the data as a three-dimensional tensor, capturing dimensions
of learners, questions, and attempts. It then densifies the data through tensor
factorization and augments it using Generative AI models, tailored to
individual learning patterns identified via clustering. Applied to data from an
AutoTutor lesson by the Center for the Study of Adult Literacy (CSAL), the 3DG
framework effectively generated scalable, personalized simulations of learning
performance. Comparative analysis revealed GAN's superior reliability over
GPT-4 in this context, underscoring its potential in addressing data sparsity
challenges in ITSs and contributing to the advancement of personalized
educational technology.</div><div><a href='http://arxiv.org/abs/2402.01746v1'>2402.01746v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14638v1")'>Personalized Programming Guidance based on Deep Programming Learning
  Style Capturing</div>
<div id='2403.14638v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T10:38:38Z</div><div>Authors: Yingfan Liu, Renyu Zhu, Ming Gao</div><div style='padding-top: 10px; width: 80ex'>With the rapid development of big data and AI technology, programming is in
high demand and has become an essential skill for students. Meanwhile,
researchers also focus on boosting the online judging system's guidance ability
to reduce students' dropout rates. Previous studies mainly targeted at
enhancing learner engagement on online platforms by providing personalized
recommendations. However, two significant challenges still need to be addressed
in programming: C1) how to recognize complex programming behaviors; C2) how to
capture intrinsic learning patterns that align with the actual learning
process. To fill these gaps, in this paper, we propose a novel model called
Programming Exercise Recommender with Learning Style (PERS), which simulates
learners' intricate programming behaviors. Specifically, since programming is
an iterative and trial-and-error process, we first introduce a positional
encoding and a differentiating module to capture the changes of consecutive
code submissions (which addresses C1). To better profile programming behaviors,
we extend the Felder-Silverman learning style model, a classical pedagogical
theory, to perceive intrinsic programming patterns. Based on this, we align
three latent vectors to record and update programming ability, processing
style, and understanding style, respectively (which addresses C2). We perform
extensive experiments on two real-world datasets to verify the rationality of
modeling programming learning styles and the effectiveness of PERS for
personalized programming guidance.</div><div><a href='http://arxiv.org/abs/2403.14638v1'>2403.14638v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07015v1")'>Adaptive Hyperparameter Optimization for Continual Learning Scenarios</div>
<div id='2403.07015v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T16:47:42Z</div><div>Authors: Rudy Semola, Julio Hurtado, Vincenzo Lomonaco, Davide Bacciu</div><div style='padding-top: 10px; width: 80ex'>Hyperparameter selection in continual learning scenarios is a challenging and
underexplored aspect, especially in practical non-stationary environments.
Traditional approaches, such as grid searches with held-out validation data
from all tasks, are unrealistic for building accurate lifelong learning
systems. This paper aims to explore the role of hyperparameter selection in
continual learning and the necessity of continually and automatically tuning
them according to the complexity of the task at hand. Hence, we propose
leveraging the nature of sequence task learning to improve Hyperparameter
Optimization efficiency. By using the functional analysis of variance-based
techniques, we identify the most crucial hyperparameters that have an impact on
performance. We demonstrate empirically that this approach, agnostic to
continual scenarios and strategies, allows us to speed up hyperparameters
optimization continually across tasks and exhibit robustness even in the face
of varying sequential task orders. We believe that our findings can contribute
to the advancement of continual learning methodologies towards more efficient,
robust and adaptable models for real-world applications.</div><div><a href='http://arxiv.org/abs/2403.07015v1'>2403.07015v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12627v1")'>A Comprehensive Review of Machine Learning Advances on Data Change: A
  Cross-Field Perspective</div>
<div id='2402.12627v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T01:16:01Z</div><div>Authors: Jeng-Lin Li, Chih-Fan Hsu, Ming-Ching Chang, Wei-Chao Chen</div><div style='padding-top: 10px; width: 80ex'>Recent artificial intelligence (AI) technologies show remarkable evolution in
various academic fields and industries. However, in the real world, dynamic
data lead to principal challenges for deploying AI models. An unexpected data
change brings about severe performance degradation in AI models. We identify
two major related research fields, domain shift and concept drift according to
the setting of the data change. Although these two popular research fields aim
to solve distribution shift and non-stationary data stream problems, the
underlying properties remain similar which also encourages similar technical
approaches. In this review, we regroup domain shift and concept drift into a
single research problem, namely the data change problem, with a systematic
overview of state-of-the-art methods in the two research fields. We propose a
three-phase problem categorization scheme to link the key ideas in the two
technical fields. We thus provide a novel scope for researchers to explore
contemporary technical strategies, learn industrial applications, and identify
future directions for addressing data change challenges.</div><div><a href='http://arxiv.org/abs/2402.12627v1'>2402.12627v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12328v1")'>Methods for Generating Drift in Text Streams</div>
<div id='2403.12328v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:48:33Z</div><div>Authors: Cristiano Mesquita Garcia, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr, Jean Paul Barddal</div><div style='padding-top: 10px; width: 80ex'>Systems and individuals produce data continuously. On the Internet, people
share their knowledge, sentiments, and opinions, provide reviews about services
and products, and so on. Automatically learning from these textual data can
provide insights to organizations and institutions, thus preventing financial
impacts, for example. To learn from textual data over time, the machine
learning system must account for concept drift. Concept drift is a frequent
phenomenon in real-world datasets and corresponds to changes in data
distribution over time. For instance, a concept drift occurs when sentiments
change or a word's meaning is adjusted over time. Although concept drift is
frequent in real-world applications, benchmark datasets with labeled drifts are
rare in the literature. To bridge this gap, this paper provides four textual
drift generation methods to ease the production of datasets with labeled
drifts. These methods were applied to Yelp and Airbnb datasets and tested using
incremental classifiers respecting the stream mining paradigm to evaluate their
ability to recover from the drifts. Results show that all methods have their
performance degraded right after the drifts, and the incremental SVM is the
fastest to run and recover the previous performance levels regarding accuracy
and Macro F1-Score.</div><div><a href='http://arxiv.org/abs/2403.12328v1'>2403.12328v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08047v1")'>Incremental Extractive Opinion Summarization Using Cover Trees</div>
<div id='2401.08047v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T02:00:17Z</div><div>Authors: Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Manzil Zaheer, Andrew McCallum, Amr Ahmed, Snigdha Chaturvedi</div><div style='padding-top: 10px; width: 80ex'>Extractive opinion summarization involves automatically producing a summary
of text about an entity (e.g., a product's reviews) by extracting
representative sentences that capture prevalent opinions in the review set.
Typically, in online marketplaces user reviews accrue over time, and opinion
summaries need to be updated periodically to provide customers with up-to-date
information. In this work, we study the task of extractive opinion
summarization in an incremental setting, where the underlying review set
evolves over time. Many of the state-of-the-art extractive opinion
summarization approaches are centrality-based, such as CentroidRank.
CentroidRank performs extractive summarization by selecting a subset of review
sentences closest to the centroid in the representation space as the summary.
However, these methods are not capable of operating efficiently in an
incremental setting, where reviews arrive one at a time. In this paper, we
present an efficient algorithm for accurately computing the CentroidRank
summaries in an incremental setting. Our approach, CoverSumm, relies on
indexing review representations in a cover tree and maintaining a reservoir of
candidate summary review sentences. CoverSumm's efficacy is supported by a
theoretical and empirical analysis of running time. Empirically, on a diverse
collection of data (both real and synthetically created to illustrate scaling
considerations), we demonstrate that CoverSumm is up to 25x faster than
baseline methods, and capable of adapting to nuanced changes in data
distribution. We also conduct human evaluations of the generated summaries and
find that CoverSumm is capable of producing informative summaries consistent
with the underlying review set.</div><div><a href='http://arxiv.org/abs/2401.08047v1'>2401.08047v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09588v1")'>Iterative Forgetting: Online Data Stream Regression Using
  Database-Inspired Adaptive Granulation</div>
<div id='2403.09588v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:26:00Z</div><div>Authors: Niket Kathiriya, Hossein Haeri, Cindy Chen, Kshitij Jerath</div><div style='padding-top: 10px; width: 80ex'>Many modern systems, such as financial, transportation, and
telecommunications systems, are time-sensitive in the sense that they demand
low-latency predictions for real-time decision-making. Such systems often have
to contend with continuous unbounded data streams as well as concept drift,
which are challenging requirements that traditional regression techniques are
unable to cater to. There exists a need to create novel data stream regression
methods that can handle these scenarios. We present a database-inspired
datastream regression model that (a) uses inspiration from R*-trees to create
granules from incoming datastreams such that relevant information is retained,
(b) iteratively forgets granules whose information is deemed to be outdated,
thus maintaining a list of only recent, relevant granules, and (c) uses the
recent data and granules to provide low-latency predictions. The
R*-tree-inspired approach also makes the algorithm amenable to integration with
database systems. Our experiments demonstrate that the ability of this method
to discard data produces a significant order-of-magnitude improvement in
latency and training time when evaluated against the most accurate
state-of-the-art algorithms, while the R*-tree-inspired granulation technique
provides competitively accurate predictions</div><div><a href='http://arxiv.org/abs/2403.09588v1'>2403.09588v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12103v1")'>LearnedWMP: Workload Memory Prediction Using Distribution of Query
  Templates</div>
<div id='2401.12103v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:38:33Z</div><div>Authors: Shaikh Quader, Andres Jaramillo, Sumona Mukhopadhyay, Ghadeer Abuoda, Calisto Zuzarte, David Kalmuk, Marin Litoiu, Manos Papagelis</div><div style='padding-top: 10px; width: 80ex'>In a modern DBMS, working memory is frequently the limiting factor when
processing in-memory analytic query operations such as joins, sorting, and
aggregation. Existing resource estimation approaches for a DBMS estimate the
resource consumption of a query by computing an estimate of each individual
database operator in the query execution plan. Such an approach is slow and
error-prone as it relies upon simplifying assumptions, such as uniformity and
independence of the underlying data. Additionally, the existing approach
focuses on individual queries separately and does not factor in other queries
in the workload that may be executed concurrently. In this research, we are
interested in query performance optimization under concurrent execution of a
batch of queries (a workload). Specifically, we focus on predicting the memory
demand for a workload rather than providing separate estimates for each query
within it. We introduce the problem of workload memory prediction and formalize
it as a distribution regression problem. We propose Learned Workload Memory
Prediction (LearnedWMP) to improve and simplify estimating the working memory
demands of workloads. Through a comprehensive experimental evaluation, we show
that LearnedWMP reduces the memory estimation error of the
state-of-the-practice method by up to 47.6%. Compared to an alternative
single-query model, during training and inferencing, the LearnedWMP model and
its variants were 3x to 10x faster. Moreover, LearnedWMP-based models were at
least 50% smaller in most cases. Overall, the results demonstrate the
advantages of the LearnedWMP approach and its potential for a broader impact on
query performance optimization.</div><div><a href='http://arxiv.org/abs/2401.12103v1'>2401.12103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07404v1")'>Accelerated Inference and Reduced Forgetting: The Dual Benefits of
  Early-Exit Networks in Continual Learning</div>
<div id='2403.07404v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T08:33:26Z</div><div>Authors: Filip Szatkowski, Fei Yang, Bartłomiej Twardowski, Tomasz Trzciński, Joost van de Weijer</div><div style='padding-top: 10px; width: 80ex'>Driven by the demand for energy-efficient employment of deep neural networks,
early-exit methods have experienced a notable increase in research attention.
These strategies allow for swift predictions by making decisions early in the
network, thereby conserving computation time and resources. However, so far the
early-exit networks have only been developed for stationary data distributions,
which restricts their application in real-world scenarios with continuous
non-stationary data. This study aims to explore the continual learning of the
early-exit networks. We adapt existing continual learning methods to fit with
early-exit architectures and investigate their behavior in the continual
setting. We notice that early network layers exhibit reduced forgetting and can
outperform standard networks even when using significantly fewer resources.
Furthermore, we analyze the impact of task-recency bias on early-exit inference
and propose Task-wise Logits Correction (TLC), a simple method that equalizes
this bias and improves the network performance for every given compute budget
in the class-incremental setting. We assess the accuracy and computational cost
of various continual learning techniques enhanced with early-exits and TLC
across standard class-incremental learning benchmarks such as 10 split CIFAR100
and ImageNetSubset and show that TLC can achieve the accuracy of the standard
methods using less than 70\% of their computations. Moreover, at full
computational budget, our method outperforms the accuracy of the standard
counterparts by up to 15 percentage points. Our research underscores the
inherent synergy between early-exit networks and continual learning,
emphasizing their practical utility in resource-constrained environments.</div><div><a href='http://arxiv.org/abs/2403.07404v1'>2403.07404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01567v1")'>Understanding Adam Optimizer via Online Learning of Updates: Adam is
  FTRL in Disguise</div>
<div id='2402.01567v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T17:00:17Z</div><div>Authors: Kwangjun Ahn, Zhiyu Zhang, Yunbum Kook, Yan Dai</div><div style='padding-top: 10px; width: 80ex'>Despite the success of the Adam optimizer in practice, the theoretical
understanding of its algorithmic components still remains limited. In
particular, most existing analyses of Adam show the convergence rate that can
be simply achieved by non-adative algorithms like SGD. In this work, we provide
a different perspective based on online learning that underscores the
importance of Adam's algorithmic components. Inspired by Cutkosky et al.
(2023), we consider the framework called online learning of updates, where we
choose the updates of an optimizer based on an online learner. With this
framework, the design of a good optimizer is reduced to the design of a good
online learner. Our main observation is that Adam corresponds to a principled
online learning framework called Follow-the-Regularized-Leader (FTRL). Building
on this observation, we study the benefits of its algorithmic components from
the online learning perspective.</div><div><a href='http://arxiv.org/abs/2402.01567v1'>2402.01567v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04923v1")'>Inconsistency-Based Data-Centric Active Open-Set Annotation</div>
<div id='2401.04923v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T04:18:02Z</div><div>Authors: Ruiyu Mao, Ouyang Xu, Yunhui Guo</div><div style='padding-top: 10px; width: 80ex'>Active learning is a commonly used approach that reduces the labeling effort
required to train deep neural networks. However, the effectiveness of current
active learning methods is limited by their closed-world assumptions, which
assume that all data in the unlabeled pool comes from a set of predefined known
classes. This assumption is often not valid in practical situations, as there
may be unknown classes in the unlabeled data, leading to the active open-set
annotation problem. The presence of unknown classes in the data can
significantly impact the performance of existing active learning methods due to
the uncertainty they introduce. To address this issue, we propose a novel
data-centric active learning method called NEAT that actively annotates
open-set data. NEAT is designed to label known classes data from a pool of both
known and unknown classes unlabeled data. It utilizes the clusterability of
labels to identify the known classes from the unlabeled pool and selects
informative samples from those classes based on a consistency criterion that
measures inconsistencies between model predictions and local feature
distribution. Unlike the recently proposed learning-centric method for the same
problem, NEAT is much more computationally efficient and is a data-centric
active open-set annotation method. Our experiments demonstrate that NEAT
achieves significantly better performance than state-of-the-art active learning
methods for active open-set annotation.</div><div><a href='http://arxiv.org/abs/2401.04923v1'>2401.04923v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15198v1")'>Bidirectional Uncertainty-Based Active Learning for Open Set Annotation</div>
<div id='2402.15198v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:59:04Z</div><div>Authors: Chen-Chen Zong, Ye-Wen Wang, Kun-Peng Ning, Haibo Ye, Sheng-Jun Huang</div><div style='padding-top: 10px; width: 80ex'>Active learning (AL) in open set scenarios presents a novel challenge of
identifying the most valuable examples in an unlabeled data pool that comprises
data from both known and unknown classes. Traditional methods prioritize
selecting informative examples with low confidence, with the risk of mistakenly
selecting unknown-class examples with similarly low confidence. Recent methods
favor the most probable known-class examples, with the risk of picking simple
already mastered examples. In this paper, we attempt to query examples that are
both likely from known classes and highly informative, and propose a
\textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework.
Specifically, we achieve this by first pushing the unknown class examples
toward regions with high-confidence predictions with our proposed
\textit{Random Label Negative Learning} method. Then, we propose a
\textit{Bidirectional Uncertainty sampling} strategy by jointly estimating
uncertainty posed by both positive and negative learning to perform consistent
and stable sampling. BUAL successfully extends existing uncertainty-based AL
methods to complex open-set scenarios. Extensive experiments on multiple
datasets with varying openness demonstrate that BUAL achieves state-of-the-art
performance.</div><div><a href='http://arxiv.org/abs/2402.15198v1'>2402.15198v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07551v1")'>Robust Semi-Supervised Learning for Self-learning Open-World Classes</div>
<div id='2401.07551v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T09:27:46Z</div><div>Authors: Wenjuan Xi, Xin Song, Weili Guo, Yang Yang</div><div style='padding-top: 10px; width: 80ex'>Existing semi-supervised learning (SSL) methods assume that labeled and
unlabeled data share the same class space. However, in real-world applications,
unlabeled data always contain classes not present in the labeled set, which may
cause classification performance degradation of known classes. Therefore,
open-world SSL approaches are researched to handle the presence of multiple
unknown classes in the unlabeled data, which aims to accurately classify known
classes while fine-grained distinguishing different unknown classes. To address
this challenge, in this paper, we propose an open-world SSL method for
Self-learning Open-world Classes (SSOC), which can explicitly self-learn
multiple unknown classes. Specifically, SSOC first defines class center tokens
for both known and unknown classes and autonomously learns token
representations according to all samples with the cross-attention mechanism. To
effectively discover novel classes, SSOC further designs a pairwise similarity
loss in addition to the entropy loss, which can wisely exploit the information
available in unlabeled data from instances' predictions and relationships.
Extensive experiments demonstrate that SSOC outperforms the state-of-the-art
baselines on multiple popular classification benchmarks. Specifically, on the
ImageNet-100 dataset with a novel ratio of 90%, SSOC achieves a remarkable 22%
improvement.</div><div><a href='http://arxiv.org/abs/2401.07551v1'>2401.07551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18495v2")'>ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype
  Learning</div>
<div id='2402.18495v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:25:06Z</div><div>Authors: Qin Zhang, Xiaowei Li, Jiexin Lu, Liping Qiu, Shirui Pan, Xiaojun Chen, Junyang Chen</div><div style='padding-top: 10px; width: 80ex'>Open-set graph learning is a practical task that aims to classify the known
class nodes and to identify unknown class samples as unknowns. Conventional
node classification methods usually perform unsatisfactorily in open-set
scenarios due to the complex data they encounter, such as out-of-distribution
(OOD) data and in-distribution (IND) noise. OOD data are samples that do not
belong to any known classes. They are outliers if they occur in training (OOD
noise), and open-set samples if they occur in testing. IND noise are training
samples which are assigned incorrect labels. The existence of IND noise and OOD
noise is prevalent, which usually cause the ambiguity problem, including the
intra-class variety problem and the inter-class confusion problem. Thus, to
explore robust open-set learning methods is necessary and difficult, and it
becomes even more difficult for non-IID graph data.To this end, we propose a
unified framework named ROG$_{PL}$ to achieve robust open-set learning on
complex noisy graph data, by introducing prototype learning. In specific,
ROG$_{PL}$ consists of two modules, i.e., denoising via label propagation and
open-set prototype learning via regions. The first module corrects noisy labels
through similarity-based label propagation and removes low-confidence samples,
to solve the intra-class variety problem caused by noise. The second module
learns open-set prototypes for each known class via non-overlapped regions and
remains both interior and border prototypes to remedy the inter-class confusion
problem.The two modules are iteratively updated under the constraints of
classification loss and prototype diversity loss. To the best of our knowledge,
the proposed ROG$_{PL}$ is the first robust open-set node classification method
for graph data with complex noise.</div><div><a href='http://arxiv.org/abs/2402.18495v2'>2402.18495v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01990v2")'>GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised
  Learning</div>
<div id='2401.01990v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T21:39:06Z</div><div>Authors: Aarash Feizi, Randall Balestriero, Adriana Romero-Soriano, Reihaneh Rabbany</div><div style='padding-top: 10px; width: 80ex'>We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.</div><div><a href='http://arxiv.org/abs/2401.01990v2'>2401.01990v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13368v1")'>Unsupervised Concept Discovery Mitigates Spurious Correlations</div>
<div id='2402.13368v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T20:48:00Z</div><div>Authors: Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello, Irina Rish, Dianbo Liu, Kenji Kawaguchi</div><div style='padding-top: 10px; width: 80ex'>Models prone to spurious correlations in training data often produce brittle
predictions and introduce unintended biases. Addressing this challenge
typically involves methods relying on prior knowledge and group annotation to
remove spurious correlations, which may not be readily available in many
applications. In this paper, we establish a novel connection between
unsupervised object-centric learning and mitigation of spurious correlations.
Instead of directly inferring sub-groups with varying correlations with labels,
our approach focuses on discovering concepts: discrete ideas that are shared
across input samples. Leveraging existing object-centric representation
learning, we introduce CoBalT: a concept balancing technique that effectively
mitigates spurious correlations without requiring human labeling of subgroups.
Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for
subpopulation shifts demonstrate superior or competitive performance compared
state-of-the-art baselines, without the need for group annotation.</div><div><a href='http://arxiv.org/abs/2402.13368v1'>2402.13368v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14140v1")'>Learning Decomposable and Debiased Representations via Attribute-Centric
  Information Bottlenecks</div>
<div id='2403.14140v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T05:33:49Z</div><div>Authors: Jinyung Hong, Eun Som Jeon, Changhoon Kim, Keun Hee Park, Utkarsh Nath, Yezhou Yang, Pavan Turaga, Theodore P. Pavlic</div><div style='padding-top: 10px; width: 80ex'>Biased attributes, spuriously correlated with target labels in a dataset, can
problematically lead to neural networks that learn improper shortcuts for
classifications and limit their capabilities for out-of-distribution (OOD)
generalization. Although many debiasing approaches have been proposed to ensure
correct predictions from biased datasets, few studies have considered learning
latent embedding consisting of intrinsic and biased attributes that contribute
to improved performance and explain how the model pays attention to attributes.
In this paper, we propose a novel debiasing framework, Debiasing Global
Workspace, introducing attention-based information bottlenecks for learning
compositional representations of attributes without defining specific bias
types. Based on our observation that learning shape-centric representation
helps robust performance on OOD datasets, we adopt those abilities to learn
robust and generalizable representations of decomposable latent embeddings
corresponding to intrinsic and biasing attributes. We conduct comprehensive
evaluations on biased datasets, along with both quantitative and qualitative
analyses, to showcase our approach's efficacy in attribute-centric
representation learning and its ability to differentiate between intrinsic and
bias-related features.</div><div><a href='http://arxiv.org/abs/2403.14140v1'>2403.14140v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18164v1")'>Autoencoder-based General Purpose Representation Learning for Customer
  Embedding</div>
<div id='2402.18164v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T08:53:20Z</div><div>Authors: Jan Henrik Bertrand, Jacopo Pio Gargano, Laurent Mombaerts, Jonathan Taws</div><div style='padding-top: 10px; width: 80ex'>In recent years, exploiting the domain-specific underlying structure of data
and its generative factors for representation learning has shown success in
various use-case agnostic applications. However, the diversity and complexity
of tabular data have made it challenging to represent these structures in a
latent space through multi-dimensional vectors. We design an autoencoder-based
framework for building general purpose embeddings, we assess the performance of
different autoencoder architectures, and show simpler models outperform complex
ones in embedding highly complex tabular data. We apply our framework to
produce plug-and-play, rich, and anonymized embeddings representing AWS
customers for usage in any model, saving up to 45% of development time, and
observe significant improvements in downstream models. Moreover, we propose a
significant improvement to the calculation of reconstruction loss for
multi-layer contractive autoencoders (CAE) by calculating the Jacobian of the
entire encoder leading to a 15% improvement in reconstruction quality when
compared to a stacked CAE.</div><div><a href='http://arxiv.org/abs/2402.18164v1'>2402.18164v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.07395v1")'>Harnessing the Power of Beta Scoring in Deep Active Learning for
  Multi-Label Text Classification</div>
<div id='2401.07395v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T00:06:24Z</div><div>Authors: Wei Tan, Ngoc Dang Nguyen, Lan Du, Wray Buntine</div><div style='padding-top: 10px; width: 80ex'>Within the scope of natural language processing, the domain of multi-label
text classification is uniquely challenging due to its expansive and uneven
label distribution. The complexity deepens due to the demand for an extensive
set of annotated data for training an advanced deep learning model, especially
in specialized fields where the labeling task can be labor-intensive and often
requires domain-specific knowledge. Addressing these challenges, our study
introduces a novel deep active learning strategy, capitalizing on the Beta
family of proper scoring rules within the Expected Loss Reduction framework. It
computes the expected increase in scores using the Beta Scoring Rules, which
are then transformed into sample vector representations. These vector
representations guide the diverse selection of informative samples, directly
linking this process to the model's expected proper score. Comprehensive
evaluations across both synthetic and real datasets reveal our method's
capability to often outperform established acquisition techniques in
multi-label text classification, presenting encouraging outcomes across various
architectural and dataset scenarios.</div><div><a href='http://arxiv.org/abs/2401.07395v1'>2401.07395v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01696v2")'>HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text
  Classification</div>
<div id='2402.01696v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T04:44:42Z</div><div>Authors: Vidit Jain, Mukund Rungta, Yuchen Zhuang, Yue Yu, Zeyu Wang, Mu Gao, Jeffrey Skolnick, Chao Zhang</div><div style='padding-top: 10px; width: 80ex'>Hierarchical text classification (HTC) is a complex subtask under multi-label
text classification, characterized by a hierarchical label taxonomy and data
imbalance. The best-performing models aim to learn a static representation by
combining document and hierarchical label information. However, the relevance
of document sections can vary based on the hierarchy level, necessitating a
dynamic document representation. To address this, we propose HiGen, a
text-generation-based framework utilizing language models to encode dynamic
text representations. We introduce a level-guided loss function to capture the
relationship between text and label name semantics. Our approach incorporates a
task-specific pretraining strategy, adapting the language model to in-domain
knowledge and significantly enhancing performance for classes with limited
examples. Furthermore, we present a new and valuable dataset called ENZYME,
designed for HTC, which comprises articles from PubMed with the goal of
predicting Enzyme Commission (EC) numbers. Through extensive experiments on the
ENZYME dataset and the widely recognized WOS and NYT datasets, our methodology
demonstrates superior performance, surpassing existing approaches while
efficiently handling data and mitigating class imbalance. The data and code
will be released publicly.</div><div><a href='http://arxiv.org/abs/2402.01696v2'>2402.01696v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00165v1")'>TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text
  Classification with Minimal Supervision</div>
<div id='2403.00165v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T22:26:07Z</div><div>Authors: Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Jinfeng Xiao, Jiaming Shen, Jiawei Han</div><div style='padding-top: 10px; width: 80ex'>Hierarchical text classification aims to categorize each document into a set
of classes in a label taxonomy. Most earlier works focus on fully or
semi-supervised methods that require a large amount of human annotated data
which is costly and time-consuming to acquire. To alleviate human efforts, in
this paper, we work on hierarchical text classification with the minimal amount
of supervision: using the sole class name of each node as the only supervision.
Recently, large language models (LLM) show competitive performance on various
tasks through zero-shot prompting, but this method performs poorly in the
hierarchical setting, because it is ineffective to include the large and
structured label space in a prompt. On the other hand, previous
weakly-supervised hierarchical text classification methods only utilize the raw
taxonomy skeleton and ignore the rich information hidden in the text corpus
that can serve as additional class-indicative features. To tackle the above
challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced
weakly-supervised hierarchical text classification, which (1) automatically
enriches the label taxonomy with class-indicative topical terms mined from the
corpus to facilitate classifier training and (2) utilizes LLMs for both data
annotation and creation tailored for the hierarchical label space. Experiments
show that TELEClass can outperform previous weakly-supervised hierarchical text
classification methods and LLM-based zero-shot prompting methods on two public
datasets.</div><div><a href='http://arxiv.org/abs/2403.00165v1'>2403.00165v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00888v1")'>Margin Discrepancy-based Adversarial Training for Multi-Domain Text
  Classification</div>
<div id='2403.00888v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T11:54:14Z</div><div>Authors: Yuan Wu</div><div style='padding-top: 10px; width: 80ex'>Multi-domain text classification (MDTC) endeavors to harness available
resources from correlated domains to enhance the classification accuracy of the
target domain. Presently, most MDTC approaches that embrace adversarial
training and the shared-private paradigm exhibit cutting-edge performance.
Unfortunately, these methods face a non-negligible challenge: the absence of
theoretical guarantees in the design of MDTC algorithms. The dearth of
theoretical underpinning poses a substantial impediment to the advancement of
MDTC algorithms. To tackle this problem, we first provide a theoretical
analysis of MDTC by decomposing the MDTC task into multiple domain adaptation
tasks. We incorporate the margin discrepancy as the measure of domain
divergence and establish a new generalization bound based on Rademacher
complexity. Subsequently, we propose a margin discrepancy-based adversarial
training (MDAT) approach for MDTC, in accordance with our theoretical analysis.
To validate the efficacy of the proposed MDAT method, we conduct empirical
studies on two MDTC benchmarks. The experimental results demonstrate that our
MDAT approach surpasses state-of-the-art baselines on both datasets.</div><div><a href='http://arxiv.org/abs/2403.00888v1'>2403.00888v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00769v1")'>Text mining in education</div>
<div id='2403.00769v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T11:41:25Z</div><div>Authors: R. Ferreira-Mello, M. Andre, A. Pinheiro, E. Costa, C. Romero</div><div style='padding-top: 10px; width: 80ex'>The explosive growth of online education environments is generating a massive
volume of data, specially in text format from forums, chats, social networks,
assessments, essays, among others. It produces exciting challenges on how to
mine text data in order to find useful knowledge for educational stakeholders.
Despite the increasing number of educational applications of text mining
published recently, we have not found any paper surveying them. In this line,
this work presents a systematic overview of the current status of the
Educational Text Mining field. Our final goal is to answer three main research
questions: Which are the text mining techniques most used in educational
environments? Which are the most used educational resources? And which are the
main applications or educational goals? Finally, we outline the conclusions and
the more interesting future trends.</div><div><a href='http://arxiv.org/abs/2403.00769v1'>2403.00769v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00232v1")'>Learning Label Hierarchy with Supervised Contrastive Learning</div>
<div id='2402.00232v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T23:21:40Z</div><div>Authors: Ruixue Lian, William A. Sethares, Junjie Hu</div><div style='padding-top: 10px; width: 80ex'>Supervised contrastive learning (SCL) frameworks treat each class as
independent and thus consider all classes to be equally important. This
neglects the common scenario in which label hierarchy exists, where
fine-grained classes under the same category show more similarity than very
different ones. This paper introduces a family of Label-Aware SCL methods
(LASCL) that incorporates hierarchical information to SCL by leveraging
similarities between classes, resulting in creating a more well-structured and
discriminative feature space. This is achieved by first adjusting the distance
between instances based on measures of the proximity of their classes with the
scaled instance-instance-wise contrastive. An additional instance-center-wise
contrastive is introduced to move within-class examples closer to their
centers, which are represented by a set of learnable label parameters. The
learned label parameters can be directly used as a nearest neighbor classifier
without further finetuning. In this way, a better feature representation is
generated with improvements of intra-cluster compactness and inter-cluster
separation. Experiments on three datasets show that the proposed LASCL works
well on text classification of distinguishing a single label among
multi-labels, outperforming the baseline supervised approaches. Our code is
publicly available.</div><div><a href='http://arxiv.org/abs/2402.00232v1'>2402.00232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01963v1")'>Improving Large-Scale k-Nearest Neighbor Text Categorization with Label
  Autoencoders</div>
<div id='2402.01963v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T00:11:29Z</div><div>Authors: Francisco J. Ribadas-Pena, Shuyuan Cao, Víctor M. Darriba Bilbao</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce a multi-label lazy learning approach to deal with
automatic semantic indexing in large document collections in the presence of
complex and structured label vocabularies with high inter-label correlation.
The proposed method is an evolution of the traditional k-Nearest Neighbors
algorithm which uses a large autoencoder trained to map the large label space
to a reduced size latent space and to regenerate the predicted labels from this
latent space. We have evaluated our proposal in a large portion of the MEDLINE
biomedical document collection which uses the Medical Subject Headings (MeSH)
thesaurus as a controlled vocabulary. In our experiments we propose and
evaluate several document representation approaches and different label
autoencoder configurations.</div><div><a href='http://arxiv.org/abs/2402.01963v1'>2402.01963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08096v1")'>Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?</div>
<div id='2402.08096v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:32:12Z</div><div>Authors: Andrew Bai, Chih-Kuan Yeh, Cho-Jui Hsieh, Ankur Taly</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning pretrained foundational models on specific tasks is now the de
facto approach for text and vision tasks. A known pitfall of this approach is
the forgetting of pretraining knowledge that happens during finetuning.
Rehearsing samples randomly from the pretrain dataset is a common approach to
alleviate such forgetting. However, we find that random mixing unintentionally
includes samples which are not (yet) forgotten or unlearnable by the model. We
propose a novel sampling scheme, mix-cd, that identifies and prioritizes
samples that actually face forgetting, which we call collateral damage. Since
directly identifying collateral damage samples is computationally expensive, we
propose a procedure to estimate the distribution of such samples by tracking
the statistics of finetuned samples. Our approach is lightweight, easy to
implement, and can be seamlessly integrated into existing models, offering an
effective means to retain pretrain performance without additional computational
costs.</div><div><a href='http://arxiv.org/abs/2402.08096v1'>2402.08096v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10800v1")'>Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data
  in Text-Image Encoders</div>
<div id='2403.10800v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T04:19:48Z</div><div>Authors: Andrew Geng, Pin-Yu Chen</div><div style='padding-top: 10px; width: 80ex'>When evaluating the performance of a pre-trained model transferred to a
downstream task, it is imperative to assess not only the in-distribution (ID)
accuracy of the downstream model but also its capacity to generalize and
identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden
costs associated with intrusive fine-tuning techniques. Specifically, we
demonstrate that commonly used fine-tuning methods not only distort the
representations necessary for generalizing to covariate-shifted OOD samples
(OOD generalization) but also distort the representations necessary for
detecting semantically-shifted OOD samples (OOD detection). To address these
challenges, we introduce a new model reprogramming approach for fine-tuning,
which we name Reprogrammer. Reprogrammer aims to improve the holistic
performance of the downstream model across ID, OOD generalization, and OOD
detection tasks. Our empirical evidence reveals that Reprogrammer is less
intrusive and yields superior downstream models. Furthermore, we demonstrate
that by appending an additional representation residual connection to
Reprogrammer, we can further preserve pre-training representations, resulting
in an even more safe and robust downstream model capable of excelling in many
ID classification, OOD generalization, and OOD detection settings.</div><div><a href='http://arxiv.org/abs/2403.10800v1'>2403.10800v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17826v1")'>Prediction-Powered Ranking of Large Language Models</div>
<div id='2402.17826v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T19:00:01Z</div><div>Authors: Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</div><div style='padding-top: 10px; width: 80ex'>Large language models are often ranked according to their level of alignment
with human preferences -- a model is better than other models if its outputs
are more frequently preferred by humans. One of the most popular ways to elicit
human preferences utilizes pairwise comparisons between the outputs provided by
different models to the same inputs. However, since gathering pairwise
comparisons by humans is costly and time-consuming, it has become a very common
practice to gather pairwise comparisons by a strong large language model -- a
model strongly aligned with human preferences. Surprisingly, practitioners
cannot currently measure the uncertainty that any mismatch between human and
model preferences may introduce in the constructed rankings. In this work, we
develop a statistical framework to bridge this gap. Given a small set of
pairwise comparisons by humans and a large set of pairwise comparisons by a
model, our framework provides a rank-set -- a set of possible ranking positions
-- for each of the models under comparison. Moreover, it guarantees that, with
a probability greater than or equal to a user-specified value, the rank-sets
cover the true ranking consistent with (the distribution of) human pairwise
preferences. Our framework is computationally efficient, easy to use, and does
not make any assumption about the distribution of human preferences nor about
the degree of alignment between the pairwise comparisons by the humans and the
strong large language model.</div><div><a href='http://arxiv.org/abs/2402.17826v1'>2402.17826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00751v1")'>Unlearnable Algorithms for In-context Learning</div>
<div id='2402.00751v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:43:04Z</div><div>Authors: Andrei Muresanu, Anvith Thudi, Michael R. Zhang, Nicolas Papernot</div><div style='padding-top: 10px; width: 80ex'>Machine unlearning is a desirable operation as models get increasingly
deployed on data with unknown provenance. However, achieving exact unlearning
-- obtaining a model that matches the model distribution when the data to be
forgotten was never used -- is challenging or inefficient, often requiring
significant retraining. In this paper, we focus on efficient unlearning methods
for the task adaptation phase of a pretrained large language model (LLM). We
observe that an LLM's ability to do in-context learning for task adaptation
allows for efficient exact unlearning of task adaptation training data. We
provide an algorithm for selecting few-shot training examples to prepend to the
prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation
cost is independent of model and dataset size, meaning it scales to large
models and datasets. We additionally compare our approach to fine-tuning
approaches and discuss the trade-offs between the two approaches. This leads us
to propose a new holistic measure of unlearning cost which accounts for varying
inference costs, and conclude that in-context learning can often be more
favourable than fine-tuning for deployments involving unlearning requests.</div><div><a href='http://arxiv.org/abs/2402.00751v1'>2402.00751v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04130v3")'>Plug-and-Play Transformer Modules for Test-Time Adaptation</div>
<div id='2401.04130v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T00:24:50Z</div><div>Authors: Xiangyu Chang, Sk Miraj Ahmed, Srikanth V. Krishnamurthy, Basak Guler, Ananthram Swami, Samet Oymak, Amit K. Roy-Chowdhury</div><div style='padding-top: 10px; width: 80ex'>Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual
Prompt Tuning (VPT) have found success in enabling adaptation to new domains by
tuning small modules within a transformer model. However, the number of domains
encountered during test time can be very large, and the data is usually
unlabeled. Thus, adaptation to new domains is challenging; it is also
impractical to generate customized tuned modules for each such domain. Toward
addressing these challenges, this work introduces PLUTO: a Plug-and-pLay
modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of
modules, each specialized for different source domains, effectively creating a
``module store''. Given a target domain with few-shot unlabeled data, we
introduce an unsupervised test-time adaptation (TTA) method to (1) select a
sparse subset of relevant modules from this store and (2) create a weighted
combination of selected modules without tuning their weights. This
plug-and-play nature enables us to harness multiple most-relevant source
domains in a single inference call. Comprehensive evaluations demonstrate that
PLUTO uniformly outperforms alternative TTA methods and that selecting $\leq$5
modules suffice to extract most of the benefit. At a high level, our method
equips pre-trained transformers with the capability to dynamically adapt to new
domains, motivating a new paradigm for efficient and scalable domain
adaptation.</div><div><a href='http://arxiv.org/abs/2401.04130v3'>2401.04130v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07368v1")'>Assessing Generalization for Subpopulation Representative Modeling via
  In-Context Learning</div>
<div id='2402.07368v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T01:55:51Z</div><div>Authors: Gabriel Simmons, Vladislav Savinov</div><div style='padding-top: 10px; width: 80ex'>This study evaluates the ability of Large Language Model (LLM)-based
Subpopulation Representative Models (SRMs) to generalize from empirical data,
utilizing in-context learning with data from the 2016 and 2020 American
National Election Studies. We explore generalization across response variables
and demographic subgroups. While conditioning with empirical data improves
performance on the whole, the benefit of in-context learning varies
considerably across demographics, sometimes hurting performance for one
demographic while helping performance for others. The inequitable benefits of
in-context learning for SRM present a challenge for practitioners implementing
SRMs, and for decision-makers who might come to rely on them. Our work
highlights a need for fine-grained benchmarks captured from diverse
subpopulations that test not only fidelity but generalization.</div><div><a href='http://arxiv.org/abs/2402.07368v1'>2402.07368v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14547v3")'>OmniPred: Language Models as Universal Regressors</div>
<div id='2402.14547v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T13:36:53Z</div><div>Authors: Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi Perel, Yutian Chen</div><div style='padding-top: 10px; width: 80ex'>Over the broad landscape of experimental design, regression has been a
powerful tool to accurately predict the outcome metrics of a system or model
given a set of parameters, but has been traditionally restricted to methods
which are only applicable to a specific task. In this paper, we propose
OmniPred, a framework for training language models as universal end-to-end
regressors over $(x,y)$ evaluation data from diverse real world experiments.
Using data sourced from Google Vizier, one of the largest blackbox optimization
databases in the world, our extensive experiments demonstrate that through only
textual representations of mathematical parameters and values, language models
are capable of very precise numerical regression, and if given the opportunity
to train over multiple tasks, can significantly outperform traditional
regression models.</div><div><a href='http://arxiv.org/abs/2402.14547v3'>2402.14547v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06563v2")'>Unraveling the Mystery of Scaling Laws: Part I</div>
<div id='2403.06563v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:05:29Z</div><div>Authors: Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai</div><div style='padding-top: 10px; width: 80ex'>Scaling law principles indicate a power-law correlation between loss and
variables such as model size, dataset size, and computational resources
utilized during training. These principles play a vital role in optimizing
various aspects of model pre-training, ultimately contributing to the success
of large language models such as GPT-4, Llama and Gemini. However, the original
scaling law paper by OpenAI did not disclose the complete details necessary to
derive the precise scaling law formulas, and their conclusions are only based
on models containing up to 1.5 billion parameters. Though some subsequent works
attempt to unveil these details and scale to larger models, they often neglect
the training dependency of important factors such as the learning rate, context
length and batch size, leading to their failure to establish a reliable formula
for predicting the test loss trajectory. In this technical report, we confirm
that the scaling law formulations proposed in the original OpenAI paper remain
valid when scaling the model size up to 33 billion, but the constant
coefficients in these formulas vary significantly with the experiment setup. We
meticulously identify influential factors and provide transparent, step-by-step
instructions to estimate all constant terms in scaling-law formulas by training
on models with only 1M~60M parameters. Using these estimated formulas, we
showcase the capability to accurately predict various attributes for models
with up to 33B parameters before their training, including (1) the minimum
possible test loss; (2) the minimum required training steps and processed
tokens to achieve a specific loss; (3) the critical batch size with an optimal
time/computation trade-off at any loss value; and (4) the complete test loss
trajectory with arbitrary batch size.</div><div><a href='http://arxiv.org/abs/2403.06563v2'>2403.06563v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14270v2")'>Take the Bull by the Horns: Hard Sample-Reweighted Continual Training
  Improves LLM Generalization</div>
<div id='2402.14270v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T04:10:57Z</div><div>Authors: Xuxi Chen, Zhendong Wang, Daouda Sow, Junjie Yang, Tianlong Chen, Yingbin Liang, Mingyuan Zhou, Zhangyang Wang</div><div style='padding-top: 10px; width: 80ex'>In the rapidly advancing arena of large language models (LLMs), a key
challenge is to enhance their capabilities amid a looming shortage of
high-quality training data. Our study starts from an empirical strategy for the
light continual training of LLMs using their original pre-training data sets,
with a specific focus on selective retention of samples that incur moderately
high losses. These samples are deemed informative and beneficial for model
refinement, contrasting with the highest-loss samples, which would be discarded
due to their correlation with data noise and complexity. We then formalize this
strategy into a principled framework of Instance-Reweighted Distributionally
Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the
training focus on informative samples through an instance reweighting
mechanism, streamlined by a closed-form solution for straightforward
integration into established training protocols. Through rigorous
experimentation with various models and datasets, our findings indicate that
our sample-targeted methods significantly improve LLM performance across
multiple benchmarks, in both continual pre-training and instruction tuning
scenarios. Our codes are available at
https://github.com/VITA-Group/HardFocusTraining.</div><div><a href='http://arxiv.org/abs/2402.14270v2'>2402.14270v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09668v1")'>How to Train Data-Efficient LLMs</div>
<div id='2402.09668v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T02:27:57Z</div><div>Authors: Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed H. Chi, James Caverlee, Julian McAuley, Derek Zhiyuan Cheng</div><div style='padding-top: 10px; width: 80ex'>The training of large language models (LLMs) is expensive. In this paper, we
study data-efficient approaches for pre-training LLMs, i.e., techniques that
aim to optimize the Pareto frontier of model quality and training resource/data
consumption. We seek to understand the tradeoffs associated with data selection
routines based on (i) expensive-to-compute data-quality estimates, and (ii)
maximization of coverage and diversity-based measures in the feature space. Our
first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of
instruction-tuned LLMs to directly assess the quality of a training example. To
target coverage, we propose Density sampling, which models the data
distribution to select a diverse sample. In our comparison of 19 samplers,
involving hundreds of evaluation tasks and pre-training runs, we find that
Ask-LLM and Density are the best methods in their respective categories.
Coverage sampling can recover the performance of the full data, while models
trained on Ask-LLM data consistently outperform full-data training -- even when
we reject 90% of the original dataset, while converging up to 70% faster.</div><div><a href='http://arxiv.org/abs/2402.09668v1'>2402.09668v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16705v1")'>SelectIT: Selective Instruction Tuning for Large Language Models via
  Uncertainty-Aware Self-Reflection</div>
<div id='2402.16705v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T16:21:53Z</div><div>Authors: Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang</div><div style='padding-top: 10px; width: 80ex'>Instruction tuning (IT) is crucial to tailoring large language models (LLMs)
towards human-centric interactions. Recent advancements have shown that the
careful selection of a small, high-quality subset of IT data can significantly
enhance the performance of LLMs. Despite this, common approaches often rely on
additional models or data sets, which increases costs and limits widespread
adoption. In this work, we propose a novel approach, termed SelectIT, that
capitalizes on the foundational capabilities of the LLM itself. Specifically,
we exploit the intrinsic uncertainty present in LLMs to more effectively select
high-quality IT data, without the need for extra resources. Furthermore, we
introduce a novel IT dataset, the Selective Alpaca, created by applying
SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT
using Selective Alpaca leads to substantial model ability enhancement. The
robustness of SelectIT has also been corroborated in various foundation models
and domain-specific tasks. Our findings suggest that longer and more
computationally intensive IT data may serve as superior sources of IT, offering
valuable insights for future research in this area. Data, code, and scripts are
freely available at https://github.com/Blue-Raincoat/SelectIT.</div><div><a href='http://arxiv.org/abs/2402.16705v1'>2402.16705v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03049v3")'>EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models</div>
<div id='2402.03049v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:33:56Z</div><div>Authors: Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.</div><div><a href='http://arxiv.org/abs/2402.03049v3'>2402.03049v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06692v1")'>An Experimental Design Framework for Label-Efficient Supervised
  Finetuning of Large Language Models</div>
<div id='2401.06692v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T16:56:54Z</div><div>Authors: Gantavya Bhatt, Yifang Chen, Arnav M. Das, Jifan Zhang, Sang T. Truong, Stephen Mussmann, Yinglun Zhu, Jeffrey Bilmes, Simon S. Du, Kevin Jamieson, Jordan T. Ash, Robert D. Nowak</div><div style='padding-top: 10px; width: 80ex'>Supervised finetuning (SFT) on instruction datasets has played a crucial role
in achieving the remarkable zero-shot generalization capabilities observed in
modern large language models (LLMs). However, the annotation efforts required
to produce high quality responses for instructions are becoming prohibitively
expensive, especially as the number of tasks spanned by instruction datasets
continues to increase. Active learning is effective in identifying useful
subsets of samples to annotate from an unlabeled pool, but its high
computational cost remains a barrier to its widespread applicability in the
context of LLMs. To mitigate the annotation cost of SFT and circumvent the
computational bottlenecks of active learning, we propose using experimental
design. Experimental design techniques select the most informative samples to
label, and typically maximize some notion of uncertainty and/or diversity. In
our work, we implement a framework that evaluates several existing and novel
experimental design techniques and find that these methods consistently yield
significant gains in label efficiency with little computational overhead. On
generative tasks, our methods achieve the same generalization performance with
only $50\%$ of annotation cost required by random sampling.</div><div><a href='http://arxiv.org/abs/2401.06692v1'>2401.06692v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.18058v1")'>LongAlign: A Recipe for Long Context Alignment of Large Language Models</div>
<div id='2401.18058v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T18:29:39Z</div><div>Authors: Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li</div><div style='padding-top: 10px; width: 80ex'>Extending large language models to effectively handle long contexts requires
instruction fine-tuning on input sequences of similar length. To address this,
we present LongAlign -- a recipe of the instruction data, training, and
evaluation for long context alignment. First, we construct a long
instruction-following dataset using Self-Instruct. To ensure the data
diversity, it covers a broad range of tasks from various long context sources.
Second, we adopt the packing and sorted batching strategies to speed up
supervised fine-tuning on data with varied length distributions. Additionally,
we develop a loss weighting method to balance the contribution to the loss
across different sequences during packing training. Third, we introduce the
LongBench-Chat benchmark for evaluating instruction-following capabilities on
queries of 10k-100k in length. Experiments show that LongAlign outperforms
existing recipes for LLMs in long context tasks by up to 30\%, while also
maintaining their proficiency in handling short, generic tasks. The code, data,
and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.</div><div><a href='http://arxiv.org/abs/2401.18058v1'>2401.18058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11138v1")'>Contrastive Instruction Tuning</div>
<div id='2402.11138v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:09:32Z</div><div>Authors: Tianyi Yan, Fei Wang, James Y. Huang, Wenxuan Zhou, Fan Yin, Aram Galstyan, Wenpeng Yin, Muhao Chen</div><div style='padding-top: 10px; width: 80ex'>Instruction tuning has been used as a promising approach to improve the
performance of large language models (LLMs) on unseen tasks. However, current
LLMs exhibit limited robustness to unseen instructions, generating inconsistent
outputs when the same instruction is phrased with slightly varied forms or
language styles. This behavior indicates LLMs' lack of robustness to textual
variations and generalizability to unseen instructions, potentially leading to
trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning,
which maximizes the similarity between the hidden representations of
semantically equivalent instruction-instance pairs while minimizing the
similarity between semantically different ones. To facilitate this approach, we
augment the existing FLAN collection by paraphrasing task instructions.
Experiments on the PromptBench benchmark show that CoIN consistently improves
LLMs' robustness to unseen instructions with variations across character, word,
sentence, and semantic levels by an average of +2.5% in accuracy.</div><div><a href='http://arxiv.org/abs/2402.11138v1'>2402.11138v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01854v3")'>Multilingual Instruction Tuning With Just a Pinch of Multilinguality</div>
<div id='2401.01854v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T17:48:10Z</div><div>Authors: Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal</div><div style='padding-top: 10px; width: 80ex'>As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages
from the pre-training corpus. We first show that many languages transfer some
instruction-following capabilities to other languages from even monolingual
tuning. Furthermore, we find that only 40 multilingual examples integrated in
an English tuning set substantially improve multilingual instruction-following,
both in seen and unseen languages during tuning. In general, we observe that
models tuned on multilingual mixtures exhibit comparable or superior
performance in multiple languages compared to monolingually tuned models,
despite training on 10x fewer examples in those languages. Finally, we find
that diversifying the instruction tuning set with even just 2-4 languages
significantly improves cross-lingual generalization. Our results suggest that
building massively multilingual instruction-tuned models can be done with only
a very small set of multilingual instruction-responses.</div><div><a href='http://arxiv.org/abs/2401.01854v3'>2401.01854v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10891v1")'>Instruction Diversity Drives Generalization To Unseen Tasks</div>
<div id='2402.10891v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:47:21Z</div><div>Authors: Dylan Zhang, Justin Wang, Francois Charton</div><div style='padding-top: 10px; width: 80ex'>Instruction tuning -- fine-tuning a large language model (LLM) on pairs of
instructions and desired outcomes -- is an approach that enables pre-trained
language models to perform real-world tasks and follow human instructions. Its
practical success depends on the model learning a broader set of instructions
than those it was trained on. Yet the factors that determine model
generalization to such \emph{unseen tasks} are not well understood. %To
understand the driving factors of generalization, In this paper, we experiment
with string rewrites, a symbolic task that serves as a building block for
Turing complete Markov algorithms while allowing experimental control of
"inputs" and "instructions". We investigate the trade-off between the number of
instructions the model is trained on and the number of training samples
provided for each instruction and observe that the diversity of the instruction
set determines generalization. Generalization emerges once a diverse enough set
of tasks is provided, even though very few examples are provided for each task.
Instruction diversity also ensures robustness with respect to non-uniform
distributions of instructions in the training set.</div><div><a href='http://arxiv.org/abs/2402.10891v1'>2402.10891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14710v1")'>IEPile: Unearthing Large-Scale Schema-Based Information Extraction
  Corpus</div>
<div id='2402.14710v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:11:38Z</div><div>Authors: Honghao Gui, Hongbin Ye, Lin Yuan, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) demonstrate remarkable potential across various
domains; however, they exhibit a significant performance gap in Information
Extraction (IE). Note that high-quality instruction data is the vital key for
enhancing the specific capabilities of LLMs, while current IE datasets tend to
be small in scale, fragmented, and lack standardized schema. To this end, we
introduce IEPile, a comprehensive bilingual (English and Chinese) IE
instruction corpus, which contains approximately 0.32B tokens. We construct
IEPile by collecting and cleaning 33 existing IE datasets, and introduce
schema-based instruction generation to unearth a large-scale corpus.
Experimental results on LLaMA and Baichuan demonstrate that using IEPile can
enhance the performance of LLMs for IE, especially the zero-shot
generalization. We open-source the resource and pre-trained models, hoping to
provide valuable support to the NLP community.</div><div><a href='http://arxiv.org/abs/2402.14710v1'>2402.14710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14950v1")'>KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable
  Adaptation</div>
<div id='2403.14950v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T04:48:41Z</div><div>Authors: Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, Wei Hu</div><div style='padding-top: 10px; width: 80ex'>Parameter-efficient finetuning (PEFT) is a key technique for adapting large
language models (LLMs) to downstream tasks. In this paper, we study leveraging
knowledge graph embeddings to improve the effectiveness of PEFT. We propose a
knowledgeable adaptation method called KnowLA. It inserts an adaptation layer
into an LLM to integrate the embeddings of entities appearing in the input
text. The adaptation layer is trained in combination with LoRA on instruction
data. Experiments on six benchmarks with two popular LLMs and three knowledge
graphs demonstrate the effectiveness and robustness of KnowLA. We show that
\modelname can help activate the relevant parameterized knowledge in an LLM to
answer a question without changing its parameters or input prompts.</div><div><a href='http://arxiv.org/abs/2403.14950v1'>2403.14950v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15246v1")'>FollowIR: Evaluating and Teaching Information Retrieval Models to Follow
  Instructions</div>
<div id='2403.15246v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:42:29Z</div><div>Authors: Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini</div><div style='padding-top: 10px; width: 80ex'>Modern Large Language Models (LLMs) are capable of following long and complex
instructions that enable a diverse amount of user tasks. However, despite
Information Retrieval (IR) models using LLMs as the backbone of their
architectures, nearly all of them still only take queries as input, with no
instructions. For the handful of recent models that do take instructions, it's
unclear how they use them. We introduce our dataset FollowIR, which contains a
rigorous instruction evaluation benchmark as well as a training set for helping
IR models learn to better follow real-world instructions. FollowIR builds off
the long history of the TREC conferences: as TREC provides human annotators
with instructions (also known as narratives) to determine document relevance,
so should IR models be able to understand and decide relevance based on these
detailed instructions. Our evaluation benchmark starts with three deeply judged
TREC collections and alters the annotator instructions, re-annotating relevant
documents. Through this process, we can measure how well IR models follow
instructions, through a new pairwise evaluation framework. Our results indicate
that existing retrieval models fail to correctly use instructions, using them
for basic keywords and struggling to understand long-form information. However,
we show that it is possible for IR models to learn to follow complex
instructions: our new FollowIR-7B model has significant improvements (over 13%)
after fine-tuning on our training set.</div><div><a href='http://arxiv.org/abs/2403.15246v1'>2403.15246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03177v1")'>CIDAR: Culturally Relevant Instruction Dataset For Arabic</div>
<div id='2402.03177v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:44:17Z</div><div>Authors: Zaid Alyafeai, Khalid Almubarak, Ahmed Ashraf, Deema Alnuhait, Saied Alshahrani, Gubran A. Q. Abdulrahman, Gamil Ahmed, Qais Gawah, Zead Saleh, Mustafa Ghaleb, Yousef Ali, Maged S. Al-Shaibani</div><div style='padding-top: 10px; width: 80ex'>Instruction tuning has emerged as a prominent methodology for teaching Large
Language Models (LLMs) to follow instructions. However, current instruction
datasets predominantly cater to English or are derived from English-dominated
LLMs, resulting in inherent biases toward Western culture. This bias
significantly impacts the linguistic structures of non-English languages such
as Arabic, which has a distinct grammar reflective of the diverse cultures
across the Arab region. This paper addresses this limitation by introducing
CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic
instruction-tuning dataset culturally-aligned by human reviewers. CIDAR
contains 10,000 instruction and output pairs that represent the Arab region. We
discuss the cultural relevance of CIDAR via the analysis and comparison to
other models fine-tuned on other datasets. Our experiments show that CIDAR can
help enrich research efforts in aligning LLMs with the Arabic culture. All the
code is available at https://github.com/ARBML/CIDAR.</div><div><a href='http://arxiv.org/abs/2402.03177v1'>2402.03177v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13586v2")'>Instruction Fine-Tuning: Does Prompt Loss Matter?</div>
<div id='2401.13586v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T16:51:23Z</div><div>Authors: Mathew Huerta-Enochian</div><div style='padding-top: 10px; width: 80ex'>We present a study analyzing the effects of prompt loss weighting (PLW) on
supervised instruction fine-tuning. We recreated Stanford's Alpaca experiment
with both LLaMA 1 and LLaMA 2 and multiple instruction datasets. We found that
performance of models fine-tuned on our short-completion dataset had a
statistically significant negative quadratic relationship with PLW, but
performance of models fine-tuned on medium- and long-completion data did not
show any relationship with PLW. I.e., prompt loss can be safely ignored for
many datasets. For short-completion data, small values (0.01-0.1) of PLW were
optimal for multiple-choice and short-generation tasks while large values (~
1.0) of PLW were optimal for long-generation tasks. We concluded that low
non-zero PLW encourages models to not diverge from pre-trained model weights
during training and high PLW reduces overfitting. Finally, we present a rough
guide for selecting PLW values based on the completion-prompt length ratio of
fine-tuning data.</div><div><a href='http://arxiv.org/abs/2401.13586v2'>2401.13586v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09723v2")'>Best Arm Identification for Prompt Learning under a Limited Budget</div>
<div id='2402.09723v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T05:31:13Z</div><div>Authors: Chengshuai Shi, Kun Yang, Jing Yang, Cong Shen</div><div style='padding-top: 10px; width: 80ex'>The remarkable instruction-following capability of large language models
(LLMs) has sparked a growing interest in automatically learning suitable
prompts. However, while many effective methods have been proposed, the cost
incurred during the learning process (e.g., accessing LLM and evaluating the
responses) has not been considered. To overcome this limitation, this work
explicitly incorporates a finite budget constraint into prompt learning.
Towards developing principled solutions, a novel connection is established
between prompt learning and fixed-budget best arm identification (BAI-FB) in
multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE
(besT aRm Identification for Prompt LEarning) is proposed to harness the power
of BAI-FB in prompt learning systematically. Unique characteristics of prompt
learning further lead to two embedding-based enhancements of TRIPLE by
exploiting the ideas of clustering and function approximation. Extensive
experiments on multiple well-adopted tasks using both GPT 3.5 and Llama2
demonstrate the significant performance improvement of TRIPLE over the previous
baselines while satisfying the limited budget constraints.</div><div><a href='http://arxiv.org/abs/2402.09723v2'>2402.09723v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10559v1")'>OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy</div>
<div id='2401.10559v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T08:50:54Z</div><div>Authors: Haowen Wang, Tao Sun, Kaixiang Ji, Jian Wang, Cong Fan, Jinjie Gu</div><div style='padding-top: 10px; width: 80ex'>We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel
multi-adapter method, OrchMoE, which capitalizes on modular skill architecture
for enhanced forward transfer in neural networks. Unlike prior models that
depend on explicit task identification inputs, OrchMoE automatically discerns
task categories, streamlining the learning process. This is achieved through an
integrated mechanism comprising an Automatic Task Classification module and a
Task-Skill Allocation module, which collectively deduce task-specific
classifications and tailor skill allocation matrices. Our extensive evaluations
on the 'Super Natural Instructions' dataset, featuring 1,600 diverse
instructional tasks, indicate that OrchMoE substantially outperforms comparable
multi-adapter baselines in terms of both performance and sample utilization
efficiency, all while operating within the same parameter constraints. These
findings suggest that OrchMoE offers a significant leap forward in multi-task
learning efficiency.</div><div><a href='http://arxiv.org/abs/2401.10559v1'>2401.10559v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09181v2")'>Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with
  Positive Forward Transfer</div>
<div id='2401.09181v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T12:44:17Z</div><div>Authors: Junhao Zheng, Qianli Ma, Zhen Liu, Binquan Wu, Huawen Feng</div><div style='padding-top: 10px; width: 80ex'>Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large
Language Models (MLLMs) to meet continuously emerging requirements without
expensive retraining. MCIT faces two major obstacles: catastrophic forgetting
(where old knowledge is forgotten) and negative forward transfer (where the
performance of future tasks is degraded). Although existing methods have
greatly alleviated catastrophic forgetting, they still suffer from negative
forward transfer. By performing singular value decomposition (SVD) on input
embeddings, we discover a large discrepancy in different input embeddings. The
discrepancy results in the model learning irrelevant information for old and
pre-trained tasks, which leads to catastrophic forgetting and negative forward
transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method
projecting prompt gradient to the residual space to minimize the interference
between tasks and to the pre-trained subspace for reusing pre-trained
knowledge. Our experiments demonstrate that Fwd-Prompt achieves
state-of-the-art performance while updating fewer parameters and requiring no
old samples. Our research sheds light on the potential of continuously adapting
MLLMs to new tasks under the instruction tuning paradigm and encourages future
studies to explore MCIT. The code will soon be publicly available.</div><div><a href='http://arxiv.org/abs/2401.09181v2'>2401.09181v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08189v3")'>PRewrite: Prompt Rewriting with Reinforcement Learning</div>
<div id='2401.08189v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T08:04:50Z</div><div>Authors: Weize Kong, Spurthi Amba Hombaiah, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky</div><div style='padding-top: 10px; width: 80ex'>Prompt engineering is critical for the development of LLM-based applications.
However, it is usually done manually in a "trial and error" fashion that can be
time consuming, ineffective, and sub-optimal. Even for the prompts which
seemingly work well, there is always a lingering question: can the prompts be
made better with further modifications?
  To address these problems, we investigate automated prompt engineering in
this paper. Specifically, we propose PRewrite, an automated method to rewrite
an under-optimized prompt to a more effective prompt. We instantiate the prompt
rewriter using a LLM. The rewriter LLM is trained using reinforcement learning
to optimize the performance on a given downstream task. We conduct experiments
on diverse benchmark datasets, which demonstrates the effectiveness of
PRewrite.</div><div><a href='http://arxiv.org/abs/2401.08189v3'>2401.08189v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06833v1")'>Can LLMs Separate Instructions From Data? And What Do We Even Mean By
  That?</div>
<div id='2403.06833v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:48:56Z</div><div>Authors: Egor Zverev, Sahar Abdelnabi, Mario Fritz, Christoph H. Lampert</div><div style='padding-top: 10px; width: 80ex'>Instruction-tuned Large Language Models (LLMs) have achieved breakthrough
results, opening countless new possibilities for many practical applications.
However, LLMs lack elementary safety features that are established norms in
other areas of computer science, such as the separation between instructions
and data, causing them to malfunction or rendering them vulnerable to
manipulation and interference by third parties e.g., via indirect
prompt/command injection. Even worse, so far, there is not even an established
definition of what precisely such a separation would mean and how its violation
could be tested. In this work, we aim to close this gap. We introduce a formal
measure to quantify the phenomenon of instruction-data separation as well as an
empirical variant of the measure that can be computed from a model`s black-box
outputs. We also introduce a new dataset, SEP (Should it be Executed or
Processed?), which allows estimating the measure, and we report results on
several state-of-the-art open-source and closed LLMs. Finally, we
quantitatively demonstrate that all evaluated LLMs fail to achieve a high
amount of separation, according to our measure. The source code and SEP dataset
are openly accessible at
https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.</div><div><a href='http://arxiv.org/abs/2403.06833v1'>2403.06833v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02207v1")'>Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large
  Language Models</div>
<div id='2402.02207v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T16:43:42Z</div><div>Authors: Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy Hospedales</div><div style='padding-top: 10px; width: 80ex'>Current vision large language models (VLLMs) exhibit remarkable capabilities
yet are prone to generate harmful content and are vulnerable to even the
simplest jailbreaking attacks. Our initial analysis finds that this is due to
the presence of harmful data during vision-language instruction fine-tuning,
and that VLLM fine-tuning can cause forgetting of safety alignment previously
learned by the underpinning LLM. To address this issue, we first curate a
vision-language safe instruction-following dataset VLGuard covering various
harmful categories. Our experiments demonstrate that integrating this dataset
into standard vision-language fine-tuning or utilizing it for post-hoc
fine-tuning effectively safety aligns VLLMs. This alignment is achieved with
minimal impact on, or even enhancement of, the models' helpfulness. The
versatility of our safety fine-tuning dataset makes it a valuable resource for
safety-testing existing VLLMs, training new models or safeguarding pre-trained
VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject
unsafe instructions and substantially reduce the success rates of several
black-box adversarial attacks, which approach zero in many cases. The code and
dataset are available at https://github.com/ys-zong/VLGuard.</div><div><a href='http://arxiv.org/abs/2402.02207v1'>2402.02207v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13459v1")'>Learning to Poison Large Language Models During Instruction Tuning</div>
<div id='2402.13459v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T01:30:03Z</div><div>Authors: Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu</div><div style='padding-top: 10px; width: 80ex'>The advent of Large Language Models (LLMs) has marked significant
achievements in language processing and reasoning capabilities. Despite their
advancements, LLMs face vulnerabilities to data poisoning attacks, where
adversaries insert backdoor triggers into training data to manipulate outputs
for malicious purposes. This work further identifies additional security risks
in LLMs by designing a new data poisoning attack tailored to exploit the
instruction tuning process. We propose a novel gradient-guided backdoor trigger
learning approach to identify adversarial triggers efficiently, ensuring an
evasion of detection by conventional defenses while maintaining content
integrity. Through experimental validation across various LLMs and tasks, our
strategy demonstrates a high success rate in compromising model outputs;
poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance
Drop Rate (PDR) of around 80\%. Our work highlights the need for stronger
defenses against data poisoning attack, offering insights into safeguarding
LLMs against these more sophisticated attacks. The source code can be found on
this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.</div><div><a href='http://arxiv.org/abs/2402.13459v1'>2402.13459v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06954v1")'>OpenFedLLM: Training Large Language Models on Decentralized Private Data
  via Federated Learning</div>
<div id='2402.06954v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T13:50:11Z</div><div>Authors: Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen</div><div style='padding-top: 10px; width: 80ex'>Trained on massive publicly available data, large language models (LLMs) have
demonstrated tremendous success across various fields. While more data
contributes to better performance, a disconcerting reality is that high-quality
public data will be exhausted in a few years. In this paper, we offer a
potential next step for contemporary LLMs: collaborative and privacy-preserving
LLM training on the underutilized distributed private data via federated
learning (FL), where multiple data owners collaboratively train a shared model
without transmitting raw data. To achieve this, we build a concise, integrated,
and research-friendly framework/codebase, named OpenFedLLM. It covers federated
instruction tuning for enhancing instruction-following capability, federated
value alignment for aligning with human values, and 7 representative FL
algorithms. Besides, OpenFedLLM supports training on diverse domains, where we
cover 8 training datasets; and provides comprehensive evaluations, where we
cover 30+ evaluation metrics. Through extensive experiments, we observe that
all FL algorithms outperform local training on training LLMs, demonstrating a
clear performance improvement across a variety of settings. Notably, in a
financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can
outperform GPT-4 by a significant margin while the model obtained through
individual training cannot, demonstrating strong motivation for clients to
participate in FL. The code is available at
https://github.com/rui-ye/OpenFedLLM.</div><div><a href='http://arxiv.org/abs/2402.06954v1'>2402.06954v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07648v1")'>Characterization of Large Language Model Development in the Datacenter</div>
<div id='2403.07648v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T13:31:14Z</div><div>Authors: Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang, Qiaoling Chen, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, Yonggang Wen, Tianwei Zhang</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have presented impressive performance across
several transformative tasks. However, it is non-trivial to efficiently utilize
large-scale cluster resources to develop LLMs, often riddled with numerous
challenges such as frequent hardware failures, intricate parallelization
strategies, and imbalanced resource utilization. In this paper, we present an
in-depth characterization study of a six-month LLM development workload trace
collected from our GPU datacenter Acme. Specifically, we investigate
discrepancies between LLMs and prior task-specific Deep Learning (DL)
workloads, explore resource utilization patterns, and identify the impact of
various job failures. Our analysis summarizes hurdles we encountered and
uncovers potential opportunities to optimize systems tailored for LLMs.
Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining,
which enhances fault tolerance through LLM-involved failure diagnosis and
automatic recovery. (2) decoupled scheduling for evaluation, which achieves
timely performance feedback via trial decomposition and scheduling
optimization.</div><div><a href='http://arxiv.org/abs/2403.07648v1'>2403.07648v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02018v3")'>The Landscape and Challenges of HPC Research and LLMs</div>
<div id='2402.02018v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T04:21:07Z</div><div>Authors: Le Chen, Nesreen K. Ahmed, Akash Dutta, Arijit Bhattacharjee, Sixing Yu, Quazi Ishtiaque Mahmud, Waqwoya Abebe, Hung Phan, Aishwarya Sarkar, Branden Butler, Niranjan Hasabnis, Gal Oren, Vy A. Vo, Juan Pablo Munoz, Theodore L. Willke, Tim Mattson, Ali Jannesari</div><div style='padding-top: 10px; width: 80ex'>Recently, language models (LMs), especially large language models (LLMs),
have revolutionized the field of deep learning. Both encoder-decoder models and
prompt-based techniques have shown immense potential for natural language
processing and code-based tasks. Over the past several years, many research
labs and institutions have invested heavily in high-performance computing,
approaching or breaching exascale performance levels. In this paper, we posit
that adapting and utilizing such language model-based techniques for tasks in
high-performance computing (HPC) would be very beneficial. This study presents
our reasoning behind the aforementioned position and highlights how existing
ideas can be improved and adapted for HPC tasks.</div><div><a href='http://arxiv.org/abs/2402.02018v3'>2402.02018v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14905v1")'>MobileLLM: Optimizing Sub-billion Parameter Language Models for
  On-Device Use Cases</div>
<div id='2402.14905v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:58:55Z</div><div>Authors: Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the growing need for efficient large language models
(LLMs) on mobile devices, driven by increasing cloud costs and latency
concerns. We focus on designing top-quality LLMs with fewer than a billion
parameters, a practical choice for mobile deployment. Contrary to prevailing
belief emphasizing the pivotal role of data and parameter quantity in
determining model quality, our investigation underscores the significance of
model architecture for sub-billion scale LLMs. Leveraging deep and thin
architectures, coupled with embedding sharing and grouped-query attention
mechanisms, we establish a strong baseline network denoted as MobileLLM, which
attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M
state-of-the-art models. Additionally, we propose an immediate block-wise
weight sharing approach with no increase in model size and only marginal
latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a
further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,
MobileLLM model family shows significant improvements compared to previous
sub-billion models on chat benchmarks, and demonstrates close correctness to
LLaMA-v2 7B in API calling tasks, highlighting the capability of small models
for common on-device use cases.</div><div><a href='http://arxiv.org/abs/2402.14905v1'>2402.14905v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12844v2")'>MELTing point: Mobile Evaluation of Language Transformers</div>
<div id='2403.12844v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T15:51:21Z</div><div>Authors: Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi</div><div style='padding-top: 10px; width: 80ex'>Transformers have revolutionized the machine learning landscape, gradually
making their way into everyday tasks and equipping our computers with ``sparks
of intelligence''. However, their runtime requirements have prevented them from
being broadly deployed on mobile. As personal devices become increasingly
powerful and prompt privacy becomes an ever more pressing issue, we explore the
current state of mobile execution of Large Language Models (LLMs). To achieve
this, we have created our own automation infrastructure, MELT, which supports
the headless execution and benchmarking of LLMs on device, supporting different
models, devices and frameworks, including Android, iOS and Nvidia Jetson
devices. We evaluate popular instruction fine-tuned LLMs and leverage different
frameworks to measure their end-to-end and granular performance, tracing their
memory and energy requirements along the way.
  Our analysis is the first systematic study of on-device LLM execution,
quantifying performance, energy efficiency and accuracy across various
state-of-the-art models and showcases the state of on-device intelligence in
the era of hyperscale models. Results highlight the performance heterogeneity
across targets and corroborates that LLM inference is largely memory-bound.
Quantization drastically reduces memory requirements and renders execution
viable, but at a non-negligible accuracy cost. Drawing from its energy
footprint and thermal behavior, the continuous execution of LLMs remains
elusive, as both factors negatively affect user experience. Last, our
experience shows that the ecosystem is still in its infancy, and algorithmic as
well as hardware breakthroughs can significantly shift the execution cost. We
expect NPU acceleration, and framework-hardware co-design to be the biggest bet
towards efficient standalone execution, with the alternative of offloading
tailored towards edge deployments.</div><div><a href='http://arxiv.org/abs/2403.12844v2'>2403.12844v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08092v1")'>A Survey of Resource-efficient LLM and Multimodal Foundation Models</div>
<div id='2401.08092v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T03:35:26Z</div><div>Authors: Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, Xuanzhe Liu</div><div style='padding-top: 10px; width: 80ex'>Large foundation models, including large language models (LLMs), vision
transformers (ViTs), diffusion, and LLM-based multimodal models, are
revolutionizing the entire machine learning lifecycle, from training to
deployment. However, the substantial advancements in versatility and
performance these models offer come at a significant cost in terms of hardware
resources. To support the growth of these large models in a scalable and
environmentally sustainable way, there has been a considerable focus on
developing resource-efficient strategies. This survey delves into the critical
importance of such research, examining both algorithmic and systemic aspects.
It offers a comprehensive analysis and valuable insights gleaned from existing
literature, encompassing a broad array of topics from cutting-edge model
architectures and training/serving algorithms to practical system designs and
implementations. The goal of this survey is to provide an overarching
understanding of how current approaches are tackling the resource challenges
posed by large foundation models and to potentially inspire future
breakthroughs in this field.</div><div><a href='http://arxiv.org/abs/2401.08092v1'>2401.08092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08895v2")'>cedar: Composable and Optimized Machine Learning Input Data Pipelines</div>
<div id='2401.08895v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T00:36:58Z</div><div>Authors: Mark Zhao, Emanuel Adamiak, Christos Kozyrakis</div><div style='padding-top: 10px; width: 80ex'>The input data pipeline is an essential component of each machine learning
(ML) training job. It is responsible for reading massive amounts of training
data, processing batches of samples using complex transformations, and loading
them onto training nodes at low latency and high throughput. Performant input
data systems are becoming increasingly critical, driven by skyrocketing data
volumes and training throughput demands. Unfortunately, current input data
systems cannot fully leverage key performance optimizations, resulting in
hugely inefficient infrastructures that require significant resources -- or
worse -- underutilize expensive accelerators.
  To address these demands, we present cedar, a programming model and framework
that allows users to easily build, optimize, and execute input data pipelines.
cedar presents an easy-to-use programming interface, allowing users to define
input data pipelines using composable operators that support arbitrary ML
frameworks and libraries. Meanwhile, cedar transparently applies a complex and
extensible set of optimization techniques (e.g., offloading, caching,
prefetching, fusion, and reordering). It then orchestrates processing across a
customizable set of local and distributed compute resources in order to
maximize processing performance and efficiency, all without user input. On
average across six diverse input data pipelines, cedar achieves a 2.49x, 1.87x,
2.18x, and 2.74x higher performance compared to tf.data, tf.data service, Ray
Data, and PyTorch's DataLoader, respectively.</div><div><a href='http://arxiv.org/abs/2401.08895v2'>2401.08895v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07608v1")'>Couler: Unified Machine Learning Workflow Optimization in Cloud</div>
<div id='2403.07608v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T12:47:32Z</div><div>Authors: Xiaoda Wang, Yuan Tang, Tengda Guo, Bo Sang, Jingji Wu, Jian Sha, Ke Zhang, Jiang Qian, Mingjie Tang</div><div style='padding-top: 10px; width: 80ex'>Machine Learning (ML) has become ubiquitous, fueling data-driven applications
across various organizations. Contrary to the traditional perception of ML in
research, ML workflows can be complex, resource-intensive, and time-consuming.
Expanding an ML workflow to encompass a wider range of data infrastructure and
data types may lead to larger workloads and increased deployment costs.
Currently, numerous workflow engines are available (with over ten being widely
recognized). This variety poses a challenge for end-users in terms of mastering
different engine APIs. While efforts have primarily focused on optimizing ML
Operations (MLOps) for a specific workflow engine, current methods largely
overlook workflow optimization across different engines.
  In this work, we design and implement Couler, a system designed for unified
ML workflow optimization in the cloud. Our main insight lies in the ability to
generate an ML workflow using natural language (NL) descriptions. We integrate
Large Language Models (LLMs) into workflow generation, and provide a unified
programming interface for various workflow engines. This approach alleviates
the need to understand various workflow engines' APIs. Moreover, Couler
enhances workflow computation efficiency by introducing automated caching at
multiple stages, enabling large workflow auto-parallelization and automatic
hyperparameters tuning. These enhancements minimize redundant computational
costs and improve fault tolerance during deep learning workflow training.
Couler is extensively deployed in real-world production scenarios at Ant Group,
handling approximately 22k workflows daily, and has successfully improved the
CPU/Memory utilization by more than 15% and the workflow completion rate by
around 17%.</div><div><a href='http://arxiv.org/abs/2403.07608v1'>2403.07608v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02338v1")'>Large Language Model Adaptation for Networking</div>
<div id='2402.02338v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T04:21:34Z</div><div>Authors: Duo Wu, Xianda Wang, Yaqi Qiao, Zhi Wang, Junchen Jiang, Shuguang Cui, Fangxin Wang</div><div style='padding-top: 10px; width: 80ex'>Many networking tasks now employ deep learning (DL) to solve complex
prediction and system optimization problems. However, current design philosophy
of DL-based algorithms entails intensive engineering overhead due to the manual
design of deep neural networks (DNNs) for different networking tasks. Besides,
DNNs tend to achieve poor generalization performance on unseen data
distributions/environments.
  Motivated by the recent success of large language models (LLMs), for the
first time, this work studies the LLM adaptation for networking to explore a
more sustainable design philosophy. With the massive pre-trained knowledge and
powerful inference ability, LLM can serve as the foundation model, and is
expected to achieve "one model for all" with even better performance and
stronger generalization for various tasks. In this paper, we present NetLLM,
the first LLM adaptation framework that efficiently adapts LLMs to solve
networking problems. NetLLM addresses many practical challenges in LLM
adaptation, from how to process task-specific information with LLMs, to how to
improve the efficiency of answer generation and acquiring domain knowledge for
networking. Across three networking-related use cases - viewport prediction
(VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we
showcase the effectiveness of NetLLM in LLM adaptation for networking. Results
show that the adapted LLM surpasses state-of-the-art algorithms by 10.1-36.6%
for VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, and also achieves superior
generalization performance.</div><div><a href='http://arxiv.org/abs/2402.02338v1'>2402.02338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14902v1")'>Hydro: Adaptive Query Processing of ML Queries</div>
<div id='2403.14902v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T01:17:07Z</div><div>Authors: Gaurav Tarlok Kakkar, Jiashen Cao, Aubhro Sengupta, Joy Arulraj, Hyesoon Kim</div><div style='padding-top: 10px; width: 80ex'>Query optimization in relational database management systems (DBMSs) is
critical for fast query processing. The query optimizer relies on precise
selectivity and cost estimates to effectively optimize queries prior to
execution. While this strategy is effective for relational DBMSs, it is not
sufficient for DBMSs tailored for processing machine learning (ML) queries. In
ML-centric DBMSs, query optimization is challenging for two reasons. First, the
performance bottleneck of the queries shifts to user-defined functions (UDFs)
that often wrap around deep learning models, making it difficult to accurately
estimate UDF statistics without profiling the query. This leads to inaccurate
statistics and sub-optimal query plans. Second, the optimal query plan for ML
queries is data-dependent, necessitating DBMSs to adapt the query plan on the
fly during execution. So, a static query plan is not sufficient for such
queries.
  In this paper, we present Hydro, an ML-centric DBMS that utilizes adaptive
query processing (AQP) for efficiently processing ML queries. Hydro is designed
to quickly evaluate UDF-based query predicates by ensuring optimal predicate
evaluation order and improving the scalability of UDF execution. By integrating
AQP, Hydro continuously monitors UDF statistics, routes data to predicates in
an optimal order, and dynamically allocates resources for evaluating
predicates. We demonstrate Hydro's efficacy through four illustrative use
cases, delivering up to 11.52x speedup over a baseline system.</div><div><a href='http://arxiv.org/abs/2403.14902v1'>2403.14902v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16791v1")'>Accelerated Cloud for Artificial Intelligence (ACAI)</div>
<div id='2401.16791v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T07:09:48Z</div><div>Authors: Dachi Chen, Weitian Ding, Chen Liang, Chang Xu, Junwei Zhang, Majd Sakr</div><div style='padding-top: 10px; width: 80ex'>Training an effective Machine learning (ML) model is an iterative process
that requires effort in multiple dimensions. Vertically, a single pipeline
typically includes an initial ETL (Extract, Transform, Load) of raw datasets, a
model training stage, and an evaluation stage where the practitioners obtain
statistics of the model performance. Horizontally, many such pipelines may be
required to find the best model within a search space of model configurations.
Many practitioners resort to maintaining logs manually and writing simple glue
code to automate the workflow. However, carrying out this process on the cloud
is not a trivial task in terms of resource provisioning, data management, and
bookkeeping of job histories to make sure the results are reproducible. We
propose an end-to-end cloud-based machine learning platform, Accelerated Cloud
for AI (ACAI), to help improve the productivity of ML practitioners. ACAI
achieves this goal by enabling cloud-based storage of indexed, labeled, and
searchable data, as well as automatic resource provisioning, job scheduling,
and experiment tracking. Specifically, ACAI provides practitioners (1) a data
lake for storing versioned datasets and their corresponding metadata, and (2)
an execution engine for executing ML jobs on the cloud with automatic resource
provisioning (auto-provision), logging and provenance tracking. To evaluate
ACAI, we test the efficacy of our auto-provisioner on the MNIST handwritten
digit classification task, and we study the usability of our system using
experiments and interviews. We show that our auto-provisioner produces a 1.7x
speed-up and 39% cost reduction, and our system reduces experiment time for ML
scientists by 20% on typical ML use cases.</div><div><a href='http://arxiv.org/abs/2401.16791v1'>2401.16791v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12916v1")'>Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of
  Machine Learning Models</div>
<div id='2402.12916v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T11:06:42Z</div><div>Authors: Jiang Wu, Hongbo Wang, Chunhe Ni, Chenwei Zhang, Wenran Lu</div><div style='padding-top: 10px; width: 80ex'>Data Pipeline plays an indispensable role in tasks such as modeling machine
learning and developing data products. With the increasing diversification and
complexity of Data sources, as well as the rapid growth of data volumes,
building an efficient Data Pipeline has become crucial for improving work
efficiency and solving complex problems. This paper focuses on exploring how to
optimize data flow through automated machine learning methods by integrating
AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to
enhance the intelligence of Data Pipeline, thereby achieving better results in
machine learning tasks. By delving into the automation and optimization of Data
flows, we uncover key strategies for constructing efficient data pipelines that
can adapt to the ever-changing data landscape. This not only accelerates the
modeling process but also provides innovative solutions to complex problems,
enabling more significant outcomes in increasingly intricate data domains.
Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning</div><div><a href='http://arxiv.org/abs/2402.12916v1'>2402.12916v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07927v1")'>Intelligent Monitoring Framework for Cloud Services: A Data-Driven
  Approach</div>
<div id='2403.07927v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T19:40:32Z</div><div>Authors: Pooja Srinivas, Fiza Husain, Anjaly Parayil, Ayush Choure, Chetan Bansal, Saravan Rajmohan</div><div style='padding-top: 10px; width: 80ex'>Cloud service owners need to continuously monitor their services to ensure
high availability and reliability. Gaps in monitoring can lead to delay in
incident detection and significant negative customer impact. Current process of
monitor creation is ad-hoc and reactive in nature. Developers create monitors
using their tribal knowledge and, primarily, a trial and error based process.
As a result, monitors often have incomplete coverage which leads to production
issues, or, redundancy which results in noise and wasted effort.
  In this work, we address this issue by proposing an intelligent monitoring
framework that recommends monitors for cloud services based on their service
properties. We start by mining the attributes of 30,000+ monitors from 791
production services at Microsoft and derive a structured ontology for monitors.
We focus on two crucial dimensions: what to monitor (resources) and which
metrics to monitor. We conduct an extensive empirical study and derive key
insights on the major classes of monitors employed by cloud services at
Microsoft, their associated dimensions, and the interrelationship between
service properties and this ontology. Using these insights, we propose a deep
learning based framework that recommends monitors based on the service
properties. Finally, we conduct a user study with engineers from Microsoft
which demonstrates the usefulness of the proposed framework. The proposed
framework along with the ontology driven projections, succeeded in creating
production quality recommendations for majority of resource classes. This was
also validated by the users from the study who rated the framework's usefulness
as 4.27 out of 5.</div><div><a href='http://arxiv.org/abs/2403.07927v1'>2403.07927v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05601v1")'>Select High-Level Features: Efficient Experts from a Hierarchical
  Classification Network</div>
<div id='2403.05601v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T00:02:42Z</div><div>Authors: André Kelm, Niels Hannemann, Bruno Heberle, Lucas Schmidt, Tim Rolff, Christian Wilms, Ehsan Yaghoubi, Simone Frintrop</div><div style='padding-top: 10px; width: 80ex'>This study introduces a novel expert generation method that dynamically
reduces task and computational complexity without compromising predictive
performance. It is based on a new hierarchical classification network topology
that combines sequential processing of generic low-level features with
parallelism and nesting of high-level features. This structure allows for the
innovative extraction technique: the ability to select only high-level features
of task-relevant categories. In certain cases, it is possible to skip almost
all unneeded high-level features, which can significantly reduce the inference
cost and is highly beneficial in resource-constrained conditions. We believe
this method paves the way for future network designs that are lightweight and
adaptable, making them suitable for a wide range of applications, from compact
edge devices to large-scale clouds. In terms of dynamic inference our
methodology can achieve an exclusion of up to 88.7\,\% of parameters and
73.4\,\% fewer giga-multiply accumulate (GMAC) operations, analysis against
comparative baselines showing an average reduction of 47.6\,\% in parameters
and 5.8\,\% in GMACs across the cases we evaluated.</div><div><a href='http://arxiv.org/abs/2403.05601v1'>2403.05601v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08040v1")'>MicroT: Low-Energy and Adaptive Models for MCUs</div>
<div id='2403.08040v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T19:23:13Z</div><div>Authors: Yushan Huang, Ranya Aloufi, Xavier Cadet, Yuchen Zhao, Payam Barnaghi, Hamed Haddadi</div><div style='padding-top: 10px; width: 80ex'>We propose MicroT, a low-energy, multi-task adaptive model framework for
resource-constrained MCUs. We divide the original model into a feature
extractor and a classifier. The feature extractor is obtained through
self-supervised knowledge distillation and further optimized into part and full
models through model splitting and joint training. These models are then
deployed on MCUs, with classifiers added and trained on local tasks, ultimately
performing stage-decision for joint inference. In this process, the part model
initially processes the sample, and if the confidence score falls below the set
threshold, the full model will resume and continue the inference. We evaluate
MicroT on two models, three datasets, and two MCU boards. Our experimental
evaluation shows that MicroT effectively improves model performance and reduces
energy consumption when dealing with multiple local tasks. Compared to the
unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On
MCUs, compared to the standard full model inference, MicroT can save up to
about 29.13% in energy consumption. MicroT also allows users to adaptively
adjust the stage-decision ratio as needed, better balancing model performance
and energy consumption. Under the standard stage-decision ratio configuration,
MicroT can increase accuracy by 5.91% and save about 14.47% of energy
consumption.</div><div><a href='http://arxiv.org/abs/2403.08040v1'>2403.08040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00068v1")'>GPT4Battery: An LLM-driven Framework for Adaptive State of Health
  Estimation of Raw Li-ion Batteries</div>
<div id='2402.00068v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T14:47:15Z</div><div>Authors: Yuyuan Feng, Guosheng Hu, Zhihong Zhang</div><div style='padding-top: 10px; width: 80ex'>State of health (SOH) is a crucial indicator for assessing the degradation
level of batteries that cannot be measured directly but requires estimation.
Accurate SOH estimation enhances detection, control, and feedback for Li-ion
batteries, allowing for safe and efficient energy management and guiding the
development of new-generation batteries. Despite the significant progress in
data-driven SOH estimation, the time and resource-consuming degradation
experiments for generating lifelong training data pose a challenge in
establishing one large model capable of handling diverse types of Li-ion
batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity.
Hence, this paper utilizes the strong generalization capability of large
language model (LLM) to proposes a novel framework for adaptable SOH estimation
across diverse batteries. To match the real scenario where unlabeled data
sequentially arrives in use with distribution shifts, the proposed model is
modified by a test-time training technique to ensure estimation accuracy even
at the battery's end of life. The validation results demonstrate that the
proposed framework achieves state-of-the-art accuracy on four widely recognized
datasets collected from 62 batteries. Furthermore, we analyze the theoretical
challenges of cross-battery estimation and provide a quantitative explanation
of the effectiveness of our method.</div><div><a href='http://arxiv.org/abs/2402.00068v1'>2402.00068v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04551v1")'>Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness
  Characterization Methods for Data-Centric AI</div>
<div id='2403.04551v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:45:03Z</div><div>Authors: Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar</div><div style='padding-top: 10px; width: 80ex'>Characterizing samples that are difficult to learn from is crucial to
developing highly performant ML models. This has led to numerous Hardness
Characterization Methods (HCMs) that aim to identify "hard" samples. However,
there is a lack of consensus regarding the definition and evaluation of
"hardness". Unfortunately, current HCMs have only been evaluated on specific
types of hardness and often only qualitatively or with respect to downstream
performance, overlooking the fundamental quantitative identification task. We
address this gap by presenting a fine-grained taxonomy of hardness types.
Additionally, we propose the Hardness Characterization Analysis Toolkit
(H-CAT), which supports comprehensive and quantitative benchmarking of HCMs
across the hardness taxonomy and can easily be extended to new HCMs, hardness
types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8
hardness types. This comprehensive evaluation encompassing over 14K setups
uncovers strengths and weaknesses of different HCMs, leading to practical tips
to guide HCM selection and future development. Our findings highlight the need
for more comprehensive HCM evaluation, while we hope our hardness taxonomy and
toolkit will advance the principled evaluation and uptake of data-centric AI
methods.</div><div><a href='http://arxiv.org/abs/2403.04551v1'>2403.04551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06485v1")'>Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid
  Approach</div>
<div id='2403.06485v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T07:48:35Z</div><div>Authors: Jinxi Kuang, Jinyang Liu, Junjie Huang, Renyi Zhong, Jiazhen Gu, Lan Yu, Rui Tan, Zengyin Yang, Michael R. Lyu</div><div style='padding-top: 10px; width: 80ex'>Due to the scale and complexity of cloud systems, a system failure would
trigger an "alert storm", i.e., massive correlated alerts. Although these
alerts can be traced back to a few root causes, the overwhelming number makes
it infeasible for manual handling. Alert aggregation is thus critical to help
engineers concentrate on the root cause and facilitate failure resolution.
Existing methods typically utilize semantic similarity-based methods or
statistical methods to aggregate alerts. However, semantic similarity-based
methods overlook the causal rationale of alerts, while statistical methods can
hardly handle infrequent alerts.
  To tackle these limitations, we introduce leveraging external knowledge,
i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose
COLA, a novel hybrid approach based on correlation mining and LLM (Large
Language Model) reasoning for online alert aggregation. The correlation mining
module effectively captures the temporal and spatial relations between alerts,
measuring their correlations in an efficient manner. Subsequently, only
uncertain pairs with low confidence are forwarded to the LLM reasoning module
for detailed analysis. This hybrid design harnesses both statistical evidence
for frequent alerts and the reasoning capabilities of computationally intensive
LLMs, ensuring the overall efficiency of COLA in handling large volumes of
alerts in practical scenarios. We evaluate COLA on three datasets collected
from the production environment of a large-scale cloud platform. The
experimental results show COLA achieves F1-scores from 0.901 to 0.930,
outperforming state-of-the-art methods and achieving comparable efficiency. We
also share our experience in deploying COLA in our real-world cloud system,
Cloud X.</div><div><a href='http://arxiv.org/abs/2403.06485v1'>2403.06485v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17583v1")'>FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in
  Large-scale Cloud Systems</div>
<div id='2402.17583v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T15:14:19Z</div><div>Authors: Junjie Huang, Jinyang Liu, Zhuangbin Chen, Zhihan Jiang, Yichen LI, Jiazhen Gu, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael R. Lyu</div><div style='padding-top: 10px; width: 80ex'>Postmortem analysis is essential in the management of incidents within cloud
systems, which provides valuable insights to improve system's reliability and
robustness. At CloudA, fault pattern profiling is performed during the
postmortem phase, which involves the classification of incidents' faults into
unique categories, referred to as fault pattern. By aggregating and analyzing
these fault patterns, engineers can discern common faults, vulnerable
components and emerging fault trends. However, this process is currently
conducted by manual labeling, which has inherent drawbacks. On the one hand,
the sheer volume of incidents means only the most severe ones are analyzed,
causing a skewed overview of fault patterns. On the other hand, the complexity
of the task demands extensive domain knowledge, which leads to errors and
inconsistencies. To address these limitations, we propose an automated
approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.
It leverages hierarchy-guided contrastive learning to train a hierarchy-aware
incident encoder and predicts fault patterns with enhanced incident
representations. We evaluate FaultProfIT using the production incidents from
CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art
methods. Our ablation study and analysis also verify the effectiveness of
hierarchy-guided contrastive learning. Additionally, we have deployed
FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+
incidents from 30+ cloud services, successfully revealing several fault trends
that have informed system improvements.</div><div><a href='http://arxiv.org/abs/2402.17583v1'>2402.17583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02052v1")'>Feature Selection using the concept of Peafowl Mating in IDS</div>
<div id='2402.02052v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T06:04:49Z</div><div>Authors: Partha Ghosh, Joy Sharma, Nilesh Pandey</div><div style='padding-top: 10px; width: 80ex'>Cloud computing has high applicability as an Internet based service that
relies on sharing computing resources. Cloud computing provides services that
are Infrastructure based, Platform based and Software based. The popularity of
this technology is due to its superb performance, high level of computing
ability, low cost of services, scalability, availability and flexibility. The
obtainability and openness of data in cloud environment make it vulnerable to
the world of cyber-attacks. To detect the attacks Intrusion Detection System is
used, that can identify the attacks and ensure information security. Such a
coherent and proficient Intrusion Detection System is proposed in this paper to
achieve higher certainty levels regarding safety in cloud environment. In this
paper, the mating behavior of peafowl is incorporated into an optimization
algorithm which in turn is used as a feature selection algorithm. The algorithm
is used to reduce the huge size of cloud data so that the IDS can work
efficiently on the cloud to detect intrusions. The proposed model has been
experimented with NSL-KDD dataset as well as Kyoto dataset and have proved to
be a better as well as an efficient IDS.</div><div><a href='http://arxiv.org/abs/2402.02052v1'>2402.02052v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12867v1")'>Towards MLOps: A DevOps Tools Recommender System for Machine Learning
  System</div>
<div id='2402.12867v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T09:57:49Z</div><div>Authors: Pir Sami Ullah Shah, Naveed Ahmad, Mirza Omer Beg</div><div style='padding-top: 10px; width: 80ex'>Applying DevOps practices to machine learning system is termed as MLOps and
machine learning systems evolve on new data unlike traditional systems on
requirements. The objective of MLOps is to establish a connection between
different open-source tools to construct a pipeline that can automatically
perform steps to construct a dataset, train the machine learning model and
deploy the model to the production as well as store different versions of model
and dataset. Benefits of MLOps is to make sure the fast delivery of the new
trained models to the production to have accurate results. Furthermore, MLOps
practice impacts the overall quality of the software products and is completely
dependent on open-source tools and selection of relevant open-source tools is
considered as challenged while a generalized method to select an appropriate
open-source tools is desirable. In this paper, we present a framework for
recommendation system that processes the contextual information (e.g., nature
of data, type of the data) of the machine learning project and recommends a
relevant toolchain (tech-stack) for the operationalization of machine learning
systems. To check the applicability of the proposed framework, four different
approaches i.e., rule-based, random forest, decision trees and k-nearest
neighbors were investigated where precision, recall and f-score is measured,
the random forest out classed other approaches with highest f-score value of
0.66.</div><div><a href='http://arxiv.org/abs/2402.12867v1'>2402.12867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07585v1")'>Identifying architectural design decisions for achieving green ML
  serving</div>
<div id='2402.07585v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:35:04Z</div><div>Authors: Francisco Durán, Silverio Martínez-Fernández, Matias Martinez, Patricia Lago</div><div style='padding-top: 10px; width: 80ex'>The growing use of large machine learning models highlights concerns about
their increasing computational demands. While the energy consumption of their
training phase has received attention, fewer works have considered the
inference phase. For ML inference, the binding of ML models to the ML system
for user access, known as ML serving, is a critical yet understudied step for
achieving efficiency in ML applications.
  We examine the literature in ML architectural design decisions and Green AI,
with a special focus on ML serving. The aim is to analyze ML serving
architectural design decisions for the purpose of understanding and identifying
them with respect to quality characteristics from the point of view of
researchers and practitioners in the context of ML serving literature.
  Our results (i) identify ML serving architectural design decisions along with
their corresponding components and associated technological stack, and (ii)
provide an overview of the quality characteristics studied in the literature,
including energy efficiency.
  This preliminary study is the first step in our goal to achieve green ML
serving. Our analysis may aid ML researchers and practitioners in making
green-aware architecture design decisions when serving their models.</div><div><a href='http://arxiv.org/abs/2402.07585v1'>2402.07585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02938v1")'>Design and Implementation of an Automated Disaster-recovery System for a
  Kubernetes Cluster Using LSTM</div>
<div id='2402.02938v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:00:31Z</div><div>Authors: Ji-Beom Kim, Je-Bum Choi, Eun-Sung Jung</div><div style='padding-top: 10px; width: 80ex'>With the increasing importance of data in the modern business environment,
effective data man-agement and protection strategies are gaining increasing
research attention. Data protection in a cloud environment is crucial for
safeguarding information assets and maintaining sustainable services. This
study introduces a system structure that integrates Kubernetes management
plat-forms with backup and restoration tools. This system is designed to
immediately detect disasters and automatically recover applications from
another kubernetes cluster. The experimental results show that this system
executes the restoration process within 15 s without human intervention,
enabling rapid recovery. This, in turn, significantly reduces the potential for
delays and errors compared with manual recovery processes, thereby enhancing
data management and recovery ef-ficiency in cloud environments. Moreover, our
research model predicts the CPU utilization of the cluster using Long
Short-Term Memory (LSTM). The necessity of scheduling through this predict is
made clearer through comparison with experiments without scheduling,
demonstrating its ability to prevent performance degradation. This research
highlights the efficiency and necessity of automatic recovery systems in cloud
environments, setting a new direction for future research.</div><div><a href='http://arxiv.org/abs/2402.02938v1'>2402.02938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.19472v1")'>Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid
  Progress</div>
<div id='2402.19472v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:58:26Z</div><div>Authors: Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, Samuel Albanie</div><div style='padding-top: 10px; width: 80ex'>Standardized benchmarks drive progress in machine learning. However, with
repeated testing, the risk of overfitting grows as algorithms over-exploit
benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by
compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As
exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet,
containing (for now) 1.69M and 1.98M test samples, respectively. While reducing
overfitting, lifelong benchmarks introduce a key challenge: the high cost of
evaluating a growing number of models across an ever-expanding sample set. To
address this challenge, we also introduce an efficient evaluation framework:
Sort \&amp; Search (S&amp;S), which reuses previously evaluated models by leveraging
dynamic programming algorithms to selectively rank and sub-select test samples,
enabling cost-effective lifelong benchmarking. Extensive empirical evaluations
across 31,000 models demonstrate that S&amp;S achieves highly-efficient approximate
accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours
(1000x reduction) on a single A100 GPU, with low approximation error. As such,
lifelong benchmarks offer a robust, practical solution to the "benchmark
exhaustion" problem.</div><div><a href='http://arxiv.org/abs/2402.19472v1'>2402.19472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12166v2")'>The Power of Few: Accelerating and Enhancing Data Reweighting with
  Coreset Selection</div>
<div id='2403.12166v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T18:30:22Z</div><div>Authors: Mohammad Jafari, Yimeng Zhang, Yihua Zhang, Sijia Liu</div><div style='padding-top: 10px; width: 80ex'>As machine learning tasks continue to evolve, the trend has been to gather
larger datasets and train increasingly larger models. While this has led to
advancements in accuracy, it has also escalated computational costs to
unsustainable levels. Addressing this, our work aims to strike a delicate
balance between computational efficiency and model accuracy, a persisting
challenge in the field. We introduce a novel method that employs core subset
selection for reweighting, effectively optimizing both computational time and
model performance. By focusing on a strategically selected coreset, our
approach offers a robust representation, as it efficiently minimizes the
influence of outliers. The re-calibrated weights are then mapped back to and
propagated across the entire dataset. Our experimental results substantiate the
effectiveness of this approach, underscoring its potential as a scalable and
precise solution for model training.</div><div><a href='http://arxiv.org/abs/2403.12166v2'>2403.12166v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02196v2")'>Sample-Efficient Clustering and Conquer Procedures for Parallel
  Large-Scale Ranking and Selection</div>
<div id='2402.02196v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T15:56:03Z</div><div>Authors: Zishi Zhang, Yijie Peng</div><div style='padding-top: 10px; width: 80ex'>We propose novel "clustering and conquer" procedures for the parallel
large-scale ranking and selection (R&amp;S) problem, which leverage correlation
information for clustering to break the bottleneck of sample efficiency. In
parallel computing environments, correlation-based clustering can achieve an
$\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal
reduction rate theoretically attainable. Our proposed framework is versatile,
allowing for seamless integration of various prevalent R&amp;S methods under both
fixed-budget and fixed-precision paradigms. It can achieve improvements without
the necessity of highly accurate correlation estimation and precise clustering.
In large-scale AI applications such as neural architecture search, a
screening-free version of our procedure surprisingly surpasses fully-sequential
benchmarks in terms of sample efficiency. This suggests that leveraging
valuable structural information, such as correlation, is a viable path to
bypassing the traditional need for screening via pairwise comparison--a step
previously deemed essential for high sample efficiency but problematic for
parallelization. Additionally, we propose a parallel few-shot clustering
algorithm tailored for large-scale problems.</div><div><a href='http://arxiv.org/abs/2402.02196v2'>2402.02196v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13831v1")'>MLXP: A framework for conducting replicable Machine Learning eXperiments
  in Python</div>
<div id='2402.13831v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T14:22:20Z</div><div>Authors: Michael Arbel, Alexandre Zouaoui</div><div style='padding-top: 10px; width: 80ex'>Replicability in machine learning (ML) research is increasingly concerning
due to the utilization of complex non-deterministic algorithms and the
dependence on numerous hyper-parameter choices, such as model architecture and
training datasets. Ensuring reproducible and replicable results is crucial for
advancing the field, yet often requires significant technical effort to conduct
systematic and well-organized experiments that yield robust conclusions.
Several tools have been developed to facilitate experiment management and
enhance reproducibility; however, they often introduce complexity that hinders
adoption within the research community, despite being well-handled in
industrial settings. To address the challenge of low adoption, we propose MLXP,
an open-source, simple, and lightweight experiment management tool based on
Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the
experimental process with minimal practitioner overhead while ensuring a high
level of reproducibility.</div><div><a href='http://arxiv.org/abs/2402.13831v1'>2402.13831v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17345v2")'>Reproducibility, energy efficiency and performance of pseudorandom
  number generators in machine learning: a comparative study of python, numpy,
  tensorflow, and pytorch implementations</div>
<div id='2401.17345v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T15:44:14Z</div><div>Authors: Benjamin Antunes, David R. C Hill</div><div style='padding-top: 10px; width: 80ex'>Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine
learning technologies because they are interesting for numerous methods. The
field of machine learning holds the potential for substantial advancements
across various domains, as exemplified by recent breakthroughs in Large
Language Models (LLMs). However, despite the growing interest, persistent
concerns include issues related to reproducibility and energy consumption.
Reproducibility is crucial for robust scientific inquiry and explainability,
while energy efficiency underscores the imperative to conserve finite global
resources. This study delves into the investigation of whether the leading
Pseudo-Random Number Generators (PRNGs) employed in machine learning languages,
libraries, and frameworks uphold statistical quality and numerical
reproducibility when compared to the original C implementation of the
respective PRNG algorithms. Additionally, we aim to evaluate the time
efficiency and energy consumption of various implementations. Our experiments
encompass Python, NumPy, TensorFlow, and PyTorch, utilizing the Mersenne
Twister, PCG, and Philox algorithms. Remarkably, we verified that the temporal
performance of machine learning technologies closely aligns with that of
C-based implementations, with instances of achieving even superior
performances. On the other hand, it is noteworthy that ML technologies consumed
only 10% more energy than their C-implementation counterparts. However, while
statistical quality was found to be comparable, achieving numerical
reproducibility across different platforms for identical seeds and algorithms
was not achieved.</div><div><a href='http://arxiv.org/abs/2401.17345v2'>2401.17345v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04669v1")'>Transfer-Learning-Based Autotuning Using Gaussian Copula</div>
<div id='2401.04669v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T16:52:57Z</div><div>Authors: Thomas Randall, Jaehoon Koo, Brice Videau, Michael Kruse, Xingfu Wu, Paul Hovland, Mary Hall, Rong Ge, Prasanna Balaprakash</div><div style='padding-top: 10px; width: 80ex'>As diverse high-performance computing (HPC) systems are built, many
opportunities arise for applications to solve larger problems than ever before.
Given the significantly increased complexity of these HPC systems and
application tuning, empirical performance tuning, such as autotuning, has
emerged as a promising approach in recent years. Despite its effectiveness,
autotuning is often a computationally expensive approach. Transfer learning
(TL)-based autotuning seeks to address this issue by leveraging the data from
prior tuning. Current TL methods for autotuning spend significant time modeling
the relationship between parameter configurations and performance, which is
ineffective for few-shot (that is, few empirical evaluations) tuning on new
tasks. We introduce the first generative TL-based autotuning approach based on
the Gaussian copula (GC) to model the high-performing regions of the search
space from prior data and then generate high-performing configurations for new
tasks. This allows a sampling-based approach that maximizes few-shot
performance and provides the first probabilistic estimation of the few-shot
budget for effective TL-based autotuning. We compare our generative TL approach
with state-of-the-art autotuning techniques on several benchmarks. We find that
the GC is capable of achieving 64.37% of peak few-shot performance in its first
evaluation. Furthermore, the GC model can determine a few-shot transfer budget
that yields up to 33.39$\times$ speedup, a dramatic improvement over the
20.58$\times$ speedup using prior techniques.</div><div><a href='http://arxiv.org/abs/2401.04669v1'>2401.04669v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13091v1")'>JaxUED: A simple and useable UED library in Jax</div>
<div id='2403.13091v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T18:40:50Z</div><div>Authors: Samuel Coward, Michael Beukman, Jakob Foerster</div><div style='padding-top: 10px; width: 80ex'>We present JaxUED, an open-source library providing minimal dependency
implementations of modern Unsupervised Environment Design (UED) algorithms in
Jax. JaxUED leverages hardware acceleration to obtain on the order of 100x
speedups compared to prior, CPU-based implementations. Inspired by CleanRL, we
provide fast, clear, understandable, and easily modifiable implementations,
with the aim of accelerating research into UED. This paper describes our
library and contains baseline results. Code can be found at
https://github.com/DramaCow/jaxued.</div><div><a href='http://arxiv.org/abs/2403.13091v1'>2403.13091v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10874v1")'>Applications of flow models to the generation of correlated lattice QCD
  ensembles</div>
<div id='2401.10874v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T18:33:52Z</div><div>Authors: Ryan Abbott, Aleksandar Botev, Denis Boyda, Daniel C. Hackett, Gurtej Kanwar, Sébastien Racanière, Danilo J. Rezende, Fernando Romero-López, Phiala E. Shanahan, Julian M. Urban</div><div style='padding-top: 10px; width: 80ex'>Machine-learned normalizing flows can be used in the context of lattice
quantum field theory to generate statistically correlated ensembles of lattice
gauge fields at different action parameters. This work demonstrates how these
correlations can be exploited for variance reduction in the computation of
observables. Three different proof-of-concept applications are demonstrated
using a novel residual flow architecture: continuum limits of gauge theories,
the mass dependence of QCD observables, and hadronic matrix elements based on
the Feynman-Hellmann approach. In all three cases, it is shown that statistical
uncertainties are significantly reduced when machine-learned flows are
incorporated as compared with the same calculations performed with uncorrelated
ensembles or direct reweighting.</div><div><a href='http://arxiv.org/abs/2401.10874v1'>2401.10874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00828v3")'>Multi-Lattice Sampling of Quantum Field Theories via Neural
  Operator-based Flows</div>
<div id='2401.00828v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T17:56:24Z</div><div>Authors: Bálint Máté, François Fleuret</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of sampling discrete field configurations $\phi$ from
the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the
lattice-discretization of the continuous Euclidean action $\mathcal S$ of some
quantum field theory. Since such densities arise as the approximation of the
underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal
S[\phi(x)]}$, we frame the task as an instance of operator learning. In
particular, we propose to approximate a time-dependent operator $\mathcal V_t$
whose time integral provides a mapping between the functional distributions of
the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal
S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal
Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the
operator $\mathcal V_t$ can be discretized to a finite dimensional,
time-dependent vector field $V_t$ which in turn induces a continuous
normalizing flow between finite dimensional distributions over the chosen
lattice. This flow can then be trained to be a diffeormorphism between the
discretized free and target theories $[d\phi] Z_0^{-1} e^{-S_{0}[\phi]}$,
$[d\phi] Z^{-1}e^{-S[\phi]}$. We run experiments on the $\phi^4$-theory to
explore to what extent such operator-based flow architectures generalize to
lattice sizes they were not trained on and show that pretraining on smaller
lattices can lead to speedup over training only a target lattice size.</div><div><a href='http://arxiv.org/abs/2401.00828v3'>2401.00828v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11705v1")'>Coarsening of chiral domains in itinerant electron magnets: A machine
  learning force field approach</div>
<div id='2403.11705v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T12:07:46Z</div><div>Authors: Yunhao Fan, Sheng Zhang, Gia-Wei Chern</div><div style='padding-top: 10px; width: 80ex'>Frustrated itinerant magnets often exhibit complex noncollinear or
noncoplanar magnetic orders which support topological electronic structures. A
canonical example is the anomalous quantum Hall state with a chiral spin order
stabilized by electron-spin interactions on a triangular lattice. While a
long-range magnetic order cannot survive thermal fluctuations in two
dimensions, the chiral order which results from the breaking of a discrete
Ising symmetry persists even at finite temperatures. We present a scalable
machine learning (ML) framework to model the complex electron-mediated
spin-spin interactions that stabilize the chiral magnetic domains in a
triangular lattice. Large-scale dynamical simulations, enabled by the ML
force-field models, are performed to investigate the coarsening of chiral
domains after a thermal quench. While the chiral phase is described by a broken
$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral
domains increases linearly with time, in stark contrast to the expected
Allen-Cahn domain growth law for a non-conserved Ising order parameter field.
The linear growth of the chiral domains is attributed to the orientational
anisotropy of domain boundaries. Our work also demonstrates the promising
potential of ML models for large-scale spin dynamics of itinerant magnets.</div><div><a href='http://arxiv.org/abs/2403.11705v1'>2403.11705v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.15195v1")'>FSD-Inference: Fully Serverless Distributed Inference with Scalable
  Cloud Communication</div>
<div id='2403.15195v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T13:31:24Z</div><div>Authors: Joe Oakley, Hakan Ferhatosmanoglu</div><div style='padding-top: 10px; width: 80ex'>Serverless computing offers attractive scalability, elasticity and
cost-effectiveness. However, constraints on memory, CPU and function runtime
have hindered its adoption for data-intensive applications and machine learning
(ML) workloads. Traditional 'server-ful' platforms enable distributed
computation via fast networks and well-established inter-process communication
(IPC) mechanisms such as MPI and shared memory. In the absence of such
solutions in the serverless domain, parallel computation with significant IPC
requirements is challenging. We present FSD-Inference, the first fully
serverless and highly scalable system for distributed ML inference. We explore
potential communication channels, in conjunction with Function-as-a-Service
(FaaS) compute, to design a state-of-the-art solution for distributed ML within
the context of serverless data-intensive computing. We introduce novel fully
serverless communication schemes for ML inference workloads, leveraging both
cloud-based publish-subscribe/queueing and object storage offerings. We
demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC
with comparable performance to object storage, while offering significantly
reduced cost at high parallelism levels. We conduct in-depth experiments on
benchmark DNNs of various sizes. The results show that when compared to
server-based alternatives, FSD-Inference is significantly more cost-effective
and scalable, and can even achieve competitive performance against optimized
HPC solutions. Experiments also confirm that our serverless solution can handle
large distributed workloads and leverage high degrees of FaaS parallelism.</div><div><a href='http://arxiv.org/abs/2403.15195v1'>2403.15195v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08859v2")'>Shabari: Delayed Decision-Making for Faster and Efficient Serverless
  Functions</div>
<div id='2401.08859v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T22:20:36Z</div><div>Authors: Prasoon Sinha, Kostis Kaffes, Neeraja J. Yadwadkar</div><div style='padding-top: 10px; width: 80ex'>Serverless computing relieves developers from the burden of resource
management, thus providing ease-of-use to the users and the opportunity to
optimize resource utilization for the providers. However, today's serverless
systems lack performance guarantees for function invocations, thus limiting
support for performance-critical applications: we observed severe performance
variability (up to 6x). Providers lack visibility into user functions and hence
find it challenging to right-size them: we observed heavy resource
underutilization (up to 80%). To understand the causes behind the performance
variability and underutilization, we conducted a measurement study of commonly
deployed serverless functions and learned that the function performance and
resource utilization depend crucially on function semantics and inputs. Our key
insight is to delay making resource allocation decisions until after the
function inputs are available. We introduce Shabari, a resource management
framework for serverless systems that makes decisions as late as possible to
right-size each invocation to meet functions' performance objectives (SLOs) and
improve resource utilization. Shabari uses an online learning agent to
right-size each function invocation based on the features of the function input
and makes cold-start-aware scheduling decisions. For a range of serverless
functions and inputs, Shabari reduces SLO violations by 11-73% while not
wasting any vCPUs and reducing wasted memory by 64-94% in the median case,
compared to state-of-the-art systems, including Aquatope, Parrotfish, and
Cypress.</div><div><a href='http://arxiv.org/abs/2401.08859v2'>2401.08859v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02310v1")'>Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</div>
<div id='2403.02310v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T18:47:08Z</div><div>Authors: Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, Ramachandran Ramjee</div><div style='padding-top: 10px; width: 80ex'>Each LLM serving request goes through two phases. The first is prefill which
processes the entire input prompt to produce one output token and the second is
decode which generates the rest of output tokens, one-at-a-time. Prefill
iterations have high latency but saturate GPU compute due to parallel
processing of the input prompt. In contrast, decode iterations have low latency
but also low compute utilization because a decode iteration processes only a
single token per request. This makes batching highly effective for decodes and
consequently for overall throughput. However, batching multiple requests leads
to an interleaving of prefill and decode iterations which makes it challenging
to achieve both high throughput and low latency.
  We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by
the techniques we originally proposed for optimizing throughput in Sarathi.
Sarathi-Serve leverages chunked-prefills from Sarathi to create stall-free
schedules that can add new requests in a batch without pausing ongoing decodes.
Stall-free scheduling unlocks the opportunity to improve throughput with large
batch sizes while minimizing the effect of batching on latency. Our evaluation
shows that Sarathi-Serve improves serving throughput within desired latency
SLOs of Mistral-7B by up to 2.6x on a single A100 GPU and up to 6.9x for
Falcon-180B on 8 A100 GPUs over Orca and vLLM.</div><div><a href='http://arxiv.org/abs/2403.02310v1'>2403.02310v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14361v1")'>MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE
  Serving</div>
<div id='2401.14361v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:07:50Z</div><div>Authors: Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh Marina</div><div style='padding-top: 10px; width: 80ex'>This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE)
serving system that realizes activation-aware expert offloading. MoE-Infinity
features sequence-level expert activation tracing, a new approach adept at
identifying sparse activations and capturing the temporal locality of MoE
inference. By analyzing these traces, MoE-Infinity performs novel
activation-aware expert prefetching and caching, substantially reducing the
latency overheads usually associated with offloading experts for improved cost
performance. Extensive experiments in a cluster show that MoE-Infinity
outperforms numerous existing systems and approaches, reducing latency by 4 -
20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's
source code is publicly available at https://github.com/TorchMoE/MoE-Infinity</div><div><a href='http://arxiv.org/abs/2401.14361v1'>2401.14361v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10857v1")'>JetTrain: IDE-Native Machine Learning Experiments</div>
<div id='2402.10857v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T17:53:08Z</div><div>Authors: Artem Trofimov, Mikhail Kostyukov, Sergei Ugdyzhekov, Natalia Ponomareva, Igor Naumov, Maksim Melekhovets</div><div style='padding-top: 10px; width: 80ex'>Integrated development environments (IDEs) are prevalent code-writing and
debugging tools. However, they have yet to be widely adopted for launching
machine learning (ML) experiments. This work aims to fill this gap by
introducing JetTrain, an IDE-integrated tool that delegates specific tasks from
an IDE to remote computational resources. A user can write and debug code
locally and then seamlessly run it remotely using on-demand hardware. We argue
that this approach can lower the entry barrier for ML training problems and
increase experiment throughput.</div><div><a href='http://arxiv.org/abs/2402.10857v1'>2402.10857v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13839v1")'>depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning
  Researchers</div>
<div id='2403.13839v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T16:17:14Z</div><div>Authors: Kaichao You, Runsheng Bai, Meng Cao, Jianmin Wang, Ion Stoica, Mingsheng Long</div><div style='padding-top: 10px; width: 80ex'>PyTorch \texttt{2.x} introduces a compiler designed to accelerate deep
learning programs. However, for machine learning researchers, adapting to the
PyTorch compiler to full potential can be challenging. The compiler operates at
the Python bytecode level, making it appear as an opaque box. To address this,
we introduce \texttt{depyf}, a tool designed to demystify the inner workings of
the PyTorch compiler. \texttt{depyf} decompiles bytecode generated by PyTorch
back into equivalent source code, and establishes connections between in-memory
code objects and their on-disk source code counterparts. This feature enables
users to step through the source code line by line using debuggers, thus
enhancing their understanding of the underlying processes. Notably,
\texttt{depyf} is non-intrusive and user-friendly, primarily relying on two
convenient context managers for its core functionality. The project is
\href{https://github.com/thuml/depyf}{ openly available} and is recognized as a
\href{https://pytorch.org/ecosystem/}{PyTorch ecosystem project}.</div><div><a href='http://arxiv.org/abs/2403.13839v1'>2403.13839v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10571v1")'>JaxDecompiler: Redefining Gradient-Informed Software Design</div>
<div id='2403.10571v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T20:32:31Z</div><div>Authors: Pierrick Pochelu</div><div style='padding-top: 10px; width: 80ex'>Among numerical libraries capable of computing gradient descent optimization,
JAX stands out by offering more features, accelerated by an intermediate
representation known as Jaxpr language. However, editing the Jaxpr code is not
directly possible. This article introduces JaxDecompiler, a tool that
transforms any JAX function into an editable Python code, especially useful for
editing the JAX function generated by the gradient function. JaxDecompiler
simplifies the processes of reverse engineering, understanding, customizing,
and interoperability of software developed by JAX. We highlight its
capabilities, emphasize its practical applications especially in deep learning
and more generally gradient-informed software, and demonstrate that the
decompiled code speed performance is similar to the original.</div><div><a href='http://arxiv.org/abs/2403.10571v1'>2403.10571v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19155v1")'>Beyond Language Models: Byte Models are Digital World Simulators</div>
<div id='2402.19155v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T13:38:07Z</div><div>Authors: Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun</div><div style='padding-top: 10px; width: 80ex'>Traditional deep learning often overlooks bytes, the basic units of the
digital world, where all forms of information and operations are encoded and
manipulated in binary format. Inspired by the success of next token prediction
in natural language processing, we introduce bGPT, a model with next byte
prediction to simulate the digital world. bGPT matches specialized models in
performance across various modalities, including text, audio, and images, and
offers new possibilities for predicting, simulating, and diagnosing algorithm
or hardware behaviour. It has almost flawlessly replicated the process of
converting symbolic music data, achieving a low error rate of 0.0011 bits per
byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates
exceptional capabilities in simulating CPU behaviour, with an accuracy
exceeding 99.99% in executing various operations. Leveraging next byte
prediction, models like bGPT can directly learn from vast binary data,
effectively simulating the intricate patterns of the digital world.</div><div><a href='http://arxiv.org/abs/2402.19155v1'>2402.19155v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03183v1")'>Predicting Configuration Performance in Multiple Environments with
  Sequential Meta-learning</div>
<div id='2402.03183v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:47:13Z</div><div>Authors: Jingzhi Gong, Tao Chen</div><div style='padding-top: 10px; width: 80ex'>Learning and predicting the performance of given software configurations are
of high importance to many software engineering activities. While configurable
software systems will almost certainly face diverse running environments (e.g.,
version, hardware, and workload), current work often either builds performance
models under a single environment or fails to properly handle data from diverse
settings, hence restricting their accuracy for new environments. In this paper,
we target configuration performance learning under multiple environments. We do
so by designing SeMPL - a meta-learning framework that learns the common
understanding from configurations measured in distinct (meta) environments and
generalizes them to the unforeseen, target environment. What makes it unique is
that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train
the meta environments in parallel, we train them sequentially, one at a time.
The order of training naturally allows discriminating the contributions among
meta environments in the meta-model built, which fits better with the
characteristic of configuration data that is known to dramatically differ
between different environments. Through comparing with 15 state-of-the-art
models under nine systems, our extensive experimental results demonstrate that
SeMPL performs considerably better on 89% of the systems with up to 99%
accuracy improvement, while being data-efficient, leading to a maximum of 3.86x
speedup. All code and data can be found at our repository:
https://github.com/ideas-labo/SeMPL.</div><div><a href='http://arxiv.org/abs/2402.03183v1'>2402.03183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.04880v2")'>Combining Cloud and Mobile Computing for Machine Learning</div>
<div id='2402.04880v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T06:14:22Z</div><div>Authors: Ruiqi Xu, Tianchi Zhang</div><div style='padding-top: 10px; width: 80ex'>Although the computing power of mobile devices is increasing, machine
learning models are also growing in size. This trend creates problems for
mobile devices due to limitations like their memory capacity and battery life.
While many services, like ChatGPT and Midjourney, run all the inferences in the
cloud, we believe a flexible and fine-grained task distribution is more
desirable. In this work, we consider model segmentation as a solution to
improving the user experience, dividing the computation between mobile devices
and the cloud in a way that offloads the compute-heavy portion of the model
while minimizing the data transfer required. We show that the division not only
reduces the wait time for users but can also be fine-tuned to optimize the
workloads of the cloud. To achieve that, we design a scheduler that collects
information about network quality, client device capability, and job
requirements, making decisions to achieve consistent performance across a range
of devices while reducing the work the cloud needs to perform.</div><div><a href='http://arxiv.org/abs/2402.04880v2'>2402.04880v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11285v1")'>Fair Resource Allocation in Virtualized O-RAN Platforms</div>
<div id='2402.11285v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T13:57:20Z</div><div>Authors: Fatih Aslan, George Iosifidis, Jose A. Ayala-Romero, Andres Garcia-Saavedra, Xavier Costa-Perez</div><div style='padding-top: 10px; width: 80ex'>O-RAN systems and their deployment in virtualized general-purpose computing
platforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented
performance gains. However, these architectures raise new implementation
challenges and threaten to worsen the already-high energy consumption of mobile
networks. This paper presents first a series of experiments which assess the
O-Cloud's energy costs and their dependency on the servers' hardware, capacity
and data traffic properties which, typically, change over time. Next, it
proposes a compute policy for assigning the base station data loads to O-Cloud
servers in an energy-efficient fashion; and a radio policy that determines at
near-real-time the minimum transmission block size for each user so as to avoid
unnecessary energy costs. The policies balance energy savings with performance,
and ensure that both of them are dispersed fairly across the servers and users,
respectively. To cater for the unknown and time-varying parameters affecting
the policies, we develop a novel online learning framework with fairness
guarantees that apply to the entire operation horizon of the system (long-term
fairness). The policies are evaluated using trace-driven simulations and are
fully implemented in an O-RAN compatible system where we measure the energy
costs and throughput in realistic scenarios.</div><div><a href='http://arxiv.org/abs/2402.11285v1'>2402.11285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04553v1")'>Improvements &amp; Evaluations on the MLCommons CloudMask Benchmark</div>
<div id='2403.04553v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:48:48Z</div><div>Authors: Varshitha Chennamsetti, Laiba Mehnaz, Dan Zhao, Banani Ghosh, Sergey V. Samsonau</div><div style='padding-top: 10px; width: 80ex'>In this paper, we report the performance benchmarking results of deep
learning models on MLCommons' Science cloud-masking benchmark using a
high-performance computing cluster at New York University (NYU): NYU Greene.
MLCommons is a consortium that develops and maintains several scientific
benchmarks that can benefit from developments in AI. We provide a description
of the cloud-masking benchmark task, updated code, and the best model for this
benchmark when using our selected hyperparameter settings. Our benchmarking
results include the highest accuracy achieved on the NYU system as well as the
average time taken for both training and inference on the benchmark across
several runs/seeds. Our code can be found on GitHub. MLCommons team has been
kept informed about our progress and may use the developed code for their
future work.</div><div><a href='http://arxiv.org/abs/2403.04553v1'>2403.04553v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04757v1")'>How predictable is language model benchmark performance?</div>
<div id='2401.04757v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T17:34:30Z</div><div>Authors: David Owen</div><div style='padding-top: 10px; width: 80ex'>We investigate large language model performance across five orders of
magnitude of compute scaling in eleven recent model architectures. We show that
average benchmark performance, aggregating over many individual tasks and
evaluations as in the commonly-used BIG-Bench dataset, is decently predictable
as a function of training compute scale. Specifically, when extrapolating
BIG-Bench Hard performance across one order of magnitude in compute, we observe
average absolute errors of 6 percentage points (pp). By contrast, extrapolation
for individual BIG-Bench tasks across an order of magnitude in compute yields
higher average errors of 18pp. Nonetheless, individual task performance remains
significantly more predictable than chance. Overall, our work suggests compute
scaling provides a promising basis to forecast AI capabilities in diverse
benchmarks, though predicting performance in specific tasks poses challenges.</div><div><a href='http://arxiv.org/abs/2401.04757v1'>2401.04757v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16919v1")'>Personalized Federated Instruction Tuning via Neural Architecture Search</div>
<div id='2402.16919v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T06:29:05Z</div><div>Authors: Pengyu Zhang, Yingbo Zhou, Ming Hu, Junxian Feng, Jiawen Weng, Mingsong Chen</div><div style='padding-top: 10px; width: 80ex'>Federated Instruction Tuning (FIT) has shown the ability to achieve
collaborative model instruction tuning among massive data owners without
sharing private data. However, it still faces two key challenges, i.e., data
and resource heterogeneity. Due to the varying data distribution and
preferences among data owners, FIT cannot adapt to the personalized data of
individual owners. Moreover, clients with superior computational abilities are
constrained since they need to maintain the same fine-tuning architecture as
the weaker clients. To address these issues, we propose a novel Personalized
Federated Instruction Tuning (PerFIT) framework based on architecture search.
Specifically, PerFIT allows each client to search for a personalized
architecture by expanding the trainable parameter space of the global model
followed by pruning the parameters to the original state. This procedure allows
personalized instruction fine-tuning within expanded parameter spaces,
concurrently preserving the same number of trainable parameters. Furthermore,
to release the abilities of heterogeneous computational resources and enhance
the performance of personalization on local data, we exploit personalized
parameter-wise aggregation. The evaluation with multiple LLMs non-IID scenarios
demonstrates that compared to the state-of-the-art FIT methods, our approach
can achieve up to a 23% decrease in perplexity.</div><div><a href='http://arxiv.org/abs/2402.16919v1'>2402.16919v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02318v1")'>Diversity Measurement and Subset Selection for Instruction Tuning
  Datasets</div>
<div id='2402.02318v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T02:09:43Z</div><div>Authors: Peiqi Wang, Yikang Shen, Zhen Guo, Matthew Stallone, Yoon Kim, Polina Golland, Rameswar Panda</div><div style='padding-top: 10px; width: 80ex'>We aim to select data subsets for the fine-tuning of large language models to
more effectively follow instructions. Prior work has emphasized the importance
of diversity in dataset curation but relied on heuristics such as the number of
tasks. In this paper, we use determinantal point processes to capture the
diversity and quality of instruction tuning datasets for subset selection. We
propose to measure dataset diversity with log determinant distance that is the
distance between the dataset of interest and a maximally diverse reference
dataset. Our experiments demonstrate that the proposed diversity measure in the
normalized weight gradient space is correlated with downstream
instruction-following performance. Consequently, it can be used to inform when
data selection is the most helpful and to analyze dataset curation strategies.
We demonstrate the utility of our approach on various instruction tuning
datasets.</div><div><a href='http://arxiv.org/abs/2402.02318v1'>2402.02318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09615v2")'>API Pack: A Massive Multilingual Dataset for API Call Generation</div>
<div id='2402.09615v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T23:09:15Z</div><div>Authors: Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda</div><div style='padding-top: 10px; width: 80ex'>We introduce API Pack, a multilingual dataset featuring over one million
instruction-API call pairs aimed at advancing large language models' API call
generation capabilities. Through experiments, we demonstrate API Pack's
efficacy in enhancing models for this specialized task while maintaining their
overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000
Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4
respectively in generating unseen API calls. Scaling to 100k examples improves
generalization to new APIs not seen during training. In addition, cross-lingual
API call generation is achieved without needing extensive data per language.
The dataset, fine-tuned models, and overall code base are publicly available at
https://github.com/zguo0525/API-Pack.</div><div><a href='http://arxiv.org/abs/2402.09615v2'>2402.09615v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10463v2")'>Critical Data Size of Language Models from a Grokking Perspective</div>
<div id='2401.10463v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T03:24:36Z</div><div>Authors: Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin</div><div style='padding-top: 10px; width: 80ex'>We explore the critical data size in language models, a threshold that marks
a fundamental shift from quick memorization to slow generalization. We
formalize the phase transition under the grokking configuration into the Data
Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus
regimes in language models training dynamics. We develop a grokking
configuration to reproduce grokking on simplistic language models stably by
rescaling initialization and weight decay. We show that generalization occurs
only when language models reach a critical size. We analyze grokking across
sample-wise and model-wise, verifying the proposed data efficiency hypothesis.
Our experiments reveal smoother phase transitions occurring at the critical
dataset size for language datasets. As the model size increases, this critical
point also becomes larger, indicating that larger models require more data. Our
results deepen the understanding of language model training, offering a novel
perspective on the role of data in the learning mechanism of language models.</div><div><a href='http://arxiv.org/abs/2401.10463v2'>2401.10463v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08819v1")'>Thermometer: Towards Universal Calibration for Large Language Models</div>
<div id='2403.08819v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:13:48Z</div><div>Authors: Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, Soumya Ghosh</div><div style='padding-top: 10px; width: 80ex'>We consider the issue of calibration in large language models (LLM). Recent
studies have found that common interventions such as instruction tuning often
result in poorly calibrated LLMs. Although calibration is well-explored in
traditional applications, calibrating LLMs is uniquely challenging. These
challenges stem as much from the severe computational requirements of LLMs as
from their versatility, which allows them to be applied to diverse tasks.
Addressing these challenges, we propose THERMOMETER, a calibration approach
tailored to LLMs. THERMOMETER learns an auxiliary model, given data from
multiple tasks, for calibrating a LLM. It is computationally efficient,
preserves the accuracy of the LLM, and produces better-calibrated responses for
new tasks. Extensive empirical evaluations across various benchmarks
demonstrate the effectiveness of the proposed method.</div><div><a href='http://arxiv.org/abs/2403.08819v1'>2403.08819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01881v2")'>Large Language Model Agent for Hyper-Parameter Optimization</div>
<div id='2402.01881v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:12:05Z</div><div>Authors: Siyi Liu, Chen Gao, Yong Li</div><div style='padding-top: 10px; width: 80ex'>Hyperparameter optimization is critical in modern machine learning, requiring
expert knowledge, numerous trials, and high computational and human resources.
Despite the advancements in Automated Machine Learning (AutoML), challenges in
terms of trial efficiency, setup complexity, and interoperability still
persist. To address these issues, we introduce a novel paradigm leveraging
Large Language Models (LLMs) to automate hyperparameter optimization across
diverse machine learning tasks, which is named AgentHPO (short for LLM
Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the
task information autonomously, conducts experiments with specific
hyperparameters (HPs), and iteratively optimizes them based on historical
trials. This human-like optimization process largely reduces the number of
required trials, simplifies the setup process, and enhances interpretability
and user trust, compared to traditional AutoML methods. Extensive empirical
experiments conducted on 12 representative machine-learning tasks indicate that
AgentHPO not only matches but also often surpasses the best human trials in
terms of performance while simultaneously providing explainable results.
Further analysis sheds light on the strategies employed by the LLM in
optimizing these tasks, highlighting its effectiveness and adaptability in
various scenarios.</div><div><a href='http://arxiv.org/abs/2402.01881v2'>2402.01881v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18381v1")'>Large Language Models As Evolution Strategies</div>
<div id='2402.18381v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:02:17Z</div><div>Authors: Robert Tjarko Lange, Yingtao Tian, Yujin Tang</div><div style='padding-top: 10px; width: 80ex'>Large Transformer models are capable of implementing a plethora of so-called
in-context learning algorithms. These include gradient descent, classification,
sequence completion, transformation, and improvement. In this work, we
investigate whether large language models (LLMs), which never explicitly
encountered the task of black-box optimization, are in principle capable of
implementing evolutionary optimization algorithms. While previous works have
solely focused on language-based task specification, we move forward and focus
on the zero-shot application of LLMs to black-box optimization. We introduce a
novel prompting strategy, consisting of least-to-most sorting of discretized
population members and querying the LLM to propose an improvement to the mean
statistic, i.e. perform a type of black-box recombination operation.
Empirically, we find that our setup allows the user to obtain an LLM-based
evolution strategy, which we call `EvoLLM', that robustly outperforms baseline
algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB
functions as well as small neuroevolution tasks. Hence, LLMs can act as
`plug-in' in-context recombination operators. We provide several comparative
studies of the LLM's model size, prompt strategy, and context construction.
Finally, we show that one can flexibly improve EvoLLM's performance by
providing teacher algorithm information via instruction fine-tuning on
previously collected teacher optimization trajectories.</div><div><a href='http://arxiv.org/abs/2402.18381v1'>2402.18381v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10510v1")'>A match made in consistency heaven: when large language models meet
  evolutionary algorithms</div>
<div id='2401.10510v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T05:58:30Z</div><div>Authors: Wang Chao, Jiaxuan Zhao, Licheng Jiao, Lingling Li, Fang Liu, Shuyuan Yang</div><div style='padding-top: 10px; width: 80ex'>Pre-trained large language models (LLMs) have powerful capabilities for
generating creative natural text. Evolutionary algorithms (EAs) can discover
diverse solutions to complex real-world problems. Motivated by the common
collective and directionality of text sequence generation and evolution, this
paper illustrates the strong consistency of LLMs and EAs, which includes
multiple one-to-one key characteristics: token embedding and genotype-phenotype
mapping, position encoding and fitness shaping, position embedding and
selection, attention and crossover, feed-forward neural network and mutation,
model training and parameter update, and multi-task learning and
multi-objective optimization. Based on this consistency perspective, existing
coupling studies are analyzed, including evolutionary fine-tuning and
LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap
for future research in coupling LLMs and EAs, while highlighting key challenges
along the way. The consistency not only reveals the evolution mechanism behind
LLMs but also facilitates the development of evolved artificial agents that
approach or surpass biological organisms.</div><div><a href='http://arxiv.org/abs/2401.10510v1'>2401.10510v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12061v1")'>All Language Models Large and Small</div>
<div id='2402.12061v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T11:28:20Z</div><div>Authors: Zhixun Chen, Yali Du, David Mguni</div><div style='padding-top: 10px; width: 80ex'>Many leading language models (LMs) use high-intensity computational resources
both during training and execution. This poses the challenge of lowering
resource costs for deployment and faster execution of decision-making tasks
among others. We introduce a novel plug-and-play LM framework named Language
Optimising Network Distribution (LONDI) framework. LONDI learns to selectively
employ large LMs only where complex decision-making and reasoning are required
while using low-resource LMs everywhere else. LONDI consists of a system of two
(off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning
module that uses switching controls to quickly learn which system states to
call the LLM. We then introduce a variant of LONDI that maintains budget
constraints on LLM calls and hence its resource usage. Theoretically, we prove
LONDI learns the subset of system states to activate the LLM required to solve
the task. We then prove that LONDI converges to optimal solutions while also
preserving budgetary constraints on LLM calls almost surely enabling it to
solve various tasks while significantly lowering computational costs. We test
LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and
demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs
while reducing GPU usage by up to 30%.</div><div><a href='http://arxiv.org/abs/2402.12061v1'>2402.12061v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04642v1")'>Teaching Large Language Models to Reason with Reinforcement Learning</div>
<div id='2403.04642v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T16:36:29Z</div><div>Authors: Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a
dominant approach for aligning LLM outputs with human preferences. Inspired by
the success of RLHF, we study the performance of multiple algorithms that learn
from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}),
Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate
both sparse and dense rewards provided to the LLM both heuristically and via a
learned reward model. We additionally start from multiple model sizes and
initializations both with and without supervised fine-tuning (\textbf{SFT})
data. Overall, we find all algorithms perform comparably, with Expert Iteration
performing best in most cases. Surprisingly, we find the sample complexity of
Expert Iteration is similar to that of PPO, requiring at most on the order of
$10^6$ samples to converge from a pretrained checkpoint. We investigate why
this is the case, concluding that during RL training models fail to explore
significantly beyond solutions already produced by SFT models. Additionally, we
discuss a trade off between maj@1 and pass@96 metric performance during SFT
training and how conversely RL training improves both simultaneously. We then
conclude by discussing the implications of our findings for RLHF and the future
role of RL in LLM fine-tuning.</div><div><a href='http://arxiv.org/abs/2403.04642v1'>2403.04642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07157v2")'>Natural Language Reinforcement Learning</div>
<div id='2402.07157v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T11:03:04Z</div><div>Authors: Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushik, Yali Du, Ying Wen, Jun Wang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning (RL) has shown remarkable abilities in learning
policies for decision-making tasks. However, RL is often hindered by issues
such as low sample efficiency, lack of interpretability, and sparse supervision
signals. To tackle these limitations, we take inspiration from the human
learning process and introduce Natural Language Reinforcement Learning (NLRL),
which innovatively combines RL principles with natural language representation.
Specifically, NLRL redefines RL concepts like task objectives, policy, value
function, Bellman equation, and policy iteration in natural language space. We
present how NLRL can be practically implemented with the latest advancements in
large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs
demonstrate the effectiveness, efficiency, and also interpretability of the
NLRL framework.</div><div><a href='http://arxiv.org/abs/2402.07157v2'>2402.07157v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01874v1")'>The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
  Learning and Large Language Models</div>
<div id='2402.01874v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:01:15Z</div><div>Authors: Moschoula Pternea, Prerna Singh, Abir Chakraborty, Yagna Oruganti, Mirco Milletari, Sayli Bapat, Kebei Jiang</div><div style='padding-top: 10px; width: 80ex'>In this work, we review research studies that combine Reinforcement Learning
(RL) and Large Language Models (LLMs), two areas that owe their momentum to the
development of deep neural networks. We propose a novel taxonomy of three main
classes based on the way that the two model types interact with each other. The
first class, RL4LLM, includes studies where RL is leveraged to improve the
performance of LLMs on tasks related to Natural Language Processing. L4LLM is
divided into two sub-categories depending on whether RL is used to directly
fine-tune an existing LLM or to improve the prompt of the LLM. In the second
class, LLM4RL, an LLM assists the training of an RL model that performs a task
that is not inherently related to natural language. We further break down
LLM4RL based on the component of the RL training framework that the LLM assists
or replaces, namely reward shaping, goal generation, and policy function.
Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a
common planning framework without either of them contributing to training or
fine-tuning of the other. We further branch this class to distinguish between
studies with and without natural language feedback. We use this taxonomy to
explore the motivations behind the synergy of LLMs and RL and explain the
reasons for its success, while pinpointing potential shortcomings and areas
where further research is needed, as well as alternative methodologies that
serve the same goal.</div><div><a href='http://arxiv.org/abs/2402.01874v1'>2402.01874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07181v1")'>Reinforcement Learning from LLM Feedback to Counteract Goal
  Misgeneralization</div>
<div id='2401.07181v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T01:09:48Z</div><div>Authors: Houda Nait El Barj, Theophile Sautory</div><div style='padding-top: 10px; width: 80ex'>We introduce a method to address goal misgeneralization in reinforcement
learning (RL), leveraging Large Language Model (LLM) feedback during training.
Goal misgeneralization, a type of robustness failure in RL occurs when an agent
retains its capabilities out-of-distribution yet pursues a proxy rather than
the intended one. Our approach utilizes LLMs to analyze an RL agent's policies
during training and identify potential failure scenarios. The RL agent is then
deployed in these scenarios, and a reward model is learnt through the LLM
preferences and feedback. This LLM-informed reward model is used to further
train the RL agent on the original dataset. We apply our method to a maze
navigation task, and show marked improvements in goal generalization,
especially in cases where true and proxy goals are somewhat distinguishable and
behavioral biases are pronounced. This study demonstrates how the LLM, despite
its lack of task proficiency, can efficiently supervise RL agents, providing
scalable oversight and valuable insights for enhancing goal-directed learning
in RL through the use of LLMs.</div><div><a href='http://arxiv.org/abs/2401.07181v1'>2401.07181v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16181v1")'>How Can LLM Guide RL? A Value-Based Approach</div>
<div id='2402.16181v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T20:07:13Z</div><div>Authors: Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) has become the de facto standard practice for
sequential decision-making problems by improving future acting policies with
feedback. However, RL algorithms may require extensive trial-and-error
interactions to collect useful feedback for improvement. On the other hand,
recent developments in large language models (LLMs) have showcased impressive
capabilities in language understanding and generation, yet they fall short in
exploration and self-improvement capabilities for planning tasks, lacking the
ability to autonomously refine their responses based on feedback. Therefore, in
this paper, we study how the policy prior provided by the LLM can enhance the
sample efficiency of RL algorithms. Specifically, we develop an algorithm named
LINVIT that incorporates LLM guidance as a regularization factor in value-based
RL, leading to significant reductions in the amount of data needed for
learning, particularly when the difference between the ideal policy and the
LLM-informed policy is small, which suggests that the initial policy is close
to optimal, reducing the need for further exploration. Additionally, we present
a practical algorithm SLINVIT that simplifies the construction of the value
function and employs subgoals to reduce the search complexity. Our experiments
across three interactive environments ALFWorld, InterCode, and BlocksWorld
demonstrate that our method achieves state-of-the-art success rates and also
surpasses previous RL and LLM approaches in terms of sample efficiency. Our
code is available at https://github.com/agentification/Language-Integrated-VI.</div><div><a href='http://arxiv.org/abs/2402.16181v1'>2402.16181v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14589v1")'>ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for
  Contrastive Self-Training</div>
<div id='2403.14589v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:43:44Z</div><div>Authors: Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotations or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.</div><div><a href='http://arxiv.org/abs/2403.14589v1'>2403.14589v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05268v3")'>AUTOACT: Automatic Agent Learning from Scratch via Self-Planning</div>
<div id='2401.05268v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T16:57:24Z</div><div>Authors: Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>Language agents have achieved considerable performance on various complex
question-answering tasks. Despite the incessant exploration in this field,
existing language agent systems still struggle with costly, non-reproducible
data reliance and face the challenge of compelling a single model for multiple
functions. To this end, we introduce AutoAct, an automatic agent learning
framework that does not rely on large-scale annotated data and synthetic
trajectories from closed-source models (e.g., GPT-4). Given limited data with a
tool library, AutoAct first automatically synthesizes planning trajectories
without any assistance from humans or strong closed-source models. Then,
AutoAct leverages a division-of-labor strategy to automatically differentiate
based on the target task information and synthesized trajectories, producing a
sub-agent group to complete the task. We conduct comprehensive experiments with
different LLMs, which demonstrates that AutoAct yields better or parallel
performance compared to various strong baselines. Further analysis demonstrates
the effectiveness of the division-of-labor strategy, with the trajectory
quality generated by AutoAct significantly outperforming that of others. Code
will be available at https://github.com/zjunlp/AutoAct.</div><div><a href='http://arxiv.org/abs/2401.05268v3'>2401.05268v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02502v1")'>Trial and Error: Exploration-Based Trajectory Optimization for LLM
  Agents</div>
<div id='2403.02502v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T21:50:29Z</div><div>Authors: Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, Bill Yuchen Lin</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have become integral components in various
autonomous agent systems. In this study, we present an exploration-based
trajectory optimization approach, referred to as ETO. This learning method is
designed to enhance the performance of open LLM agents. Contrary to previous
studies that exclusively train on successful expert trajectories, our method
allows agents to learn from their exploration failures. This leads to improved
performance through an iterative optimization framework. During the exploration
phase, the agent interacts with the environment while completing given tasks,
gathering failure trajectories to create contrastive trajectory pairs. In the
subsequent training phase, the agent utilizes these trajectory preference pairs
to update its policy using contrastive learning methods like DPO. This
iterative cycle of exploration and training fosters continued improvement in
the agents. Our experiments on three complex tasks demonstrate that ETO
consistently surpasses baseline performance by a large margin. Furthermore, an
examination of task-solving efficiency and potential in scenarios lacking
expert trajectory underscores the effectiveness of our approach.</div><div><a href='http://arxiv.org/abs/2403.02502v1'>2403.02502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17753v1")'>Evaluating Very Long-Term Conversational Memory of LLM Agents</div>
<div id='2402.17753v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:42:31Z</div><div>Authors: Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang</div><div style='padding-top: 10px; width: 80ex'>Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.</div><div><a href='http://arxiv.org/abs/2402.17753v1'>2402.17753v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09954v2")'>Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of
  In-Context Learning for Persona-based Dialogue Generation</div>
<div id='2402.09954v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:03:33Z</div><div>Authors: Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang</div><div style='padding-top: 10px; width: 80ex'>Previous in-context learning (ICL) research has focused on tasks such as
classification, machine translation, text2table, etc., while studies on whether
ICL can improve human-like dialogue generation are scarce. Our work fills this
gap by systematically investigating the ICL capabilities of large language
models (LLMs) in persona-based dialogue generation, conducting extensive
experiments on high-quality real human Chinese dialogue datasets. From
experimental results, we draw three conclusions: 1) adjusting prompt
instructions is the most direct, effective, and economical way to improve
generation quality; 2) randomly retrieving demonstrations (demos) achieves the
best results, possibly due to the greater diversity and the amount of effective
information; counter-intuitively, retrieving demos with a context identical to
the query performs the worst; 3) even when we destroy the multi-turn
associations and single-turn semantics in the demos, increasing the number of
demos still improves dialogue performance, proving that LLMs can learn from
corrupted dialogue demos. Previous explanations of the ICL mechanism, such as
$n$-gram induction head, cannot fully account for this phenomenon.</div><div><a href='http://arxiv.org/abs/2402.09954v2'>2402.09954v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02285v1")'>SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State
  Tracking</div>
<div id='2402.02285v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T22:49:00Z</div><div>Authors: Atharva Kulkarni, Bo-Hsiang Tseng, Joel Ruben Antony Moniz, Dhivya Piraviperumal, Hong Yu, Shruti Bhargava</div><div style='padding-top: 10px; width: 80ex'>In-context learning with Large Language Models (LLMs) has emerged as a
promising avenue of research in Dialog State Tracking (DST). However, the
best-performing in-context learning methods involve retrieving and adding
similar examples to the prompt, requiring access to labeled training data.
Procuring such training data for a wide range of domains and applications is
time-consuming, expensive, and, at times, infeasible. While zero-shot learning
requires no training data, it significantly lags behind the few-shot setup.
Thus, `\textit{Can we efficiently generate synthetic data for any dialogue
schema to enable few-shot prompting?}' Addressing this question, we propose
\method, a data generation framework tailored for DST, utilizing LLMs. Our
approach only requires the dialogue schema and a few hand-crafted dialogue
templates to synthesize natural, coherent, and free-flowing dialogues with DST
annotations. Few-shot learning using data from {\method} results in $4-5%$
improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1
and 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of
the performance compared to the few-shot setup using human-annotated training
data. Our synthetic data and code can be accessed at
https://github.com/apple/ml-synthdst</div><div><a href='http://arxiv.org/abs/2402.02285v1'>2402.02285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03367v2")'>RAG-Fusion: a New Take on Retrieval-Augmented Generation</div>
<div id='2402.03367v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T22:06:07Z</div><div>Authors: Zackary Rackauckas</div><div style='padding-top: 10px; width: 80ex'>Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.</div><div><a href='http://arxiv.org/abs/2402.03367v2'>2402.03367v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08221v1")'>Towards Causal Relationship in Indefinite Data: Baseline Model and New
  Datasets</div>
<div id='2401.08221v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T09:15:43Z</div><div>Authors: Hang Chen, Xinyu Yang, Keqing Du</div><div style='padding-top: 10px; width: 80ex'>Integrating deep learning and causal discovery has encouraged us to spot that
learning causal structures and representations in dialogue and video is full of
challenges. We defined These data forms as "Indefinite Data", characterized by
multi-structure data and multi-value representations. Unlike existing adaptable
data forms, Indefinite Data still faces gaps in datasets and methods. To
address the dataset gap, we release two high-quality datasets - Causalogue and
Causaction, containing text dialogue samples and video action samples with
causal annotations respectively. Moreover, the method gap arises from the
coexistence of multi-structure data and multi-value representations, breaking
the assumptions of all current methods and rendering them infeasible on
Indefinite Data. To this end, we propose a probabilistic framework as a
baseline, incorporating three designed highlights for this gap: 1) establishing
Causation Condition of representations using the independence of noise terms
under non-fixed causal structures, 2) treating causal strength as a latent
variable and measuring the reconstruction loss in the correlation space, and 3)
estimating the effects of latent confounders. These highpoints make the
probabilistic model capable of overcoming challenges brought by the coexistence
of multi-structure data and multi-value representations and pave the way for
the extension of latent confounders. Comprehensive experiments have evaluated
baseline results of causal structures, causal representations, and confounding
disentanglement.</div><div><a href='http://arxiv.org/abs/2401.08221v1'>2401.08221v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.08978v1")'>AutoGuide: Automated Generation and Selection of State-Aware Guidelines
  for Large Language Model Agents</div>
<div id='2403.08978v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-13T22:06:03Z</div><div>Authors: Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran, Kyunghoon Bae, Honglak Lee</div><div style='padding-top: 10px; width: 80ex'>The primary limitation of large language models (LLMs) is their restricted
understanding of the world. This poses significant difficulties for LLM-based
agents, particularly in domains where pre-trained LLMs lack sufficient
knowledge. In this paper, we introduce a novel framework, called AutoGuide,
that bridges the knowledge gap in pre-trained LLMs by leveraging implicit
knowledge in offline experiences. Specifically, AutoGuide effectively extracts
knowledge embedded in offline data by extracting a set of state-aware
guidelines. Importantly, each state-aware guideline is expressed in concise
natural language and follows a conditional structure, clearly describing the
state where it is applicable. As such, the resulting guidelines enable a
principled way to provide helpful knowledge pertinent to an agent's current
decision-making process. We show that our approach outperforms competitive
LLM-based baselines by a large margin in sequential decision-making benchmarks.</div><div><a href='http://arxiv.org/abs/2403.08978v1'>2403.08978v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15371v1")'>Can large language models explore in-context?</div>
<div id='2403.15371v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:50:43Z</div><div>Authors: Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins</div><div style='padding-top: 10px; width: 80ex'>We investigate the extent to which contemporary Large Language Models (LLMs)
can engage in exploration, a core capability in reinforcement learning and
decision making. We focus on native performance of existing LLMs, without
training interventions. We deploy LLMs as agents in simple multi-armed bandit
environments, specifying the environment description and interaction history
entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,
GPT-4, and Llama2, using a variety of prompt designs, and find that the models
do not robustly engage in exploration without substantial interventions: i)
Across all of our experiments, only one configuration resulted in satisfactory
exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally
summarized interaction history, presented as sufficient statistics; ii) All
other configurations did not result in robust exploratory behavior, including
those with chain-of-thought reasoning but unsummarized history. Although these
findings can be interpreted positively, they suggest that external
summarization -- which may not be possible in more complex settings -- is
important for obtaining desirable behavior from LLM agents. We conclude that
non-trivial algorithmic interventions, such as fine-tuning or dataset curation,
may be required to empower LLM-based decision making agents in complex
settings.</div><div><a href='http://arxiv.org/abs/2403.15371v1'>2403.15371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14530v1")'>Relative Value Biases in Large Language Models</div>
<div id='2401.14530v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T21:49:32Z</div><div>Authors: William M. Hayes, Nicolas Yax, Stefano Palminteri</div><div style='padding-top: 10px; width: 80ex'>Studies of reinforcement learning in humans and animals have demonstrated a
preference for options that yielded relatively better outcomes in the past,
even when those options are associated with lower absolute reward. The present
study tested whether large language models would exhibit a similar bias. We had
gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between
pairs of options with the goal of maximizing payoffs. A complete record of
previous outcomes was included in each prompt. Both models exhibited relative
value decision biases similar to those observed in humans and animals. Making
relative comparisons among outcomes more explicit magnified the bias, whereas
prompting the models to estimate expected outcomes caused the bias to
disappear. These results have implications for the potential mechanisms that
contribute to context-dependent choice in human agents.</div><div><a href='http://arxiv.org/abs/2401.14530v1'>2401.14530v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05612v1")'>Unfamiliar Finetuning Examples Control How Language Models Hallucinate</div>
<div id='2403.05612v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T18:28:13Z</div><div>Authors: Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, Sergey Levine</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have a tendency to generate plausible-sounding
yet factually incorrect responses, especially when queried on unfamiliar
concepts. In this work, we explore the underlying mechanisms that govern how
finetuned LLMs hallucinate. Our investigation reveals an interesting pattern:
as inputs become more unfamiliar, LLM outputs tend to default towards a
``hedged'' prediction, whose form is determined by how the unfamiliar examples
in the finetuning data are supervised. Thus, by strategically modifying these
examples' supervision, we can control LLM predictions for unfamiliar inputs
(e.g., teach them to say ``I don't know''). Based on these principles, we
develop an RL approach that more reliably mitigates hallucinations for
long-form generation tasks, by tackling the challenges presented by reward
model hallucinations. We validate our findings with a series of controlled
experiments in multiple-choice QA on MMLU, as well as long-form biography and
book/movie plot generation tasks.</div><div><a href='http://arxiv.org/abs/2403.05612v1'>2403.05612v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09629v2")'>Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking</div>
<div id='2403.09629v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:58:16Z</div><div>Authors: Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman</div><div style='padding-top: 10px; width: 80ex'>When writing and talking, people sometimes pause to think. Although
reasoning-focused works have often framed reasoning as a method of answering
questions or completing agentic tasks, reasoning is implicit in almost all
written text. For example, this applies to the steps not stated between the
lines of a proof or to the theory of mind underlying a conversation. In the
Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned
by inferring rationales from few-shot examples in question-answering and
learning from those that lead to a correct answer. This is a highly constrained
setting -- ideally, a language model could instead learn to infer unstated
rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR
in which LMs learn to generate rationales at each token to explain future text,
improving their predictions. We address key challenges, including 1) the
computational cost of generating continuations, 2) the fact that the LM does
not initially know how to generate or use internal thoughts, and 3) the need to
predict beyond individual next tokens. To resolve these, we propose a tokenwise
parallel sampling algorithm, using learnable tokens indicating a thought's
start and end, and an extended teacher-forcing technique. Encouragingly,
generated rationales disproportionately help model difficult-to-predict tokens
and improve the LM's ability to directly answer difficult questions. In
particular, after continued pretraining of an LM on a corpus of internet text
with Quiet-STaR, we find zero-shot improvements on GSM8K
(5.9%$\rightarrow$10.9%) and CommonsenseQA (36.3%$\rightarrow$47.2%) and
observe a perplexity improvement of difficult tokens in natural text.
Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR
marks a step towards LMs that can learn to reason in a more general and
scalable way.</div><div><a href='http://arxiv.org/abs/2403.09629v2'>2403.09629v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10949v1")'>SelfIE: Self-Interpretation of Large Language Model Embeddings</div>
<div id='2403.10949v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T15:30:34Z</div><div>Authors: Haozhe Chen, Carl Vondrick, Chengzhi Mao</div><div style='padding-top: 10px; width: 80ex'>How do large language models (LLMs) obtain their answers? The ability to
explain and control an LLM's reasoning process is key for reliability,
transparency, and future model developments. We propose SelfIE
(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret
their own embeddings in natural language by leveraging their ability to respond
inquiry about a given passage. Capable of interpreting open-world concepts in
the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as
making ethical decisions, internalizing prompt injection, and recalling harmful
knowledge. SelfIE's text descriptions on hidden embeddings also open up new
avenues to control LLM reasoning. We propose Supervised Control, which allows
editing open-ended concepts while only requiring gradient computation of
individual layer. We extend RLHF to hidden embeddings and propose Reinforcement
Control that erases harmful knowledge in LLM without supervision targets.</div><div><a href='http://arxiv.org/abs/2403.10949v1'>2403.10949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18312v1")'>How to think step-by-step: A mechanistic understanding of
  chain-of-thought reasoning</div>
<div id='2402.18312v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T13:14:20Z</div><div>Authors: Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty</div><div style='padding-top: 10px; width: 80ex'>Despite superior reasoning prowess demonstrated by Large Language Models
(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails
around the internal mechanisms of the models that facilitate CoT generation.
This work investigates the neural sub-structures within LLMs that manifest CoT
reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B
applied to multistep reasoning over fictional ontologies, we demonstrate that
LLMs deploy multiple parallel pathways of answer generation for step-by-step
reasoning. These parallel pathways provide sequential answers from the input
question context as well as the generated CoT. We observe a striking functional
rift in the middle layers of the LLM. Token representations in the initial half
remain strongly biased towards the pretraining prior, with the in-context
taking over abruptly in the later half. This internal phase shift manifests in
different functional components: attention heads that write the answer token
predominantly appear in the later half, attention heads that move information
along ontological relationships appear exclusively in the initial half, and so
on. To the best of our knowledge, this is the first attempt towards mechanistic
investigation of CoT reasoning in LLMs.</div><div><a href='http://arxiv.org/abs/2402.18312v1'>2402.18312v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07754v1")'>Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language
  Models</div>
<div id='2402.07754v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:23:28Z</div><div>Authors: Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Zhenguo Li, Wei Bi, Lingpeng Kong</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have gained attention in text processing, offering many
potential advantages over traditional autoregressive models. This work explores
the integration of diffusion models and Chain-of-Thought (CoT), a
well-established technique to improve the reasoning ability in autoregressive
language models. We propose Diffusion-of-Thought (DoT), allowing reasoning
steps to diffuse over time through the diffusion process. In contrast to
traditional autoregressive language models that make decisions in a
left-to-right, token-by-token manner, DoT offers more flexibility in the
trade-off between computation and reasoning performance. Our experimental
results demonstrate the effectiveness of DoT in multi-digit multiplication and
grade school math problems. Additionally, DoT showcases promising
self-correction abilities and benefits from existing reasoning-enhancing
techniques like self-consistency decoding. Our findings contribute to the
understanding and development of reasoning capabilities in diffusion language
models.</div><div><a href='http://arxiv.org/abs/2402.07754v1'>2402.07754v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01332v1")'>Chaining thoughts and LLMs to learn DNA structural biophysics</div>
<div id='2403.01332v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T22:38:01Z</div><div>Authors: Tyler D. Ross, Ashwin Gopinath</div><div style='padding-top: 10px; width: 80ex'>The future development of an AI scientist, a tool that is capable of
integrating a variety of experimental data and generating testable hypotheses,
holds immense potential. So far, bespoke machine learning models have been
created to specialize in singular scientific tasks, but otherwise lack the
flexibility of a general purpose model. Here, we show that a general purpose
large language model, chatGPT 3.5-turbo, can be fine-tuned to learn the
structural biophysics of DNA. We find that both fine-tuning models to return
chain-of-thought responses and chaining together models fine-tuned for subtasks
have an enhanced ability to analyze and design DNA sequences and their
structures.</div><div><a href='http://arxiv.org/abs/2403.01332v1'>2403.01332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15309v1")'>Counterfactual Generation with Identifiability Guarantees</div>
<div id='2402.15309v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:24:19Z</div><div>Authors: Hanqi Yan, Lingjing Kong, Lin Gui, Yuejie Chi, Eric Xing, Yulan He, Kun Zhang</div><div style='padding-top: 10px; width: 80ex'>Counterfactual generation lies at the core of various machine learning tasks,
including image translation and controllable text generation. This generation
process usually requires the identification of the disentangled latent
representations, such as content and style, that underlie the observed data.
However, it becomes more challenging when faced with a scarcity of paired data
and labeling information. Existing disentangled methods crucially rely on
oversimplified assumptions, such as assuming independent content and style
variables, to identify the latent variables, even though such assumptions may
not hold for complex data distributions. For instance, food reviews tend to
involve words like tasty, whereas movie reviews commonly contain words such as
thrilling for the same positive sentiment. This problem is exacerbated when
data are sampled from multiple domains since the dependence between content and
style may vary significantly over domains. In this work, we tackle the
domain-varying dependence between the content and the style variables inherent
in the counterfactual generation task. We provide identification guarantees for
such latent-variable models by leveraging the relative sparsity of the
influences from different latent variables. Our theoretical insights enable the
development of a doMain AdapTive counTerfactual gEneration model, called
(MATTE). Our theoretically grounded framework achieves state-of-the-art
performance in unsupervised style transfer tasks, where neither paired data nor
style labels are utilized, across four large-scale datasets. Code is available
at https://github.com/hanqi-qi/Matte.git</div><div><a href='http://arxiv.org/abs/2402.15309v1'>2402.15309v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07282v2")'>How do Large Language Models Navigate Conflicts between Honesty and
  Helpfulness?</div>
<div id='2402.07282v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T19:13:26Z</div><div>Authors: Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths</div><div style='padding-top: 10px; width: 80ex'>In day-to-day communication, people often approximate the truth - for
example, rounding the time or omitting details - in order to be maximally
helpful to the listener. How do large language models (LLMs) handle such
nuanced trade-offs? To address this question, we use psychological models and
experiments designed to characterize human behavior to analyze LLMs. We test a
range of LLMs and explore how optimization for human preferences or
inference-time reasoning affects these trade-offs. We find that reinforcement
learning from human feedback improves both honesty and helpfulness, while
chain-of-thought prompting skews LLMs towards helpfulness over honesty.
Finally, GPT-4 Turbo demonstrates human-like response patterns including
sensitivity to the conversational framing and listener's decision context. Our
findings reveal the conversational values internalized by LLMs and suggest that
even these abstract values can, to a degree, be steered by zero-shot prompting.</div><div><a href='http://arxiv.org/abs/2402.07282v2'>2402.07282v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14846v1")'>Stick to your Role! Stability of Personal Values Expressed in Large
  Language Models</div>
<div id='2402.14846v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:53:01Z</div><div>Authors: Grgur Kovač, Rémy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer</div><div style='padding-top: 10px; width: 80ex'>The standard way to study Large Language Models (LLMs) through benchmarks or
psychology questionnaires is to provide many different queries from similar
minimal contexts (e.g. multiple choice questions). However, due to LLM's highly
context-dependent nature, conclusions from such minimal-context evaluations may
be little informative about the model's behavior in deployment (where it will
be exposed to many new contexts). We argue that context-dependence should be
studied as another dimension of LLM comparison alongside others such as
cognitive abilities, knowledge, or model size. In this paper, we present a
case-study about the stability of value expression over different contexts
(simulated conversations on different topics), and as measured using a standard
psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19
open-sourced LLMs from five families. Reusing methods from psychology, we study
Rank-order stability on the population (interpersonal) level, and Ipsative
stability on the individual (intrapersonal) level. We explore two settings:
with and without instructing LLMs to simulate particular personalities. We
observe similar trends in the stability of models and model families - Mixtral,
Mistral and Qwen families being more stable than LLaMa-2 and Phi - over those
two settings, two different simulated populations, and even in the downstream
behavioral task. When instructed to simulate particular personas, LLMs exhibit
low Rank-Order stability, and this stability further diminishes with
conversation length. This highlights the need for future research directions on
LLMs that can coherently simulate a diversity of personas, as well as how
context-dependence can be studied in more thorough and efficient ways. This
paper provides a foundational step in that direction, and, to our knowledge, it
is the first study of value stability in LLMs.</div><div><a href='http://arxiv.org/abs/2402.14846v1'>2402.14846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08438v1")'>CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language
  Models</div>
<div id='2401.08438v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T03:59:59Z</div><div>Authors: Yaojia Lv, Haojie Pan, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin</div><div style='padding-top: 10px; width: 80ex'>Cognitive dynamics are pivotal to advance human understanding of the world.
Recent advancements in large language models (LLMs) reveal their potential for
cognitive simulation. However, these LLM-based cognitive studies primarily
focus on static modeling, overlooking the dynamic nature of cognition. To
bridge this gap, we propose the concept of the cognitive dynamics of LLMs and
present a corresponding task with the inspiration of longitudinal studies.
Towards the task, we develop CogBench, a novel benchmark to assess the
cognitive dynamics of LLMs and validate it through participant surveys. We also
design two evaluation metrics for CogBench, including Authenticity and
Rationality. Recognizing the inherent static nature of LLMs, we introduce
CogGPT for the task, which features an innovative iterative cognitive mechanism
aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate
the superiority of CogGPT over existing methods, particularly in its ability to
facilitate role-specific cognitive dynamics under continuous information flows.</div><div><a href='http://arxiv.org/abs/2401.08438v1'>2401.08438v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01519v3")'>Exploring the Frontiers of LLMs in Psychological Applications: A
  Comprehensive Review</div>
<div id='2401.01519v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T03:01:29Z</div><div>Authors: Luoma Ke, Song Tong, Peng Cheng, Kaiping Peng</div><div style='padding-top: 10px; width: 80ex'>This paper explores the frontiers of large language models (LLMs) in
psychology applications. Psychology has undergone several theoretical changes,
and the current use of Artificial Intelligence (AI) and Machine Learning,
particularly LLMs, promises to open up new research directions. We provide a
detailed exploration of how LLMs like ChatGPT are transforming psychological
research. It discusses the impact of LLMs across various branches of
psychology, including cognitive and behavioral, clinical and counseling,
educational and developmental, and social and cultural psychology, highlighting
their potential to simulate aspects of human cognition and behavior. The paper
delves into the capabilities of these models to emulate human-like text
generation, offering innovative tools for literature review, hypothesis
generation, experimental design, experimental subjects, data analysis, academic
writing, and peer review in psychology. While LLMs are essential in advancing
research methodologies in psychology, the paper also cautions about their
technical and ethical challenges. There are issues like data privacy, the
ethical implications of using LLMs in psychological research, and the need for
a deeper understanding of these models' limitations. Researchers should
responsibly use LLMs in psychological studies, adhering to ethical standards
and considering the potential consequences of deploying these technologies in
sensitive areas. Overall, the article provides a comprehensive overview of the
current state of LLMs in psychology, exploring potential benefits and
challenges. It serves as a call to action for researchers to leverage LLMs'
advantages responsibly while addressing associated risks.</div><div><a href='http://arxiv.org/abs/2401.01519v3'>2401.01519v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14701v1")'>COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies
  with Language Modeling</div>
<div id='2402.14701v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:56:44Z</div><div>Authors: Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl Corcoran, Guillermo Cecchi</div><div style='padding-top: 10px; width: 80ex'>The therapeutic working alliance is a critical factor in predicting the
success of psychotherapy treatment. Traditionally, working alliance assessment
relies on questionnaires completed by both therapists and patients. In this
paper, we present COMPASS, a novel framework to directly infer the therapeutic
working alliance from the natural language used in psychotherapy sessions. Our
approach utilizes advanced large language models to analyze transcripts of
psychotherapy sessions and compare them with distributed representations of
statements in the working alliance inventory. Analyzing a dataset of over 950
sessions covering diverse psychiatric conditions, we demonstrate the
effectiveness of our method in microscopically mapping patient-therapist
alignment trajectories and providing interpretability for clinical psychiatry
and in identifying emerging patterns related to the condition being treated. By
employing various neural topic modeling techniques in combination with
generative language prompting, we analyze the topical characteristics of
different psychiatric conditions and incorporate temporal modeling to capture
the evolution of topics at a turn-level resolution. This combined framework
enhances the understanding of therapeutic interactions, enabling timely
feedback for therapists regarding conversation quality and providing
interpretable insights to improve the effectiveness of psychotherapy.</div><div><a href='http://arxiv.org/abs/2402.14701v1'>2402.14701v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12326v1")'>LLM Agents for Psychology: A Study on Gamified Assessments</div>
<div id='2402.12326v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:00:30Z</div><div>Authors: Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang</div><div style='padding-top: 10px; width: 80ex'>Psychological measurement is essential for mental health, self-understanding,
and personal development. Traditional methods, such as self-report scales and
psychologist interviews, often face challenges with engagement and
accessibility. While game-based and LLM-based tools have been explored to
improve user interest and automate assessment, they struggle to balance
engagement with generalizability. In this work, we propose PsychoGAT
(Psychological Game AgenTs) to achieve a generic gamification of psychological
assessment. The main insight is that powerful LLMs can function both as adept
psychologists and innovative game designers. By incorporating LLM agents into
designated roles and carefully managing their interactions, PsychoGAT can
transform any standardized scales into personalized and engaging interactive
fiction games. To validate the proposed method, we conduct psychometric
evaluations to assess its effectiveness and employ human evaluators to examine
the generated content across various psychological constructs, including
depression, cognitive distortions, and personality traits. Results demonstrate
that PsychoGAT serves as an effective assessment tool, achieving statistically
significant excellence in psychometric metrics such as reliability, convergent
validity, and discriminant validity. Moreover, human evaluations confirm
PsychoGAT's enhancements in content coherence, interactivity, interest,
immersion, and satisfaction.</div><div><a href='http://arxiv.org/abs/2402.12326v1'>2402.12326v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12398v1")'>Primary and Secondary Factor Consistency as Domain Knowledge to Guide
  Happiness Computing in Online Assessment</div>
<div id='2402.12398v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T05:39:48Z</div><div>Authors: Xiaohua Wu, Lin Li, Xiaohui Tao, Frank Xing, Jingling Yuan</div><div style='padding-top: 10px; width: 80ex'>Happiness computing based on large-scale online web data and machine learning
methods is an emerging research topic that underpins a range of issues, from
personal growth to social stability. Many advanced Machine Learning (ML) models
with explanations are used to compute the happiness online assessment while
maintaining high accuracy of results. However, domain knowledge constraints,
such as the primary and secondary relations of happiness factors, are absent
from these models, which limits the association between computing results and
the right reasons for why they occurred. This article attempts to provide new
insights into the explanation consistency from an empirical study perspective.
Then we study how to represent and introduce domain knowledge constraints to
make ML models more trustworthy. We achieve this through: (1) proving that
multiple prediction models with additive factor attributions will have the
desirable property of primary and secondary relations consistency, and (2)
showing that factor relations with quantity can be represented as an importance
distribution for encoding domain knowledge. Factor explanation difference is
penalized by the Kullback-Leibler divergence-based loss among computing models.
Experimental results using two online web datasets show that domain knowledge
of stable factor relations exists. Using this knowledge not only improves
happiness computing accuracy but also reveals more significative happiness
factors for assisting decisions well.</div><div><a href='http://arxiv.org/abs/2402.12398v1'>2402.12398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05949v1")'>An explainable machine learning-based approach for analyzing customers'
  online data to identify the importance of product attributes</div>
<div id='2402.05949v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T20:50:48Z</div><div>Authors: Aigin Karimzadeh, Amir Zakery, Mohammadreza Mohammadi, Ali Yavari</div><div style='padding-top: 10px; width: 80ex'>Online customer data provides valuable information for product design and
marketing research, as it can reveal the preferences of customers. However,
analyzing these data using artificial intelligence (AI) for data-driven design
is a challenging task due to potential concealed patterns. Moreover, in these
research areas, most studies are only limited to finding customers' needs. In
this study, we propose a game theory machine learning (ML) method that extracts
comprehensive design implications for product development. The method first
uses a genetic algorithm to select, rank, and combine product features that can
maximize customer satisfaction based on online ratings. Then, we use SHAP
(SHapley Additive exPlanations), a game theory method that assigns a value to
each feature based on its contribution to the prediction, to provide a
guideline for assessing the importance of each feature for the total
satisfaction. We apply our method to a real-world dataset of laptops from
Kaggle, and derive design implications based on the results. Our approach
tackles a major challenge in the field of multi-criteria decision making and
can help product designers and marketers, to understand customer preferences
better with less data and effort. The proposed method outperforms benchmark
methods in terms of relevant performance metrics.</div><div><a href='http://arxiv.org/abs/2402.05949v1'>2402.05949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17435v3")'>Can Large Language Models Replace Economic Choice Prediction Labs?</div>
<div id='2401.17435v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T20:49:47Z</div><div>Authors: Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz</div><div style='padding-top: 10px; width: 80ex'>Economic choice prediction is an essential challenging task, often
constrained by the difficulties in acquiring human choice data. Indeed,
experimental economics studies had focused mostly on simple choice settings.
The AI community has recently contributed to that effort in two ways:
considering whether LLMs can substitute for humans in the above-mentioned
simple choice prediction settings, and the study through ML lens of more
elaborated but still rigorous experimental economics settings, employing
incomplete information, repetitive play, and natural language communication,
notably language-based persuasion games. This leaves us with a major
inspiration: can LLMs be used to fully simulate the economic environment and
generate data for efficient human choice prediction, substituting for the
elaborated economic lab studies? We pioneer the study of this subject,
demonstrating its feasibility. In particular, we show that a model trained
solely on LLM-generated data can effectively predict human behavior in a
language-based persuasion game, and can even outperform models trained on
actual human data.</div><div><a href='http://arxiv.org/abs/2401.17435v3'>2401.17435v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01786v1")'>COA-GPT: Generative Pre-trained Transformers for Accelerated Course of
  Action Development in Military Operations</div>
<div id='2402.01786v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T21:51:09Z</div><div>Authors: Vinicius G. Goecks, Nicholas Waytowich</div><div style='padding-top: 10px; width: 80ex'>The development of Courses of Action (COAs) in military operations is
traditionally a time-consuming and intricate process. Addressing this
challenge, this study introduces COA-GPT, a novel algorithm employing Large
Language Models (LLMs) for rapid and efficient generation of valid COAs.
COA-GPT incorporates military doctrine and domain expertise to LLMs through
in-context learning, allowing commanders to input mission information - in both
text and image formats - and receive strategically aligned COAs for review and
approval. Uniquely, COA-GPT not only accelerates COA development, producing
initial COAs within seconds, but also facilitates real-time refinement based on
commander feedback. This work evaluates COA-GPT in a military-relevant scenario
within a militarized version of the StarCraft II game, comparing its
performance against state-of-the-art reinforcement learning algorithms. Our
results demonstrate COA-GPT's superiority in generating strategically sound
COAs more swiftly, with added benefits of enhanced adaptability and alignment
with commander intentions. COA-GPT's capability to rapidly adapt and update
COAs during missions presents a transformative potential for military planning,
particularly in addressing planning discrepancies and capitalizing on emergent
windows of opportunities.</div><div><a href='http://arxiv.org/abs/2402.01786v1'>2402.01786v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11997v1")'>Remember This Event That Year? Assessing Temporal Information and
  Reasoning in Large Language Models</div>
<div id='2402.11997v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:43:03Z</div><div>Authors: Himanshu Beniwal, Kowsik Nandagopan D, Mayank Singh</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their
ability to reason about and retain temporal information remains limited. This
hinders their application in real-world scenarios where understanding the
sequential nature of events is crucial. This paper experiments with
state-of-the-art models on a novel, large-scale temporal dataset,
\textbf{TempUN}, to reveal significant limitations in temporal retention and
reasoning abilities. Interestingly, closed-source models indicate knowledge
gaps more frequently, potentially suggesting a trade-off between uncertainty
awareness and incorrect responses. Further, exploring various fine-tuning
approaches yielded no major performance improvements. The associated dataset
and code are available at the following URL
(https://github.com/lingoiitgn/TempUN).</div><div><a href='http://arxiv.org/abs/2402.11997v1'>2402.11997v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00987v1")'>Self-Supervised Contrastive Pre-Training for Multivariate Point
  Processes</div>
<div id='2402.00987v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T20:05:04Z</div><div>Authors: Xiao Shou, Dharmashankar Subramanian, Debarun Bhattacharjya, Tian Gao, Kristin P. Bennet</div><div style='padding-top: 10px; width: 80ex'>Self-supervision is one of the hallmarks of representation learning in the
increasingly popular suite of foundation models including large language models
such as BERT and GPT-3, but it has not been pursued in the context of
multivariate event streams, to the best of our knowledge. We introduce a new
paradigm for self-supervised learning for multivariate point processes using a
transformer encoder. Specifically, we design a novel pre-training strategy for
the encoder where we not only mask random event epochs but also insert randomly
sampled "void" epochs where an event does not occur; this differs from the
typical discrete-time pretext tasks such as word-masking in BERT but expands
the effectiveness of masking to better capture continuous-time dynamics. To
improve downstream tasks, we introduce a contrasting module that compares real
events to simulated void instances. The pre-trained model can subsequently be
fine-tuned on a potentially much smaller event dataset, similar conceptually to
the typical transfer of popular pre-trained language models. We demonstrate the
effectiveness of our proposed paradigm on the next-event prediction task using
synthetic datasets and 3 real applications, observing a relative performance
boost of as high as up to 20% compared to state-of-the-art models.</div><div><a href='http://arxiv.org/abs/2402.00987v1'>2402.00987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15935v2")'>Self-Supervised Learning in Event Sequences: A Comparative Study and
  Hybrid Approach of Generative Modeling and Contrastive Learning</div>
<div id='2401.15935v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T07:50:28Z</div><div>Authors: Viktor Moskvoretskii, Dmitry Osin, Egor Shvetsov, Igor Udovichenko, Maxim Zhelnin, Andrey Dukhovny, Anna Zhimerikina, Albert Efimov, Evgeny Burnaev</div><div style='padding-top: 10px; width: 80ex'>This study investigates self-supervised learning techniques to obtain
representations of Event Sequences. It is a key modality in various
applications, including but not limited to banking, e-commerce, and healthcare.
  We perform a comprehensive study of generative and contrastive approaches in
self-supervised learning, applying them both independently. We find that there
is no single supreme method. Consequently, we explore the potential benefits of
combining these approaches. To achieve this goal, we introduce a novel method
that aligns generative and contrastive embeddings as distinct modalities,
drawing inspiration from contemporary multimodal research.
  Generative and contrastive approaches are often treated as mutually
exclusive, leaving a gap for their combined exploration. Our results
demonstrate that this aligned model performs at least on par with, and mostly
surpasses, existing methods and is more universal across a variety of tasks.
Furthermore, we demonstrate that self-supervised methods consistently
outperform the supervised approach on our datasets.</div><div><a href='http://arxiv.org/abs/2401.15935v2'>2401.15935v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03222v1")'>Knowledge-guided EEG Representation Learning</div>
<div id='2403.03222v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T01:52:44Z</div><div>Authors: Aditya Kommineni, Kleanthis Avramidis, Richard Leahy, Shrikanth Narayanan</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning has produced impressive results in multimedia
domains of audio, vision and speech. This paradigm is equally, if not more,
relevant for the domain of biosignals, owing to the scarcity of labelled data
in such scenarios. The ability to leverage large-scale unlabelled data to learn
robust representations could help improve the performance of numerous inference
tasks on biosignals. Given the inherent domain differences between multimedia
modalities and biosignals, the established objectives for self-supervised
learning may not translate well to this domain. Hence, there is an unmet need
to adapt these methods to biosignal analysis. In this work we propose a
self-supervised model for EEG, which provides robust performance and remarkable
parameter efficiency by using state space-based deep learning architecture. We
also propose a novel knowledge-guided pre-training objective that accounts for
the idiosyncrasies of the EEG signal. The results indicate improved embedding
representation learning and downstream performance compared to prior works on
exemplary tasks. Also, the proposed objective significantly reduces the amount
of pre-training data required to obtain performance equivalent to prior works.</div><div><a href='http://arxiv.org/abs/2403.03222v1'>2403.03222v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03246v1")'>SeqNAS: Neural Architecture Search for Event Sequence Classification</div>
<div id='2401.03246v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T16:00:26Z</div><div>Authors: Igor Udovichenko, Egor Shvetsov, Denis Divitsky, Dmitry Osin, Ilya Trofimov, Anatoly Glushenko, Ivan Sukharev, Dmitry Berestenev, Evgeny Burnaev</div><div style='padding-top: 10px; width: 80ex'>Neural Architecture Search (NAS) methods are widely used in various
industries to obtain high quality taskspecific solutions with minimal human
intervention. Event Sequences find widespread use in various industrial
applications including churn prediction customer segmentation fraud detection
and fault diagnosis among others. Such data consist of categorical and
real-valued components with irregular timestamps. Despite the usefulness of NAS
methods previous approaches only have been applied to other domains images
texts or time series. Our work addresses this limitation by introducing a novel
NAS algorithm SeqNAS specifically designed for event sequence classification.
We develop a simple yet expressive search space that leverages commonly used
building blocks for event sequence classification including multihead self
attention convolutions and recurrent cells. To perform the search we adopt
sequential Bayesian Optimization and utilize previously trained models as an
ensemble of teachers to augment knowledge distillation. As a result of our work
we demonstrate that our method surpasses state of the art NAS methods and
popular architectures suitable for sequence classification and holds great
potential for various industrial applications.</div><div><a href='http://arxiv.org/abs/2401.03246v1'>2401.03246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.15170v1")'>Exploring the Task-agnostic Trait of Self-supervised Learning in the
  Context of Detecting Mental Disorders</div>
<div id='2403.15170v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T12:46:58Z</div><div>Authors: Rohan Kumar Gupta, Rohit Sinha</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) has been investigated to generate
task-agnostic representations across various domains. However, such
investigation has not been conducted for detecting multiple mental disorders.
The rationale behind the existence of a task-agnostic representation lies in
the overlapping symptoms among multiple mental disorders. Consequently, the
behavioural data collected for mental health assessment may carry a mixed bag
of attributes related to multiple disorders. Motivated by that, in this study,
we explore a task-agnostic representation derived through SSL in the context of
detecting major depressive disorder (MDD) and post-traumatic stress disorder
(PTSD) using audio and video data collected during interactive sessions. This
study employs SSL models trained by predicting multiple fixed targets or masked
frames. We propose a list of fixed targets to make the generated representation
more efficient for detecting MDD and PTSD. Furthermore, we modify the
hyper-parameters of the SSL encoder predicting fixed targets to generate global
representations that capture varying temporal contexts. Both these innovations
are noted to yield improved detection performances for considered mental
disorders and exhibit task-agnostic traits. In the context of the SSL model
predicting masked frames, the generated global representations are also noted
to exhibit task-agnostic traits.</div><div><a href='http://arxiv.org/abs/2403.15170v1'>2403.15170v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02991v1")'>GLIDE-RL: Grounded Language Instruction through DEmonstration in RL</div>
<div id='2401.02991v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T17:32:13Z</div><div>Authors: Chaitanya Kharyal, Sai Krishna Gottipati, Tanmay Kumar Sinha, Srijita Das, Matthew E. Taylor</div><div style='padding-top: 10px; width: 80ex'>One of the final frontiers in the development of complex human - AI
collaborative systems is the ability of AI agents to comprehend the natural
language and perform tasks accordingly. However, training efficient
Reinforcement Learning (RL) agents grounded in natural language has been a
long-standing challenge due to the complexity and ambiguity of the language and
sparsity of the rewards, among other factors. Several advances in reinforcement
learning, curriculum learning, continual learning, language models have
independently contributed to effective training of grounded agents in various
environments. Leveraging these developments, we present a novel algorithm,
Grounded Language Instruction through DEmonstration in RL (GLIDE-RL) that
introduces a teacher-instructor-student curriculum learning framework for
training an RL agent capable of following natural language instructions that
can generalize to previously unseen language instructions. In this multi-agent
framework, the teacher and the student agents learn simultaneously based on the
student's current skill level. We further demonstrate the necessity for
training the student agent with not just one, but multiple teacher agents.
Experiments on a complex sparse reward environment validates the effectiveness
of our proposed approach.</div><div><a href='http://arxiv.org/abs/2401.02991v1'>2401.02991v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03101v1")'>KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents</div>
<div id='2403.03101v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T16:39:12Z</div><div>Authors: Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have demonstrated great potential in complex
reasoning tasks, yet they fall short when tackling more sophisticated
challenges, especially when interacting with environments through generating
executable actions. This inadequacy primarily stems from the lack of built-in
action knowledge in language agents, which fails to effectively guide the
planning trajectories during task solving and results in planning
hallucination. To address this issue, we introduce KnowAgent, a novel approach
designed to enhance the planning capabilities of LLMs by incorporating explicit
action knowledge. Specifically, KnowAgent employs an action knowledge base and
a knowledgeable self-learning strategy to constrain the action path during
planning, enabling more reasonable trajectory synthesis, and thereby enhancing
the planning performance of language agents. Experimental results on HotpotQA
and ALFWorld based on various backbone models demonstrate that KnowAgent can
achieve comparable or superior performance to existing baselines. Further
analysis indicates the effectiveness of KnowAgent in terms of planning
hallucinations mitigation. Code is available in
https://github.com/zjunlp/KnowAgent.</div><div><a href='http://arxiv.org/abs/2403.03101v1'>2403.03101v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00798v2")'>Formal-LLM: Integrating Formal Language and Natural Language for
  Controllable LLM-based Agents</div>
<div id='2402.00798v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:30:50Z</div><div>Authors: Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang</div><div style='padding-top: 10px; width: 80ex'>Recent advancements on Large Language Models (LLMs) enable AI Agents to
automatically generate and execute multi-step plans to solve complex tasks.
However, since LLM's content generation process is hardly controllable, current
LLM-based agents frequently generate invalid or non-executable plans, which
jeopardizes the performance of the generated plans and corrupts users' trust in
LLM-based agents. In response, this paper proposes a novel ``Formal-LLM''
framework for LLM-based agents by integrating the expressiveness of natural
language and the precision of formal language. Specifically, the framework
allows human users to express their requirements or constraints for the
planning process as an automaton. A stack-based LLM plan generation process is
then conducted under the supervision of the automaton to ensure that the
generated plan satisfies the constraints, making the planning process
controllable. We conduct experiments on both benchmark tasks and practical
real-life tasks, and our framework achieves over 50% overall performance
increase, which validates the feasibility and effectiveness of employing
Formal-LLM to guide the plan generation of agents, preventing the agents from
generating invalid and unsuccessful plans. Further, more controllable LLM-based
agents can facilitate the broader utilization of LLM in application scenarios
where high validity of planning is essential. The work is open-sourced at
https://github.com/agiresearch/Formal-LLM.</div><div><a href='http://arxiv.org/abs/2402.00798v2'>2402.00798v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01695v1")'>Language-Guided World Models: A Model-Based Approach to AI Control</div>
<div id='2402.01695v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T03:11:36Z</div><div>Authors: Alex Zhang, Khanh Nguyen, Jens Tuyls, Albert Lin, Karthik Narasimhan</div><div style='padding-top: 10px; width: 80ex'>Installing probabilistic world models into artificial agents opens an
efficient channel for humans to communicate with and control these agents. In
addition to updating agent policies, humans can modify their internal world
models in order to influence their decisions. The challenge, however, is that
currently existing world models are difficult for humans to adapt because they
lack a natural communication interface. Aimed at addressing this shortcoming,
we develop Language-Guided World Models (LWMs), which can capture environment
dynamics by reading language descriptions. These models enhance agent
communication efficiency, allowing humans to simultaneously alter their
behavior on multiple tasks with concise language feedback. They also enable
agents to self-learn from texts originally written to instruct humans. To
facilitate the development of LWMs, we design a challenging benchmark based on
the game of MESSENGER (Hanjie et al., 2021), requiring compositional
generalization to new language descriptions and environment dynamics. Our
experiments reveal that the current state-of-the-art Transformer architecture
performs poorly on this benchmark, motivating us to design a more robust
architecture. To showcase the practicality of our proposed LWMs, we simulate a
scenario where these models augment the interpretability and safety of an agent
by enabling it to generate and discuss plans with a human before execution. By
effectively incorporating language feedback on the plan, the models boost the
agent performance in the real environment by up to three times without
collecting any interactive experiences in this environment.</div><div><a href='http://arxiv.org/abs/2402.01695v1'>2402.01695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17930v1")'>Pragmatic Instruction Following and Goal Assistance via Cooperative
  Language-Guided Inverse Planning</div>
<div id='2402.17930v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T23:06:53Z</div><div>Authors: Tan Zhi-Xuan, Lance Ying, Vikash Mansinghka, Joshua B. Tenenbaum</div><div style='padding-top: 10px; width: 80ex'>People often give instructions whose meaning is ambiguous without further
context, expecting that their actions or goals will disambiguate their
intentions. How can we build assistive agents that follow such instructions in
a flexible, context-sensitive manner? This paper introduces cooperative
language-guided inverse plan search (CLIPS), a Bayesian agent architecture for
pragmatic instruction following and goal assistance. Our agent assists a human
by modeling them as a cooperative planner who communicates joint plans to the
assistant, then performs multimodal Bayesian inference over the human's goal
from actions and language, using large language models (LLMs) to evaluate the
likelihood of an instruction given a hypothesized plan. Given this posterior,
our assistant acts to minimize expected goal achievement cost, enabling it to
pragmatically follow ambiguous instructions and provide effective assistance
even when uncertain about the goal. We evaluate these capabilities in two
cooperative planning domains (Doors, Keys &amp; Gems and VirtualHome), finding that
CLIPS significantly outperforms GPT-4V, LLM-based literal instruction following
and unimodal inverse planning in both accuracy and helpfulness, while closely
matching the inferences and assistive judgments provided by human raters.</div><div><a href='http://arxiv.org/abs/2402.17930v1'>2402.17930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16905v1")'>Enforcing Temporal Constraints on Generative Agent Behavior with
  Reactive Synthesis</div>
<div id='2402.16905v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T21:36:26Z</div><div>Authors: Raven Rothkopf, Hannah Tongxin Zeng, Mark Santolucito</div><div style='padding-top: 10px; width: 80ex'>The surge in popularity of Large Language Models (LLMs) has opened doors for
new approaches to the creation of interactive agents. However, managing the
temporal behavior of such agents over the course of an interaction remains
challenging. The stateful, long-term horizon and quantitative reasoning
required for coherent agent behavior does not fit well into the LLM paradigm.
We propose a combination of formal logic-based program synthesis and LLM
content generation to create generative agents that adhere to temporal
constraints. Our approach uses Temporal Stream Logic (TSL) to generate an
automaton that enforces a temporal structure on an agent and leaves the details
of each action for a moment in time to an LLM. By using TSL, we are able to
augment the generative agent where users have a higher level of guarantees on
behavior, better interpretability of the system, and more ability to build
agents in a modular way. We evaluate our approach on different tasks involved
in creating a coherent interactive agent specialized for various application
domains. We found that over all of the tasks, our approach using TSL achieves
at least 96% adherence, whereas the pure LLM-based approach demonstrates as low
as 14.67% adherence.</div><div><a href='http://arxiv.org/abs/2402.16905v1'>2402.16905v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15098v2")'>Hierarchical Continual Reinforcement Learning via Large Language Model</div>
<div id='2401.15098v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T03:06:51Z</div><div>Authors: Chaofan Pan, Xin Yang, Hao Wang, Wei Wei, Tianrui Li</div><div style='padding-top: 10px; width: 80ex'>The ability to learn continuously in dynamic environments is a crucial
requirement for reinforcement learning (RL) agents applying in the real world.
Despite the progress in continual reinforcement learning (CRL), existing
methods often suffer from insufficient knowledge transfer, particularly when
the tasks are diverse. To address this challenge, we propose a new framework,
Hierarchical Continual reinforcement learning via large language model
(Hi-Core), designed to facilitate the transfer of high-level knowledge. Hi-Core
orchestrates a twolayer structure: high-level policy formulation by a large
language model (LLM), which represents agenerates a sequence of goals, and
low-level policy learning that closely aligns with goal-oriented RL practices,
producing the agent's actions in response to the goals set forth. The framework
employs feedback to iteratively adjust and verify highlevel policies, storing
them along with low-level policies within a skill library. When encountering a
new task, Hi-Core retrieves relevant experience from this library to help to
learning. Through experiments on Minigrid, Hi-Core has demonstrated its
effectiveness in handling diverse CRL tasks, which outperforms popular
baselines.</div><div><a href='http://arxiv.org/abs/2401.15098v2'>2401.15098v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07069v1")'>Using Large Language Models to Automate and Expedite Reinforcement
  Learning with Reward Machine</div>
<div id='2402.07069v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T00:00:05Z</div><div>Authors: Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk Topcu, Zhe Xu</div><div style='padding-top: 10px; width: 80ex'>We present LARL-RM (Large language model-generated Automaton for
Reinforcement Learning with Reward Machine) algorithm in order to encode
high-level knowledge into reinforcement learning using automaton to expedite
the reinforcement learning. Our method uses Large Language Models (LLM) to
obtain high-level domain-specific knowledge using prompt engineering instead of
providing the reinforcement learning algorithm directly with the high-level
knowledge which requires an expert to encode the automaton. We use
chain-of-thought and few-shot methods for prompt engineering and demonstrate
that our method works using these approaches. Additionally, LARL-RM allows for
fully closed-loop reinforcement learning without the need for an expert to
guide and supervise the learning since LARL-RM can use the LLM directly to
generate the required high-level knowledge for the task at hand. We also show
the theoretical guarantee of our algorithm to converge to an optimal policy. We
demonstrate that LARL-RM speeds up the convergence by 30% by implementing our
method in two case studies.</div><div><a href='http://arxiv.org/abs/2402.07069v1'>2402.07069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17501v1")'>Intensive Care as One Big Sequence Modeling Problem</div>
<div id='2402.17501v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T13:36:55Z</div><div>Authors: Vadim Liventsev, Tobias Fritz</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning in Healthcare is typically concerned with narrow
self-contained tasks such as sepsis prediction or anesthesia control. However,
previous research has demonstrated the potential of generalist models (the
prime example being Large Language Models) to outperform task-specific
approaches due to their capability for implicit transfer learning. To enable
training of foundation models for Healthcare as well as leverage the
capabilities of state of the art Transformer architectures, we propose the
paradigm of Healthcare as Sequence Modeling, in which interaction between the
patient and the healthcare provider is represented as an event stream and tasks
like diagnosis and treatment selection are modeled as prediction of future
events in the stream. To explore this paradigm experimentally we develop
MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous
clinical records from MIMIC-IV dataset into a uniform event stream format,
train a baseline model and explore its capabilities.</div><div><a href='http://arxiv.org/abs/2402.17501v1'>2402.17501v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15420v1")'>PREDILECT: Preferences Delineated with Zero-Shot Language-based
  Reasoning in Reinforcement Learning</div>
<div id='2402.15420v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:30:05Z</div><div>Authors: Simon Holk, Daniel Marta, Iolanda Leite</div><div style='padding-top: 10px; width: 80ex'>Preference-based reinforcement learning (RL) has emerged as a new field in
robot learning, where humans play a pivotal role in shaping robot behavior by
expressing preferences on different sequences of state-action pairs. However,
formulating realistic policies for robots demands responses from humans to an
extensive array of queries. In this work, we approach the sample-efficiency
challenge by expanding the information collected per query to contain both
preferences and optional text prompting. To accomplish this, we leverage the
zero-shot capabilities of a large language model (LLM) to reason from the text
provided by humans. To accommodate the additional query information, we
reformulate the reward learning objectives to contain flexible highlights --
state-action pairs that contain relatively high information and are related to
the features processed in a zero-shot fashion from a pretrained LLM. In both a
simulated scenario and a user study, we reveal the effectiveness of our work by
analyzing the feedback and its implications. Additionally, the collective
feedback collected serves to train a robot on socially compliant trajectories
in a simulated social navigation landscape. We provide video examples of the
trained policies at https://sites.google.com/view/rl-predilect</div><div><a href='http://arxiv.org/abs/2402.15420v1'>2402.15420v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06003v1")'>A Generalized Acquisition Function for Preference-based Reward Learning</div>
<div id='2403.06003v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T20:32:17Z</div><div>Authors: Evan Ellis, Gaurav R. Ghosal, Stuart J. Russell, Anca Dragan, Erdem Bıyık</div><div style='padding-top: 10px; width: 80ex'>Preference-based reward learning is a popular technique for teaching robots
and autonomous systems how a human user wants them to perform a task. Previous
works have shown that actively synthesizing preference queries to maximize
information gain about the reward function parameters improves data efficiency.
The information gain criterion focuses on precisely identifying all parameters
of the reward function. This can potentially be wasteful as many parameters may
result in the same reward, and many rewards may result in the same behavior in
the downstream tasks. Instead, we show that it is possible to optimize for
learning the reward function up to a behavioral equivalence class, such as
inducing the same ranking over behaviors, distribution over choices, or other
related definitions of what makes two rewards similar. We introduce a tractable
framework that can capture such definitions of similarity. Our experiments in a
synthetic environment, an assistive robotics environment with domain transfer,
and a natural language processing problem with real datasets demonstrate the
superior performance of our querying method over the state-of-the-art
information gain method.</div><div><a href='http://arxiv.org/abs/2403.06003v1'>2403.06003v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19299v1")'>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</div>
<div id='2402.19299v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T16:07:22Z</div><div>Authors: Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have demonstrated proficiency in utilizing
various tools by coding, yet they face limitations in handling intricate logic
and precise control. In embodied tasks, high-level planning is amenable to
direct coding, while low-level actions often necessitate task-specific
refinement, such as Reinforcement Learning (RL). To seamlessly integrate both
modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising
a slow agent and a fast agent. The slow agent analyzes actions suitable for
coding, while the fast agent executes coding tasks. This decomposition
effectively focuses each agent on specific tasks, proving highly efficient
within our pipeline. Our approach outperforms traditional RL methods and
existing GPT agents, demonstrating superior efficiency. In the Minecraft game,
it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it
achieves SOTA performance across all designated MineDojo tasks.</div><div><a href='http://arxiv.org/abs/2402.19299v1'>2402.19299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04746v1")'>LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error</div>
<div id='2403.04746v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:50:51Z</div><div>Authors: Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su</div><div style='padding-top: 10px; width: 80ex'>Tools are essential for large language models (LLMs) to acquire up-to-date
information and take consequential actions in external environments. Existing
work on tool-augmented LLMs primarily focuses on the broad coverage of tools
and the flexibility of adding new tools. However, a critical aspect that has
surprisingly been understudied is simply how accurately an LLM uses tools for
which it has been trained. We find that existing LLMs, including GPT-4 and
open-source LLMs specifically fine-tuned for tool use, only reach a correctness
rate in the range of 30% to 60%, far from reliable use in practice. We propose
a biologically inspired method for tool-augmented LLMs, simulated trial and
error (STE), that orchestrates three key mechanisms for successful tool use
behaviors in the biological system: trial and error, imagination, and memory.
Specifically, STE leverages an LLM's 'imagination' to simulate plausible
scenarios for using a tool, after which the LLM interacts with the tool to
learn from its execution feedback. Both short-term and long-term memory are
employed to improve the depth and breadth of the exploration, respectively.
Comprehensive experiments on ToolBench show that STE substantially improves
tool learning for LLMs under both in-context learning and fine-tuning settings,
bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform
GPT-4. We also show effective continual learning of tools via a simple
experience replay strategy.</div><div><a href='http://arxiv.org/abs/2403.04746v1'>2403.04746v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18225v1")'>CogBench: a large language model walks into a psychology lab</div>
<div id='2402.18225v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T10:43:54Z</div><div>Authors: Julian Coda-Forno, Marcel Binz, Jane X. Wang, Eric Schulz</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have significantly advanced the field of
artificial intelligence. Yet, evaluating them comprehensively remains
challenging. We argue that this is partly due to the predominant focus on
performance metrics in most benchmarks. This paper introduces CogBench, a
benchmark that includes ten behavioral metrics derived from seven cognitive
psychology experiments. This novel approach offers a toolkit for phenotyping
LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse
dataset. We analyze this data using statistical multilevel modeling techniques,
accounting for the nested dependencies among fine-tuned versions of specific
LLMs. Our study highlights the crucial role of model size and reinforcement
learning from human feedback (RLHF) in improving performance and aligning with
human behavior. Interestingly, we find that open-source models are less
risk-prone than proprietary models and that fine-tuning on code does not
necessarily enhance LLMs' behavior. Finally, we explore the effects of
prompt-engineering techniques. We discover that chain-of-thought prompting
improves probabilistic reasoning, while take-a-step-back prompting fosters
model-based behaviors.</div><div><a href='http://arxiv.org/abs/2402.18225v1'>2402.18225v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05767v1")'>Extending Activation Steering to Broad Skills and Multiple Behaviours</div>
<div id='2403.05767v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T02:30:04Z</div><div>Authors: Teun van der Weij, Massimo Poesio, Nandi Schoots</div><div style='padding-top: 10px; width: 80ex'>Current large language models have dangerous capabilities, which are likely
to become more problematic in the future. Activation steering techniques can be
used to reduce risks from these capabilities. In this paper, we investigate the
efficacy of activation steering for broad skills and multiple behaviours.
First, by comparing the effects of reducing performance on general coding
ability and Python-specific ability, we find that steering broader skills is
competitive to steering narrower skills. Second, we steer models to become more
or less myopic and wealth-seeking, among other behaviours. In our experiments,
combining steering vectors for multiple different behaviours into one steering
vector is largely unsuccessful. On the other hand, injecting individual
steering vectors at different places in a model simultaneously is promising.</div><div><a href='http://arxiv.org/abs/2403.05767v1'>2403.05767v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03303v1")'>Nevermind: Instruction Override and Moderation in Large Language Models</div>
<div id='2402.03303v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:58:19Z</div><div>Authors: Edward Kim</div><div style='padding-top: 10px; width: 80ex'>Given the impressive capabilities of recent Large Language Models (LLMs), we
investigate and benchmark the most popular proprietary and different sized open
source models on the task of explicit instruction following in conflicting
situations, e.g. overrides. These include the ability of the model to override
the knowledge within the weights of the model, the ability to override (or
moderate) extracted knowledge in the prompt, and lastly the ability to perform
a full jailbreak. Experimentation performed suggest several key findings to
improve instruction following - larger models perform the best in following
instructions that override internal and contextual instructions, and are
obedient, even to a fault. When scaling to longer contexts via rope scaling, a
significant buffer needs to be maintained from the edge of the perplexity cliff
in order to maintain instruction following capabilities. Finally, we observe
improving instruction following, and subsequently instruction
overrides/jailbreaks, is fundamentally at odds with the ability of a language
model to follow given safety filters or guidelines. Thus, we postulate the most
effective approach for safe, trustworthy AI should be dealt external to the LLM
itself.</div><div><a href='http://arxiv.org/abs/2402.03303v1'>2402.03303v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01812v1")'>Distilling LLMs' Decomposition Abilities into Compact Language Models</div>
<div id='2402.01812v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:23:15Z</div><div>Authors: Denis Tarasov, Kumar Shridhar</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have demonstrated proficiency in their reasoning
abilities, yet their large size presents scalability challenges and limits any
further customization. In contrast, compact models offer customized training
but often fall short in solving complex reasoning tasks. This study focuses on
distilling the LLMs' decomposition skills into compact models using offline
reinforcement learning. We leverage the advancements in the LLM`s capabilities
to provide feedback and generate a specialized task-specific dataset for
training compact models. The development of an AI-generated dataset and the
establishment of baselines constitute the primary contributions of our work,
underscoring the potential of compact models in replicating complex
problem-solving skills.</div><div><a href='http://arxiv.org/abs/2402.01812v1'>2402.01812v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09479v1")'>Laying the Foundation First? Investigating the Generalization from
  Atomic Skills to Complex Reasoning Tasks</div>
<div id='2403.09479v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:20:54Z</div><div>Authors: Yuncheng Huang, Qianyu He, Yipei Xu, Jiaqing Liang, Yanghua Xiao</div><div style='padding-top: 10px; width: 80ex'>Current language models have demonstrated their capability to develop basic
reasoning, but struggle in more complicated reasoning tasks that require a
combination of atomic skills, such as math word problem requiring skills like
arithmetic and unit conversion. Previous methods either do not improve the
inherent atomic skills of models or not attempt to generalize the atomic skills
to complex reasoning tasks. In this paper, we first propose a probing framework
to investigate whether the atomic skill can spontaneously generalize to complex
reasoning tasks. Then, we introduce a hierarchical curriculum learning training
strategy to achieve better skill generalization. In our experiments, we find
that atomic skills can not spontaneously generalize to compositional tasks. By
leveraging hierarchical curriculum learning, we successfully induce
generalization, significantly improve the performance of open-source LMs on
complex reasoning tasks. Promisingly, the skill generalization exhibit
effective in cross-dataset and cross-domain scenarios. Complex reasoning can
also help enhance atomic skills. Our findings offer valuable guidance for
designing better training strategies for complex reasoning tasks.</div><div><a href='http://arxiv.org/abs/2403.09479v1'>2403.09479v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03244v1")'>Skill Set Optimization: Reinforcing Language Model Behavior via
  Transferable Skills</div>
<div id='2402.03244v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:59:00Z</div><div>Authors: Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, Roy Fox</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have recently been used for sequential decision
making in interactive environments. However, leveraging environment reward
signals for continual LLM actor improvement is not straightforward. We propose
Skill Set Optimization (SSO) for improving LLM actor performance through
constructing and refining sets of transferable skills. SSO constructs skills by
extracting common subtrajectories with high rewards and generating subgoals and
instructions to represent each skill. These skills are provided to the LLM
actor in-context to reinforce behaviors with high rewards. Then, SSO further
refines the skill set by pruning skills that do not continue to result in high
rewards. We evaluate our method in the classic videogame NetHack and the text
environment ScienceWorld to demonstrate SSO's ability to optimize a set of
skills and perform in-context policy improvement. SSO outperforms baselines by
40% in our custom NetHack task and outperforms the previous state-of-the-art in
ScienceWorld by 35%.</div><div><a href='http://arxiv.org/abs/2402.03244v1'>2402.03244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07876v3")'>Policy Improvement using Language Feedback Models</div>
<div id='2402.07876v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:41:34Z</div><div>Authors: Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre Côté</div><div style='padding-top: 10px; width: 80ex'>We introduce Language Feedback Models (LFMs) that identify desirable
behaviour - actions that help achieve tasks specified in the instruction - for
imitation learning in instruction following. To train LFMs, we obtain feedback
from Large Language Models (LLMs) on visual trajectories verbalized to language
descriptions. First, by using LFMs to identify desirable behaviour to imitate,
we improve in task-completion rate over strong behavioural cloning baselines on
three distinct language grounding environments (Touchdown, ScienceWorld, and
ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict
actions, when controlling for the number of LLM output tokens. Third, LFMs
generalize to unseen environments, improving task-completion rate by 3.5-12.0%
through one round of adaptation. Finally, LFM can be modified to provide
human-interpretable feedback without performance loss, allowing human
verification of desirable behaviour for imitation learning.</div><div><a href='http://arxiv.org/abs/2402.07876v3'>2402.07876v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14245v1")'>Enhancing Robotic Manipulation with AI Feedback from Multimodal Large
  Language Models</div>
<div id='2402.14245v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T03:14:03Z</div><div>Authors: Jinyi Liu, Yifu Yuan, Jianye Hao, Fei Ni, Lingzhi Fu, Yibin Chen, Yan Zheng</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been considerable attention towards leveraging large
language models (LLMs) to enhance decision-making processes. However, aligning
the natural language text instructions generated by LLMs with the vectorized
operations required for execution presents a significant challenge, often
necessitating task-specific details. To circumvent the need for such
task-specific granularity, inspired by preference-based policy learning
approaches, we investigate the utilization of multimodal LLMs to provide
automated preference feedback solely from image inputs to guide
decision-making. In this study, we train a multimodal LLM, termed CriticGPT,
capable of understanding trajectory videos in robot manipulation tasks, serving
as a critic to offer analysis and preference feedback. Subsequently, we
validate the effectiveness of preference labels generated by CriticGPT from a
reward modeling perspective. Experimental evaluation of the algorithm's
preference accuracy demonstrates its effective generalization ability to new
tasks. Furthermore, performance on Meta-World tasks reveals that CriticGPT's
reward model efficiently guides policy learning, surpassing rewards based on
state-of-the-art pre-trained representation models.</div><div><a href='http://arxiv.org/abs/2402.14245v1'>2402.14245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06420v2")'>RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic
  Manipulations With Large Language Models</div>
<div id='2403.06420v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T04:13:26Z</div><div>Authors: Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present a method for extracting the prior
knowledge of LLMs by prompt engineering so that a preliminary rule-based robot
controller for a specific task can be generated in a user-friendly manner.
Despite being imperfect, the LLM-generated robot controller is utilized to
produce action samples during rollouts with a decaying probability, thereby
improving RL's sample efficiency. We employ TD3, the widely-used RL baseline
method, and modify the actor loss to regularize the policy learning towards the
LLM-generated controller. RLingua also provides a novel method of improving the
imperfect LLM-generated robot controllers by RL. We demonstrate that RLingua
can significantly reduce the sample complexity of TD3 in four robot tasks of
panda_gym and achieve high success rates in 12 sampled sparsely rewarded robot
tasks in RLBench, where the standard TD3 fails. Additionally, We validated
RLingua's effectiveness in real-world robot experiments through Sim2Real,
demonstrating that the learned policies are effectively transferable to real
robot tasks. Further details about our work are available at our project
website https://rlingua.github.io.</div><div><a href='http://arxiv.org/abs/2403.06420v2'>2403.06420v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00677v1")'>Neural Policy Style Transfer</div>
<div id='2402.00677v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:37:42Z</div><div>Authors: Raul Fernandez-Fernandez, Juan G. Victores, Jennifer J. Gago, David Estevez, Carlos Balaguer</div><div style='padding-top: 10px; width: 80ex'>Style Transfer has been proposed in a number of fields: fine arts, natural
language processing, and fixed trajectories. We scale this concept up to
control policies within a Deep Reinforcement Learning infrastructure. Each
network is trained to maximize the expected reward, which typically encodes the
goal of an action, and can be described as the content. The expressive power of
deep neural networks enables encoding a secondary task, which can be described
as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to
transfer the style of one policy to another, while maintaining the content of
the latter. Different policies are defined via Deep Q-Network architectures.
These models are trained using demonstrations through Inverse Reinforcement
Learning. Two different sets of user demonstrations are performed, one for
content and other for style. Different styles are encoded as defined by user
demonstrations. The generated policy is the result of feeding a content policy
and a style policy to the NPST algorithm. Experiments are performed in a
catch-ball game inspired by the Deep Reinforcement Learning classical Atari
games; and a real-world painting scenario with a full-sized humanoid robot,
based on previous works of the authors. The implementation of three different
Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode
the policies within the NPST framework is proposed and the results obtained in
the experiments with each of these architectures compared.</div><div><a href='http://arxiv.org/abs/2402.00677v1'>2402.00677v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00722v1")'>Neural Style Transfer with Twin-Delayed DDPG for Shared Control of
  Robotic Manipulators</div>
<div id='2402.00722v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:14:32Z</div><div>Authors: Raul Fernandez-Fernandez, Marco Aggravi, Paolo Robuffo Giordano, Juan G. Victores, Claudio Pacchierotti</div><div style='padding-top: 10px; width: 80ex'>Neural Style Transfer (NST) refers to a class of algorithms able to
manipulate an element, most often images, to adopt the appearance or style of
another one. Each element is defined as a combination of Content and Style: the
Content can be conceptually defined as the what and the Style as the how of
said element. In this context, we propose a custom NST framework for
transferring a set of styles to the motion of a robotic manipulator, e.g., the
same robotic task can be carried out in an angry, happy, calm, or sad way. An
autoencoder architecture extracts and defines the Content and the Style of the
target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3)
network generates the robot control policy using the loss defined by the
autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the
robot motion by introducing the trained style. Such an approach can be
implemented either offline, for carrying out autonomous robot motions in
dynamic environments, or online, for adapting at runtime the style of a
teleoperated robot. The considered styles can be learned online from human
demonstrations. We carried out an evaluation with human subjects enrolling 73
volunteers, asking them to recognize the style behind some representative
robotic motions. Results show a good recognition rate, proving that it is
possible to convey different styles to a robot using this approach.</div><div><a href='http://arxiv.org/abs/2402.00722v1'>2402.00722v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00676v1")'>Deep Robot Sketching: An application of Deep Q-Learning Networks for
  human-like sketching</div>
<div id='2402.00676v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:37:23Z</div><div>Authors: Raul Fernandez-Fernandez, Juan G. Victores, Carlos Balaguer</div><div style='padding-top: 10px; width: 80ex'>The current success of Reinforcement Learning algorithms for its performance
in complex environments has inspired many recent theoretical approaches to
cognitive science. Artistic environments are studied within the cognitive
science community as rich, natural, multi-sensory, multi-cultural environments.
In this work, we propose the introduction of Reinforcement Learning for
improving the control of artistic robot applications. Deep Q-learning Neural
Networks (DQN) is one of the most successful algorithms for the implementation
of Reinforcement Learning in robotics. DQN methods generate complex control
policies for the execution of complex robot applications in a wide set of
environments. Current art painting robot applications use simple control laws
that limits the adaptability of the frameworks to a set of simple environments.
In this work, the introduction of DQN within an art painting robot application
is proposed. The goal is to study how the introduction of a complex control
policy impacts the performance of a basic art painting robot application. The
main expected contribution of this work is to serve as a first baseline for
future works introducing DQN methods for complex art painting robot frameworks.
Experiments consist of real world executions of human drawn sketches using the
DQN generated policy and TEO, the humanoid robot. Results are compared in terms
of similarity and obtained reward with respect to the reference inputs</div><div><a href='http://arxiv.org/abs/2402.00676v1'>2402.00676v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.14151v2")'>True Knowledge Comes from Practice: Aligning LLMs with Embodied
  Environments via Reinforcement Learning</div>
<div id='2401.14151v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T13:03:20Z</div><div>Authors: Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An</div><div style='padding-top: 10px; width: 80ex'>Despite the impressive performance across numerous tasks, large language
models (LLMs) often fail in solving simple decision-making tasks due to the
misalignment of the knowledge in LLMs with environments. On the contrary,
reinforcement learning (RL) agents learn policies from scratch, which makes
them always align with environments but difficult to incorporate prior
knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a
novel general online framework that deploys LLMs as decision-making agents to
efficiently interact and align with embodied environments via RL without
requiring any prepared datasets or prior knowledge of the environments.
Firstly, we query the joint probabilities of each valid action with LLMs to
form behavior policies. Then, to enhance the stability and robustness of the
policies, we propose two normalization methods and summarize four prompt design
principles. Finally, we design a novel parameter-efficient training
architecture where the actor and critic share one frozen LLM equipped with
low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to
evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency
and performance compared to the conventional RL method, PPO, and prompt tuning
method, SayCan, in both classical decision-making environment, Overcooked, and
simulated household environment, VirtualHome. ii) Benefiting from LLMs'
open-vocabulary feature, TWOSOME shows superior generalization ability to
unseen tasks. iii) Under our framework, there is no significant loss of the
LLMs' original ability during online PPO finetuning.</div><div><a href='http://arxiv.org/abs/2401.14151v2'>2401.14151v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12366v1")'>A Critical Evaluation of AI Feedback for Aligning Large Language Models</div>
<div id='2402.12366v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:53:54Z</div><div>Authors: Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, Thomas Kollar</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for
improving the instruction-following abilities of powerful pre-trained language
models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations
from a teacher model and then further fine-tunes the model with reinforcement
learning (RL), using feedback from a critic model. While recent popular
open-source models have demonstrated substantial improvements in performance
from the RL step, in this paper we question whether the complexity of this RL
step is truly warranted for AI feedback. We show that the improvements of the
RL step are virtually entirely due to the widespread practice of using a weaker
teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g.,
GPT-4) used for AI feedback generation. Specifically, we show that simple
supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF
pipelines. More generally, we find that the gains from RLAIF vary substantially
across base model families, test-time evaluation protocols, and critic models.
Finally, we provide a mechanistic explanation for when SFT may outperform the
full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally
useful in practice.</div><div><a href='http://arxiv.org/abs/2402.12366v1'>2402.12366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13578v1")'>Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for
  Counselor Reflection Generation</div>
<div id='2403.13578v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T13:24:41Z</div><div>Authors: Do June Min, Veronica Perez-Rosas, Kenneth Resnicow, Rada Mihalcea</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the problem of multi-reward reinforcement learning to
jointly optimize for multiple text qualities for natural language generation.
We focus on the task of counselor reflection generation, where we optimize the
generators to simultaneously improve the fluency, coherence, and reflection
quality of generated counselor responses. We introduce two novel bandit
methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining
rewards into a single value and optimizing them simultaneously. Specifically,
we employ non-contextual and contextual multi-arm bandits to dynamically adjust
multiple reward weights during training. Through automatic and manual
evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt,
outperform existing naive and bandit baselines, showcasing their potential for
enhancing language models.</div><div><a href='http://arxiv.org/abs/2403.13578v1'>2403.13578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19446v1")'>ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</div>
<div id='2402.19446v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:45:56Z</div><div>Authors: Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar</div><div style='padding-top: 10px; width: 80ex'>A broad use case of large language models (LLMs) is in goal-directed
decision-making tasks (or "agent" tasks), where an LLM needs to not just
generate completions for a given prompt, but rather make intelligent decisions
over a multi-turn interaction to accomplish a task (e.g., when interacting with
the web, using tools, or providing customer support). Reinforcement learning
(RL) provides a general paradigm to address such agent tasks, but current RL
methods for LLMs largely focus on optimizing single-turn rewards. By
construction, most single-turn RL methods cannot endow LLMs with the ability to
intelligently seek information over multiple turns, perform credit assignment,
or reason about their past actions -- all of which are critical in agent tasks.
This raises the question: how can we design effective and efficient multi-turn
RL algorithms for LLMs? In this paper, we develop a framework for building
multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility
of existing single-turn RL methods for LLMs (e.g., proximal policy
optimization), while accommodating multiple turns, long horizons, and delayed
rewards effectively. To do this, our framework adopts a hierarchical RL
approach and runs two RL algorithms in parallel: a high-level off-policy
value-based RL algorithm to aggregate reward over utterances, and a low-level
RL algorithm that utilizes this high-level value function to train a token
policy within each utterance or turn. Our hierarchical framework, Actor-Critic
Framework with a Hierarchical Structure (ArCHer), can also give rise to other
RL methods. Empirically, we find that ArCHer significantly improves efficiency
and performance on agent tasks, attaining a sample efficiency of about 100x
over existing methods, while also improving with larger model capacity (upto
the 7 billion scale that we tested on).</div><div><a href='http://arxiv.org/abs/2402.19446v1'>2402.19446v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06754v2")'>ALaRM: Align Language Models via Hierarchical Rewards Modeling</div>
<div id='2403.06754v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T14:28:40Z</div><div>Authors: Yuhang Lai, Siyuan Wang, Shujun Liu, Xuanjing Huang, Zhongyu Wei</div><div style='padding-top: 10px; width: 80ex'>We introduce ALaRM, the first framework modeling hierarchical rewards in
reinforcement learning from human feedback (RLHF), which is designed to enhance
the alignment of large language models (LLMs) with human preferences. The
framework addresses the limitations of current alignment approaches, which
often struggle with the inconsistency and sparsity of human supervision
signals, by integrating holistic rewards with aspect-specific rewards. This
integration enables more precise and consistent guidance of language models
towards desired outcomes, particularly in complex and open text generation
tasks. By employing a methodology that filters and combines multiple rewards
based on their consistency, the framework provides a reliable mechanism for
improving model alignment. We validate our approach through applications in
long-form question answering and machine translation tasks, employing
gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over
existing baselines. Our work underscores the effectiveness of hierarchical
rewards modeling in refining LLM training processes for better human preference
alignment. We release our code at https://ALaRM-fdu.github.io.</div><div><a href='http://arxiv.org/abs/2403.06754v2'>2403.06754v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12086v1")'>West-of-N: Synthetic Preference Generation for Improved Reward Modeling</div>
<div id='2401.12086v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:24:43Z</div><div>Authors: Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn</div><div style='padding-top: 10px; width: 80ex'>The success of reinforcement learning from human feedback (RLHF) in language
model alignment is strongly dependent on the quality of the underlying reward
model. In this paper, we present a novel approach to improve reward model
quality by generating synthetic preference data, thereby augmenting the
training dataset with on-policy, high-quality preference pairs. Motivated by
the promising results of Best-of-N sampling strategies in language model
training, we extend their application to reward model training. This results in
a self-training strategy to generate preference pairs by selecting the best and
worst candidates in a pool of responses to a given query. Empirically, we find
that this approach improves the performance of any reward model, with an effect
comparable to the addition of a similar quantity of human preference data. This
work opens up new avenues of research for improving RLHF for language model
alignment, by offering synthetic preference generation as a solution to reward
modeling challenges.</div><div><a href='http://arxiv.org/abs/2401.12086v1'>2401.12086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18571v3")'>Arithmetic Control of LLMs for Diverse User Preferences: Directional
  Preference Alignment with Multi-Objective Rewards</div>
<div id='2402.18571v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T18:58:25Z</div><div>Authors: Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>Fine-grained control over large language models (LLMs) remains a significant
challenge, hindering their adaptability to diverse user needs. While
Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning
LLMs, its reliance on scalar rewards often limits its ability to capture
diverse user preferences in real-world applications. To address this
limitation, we introduce the Directional Preference Alignment (DPA) framework.
Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling
to represent diverse preference profiles. Additionally, DPA models user
preferences as directions (i.e., unit vectors) in the reward space to achieve
user-dependent preference control. Our method involves training a
multi-objective reward model and then fine-tuning the LLM with a
preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF
method adopted by Llama 2. This method enjoys a better performance trade-off
across various reward objectives. In comparison with the scalar-reward RLHF,
DPA offers users intuitive control over LLM generation: they can arithmetically
specify their desired trade-offs (e.g., more helpfulness with less verbosity).
We also validate the effectiveness of DPA with real-world alignment experiments
on Mistral-7B. Our method provides straightforward arithmetic control over the
trade-off between helpfulness and verbosity while maintaining competitive
performance with strong baselines such as Direct Preference Optimization (DPO).</div><div><a href='http://arxiv.org/abs/2402.18571v3'>2402.18571v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05133v1")'>Personalized Language Modeling from Personalized Human Feedback</div>
<div id='2402.05133v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T04:18:58Z</div><div>Authors: Xinyu Li, Zachary C. Lipton, Liu Leqi</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) is the current dominating
framework to fine-tune large language models to better align with human
preferences. However, the underlying premise of algorithms developed under this
framework can be problematic when user preferences encoded in human feedback
are diverse. In this work, we aim to address this problem by developing methods
for building personalized language models. We first formally introduce the task
of learning from personalized human feedback and explain why vanilla RLHF can
be problematic in this context. We then propose a general Personalized-RLHF
(P-RLHF) framework, which requires one to jointly learn a user model and a
language (or reward) model. The user model takes in user information and
outputs user representations. Its structure encodes our assumptions about user
preferences underlying the feedback data. We develop new learning objectives
for personalized reward modeling and personalized Direct Preference
Optimization. To demonstrate the efficacy of our method, we test it on
real-world text summarization data with annotated preferences and annotator
information. We fine-tune GPT-J 6B to obtain personalized language (and reward)
models, which outperform non-personalized models in terms of aligning with
individual preferences.</div><div><a href='http://arxiv.org/abs/2402.05133v1'>2402.05133v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10704v1")'>PERL: Parameter Efficient Reinforcement Learning from Human Feedback</div>
<div id='2403.10704v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T21:43:46Z</div><div>Authors: Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu, Simral Chaudhary, Bowen Li, Saravanan Ganesh, Bill Byrne, Jessica Hoffmann, Hassan Mansoor, Wei Li, Abhinav Rastogi, Lucas Dixon</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong
method to align Pretrained Large Language Models (LLMs) with human preferences.
But training models with RLHF is computationally expensive, and an overall
complex process. In this work, we study RLHF where the underlying models are
trained using the parameter efficient method of Low-Rank Adaptation (LoRA)
introduced by Hu et al. [2021]. We investigate the setup of "Parameter
Efficient Reinforcement Learning" (PERL), in which we perform reward model
training and reinforcement learning using LoRA. We compare PERL to conventional
fine-tuning (full-tuning) across various configurations for 7 benchmarks,
including 2 novel datasets, of reward modeling and reinforcement learning. We
find that PERL performs on par with the conventional RLHF setting, while
training faster, and with less memory. This enables the high performance of
RLHF, while reducing the computational burden that limits its adoption as an
alignment technique for Large Language Models. We also release 2 novel thumbs
up/down preference datasets: "Taskmaster Coffee", and "Taskmaster Ticketing" to
promote research around RLHF.</div><div><a href='http://arxiv.org/abs/2403.10704v1'>2403.10704v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14740v2")'>Back to Basics: Revisiting REINFORCE Style Optimization for Learning
  from Human Feedback in LLMs</div>
<div id='2402.14740v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:52:34Z</div><div>Authors: Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker</div><div style='padding-top: 10px; width: 80ex'>AI alignment in the shape of Reinforcement Learning from Human Feedback
(RLHF) is increasingly treated as a crucial ingredient for high performance
large language models. Proximal Policy Optimization (PPO) has been positioned
by recent literature as the canonical method for the RL part of RLHF. However,
it involves both high computational cost and sensitive hyperparameter tuning.
We posit that most of the motivational principles that led to the development
of PPO are less of a practical concern in RLHF and advocate for a less
computationally expensive method that preserves and even increases performance.
We revisit the formulation of alignment from human preferences in the context
of RL. Keeping simplicity as a guiding principle, we show that many components
of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style
optimization variants outperform both PPO and newly proposed "RL-free" methods
such as DPO and RAFT. Our work suggests that careful adaptation to LLMs
alignment characteristics enables benefiting from online RL optimization at low
cost.</div><div><a href='http://arxiv.org/abs/2402.14740v2'>2402.14740v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08925v1")'>MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with
  Diverse Human Preferences</div>
<div id='2402.08925v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T03:56:27Z</div><div>Authors: Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, Mengdi Wang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) aligns language models to
human preferences by employing a singular reward model derived from preference
data. However, such an approach overlooks the rich diversity of human
preferences inherent in data collected from multiple users. In this work, we
first derive an impossibility result of alignment with single reward RLHF,
thereby highlighting its insufficiency in representing diverse human
preferences. To provide an equitable solution to the problem, we learn a
mixture of preference distributions via an expectation-maximization algorithm
and propose a MaxMin alignment objective for policy learning inspired by the
Egalitarian principle in social choice theory to better represent diverse human
preferences. We elucidate the connection of our proposed approach to
distributionally robust optimization and general utility RL, thereby
highlighting the generality and robustness of our proposed solution. We present
comprehensive experimental results on small-scale (GPT-2) and large-scale
language models (with Tulu2-7B) and show the efficacy of the proposed approach
in the presence of diversity among human preferences. Our algorithm achieves an
average improvement of more than 16% in win-rates over conventional RLHF
algorithms and improves the win-rate (accuracy) for minority groups by over 33%
without compromising the performance of majority groups, showcasing the
robustness and fairness of our approach. We remark that our findings in this
work are not only limited to language models but also extend to reinforcement
learning in general.</div><div><a href='http://arxiv.org/abs/2402.08925v1'>2402.08925v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14760v1")'>Generalizing Reward Modeling for Out-of-Distribution Preference Learning</div>
<div id='2402.14760v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:20:33Z</div><div>Authors: Chen Jia</div><div style='padding-top: 10px; width: 80ex'>Preference learning (PL) with large language models (LLMs) aims to align the
LLMs' generations with human preferences. Previous work on reinforcement
learning from human feedback (RLHF) has demonstrated promising results in
in-distribution PL. However, due to the difficulty of obtaining human feedback,
discretely training reward models for every encountered distribution is
challenging. Thus, out-of-distribution (OOD) PL is practically useful for
enhancing the generalization ability of LLMs with limited preference feedback.
This work addresses OOD PL by optimizing a general reward model through a
meta-learning approach. During meta-training, a bilevel optimization algorithm
is utilized to learn a reward model capable of guiding policy learning to align
with human preferences across various distributions. When encountering a test
distribution, the meta-test procedure conducts regularized policy optimization
using the learned reward model for PL. We theoretically demonstrate the
convergence rate of the bilevel optimization algorithm under reasonable
assumptions. Additionally, we conduct experiments on two text generation tasks
across 20 held-out domains and outperform a variety of strong baselines across
various evaluation metrics.</div><div><a href='http://arxiv.org/abs/2402.14760v1'>2402.14760v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09401v1")'>Reinforcement Learning from Human Feedback with Active Queries</div>
<div id='2402.09401v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:58:40Z</div><div>Authors: Kaixuan Ji, Jiafan He, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>Aligning large language models (LLM) with human preference plays a key role
in building modern generative models and can be achieved by reinforcement
learning from human feedback (RLHF). Despite their superior performance,
current RLHF approaches often require a large amount of human-labelled
preference data, which is expensive to collect. In this paper, inspired by the
success of active learning, we address this problem by proposing
query-efficient RLHF methods. We first formalize the alignment problem as a
contextual dueling bandit problem and design an active-query-based proximal
policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret
bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the
dimension of feature space and $\Delta$ is the sub-optimality gap over all the
contexts. We then propose ADPO, a practical version of our algorithm based on
direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our
experiments show that ADPO, while only making about half of queries for human
preference, matches the performance of the state-of-the-art DPO method.</div><div><a href='http://arxiv.org/abs/2402.09401v1'>2402.09401v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08114v1")'>Active Preference Learning for Large Language Models</div>
<div id='2402.08114v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T23:09:00Z</div><div>Authors: William Muldrew, Peter Hayes, Mingtian Zhang, David Barber</div><div style='padding-top: 10px; width: 80ex'>As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.</div><div><a href='http://arxiv.org/abs/2402.08114v1'>2402.08114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10958v1")'>Relative Preference Optimization: Enhancing LLM Alignment through
  Contrasting Responses across Identical and Diverse Prompts</div>
<div id='2402.10958v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:47:57Z</div><div>Authors: Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, Mingyuan Zhou</div><div style='padding-top: 10px; width: 80ex'>In the field of large language models (LLMs), aligning models with the
diverse preferences of users is a critical challenge. Direct Preference
Optimization (DPO) has played a key role in this area. It works by using pairs
of preferences derived from the same prompts, and it functions without needing
an additional reward model. However, DPO does not fully reflect the complex
nature of human learning, which often involves understanding contrasting
responses to not only identical but also similar questions. To overcome this
shortfall, we propose Relative Preference Optimization (RPO). RPO is designed
to discern between more and less preferred responses derived from both
identical and related prompts. It introduces a contrastive weighting mechanism,
enabling the tuning of LLMs using a broader range of preference data, including
both paired and unpaired sets. This approach expands the learning capabilities
of the model, allowing it to leverage insights from a more varied set of
prompts. Through empirical tests, including dialogue and summarization tasks,
and evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a
superior ability to align LLMs with user preferences and to improve their
adaptability during the training process. The PyTorch code necessary to
reproduce the results presented in the paper will be made available on GitHub
for public access.</div><div><a href='http://arxiv.org/abs/2402.10958v1'>2402.10958v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08005v1")'>Refined Direct Preference Optimization with Synthetic Data for
  Behavioral Alignment of LLMs</div>
<div id='2402.08005v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:10:13Z</div><div>Authors: Víctor Gallego</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce \emph{refined Direct Preference Optimization}
(rDPO), a method for improving the behavioral alignment of Large Language
Models (LLMs) without the need for human-annotated data. The method involves
creating synthetic data using self-critique prompting by a teacher LLM and then
utilising a generalized DPO loss function to distil to a student LLM. The loss
function incorporates an additional external reward model to improve the
quality of synthetic data, making rDPO robust to potential noise in the
synthetic dataset. rDPO is shown to be effective in a diverse set of
behavioural alignment tasks, such as improved safety, robustness against
role-playing, and reduced sycophancy. Code to be released at
https://github.com/vicgalle/refined-dpo.</div><div><a href='http://arxiv.org/abs/2402.08005v1'>2402.08005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14860v2")'>Ranking Large Language Models without Ground Truth</div>
<div id='2402.14860v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T00:49:43Z</div><div>Authors: Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</div><div style='padding-top: 10px; width: 80ex'>Evaluation and ranking of large language models (LLMs) has become an
important problem with the proliferation of these models and their impact.
Evaluation methods either require human responses which are expensive to
acquire or use pairs of LLMs to evaluate each other which can be unreliable. In
this paper, we provide a novel perspective where, given a dataset of prompts
(viz. questions, instructions, etc.) and a set of LLMs, we rank them without
access to any ground truth or reference responses. Inspired by real life where
both an expert and a knowledgeable person can identify a novice our main idea
is to consider triplets of models, where each one of them evaluates the other
two, correctly identifying the worst model in the triplet with high
probability. We also analyze our idea and provide sufficient conditions for it
to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.
In experiments on different generative tasks (summarization, multiple-choice,
and dialog), our methods reliably recover close to true rankings without
reference data. This points to a viable low-resource mechanism for practical
use.</div><div><a href='http://arxiv.org/abs/2402.14860v2'>2402.14860v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12146v1")'>Meta Ranking: Less Capable Language Models are Capable for Single
  Response Judgement</div>
<div id='2402.12146v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T13:57:55Z</div><div>Authors: Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>Although Large Language Models (LLMs) have demonstrated strong performance on
a wide range of tasks, they still face reliability challenges such as
hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are
effective in judging the reliability of individual responses, while less
capable ones are often tuned to evaluate the relative reliability of responses
to the same query. To enable less capable LLMs to effectively judge the
reliability of individual responses, we propose a novel method named
$\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess
the response directly, we achieve the judgement by comparing the target
query-response pair with reference query-response pairs. We found its
remarkable effectiveness in error detection for LLM responses on reasoning
tasks, where less capable LLMs could outperform strong baselines, even without
fine-tuning. We further demonstrate that MR can be used to enhance the
performance of LLMs in two practical applications: query routing and iterative
training data filtering. The former achieves GPT-4-turbo comparable performance
with less than half the token consumption, while the latter makes the
instruction-tuned LLaMA-7B and Phi-2, a 2.7B model, significantly surpass
Alpaca-13B over fewer training samples, underscoring the high potential of our
proposed method.</div><div><a href='http://arxiv.org/abs/2402.12146v1'>2402.12146v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13228v1")'>Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive</div>
<div id='2402.13228v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:42:34Z</div><div>Authors: Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White</div><div style='padding-top: 10px; width: 80ex'>Direct Preference Optimisation (DPO) is effective at significantly improving
the performance of large language models (LLMs) on downstream tasks such as
reasoning, summarisation, and alignment. Using pairs of preferred and
dispreferred data, DPO models the \textit{relative} probability of picking one
response over another. In this work, first we show theoretically that the
standard DPO loss can lead to a \textit{reduction} of the model's likelihood of
the preferred examples, as long as the relative probability between the
preferred and dispreferred classes increases. We then show empirically that
this phenomenon occurs when fine-tuning LLMs on common datasets, especially
datasets in which the edit distance between pairs of completions is low. Using
these insights, we design DPO-Positive (DPOP), a new loss function and training
procedure which avoids this failure mode. Surprisingly, we also find that DPOP
significantly outperforms DPO across a wide variety of datasets and downstream
tasks, including datasets with high edit distances between completions. By
fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which
achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly
2\% better than any other open-source model on the HuggingFace Open LLM
Leaderboard and becomes the first open-source LLM to surpass an average
accuracy of 80\%.</div><div><a href='http://arxiv.org/abs/2402.13228v1'>2402.13228v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00409v1")'>Provably Robust DPO: Aligning Language Models with Noisy Feedback</div>
<div id='2403.00409v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T09:55:18Z</div><div>Authors: Sayak Ray Chowdhury, Anush Kini, Nagarajan Natarajan</div><div style='padding-top: 10px; width: 80ex'>Learning from preference-based feedback has recently gained traction as a
promising approach to align language models with human interests. While these
aligned generative models have demonstrated impressive capabilities across
various tasks, their dependence on high-quality human preference data poses a
bottleneck in practical applications. Specifically, noisy (incorrect and
ambiguous) preference pairs in the dataset might restrict the language models
from capturing human intent accurately. While practitioners have recently
proposed heuristics to mitigate the effect of noisy preferences, a complete
theoretical understanding of their workings remain elusive.
  In this work, we aim to bridge this gap by by introducing a general framework
for policy optimization in the presence of random preference flips. We focus on
the direct preference optimization (DPO) algorithm in particular since it
assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising
concerns about the impact of noisy data on the learned policy. We design a
novel loss function, which de-bias the effect of noise on average, making a
policy trained by minimizing that loss robust to the noise. Under log-linear
parameterization of the policy class and assuming good feature coverage of the
SFT policy, we prove that the sub-optimality gap of the proposed robust DPO
(rDPO) policy compared to the optimal policy is of the order
$O(\frac{1}{1-2\epsilon}\sqrt{\frac{d}{n}})$, where $\epsilon &lt; 1/2$ is flip
rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset.
Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless
dataset show that rDPO is robust to noise in preference labels compared to
vanilla DPO and other heuristics proposed by practitioners.</div><div><a href='http://arxiv.org/abs/2403.00409v1'>2403.00409v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06688v1")'>Don't Rank, Combine! Combining Machine Translation Hypotheses Using
  Quality Estimation</div>
<div id='2401.06688v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T16:52:41Z</div><div>Authors: Giorgos Vernikos, Andrei Popescu-Belis</div><div style='padding-top: 10px; width: 80ex'>Neural machine translation systems estimate probabilities of target sentences
given source sentences, yet these estimates may not align with human
preferences. This work introduces QE-fusion, a method utilizing a quality
estimation metric (QE) that better correlates with human judgments to
synthesize improved translations. QE-fusion leverages a candidate pool sampled
from a model, combining spans from different candidates using QE metrics such
as CometKiwi. We compare QE-fusion against beam search and recent reranking
techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method
consistently improves translation quality in terms of COMET and BLEURT scores
when applied to large language models (LLMs) used for translation (PolyLM,
XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over
five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs
due to their ability to generate diverse outputs. We demonstrate that our
approach generates novel translations in over half of the cases and
consistently outperforms other methods across varying numbers of candidates
(5-200). Furthermore, we empirically establish that QE-fusion scales linearly
with the number of candidates in the pool. QE-fusion proves effective in
enhancing LLM-based translation without the need for costly retraining of LLMs.</div><div><a href='http://arxiv.org/abs/2401.06688v1'>2401.06688v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07230v1")'>Curry-DPO: Enhancing Alignment using Curriculum Learning &amp; Ranked
  Preferences</div>
<div id='2403.07230v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T00:58:19Z</div><div>Authors: Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, Sathwik Tejaswi Madhusudhan</div><div style='padding-top: 10px; width: 80ex'>Direct Preference Optimization (DPO) is an effective technique that leverages
pairwise preference data (usually one chosen and rejected response pair per
user prompt) to align LLMs to human preferences. In practice, multiple
responses can exist for a given prompt with varying quality relative to each
other. With availability of such quality ratings for multiple responses, we
propose utilizing these responses to create multiple preference pairs for a
given prompt. Our work focuses on systematically using the constructed multiple
preference pair in DPO training via curriculum learning methodology. In
particular, we order these multiple pairs of preference data from easy to hard
(emulating curriculum training) according to various criteria. We show detailed
comparisons of our proposed approach to the standard single-pair DPO setting.
Our method, which we call Curry-DPO consistently shows increased performance
gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,
highlighting its effectiveness. More specifically, Curry-DPO achieves a score
of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs
with similar parameter size. Curry-DPO also achieves the highest adjusted win
rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and
87.9% respectively) in our experiments, with notable gains of upto 7.5% when
compared to standard DPO technique.</div><div><a href='http://arxiv.org/abs/2403.07230v1'>2403.07230v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14228v2")'>COPR: Continual Human Preference Learning via Optimal Policy
  Regularization</div>
<div id='2402.14228v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T02:20:08Z</div><div>Authors: Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to
improve the alignment of Large Language Models (LLMs) with human preferences.
Given the evolving nature of human preferences, continual alignment becomes
more crucial and practical in comparison to traditional static alignment.
Nevertheless, making RLHF compatible with Continual Learning (CL) is
challenging due to its complex process. Meanwhile, directly learning new human
preferences may lead to Catastrophic Forgetting (CF) of historical preferences,
resulting in helpless or harmful outputs. To overcome these challenges, we
propose the Continual Optimal Policy Regularization (COPR) method, which draws
inspiration from the optimal policy theory. COPR utilizes a sampling
distribution as a demonstration and regularization constraints for CL. It
adopts the Lagrangian Duality (LD) method to dynamically regularize the current
policy based on the historically optimal policy, which prevents CF and avoids
over-emphasizing unbalanced objectives. We also provide formal proof for the
learnability of COPR. The experimental results show that COPR outperforms
strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4
evaluations and human assessment. Furthermore, we validate the robustness of
COPR under various CL settings, including different backbones, replay memory
sizes, and learning orders.</div><div><a href='http://arxiv.org/abs/2402.14228v2'>2402.14228v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11525v3")'>Advancing Translation Preference Modeling with RLHF: A Step Towards
  Cost-Effective Solution</div>
<div id='2402.11525v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T09:51:49Z</div><div>Authors: Nuo Xu, Jun Zhao, Can Zu, Sixian Li, Lu Chen, Zhihao Zhang, Rui Zheng, Shihan Dou, Wenjuan Qin, Tao Gui, Qi Zhang, Xuanjing Huang</div><div style='padding-top: 10px; width: 80ex'>Faithfulness, expressiveness, and elegance is the constant pursuit in machine
translation. However, traditional metrics like \textit{BLEU} do not strictly
align with human preference of translation quality. In this paper, we explore
leveraging reinforcement learning with human feedback (\textit{RLHF}) to
improve translation quality. It is non-trivial to collect a large high-quality
dataset of human comparisons between translations, especially for low-resource
languages. To address this issue, we propose a cost-effective preference
learning strategy, optimizing reward models by distinguishing between human and
machine translations. In this manner, the reward model learns the deficiencies
of machine translation compared to human and guides subsequent improvements in
machine translation. Experimental results demonstrate that \textit{RLHF} can
effectively enhance translation quality and this improvement benefits other
translation directions not trained with \textit{RLHF}. Further analysis
indicates that the model's language capabilities play a crucial role in
preference learning. A reward model with strong language capabilities can more
sensitively learn the subtle differences in translation quality and align
better with real human translation preferences.</div><div><a href='http://arxiv.org/abs/2402.11525v3'>2402.11525v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00843v1")'>Enhancing Long-Term Recommendation with Bi-level Learnable Large
  Language Model Planning</div>
<div id='2403.00843v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T13:49:56Z</div><div>Authors: Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, Fuli Feng</div><div style='padding-top: 10px; width: 80ex'>Traditional recommendation setting tends to excessively cater to users'
immediate interests and neglect their long-term engagement. To address it, it
is crucial to incorporate planning capabilities into the recommendation
decision-making process to develop policies that take into account both
immediate interests and long-term engagement. Despite Reinforcement Learning
(RL) can learn planning capacity by maximizing cumulative reward, the scarcity
of recommendation data presents challenges such as instability and
susceptibility to overfitting when training RL models from scratch.
  In this context, we propose to leverage the remarkable planning capabilities
over sparse data of Large Language Models (LLMs) for long-term recommendation.
The key lies in enabling a language model to understand and apply task-solving
principles effectively in personalized recommendation scenarios, as the model's
pre-training may not naturally encompass these principles, necessitating the
need to inspire or teach the model. To achieve this, we propose a Bi-level
Learnable LLM Planner framework, which combines macro-learning and
micro-learning through a hierarchical mechanism. The framework includes a
Planner and Reflector for acquiring high-level guiding principles and an
Actor-Critic component for planning personalization. Extensive experiments
validate the superiority of the framework in learning to plan for long-term
recommendations.</div><div><a href='http://arxiv.org/abs/2403.00843v1'>2403.00843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09345v3")'>Mitigating Reward Hacking via Information-Theoretic Reward Modeling</div>
<div id='2402.09345v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T17:49:07Z</div><div>Authors: Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Despite the success of reinforcement learning from human feedback (RLHF) in
aligning language models with human values, reward hacking, also termed reward
overoptimization, remains a critical challenge, which primarily stems from
limitations in reward modeling, i.e., generalizability of the reward model and
inconsistency in the preference dataset. In this work, we tackle this problem
from an information theoretic-perspective, and propose a generalizable and
robust framework for reward modeling, namely InfoRM, by introducing a
variational information bottleneck objective to filter out irrelevant
information and developing a mechanism for model complexity modulation.
Notably, we further identify a correlation between overoptimization and
outliers in the latent space, establishing InfoRM as a promising tool for
detecting reward overoptimization. Inspired by this finding, we propose the
Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the
latent space, as an indicator of reward overoptimization to facilitate the
development of online mitigation strategies. Extensive experiments on a wide
range of settings and model scales (70M, 440M, 1.4B, and 7B) support the
effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization
detection mechanism is effective, potentially signifying a notable advancement
in the field of RLHF. Code will be released upon acceptance.</div><div><a href='http://arxiv.org/abs/2402.09345v3'>2402.09345v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16335v1")'>Iterative Data Smoothing: Mitigating Reward Overfitting and
  Overoptimization in RLHF</div>
<div id='2401.16335v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:43:42Z</div><div>Authors: Banghua Zhu, Michael I. Jordan, Jiantao Jiao</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that
aligns language models closely with human-centric values. The initial phase of
RLHF involves learning human values using a reward model from ranking data. It
is observed that the performance of the reward model degrades after one epoch
of training, and optimizing too much against the learned reward model
eventually hinders the true objective. This paper delves into these issues,
leveraging the theoretical insights to design improved reward learning
algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during
each training epoch, we not only update the model with the data, but also
update the date using the model, replacing hard labels with soft labels. Our
empirical findings highlight the superior performance of this approach over the
traditional methods.</div><div><a href='http://arxiv.org/abs/2401.16335v1'>2401.16335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08959v1")'>Towards Off-Policy Reinforcement Learning for Ranking Policies with
  Human Feedback</div>
<div id='2401.08959v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T04:19:33Z</div><div>Authors: Teng Xiao, Suhang Wang</div><div style='padding-top: 10px; width: 80ex'>Probabilistic learning to rank (LTR) has been the dominating approach for
optimizing the ranking metric, but cannot maximize long-term rewards.
Reinforcement learning models have been proposed to maximize user long-term
rewards by formulating the recommendation as a sequential decision-making
problem, but could only achieve inferior accuracy compared to LTR counterparts,
primarily due to the lack of online interactions and the characteristics of
ranking. In this paper, we propose a new off-policy value ranking (VR)
algorithm that can simultaneously maximize user long-term rewards and optimize
the ranking metric offline for improved sample efficiency in a unified
Expectation-Maximization (EM) framework. We theoretically and empirically show
that the EM process guides the leaned policy to enjoy the benefit of
integration of the future reward and ranking metric, and learn without any
online interactions. Extensive offline and online experiments demonstrate the
effectiveness of our methods.</div><div><a href='http://arxiv.org/abs/2401.08959v1'>2401.08959v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00907v1")'>AlphaRank: An Artificial Intelligence Approach for Ranking and Selection
  Problems</div>
<div id='2402.00907v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T03:47:05Z</div><div>Authors: Ruihan Zhou, L. Jeff Hong, Yijie Peng</div><div style='padding-top: 10px; width: 80ex'>We introduce AlphaRank, an artificial intelligence approach to address the
fixed-budget ranking and selection (R&amp;S) problems. We formulate the sequential
sampling decision as a Markov decision process and propose a Monte Carlo
simulation-based rollout policy that utilizes classic R&amp;S procedures as base
policies for efficiently learning the value function of stochastic dynamic
programming. We accelerate online sample-allocation by using deep reinforcement
learning to pre-train a neural network model offline based on a given prior. We
also propose a parallelizable computing framework for large-scale problems,
effectively combining "divide and conquer" and "recursion" for enhanced
scalability and efficiency. Numerical experiments demonstrate that the
performance of AlphaRank is significantly improved over the base policies,
which could be attributed to AlphaRank's superior capability on the trade-off
among mean, variance, and induced correlation overlooked by many existing
policies.</div><div><a href='http://arxiv.org/abs/2402.00907v1'>2402.00907v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10038v1")'>RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization
  Method for Alignment of Large Language Models</div>
<div id='2402.10038v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T16:00:58Z</div><div>Authors: Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning from human feedback (RLHF) has been extensively
employed to align large language models with user intent. However, proximal
policy optimization (PPO) based RLHF is occasionally unstable requiring
significant hyperparameter finetuning, and computationally expensive to
maximize the estimated reward during alignment. Recently, direct preference
optimization (DPO) is proposed to address those challenges. However, DPO relies
on contrastive responses generated from human annotator and alternative LLM,
instead of the policy model, limiting the effectiveness of the RLHF. In this
paper, we addresses both challenges by systematically combining rejection
sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the
development of a supervised fine-tuned policy model (SFT). A varied set of k
responses per prompt are sampled directly from the SFT model. RS-DPO identifies
pairs of contrastive samples based on their reward distribution. Finally, we
apply DPO with the contrastive samples to align the model to human preference.
Our experiments indicate that our proposed method effectively fine-tunes LLMs
with limited resource environments, leading to improved alignment with user
intent. Furthermore, it outperforms existing methods, including RS, PPO, and
DPO.</div><div><a href='http://arxiv.org/abs/2402.10038v1'>2402.10038v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13210v1")'>Bayesian Reward Models for LLM Alignment</div>
<div id='2402.13210v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:20:59Z</div><div>Authors: Adam X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang, Haitham Bou-Ammar, Laurence Aitchison</div><div style='padding-top: 10px; width: 80ex'>To ensure that large language model (LLM) responses are helpful and
non-toxic, we usually fine-tune a reward model on human preference data. We
then select policy responses with high rewards (best-of-n sampling) or further
optimize the policy to produce responses with high rewards (reinforcement
learning from human feedback). However, this process is vulnerable to reward
overoptimization or hacking, in which the responses selected have high rewards
due to errors in the reward model rather than a genuine preference. This is
especially problematic as the prompt or response diverges from the training
data. It should be possible to mitigate these issues by training a Bayesian
reward model, which signals higher uncertainty further from the training data
distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA
(Yang et al., 2024) and found that the resulting uncertainty estimates can
successfully mitigate reward overoptimization in best-of-n sampling.</div><div><a href='http://arxiv.org/abs/2402.13210v1'>2402.13210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02479v1")'>BRAIn: Bayesian Reward-conditioned Amortized Inference for natural
  language generation from feedback</div>
<div id='2402.02479v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T13:16:29Z</div><div>Authors: Gaurav Pandey, Yatin Nandwani, Tahira Naseem, Mayank Mishra, Guangxuan Xu, Dinesh Raghu, Sachindra Joshi, Asim Munawar, Ramón Fernandez Astudillo</div><div style='padding-top: 10px; width: 80ex'>Following the success of Proximal Policy Optimization (PPO) for Reinforcement
Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood
Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that
are offline in nature and use rewards in an indirect manner. These techniques,
in particular DPO, have recently become the tools of choice for LLM alignment
due to their scalability and performance. However, they leave behind important
features of the PPO approach. Methods such as SLiC or RRHF make use of the
Reward Model (RM) only for ranking/preference, losing fine-grained information
and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce),
while methods such as DPO do not use even a separate reward model. In this
work, we propose a novel approach, named BRAIn, that re-introduces the RM as
part of a distribution matching approach.BRAIn considers the LLM distribution
conditioned on the assumption of output goodness and applies Bayes theorem to
derive an intractable posterior distribution where the RM is explicitly
represented. BRAIn then distills this posterior into an amortized inference
network through self-normalized importance sampling, leading to a scalable
offline algorithm that significantly outperforms prior art in summarization and
AntropicHH tasks. BRAIn also has interesting connections to PPO and DPO for
specific RM choices.</div><div><a href='http://arxiv.org/abs/2402.02479v1'>2402.02479v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15473v1")'>Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A
  Case-Study in E-Commerce Opinion Summarization</div>
<div id='2402.15473v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:05:06Z</div><div>Authors: Swaroop Nath, Tejpalsingh Siledar, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Harshad Khadilkar, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) has become a dominating
strategy in steering Language Models (LMs) towards human values/goals. The key
to the strategy is employing a reward model ({$\varphi$}) which can reflect a
latent reward model with humans. While this strategy has proven to be
effective, the training methodology requires a lot of human preference
annotation (usually of the order of tens of thousands) to train {$\varphi$}.
Such large-scale preference annotations can be achievable if the reward model
can be ubiquitously used. However, human values/goals are subjective and depend
on the nature of the task. This poses a challenge in collecting diverse
preferences for downstream applications. To address this, we propose a novel
methodology to infuse domain knowledge into {$\varphi$}, which reduces the size
of preference annotation required. We validate our approach in E-Commerce
Opinion Summarization, with a significant reduction in dataset size (just $940$
samples) while advancing the state-of-the-art. Our contributions include a
novel Reward Modelling technique, a new dataset (PromptOpinSumm) for Opinion
Summarization, and a human preference dataset (OpinPref). The proposed
methodology opens avenues for efficient RLHF, making it more adaptable to
diverse applications with varying human values. We release the artifacts for
usage under MIT License.</div><div><a href='http://arxiv.org/abs/2402.15473v1'>2402.15473v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01878v1")'>LiPO: Listwise Preference Optimization through Learning-to-Rank</div>
<div id='2402.01878v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:08:10Z</div><div>Authors: Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, Peter J. Liu, Xuanhui Wang</div><div style='padding-top: 10px; width: 80ex'>Aligning language models (LMs) with curated human feedback is critical to
control their behaviors in real-world applications. Several recent policy
optimization methods, such as DPO and SLiC, serve as promising alternatives to
the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In
practice, human feedback often comes in a format of a ranked list over multiple
responses to amortize the cost of reading prompt. Multiple responses can also
be ranked by reward models or AI feedback. There lacks such a study on directly
fitting upon a list of responses. In this work, we formulate the LM alignment
as a listwise ranking problem and describe the Listwise Preference Optimization
(LiPO) framework, where the policy can potentially learn more effectively from
a ranked list of plausible responses given the prompt. This view draws an
explicit connection to Learning-to-Rank (LTR), where most existing preference
optimization work can be mapped to existing ranking objectives, especially
pairwise ones. Following this connection, we provide an examination of ranking
objectives that are not well studied for LM alignment withDPO and SLiC as
special cases when list size is two. In particular, we highlight a specific
method, LiPO-{\lambda}, which leverages a state-of-the-art listwise ranking
objective and weights each preference pair in a more advanced manner. We show
that LiPO-{\lambda} can outperform DPO and SLiC by a clear margin on two
preference alignment tasks.</div><div><a href='http://arxiv.org/abs/2402.01878v1'>2402.01878v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10893v1")'>RLVF: Learning from Verbal Feedback without Overgeneralization</div>
<div id='2402.10893v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:50:24Z</div><div>Authors: Moritz Stephan, Alexander Khazatsky, Eric Mitchell, Annie S Chen, Sheryl Hsu, Archit Sharma, Chelsea Finn</div><div style='padding-top: 10px; width: 80ex'>The diversity of contexts in which large language models (LLMs) are deployed
requires the ability to modify or customize default model behaviors to
incorporate nuanced requirements and preferences. A convenient interface to
specify such model adjustments is high-level verbal feedback, such as "Don't
use emojis when drafting emails to my boss." However, while writing high-level
feedback is far simpler than collecting annotations for reinforcement learning
from human feedback (RLHF), we find that simply prompting a model with such
feedback leads to overgeneralization of the feedback to contexts where it is
not relevant. We study the problem of incorporating verbal feedback without
such overgeneralization, inspiring a new method Contextualized Critiques with
Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level
feedback to generate a small synthetic preference dataset specifying how the
feedback should (and should not) be applied. It then fine-tunes the model in
accordance with the synthetic preference data while minimizing the divergence
from the original model for prompts where the feedback does not apply. Our
experimental results indicate that our approach effectively applies verbal
feedback to relevant scenarios while preserving existing behaviors for other
contexts. For both human- and GPT-4-generated high-level feedback, C3PO
effectively adheres to the given feedback comparably to in-context baselines
while reducing overgeneralization by 30%.</div><div><a href='http://arxiv.org/abs/2402.10893v1'>2402.10893v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12017v1")'>Supervised Fine-Tuning as Inverse Reinforcement Learning</div>
<div id='2403.12017v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:52:57Z</div><div>Authors: Hao Sun</div><div style='padding-top: 10px; width: 80ex'>The prevailing approach to aligning Large Language Models (LLMs) typically
relies on human or AI feedback and assumes access to specific types of
preference datasets. In our work, we question the efficacy of such datasets and
explore various scenarios where alignment with expert demonstrations proves
more realistic. We build a sequential decision-making framework to formulate
the problem of aligning LLMs using demonstration datasets. Drawing insights
from inverse reinforcement learning and imitation learning, we introduce
various approaches for divergence minimization in the LLM alignment tasks. Our
analysis highlights the mass-covering and mode-seeking behaviors of these
different approaches. Inclusively, we examine the pros and cons of the
classical supervised fine-tuning method, elaborating on scenarios where
different methods shine.</div><div><a href='http://arxiv.org/abs/2403.12017v1'>2403.12017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02992v1")'>Decoding-time Realignment of Language Models</div>
<div id='2402.02992v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:31:28Z</div><div>Authors: Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, Mathieu Blondel</div><div style='padding-top: 10px; width: 80ex'>Aligning language models with human preferences is crucial for reducing
errors and biases in these models. Alignment techniques, such as reinforcement
learning from human feedback (RLHF), are typically cast as optimizing a
tradeoff between human preference rewards and a proximity regularization term
that encourages staying close to the unaligned model. Selecting an appropriate
level of regularization is critical: insufficient regularization can lead to
reduced model capabilities due to reward hacking, whereas excessive
regularization hinders alignment. Traditional methods for finding the optimal
regularization level require retraining multiple models with varying
regularization strengths. This process, however, is resource-intensive,
especially for large models. To address this challenge, we propose
decoding-time realignment (DeRa), a simple method to explore and evaluate
different regularization strengths in aligned models without retraining. DeRa
enables control over the degree of alignment, allowing users to smoothly
transition between unaligned and aligned models. It also enhances the
efficiency of hyperparameter tuning by enabling the identification of effective
regularization strengths using a validation dataset.</div><div><a href='http://arxiv.org/abs/2402.02992v1'>2402.02992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01694v1")'>ARGS: Alignment as Reward-Guided Search</div>
<div id='2402.01694v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T23:42:41Z</div><div>Authors: Maxim Khanov, Jirayu Burapacheep, Yixuan Li</div><div style='padding-top: 10px; width: 80ex'>Aligning large language models with human objectives is paramount, yet common
approaches including RLHF suffer from unstable and resource-intensive training.
In response to this challenge, we introduce ARGS, Alignment as Reward-Guided
Search, a novel framework that integrates alignment into the decoding process,
eliminating the need for expensive RL training. By adjusting the model's
probabilistic predictions using a reward signal, ARGS generates texts with
semantic diversity while being aligned with human preferences, offering a
promising and flexible solution for aligning language models. Notably, ARGS
demonstrates consistent enhancements in average reward compared to baselines
across diverse alignment tasks and various model dimensions. For example, under
the same greedy-based decoding strategy, our method improves the average reward
by 19.56% relative to the baseline and secures a preference or tie score of
64.33% in GPT-4 evaluation. We believe that our framework, emphasizing
decoding-time alignment, paves the way for more responsive language models in
the future. Code is publicly available at:
\url{https://github.com/deeplearning-wisc/args}.</div><div><a href='http://arxiv.org/abs/2402.01694v1'>2402.01694v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01335v2")'>Self-Play Fine-Tuning Converts Weak Language Models to Strong Language
  Models</div>
<div id='2401.01335v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T18:53:13Z</div><div>Authors: Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>Harnessing the power of human-annotated data through Supervised Fine-Tuning
(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we
delve into the prospect of growing a strong LLM out of a weak one without the
need for acquiring additional human-annotated data. We propose a new
fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a
supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,
where the LLM refines its capability by playing against instances of itself.
More specifically, the LLM generates its own training data from its previous
iterations, refining its policy by discerning these self-generated responses
from those obtained from human-annotated data. Our method progressively
elevates the LLM from a nascent model to a formidable one, unlocking the full
potential of human-annotated demonstration data for SFT. Theoretically, we
prove that the global optimum to the training objective function of our method
is achieved only when the LLM policy aligns with the target data distribution.
Empirically, we evaluate our method on several benchmark datasets including the
HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our
results show that SPIN can significantly improve the LLM's performance across a
variety of benchmarks and even outperform models trained through direct
preference optimization (DPO) supplemented with extra GPT-4 preference data.
This sheds light on the promise of self-play, enabling the achievement of
human-level performance in LLMs without the need for expert opponents. Codes
are available at https://github.com/uclaml/SPIN.</div><div><a href='http://arxiv.org/abs/2401.01335v2'>2401.01335v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04283v1")'>Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model
  with Proxy</div>
<div id='2403.04283v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T07:31:00Z</div><div>Authors: Yu Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, Bo Tang, Tianzhu Zhang, Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, Mingchuan yang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach
to ensure Large Language Models (LLMs) align with human values. However,
existing RLHF methods require a high computational cost, one main reason being
that RLHF assigns both the generation and alignment tasks to the LLM
simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the
generation and alignment processes of LLMs, achieving alignment with human
values at a much lower computational cost. We start with a novel Markov
Decision Process (MDP) designed for the alignment process and employ
Reinforcement Learning (RL) to train a streamlined proxy model that oversees
the token generation of the LLM, without altering the LLM itself. Experiments
show that our method achieves a comparable level of alignment with only 1\% of
the training parameters of other methods.</div><div><a href='http://arxiv.org/abs/2403.04283v1'>2403.04283v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07818v2")'>Differentially Private Zeroth-Order Methods for Scalable Large Language
  Model Finetuning</div>
<div id='2402.07818v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:24:15Z</div><div>Authors: Z Liu, J Lou, W Bao, Z Qin, K Ren</div><div style='padding-top: 10px; width: 80ex'>Finetuning on task-specific datasets is a widely-embraced paradigm of
harnessing the powerful capability of pretrained LLMs for various downstream
tasks. Due to the popularity of LLMs finetuning and its accompanying privacy
concerns, differentially private (DP) finetuning of pretrained LLMs has
garnered increasing attention to safeguarding the privacy of task-specific
datasets. Lying at the design core of DP LLM finetuning methods is the
satisfactory tradeoff between privacy, utility, and scalability. Most existing
methods build upon the seminal work of DP-SGD. Despite pushing the scalability
of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately
limited by the inherent inefficiency of SGD. In this paper, we investigate the
potential of DP zeroth-order methods for LLM pretraining, which avoids the
scalability bottleneck of SGD by approximating the gradient with the more
efficient zeroth-order gradient. Rather than treating the zeroth-order method
as a drop-in replacement for SGD, this paper presents a comprehensive study
both theoretically and empirically. First, we propose the stagewise DP
zeroth-order method that dynamically schedules key hyperparameters. This design
is grounded on the synergy between DP random perturbation and the gradient
approximation error of the zeroth-order method, and its effect on finetuning
trajectory. Second, we further enhance the scalability by reducing the
trainable parameters that are identified by repurposing a data-free pruning
technique requiring no additional data or extra privacy budget. We provide
theoretical analysis for both proposed methods. We conduct extensive empirical
analysis on both encoder-only masked language model and decoder-only
autoregressive language model, achieving impressive results in terms of
scalability and utility.</div><div><a href='http://arxiv.org/abs/2402.07818v2'>2402.07818v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19464v1")'>Curiosity-driven Red-teaming for Large Language Models</div>
<div id='2402.19464v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:55:03Z</div><div>Authors: Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James Glass, Akash Srivastava, Pulkit Agrawal</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) hold great potential for many natural language
applications but risk generating incorrect or toxic content. To probe when an
LLM generates unwanted content, the current paradigm is to recruit a
\textit{red team} of human testers to design input prompts (i.e., test cases)
that elicit undesirable responses from LLMs. However, relying solely on human
testers is expensive and time-consuming. Recent works automate red teaming by
training a separate red team LLM with reinforcement learning (RL) to generate
test cases that maximize the chance of eliciting undesirable responses from the
target LLM. However, current RL methods are only able to generate a small
number of effective test cases resulting in a low coverage of the span of
prompts that elicit undesirable responses from the target LLM. To overcome this
limitation, we draw a connection between the problem of increasing the coverage
of generated test cases and the well-studied approach of curiosity-driven
exploration that optimizes for novelty. Our method of curiosity-driven red
teaming (CRT) achieves greater coverage of test cases while mantaining or
increasing their effectiveness compared to existing methods. Our method, CRT
successfully provokes toxic responses from LLaMA2 model that has been heavily
fine-tuned using human preferences to avoid toxic outputs. Code is available at
\url{https://github.com/Improbable-AI/curiosity_redteam}</div><div><a href='http://arxiv.org/abs/2402.19464v1'>2402.19464v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03469v1")'>Preference-free Alignment Learning with Regularized Relevance Reward</div>
<div id='2402.03469v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:58:08Z</div><div>Authors: Sungdong Kim, Minjoon Seo</div><div style='padding-top: 10px; width: 80ex'>Learning from human preference has been considered key to aligning Large
Language Models (LLMs) with human values. However, contrary to popular belief,
our preliminary study reveals that reward models trained on human preference
datasets tend to give higher scores to long off-topic responses than short
on-topic ones. Motivated by this observation, we explore a preference-free
approach utilizing `relevance' as a key objective for alignment. On our first
attempt, we find that the relevance score obtained by a retriever alone is
vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when
we utilize the score as a reward for reinforcement learning. To mitigate it, we
integrate effective inductive biases into the vanilla relevance to regularize
each other, resulting in a mixture of reward functions: Regularized Relevance
Reward ($R^3$). $R^3$ significantly improves performance on preference
benchmarks by providing a robust reward signal. Notably, $R^3$ does not require
any human preference datasets (i.e., preference-free), outperforming
open-source reward models in improving human preference. Our analysis
demonstrates that $R^3$ has advantages in elevating human preference while
minimizing its side effects. Finally, we show the generalizability of $R^3$,
consistently improving instruction-tuned models in various backbones and sizes
without additional dataset cost. Our code is available at
https://github.com/naver-ai/RRR.</div><div><a href='http://arxiv.org/abs/2402.03469v1'>2402.03469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11253v2")'>Aligning Large Language Models by On-Policy Self-Judgment</div>
<div id='2402.11253v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T11:25:26Z</div><div>Authors: Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu</div><div style='padding-top: 10px; width: 80ex'>Existing approaches for aligning large language models with human preferences
face a trade-off that requires a separate reward model (RM) for on-policy
learning. In this paper, we present a novel alignment framework, \method{} that
(1) does on-policy learning and 2) is parameter efficient, as it does not
require an additional RM for evaluating the samples for on-policy learning. To
this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a
single model to act as both a policy and a judge. Specifically, we view the
pairwise judgment task, choosing the better response from a response pair, as a
special case of the instruction-following task. The resulting model can judge
preferences of on-the-fly responses from current policy initialized from
itself. Experimental results show the efficacy of \method{}, outperforming
baselines in preference benchmarks. We also show that the rejecting sampling by
itself can improve performance further without an additional evaluator.</div><div><a href='http://arxiv.org/abs/2402.11253v2'>2402.11253v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15018v1")'>Unintended Impacts of LLM Alignment on Global Representation</div>
<div id='2402.15018v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T23:31:22Z</div><div>Authors: Michael J. Ryan, William Held, Diyi Yang</div><div style='padding-top: 10px; width: 80ex'>Before being deployed for user-facing applications, developers align Large
Language Models (LLMs) to user preferences through a variety of procedures,
such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference
Optimization (DPO). Current evaluations of these procedures focus on benchmarks
of instruction following, reasoning, and truthfulness. However, human
preferences are not universal, and aligning to specific preference sets may
have unintended effects. We explore how alignment impacts performance along
three axes of global representation: English dialects, multilingualism, and
opinions from and about countries worldwide. Our results show that current
alignment procedures create disparities between English dialects and global
opinions. We find alignment improves capabilities in several languages. We
conclude by discussing design decisions that led to these unintended impacts
and recommendations for more equitable preference tuning.</div><div><a href='http://arxiv.org/abs/2402.15018v1'>2402.15018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00827v1")'>Self-Refinement of Language Models from External Proxy Metrics Feedback</div>
<div id='2403.00827v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T19:13:01Z</div><div>Authors: Keshav Ramji, Young-Suk Lee, Ramón Fernandez Astudillo, Md Arafat Sultan, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos</div><div style='padding-top: 10px; width: 80ex'>It is often desirable for Large Language Models (LLMs) to capture multiple
objectives when providing a response. In document-grounded response generation,
for example, agent responses are expected to be relevant to a user's query
while also being grounded in a given document. In this paper, we introduce
Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine
its own initial response along key dimensions of quality guided by external
metrics feedback, yielding an overall better final response. ProMiSe leverages
feedback on response quality through principle-specific proxy metrics, and
iteratively refines its response one principle at a time. We apply ProMiSe to
open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its
performance on document-grounded question answering datasets, MultiDoc2Dial and
QuAC, demonstrating that self-refinement improves response quality. We further
show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated
by ProMiSe yields significant performance improvements over the zero-shot
baseline as well as a supervised fine-tuned model on human annotated data.</div><div><a href='http://arxiv.org/abs/2403.00827v1'>2403.00827v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01916v2")'>AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse
  Datasets</div>
<div id='2401.01916v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T04:47:02Z</div><div>Authors: Ernest Perkowski, Rui Pan, Tuan Dung Nguyen, Yuan-Sen Ting, Sandor Kruk, Tong Zhang, Charlie O'Neill, Maja Jablonska, Zechang Sun, Michael J. Smith, Huiling Liu, Kevin Schawinski, Kartheik Iyer, Ioana Ciucă for UniverseTBD</div><div style='padding-top: 10px; width: 80ex'>We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpora -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 excel in broader question-answering scenarios due to superior
reasoning capabilities, our findings suggest that continual pre-training with
limited resources can still enhance model performance on specialized topics.
Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B
LLaMA model on a domain-specific conversational dataset, culminating in the
release of the chat-enabled AstroLLaMA for community use. Comprehensive
quantitative benchmarking is currently in progress and will be detailed in an
upcoming full paper. The model, AstroLLaMA-Chat, is now available at
https://huggingface.co/universeTBD, providing the first open-source
conversational AI tool tailored for the astronomy community.</div><div><a href='http://arxiv.org/abs/2401.01916v2'>2401.01916v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14666v1")'>SyllabusQA: A Course Logistics Question Answering Dataset</div>
<div id='2403.14666v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T03:01:14Z</div><div>Authors: Nigel Fernandez, Alexander Scarlatos, Andrew Lan</div><div style='padding-top: 10px; width: 80ex'>Automated teaching assistants and chatbots have significant potential to
reduce the workload of human instructors, especially for logistics-related
question answering, which is important to students yet repetitive for
instructors. However, due to privacy concerns, there is a lack of publicly
available datasets. We introduce SyllabusQA, an open-source dataset with 63
real course syllabi covering 36 majors, containing 5,078 open-ended course
logistics-related question-answer pairs that are diverse in both question types
and answer formats. Since many logistics-related questions contain critical
information like the date of an exam, it is important to evaluate the
factuality of answers. We benchmark several strong baselines on this task, from
large language model prompting to retrieval-augmented generation. We find that
despite performing close to humans on traditional metrics of textual
similarity, there remains a significant gap between automated approaches and
humans in terms of fact precision.</div><div><a href='http://arxiv.org/abs/2403.14666v1'>2403.14666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04798v1")'>JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using
  in-context learning with GPT and instruction-tuned Llama models</div>
<div id='2403.04798v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T12:07:18Z</div><div>Authors: Arefa, Mohammed Abbas Ansari, Chandni Saxena, Tanvir Ahmad</div><div style='padding-top: 10px; width: 80ex'>This paper presents our system development for SemEval-2024 Task 3: "The
Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively
capturing emotions in human conversations requires integrating multiple
modalities such as text, audio, and video. However, the complexities of these
diverse modalities pose challenges for developing an efficient multimodal
emotion cause analysis (ECA) system. Our proposed approach addresses these
challenges by a two-step framework. We adopt two different approaches in our
implementation. In Approach 1, we employ instruction-tuning with two separate
Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V
for conversation-level video description and employ in-context learning with
annotated conversation using GPT 3.5. Our system wins rank 4, and system
ablation experiments demonstrate that our proposed solutions achieve
significant performance gains. All the experimental codes are available on
Github.</div><div><a href='http://arxiv.org/abs/2403.04798v1'>2403.04798v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17269v2")'>Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion
  Recognition</div>
<div id='2402.17269v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T07:28:05Z</div><div>Authors: Cam-Van Thi Nguyen, Cao-Bach Nguyen, Quang-Thuy Ha, Duc-Trong Le</div><div style='padding-top: 10px; width: 80ex'>Emotion recognition in conversation (ERC) is a crucial task in natural
language processing and affective computing. This paper proposes MultiDAG+CL, a
novel approach for Multimodal Emotion Recognition in Conversation (ERC) that
employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual
features within a unified framework. The model is enhanced by Curriculum
Learning (CL) to address challenges related to emotional shifts and data
imbalance. Curriculum learning facilitates the learning process by gradually
presenting training samples in a meaningful order, thereby improving the
model's performance in handling emotional variations and data imbalance.
Experimental results on the IEMOCAP and MELD datasets demonstrate that the
MultiDAG+CL models outperform baseline models. We release the code for
MultiDAG+CL and experiments: https://github.com/vanntc711/MultiDAG-CL</div><div><a href='http://arxiv.org/abs/2402.17269v2'>2402.17269v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00782v1")'>Dense Reward for Free in Reinforcement Learning from Human Feedback</div>
<div id='2402.00782v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:10:35Z</div><div>Authors: Alex J. Chan, Hao Sun, Samuel Holt, Mihaela van der Schaar</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) has been credited as the
key advance that has allowed Large Language Models (LLMs) to effectively follow
instructions and produce useful assistance. Classically, this involves
generating completions from the LLM in response to a query before using a
separate reward model to assign a score to the full completion. As an
auto-regressive process, the LLM has to take many "actions" (selecting
individual tokens) and only receives a single, sparse reward at the end of an
episode, a setup that is known to be difficult to optimise in traditional
reinforcement learning. In this work we leverage the fact that the reward model
contains more information than just its scalar output, in particular, it
calculates an attention map over tokens as part of the transformer
architecture. We use these attention weights to redistribute the reward along
the whole completion, effectively densifying the signal and highlighting the
most important tokens, all without incurring extra computational cost or
requiring any additional modelling. We demonstrate that, theoretically, this
approach is equivalent to potential-based reward shaping, ensuring that the
optimal policy remains unchanged. Empirically, we show that it stabilises
training, accelerates the rate of learning, and, in practical cases, may lead
to better local optima.</div><div><a href='http://arxiv.org/abs/2402.00782v1'>2402.00782v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05171v1")'>Overcoming Reward Overoptimization via Adversarial Policy Optimization
  with Lightweight Uncertainty Estimation</div>
<div id='2403.05171v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T09:20:12Z</div><div>Authors: Xiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the
pervasive issue of reward over-optimization in Reinforcement Learning from
Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization
occurs when a reward model serves as an imperfect proxy for human preference,
and RL-driven policy optimization erroneously exploits reward inaccuracies. In
this paper, we begin by introducing a lightweight way to quantify uncertainties
in rewards, relying solely on the last layer embeddings of the reward model,
without the need for computationally expensive reward ensembles. AdvPO then
addresses a distributionally robust optimization problem centred around the
confidence interval of the reward model's predictions for policy improvement.
Through comprehensive experiments on the Anthropic HH and TL;DR summarization
datasets, we illustrate the efficacy of AdvPO in mitigating the
overoptimization issue, consequently resulting in enhanced performance as
evaluated through human-assisted evaluation.</div><div><a href='http://arxiv.org/abs/2403.05171v1'>2403.05171v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12187v1")'>WARM: On the Benefits of Weight Averaged Reward Models</div>
<div id='2401.12187v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:27:08Z</div><div>Authors: Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret</div><div style='padding-top: 10px; width: 80ex'>Aligning large language models (LLMs) with human preferences through
reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit
failures in the reward model (RM) to achieve seemingly high rewards without
meeting the underlying objectives. We identify two primary challenges when
designing RMs to mitigate reward hacking: distribution shifts during the RL
process and inconsistencies in human preferences. As a solution, we propose
Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then
averaging them in the weight space. This strategy follows the observation that
fine-tuned weights remain linearly mode connected when sharing the same
pre-training. By averaging weights, WARM improves efficiency compared to the
traditional ensembling of predictions, while improving reliability under
distribution shifts and robustness to preference inconsistencies. Our
experiments on summarization tasks, using best-of-N and RL methods, shows that
WARM improves the overall quality and alignment of LLM predictions; for
example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy
RL fine-tuned with a single RM.</div><div><a href='http://arxiv.org/abs/2401.12187v1'>2401.12187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16635v1")'>Improving Reinforcement Learning from Human Feedback with Efficient
  Reward Model Ensemble</div>
<div id='2401.16635v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T00:17:37Z</div><div>Authors: Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing Sun, Chuang Gan</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning from Human Feedback (RLHF) is a widely adopted
approach for aligning large language models with human values. However, RLHF
relies on a reward model that is trained with a limited amount of human
preference data, which could lead to inaccurate predictions. As a result, RLHF
may produce outputs that are misaligned with human values. To mitigate this
issue, we contribute a reward ensemble method that allows the reward model to
make more accurate predictions. As using an ensemble of large language
model-based reward models can be computationally and resource-expensive, we
explore efficient ensemble methods including linear-layer ensemble and
LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy
Optimization with our ensembled reward models, and verify that our ensemble
methods help improve the alignment performance of RLHF outputs.</div><div><a href='http://arxiv.org/abs/2401.16635v1'>2401.16635v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07319v1")'>ODIN: Disentangled Reward Mitigates Hacking in RLHF</div>
<div id='2402.07319v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T22:40:12Z</div><div>Authors: Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro</div><div style='padding-top: 10px; width: 80ex'>In this work, we study the issue of reward hacking on the response length, a
challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on
LLMs. A well-formatted, verbose but less helpful response from the LLMs can
often deceive LLMs or even human evaluators to achieve high scores. The same
issue also holds for some reward models in RL. To address the challenges in
both training and evaluation, we establish a more reliable evaluation protocol
for comparing different training configurations, which inspects the trade-off
between LLM evaluation score and response length obtained by varying training
hyperparameters. Based on this evaluation, we conduct large-scale studies,
where the results shed insights into the efficacy of hyperparameters and tricks
used in RL on mitigating length bias. We further propose to improve the reward
model by jointly training two linear heads on shared feature representations to
predict the rewards, one trained to correlate with length, and the other
trained to decorrelate with length and therefore focus more on the actual
content. We then discard the length head in RL to prevent reward hacking on
length. Experiments demonstrate that our approach almost eliminates the reward
correlation with length, and improves the obtained policy by a significant
margin.</div><div><a href='http://arxiv.org/abs/2402.07319v1'>2402.07319v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03659v3")'>Learning to Generate Explainable Stock Predictions using Self-Reflective
  Large Language Models</div>
<div id='2402.03659v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T03:18:58Z</div><div>Authors: Kelvin J. L. Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua</div><div style='padding-top: 10px; width: 80ex'>Explaining stock predictions is generally a difficult task for traditional
non-generative deep learning models, where explanations are limited to
visualizing the attention weights on important texts. Today, Large Language
Models (LLMs) present a solution to this problem, given their known
capabilities to generate human-readable explanations for their decision-making
process. However, the task of stock prediction remains challenging for LLMs, as
it requires the ability to weigh the varying impacts of chaotic social texts on
stock prices. The problem gets progressively harder with the introduction of
the explanation component, which requires LLMs to explain verbally why certain
factors are more important than the others. On the other hand, to fine-tune
LLMs for such a task, one would need expert-annotated samples of explanation
for every stock movement in the training set, which is expensive and
impractical to scale. To tackle these issues, we propose our
Summarize-Explain-Predict (SEP) framework, which utilizes a self-reflective
agent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to
generate explainable stock predictions in a fully autonomous manner. The
reflective agent learns how to explain past stock movements through
self-reasoning, while the PPO trainer trains the model to generate the most
likely explanations from input texts. The training samples for the PPO trainer
are also the responses generated during the reflective process, which
eliminates the need for human annotators. Using our SEP framework, we fine-tune
a LLM that can outperform both traditional deep-learning and LLM methods in
prediction accuracy and Matthews correlation coefficient for the stock
classification task. To justify the generalization capability of our framework,
we further test it on the portfolio construction task, and demonstrate its
effectiveness through various portfolio metrics.</div><div><a href='http://arxiv.org/abs/2402.03659v3'>2402.03659v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06179v1")'>CNN-DRL for Scalable Actions in Finance</div>
<div id='2401.06179v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:04:57Z</div><div>Authors: Sina Montazeri, Akram Mirzaeinia, Haseebullah Jumakhan, Amir Mirzaeinia</div><div style='padding-top: 10px; width: 80ex'>The published MLP-based DRL in finance has difficulties in learning the
dynamics of the environment when the action scale increases. If the buying and
selling increase to one thousand shares, the MLP agent will not be able to
effectively adapt to the environment. To address this, we designed a CNN agent
that concatenates the data from the last ninety days of the daily feature
vector to create the CNN input matrix. Our extensive experiments demonstrate
that the MLP-based agent experiences a loss corresponding to the initial
environment setup, while our designed CNN remains stable, effectively learns
the environment, and leads to an increase in rewards.</div><div><a href='http://arxiv.org/abs/2401.06179v1'>2401.06179v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03338v1")'>CNN-DRL with Shuffled Features in Finance</div>
<div id='2402.03338v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:46:28Z</div><div>Authors: Sina Montazeri, Akram Mirzaeinia, Amir Mirzaeinia</div><div style='padding-top: 10px; width: 80ex'>In prior methods, it was observed that the application of Convolutional
Neural Networks agent in Deep Reinforcement Learning to financial data resulted
in an enhanced reward. In this study, a specific permutation was applied to the
feature vector, thereby generating a CNN matrix that strategically positions
more pertinent features in close proximity. Our comprehensive experimental
evaluations unequivocally demonstrate a substantial enhancement in reward
attainment.</div><div><a href='http://arxiv.org/abs/2402.03338v1'>2402.03338v1</a></div>
</div></div>
    <div><a href="arxiv_24.html">Prev (24)</a></div>
    <div><a href="arxiv_26.html">Next (26)</a></div>
    