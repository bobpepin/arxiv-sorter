
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02465v1")'>Interpretable Time Series Models for Wastewater Modeling in Combined
  Sewer Overflows</div>
<div id='2401.02465v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T11:48:27Z</div><div>Authors: Teodor Chiaburu, Felix Biessmann</div><div style='padding-top: 10px; width: 80ex'>Climate change poses increasingly complex challenges to our society. Extreme
weather events such as floods, wild fires or droughts are becoming more
frequent, spontaneous and difficult to foresee or counteract. In this work we
specifically address the problem of sewage water polluting surface water bodies
after spilling over from rain tanks as a consequence of heavy rain events. We
investigate to what extent state-of-the-art interpretable time series models
can help predict such critical water level points, so that the excess can
promptly be redistributed across the sewage network. Our results indicate that
modern time series models can contribute to better waste water management and
prevention of environmental pollution from sewer systems. All the code and
experiments can be found in our repository:
https://github.com/TeodorChiaburu/RIWWER_TimeSeries.</div><div><a href='http://arxiv.org/abs/2401.02465v1'>2401.02465v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13304v1")'>Harmful algal bloom forecasting. A comparison between stream and batch
  learning</div>
<div id='2402.13304v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:01:11Z</div><div>Authors: Andres Molares-Ulloa, Elisabet Rocruz, Daniel Rivero, Xosé A. Padin, Rita Nolasco, Jesús Dubert, Enrique Fernandez-Blanco</div><div style='padding-top: 10px; width: 80ex'>Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from
shellfish contaminated with toxins produced by dinoflagellates. The condition,
with its widespread incidence, high morbidity rate, and persistent shellfish
toxicity, poses risks to public health and the shellfish industry. High biomass
of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs).
Monitoring and forecasting systems are crucial for mitigating HABs impact.
Predicting harmful algal blooms involves a time-series-based problem with a
strong historical seasonal component, however, recent anomalies due to changes
in meteorological and oceanographic events have been observed. Stream Learning
stands out as one of the most promising approaches for addressing
time-series-based problems with concept drifts. However, its efficacy in
predicting HABs remains unproven and needs to be tested in comparison with
Batch Learning. Historical data availability is a critical point in developing
predictive systems. In oceanography, the available data collection can have
some constrains and limitations, which has led to exploring new tools to obtain
more exhaustive time series. In this study, a machine learning workflow for
predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata,
was developed with several key advancements. Seven machine learning algorithms
were compared within two learning paradigms. Notably, the output data from
CROCO, the ocean hydrodynamic model, was employed as the primary dataset,
palliating the limitation of time-continuous historical data. This study
highlights the value of models interpretability, fair models comparison
methodology, and the incorporation of Stream Learning models. The model DoME,
with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most
effective and interpretable predictor, outperforming the other algorithms.</div><div><a href='http://arxiv.org/abs/2402.13304v1'>2402.13304v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00607v1")'>Are Synthetic Time-series Data Really not as Good as Real Data?</div>
<div id='2402.00607v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T13:59:04Z</div><div>Authors: Fanzhe Fu, Junru Chen, Jing Zhang, Carl Yang, Lvbin Ma, Yang Yang</div><div style='padding-top: 10px; width: 80ex'>Time-series data presents limitations stemming from data quality issues, bias
and vulnerabilities, and generalization problem. Integrating universal data
synthesis methods holds promise in improving generalization. However, current
methods cannot guarantee that the generator's output covers all unseen real
data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain
data synthesizing framework with time series representation learning
capability. We have developed a method based on synthetic data that enables
model training without the need for real data, surpassing the performance of
models trained with real data. Additionally, we have trained a universal
feature extractor based on our synthetic data that is applicable to all
time-series data. Our approach overcomes interference from multiple sources
rhythmic signal, noise interference, and long-period features that exceed
sampling window capabilities. Through experiments, our non-deep-learning
synthetic data enables models to achieve superior reconstruction performance
and universal explicit representation extraction without the need for real
data.</div><div><a href='http://arxiv.org/abs/2402.00607v1'>2402.00607v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02814v1")'>InjectTST: A Transformer Method of Injecting Global Information into
  Independent Channels for Long Time Series Forecasting</div>
<div id='2403.02814v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T09:33:36Z</div><div>Authors: Ce Chi, Xing Wang, Kexin Yang, Zhiyan Song, Di Jin, Lin Zhu, Chao Deng, Junlan Feng</div><div style='padding-top: 10px; width: 80ex'>Transformer has become one of the most popular architectures for multivariate
time series (MTS) forecasting. Recent Transformer-based MTS models generally
prefer channel-independent structures with the observation that channel
independence can alleviate noise and distribution drift issues, leading to more
robustness. Nevertheless, it is essential to note that channel dependency
remains an inherent characteristic of MTS, carrying valuable information.
Designing a model that incorporates merits of both channel-independent and
channel-mixing structures is a key to further improvement of MTS forecasting,
which poses a challenging conundrum. To address the problem, an injection
method for global information into channel-independent Transformer, InjectTST,
is proposed in this paper. Instead of designing a channel-mixing model
directly, we retain the channel-independent backbone and gradually inject
global information into individual channels in a selective way. A channel
identifier, a global mixing module and a self-contextual attention module are
devised in InjectTST. The channel identifier can help Transformer distinguish
channels for better representation. The global mixing module produces
cross-channel global information. Through the self-contextual attention module,
the independent channels can selectively concentrate on useful global
information without robustness degradation, and channel mixing is achieved
implicitly. Experiments indicate that InjectTST can achieve stable improvement
compared with state-of-the-art models.</div><div><a href='http://arxiv.org/abs/2403.02814v1'>2403.02814v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01000v2")'>Multivariate Probabilistic Time Series Forecasting with Correlated
  Errors</div>
<div id='2402.01000v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T20:27:19Z</div><div>Authors: Vincent Zhihao Zheng, Lijun Sun</div><div style='padding-top: 10px; width: 80ex'>Modeling the correlations among errors is closely associated with how
accurately the model can quantify predictive uncertainty in probabilistic time
series forecasting. Recent multivariate models have made significant progress
in accounting for contemporaneous correlations among errors, while a common
assumption on these errors is that they are temporally independent for the sake
of statistical simplicity. However, real-world observations often deviate from
this assumption, since errors usually exhibit substantial autocorrelation due
to various factors such as the exclusion of temporally correlated covariates.
In this work, we propose an efficient method, based on a low-rank-plus-diagonal
parameterization of the covariance matrix, which can effectively characterize
the autocorrelation of errors. The proposed method possesses several desirable
properties: the complexity does not scale with the number of time series, the
resulting covariance can be used for calibrating predictions, and it can
seamlessly integrate with any model with Gaussian-distributed errors. We
empirically demonstrate these properties using two distinct neural forecasting
models-GPVar and Transformer. Our experimental results confirm the
effectiveness of our method in enhancing predictive accuracy and the quality of
uncertainty quantification on multiple real-world datasets.</div><div><a href='http://arxiv.org/abs/2402.01000v2'>2402.01000v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14684v1")'>Adaptive time series forecasting with markovian variance switching</div>
<div id='2402.14684v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:40:55Z</div><div>Authors: Baptiste Abélès, Joseph de Vilmarest, Olivier Wintemberger</div><div style='padding-top: 10px; width: 80ex'>Adaptive time series forecasting is essential for prediction under regime
changes. Several classical methods assume linear Gaussian state space model
(LGSSM) with variances constant in time. However, there are many real-world
processes that cannot be captured by such models. We consider a state-space
model with Markov switching variances. Such dynamical systems are usually
intractable because of their computational complexity increasing exponentially
with time; Variational Bayes (VB) techniques have been applied to this problem.
In this paper, we propose a new way of estimating variances based on online
learning theory; we adapt expert aggregation methods to learn the variances
over time. We apply the proposed method to synthetic data and to the problem of
electricity load forecasting. We show that this method is robust to
misspecification and outperforms traditional expert aggregation.</div><div><a href='http://arxiv.org/abs/2402.14684v1'>2402.14684v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12767v1")'>When and How: Learning Identifiable Latent States for Nonstationary Time
  Series Forecasting</div>
<div id='2402.12767v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T07:16:12Z</div><div>Authors: Zijian Li, Ruichu Cai, Zhenhui Yang, Haiqin Huang, Guangyi Chen, Yifan Shen, Zhengming Chen, Xiangchen Song, Zhifeng Hao, Kun Zhang</div><div style='padding-top: 10px; width: 80ex'>Temporal distribution shifts are ubiquitous in time series data. One of the
most popular methods assumes that the temporal distribution shift occurs
uniformly to disentangle the stationary and nonstationary dependencies. But
this assumption is difficult to meet, as we do not know when the distribution
shifts occur. To solve this problem, we propose to learn IDentifiable latEnt
stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we
further disentangle the stationary and nonstationary latent states via
sufficient observation assumption to learn how the latent states change.
Specifically, we formalize the causal process with environment-irrelated
stationary and environment-related nonstationary variables. Under mild
conditions, we show that latent environments and stationary/nonstationary
variables are identifiable. Based on these theories, we devise the IDEA model,
which incorporates an autoregressive hidden Markov model to estimate latent
environments and modular prior networks to identify latent states. The IDEA
model outperforms several latest nonstationary forecasting methods on various
benchmark datasets, highlighting its advantages in real-world scenarios.</div><div><a href='http://arxiv.org/abs/2402.12767v1'>2402.12767v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07231v3")'>Use of Prior Knowledge to Discover Causal Additive Models with
  Unobserved Variables and its Application to Time Series Data</div>
<div id='2401.07231v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T08:32:32Z</div><div>Authors: Takashi Nicholas Maeda, Shohei Shimizu</div><div style='padding-top: 10px; width: 80ex'>This paper proposes two methods for causal additive models with unobserved
variables (CAM-UV). CAM-UV assumes that the causal functions take the form of
generalized additive models and that latent confounders are present. First, we
propose a method that leverages prior knowledge for efficient causal discovery.
Then, we propose an extension of this method for inferring causality in time
series data. The original CAM-UV algorithm differs from other existing causal
function models in that it does not seek the causal order between observed
variables, but rather aims to identify the causes for each observed variable.
Therefore, the first proposed method in this paper utilizes prior knowledge,
such as understanding that certain variables cannot be causes of specific
others. Moreover, by incorporating the prior knowledge that causes precedes
their effects in time, we extend the first algorithm to the second method for
causal discovery in time series data. We validate the first proposed method by
using simulated data to demonstrate that the accuracy of causal discovery
increases as more prior knowledge is accumulated. Additionally, we test the
second proposed method by comparing it with existing time series causal
discovery methods, using both simulated data and real-world data.</div><div><a href='http://arxiv.org/abs/2401.07231v3'>2401.07231v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12331v1")'>Generating Survival Interpretable Trajectories and Data</div>
<div id='2402.12331v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:02:10Z</div><div>Authors: Andrei V. Konstantinov, Stanislav R. Kirpichenko, Lev V. Utkin</div><div style='padding-top: 10px; width: 80ex'>A new model for generating survival trajectories and data based on applying
an autoencoder of a specific structure is proposed. It solves three tasks.
First, it provides predictions in the form of the expected event time and the
survival function for a new generated feature vector on the basis of the Beran
estimator. Second, the model generates additional data based on a given
training set that would supplement the original dataset. Third, the most
important, it generates a prototype time-dependent trajectory for an object,
which characterizes how features of the object could be changed to achieve a
different time to an event. The trajectory can be viewed as a type of the
counterfactual explanation. The proposed model is robust during training and
inference due to a specific weighting scheme incorporating into the variational
autoencoder. The model also determines the censored indicators of new generated
data by solving a classification task. The paper demonstrates the efficiency
and properties of the proposed model using numerical experiments on synthetic
and real datasets. The code of the algorithm implementing the proposed model is
publicly available.</div><div><a href='http://arxiv.org/abs/2402.12331v1'>2402.12331v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05406v1")'>Considering Nonstationary within Multivariate Time Series with
  Variational Hierarchical Transformer for Forecasting</div>
<div id='2403.05406v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T16:04:36Z</div><div>Authors: Muyao Wang, Wenchao Chen, Bo Chen</div><div style='padding-top: 10px; width: 80ex'>The forecasting of Multivariate Time Series (MTS) has long been an important
but challenging task. Due to the non-stationary problem across long-distance
time steps, previous studies primarily adopt stationarization method to
attenuate the non-stationary problem of the original series for better
predictability. However, existing methods always adopt the stationarized
series, which ignores the inherent non-stationarity, and has difficulty in
modeling MTS with complex distributions due to the lack of stochasticity. To
tackle these problems, we first develop a powerful hierarchical probabilistic
generative module to consider the non-stationarity and stochastic
characteristics within MTS, and then combine it with transformer for a
well-defined variational generative dynamic model named Hierarchical Time
series Variational Transformer (HTV-Trans), which recovers the intrinsic
non-stationary information into temporal dependencies. Being a powerful
probabilistic model, HTV-Trans is utilized to learn expressive representations
of MTS and applied to forecasting tasks. Extensive experiments on diverse
datasets show the efficiency of HTV-Trans on MTS forecasting tasks</div><div><a href='http://arxiv.org/abs/2403.05406v1'>2403.05406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12722v1")'>Structural Knowledge Informed Continual Multivariate Time Series
  Forecasting</div>
<div id='2402.12722v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T05:11:20Z</div><div>Authors: Zijie Pan, Yushan Jiang, Dongjin Song, Sahil Garg, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka</div><div style='padding-top: 10px; width: 80ex'>Recent studies in multivariate time series (MTS) forecasting reveal that
explicitly modeling the hidden dependencies among different time series can
yield promising forecasting performance and reliable explanations. However,
modeling variable dependencies remains underexplored when MTS is continuously
accumulated under different regimes (stages). Due to the potential distribution
and dependency disparities, the underlying model may encounter the catastrophic
forgetting problem, i.e., it is challenging to memorize and infer different
types of variable dependencies across different regimes while maintaining
forecasting performance. To address this issue, we propose a novel Structural
Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS
forecasting within a continual learning paradigm, which leverages structural
knowledge to steer the forecasting model toward identifying and adapting to
different regimes, and selects representative MTS samples from each regime for
memory replay. Specifically, we develop a forecasting model based on graph
structure learning, where a consistency regularization scheme is imposed
between the learned variable dependencies and the structural knowledge while
optimizing the forecasting objective over the MTS data. As such, MTS
representations learned in each regime are associated with distinct structural
knowledge, which helps the model memorize a variety of conceivable scenarios
and results in accurate forecasts in the continual learning context. Meanwhile,
we develop a representation-matching memory replay scheme that maximizes the
temporal coverage of MTS data to efficiently preserve the underlying temporal
dynamics and dependency structures of each regime. Thorough empirical studies
on synthetic and real-world benchmarks validate SKI-CL's efficacy and
advantages over the state-of-the-art for continual MTS forecasting tasks.</div><div><a href='http://arxiv.org/abs/2402.12722v1'>2402.12722v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14973v1")'>Discovering group dynamics in synchronous time series via hierarchical
  recurrent switching-state models</div>
<div id='2401.14973v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T16:06:01Z</div><div>Authors: Michael Wojnowicz, Preetish Rath, Eric Miller, Jeffrey Miller, Clifford Hancock, Meghan O'Donovan, Seth Elkin-Frankston, Thaddeus Brunye, Michael C. Hughes</div><div style='padding-top: 10px; width: 80ex'>We seek to model a collection of time series arising from multiple entities
interacting over the same time period. Recent work focused on modeling
individual time series is inadequate for our intended applications, where
collective system-level behavior influences the trajectories of individual
entities. To address such problems, we present a new hierarchical
switching-state model that can be trained in an unsupervised fashion to
simultaneously explain both system-level and individual-level dynamics. We
employ a latent system-level discrete state Markov chain that drives latent
entity-level chains which in turn govern the dynamics of each observed time
series. Feedback from the observations to the chains at both the entity and
system levels improves flexibility via context-dependent state transitions. Our
hierarchical switching recurrent dynamical models can be learned via
closed-form variational coordinate ascent updates to all latent chains that
scale linearly in the number of individual time series. This is asymptotically
no more costly than fitting separate models for each entity. Experiments on
synthetic and real datasets show that our model can produce better forecasts of
future entity behavior than existing methods. Moreover, the availability of
latent state chains at both the entity and system level enables interpretation
of group dynamics.</div><div><a href='http://arxiv.org/abs/2401.14973v1'>2401.14973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18995v1")'>Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous
  Overdispersed Count Time Series</div>
<div id='2402.18995v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T09:46:47Z</div><div>Authors: Rui Huang, Sikun Yang, Heinz Koeppl</div><div style='padding-top: 10px; width: 80ex'>Modeling count-valued time series has been receiving increasing attention
since count time series naturally arise in physical and social domains. Poisson
gamma dynamical systems (PGDSs) are newly-developed methods, which can well
capture the expressive latent transition structure and bursty dynamics behind
count sequences. In particular, PGDSs demonstrate superior performance in terms
of data imputation and prediction, compared with canonical linear dynamical
system (LDS) based methods. Despite these advantages, PGDS cannot capture the
heterogeneous overdispersed behaviours of the underlying dynamic processes. To
mitigate this defect, we propose a negative-binomial-randomized gamma Markov
process, which not only significantly improves the predictive performance of
the proposed dynamical system, but also facilitates the fast convergence of the
inference algorithm. Moreover, we develop methods to estimate both
factor-structured and graph-structured transition dynamics, which enable us to
infer more explainable latent structure, compared with PGDSs. Finally, we
demonstrate the explainable latent structure learned by the proposed method,
and show its superior performance in imputing missing data and forecasting
future observations, compared with the related models.</div><div><a href='http://arxiv.org/abs/2402.18995v1'>2402.18995v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16297v1")'>Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics</div>
<div id='2402.16297v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T04:39:01Z</div><div>Authors: Jiahao Wang, Sikun Yang, Heinz Koeppl, Xiuzhen Cheng, Pengfei Hu, Guoming Zhang</div><div style='padding-top: 10px; width: 80ex'>Bayesian methodologies for handling count-valued time series have gained
prominence due to their ability to infer interpretable latent structures and to
estimate uncertainties, and thus are especially suitable for dealing with noisy
and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical
Systems (PGDSs) are proven to be effective in capturing the evolving dynamics
underlying observed count sequences. However, the state-of-the-art PGDS still
falls short in capturing the time-varying transition dynamics that are commonly
observed in real-world count time series. To mitigate this limitation, a
non-stationary PGDS is proposed to allow the underlying transition matrices to
evolve over time, and the evolving transition matrices are modeled by
sophisticatedly-designed Dirichlet Markov chains. Leveraging
Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and
efficient Gibbs sampler is developed to perform posterior simulation.
Experiments show that, in comparison with related models, the proposed
non-stationary PGDS achieves improved predictive performance due to its
capacity to learn non-stationary dependency structure captured by the
time-evolving transition matrices.</div><div><a href='http://arxiv.org/abs/2402.16297v1'>2402.16297v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02860v2")'>Framework for Variable-lag Motif Following Relation Inference In Time
  Series using Matrix Profile analysis</div>
<div id='2401.02860v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T15:32:24Z</div><div>Authors: Naaek Chinpattanakarn, Chainarong Amornbunchornvej</div><div style='padding-top: 10px; width: 80ex'>Knowing who follows whom and what patterns they are following are crucial
steps to understand collective behaviors (e.g. a group of human, a school of
fish, or a stock market). Time series is one of resources that can be used to
get insight regarding following relations. However, the concept of following
patterns or motifs and the solution to find them in time series are not
obvious. In this work, we formalize a concept of following motifs between two
time series and present a framework to infer following patterns between two
time series. The framework utilizes one of efficient and scalable methods to
retrieve motifs from time series called the Matrix Profile Method. We compare
our proposed framework with several baselines. The framework performs better
than baselines in the simulation datasets. In the dataset of sound recording,
the framework is able to retrieve the following motifs within a pair of time
series that two singers sing following each other. In the cryptocurrency
dataset, the framework is capable of capturing the following motifs within a
pair of time series from two digital currencies, which implies that the values
of one currency follow the values of another currency patterns. Our framework
can be utilized in any field of time series to get insight regarding following
patterns between time series.</div><div><a href='http://arxiv.org/abs/2401.02860v2'>2401.02860v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08672v1")'>Model Assessment and Selection under Temporal Distribution Shift</div>
<div id='2402.08672v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:54:08Z</div><div>Authors: Elise Han, Chengpiao Huang, Kaizheng Wang</div><div style='padding-top: 10px; width: 80ex'>We investigate model assessment and selection in a changing environment, by
synthesizing datasets from both the current time period and historical epochs.
To tackle unknown and potentially arbitrary temporal distribution shift, we
develop an adaptive rolling window approach to estimate the generalization
error of a given model. This strategy also facilitates the comparison between
any two candidate models by estimating the difference of their generalization
errors. We further integrate pairwise comparisons into a single-elimination
tournament, achieving near-optimal model selection from a collection of
candidates. Theoretical analyses and numerical experiments demonstrate the
adaptivity of our proposed methods to the non-stationarity in data.</div><div><a href='http://arxiv.org/abs/2402.08672v1'>2402.08672v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05446v1")'>An Improved Algorithm for Learning Drifting Discrete Distributions</div>
<div id='2403.05446v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T16:54:27Z</div><div>Authors: Alessio Mazzetto</div><div style='padding-top: 10px; width: 80ex'>We present a new adaptive algorithm for learning discrete distributions under
distribution drift. In this setting, we observe a sequence of independent
samples from a discrete distribution that is changing over time, and the goal
is to estimate the current distribution. Since we have access to only a single
sample for each time step, a good estimation requires a careful choice of the
number of past samples to use. To use more samples, we must resort to samples
further in the past, and we incur a drift error due to the bias introduced by
the change in distribution. On the other hand, if we use a small number of past
samples, we incur a large statistical error as the estimation has a high
variance. We present a novel adaptive algorithm that can solve this trade-off
without any prior knowledge of the drift. Unlike previous adaptive results, our
algorithm characterizes the statistical error using data-dependent bounds. This
technicality enables us to overcome the limitations of the previous work that
require a fixed finite support whose size is known in advance and that cannot
change over time. Additionally, we can obtain tighter bounds depending on the
complexity of the drifting distribution, and also consider distributions with
infinite support.</div><div><a href='http://arxiv.org/abs/2403.05446v1'>2403.05446v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01139v1")'>Online conformal prediction with decaying step sizes</div>
<div id='2402.01139v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T04:42:09Z</div><div>Authors: Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</div><div style='padding-top: 10px; width: 80ex'>We introduce a method for online conformal prediction with decaying step
sizes. Like previous methods, ours possesses a retrospective guarantee of
coverage for arbitrary sequences. However, unlike previous methods, we can
simultaneously estimate a population quantile when it exists. Our theory and
experiments indicate substantially improved practical properties: in
particular, when the distribution is stable, the coverage is close to the
desired level for every time point, not just on average over the observed
sequence.</div><div><a href='http://arxiv.org/abs/2402.01139v1'>2402.01139v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00028v1")'>Lower Bounds for Differential Privacy Under Continual Observation and
  Online Threshold Queries</div>
<div id='2403.00028v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T16:45:54Z</div><div>Authors: Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, Uri Stemmer</div><div style='padding-top: 10px; width: 80ex'>One of the most basic problems for studying the "price of privacy over time"
is the so called private counter problem, introduced by Dwork et al. (2010) and
Chan et al. (2010). In this problem, we aim to track the number of events that
occur over time, while hiding the existence of every single event. More
specifically, in every time step $t\in[T]$ we learn (in an online fashion) that
$\Delta_t\geq 0$ new events have occurred, and must respond with an estimate
$n_t\approx\sum_{j=1}^t \Delta_j$. The privacy requirement is that all of the
outputs together, across all time steps, satisfy event level differential
privacy. The main question here is how our error needs to depend on the total
number of time steps $T$ and the total number of events $n$. Dwork et al.
(2015) showed an upper bound of $O\left(\log(T)+\log^2(n)\right)$, and
Henzinger et al. (2023) showed a lower bound of $\Omega\left(\min\{\log n, \log
T\}\right)$. We show a new lower bound of $\Omega\left(\min\{n,\log
T\}\right)$, which is tight w.r.t. the dependence on $T$, and is tight in the
sparse case where $\log^2 n=O(\log T)$. Our lower bound has the following
implications:
  $\bullet$ We show that our lower bound extends to the "online thresholds
problem", where the goal is to privately answer many "quantile queries" when
these queries are presented one-by-one. This resolves an open question of Bun
et al. (2017).
  $\bullet$ Our lower bound implies, for the first time, a separation between
the number of mistakes obtainable by a private online learner and a non-private
online learner. This partially resolves a COLT'22 open question published by
Sanyal and Ramponi.
  $\bullet$ Our lower bound also yields the first separation between the
standard model of private online learning and a recently proposed relaxed
variant of it, called private online prediction.</div><div><a href='http://arxiv.org/abs/2403.00028v1'>2403.00028v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00796v1")'>Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes:
  Functional and Augmented Data Structures in Financial Forecasting</div>
<div id='2403.00796v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:09:45Z</div><div>Authors: Narayan Tondapu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we explore the application of Gaussian Processes (GPs) for
predicting mean-reverting time series with an underlying structure, using
relatively unexplored functional and augmented data structures. While many
conventional forecasting methods concentrate on the short-term dynamics of time
series data, GPs offer the potential to forecast not just the average
prediction but the entire probability distribution over a future trajectory.
This is particularly beneficial in financial contexts, where accurate
predictions alone may not suffice if incorrect volatility assessments lead to
capital losses. Moreover, in trade selection, GPs allow for the forecasting of
multiple Sharpe ratios adjusted for transaction costs, aiding in
decision-making. The functional data representation utilized in this study
enables longer-term predictions by leveraging information from previous years,
even as the forecast moves away from the current year's training data.
Additionally, the augmented representation enriches the training set by
incorporating multiple targets for future points in time, facilitating
long-term predictions. Our implementation closely aligns with the methodology
outlined in, which assessed effectiveness on commodity futures. However, our
testing methodology differs. Instead of real data, we employ simulated data
with similar characteristics. We construct a testing environment to evaluate
both data representations and models under conditions of increasing noise, fat
tails, and inappropriate kernels-conditions commonly encountered in practice.
By simulating data, we can compare our forecast distribution over time against
a full simulation of the actual distribution of our test set, thereby reducing
the inherent uncertainty in testing time series models on real data. We enable
feature prediction through augmentation and employ sub-sampling to ensure the
feasibility of GPs.</div><div><a href='http://arxiv.org/abs/2403.00796v1'>2403.00796v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03006v2")'>The Rise of Diffusion Models in Time-Series Forecasting</div>
<div id='2401.03006v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T11:35:10Z</div><div>Authors: Caspar Meijer, Lydia Y. Chen</div><div style='padding-top: 10px; width: 80ex'>This survey delves into the application of diffusion models in time-series
forecasting. Diffusion models are demonstrating state-of-the-art results in
various fields of generative AI. The paper includes comprehensive background
information on diffusion models, detailing their conditioning methods and
reviewing their use in time-series forecasting. The analysis covers 11 specific
time-series implementations, the intuition and theory behind them, the
effectiveness on different datasets, and a comparison among each other. Key
contributions of this work are the thorough exploration of diffusion models'
applications in time-series forecasting and a chronologically ordered overview
of these models. Additionally, the paper offers an insightful discussion on the
current state-of-the-art in this domain and outlines potential future research
directions. This serves as a valuable resource for researchers in AI and
time-series analysis, offering a clear view of the latest advancements and
future potential of diffusion models.</div><div><a href='http://arxiv.org/abs/2401.03006v2'>2401.03006v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05751v2")'>MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided
  Learning Process</div>
<div id='2403.05751v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T01:15:03Z</div><div>Authors: Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, Jiang Bian</div><div style='padding-top: 10px; width: 80ex'>Recently, diffusion probabilistic models have attracted attention in
generative time series forecasting due to their remarkable capacity to generate
high-fidelity samples. However, the effective utilization of their strong
modeling ability in the probabilistic time series forecasting task remains an
open question, partially due to the challenge of instability arising from their
stochastic nature. To address this challenge, we introduce a novel
Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves
state-of-the-art predictive performance by leveraging the inherent granularity
levels within the data as given targets at intermediate diffusion steps to
guide the learning process of diffusion models. The way to construct the
targets is motivated by the observation that the forward process of the
diffusion model, which sequentially corrupts the data distribution to a
standard normal distribution, intuitively aligns with the process of smoothing
fine-grained data into a coarse-grained representation, both of which result in
a gradual loss of fine distribution features. In the study, we derive a novel
multi-granularity guidance diffusion loss function and propose a concise
implementation method to effectively utilize coarse-grained data across various
granularity levels. More importantly, our approach does not rely on additional
external data, making it versatile and applicable across various domains.
Extensive experiments conducted on real-world datasets demonstrate that our
MG-TSD model outperforms existing time series prediction methods.</div><div><a href='http://arxiv.org/abs/2403.05751v2'>2403.05751v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02682v1")'>Time Weaver: A Conditional Time Series Generation Model</div>
<div id='2403.02682v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:10:22Z</div><div>Authors: Sai Shankar Narasimhan, Shubhankar Agarwal, Oguzhan Akcin, Sujay Sanghavi, Sandeep Chinchali</div><div style='padding-top: 10px; width: 80ex'>Imagine generating a city's electricity demand pattern based on weather, the
presence of an electric vehicle, and location, which could be used for capacity
planning during a winter freeze. Such real-world time series are often enriched
with paired heterogeneous contextual metadata (weather, location, etc.).
Current approaches to time series generation often ignore this paired metadata,
and its heterogeneity poses several practical challenges in adapting existing
conditional generation approaches from the image, audio, and video domains to
the time series domain. To address this gap, we introduce Time Weaver, a novel
diffusion-based model that leverages the heterogeneous metadata in the form of
categorical, continuous, and even time-variant variables to significantly
improve time series generation. Additionally, we show that naive extensions of
standard evaluation metrics from the image to the time series domain are
insufficient. These metrics do not penalize conditional generation approaches
for their poor specificity in reproducing the metadata-specific features in the
generated time series. Thus, we innovate a novel evaluation metric that
accurately captures the specificity of conditional generation and the realism
of the generated time series. We show that Time Weaver outperforms
state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by
up to 27% in downstream classification tasks on real-world energy, medical, air
quality, and traffic data sets.</div><div><a href='http://arxiv.org/abs/2403.02682v1'>2403.02682v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13548v1")'>DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of
  EV Charging Load</div>
<div id='2402.13548v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T06:07:33Z</div><div>Authors: Siyang Li, Hui Xiong, Yize Chen</div><div style='padding-top: 10px; width: 80ex'>Due to the vast electric vehicle (EV) penetration to distribution grid,
charging load forecasting is essential to promote charging station operation
and demand-side management.However, the stochastic charging behaviors and
associated exogenous factors render future charging load patterns quite
volatile and hard to predict. Accordingly, we devise a novel Diffusion model
termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can
explicitly approximate the predictive load distribution conditioned on
historical data and related covariates. Specifically, we leverage a denoising
diffusion model, which can progressively convert the Gaussian prior to real
time-series data by learning a reversal of the diffusion process. Besides, we
couple such diffusion model with a cross-attention-based conditioning mechanism
to execute conditional generation for possible charging demand profiles. We
also propose a task-informed fine-tuning technique to better adapt DiffPLF to
the probabilistic time-series forecasting task and acquire more accurate and
reliable predicted intervals. Finally, we conduct multiple experiments to
validate the superiority of DiffPLF to predict complex temporal patterns of
erratic charging load and carry out controllable generation based on certain
covariate. Results demonstrate that we can attain a notable rise of 39.58% and
49.87% on MAE and CRPS respectively compared to the conventional method.</div><div><a href='http://arxiv.org/abs/2402.13548v1'>2402.13548v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.09261v1")'>MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series
  Forecasting</div>
<div id='2401.09261v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T15:12:11Z</div><div>Authors: Zongjiang Shang, Ling Chen</div><div style='padding-top: 10px; width: 80ex'>Demystifying interactions between temporal patterns of different scales is
fundamental to precise long-range time series forecasting. However, previous
works lack the ability to model high-order interactions. To promote more
comprehensive pattern interaction modeling for long-range time series
forecasting, we propose a Multi-Scale Hypergraph Transformer (MSHyper)
framework. Specifically, a multi-scale hypergraph is introduced to provide
foundations for modeling high-order pattern interactions. Then by treating
hyperedges as nodes, we also build a hyperedge graph to enhance hypergraph
modeling. In addition, a tri-stage message passing mechanism is introduced to
aggregate pattern information and learn the interaction strength between
temporal patterns of different scales. Extensive experiments on five real-world
datasets demonstrate that MSHyper achieves state-of-the-art performance,
reducing prediction errors by an average of 8.73% and 7.15% over the best
baseline in MSE and MAE, respectively.</div><div><a href='http://arxiv.org/abs/2401.09261v1'>2401.09261v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07435v1")'>Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and
  IV Models for GBP/USD and EUR/GBP Pairs</div>
<div id='2402.07435v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T06:29:57Z</div><div>Authors: Narayan Tondapu</div><div style='padding-top: 10px; width: 80ex'>In this study, we examine the fluctuation in the value of the Great Britain
Pound (GBP). We focus particularly on its relationship with the United States
Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15,
2018, to June 15, 2023, we apply various mathematical models to assess their
effectiveness in predicting the 20-day variation in the pairs' daily returns.
Our analysis involves the implementation of Exponentially Weighted Moving
Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity
(GARCH) models, and Implied Volatility (IV) models. To evaluate their
performance, we compare the accuracy of their predictions using Root Mean
Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the
intricacies of GARCH models, examining their statistical characteristics when
applied to the provided dataset. Our findings suggest the existence of
asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for
the GBP/USD pair. Additionally, we observe that GARCH-type models better fit
the data when assuming residuals follow a standard t-distribution rather than a
standard normal distribution. Furthermore, we investigate the efficacy of
different forecasting techniques within GARCH-type models. Comparing rolling
window forecasts to expanding window forecasts, we find no definitive
superiority in either approach across the tested scenarios. Our experiments
reveal that for the GBP/USD pair, the most accurate volatility forecasts stem
from the utilization of GARCH models employing a rolling window methodology.
Conversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH
models and Ordinary Least Squares (OLS) models incorporating the annualized
implied volatility of the exchange rate as an independent variable.</div><div><a href='http://arxiv.org/abs/2402.07435v1'>2402.07435v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17042v1")'>Forecasting VIX using Bayesian Deep Learning</div>
<div id='2401.17042v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T14:23:01Z</div><div>Authors: Héctor J. Hortúa, Andrés Mora-Valencia</div><div style='padding-top: 10px; width: 80ex'>Recently, deep learning techniques are gradually replacing traditional
statistical and machine learning models as the first choice for price
forecasting tasks. In this paper, we leverage probabilistic deep learning for
inferring the volatility index VIX. We employ the probabilistic counterpart of
WaveNet, Temporal Convolutional Network (TCN), and Transformers. We show that
TCN outperforms all models with an RMSE around 0.189. In addition, it has been
well known that modern neural networks provide inaccurate uncertainty
estimates. For solving this problem, we use the standard deviation scaling to
calibrate the networks. Furthermore, we found out that MNF with Gaussian prior
outperforms Reparameterization Trick and Flipout models in terms of precision
and uncertainty predictions. Finally, we claim that MNF with Cauchy and
LogUniform prior distributions yield well calibrated TCN and WaveNet networks
being the former that best infer the VIX values.</div><div><a href='http://arxiv.org/abs/2401.17042v1'>2401.17042v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06642v1")'>From GARCH to Neural Network for Volatility Forecast</div>
<div id='2402.06642v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T07:11:13Z</div><div>Authors: Pengfei Zhao, Haoren Zhu, Wilfred Siu Hung NG, Dik Lun Lee</div><div style='padding-top: 10px; width: 80ex'>Volatility, as a measure of uncertainty, plays a crucial role in numerous
financial activities such as risk management. The Econometrics and Machine
Learning communities have developed two distinct approaches for financial
volatility forecasting: the stochastic approach and the neural network (NN)
approach. Despite their individual strengths, these methodologies have
conventionally evolved in separate research trajectories with little
interaction between them. This study endeavors to bridge this gap by
establishing an equivalence relationship between models of the GARCH family and
their corresponding NN counterparts. With the equivalence relationship
established, we introduce an innovative approach, named GARCH-NN, for
constructing NN-based volatility models. It obtains the NN counterparts of
GARCH models and integrates them as components into an established NN
architecture, thereby seamlessly infusing volatility stylized facts (SFs)
inherent in the GARCH models into the neural network. We develop the GARCH-LSTM
model to showcase the power of the GARCH-NN approach. Experiment results
validate that amalgamating the NN counterparts of the GARCH family models into
established NN models leads to enhanced outcomes compared to employing the
stochastic and NN models in isolation.</div><div><a href='http://arxiv.org/abs/2402.06642v1'>2402.06642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00746v1")'>A time-stepping deep gradient flow method for option pricing in (rough)
  diffusion models</div>
<div id='2403.00746v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T18:46:26Z</div><div>Authors: Antonis Papapantoleon, Jasper Rou</div><div style='padding-top: 10px; width: 80ex'>We develop a novel deep learning approach for pricing European options in
diffusion models, that can efficiently handle high-dimensional problems
resulting from Markovian approximations of rough volatility models. The option
pricing partial differential equation is reformulated as an energy minimization
problem, which is approximated in a time-stepping fashion by deep artificial
neural networks. The proposed scheme respects the asymptotic behavior of option
prices for large levels of moneyness, and adheres to a priori known bounds for
option prices. The accuracy and efficiency of the proposed method is assessed
in a series of numerical examples, with particular focus in the lifted Heston
model.</div><div><a href='http://arxiv.org/abs/2403.00746v1'>2403.00746v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06740v1")'>A deep implicit-explicit minimizing movement method for option pricing
  in jump-diffusion models</div>
<div id='2401.06740v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:21:01Z</div><div>Authors: Emmanuil H. Georgoulis, Antonis Papapantoleon, Costas Smaragdakis</div><div style='padding-top: 10px; width: 80ex'>We develop a novel deep learning approach for pricing European basket options
written on assets that follow jump-diffusion dynamics. The option pricing
problem is formulated as a partial integro-differential equation, which is
approximated via a new implicit-explicit minimizing movement time-stepping
approach, involving approximation by deep, residual-type Artificial Neural
Networks (ANNs) for each time step. The integral operator is discretized via
two different approaches: a) a sparse-grid Gauss--Hermite approximation
following localised coordinate axes arising from singular value decompositions,
and b) an ANN-based high-dimensional special-purpose quadrature rule.
Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of
the solution for large values of the underlyings and also leads to consistent
outputs with respect to a priori known qualitative properties of the solution.
The performance and robustness with respect to the dimension of the methods are
assessed in a series of numerical experiments involving the Merton
jump-diffusion model.</div><div><a href='http://arxiv.org/abs/2401.06740v1'>2401.06740v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05441v1")'>Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity
  Prices</div>
<div id='2403.05441v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T16:51:27Z</div><div>Authors: Daniel Nickelsen, Gernot Müller</div><div style='padding-top: 10px; width: 80ex'>We present a first study of Bayesian forecasting of electricity prices traded
on the German continuous intraday market which fully incorporates parameter
uncertainty. Our target variable is the IDFull price index, forecasts are given
in terms of posterior predictive distributions. For validation we use the
exceedingly volatile electricity prices of 2022, which have hardly been the
subject of forecasting studies before. As a benchmark model, we use all
available intraday transactions at the time of forecast creation to compute a
current value for the IDFull. According to the weak-form efficiency hypothesis,
it would not be possible to significantly improve this benchmark built from
last price information. We do, however, observe statistically significant
improvement in terms of both point measures and probability scores. Finally, we
challenge the declared gold standard of using LASSO for feature selection in
electricity price forecasting by presenting strong statistical evidence that
Orthogonal Matching Pursuit (OMP) leads to better forecasting performance.</div><div><a href='http://arxiv.org/abs/2403.05441v1'>2403.05441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13870v1")'>Generative Probabilistic Time Series Forecasting and Applications in
  Grid Operations</div>
<div id='2402.13870v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T15:23:21Z</div><div>Authors: Xinyi Wang, Lang Tong, Qing Zhao</div><div style='padding-top: 10px; width: 80ex'>Generative probabilistic forecasting produces future time series samples
according to the conditional probability distribution given past time series
observations. Such techniques are essential in risk-based decision-making and
planning under uncertainty with broad applications in grid operations,
including electricity price forecasting, risk-based economic dispatch, and
stochastic optimizations. Inspired by Wiener and Kallianpur's innovation
representation, we propose a weak innovation autoencoder architecture and a
learning algorithm to extract independent and identically distributed
innovation sequences from nonparametric stationary time series. We show that
the weak innovation sequence is Bayesian sufficient, which makes the proposed
weak innovation autoencoder a canonical architecture for generative
probabilistic forecasting. The proposed technique is applied to forecasting
highly volatile real-time electricity prices, demonstrating superior
performance across multiple forecasting measures over leading probabilistic and
point forecasting techniques.</div><div><a href='http://arxiv.org/abs/2402.13870v1'>2402.13870v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05743v1")'>Generative Probabilistic Forecasting with Applications in Market
  Operations</div>
<div id='2403.05743v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T00:41:30Z</div><div>Authors: Xinyi Wang, Lang Tong</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel generative probabilistic forecasting approach
derived from the Wiener-Kallianpur innovation representation of nonparametric
time series. Under the paradigm of generative artificial intelligence, the
proposed forecasting architecture includes an autoencoder that transforms
nonparametric multivariate random processes into canonical innovation
sequences, from which future time series samples are generated according to
their probability distributions conditioned on past samples. A novel
deep-learning algorithm is proposed that constrains the latent process to be an
independent and identically distributed sequence with matching autoencoder
input-output conditional probability distributions. Asymptotic optimality and
structural convergence properties of the proposed generative forecasting
approach are established. Three applications involving highly dynamic and
volatile time series in real-time market operations are considered: (i)
locational marginal price forecasting for merchant storage participants, {(ii)
interregional price spread forecasting for interchange markets,} and (iii) area
control error forecasting for frequency regulations. Numerical studies based on
market data from multiple independent system operators demonstrate superior
performance against leading traditional and machine learning-based forecasting
techniques under both probabilistic and point forecast metrics.</div><div><a href='http://arxiv.org/abs/2403.05743v1'>2403.05743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02500v1")'>RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder
  for Stock Returns Prediction</div>
<div id='2403.02500v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T21:48:32Z</div><div>Authors: Yilun Wang, Shengjie Guo</div><div style='padding-top: 10px; width: 80ex'>In recent years, the dynamic factor model has emerged as a dominant tool in
economics and finance, particularly for investment strategies. This model
offers improved handling of complex, nonlinear, and noisy market conditions
compared to traditional static factor models. The advancement of machine
learning, especially in dealing with nonlinear data, has further enhanced asset
pricing methodologies. This paper introduces a groundbreaking dynamic factor
model named RVRAE. This model is a probabilistic approach that addresses the
temporal dependencies and noise in market data. RVRAE ingeniously combines the
principles of dynamic factor modeling with the variational recurrent
autoencoder (VRAE) from deep learning. A key feature of RVRAE is its use of a
prior-posterior learning method. This method fine-tunes the model's learning
process by seeking an optimal posterior factor model informed by future data.
Notably, RVRAE is adept at risk modeling in volatile stock markets, estimating
variances from latent space distributions while also predicting returns. Our
empirical tests with real stock market data underscore RVRAE's superior
performance compared to various established baseline methods.</div><div><a href='http://arxiv.org/abs/2403.02500v1'>2403.02500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14063v1")'>DiffSTOCK: Probabilistic relational Stock Market Predictions using
  Diffusion Models</div>
<div id='2403.14063v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T01:20:32Z</div><div>Authors: Divyanshu Daiya, Monika Yadav, Harshit Singh Rao</div><div style='padding-top: 10px; width: 80ex'>In this work, we propose an approach to generalize denoising diffusion
probabilistic models for stock market predictions and portfolio management.
Present works have demonstrated the efficacy of modeling interstock relations
for market time-series forecasting and utilized Graph-based learning models for
value prediction and portfolio management. Though convincing, these
deterministic approaches still fall short of handling uncertainties i.e., due
to the low signal-to-noise ratio of the financial data, it is quite challenging
to learn effective deterministic models. Since the probabilistic methods have
shown to effectively emulate higher uncertainties for time-series predictions.
To this end, we showcase effective utilisation of Denoising Diffusion
Probabilistic Models (DDPM), to develop an architecture for providing better
market predictions conditioned on the historical financial indicators and
inter-stock relations. Additionally, we also provide a novel deterministic
architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit
inter-stock relations along with historical stock features. We demonstrate that
our model achieves SOTA performance for movement predication and Portfolio
management.</div><div><a href='http://arxiv.org/abs/2403.14063v1'>2403.14063v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01846v1")'>DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement
  Prediction</div>
<div id='2401.01846v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T17:36:27Z</div><div>Authors: Zinuo You, Zijian Shi, Hongbo Bo, John Cartlidge, Li Zhang, Yan Ge</div><div style='padding-top: 10px; width: 80ex'>Forecasting future stock trends remains challenging for academia and industry
due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics
influencing stock prices. In recent years, graph neural networks have achieved
remarkable performance in this problem by formulating multiple stocks as
graph-structured data. However, most of these approaches rely on artificially
defined factors to construct static stock graphs, which fail to capture the
intrinsic interdependencies between stocks that rapidly evolve. In addition,
these methods often ignore the hierarchical features of the stocks and lose
distinctive information within. In this work, we propose a novel graph learning
approach implemented without expert knowledge to address these issues. First,
our approach automatically constructs dynamic stock graphs by entropy-driven
edge generation from a signal processing perspective. Then, we further learn
task-optimal dependencies between stocks via a generalized graph diffusion
process on constructed stock graphs. Last, a decoupled representation learning
scheme is adopted to capture distinctive hierarchical intra-stock features.
Experimental results demonstrate substantial improvements over state-of-the-art
baselines on real-world datasets. Moreover, the ablation study and sensitivity
study further illustrate the effectiveness of the proposed method in modeling
the time-evolving inter-stock and intra-stock dynamics.</div><div><a href='http://arxiv.org/abs/2401.01846v1'>2401.01846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05430v1")'>Multi-relational Graph Diffusion Neural Network with Parallel Retention
  for Stock Trends Classification</div>
<div id='2401.05430v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T17:15:45Z</div><div>Authors: Zinuo You, Pengju Zhang, Jin Zheng, John Cartlidge</div><div style='padding-top: 10px; width: 80ex'>Stock trend classification remains a fundamental yet challenging task, owing
to the intricate time-evolving dynamics between and within stocks. To tackle
these two challenges, we propose a graph-based representation learning approach
aimed at predicting the future movements of multiple stocks. Initially, we
model the complex time-varying relationships between stocks by generating
dynamic multi-relational stock graphs. This is achieved through a novel edge
generation algorithm that leverages information entropy and signal energy to
quantify the intensity and directionality of inter-stock relations on each
trading day. Then, we further refine these initial graphs through a stochastic
multi-relational diffusion process, adaptively learning task-optimal edges.
Subsequently, we implement a decoupled representation learning scheme with
parallel retention to obtain the final graph representation. This strategy
better captures the unique temporal features within individual stocks while
also capturing the overall structure of the stock graph. Comprehensive
experiments conducted on real-world datasets from two US markets (NASDAQ and
NYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the
effectiveness of our method. Our approach consistently outperforms
state-of-the-art baselines in forecasting next trading day stock trends across
three test periods spanning seven years. Datasets and code have been released
(https://github.com/pixelhero98/MGDPR).</div><div><a href='http://arxiv.org/abs/2401.05430v1'>2401.05430v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06633v1")'>MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive
  and Dynamic Stock Investment Prediction</div>
<div id='2402.06633v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T02:51:29Z</div><div>Authors: Hao Qian, Hongting Zhou, Qian Zhao, Hao Chen, Hongxiang Yao, Jingwei Wang, Ziqi Liu, Fei Yu, Zhiqiang Zhang, Jun Zhou</div><div style='padding-top: 10px; width: 80ex'>The stock market is a crucial component of the financial system, but
predicting the movement of stock prices is challenging due to the dynamic and
intricate relations arising from various aspects such as economic indicators,
financial reports, global news, and investor sentiment. Traditional sequential
methods and graph-based models have been applied in stock movement prediction,
but they have limitations in capturing the multifaceted and temporal influences
in stock price movements. To address these challenges, the Multi-relational
Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a
discrete dynamic graph to comprehensively capture multifaceted relations among
stocks and their evolution over time. The representation generated from the
graph offers a complete perspective on the interrelationships among stocks and
associated entities. Additionally, the power of the Transformer structure is
leveraged to encode the temporal evolution of multiplex relations, providing a
dynamic and effective approach to predicting stock investment. Further, our
proposed MDGNN framework achieves the best performance in public datasets
compared with state-of-the-art (SOTA) stock investment methods.</div><div><a href='http://arxiv.org/abs/2402.06633v1'>2402.06633v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14199v2")'>MTRGL:Effective Temporal Correlation Discerning through Multi-modal
  Temporal Relational Graph Learning</div>
<div id='2401.14199v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T14:21:14Z</div><div>Authors: Junwei Su, Shan Wu, Jinhui Li</div><div style='padding-top: 10px; width: 80ex'>In this study, we explore the synergy of deep learning and financial market
applications, focusing on pair trading. This market-neutral strategy is
integral to quantitative finance and is apt for advanced deep-learning
techniques. A pivotal challenge in pair trading is discerning temporal
correlations among entities, necessitating the integration of diverse data
modalities. Addressing this, we introduce a novel framework, Multi-modal
Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and
discrete features into a temporal graph and employs a memory-based temporal
graph neural network. This approach reframes temporal correlation
identification as a temporal graph link prediction task, which has shown
empirical success. Our experiments on real-world datasets confirm the superior
performance of MTRGL, emphasizing its promise in refining automated pair
trading strategies.</div><div><a href='http://arxiv.org/abs/2401.14199v2'>2401.14199v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13096v1")'>Probabilistic Demand Forecasting with Graph Neural Networks</div>
<div id='2401.13096v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T21:20:48Z</div><div>Authors: Nikita Kozodoi, Elizaveta Zinovyeva, Simon Valentin, João Pereira, Rodrigo Agundez</div><div style='padding-top: 10px; width: 80ex'>Demand forecasting is a prominent business use case that allows retailers to
optimize inventory planning, logistics, and core business decisions. One of the
key challenges in demand forecasting is accounting for relationships and
interactions between articles. Most modern forecasting approaches provide
independent article-level predictions that do not consider the impact of
related articles. Recent research has attempted addressing this challenge using
Graph Neural Networks (GNNs) and showed promising results. This paper builds on
previous research on GNNs and makes two contributions. First, we integrate a
GNN encoder into a state-of-the-art DeepAR model. The combined model produces
probabilistic forecasts, which are crucial for decision-making under
uncertainty. Second, we propose to build graphs using article attribute
similarity, which avoids reliance on a pre-defined graph structure. Experiments
on three real-world datasets show that the proposed approach consistently
outperforms non-graph benchmarks. We also show that our approach produces
article embeddings that encode article similarity and demand dynamics and are
useful for other downstream business tasks beyond forecasting.</div><div><a href='http://arxiv.org/abs/2401.13096v1'>2401.13096v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15299v1")'>SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph
  Neural Networks</div>
<div id='2401.15299v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T05:14:17Z</div><div>Authors: Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have gained traction across different domains
such as transportation, bio-informatics, language processing, and computer
vision. However, there is a noticeable absence of research on applying GNNs to
supply chain networks. Supply chain networks are inherently graph-like in
structure, making them prime candidates for applying GNN methodologies. This
opens up a world of possibilities for optimizing, predicting, and solving even
the most complex supply chain problems. A major setback in this approach lies
in the absence of real-world benchmark datasets to facilitate the research and
resolution of supply chain problems using GNNs. To address the issue, we
present a real-world benchmark dataset for temporal tasks, obtained from one of
the leading FMCG companies in Bangladesh, focusing on supply chain planning for
production purposes. The dataset includes temporal data as node features to
enable sales predictions, production planning, and the identification of
factory issues. By utilizing this dataset, researchers can employ GNNs to
address numerous supply chain problems, thereby advancing the field of supply
chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph</div><div><a href='http://arxiv.org/abs/2401.15299v1'>2401.15299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10760v1")'>RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval
  Construction</div>
<div id='2402.10760v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T15:34:07Z</div><div>Authors: Jingyi Gu, Wenlu Du, Guiling Wang</div><div style='padding-top: 10px; width: 80ex'>Efforts to predict stock market outcomes have yielded limited success due to
the inherently stochastic nature of the market, influenced by numerous
unpredictable factors. Many existing prediction approaches focus on
single-point predictions, lacking the depth needed for effective
decision-making and often overlooking market risk. To bridge this gap, we
propose a novel model, RAGIC, which introduces sequence generation for stock
interval prediction to quantify uncertainty more effectively. Our approach
leverages a Generative Adversarial Network (GAN) to produce future price
sequences infused with randomness inherent in financial markets. RAGIC's
generator includes a risk module, capturing the risk perception of informed
investors, and a temporal module, accounting for historical price trends and
seasonality. This multi-faceted generator informs the creation of
risk-sensitive intervals through statistical inference, incorporating
horizon-wise insights. The interval's width is carefully adjusted to reflect
market volatility. Importantly, our approach relies solely on publicly
available data and incurs only low computational overhead. RAGIC's evaluation
across globally recognized broad-based indices demonstrates its balanced
performance, offering both accuracy and informativeness. Achieving a consistent
95% coverage, RAGIC maintains a narrow interval width. This promising outcome
suggests that our approach effectively addresses the challenges of stock market
prediction while incorporating vital risk considerations.</div><div><a href='http://arxiv.org/abs/2402.10760v1'>2402.10760v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17350v2")'>Time Series Supplier Allocation via Deep Black-Litterman Model</div>
<div id='2401.17350v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:57:07Z</div><div>Authors: Jiayuan Luo, Wentao Zhang, Yuchen Fang, Xiaowei Gao, Dingyi Zhuang, Hao Chen, Xinke Jiang</div><div style='padding-top: 10px; width: 80ex'>Time Series Supplier Allocation (TSSA) poses a complex NP-hard challenge,
aimed at refining future order dispatching strategies to satisfy order demands
with maximum supply efficiency fully. Traditionally derived from financial
portfolio management, the Black-Litterman (BL) model offers a new perspective
for the TSSA scenario by balancing expected returns against insufficient supply
risks. However, its application within TSSA is constrained by the reliance on
manually constructed perspective matrices and spatio-temporal market dynamics,
coupled with the absence of supervisory signals and data unreliability inherent
to supplier information. To solve these limitations, we introduce the
pioneering Deep Black-Litterman Model (DBLM), which innovatively adapts the BL
model from financial roots to supply chain context. Leveraging the
Spatio-Temporal Graph Neural Networks (STGNNS), DBLM automatically generates
future perspective matrices for TSSA, by integrating spatio-temporal
dependency. Moreover, a novel Spearman rank correlation distinctively
supervises our approach to address the lack of supervisory signals,
specifically designed to navigate through the complexities of supplier risks
and interactions. This is further enhanced by a masking mechanism aimed at
counteracting the biases from unreliable data, thereby improving the model's
precision and reliability. Extensive experimentation on two datasets
unequivocally demonstrates DBLM's enhanced performance in TSSA, setting new
standards for the field. Our findings and methodology are made available for
community access and further development.</div><div><a href='http://arxiv.org/abs/2401.17350v2'>2401.17350v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08233v1")'>End-to-End Policy Learning of a Statistical Arbitrage Autoencoder
  Architecture</div>
<div id='2402.08233v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T05:53:00Z</div><div>Authors: Fabian Krause, Jan-Peter Calliess</div><div style='padding-top: 10px; width: 80ex'>In Statistical Arbitrage (StatArb), classical mean reversion trading
strategies typically hinge on asset-pricing or PCA based models to identify the
mean of a synthetic asset. Once such a (linear) model is identified, a separate
mean reversion strategy is then devised to generate a trading signal. With a
view of generalising such an approach and turning it truly data-driven, we
study the utility of Autoencoder architectures in StatArb. As a first approach,
we employ a standard Autoencoder trained on US stock returns to derive trading
strategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance
this model, we take a policy-learning approach and embed the Autoencoder
network into a neural network representation of a space of portfolio trading
policies. This integration outputs portfolio allocations directly and is
end-to-end trainable by backpropagation of the risk-adjusted returns of the
neural policy. Our findings demonstrate that this innovative end-to-end policy
learning approach not only simplifies the strategy development process, but
also yields superior gross returns over its competitors illustrating the
potential of end-to-end training over classical two-stage approaches.</div><div><a href='http://arxiv.org/abs/2402.08233v1'>2402.08233v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09267v1")'>Deep Limit Order Book Forecasting</div>
<div id='2403.09267v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T10:44:10Z</div><div>Authors: Antonio Briola, Silvia Bartolucci, Tomaso Aste</div><div style='padding-top: 10px; width: 80ex'>We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base, to efficiently process
large-scale Limit Order Book data and quantitatively assess state-of-the-art
deep learning models' forecasting capabilities. Our results are twofold. We
demonstrate that the stocks' microstructural characteristics influence the
efficacy of deep learning methods and that their high forecasting power does
not necessarily correspond to actionable trading signals. We argue that
traditional machine learning metrics fail to adequately assess the quality of
forecasts in the Limit Order Book context. As an alternative, we propose an
innovative operational framework that assesses predictions' practicality by
focusing on the probability of accurately forecasting complete transactions.
This work offers academics and practitioners an avenue to make informed and
robust decisions on the application of deep learning techniques, their scope
and limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.</div><div><a href='http://arxiv.org/abs/2403.09267v1'>2403.09267v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15994v1")'>Optimizing Portfolio Management and Risk Assessment in Digital Assets
  Using Deep Learning for Predictive Analysis</div>
<div id='2402.15994v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T05:23:57Z</div><div>Authors: Qishuo Cheng, Le Yang, Jiajian Zheng, Miao Tian, Duan Xin</div><div style='padding-top: 10px; width: 80ex'>Portfolio management issues have been extensively studied in the field of
artificial intelligence in recent years, but existing deep learning-based
quantitative trading methods have some areas where they could be improved.
First of all, the prediction mode of stocks is singular; often, only one
trading expert is trained by a model, and the trading decision is solely based
on the prediction results of the model. Secondly, the data source used by the
model is relatively simple, and only considers the data of the stock itself,
ignoring the impact of the whole market risk on the stock. In this paper, the
DQN algorithm is introduced into asset management portfolios in a novel and
straightforward way, and the performance greatly exceeds the benchmark, which
fully proves the effectiveness of the DRL algorithm in portfolio management.
This also inspires us to consider the complexity of financial problems, and the
use of algorithms should be fully combined with the problems to adapt. Finally,
in this paper, the strategy is implemented by selecting the assets and actions
with the largest Q value. Since different assets are trained separately as
environments, there may be a phenomenon of Q value drift among different assets
(different assets have different Q value distribution areas), which may easily
lead to incorrect asset selection. Consider adding constraints so that the Q
values of different assets share a Q value distribution to improve results.</div><div><a href='http://arxiv.org/abs/2402.15994v1'>2402.15994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16609v1")'>Combining Transformer based Deep Reinforcement Learning with
  Black-Litterman Model for Portfolio Optimization</div>
<div id='2402.16609v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:01:37Z</div><div>Authors: Ruoyu Sun, Angelos Stefanidis, Zhengyong Jiang, Jionglong Su</div><div style='padding-top: 10px; width: 80ex'>As a model-free algorithm, deep reinforcement learning (DRL) agent learns and
makes decisions by interacting with the environment in an unsupervised way. In
recent years, DRL algorithms have been widely applied by scholars for portfolio
optimization in consecutive trading periods, since the DRL agent can
dynamically adapt to market changes and does not rely on the specification of
the joint dynamics across the assets. However, typical DRL agents for portfolio
optimization cannot learn a policy that is aware of the dynamic correlation
between portfolio asset returns. Since the dynamic correlations among portfolio
assets are crucial in optimizing the portfolio, the lack of such knowledge
makes it difficult for the DRL agent to maximize the return per unit of risk,
especially when the target market permits short selling (i.e., the US stock
market). In this research, we propose a hybrid portfolio optimization model
combining the DRL agent and the Black-Litterman (BL) model to enable the DRL
agent to learn the dynamic correlation between the portfolio asset returns and
implement an efficacious long/short strategy based on the correlation.
Essentially, the DRL agent is trained to learn the policy to apply the BL model
to determine the target portfolio weights. To test our DRL agent, we construct
the portfolio based on all the Dow Jones Industrial Average constitute stocks.
Empirical results of the experiments conducted on real-world United States
stock market data demonstrate that our DRL agent significantly outperforms
various comparison portfolio choice strategies and alternative DRL frameworks
by at least 42% in terms of accumulated return. In terms of the return per unit
of risk, our DRL agent significantly outperforms various comparative portfolio
choice strategies and alternative strategies based on other machine learning
frameworks.</div><div><a href='http://arxiv.org/abs/2402.16609v1'>2402.16609v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00515v2")'>Developing A Multi-Agent and Self-Adaptive Framework with Deep
  Reinforcement Learning for Dynamic Portfolio Risk Management</div>
<div id='2402.00515v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T11:31:26Z</div><div>Authors: Zhenglong Li, Vincent Tam, Kwan L. Yeung</div><div style='padding-top: 10px; width: 80ex'>Deep or reinforcement learning (RL) approaches have been adapted as reactive
agents to quickly learn and respond with new investment strategies for
portfolio management under the highly turbulent financial market environments
in recent years. In many cases, due to the very complex correlations among
various financial sectors, and the fluctuating trends in different financial
markets, a deep or reinforcement learning based agent can be biased in
maximising the total returns of the newly formulated investment portfolio while
neglecting its potential risks under the turmoil of various market conditions
in the global or regional sectors. Accordingly, a multi-agent and self-adaptive
framework namely the MASA is proposed in which a sophisticated multi-agent
reinforcement learning (RL) approach is adopted through two cooperating and
reactive agents to carefully and dynamically balance the trade-off between the
overall portfolio returns and their potential risks. Besides, a very flexible
and proactive agent as the market observer is integrated into the MASA
framework to provide some additional information on the estimated market trends
as valuable feedbacks for multi-agent RL approach to quickly adapt to the
ever-changing market conditions. The obtained empirical results clearly reveal
the potential strengths of our proposed MASA framework based on the multi-agent
RL approach against many well-known RL-based approaches on the challenging data
sets of the CSI 300, Dow Jones Industrial Average and S&amp;P 500 indexes over the
past 10 years. More importantly, our proposed MASA framework shed lights on
many possible directions for future investigation.</div><div><a href='http://arxiv.org/abs/2402.00515v2'>2402.00515v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07916v1")'>Advancing Investment Frontiers: Industry-grade Deep Reinforcement
  Learning for Portfolio Optimization</div>
<div id='2403.07916v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T14:08:31Z</div><div>Authors: Philip Ndikum, Serge Ndikum</div><div style='padding-top: 10px; width: 80ex'>This research paper delves into the application of Deep Reinforcement
Learning (DRL) in asset-class agnostic portfolio optimization, integrating
industry-grade methodologies with quantitative finance. At the heart of this
integration is our robust framework that not only merges advanced DRL
algorithms with modern computational techniques but also emphasizes stringent
statistical analysis, software engineering and regulatory compliance. To the
best of our knowledge, this is the first study integrating financial
Reinforcement Learning with sim-to-real methodologies from robotics and
mathematical physics, thus enriching our frameworks and arguments with this
unique perspective. Our research culminates with the introduction of
AlphaOptimizerNet, a proprietary Reinforcement Learning agent (and
corresponding library). Developed from a synthesis of state-of-the-art (SOTA)
literature and our unique interdisciplinary methodology, AlphaOptimizerNet
demonstrates encouraging risk-return optimization across various asset classes
with realistic constraints. These preliminary results underscore the practical
efficacy of our frameworks. As the finance sector increasingly gravitates
towards advanced algorithmic solutions, our study bridges theoretical
advancements with real-world applicability, offering a template for ensuring
safety and robust standards in this technologically driven future.</div><div><a href='http://arxiv.org/abs/2403.07916v1'>2403.07916v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01441v1")'>Learning the Market: Sentiment-Based Ensemble Trading Agents</div>
<div id='2402.01441v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:34:22Z</div><div>Authors: Andrew Ye, James Xu, Yi Wang, Yifan Yu, Daniel Yan, Ryan Chen, Bosheng Dong, Vipin Chaudhary, Shuai Xu</div><div style='padding-top: 10px; width: 80ex'>We propose the integration of sentiment analysis and deep-reinforcement
learning ensemble algorithms for stock trading, and design a strategy capable
of dynamically altering its employed agent given concurrent market sentiment.
In particular, we create a simple-yet-effective method for extracting news
sentiment and combine this with general improvements upon existing works,
resulting in automated trading agents that effectively consider both
qualitative market factors and quantitative stock data. We show that our
approach results in a strategy that is profitable, robust, and risk-minimal --
outperforming the traditional ensemble strategy as well as single agent
algorithms and market metrics. Our findings determine that the conventional
practice of switching ensemble agents every fixed-number of months is
sub-optimal, and that a dynamic sentiment-based framework greatly unlocks
additional performance within these agents. Furthermore, as we have designed
our algorithm with simplicity and efficiency in mind, we hypothesize that the
transition of our method from historical evaluation towards real-time trading
with live data should be relatively simple.</div><div><a href='http://arxiv.org/abs/2402.01441v1'>2402.01441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12285v1")'>FinLlama: Financial Sentiment Classification for Algorithmic Trading
  Applications</div>
<div id='2403.12285v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T22:11:00Z</div><div>Authors: Thanos Konstantinidis, Giorgos Iacovides, Mingxue Xu, Tony G. Constantinides, Danilo Mandic</div><div style='padding-top: 10px; width: 80ex'>There are multiple sources of financial news online which influence market
movements and trader's decisions. This highlights the need for accurate
sentiment analysis, in addition to having appropriate algorithmic trading
techniques, to arrive at better informed trading decisions. Standard lexicon
based sentiment approaches have demonstrated their power in aiding financial
decisions. However, they are known to suffer from issues related to context
sensitivity and word ordering. Large Language Models (LLMs) can also be used in
this context, but they are not finance-specific and tend to require significant
computational resources. To facilitate a finance specific LLM framework, we
introduce a novel approach based on the Llama 2 7B foundational model, in order
to benefit from its generative nature and comprehensive language manipulation.
This is achieved by fine-tuning the Llama2 7B model on a small portion of
supervised financial sentiment analysis data, so as to jointly handle the
complexities of financial lexicon and context, and further equipping it with a
neural network based decision mechanism. Such a generator-classifier scheme,
referred to as FinLlama, is trained not only to classify the sentiment valence
but also quantify its strength, thus offering traders a nuanced insight into
financial news articles. Complementing this, the implementation of
parameter-efficient fine-tuning through LoRA optimises trainable parameters,
thus minimising computational and memory requirements, without sacrificing
accuracy. Simulation results demonstrate the ability of the proposed FinLlama
to provide a framework for enhanced portfolio management decisions and
increased market returns. These results underpin the ability of FinLlama to
construct high-return portfolios which exhibit enhanced resilience, even during
volatile periods and unpredictable market events.</div><div><a href='http://arxiv.org/abs/2403.12285v1'>2403.12285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00785v1")'>Applying News and Media Sentiment Analysis for Generating Forex Trading
  Signals</div>
<div id='2403.00785v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:43:55Z</div><div>Authors: Oluwafemi F Olaiyapo</div><div style='padding-top: 10px; width: 80ex'>The objective of this research is to examine how sentiment analysis can be
employed to generate trading signals for the Foreign Exchange (Forex) market.
The author assessed sentiment in social media posts and news articles
pertaining to the United States Dollar (USD) using a combination of methods:
lexicon-based analysis and the Naive Bayes machine learning algorithm. The
findings indicate that sentiment analysis proves valuable in forecasting market
movements and devising trading signals. Notably, its effectiveness is
consistent across different market conditions. The author concludes that by
analyzing sentiment expressed in news and social media, traders can glean
insights into prevailing market sentiments towards the USD and other pertinent
countries, thereby aiding trading decision-making. This study underscores the
importance of weaving sentiment analysis into trading strategies as a pivotal
tool for predicting market dynamics.</div><div><a href='http://arxiv.org/abs/2403.00785v1'>2403.00785v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03353v1")'>Tweet Influence on Market Trends: Analyzing the Impact of Social Media
  Sentiment on Biotech Stocks</div>
<div id='2402.03353v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T15:43:27Z</div><div>Authors: C. Sarai R. Avila</div><div style='padding-top: 10px; width: 80ex'>This study investigates the relationship between tweet sentiment across
diverse categories: news, company opinions, CEO opinions, competitor opinions,
and stock market behavior in the biotechnology sector, with a focus on
understanding the impact of social media discourse on investor sentiment and
decision-making processes. We analyzed historical stock market data for ten of
the largest and most influential pharmaceutical companies alongside Twitter
data related to COVID-19, vaccines, the companies, and their respective CEOs.
Using VADER sentiment analysis, we examined the sentiment scores of tweets and
assessed their relationships with stock market performance. We employed ARIMA
(AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression)
models to forecast stock market performance, incorporating sentiment covariates
to improve predictions. Our findings revealed a complex interplay between tweet
sentiment, news, biotech companies, their CEOs, and stock market performance,
emphasizing the importance of considering diverse factors when modeling and
predicting stock prices. This study provides valuable insights into the
influence of social media on the financial sector and lays a foundation for
future research aimed at refining stock price prediction models.</div><div><a href='http://arxiv.org/abs/2402.03353v1'>2402.03353v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00772v1")'>Do Weibo platform experts perform better at predicting stock market?</div>
<div id='2403.00772v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T10:04:54Z</div><div>Authors: Ziyuan Ma, Conor Ryan, Jim Buckley, Muslim Chochlov</div><div style='padding-top: 10px; width: 80ex'>Sentiment analysis can be used for stock market prediction. However, existing
research has not studied the impact of a user's financial background on
sentiment-based forecasting of the stock market using artificial neural
networks. In this work, a novel combination of neural networks is used for the
assessment of sentiment-based stock market prediction, based on the financial
background of the population that generated the sentiment. The state-of-the-art
language processing model Bidirectional Encoder Representations from
Transformers (BERT) is used to classify the sentiment and a Long-Short Term
Memory (LSTM) model is used for time-series based stock market prediction. For
evaluation, the Weibo social networking platform is used as a sentiment data
collection source. Weibo users (and their comments respectively) are divided
into Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor
(UFA) groups according to their background information, as collected by Weibo.
The Hong Kong Hang Seng index is used to extract historical stock market change
data. The results indicate that stock market prediction learned from the AFA
group users is 39.67% more precise than that learned from the UFA group users
and shows the highest accuracy (87%) when compared to existing approaches.</div><div><a href='http://arxiv.org/abs/2403.00772v1'>2403.00772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15992v1")'>A Machine Learning Approach to Detect Customer Satisfaction From
  Multiple Tweet Parameters</div>
<div id='2402.15992v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T05:16:43Z</div><div>Authors: Md Mahmudul Hasan, Dr. Shaikh Anowarul Fattah</div><div style='padding-top: 10px; width: 80ex'>Since internet technologies have advanced, one of the primary factors in
company development is customer happiness. Online platforms have become
prominent places for sharing reviews. Twitter is one of these platforms where
customers frequently post their thoughts. Reviews of flights on these platforms
have become a concern for the airline business. A positive review can help the
company grow, while a negative one can quickly ruin its revenue and reputation.
So it's vital for airline businesses to examine the feedback and experiences of
their customers and enhance their services to remain competitive. But studying
thousands of tweets and analyzing them to find the satisfaction of the customer
is quite a difficult task. This tedious process can be made easier by using a
machine learning approach to analyze tweets to determine client satisfaction
levels. Some work has already been done on this strategy to automate the
procedure using machine learning and deep learning techniques. However, they
are all purely concerned with assessing the text's sentiment. In addition to
the text, the tweet also includes the time, location, username, airline name,
and so on. This additional information can be crucial for improving the model's
outcome. To provide a machine learning based solution, this work has broadened
its perspective to include these qualities. And it has come as no surprise that
the additional features beyond text sentiment analysis produce better outcomes
in machine learning based models.</div><div><a href='http://arxiv.org/abs/2402.15992v1'>2402.15992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01487v1")'>Natural Language Processing and Multimodal Stock Price Prediction</div>
<div id='2401.01487v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T01:21:30Z</div><div>Authors: Kevin Taylor, Jerry Ng</div><div style='padding-top: 10px; width: 80ex'>In the realm of financial decision-making, predicting stock prices is
pivotal. Artificial intelligence techniques such as long short-term memory
networks (LSTMs), support-vector machines (SVMs), and natural language
processing (NLP) models are commonly employed to predict said prices. This
paper utilizes stock percentage change as training data, in contrast to the
traditional use of raw currency values, with a focus on analyzing publicly
released news articles. The choice of percentage change aims to provide models
with context regarding the significance of price fluctuations and overall price
change impact on a given stock. The study employs specialized BERT natural
language processing models to predict stock price trends, with a particular
emphasis on various data modalities. The results showcase the capabilities of
such strategies with a small natural language processing model to accurately
predict overall stock trends, and highlight the effectiveness of certain data
features and sector-specific data.</div><div><a href='http://arxiv.org/abs/2401.01487v1'>2401.01487v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05073v1")'>Hierarchical Classification of Transversal Skills in Job Ads Based on
  Sentence Embeddings</div>
<div id='2401.05073v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T11:07:32Z</div><div>Authors: Florin Leon, Marius Gavrilescu, Sabina-Adriana Floria, Alina-Adriana Minea</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a classification framework aimed at identifying
correlations between job ad requirements and transversal skill sets, with a
focus on predicting the necessary skills for individual job descriptions using
a deep learning model. The approach involves data collection, preprocessing,
and labeling using ESCO (European Skills, Competences, and Occupations)
taxonomy. Hierarchical classification and multi-label strategies are used for
skill identification, while augmentation techniques address data imbalance,
enhancing model robustness. A comparison between results obtained with
English-specific and multi-language sentence embedding models reveals close
accuracy. The experimental case studies detail neural network configurations,
hyperparameters, and cross-validation results, highlighting the efficacy of the
hierarchical approach and the suitability of the multi-language model for the
diverse European job market. Thus, a new approach is proposed for the
hierarchical classification of transversal skills from job ads.</div><div><a href='http://arxiv.org/abs/2401.05073v1'>2401.05073v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.06635v1")'>Large (and Deep) Factor Models</div>
<div id='2402.06635v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T14:03:31Z</div><div>Authors: Bryan Kelly, Boris Kuznetsov, Semyon Malamud, Teng Andrea Xu</div><div style='padding-top: 10px; width: 80ex'>We open up the black box behind Deep Learning for portfolio optimization and
prove that a sufficiently wide and arbitrarily deep neural network (DNN)
trained to maximize the Sharpe ratio of the Stochastic Discount Factor (SDF) is
equivalent to a large factor model (LFM): A linear factor pricing model that
uses many non-linear characteristics. The nature of these characteristics
depends on the architecture of the DNN in an explicit, tractable fashion. This
makes it possible to derive end-to-end trained DNN-based SDFs in closed form
for the first time. We evaluate LFMs empirically and show how various
architectural choices impact SDF performance. We document the virtue of depth
complexity: With enough data, the out-of-sample performance of DNN-SDF is
increasing in the NN depth, saturating at huge depths of around 100 hidden
layers.</div><div><a href='http://arxiv.org/abs/2402.06635v1'>2402.06635v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15243v1")'>Robust Utility Optimization via a GAN Approach</div>
<div id='2403.15243v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:36:39Z</div><div>Authors: Florian Krach, Josef Teichmann, Hanna Wutte</div><div style='padding-top: 10px; width: 80ex'>Robust utility optimization enables an investor to deal with market
uncertainty in a structured way, with the goal of maximizing the worst-case
outcome. In this work, we propose a generative adversarial network (GAN)
approach to (approximately) solve robust utility optimization problems in
general and realistic settings. In particular, we model both the investor and
the market by neural networks (NN) and train them in a mini-max zero-sum game.
This approach is applicable for any continuous utility function and in
realistic market settings with trading costs, where only observable information
of the market can be used. A large empirical study shows the versatile
usability of our method. Whenever an optimal reference strategy is available,
our method performs on par with it and in the (many) settings without known
optimal strategy, our method outperforms all other reference strategies.
Moreover, we can conclude from our study that the trained path-dependent
strategies do not outperform Markovian ones. Lastly, we uncover that our
generative approach for learning optimal, (non-) robust investments under
trading costs generates universally applicable alternatives to well known
asymptotic strategies of idealized settings.</div><div><a href='http://arxiv.org/abs/2403.15243v1'>2403.15243v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07365v1")'>A Deep Learning Method for Optimal Investment Under Relative Performance
  Criteria Among Heterogeneous Agents</div>
<div id='2402.07365v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T01:40:31Z</div><div>Authors: Mathieu Laurière, Ludovic Tangpi, Xuchen Zhou</div><div style='padding-top: 10px; width: 80ex'>Graphon games have been introduced to study games with many players who
interact through a weighted graph of interaction. By passing to the limit, a
game with a continuum of players is obtained, in which the interactions are
through a graphon. In this paper, we focus on a graphon game for optimal
investment under relative performance criteria, and we propose a deep learning
method. The method builds upon two key ingredients: first, a characterization
of Nash equilibria by forward-backward stochastic differential equations and,
second, recent advances of machine learning algorithms for stochastic
differential games. We provide numerical experiments on two different financial
models. In each model, we compare the effect of several graphons, which
correspond to different structures of interactions.</div><div><a href='http://arxiv.org/abs/2402.07365v1'>2402.07365v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16985v1")'>Multiple Yield Curve Modeling and Forecasting using Deep Learning</div>
<div id='2401.16985v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T13:14:54Z</div><div>Authors: Ronald Richman, Salvatore Scognamiglio</div><div style='padding-top: 10px; width: 80ex'>This manuscript introduces deep learning models that simultaneously describe
the dynamics of several yield curves. We aim to learn the dependence structure
among the different yield curves induced by the globalization of financial
markets and exploit it to produce more accurate forecasts. By combining the
self-attention mechanism and nonparametric quantile regression, our model
generates both point and interval forecasts of future yields. The architecture
is designed to avoid quantile crossing issues affecting multiple quantile
regression models. Numerical experiments conducted on two different datasets
confirm the effectiveness of our approach. Finally, we explore potential
extensions and enhancements by incorporating deep ensemble methods and transfer
learning mechanisms.</div><div><a href='http://arxiv.org/abs/2401.16985v1'>2401.16985v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03631v1")'>Tackling Missing Values in Probabilistic Wind Power Forecasting: A
  Generative Approach</div>
<div id='2403.03631v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T11:38:08Z</div><div>Authors: Honglin Wen, Pierre Pinson, Jie Gu, Zhijian Jin</div><div style='padding-top: 10px; width: 80ex'>Machine learning techniques have been successfully used in probabilistic wind
power forecasting. However, the issue of missing values within datasets due to
sensor failure, for instance, has been overlooked for a long time. Although it
is natural to consider addressing this issue by imputing missing values before
model estimation and forecasting, we suggest treating missing values and
forecasting targets indifferently and predicting all unknown values
simultaneously based on observations. In this paper, we offer an efficient
probabilistic forecasting approach by estimating the joint distribution of
features and targets based on a generative model. It is free of preprocessing,
and thus avoids introducing potential errors. Compared with the traditional
"impute, then predict" pipeline, the proposed approach achieves better
performance in terms of continuous ranked probability score.</div><div><a href='http://arxiv.org/abs/2403.03631v1'>2403.03631v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13287v1")'>Manipulating hidden-Markov-model inferences by corrupting batch data</div>
<div id='2402.13287v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T12:22:22Z</div><div>Authors: William N. Caballero, Jose Manuel Camacho, Tahir Ekin, Roi Naveiro</div><div style='padding-top: 10px; width: 80ex'>Time-series models typically assume untainted and legitimate streams of data.
However, a self-interested adversary may have incentive to corrupt this data,
thereby altering a decision maker's inference. Within the broader field of
adversarial machine learning, this research provides a novel, probabilistic
perspective toward the manipulation of hidden Markov model inferences via
corrupted data. In particular, we provision a suite of corruption problems for
filtering, smoothing, and decoding inferences leveraging an adversarial risk
analysis approach. Multiple stochastic programming models are set forth that
incorporate realistic uncertainties and varied attacker objectives. Three
general solution methods are developed by alternatively viewing the problem
from frequentist and Bayesian perspectives. The efficacy of each method is
illustrated via extensive, empirical testing. The developed methods are
characterized by their solution quality and computational effort, resulting in
a stratification of techniques across varying problem-instance architectures.
This research highlights the weaknesses of hidden Markov models under
adversarial activity, thereby motivating the need for robustification
techniques to ensure their security.</div><div><a href='http://arxiv.org/abs/2402.13287v1'>2402.13287v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03850v1")'>Conformal prediction for multi-dimensional time series by ellipsoidal
  sets</div>
<div id='2403.03850v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:55:40Z</div><div>Authors: Chen Xu, Hanyang Jiang, Yao Xie</div><div style='padding-top: 10px; width: 80ex'>Conformal prediction (CP) has been a popular method for uncertainty
quantification because it is distribution-free, model-agnostic, and
theoretically sound. For forecasting problems in supervised learning, most CP
methods focus on building prediction intervals for univariate responses. In
this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$
that builds prediction regions for a multivariate response, especially in the
context of multivariate time series, which are not exchangeable. Theoretically,
we estimate finite-sample high-probability bounds on the conditional coverage
gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid
coverage on a wide range of multivariate time series while producing smaller
prediction regions than CP and non-CP baselines.</div><div><a href='http://arxiv.org/abs/2403.03850v1'>2403.03850v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06868v1")'>Multicriteria decision support employing adaptive prediction in a
  tensor-based feature representation</div>
<div id='2401.06868v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T19:46:29Z</div><div>Authors: Betania Silva Carneiro Campello, Leonardo Tomazeli Duarte, João Marcos Travassos Romano</div><div style='padding-top: 10px; width: 80ex'>Multicriteria decision analysis (MCDA) is a widely used tool to support
decisions in which a set of alternatives should be ranked or classified based
on multiple criteria. Recent studies in MCDA have shown the relevance of
considering not only current evaluations of each criterion but also past data.
Past-data-based approaches carry new challenges, especially in time-varying
environments. This study deals with this challenge via essential tools of
signal processing, such as tensorial representations and adaptive prediction.
More specifically, we structure the criteria' past data as a tensor and, by
applying adaptive prediction, we compose signals with these prediction values
of the criteria. Besides, we transform the prediction in the time domain into a
most favorable decision making domain, called the feature domain. We present a
novel extension of the MCDA method PROMETHEE II, aimed at addressing the tensor
in the feature domain to obtain a ranking of alternatives. Numerical
experiments were performed using real-world time series, and our approach is
compared with other existing strategies. The results highlight the relevance
and efficiency of our proposal, especially for nonstationary time series.</div><div><a href='http://arxiv.org/abs/2401.06868v1'>2401.06868v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02399v1")'>FreDF: Learning to Forecast in Frequency Domain</div>
<div id='2402.02399v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T08:23:41Z</div><div>Authors: Hao Wang, Licheng Pan, Zhichao Chen, Degui Yang, Sen Zhang, Yifei Yang, Xinggao Liu, Haoxuan Li, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Time series modeling is uniquely challenged by the presence of
autocorrelation in both historical and label sequences. Current research
predominantly focuses on handling autocorrelation within the historical
sequence but often neglects its presence in the label sequence. Specifically,
emerging forecast models mainly conform to the direct forecast (DF) paradigm,
generating multi-step forecasts under the assumption of conditional
independence within the label sequence. This assumption disregards the inherent
autocorrelation in the label sequence, thereby limiting the performance of
DF-based models. In response to this gap, we introduce the Frequency-enhanced
Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation
by learning to forecast in the frequency domain. Our experiments demonstrate
that FreDF substantially outperforms existing state-of-the-art methods
including iTransformer and is compatible with a variety of forecast models.</div><div><a href='http://arxiv.org/abs/2402.02399v1'>2402.02399v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11144v1")'>Is Mamba Effective for Time Series Forecasting?</div>
<div id='2403.11144v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T08:50:44Z</div><div>Authors: Zihan Wang, Fanheng Kong, Shi Feng, Ming Wang, Han Zhao, Daling Wang, Yifei Zhang</div><div style='padding-top: 10px; width: 80ex'>In the realm of time series forecasting (TSF), the Transformer has
consistently demonstrated robust performance due to its ability to focus on the
global context and effectively capture long-range dependencies within time, as
well as discern correlations between multiple variables. However, due to the
inefficiencies of the Transformer model and questions surrounding its ability
to capture dependencies, ongoing efforts to refine the Transformer architecture
persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction
due to their ability to capture complex dependencies in sequences, similar to
the Transformer, while maintaining near-linear complexity. In text and image
tasks, Mamba-based models can improve performance and cost savings, creating a
win-win situation. This has piqued our interest in exploring SSM's potential in
TSF tasks. In this paper, we introduce two straightforward SSM-based models for
TSF, S-Mamba and D-Mamba, both employing the Mamba Block to extract variate
correlations. Remarkably, S-Mamba and D-Mamba achieve superior performance
while saving GPU memory and training time. Furthermore, we conduct extensive
experiments to delve deeper into the potential of Mamba compared to the
Transformer in the TSF, aiming to explore a new research direction for this
field. Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba.</div><div><a href='http://arxiv.org/abs/2403.11144v1'>2403.11144v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01999v1")'>A Novel Hyperdimensional Computing Framework for Online Time Series
  Forecasting on the Edge</div>
<div id='2402.01999v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T02:42:53Z</div><div>Authors: Mohamed Mejri, Chandramouli Amarnath, Abhijit Chatterjee</div><div style='padding-top: 10px; width: 80ex'>In recent years, both online and offline deep learning models have been
developed for time series forecasting. However, offline deep forecasting models
fail to adapt effectively to changes in time-series data, while online deep
forecasting models are often expensive and have complex training procedures. In
this paper, we reframe the online nonlinear time-series forecasting problem as
one of linear hyperdimensional time-series forecasting. Nonlinear
low-dimensional time-series data is mapped to high-dimensional
(hyperdimensional) spaces for linear hyperdimensional prediction, allowing
fast, efficient and lightweight online time-series forecasting. Our framework,
TSF-HD, adapts to time-series distribution shifts using a novel co-training
framework for its hyperdimensional mapping and its linear hyperdimensional
predictor. TSF-HD is shown to outperform the state of the art, while having
reduced inference latency, for both short-term and long-term time series
forecasting. Our code is publicly available at
http://github.com/tsfhd2024/tsf-hd.git</div><div><a href='http://arxiv.org/abs/2402.01999v1'>2402.01999v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04477v1")'>Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting</div>
<div id='2403.04477v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T13:22:25Z</div><div>Authors: Kiran Madhusudhanan, Shayan Jawed, Lars Schmidt-Thieme</div><div style='padding-top: 10px; width: 80ex'>Time series forecasting attempts to predict future events by analyzing past
trends and patterns. Although well researched, certain critical aspects
pertaining to the use of deep learning in time series forecasting remain
ambiguous. Our research primarily focuses on examining the impact of specific
hyperparameters related to time series, such as context length and validation
strategy, on the performance of the state-of-the-art MLP model in time series
forecasting. We have conducted a comprehensive series of experiments involving
4800 configurations per dataset across 20 time series forecasting datasets, and
our findings demonstrate the importance of tuning these parameters.
Furthermore, in this work, we introduce the largest metadataset for timeseries
forecasting to date, named TSBench, comprising 97200 evaluations, which is a
twentyfold increase compared to previous works in the field. Finally, we
demonstrate the utility of the created metadataset on multi-fidelity
hyperparameter optimization tasks.</div><div><a href='http://arxiv.org/abs/2403.04477v1'>2403.04477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07570v2")'>Only the Curve Shape Matters: Training Foundation Models for Zero-Shot
  Multivariate Time Series Forecasting through Next Curve Shape Prediction</div>
<div id='2402.07570v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:04:14Z</div><div>Authors: Cheng Feng, Long Huang, Denis Krompass</div><div style='padding-top: 10px; width: 80ex'>We present General Time Transformer (GTT), an encoder-only style foundation
model for zero-shot multivariate time series forecasting. GTT is pretrained on
a large dataset of 200M high-quality time series samples spanning diverse
domains. In our proposed framework, the task of multivariate time series
forecasting is formulated as a channel-wise next curve shape prediction
problem, where each time series sample is represented as a sequence of
non-overlapping curve shapes with a unified numerical magnitude. GTT is trained
to predict the next curve shape based on a window of past curve shapes in a
channel-wise manner. Experimental results demonstrate that GTT exhibits
superior zero-shot multivariate forecasting capabilities on unseen time series
datasets, even surpassing state-of-the-art supervised baselines. Additionally,
we investigate the impact of varying GTT model parameters and training dataset
scales, observing that the scaling law also holds in the context of zero-shot
multivariate time series forecasting.</div><div><a href='http://arxiv.org/abs/2402.07570v2'>2402.07570v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14587v1")'>An Analysis of Linear Time Series Forecasting Models</div>
<div id='2403.14587v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:42:45Z</div><div>Authors: William Toner, Luke Darlow</div><div style='padding-top: 10px; width: 80ex'>Despite their simplicity, linear models perform well at time series
forecasting, even when pitted against deeper and more expensive models. A
number of variations to the linear model have been proposed, often including
some form of feature normalisation that improves model generalisation. In this
paper we analyse the sets of functions expressible using these linear model
architectures. In so doing we show that several popular variants of linear
models for time series forecasting are equivalent and functionally
indistinguishable from standard, unconstrained linear regression. We
characterise the model classes for each linear variant. We demonstrate that
each model can be reinterpreted as unconstrained linear regression over a
suitably augmented feature set, and therefore admit closed-form solutions when
using a mean-squared loss function. We provide experimental evidence that the
models under inspection learn nearly identical solutions, and finally
demonstrate that the simpler closed form solutions are superior forecasters
across 72% of test settings.</div><div><a href='http://arxiv.org/abs/2403.14587v1'>2403.14587v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13836v1")'>Tree-based Learning for High-Fidelity Prediction of Chaos</div>
<div id='2403.13836v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T01:16:29Z</div><div>Authors: Adam Giammarese, Kamal Rana, Erik M. Bollt, Nishant Malik</div><div style='padding-top: 10px; width: 80ex'>Model-free forecasting of the temporal evolution of chaotic systems is
crucial but challenging. Existing solutions require hyperparameter tuning,
significantly hindering their wider adoption. In this work, we introduce a
tree-based approach not requiring hyperparameter tuning: TreeDOX. It uses time
delay overembedding as explicit short-term memory and Extra-Trees Regressors to
perform feature reduction and forecasting. We demonstrate the state-of-the-art
performance of TreeDOX using the Henon map, Lorenz and Kuramoto-Sivashinsky
systems, and the real-world Southern Oscillation Index.</div><div><a href='http://arxiv.org/abs/2403.13836v1'>2403.13836v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05713v2")'>tsGT: Stochastic Time Series Modeling With Transformer</div>
<div id='2403.05713v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T22:59:41Z</div><div>Authors: Łukasz Kuciński, Witold Drzewakowski, Mateusz Olko, Piotr Kozakowski, Łukasz Maziarka, Marta Emilia Nowakowska, Łukasz Kaiser, Piotr Miłoś</div><div style='padding-top: 10px; width: 80ex'>Time series methods are of fundamental importance in virtually any field of
science that deals with temporally structured data. Recently, there has been a
surge of deterministic transformer models with time series-specific
architectural biases. In this paper, we go in a different direction by
introducing tsGT, a stochastic time series model built on a general-purpose
transformer architecture. We focus on using a well-known and theoretically
justified rolling window backtesting and evaluation protocol. We show that tsGT
outperforms the state-of-the-art models on MAD and RMSE, and surpasses its
stochastic peers on QL and CRPS, on four commonly used datasets. We complement
these results with a detailed analysis of tsGT's ability to model the data
distribution and predict marginal quantile values.</div><div><a href='http://arxiv.org/abs/2403.05713v2'>2403.05713v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19402v1")'>A Scalable and Transferable Time Series Prediction Framework for Demand
  Forecasting</div>
<div id='2402.19402v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:01:07Z</div><div>Authors: Young-Jin Park, Donghyun Kim, Frédéric Odermatt, Juho Lee, Kyung-Min Kim</div><div style='padding-top: 10px; width: 80ex'>Time series forecasting is one of the most essential and ubiquitous tasks in
many business problems, including demand forecasting and logistics
optimization. Traditional time series forecasting methods, however, have
resulted in small models with limited expressive power because they have
difficulty in scaling their model size up while maintaining high accuracy. In
this paper, we propose Forecasting orchestra (Forchestra), a simple but
powerful framework capable of accurately predicting future demand for a diverse
range of items. We empirically demonstrate that the model size is scalable to
up to 0.8 billion parameters. The proposed method not only outperforms existing
forecasting models with a significant margin, but it could generalize well to
unseen data points when evaluated in a zero-shot fashion on downstream
datasets. Last but not least, we present extensive qualitative and quantitative
studies to analyze how the proposed model outperforms baseline models and
differs from conventional approaches. The original paper was presented as a
full paper at ICDM 2022 and is available at:
https://ieeexplore.ieee.org/document/10027662.</div><div><a href='http://arxiv.org/abs/2402.19402v1'>2402.19402v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04180v1")'>RATSF: Empowering Customer Service Volume Management through
  Retrieval-Augmented Time-Series Forecasting</div>
<div id='2403.04180v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T03:23:13Z</div><div>Authors: Tianfeng Wang, Gaojie Cui</div><div style='padding-top: 10px; width: 80ex'>An efficient customer service management system hinges on precise forecasting
of service volume. In this scenario, where data non-stationarity is pronounced,
successful forecasting heavily relies on identifying and leveraging similar
historical data rather than merely summarizing periodic patterns. Existing
models based on RNN or Transformer architectures often struggle with this
flexible and effective utilization. To address this challenge, we propose an
efficient and adaptable cross-attention module termed RACA, which effectively
leverages historical segments in forecasting task, and we devised a precise
representation scheme for querying historical sequences, coupled with the
design of a knowledge repository. These critical components collectively form
our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF
not only significantly enhances performance in the context of Fliggy hotel
service volume forecasting but, more crucially, can be seamlessly integrated
into other Transformer-based time-series forecasting models across various
application scenarios. Extensive experimentation has validated the
effectiveness and generalizability of this system design across multiple
diverse contexts.</div><div><a href='http://arxiv.org/abs/2403.04180v1'>2403.04180v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00965v1")'>Improve Fidelity and Utility of Synthetic Credit Card Transaction Time
  Series from Data-centric Perspective</div>
<div id='2401.00965v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T22:34:14Z</div><div>Authors: Din-Yin Hsieh, Chi-Hua Wang, Guang Cheng</div><div style='padding-top: 10px; width: 80ex'>Exploring generative model training for synthetic tabular data, specifically
in sequential contexts such as credit card transaction data, presents
significant challenges. This paper addresses these challenges, focusing on
attaining both high fidelity to actual data and optimal utility for machine
learning tasks. We introduce five pre-processing schemas to enhance the
training of the Conditional Probabilistic Auto-Regressive Model (CPAR),
demonstrating incremental improvements in the synthetic data's fidelity and
utility. Upon achieving satisfactory fidelity levels, our attention shifts to
training fraud detection models tailored for time-series data, evaluating the
utility of the synthetic data. Our findings offer valuable insights and
practical guidelines for synthetic data practitioners in the finance sector,
transitioning from real to synthetic datasets for training purposes, and
illuminating broader methodologies for synthesizing credit card transaction
time series.</div><div><a href='http://arxiv.org/abs/2401.00965v1'>2401.00965v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01641v2")'>Towards a Foundation Purchasing Model: Pretrained Generative
  Autoregression on Transaction Sequences</div>
<div id='2401.01641v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T09:32:48Z</div><div>Authors: Piotr Skalski, David Sutton, Stuart Burrell, Iker Perez, Jason Wong</div><div style='padding-top: 10px; width: 80ex'>Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.</div><div><a href='http://arxiv.org/abs/2401.01641v2'>2401.01641v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16913v1")'>PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from
  the perspective of partial differential equations</div>
<div id='2402.16913v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T17:39:44Z</div><div>Authors: Shiyi Qi, Zenglin Xu, Yiduo Li, Liangjian Wen, Qingsong Wen, Qifan Wang, Yuan Qi</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in deep learning have led to the development of various
models for long-term multivariate time-series forecasting (LMTF), many of which
have shown promising results. Generally, the focus has been on
historical-value-based models, which rely on past observations to predict
future series. Notably, a new trend has emerged with time-index-based models,
offering a more nuanced understanding of the continuous dynamics underlying
time series. Unlike these two types of models that aggregate the information of
spatial domains or temporal domains, in this paper, we consider multivariate
time series as spatiotemporal data regularly sampled from a continuous
dynamical system, which can be represented by partial differential equations
(PDEs), with the spatial domain being fixed. Building on this perspective, we
present PDETime, a novel LMTF model inspired by the principles of Neural PDE
solvers, following the encoding-integration-decoding operations. Our extensive
experimentation across seven diverse real-world LMTF datasets reveals that
PDETime not only adapts effectively to the intrinsic spatiotemporal nature of
the data but also sets new benchmarks, achieving state-of-the-art results</div><div><a href='http://arxiv.org/abs/2402.16913v1'>2402.16913v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15284v1")'>Spatiotemporal Observer Design for Predictive Learning of
  High-Dimensional Data</div>
<div id='2402.15284v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:28:31Z</div><div>Authors: Tongyi Liang, Han-Xiong Li</div><div style='padding-top: 10px; width: 80ex'>Although deep learning-based methods have shown great success in
spatiotemporal predictive learning, the framework of those models is designed
mainly by intuition. How to make spatiotemporal forecasting with theoretical
guarantees is still a challenging issue. In this work, we tackle this problem
by applying domain knowledge from the dynamical system to the framework design
of deep learning models. An observer theory-guided deep learning architecture,
called Spatiotemporal Observer, is designed for predictive learning of high
dimensional data. The characteristics of the proposed framework are twofold:
firstly, it provides the generalization error bound and convergence guarantee
for spatiotemporal prediction; secondly, dynamical regularization is introduced
to enable the model to learn system dynamics better during training. Further
experimental results show that this framework could capture the spatiotemporal
dynamics and make accurate predictions in both one-step-ahead and
multi-step-ahead forecasting scenarios.</div><div><a href='http://arxiv.org/abs/2402.15284v1'>2402.15284v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12921v2")'>Right on Time: Revising Time Series Models by Constraining their
  Explanations</div>
<div id='2402.12921v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T11:15:13Z</div><div>Authors: Maurice Kraus, David Steinmann, Antonia Wüst, Andre Kokozinski, Kristian Kersting</div><div style='padding-top: 10px; width: 80ex'>The reliability of deep time series models is often compromised by their
tendency to rely on confounding factors, which may lead to misleading results.
Our newly recorded, naturally confounded dataset named P2S from a real
mechanical production line emphasizes this. To tackle the challenging problem
of mitigating confounders in time series data, we introduce Right on Time
(RioT). Our method enables interactions with model explanations across both the
time and frequency domain. Feedback on explanations in both domains is then
used to constrain the model, steering it away from the annotated confounding
factors. The dual-domain interaction strategy is crucial for effectively
addressing confounders in time series datasets. We empirically demonstrate that
RioT can effectively guide models away from the wrong reasons in P2S as well as
popular time series classification and forecasting datasets.</div><div><a href='http://arxiv.org/abs/2402.12921v2'>2402.12921v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09305v1")'>Embracing the black box: Heading towards foundation models for causal
  discovery from time series data</div>
<div id='2402.09305v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T16:49:13Z</div><div>Authors: Gideon Stein, Maha Shadaydeh, Joachim Denzler</div><div style='padding-top: 10px; width: 80ex'>Causal discovery from time series data encompasses many existing solutions,
including those based on deep learning techniques. However, these methods
typically do not endorse one of the most prevalent paradigms in deep learning:
End-to-end learning. To address this gap, we explore what we call Causal
Pretraining. A methodology that aims to learn a direct mapping from
multivariate time series to the underlying causal graphs in a supervised
manner. Our empirical findings suggest that causal discovery in a supervised
manner is possible, assuming that the training and test time series samples
share most of their dynamics. More importantly, we found evidence that the
performance of Causal Pretraining can increase with data and model size, even
if the additional data do not share the same dynamics. Further, we provide
examples where causal discovery for real-world data with causally pretrained
neural networks is possible within limits. We argue that this hints at the
possibility of a foundation model for causal discovery.</div><div><a href='http://arxiv.org/abs/2402.09305v1'>2402.09305v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03614v1")'>Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series
  Data</div>
<div id='2402.03614v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T01:01:23Z</div><div>Authors: He Zhao, Edwin V. Bonilla</div><div style='padding-top: 10px; width: 80ex'>We study the problem of automatically discovering Granger causal relations
from observational multivariate time-series data. Vector autoregressive (VAR)
models have been time-tested for this problem, including Bayesian variants and
more recent developments using deep neural networks. Most existing VAR methods
for Granger causality use sparsity-inducing penalties/priors or post-hoc
thresholds to interpret their coefficients as Granger causal graphs. Instead,
we propose a new Bayesian VAR model with a hierarchical graph prior over binary
Granger causal graphs, separately from the VAR coefficients. We develop an
efficient algorithm to infer the posterior over binary Granger causal graphs.
Our method provides better uncertainty quantification, has less
hyperparameters, and achieves better performance than competing approaches,
especially on sparse multivariate time-series data.</div><div><a href='http://arxiv.org/abs/2402.03614v1'>2402.03614v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08386v1")'>Deep Learning-based Group Causal Inference in Multivariate Time-series</div>
<div id='2401.08386v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:19:28Z</div><div>Authors: Wasim Ahmad, Maha Shadaydeh, Joachim Denzler</div><div style='padding-top: 10px; width: 80ex'>Causal inference in a nonlinear system of multivariate timeseries is
instrumental in disentangling the intricate web of relationships among
variables, enabling us to make more accurate predictions and gain deeper
insights into real-world complex systems. Causality methods typically identify
the causal structure of a multivariate system by considering the cause-effect
relationship of each pair of variables while ignoring the collective effect of
a group of variables or interactions involving more than two-time series
variables. In this work, we test model invariance by group-level interventions
on the trained deep networks to infer causal direction in groups of variables,
such as climate and ecosystem, brain networks, etc. Extensive testing with
synthetic and real-world time series data shows a significant improvement of
our method over other applied group causality methods and provides us insights
into real-world time series. The code for our method can be found
at:https://github.com/wasimahmadpk/gCause.</div><div><a href='http://arxiv.org/abs/2401.08386v1'>2401.08386v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08552v2")'>Explaining Time Series via Contrastive and Locally Sparse Perturbations</div>
<div id='2401.08552v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:27:37Z</div><div>Authors: Zichuan Liu, Yingying Zhang, Tianchun Wang, Zefan Wang, Dongsheng Luo, Mengnan Du, Min Wu, Yi Wang, Chunlin Chen, Lunting Fan, Qingsong Wen</div><div style='padding-top: 10px; width: 80ex'>Explaining multivariate time series is a compound challenge, as it requires
identifying important locations in the time series and matching complex
temporal patterns. Although previous saliency-based methods addressed the
challenges, their perturbation may not alleviate the distribution shift issue,
which is inevitable especially in heterogeneous samples. We present ContraLSP,
a locally sparse model that introduces counterfactual samples to build
uninformative perturbations but keeps distribution using contrastive learning.
Furthermore, we incorporate sample-specific sparse gates to generate more
binary-skewed and smooth masks, which easily integrate temporal trends and
select the salient features parsimoniously. Empirical studies on both synthetic
and real-world datasets show that ContraLSP outperforms state-of-the-art
models, demonstrating a substantial improvement in explanation quality for time
series data. The source code is available at
\url{https://github.com/zichuan-liu/ContraLSP}.</div><div><a href='http://arxiv.org/abs/2401.08552v2'>2401.08552v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10288v1")'>Rough Transformers for Continuous and Efficient Time-Series Modelling</div>
<div id='2403.10288v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T13:29:45Z</div><div>Authors: Fernando Moreno-Pino, Álvaro Arroyo, Harrison Waldon, Xiaowen Dong, Álvaro Cartea</div><div style='padding-top: 10px; width: 80ex'>Time-series data in real-world medical settings typically exhibit long-range
dependencies and are observed at non-uniform intervals. In such contexts,
traditional sequence-based recurrent models struggle. To overcome this,
researchers replace recurrent architectures with Neural ODE-based models to
model irregularly sampled data and use Transformer-based architectures to
account for long-range dependencies. Despite the success of these two
approaches, both incur very high computational costs for input sequences of
moderate lengths and greater. To mitigate this, we introduce the Rough
Transformer, a variation of the Transformer model which operates on
continuous-time representations of input sequences and incurs significantly
reduced computational costs, critical for addressing long-range dependencies
common in medical contexts. In particular, we propose multi-view signature
attention, which uses path signatures to augment vanilla attention and to
capture both local and global dependencies in input data, while remaining
robust to changes in the sequence length and sampling frequency. We find that
Rough Transformers consistently outperform their vanilla attention counterparts
while obtaining the benefits of Neural ODE-based models using a fraction of the
computational time and memory resources on synthetic and real-world time-series
tasks.</div><div><a href='http://arxiv.org/abs/2403.10288v1'>2403.10288v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02258v1")'>XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event
  Prediction</div>
<div id='2402.02258v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T20:33:39Z</div><div>Authors: Tingsong Xiao, Zelin Xu, Wenchong He, Jim Su, Yupu Zhang, Raymond Opoku, Ronald Ison, Jason Petho, Jiang Bian, Patrick Tighe, Parisa Rashidi, Zhe Jiang</div><div style='padding-top: 10px; width: 80ex'>Event prediction aims to forecast the time and type of a future event based
on a historical event sequence. Despite its significance, several challenges
exist, including the irregularity of time intervals between consecutive events,
the existence of cycles, periodicity, and multi-scale event interactions, as
well as the high computational costs for long event sequences. Existing neural
temporal point processes (TPPs) methods do not capture the multi-scale nature
of event interactions, which is common in many real-world applications such as
clinical event data. To address these issues, we propose the
cross-temporal-scale transformer (XTSFormer), designed specifically for
irregularly timed event data. Our model comprises two vital components: a novel
Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures
the cyclical nature of time, and a hierarchical multi-scale temporal attention
mechanism. These scales are determined by a bottom-up clustering algorithm.
Extensive experiments on several real-world datasets show that our XTSFormer
outperforms several baseline methods in prediction performance.</div><div><a href='http://arxiv.org/abs/2402.02258v1'>2402.02258v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15846v1")'>Meta-Learning for Neural Network-based Temporal Point Processes</div>
<div id='2401.15846v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T02:42:22Z</div><div>Authors: Yoshiaki Takimoto, Yusuke Tanaka, Tomoharu Iwata, Maya Okawa, Hideaki Kim, Hiroyuki Toda, Takeshi Kurashima</div><div style='padding-top: 10px; width: 80ex'>Human activities generate various event sequences such as taxi trip records,
bike-sharing pick-ups, crime occurrence, and infectious disease transmission.
The point process is widely used in many applications to predict such events
related to human activities. However, point processes present two problems in
predicting events related to human activities. First, recent high-performance
point process models require the input of sufficient numbers of events
collected over a long period (i.e., long sequences) for training, which are
often unavailable in realistic situations. Second, the long-term predictions
required in real-world applications are difficult. To tackle these problems, we
propose a novel meta-learning approach for periodicity-aware prediction of
future events given short sequences. The proposed method first embeds short
sequences into hidden representations (i.e., task representations) via
recurrent neural networks for creating predictions from short sequences. It
then models the intensity of the point process by monotonic neural networks
(MNNs), with the input being the task representations. We transfer the prior
knowledge learned from related tasks and can improve event prediction given
short sequences of target tasks. We design the MNNs to explicitly take temporal
periodic patterns into account, contributing to improved long-term prediction
performance. Experiments on multiple real-world datasets demonstrate that the
proposed method has higher prediction performance than existing alternatives.</div><div><a href='http://arxiv.org/abs/2401.15846v1'>2401.15846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15964v1")'>Spatio-Temporal Attention Graph Neural Network for Remaining Useful Life
  Prediction</div>
<div id='2401.15964v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:49:53Z</div><div>Authors: Zhixin Huang, Yujiang He, Bernhard Sick</div><div style='padding-top: 10px; width: 80ex'>Remaining useful life prediction plays a crucial role in the health
management of industrial systems. Given the increasing complexity of systems,
data-driven predictive models have attracted significant research interest.
Upon reviewing the existing literature, it appears that many studies either do
not fully integrate both spatial and temporal features or employ only a single
attention mechanism. Furthermore, there seems to be inconsistency in the choice
of data normalization methods, particularly concerning operating conditions,
which might influence predictive performance. To bridge these observations,
this study presents the Spatio-Temporal Attention Graph Neural Network. Our
model combines graph neural networks and temporal convolutional neural networks
for spatial and temporal feature extraction, respectively. The cascade of these
extractors, combined with multi-head attention mechanisms for both
spatio-temporal dimensions, aims to improve predictive precision and refine
model explainability. Comprehensive experiments were conducted on the C-MAPSS
dataset to evaluate the impact of unified versus clustering normalization. The
findings suggest that our model performs state-of-the-art results using only
the unified normalization. Additionally, when dealing with datasets with
multiple operating conditions, cluster normalization enhances the performance
of our proposed model by up to 27%.</div><div><a href='http://arxiv.org/abs/2401.15964v1'>2401.15964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09433v1")'>Electrical Behavior Association Mining for Household ShortTerm Energy
  Consumption Forecasting</div>
<div id='2402.09433v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T03:23:09Z</div><div>Authors: Heyang Yu, Yuxi Sun, Yintao Liu, Guangchao Geng, Quanyuan Jiang</div><div style='padding-top: 10px; width: 80ex'>Accurate household short-term energy consumption forecasting (STECF) is
crucial for home energy management, but it is technically challenging, due to
highly random behaviors of individual residential users. To improve the
accuracy of STECF on a day-ahead scale, this paper proposes an novel STECF
methodology that leverages association mining in electrical behaviors. First, a
probabilistic association quantifying and discovering method is proposed to
model the pairwise behaviors association and generate associated clusters.
Then, a convolutional neural network-gated recurrent unit (CNN-GRU) based
forecasting is provided to explore the temporal correlation and enhance
accuracy. The testing results demonstrate that this methodology yields a
significant enhancement in the STECF.</div><div><a href='http://arxiv.org/abs/2402.09433v1'>2402.09433v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07249v1")'>Imputation with Inter-Series Information from Prototypes for Irregular
  Sampled Time Series</div>
<div id='2401.07249v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T10:51:30Z</div><div>Authors: Zhihao Yu, Xu Chu, Liantao Ma, Yasha Wang, Wenwu Zhu</div><div style='padding-top: 10px; width: 80ex'>Irregularly sampled time series are ubiquitous, presenting significant
challenges for analysis due to missing values. Despite existing methods address
imputation, they predominantly focus on leveraging intra-series information,
neglecting the potential benefits that inter-series information could provide,
such as reducing uncertainty and memorization effect. To bridge this gap, we
propose PRIME, a Prototype Recurrent Imputation ModEl, which integrates both
intra-series and inter-series information for imputing missing values in
irregularly sampled time series. Our framework comprises a prototype memory
module for learning inter-series information, a bidirectional gated recurrent
unit utilizing prototype information for imputation, and an attentive
prototypical refinement module for adjusting imputations. We conducted
extensive experiments on three datasets, and the results underscore PRIME's
superiority over the state-of-the-art models by up to 26% relative improvement
on mean square error.</div><div><a href='http://arxiv.org/abs/2401.07249v1'>2401.07249v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02258v1")'>Uncertainty-Aware Deep Attention Recurrent Neural Network for
  Heterogeneous Time Series Imputation</div>
<div id='2401.02258v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T13:21:11Z</div><div>Authors: Linglong Qian, Zina Ibrahim, Richard Dobson</div><div style='padding-top: 10px; width: 80ex'>Missingness is ubiquitous in multivariate time series and poses an obstacle
to reliable downstream analysis. Although recurrent network imputation achieved
the SOTA, existing models do not scale to deep architectures that can
potentially alleviate issues arising in complex data. Moreover, imputation
carries the risk of biased estimations of the ground truth. Yet, confidence in
the imputed values is always unmeasured or computed post hoc from model output.
We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates
missing values and their associated uncertainty in heterogeneous multivariate
time series. By jointly representing feature-wise correlations and temporal
dynamics, we adopt a self attention mechanism, along with an effective residual
component, to achieve a deep recurrent neural network with good imputation
performance and stable convergence. We also leverage self-supervised metric
learning to boost performance by optimizing sample similarity. Finally, we
transform DEARI into a Bayesian neural network through a novel Bayesian
marginalization strategy to produce stochastic DEARI, which outperforms its
deterministic equivalent. Experiments show that DEARI surpasses the SOTA in
diverse imputation tasks using real-world datasets, namely air quality control,
healthcare and traffic.</div><div><a href='http://arxiv.org/abs/2401.02258v1'>2401.02258v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11558v2")'>A Temporally Disentangled Contrastive Diffusion Model for Spatiotemporal
  Imputation</div>
<div id='2402.11558v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T11:59:04Z</div><div>Authors: Yakun Chen, Kaize Shi, Zhangkai Wu, Juan Chen, Xianzhi Wang, Julian McAuley, Guandong Xu, Shui Yu</div><div style='padding-top: 10px; width: 80ex'>Spatiotemporal data analysis is pivotal across various domains, such as
transportation, meteorology, and healthcare. The data collected in real-world
scenarios are often incomplete due to device malfunctions and network errors.
Spatiotemporal imputation aims to predict missing values by exploiting the
spatial and temporal dependencies in the observed data. Traditional imputation
approaches based on statistical and machine learning techniques require the
data to conform to their distributional assumptions, while graph and recurrent
neural networks are prone to error accumulation problems due to their recurrent
structures. Generative models, especially diffusion models, can potentially
circumvent the reliance on inaccurate, previously imputed values for future
predictions; However, diffusion models still face challenges in generating
stable results. We propose to address these challenges by designing conditional
information to guide the generative process and expedite the training process.
We introduce a conditional diffusion framework called C$^2$TSD, which
incorporates disentangled temporal (trend and seasonality) representations as
conditional information and employs contrastive learning to improve
generalizability. Our extensive experiments on three real-world datasets
demonstrate the superior performance of our approach compared to a number of
state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2402.11558v2'>2402.11558v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11960v1")'>CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for
  Spatiotemporal Time Series Imputation</div>
<div id='2403.11960v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:57:16Z</div><div>Authors: Baoyu Jing, Dawei Zhou, Kan Ren, Carl Yang</div><div style='padding-top: 10px; width: 80ex'>Spatiotemporal time series is the foundation of understanding human
activities and their impacts, which is usually collected via monitoring sensors
placed at different locations. The collected data usually contains missing
values due to various failures, which have significant impact on data analysis.
To impute the missing values, a lot of methods have been introduced. When
recovering a specific data point, most existing methods tend to take into
consideration all the information relevant to that point regardless of whether
they have a cause-and-effect relationship. During data collection, it is
inevitable that some unknown confounders are included, e.g., background noise
in time series and non-causal shortcut edges in the constructed sensor network.
These confounders could open backdoor paths between the input and output, in
other words, they establish non-causal correlations between the input and
output. Over-exploiting these non-causal correlations could result in
overfitting and make the model vulnerable to noises. In this paper, we first
revisit spatiotemporal time series imputation from a causal perspective, which
shows the causal relationships among the input, output, embeddings and
confounders. Next, we show how to block the confounders via the frontdoor
adjustment. Based on the results of the frontdoor adjustment, we introduce a
novel Causality-Aware SPatiotEmpoRal graph neural network (CASPER), which
contains a novel Spatiotemporal Causal Attention (SCA) and a Prompt Based
Decoder (PBD). PBD could reduce the impact of confounders and SCA could
discover the sparse causal relationships among embeddings. Theoretical analysis
reveals that SCA discovers causal relationships based on the values of
gradients. We evaluate Casper on three real-world datasets, and the
experimental results show that Casper outperforms the baselines and effectively
discovers causal relationships.</div><div><a href='http://arxiv.org/abs/2403.11960v1'>2403.11960v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00388v1")'>Cumulative Distribution Function based General Temporal Point Processes</div>
<div id='2402.00388v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T07:21:30Z</div><div>Authors: Maolin Wang, Yu Pan, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao, Wanyu Wang, Yiqi Wang, Zitao Liu, Langming Liu</div><div style='padding-top: 10px; width: 80ex'>Temporal Point Processes (TPPs) hold a pivotal role in modeling event
sequences across diverse domains, including social networking and e-commerce,
and have significantly contributed to the advancement of recommendation systems
and information retrieval strategies. Through the analysis of events such as
user interactions and transactions, TPPs offer valuable insights into
behavioral patterns, facilitating the prediction of future trends. However,
accurately forecasting future events remains a formidable challenge due to the
intricate nature of these patterns. The integration of Neural Networks with
TPPs has ushered in the development of advanced deep TPP models. While these
models excel at processing complex and nonlinear temporal data, they encounter
limitations in modeling intensity functions, grapple with computational
complexities in integral computations, and struggle to capture long-range
temporal dependencies effectively. In this study, we introduce the CuFun model,
representing a novel approach to TPPs that revolves around the Cumulative
Distribution Function (CDF). CuFun stands out by uniquely employing a monotonic
neural network for CDF representation, utilizing past events as a scaling
factor. This innovation significantly bolsters the model's adaptability and
precision across a wide range of data scenarios. Our approach addresses several
critical issues inherent in traditional TPP modeling: it simplifies
log-likelihood calculations, extends applicability beyond predefined density
function forms, and adeptly captures long-range temporal patterns. Our
contributions encompass the introduction of a pioneering CDF-based TPP model,
the development of a methodology for incorporating past event information into
future event prediction, and empirical validation of CuFun's effectiveness
through extensive experimentation on synthetic and real-world datasets.</div><div><a href='http://arxiv.org/abs/2402.00388v1'>2402.00388v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04612v1")'>Distribution-Free Conformal Joint Prediction Regions for Neural Marked
  Temporal Point Processes</div>
<div id='2401.04612v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T15:28:29Z</div><div>Authors: Victor Dheur, Tanguy Bosser, Rafael Izbicki, Souhaib Ben Taieb</div><div style='padding-top: 10px; width: 80ex'>Sequences of labeled events observed at irregular intervals in continuous
time are ubiquitous across various fields. Temporal Point Processes (TPPs)
provide a mathematical framework for modeling these sequences, enabling
inferences such as predicting the arrival time of future events and their
associated label, called mark. However, due to model misspecification or lack
of training data, these probabilistic models may provide a poor approximation
of the true, unknown underlying process, with prediction regions extracted from
them being unreliable estimates of the underlying uncertainty. This paper
develops more reliable methods for uncertainty quantification in neural TPP
models via the framework of conformal prediction. A primary objective is to
generate a distribution-free joint prediction region for the arrival time and
mark, with a finite-sample marginal coverage guarantee. A key challenge is to
handle both a strictly positive, continuous response and a categorical
response, without distributional assumptions. We first consider a simple but
overly conservative approach that combines individual prediction regions for
the event arrival time and mark. Then, we introduce a more effective method
based on bivariate highest density regions derived from the joint predictive
density of event arrival time and mark. By leveraging the dependencies between
these two variables, this method exclude unlikely combinations of the two,
resulting in sharper prediction regions while still attaining the pre-specified
coverage level. We also explore the generation of individual univariate
prediction regions for arrival times and marks through conformal regression and
classification techniques. Moreover, we investigate the stronger notion of
conditional coverage. Finally, through extensive experimentation on both
simulated and real-world datasets, we assess the validity and efficiency of
these methods.</div><div><a href='http://arxiv.org/abs/2401.04612v1'>2401.04612v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15730v1")'>Understanding Missingness in Time-series Electronic Health Records for
  Individualized Representation</div>
<div id='2402.15730v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T05:48:39Z</div><div>Authors: Ghadeer O. Ghosheh, Jin Li, Tingting Zhu</div><div style='padding-top: 10px; width: 80ex'>With the widespread of machine learning models for healthcare applications,
there is increased interest in building applications for personalized medicine.
Despite the plethora of proposed research for personalized medicine, very few
focus on representing missingness and learning from the missingness patterns in
time-series Electronic Health Records (EHR) data. The lack of focus on
missingness representation in an individualized way limits the full utilization
of machine learning applications towards true personalization. In this brief
communication, we highlight new insights into patterns of missingness with
real-world examples and implications of missingness in EHRs. The insights in
this work aim to bridge the gap between theoretical assumptions and practical
observations in real-world EHRs. We hope this work will open new doors for
exploring directions for better representation in predictive modelling for true
personalization.</div><div><a href='http://arxiv.org/abs/2402.15730v1'>2402.15730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09558v1")'>Bidirectional Generative Pre-training for Improving Time Series
  Representation Learning</div>
<div id='2402.09558v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T20:19:24Z</div><div>Authors: Ziyang Song, Qincheng Lu, He Zhu, Yue Li</div><div style='padding-top: 10px; width: 80ex'>Learning time-series representations for discriminative tasks has been a
long-standing challenge. Current pre-training methods are limited in either
unidirectional next-token prediction or randomly masked token prediction. We
propose a novel architecture called Bidirectional Timely Generative Pre-trained
Transformer (BiTimelyGPT), which pre-trains on time-series data by both
next-token and previous-token predictions in alternating transformer layers.
This pre-training task preserves original distribution and data shapes of the
time-series. Additionally, the full-rank forward and backward attention
matrices exhibit more expressive representation capabilities. Using biosignal
data, BiTimelyGPT demonstrates superior performance in predicting neurological
functionality, disease diagnosis, and physiological signs. By visualizing the
attention heatmap, we observe that the pre-trained BiTimelyGPT can identify
discriminative segments from time-series sequences, even more so after
fine-tuning on the task.</div><div><a href='http://arxiv.org/abs/2402.09558v1'>2402.09558v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01987v1")'>Representation Learning of Multivariate Time Series using Attention and
  Adversarial Training</div>
<div id='2401.01987v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T21:32:46Z</div><div>Authors: Leon Scharwächter, Sebastian Otte</div><div style='padding-top: 10px; width: 80ex'>A critical factor in trustworthy machine learning is to develop robust
representations of the training data. Only under this guarantee methods are
legitimate to artificially generate data, for example, to counteract imbalanced
datasets or provide counterfactual explanations for blackbox decision-making
systems. In recent years, Generative Adversarial Networks (GANs) have shown
considerable results in forming stable representations and generating realistic
data. While many applications focus on generating image data, less effort has
been made in generating time series data, especially multivariate signals. In
this work, a Transformer-based autoencoder is proposed that is regularized
using an adversarial training scheme to generate artificial multivariate time
series signals. The representation is evaluated using t-SNE visualizations,
Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the
generated signals exhibit higher similarity to an exemplary dataset than using
a convolutional network approach.</div><div><a href='http://arxiv.org/abs/2401.01987v1'>2401.01987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09749v1")'>Towards Diverse Perspective Learning with Selection over Multiple
  Temporal Poolings</div>
<div id='2403.09749v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T05:02:00Z</div><div>Authors: Jihyeon Seong, Jungmin Kim, Jaesik Choi</div><div style='padding-top: 10px; width: 80ex'>In Time Series Classification (TSC), temporal pooling methods that consider
sequential information have been proposed. However, we found that each temporal
pooling has a distinct mechanism, and can perform better or worse depending on
time series data. We term this fixed pooling mechanism a single perspective of
temporal poolings. In this paper, we propose a novel temporal pooling method
with diverse perspective learning: Selection over Multiple Temporal Poolings
(SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among
multiple methods for each data by attention. The dynamic pooling selection is
motivated by the ensemble concept of Multiple Choice Learning (MCL), which
selects the best among multiple outputs. The pooling selection by SoM-TP's
attention enables a non-iterative pooling ensemble within a single classifier.
Additionally, we define a perspective loss and Diverse Perspective Learning
Network (DPLN). The loss works as a regularizer to reflect all the pooling
perspectives from DPLN. Our perspective analysis using Layer-wise Relevance
Propagation (LRP) reveals the limitation of a single perspective and ultimately
demonstrates diverse perspective learning of SoM-TP. We also show that SoM-TP
outperforms CNN models based on other temporal poolings and state-of-the-art
models in TSC with extensive UCR/UEA repositories.</div><div><a href='http://arxiv.org/abs/2403.09749v1'>2403.09749v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05798v1")'>$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM
  for Time Series Forecasting</div>
<div id='2403.05798v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T05:20:48Z</div><div>Authors: Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, Dongjin Song</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been a growing interest in leveraging pre-trained large
language models (LLMs) for various time series applications. However, the
semantic space of LLMs, established through the pre-training, is still
underexplored and may help yield more distinctive and informative
representations to facilitate time series forecasting. To this end, we propose
Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the
pre-trained semantic space with time series embeddings space and perform time
series forecasting based on learned prompts from the joint space. We first
design a tokenization module tailored for cross-modality alignment, which
explicitly concatenates patches of decomposed time series components to create
embeddings that effectively encode the temporal dynamics. Next, we leverage the
pre-trained word token embeddings to derive semantic anchors and align selected
anchors with time series embeddings by maximizing the cosine similarity in the
joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as
prompts to provide strong indicators (context) for time series that exhibit
different temporal dynamics. With thorough empirical studies on multiple
benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve
superior forecasting performance over state-of-the-art baselines. Furthermore,
our ablation studies and visualizations verify the necessity of prompt learning
informed by semantic space.</div><div><a href='http://arxiv.org/abs/2403.05798v1'>2403.05798v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09034v1")'>Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST)
  Activation Under Data Constraints</div>
<div id='2402.09034v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T09:20:13Z</div><div>Authors: Barathi Subramanian, Rathinaraja Jeyaraj, Rakhmonov Akhrorjon Akhmadjon Ugli, Jeonghong Kim</div><div style='padding-top: 10px; width: 80ex'>Activation functions enable neural networks to learn complex representations
by introducing non-linearities. While feedforward models commonly use rectified
linear units, sequential models like recurrent neural networks, long short-term
memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH
activation functions. However, these classical activation functions often
struggle to model sparse patterns when trained on small sequential datasets to
effectively capture temporal dependencies. To address this limitation, we
propose squared Sigmoid TanH (SST) activation specifically tailored to enhance
the learning capability of sequential models under data constraints. SST
applies mathematical squaring to amplify differences between strong and weak
activations as signals propagate over time, facilitating improved gradient flow
and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse
applications, such as sign language recognition, regression, and time-series
classification tasks, where the dataset is limited. Our experiments demonstrate
that SST models consistently outperform RNN-based models with baseline
activations, exhibiting improved test accuracy.</div><div><a href='http://arxiv.org/abs/2402.09034v1'>2402.09034v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01690v1")'>Linguistic-Based Mild Cognitive Impairment Detection Using Informative
  Loss</div>
<div id='2402.01690v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T16:30:22Z</div><div>Authors: Ali Pourramezan Fard, Mohammad H. Mahoor, Muath Alsuhaibani, Hiroko H. Dodgec</div><div style='padding-top: 10px; width: 80ex'>This paper presents a deep learning method using Natural Language Processing
(NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and
Normal Cognitive (NC) conditions in older adults. We propose a framework that
analyzes transcripts generated from video interviews collected within the
I-CONECT study project, a randomized controlled trial aimed at improving
cognitive functions through video chats. Our proposed NLP framework consists of
two Transformer-based modules, namely Sentence Embedding (SE) and Sentence
Cross Attention (SCA). First, the SE module captures contextual relationships
between words within each sentence. Subsequently, the SCA module extracts
temporal features from a sequence of sentences. This feature is then used by a
Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC.
To build a robust model, we propose a novel loss function, called InfoLoss,
that considers the reduction in entropy by observing each sequence of sentences
to ultimately enhance the classification accuracy. The results of our
comprehensive model evaluation using the I-CONECT dataset show that our
framework can distinguish between MCI and NC with an average area under the
curve of 84.75%.</div><div><a href='http://arxiv.org/abs/2402.01690v1'>2402.01690v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.18546v3")'>Generalizability Under Sensor Failure: Tokenization + Transformers
  Enable More Robust Latent Spaces</div>
<div id='2402.18546v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T18:29:25Z</div><div>Authors: Geeling Chau, Yujin An, Ahamed Raffey Iqbal, Soon-Jo Chung, Yisong Yue, Sabera Talukder</div><div style='padding-top: 10px; width: 80ex'>A major goal in neuroscience is to discover neural data representations that
generalize. This goal is challenged by variability along recording sessions
(e.g. environment), subjects (e.g. varying neural structures), and sensors
(e.g. sensor noise), among others. Recent work has begun to address
generalization across sessions and subjects, but few study robustness to sensor
failure which is highly prevalent in neuroscience experiments. In order to
address these generalizability dimensions we first collect our own
electroencephalography dataset with numerous sessions, subjects, and sensors,
then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM
(Talukder et al., 2024). EEGNet is a widely used convolutional neural network,
while TOTEM is a discrete time series tokenizer and transformer model. We find
that TOTEM outperforms or matches EEGNet across all generalizability cases.
Finally through analysis of TOTEM's latent codebook we observe that
tokenization enables generalization.</div><div><a href='http://arxiv.org/abs/2402.18546v3'>2402.18546v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10859v1")'>Neural-Kernel Conditional Mean Embeddings</div>
<div id='2403.10859v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T08:51:02Z</div><div>Authors: Eiki Shimizu, Kenji Fukumizu, Dino Sejdinovic</div><div style='padding-top: 10px; width: 80ex'>Kernel conditional mean embeddings (CMEs) offer a powerful framework for
representing conditional distribution, but they often face scalability and
expressiveness challenges. In this work, we propose a new method that
effectively combines the strengths of deep learning with CMEs in order to
address these challenges. Specifically, our approach leverages the end-to-end
neural network (NN) optimization framework using a kernel-based objective. This
design circumvents the computationally expensive Gram matrix inversion required
by current CME methods. To further enhance performance, we provide efficient
strategies to optimize the remaining kernel hyperparameters. In conditional
density estimation tasks, our NN-CME hybrid achieves competitive performance
and often surpasses existing deep learning-based methods. Lastly, we showcase
its remarkable versatility by seamlessly integrating it into reinforcement
learning (RL) contexts. Building on Q-learning, our approach naturally leads to
a new variant of distributional RL methods, which demonstrates consistent
effectiveness across different environments.</div><div><a href='http://arxiv.org/abs/2403.10859v1'>2403.10859v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15404v1")'>United We Pretrain, Divided We Fail! Representation Learning for Time
  Series by Pretraining on 75 Datasets at Once</div>
<div id='2402.15404v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:06:38Z</div><div>Authors: Maurice Kraus, Felix Divo, David Steinmann, Devendra Singh Dhami, Kristian Kersting</div><div style='padding-top: 10px; width: 80ex'>In natural language processing and vision, pretraining is utilized to learn
effective representations. Unfortunately, the success of pretraining does not
easily carry over to time series due to potential mismatch between sources and
target. Actually, common belief is that multi-dataset pretraining does not work
for time series! Au contraire, we introduce a new self-supervised contrastive
pretraining approach to learn one encoding from many unlabeled and diverse time
series datasets, so that the single learned representation can then be reused
in several target domains for, say, classification. Specifically, we propose
the XD-MixUp interpolation method and the Soft Interpolation Contextual
Contrasting (SICC) loss. Empirically, this outperforms both supervised training
and other self-supervised pretraining methods when finetuning on low-data
regimes. This disproves the common belief: We can actually learn from multiple
time series datasets, even from 75 at once.</div><div><a href='http://arxiv.org/abs/2402.15404v1'>2402.15404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02475v1")'>TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling</div>
<div id='2402.02475v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T13:10:51Z</div><div>Authors: Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin Wang, Mingsheng Long</div><div style='padding-top: 10px; width: 80ex'>Time series pre-training has recently garnered wide attention for its
potential to reduce labeling expenses and benefit various downstream tasks.
Prior methods are mainly based on pre-training techniques well-acknowledged in
vision or language, such as masked modeling and contrastive learning. However,
randomly masking time series or calculating series-wise similarity will distort
or neglect inherent temporal correlations crucial in time series data. To
emphasize temporal correlation modeling, this paper proposes TimeSiam as a
simple but effective self-supervised pre-training framework for Time series
based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to
capture intrinsic temporal correlations between randomly sampled past and
current subseries. With a simple data augmentation method (e.g.~masking),
TimeSiam can benefit from diverse augmented subseries and learn internal
time-dependent representations through a past-to-current reconstruction.
Moreover, learnable lineage embeddings are also introduced to distinguish
temporal distance between sampled series and further foster the learning of
diverse temporal correlations. TimeSiam consistently outperforms extensive
advanced pre-training baselines, demonstrating superior forecasting and
classification capabilities across 13 standard benchmarks in both intra- and
cross-domain scenarios.</div><div><a href='http://arxiv.org/abs/2402.02475v1'>2402.02475v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03698v1")'>Towards Controllable Time Series Generation</div>
<div id='2403.03698v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T13:27:34Z</div><div>Authors: Yifan Bao, Yihao Ang, Qiang Huang, Anthony K. H. Tung, Zhiyong Huang</div><div style='padding-top: 10px; width: 80ex'>Time Series Generation (TSG) has emerged as a pivotal technique in
synthesizing data that accurately mirrors real-world time series, becoming
indispensable in numerous applications. Despite significant advancements in
TSG, its efficacy frequently hinges on having large training datasets. This
dependency presents a substantial challenge in data-scarce scenarios,
especially when dealing with rare or unique conditions. To confront these
challenges, we explore a new problem of Controllable Time Series Generation
(CTSG), aiming to produce synthetic time series that can adapt to various
external conditions, thereby tackling the data scarcity issue.
  In this paper, we propose \textbf{C}ontrollable \textbf{T}ime \textbf{S}eries
(\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key
feature of \textsf{CTS} is that it decouples the mapping process from standard
VAE training, enabling precise learning of a complex interplay between latent
features and external conditions. Moreover, we develop a comprehensive
evaluation scheme for CTSG. Extensive experiments across three real-world time
series datasets showcase \textsf{CTS}'s exceptional capabilities in generating
high-quality, controllable outputs. This underscores its adeptness in
seamlessly integrating latent features with external conditions. Extending
\textsf{CTS} to the image domain highlights its remarkable potential for
explainability and further reinforces its versatility across different
modalities.</div><div><a href='http://arxiv.org/abs/2403.03698v1'>2403.03698v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06576v1")'>FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing
  Fourier Transform and Auto-encoder</div>
<div id='2403.06576v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:26:04Z</div><div>Authors: Yang Chen, Dustin J. Kempton, Rafal A. Angryk</div><div style='padding-top: 10px; width: 80ex'>The success of deep learning-based generative models in producing realistic
images, videos, and audios has led to a crucial consideration: how to
effectively assess the quality of synthetic samples. While the Fr\'{e}chet
Inception Distance (FID) serves as the standard metric for evaluating
generative models in image synthesis, a comparable metric for time series data
is notably absent. This gap in assessment capabilities stems from the absence
of a widely accepted feature vector extractor pre-trained on benchmark time
series datasets. In addressing these challenges related to assessing the
quality of time series, particularly in the context of Fr\'echet Distance, this
work proposes a novel solution leveraging the Fourier transform and
Auto-encoder, termed the Fr\'{e}chet Fourier-transform Auto-encoder Distance
(FFAD). Through our experimental results, we showcase the potential of FFAD for
effectively distinguishing samples from different classes. This novel metric
emerges as a fundamental tool for the evaluation of generative time series
data, contributing to the ongoing efforts of enhancing assessment methodologies
in the realm of deep learning-based generative models.</div><div><a href='http://arxiv.org/abs/2403.06576v1'>2403.06576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09809v1")'>Self-Supervised Learning for Time Series: Contrastive or Generative?</div>
<div id='2403.09809v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T18:58:06Z</div><div>Authors: Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) has recently emerged as a powerful approach to
learning representations from large-scale unlabeled data, showing promising
results in time series analysis. The self-supervised representation learning
can be categorized into two mainstream: contrastive and generative. In this
paper, we will present a comprehensive comparative study between contrastive
and generative methods in time series. We first introduce the basic frameworks
for contrastive and generative SSL, respectively, and discuss how to obtain the
supervision signal that guides the model optimization. We then implement
classical algorithms (SimCLR vs. MAE) for each type and conduct a comparative
analysis in fair settings. Our results provide insights into the strengths and
weaknesses of each approach and offer practical recommendations for choosing
suitable SSL methods. We also discuss the implications of our findings for the
broader field of representation learning and propose future research
directions. All the code and data are released at
\url{https://github.com/DL4mHealth/SSL_Comparison}.</div><div><a href='http://arxiv.org/abs/2403.09809v1'>2403.09809v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12035v1")'>Class-incremental Learning for Time Series: Benchmark and Evaluation</div>
<div id='2402.12035v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T10:43:13Z</div><div>Authors: Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H Le, P. N. Suganthan, Xudong Jiang, Ramasamy Savitha</div><div style='padding-top: 10px; width: 80ex'>Real-world environments are inherently non-stationary, frequently introducing
new classes over time. This is especially common in time series classification,
such as the emergence of new disease classification in healthcare or the
addition of new activities in human activity recognition. In such cases, a
learning system is required to assimilate novel classes effectively while
avoiding catastrophic forgetting of the old ones, which gives rise to the
Class-incremental Learning (CIL) problem. However, despite the encouraging
progress in the image and language domains, CIL for time series data remains
relatively understudied. Existing studies suffer from inconsistent experimental
designs, necessitating a comprehensive evaluation and benchmarking of methods
across a wide range of datasets. To this end, we first present an overview of
the Time Series Class-incremental Learning (TSCIL) problem, highlight its
unique challenges, and cover the advanced methodologies. Further, based on
standardized settings, we develop a unified experimental framework that
supports the rapid development of new algorithms, easy integration of new
datasets, and standardization of the evaluation process. Using this framework,
we conduct a comprehensive evaluation of various generic and
time-series-specific CIL methods in both standard and privacy-sensitive
scenarios. Our extensive experiments not only provide a standard baseline to
support future research but also shed light on the impact of various design
factors such as normalization layers or memory budget thresholds. Codes are
available at https://github.com/zqiao11/TSCIL.</div><div><a href='http://arxiv.org/abs/2402.12035v1'>2402.12035v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10787v1")'>Time Series Representation Learning with Supervised Contrastive Temporal
  Transformer</div>
<div id='2403.10787v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T03:37:19Z</div><div>Authors: Yuansan Liu, Sudanthi Wijewickrema, Christofer Bester, Stephen O'Leary, James Bailey</div><div style='padding-top: 10px; width: 80ex'>Finding effective representations for time series data is a useful but
challenging task. Several works utilize self-supervised or unsupervised
learning methods to address this. However, there still remains the open
question of how to leverage available label information for better
representations. To answer this question, we exploit pre-existing techniques in
time series and representation learning domains and develop a simple, yet novel
fusion model, called: \textbf{S}upervised \textbf{CO}ntrastive
\textbf{T}emporal \textbf{T}ransformer (SCOTT). We first investigate suitable
augmentation methods for various types of time series data to assist with
learning change-invariant representations. Secondly, we combine Transformer and
Temporal Convolutional Networks in a simple way to efficiently learn both
global and local features. Finally, we simplify Supervised Contrastive Loss for
representation learning of labelled time series data. We preliminarily evaluate
SCOTT on a downstream task, Time Series Classification, using 45 datasets from
the UCR archive. The results show that with the representations learnt by
SCOTT, even a weak classifier can perform similar to or better than existing
state-of-the-art models (best performance on 23/45 datasets and highest rank
against 9 baseline models). Afterwards, we investigate SCOTT's ability to
address a real-world task, online Change Point Detection (CPD), on two
datasets: a human activity dataset and a surgical patient dataset. We show that
the model performs with high reliability and efficiency on the online CPD
problem ($\sim$98\% and $\sim$97\% area under precision-recall curve
respectively). Furthermore, we demonstrate the model's potential in tackling
early detection and show it performs best compared to other candidates.</div><div><a href='http://arxiv.org/abs/2403.10787v1'>2403.10787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01479v2")'>Kernel-U-Net: Symmetric and Hierarchical Architecture for Multivariate
  Time Series Forecasting</div>
<div id='2401.01479v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T00:49:51Z</div><div>Authors: Jiang You, René Natowicz, Arben Cela, Jacob Ouanounou, Patrick Siarry</div><div style='padding-top: 10px; width: 80ex'>Time series forecasting task predicts future trends based on historical
information. Transformer-based U-Net architectures, despite their success in
medical image segmentation, have limitations in both expressiveness and
computation efficiency in time series forecasting as evidenced in YFormer. To
tackle these challenges, we introduce Kernel-U-Net, a symmetric and
hierarchical U-shape neural network architecture. The kernel-U-Net encoder
compresses gradually input series into latent vectors, and its symmetric
decoder subsequently expands these vectors into output series. Specifically,
Kernel-U-Net separates the procedure of partitioning input time series into
patches from kernel manipulation, thereby providing the convenience of
executing customized kernels. Our method offers two primary advantages: 1)
Flexibility in kernel customization to adapt to specific datasets; 2) Enhanced
computational efficiency, with the complexity of the Transformer layer reduced
to linear. Experiments on seven real-world datasets, considering both
multivariate and univariate settings, demonstrate that Kernel-U-Net's
performance either exceeds or meets that of the existing state-of-the-art model
PatchTST in the majority of cases and outperforms Yformer. The source code for
Kernel-U-Net will be made publicly available for further research and
application.</div><div><a href='http://arxiv.org/abs/2401.01479v2'>2401.01479v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13233v2")'>SMORE: Similarity-based Hyperdimensional Domain Adaptation for
  Multi-Sensor Time Series Classification</div>
<div id='2402.13233v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:48:49Z</div><div>Authors: Junyao Wang, Mohammad Abdullah Al Faruque</div><div style='padding-top: 10px; width: 80ex'>Many real-world applications of the Internet of Things (IoT) employ machine
learning (ML) algorithms to analyze time series information collected by
interconnected sensors. However, distribution shift, a fundamental challenge in
data-driven ML, arises when a model is deployed on a data distribution
different from the training data and can substantially degrade model
performance. Additionally, increasingly sophisticated deep neural networks
(DNNs) are required to capture intricate spatial and temporal dependencies in
multi-sensor time series data, often exceeding the capabilities of today's edge
devices. In this paper, we propose SMORE, a novel resource-efficient domain
adaptation (DA) algorithm for multi-sensor time series classification,
leveraging the efficient and parallel operations of hyperdimensional computing.
SMORE dynamically customizes test-time models with explicit consideration of
the domain context of each sample to mitigate the negative impacts of domain
shifts. Our evaluation on a variety of multi-sensor time series classification
tasks shows that SMORE achieves on average 1.98% higher accuracy than
state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and
4.63x faster inference.</div><div><a href='http://arxiv.org/abs/2402.13233v2'>2402.13233v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09782v3")'>MC-DBN: A Deep Belief Network-Based Model for Modality Completion</div>
<div id='2402.09782v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T08:21:50Z</div><div>Authors: Zihong Luo, Zheng Tao, Yuxuan Huang, Kexin He, Chengzhi Liu</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in multi-modal artificial intelligence (AI) have
revolutionized the fields of stock market forecasting and heart rate
monitoring. Utilizing diverse data sources can substantially improve prediction
accuracy. Nonetheless, additional data may not always align with the original
dataset. Interpolation methods are commonly utilized for handling missing
values in modal data, though they may exhibit limitations in the context of
sparse information. Addressing this challenge, we propose a Modality Completion
Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit
features of complete data to compensate for gaps between itself and additional
incomplete data. It ensures that the enhanced multi-modal data closely aligns
with the dynamic nature of the real world to enhance the effectiveness of the
model. We conduct evaluations of the MC-DBN model in two datasets from the
stock market forecasting and heart rate monitoring domains. Comprehensive
experiments showcase the model's capacity to bridge the semantic divide present
in multi-modal data, subsequently enhancing its performance. The source code is
available at: https://github.com/logan-0623/DBN-generate</div><div><a href='http://arxiv.org/abs/2402.09782v3'>2402.09782v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12641v1")'>Automated Contrastive Learning Strategy Search for Time Series</div>
<div id='2403.12641v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T11:24:14Z</div><div>Authors: Baoyu Jing, Yansen Wang, Guoxin Sui, Jing Hong, Jingrui He, Yuqing Yang, Dongsheng Li, Kan Ren</div><div style='padding-top: 10px; width: 80ex'>In recent years, Contrastive Learning (CL) has become a predominant
representation learning paradigm for time series. Most existing methods in the
literature focus on manually building specific Contrastive Learning Strategies
(CLS) by human heuristics for certain datasets and tasks. However, manually
developing CLS usually require excessive prior knowledge about the datasets and
tasks, e.g., professional cognition of the medical time series in healthcare,
as well as huge human labor and massive experiments to determine the detailed
learning configurations. In this paper, we present an Automated Machine
Learning (AutoML) practice at Microsoft, which automatically learns to
contrastively learn representations for various time series datasets and tasks,
namely Automated Contrastive Learning (AutoCL). We first construct a principled
universal search space of size over 3x1012, covering data augmentation,
embedding transformation, contrastive pair construction and contrastive losses.
Further, we introduce an efficient reinforcement learning algorithm, which
optimizes CLS from the performance on the validation tasks, to obtain more
effective CLS within the space. Experimental results on various real-world
tasks and datasets demonstrate that AutoCL could automatically find the
suitable CLS for a given dataset and task. From the candidate CLS found by
AutoCL on several public datasets/tasks, we compose a transferable Generally
Good Strategy (GGS), which has a strong performance for other datasets. We also
provide empirical analysis as a guidance for future design of CLS.</div><div><a href='http://arxiv.org/abs/2403.12641v1'>2403.12641v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.18057v1")'>Rank Supervised Contrastive Learning for Time Series Classification</div>
<div id='2401.18057v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T18:29:10Z</div><div>Authors: Qianying Ren, Dongsheng Luo, Dongjin Song</div><div style='padding-top: 10px; width: 80ex'>Recently, various contrastive learning techniques have been developed to
categorize time series data and exhibit promising performance. A general
paradigm is to utilize appropriate augmentations and construct feasible
positive samples such that the encoder can yield robust and discriminative
representations by mapping similar data points closer together in the feature
space while pushing dissimilar data points farther apart. Despite its efficacy,
the fine-grained relative similarity (e.g., rank) information of positive
samples is largely ignored, especially when labeled samples are limited. To
this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform
time series classification. Different from conventional contrastive learning
frameworks, RankSCL augments raw data in a targeted way in the embedding space
and adopts certain filtering rules to select more informative positive and
negative pairs of samples. Moreover, a novel rank loss is developed to assign
different weights for different levels of positive samples, enable the encoder
to extract the fine-grained information of the same class, and produce a clear
boundary among different classes. Thoroughly empirical studies on 128 UCR
datasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve
state-of-the-art performance compared to existing baseline methods.</div><div><a href='http://arxiv.org/abs/2401.18057v1'>2401.18057v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05431v1")'>TRLS: A Time Series Representation Learning Framework via Spectrogram
  for Medical Signal Processing</div>
<div id='2401.05431v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T02:26:02Z</div><div>Authors: Luyuan Xie, Cong Li, Xin Zhang, Shengfang Zhai, Yuejian Fang, Qingni Shen, Zhonghai Wu</div><div style='padding-top: 10px; width: 80ex'>Representation learning frameworks in unlabeled time series have been
proposed for medical signal processing. Despite the numerous excellent
progresses have been made in previous works, we observe the representation
extracted for the time series still does not generalize well. In this paper, we
present a Time series (medical signal) Representation Learning framework via
Spectrogram (TRLS) to get more informative representations. We transform the
input time-domain medical signals into spectrograms and design a time-frequency
encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale
representations from the augmented spectrograms. Our TRLS takes spectrogram as
input with two types of different data augmentations and maximizes the
similarity between positive ones, which effectively circumvents the problem of
designing negative samples. Our evaluation of four real-world medical signal
datasets focusing on medical signal classification shows that TRLS is superior
to the existing frameworks.</div><div><a href='http://arxiv.org/abs/2401.05431v1'>2401.05431v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08940v1")'>CEL: A Continual Learning Model for Disease Outbreak Prediction by
  Leveraging Domain Adaptation via Elastic Weight Consolidation</div>
<div id='2401.08940v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T03:26:04Z</div><div>Authors: Saba Aslam, Abdur Rasool, Hongyan Wu, Xiaoli Li</div><div style='padding-top: 10px; width: 80ex'>Continual learning, the ability of a model to learn over time without
forgetting previous knowledge and, therefore, be adaptive to new data, is
paramount in dynamic fields such as disease outbreak prediction. Deep neural
networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This
study introduces a novel CEL model for continual learning by leveraging domain
adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate
the catastrophic forgetting phenomenon in a domain incremental setting. The
Fisher Information Matrix (FIM) is constructed with EWC to develop a
regularization term that penalizes changes to important parameters, namely, the
important previous knowledge. CEL's performance is evaluated on three distinct
diseases, Influenza, Mpox, and Measles, with different metrics. The high
R-squared values during evaluation and reevaluation outperform the other
state-of-the-art models in several contexts, indicating that CEL adapts to
incremental data well. CEL's robustness and reliability are underscored by its
minimal 65% forgetting rate and 18% higher memory stability compared to
existing benchmark studies. This study highlights CEL's versatility in disease
outbreak prediction, addressing evolving data with temporal patterns. It offers
a valuable model for proactive disease control with accurate, timely
predictions.</div><div><a href='http://arxiv.org/abs/2401.08940v1'>2401.08940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07245v1")'>Dataset Condensation for Time Series Classification via Dual Domain
  Matching</div>
<div id='2403.07245v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T02:05:06Z</div><div>Authors: Zhanyu Liu, Ke Hao, Guanjie Zheng, Yanwei Yu</div><div style='padding-top: 10px; width: 80ex'>Time series data has been demonstrated to be crucial in various research
fields. The management of large quantities of time series data presents
challenges in terms of deep learning tasks, particularly for training a deep
neural network. Recently, a technique named \textit{Dataset Condensation} has
emerged as a solution to this problem. This technique generates a smaller
synthetic dataset that has comparable performance to the full real dataset in
downstream tasks such as classification. However, previous methods are
primarily designed for image and graph datasets, and directly adapting them to
the time series dataset leads to suboptimal performance due to their inability
to effectively leverage the rich information inherent in time series data,
particularly in the frequency domain. In this paper, we propose a novel
framework named Dataset \textit{\textbf{Cond}}ensation for
\textit{\textbf{T}}ime \textit{\textbf{S}}eries
\textit{\textbf{C}}lassification via Dual Domain Matching (\textbf{CondTSC})
which focuses on the time series classification dataset condensation task.
Different from previous methods, our proposed framework aims to generate a
condensed dataset that matches the surrogate objectives in both the time and
frequency domains. Specifically, CondTSC incorporates multi-view data
augmentation, dual domain training, and dual surrogate objectives to enhance
the dataset condensation process in the time and frequency domains. Through
extensive experiments, we demonstrate the effectiveness of our proposed
framework, which outperforms other baselines and learns a condensed synthetic
dataset that exhibits desirable characteristics such as conforming to the
distribution of the original data.</div><div><a href='http://arxiv.org/abs/2403.07245v1'>2403.07245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12372v1")'>Learning Transferable Time Series Classifier with Cross-Domain
  Pre-training from Language Model</div>
<div id='2403.12372v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:32:47Z</div><div>Authors: Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, Chenyi Lei</div><div style='padding-top: 10px; width: 80ex'>Advancements in self-supervised pre-training (SSL) have significantly
advanced the field of learning transferable time series representations, which
can be very useful in enhancing the downstream task. Despite being effective,
most existing works struggle to achieve cross-domain SSL pre-training, missing
valuable opportunities to integrate patterns and features from different
domains. The main challenge lies in the significant differences in the
characteristics of time-series data across different domains, such as
variations in the number of channels and temporal resolution scales. To address
this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning
framework to learn transferable knowledge from various domains to largely
benefit the target downstream task. One of the key characteristics of
CrossTimeNet is the newly designed time series tokenization module, which could
effectively convert the raw time series into a sequence of discrete tokens
based on a reconstruction optimization process. Besides, we highlight that
predicting a high proportion of corrupted tokens can be very helpful for
extracting informative patterns across different domains during SSL
pre-training, which has been largely overlooked in past years. Furthermore,
unlike previous works, our work treats the pre-training language model (PLM) as
the initialization of the encoder network, investigating the feasibility of
transferring the knowledge learned by the PLM to the time series area. Through
these efforts, the path to cross-domain pre-training of a generic time series
model can be effectively paved. We conduct extensive experiments in a
real-world scenario across various time series classification domains. The
experimental results clearly confirm CrossTimeNet's superior performance.</div><div><a href='http://arxiv.org/abs/2403.12372v1'>2403.12372v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12371v1")'>Advancing Time Series Classification with Multimodal Language Modeling</div>
<div id='2403.12371v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:32:24Z</div><div>Authors: Mingyue Cheng, Yiheng Chen, Qi Liu, Zhiding Liu, Yucong Luo</div><div style='padding-top: 10px; width: 80ex'>For the advancements of time series classification, scrutinizing previous
studies, most existing methods adopt a common learning-to-classify paradigm - a
time series classifier model tries to learn the relation between sequence
inputs and target label encoded by one-hot distribution. Although effective,
this paradigm conceals two inherent limitations: (1) encoding target categories
with one-hot distribution fails to reflect the comparability and similarity
between labels, and (2) it is very difficult to learn transferable model across
domains, which greatly hinder the development of universal serving paradigm. In
this work, we propose InstructTime, a novel attempt to reshape time series
classification as a learning-to-generate paradigm. Relying on the powerful
generative capacity of the pre-trained language model, the core idea is to
formulate the classification of time series as a multimodal understanding task,
in which both task-specific instructions and raw time series are treated as
multimodal inputs while the label information is represented by texts. To
accomplish this goal, three distinct designs are developed in the InstructTime.
Firstly, a time series discretization module is designed to convert continuous
time series into a sequence of hard tokens to solve the inconsistency issue
across modal inputs. To solve the modality representation gap issue, for one
thing, we introduce an alignment projected layer before feeding the transformed
token of time series into language models. For another, we highlight the
necessity of auto-regressive pre-training across domains, which can facilitate
the transferability of the language model and boost the generalization
performance. Extensive experiments are conducted over benchmark datasets, whose
results uncover the superior performance of InstructTime and the potential for
a universal foundation model in time series classification.</div><div><a href='http://arxiv.org/abs/2403.12371v1'>2403.12371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02370v1")'>AutoTimes: Autoregressive Time Series Forecasters via Large Language
  Models</div>
<div id='2402.02370v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T06:59:21Z</div><div>Authors: Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long</div><div style='padding-top: 10px; width: 80ex'>Foundation models of time series have not been fully developed due to the
limited availability of large-scale time series and the underexploration of
scalable pre-training. Based on the similar sequential structure of time series
and natural language, increasing research demonstrates the feasibility of
leveraging large language models (LLM) for time series. Nevertheless, prior
methods may overlook the consistency in aligning time series and natural
language, resulting in insufficient utilization of the LLM potentials. To fully
exploit the general-purpose token transitions learned from language modeling,
we propose AutoTimes to repurpose LLMs as Autoregressive Time series
forecasters, which is consistent with the acquisition and utilization of LLMs
without updating the parameters. The consequent forecasters can handle flexible
series lengths and achieve competitive performance as prevalent models.
Further, we present token-wise prompting that utilizes corresponding timestamps
to make our method applicable to multimodal scenarios. Analysis demonstrates
our forecasters inherit zero-shot and in-context learning capabilities of LLMs.
Empirically, AutoTimes exhibits notable method generality and achieves enhanced
performance by basing on larger LLMs, additional texts, or time series as
instructions.</div><div><a href='http://arxiv.org/abs/2402.02370v1'>2402.02370v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05531v2")'>VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational
  Inference for Improved Generalization in Audio Pattern Recognition</div>
<div id='2401.05531v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T19:55:44Z</div><div>Authors: John Fischer, Marko Orescanin, Eric Eckstrand</div><div style='padding-top: 10px; width: 80ex'>Transfer learning (TL) is an increasingly popular approach to training deep
learning (DL) models that leverages the knowledge gained by training a
foundation model on diverse, large-scale datasets for use on downstream tasks
where less domain- or task-specific data is available. The literature is rich
with TL techniques and applications; however, the bulk of the research makes
use of deterministic DL models which are often uncalibrated and lack the
ability to communicate a measure of epistemic (model) uncertainty in
prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models
are often well-calibrated, provide access to epistemic uncertainty for a
prediction, and are capable of achieving competitive predictive performance. In
this study, we propose variational inference pre-trained audio neural networks
(VI-PANNs). VI-PANNs are a variational inference variant of the popular
ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio
event detection dataset. We evaluate the quality of the resulting uncertainty
when transferring knowledge from VI-PANNs to other downstream acoustic
classification tasks using the ESC-50, UrbanSound8K, and DCASE2013 datasets. We
demonstrate, for the first time, that it is possible to transfer calibrated
uncertainty information along with knowledge from upstream tasks to enhance a
model's capability to perform downstream tasks.</div><div><a href='http://arxiv.org/abs/2401.05531v2'>2401.05531v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02135v1")'>PosCUDA: Position based Convolution for Unlearnable Audio Datasets</div>
<div id='2401.02135v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T08:39:49Z</div><div>Authors: Vignesh Gokul, Shlomo Dubnov</div><div style='padding-top: 10px; width: 80ex'>Deep learning models require large amounts of clean data to acheive good
performance. To avoid the cost of expensive data acquisition, researchers use
the abundant data available on the internet. This raises significant privacy
concerns on the potential misuse of personal data for model training without
authorisation. Recent works such as CUDA propose solutions to this problem by
adding class-wise blurs to make datasets unlearnable, i.e a model can never use
the acquired dataset for learning. However these methods often reduce the
quality of the data making it useless for practical applications. We introduce
PosCUDA, a position based convolution for creating unlearnable audio datasets.
PosCUDA uses class-wise convolutions on small patches of audio. The location of
the patches are based on a private key for each class, hence the model learns
the relations between positional blurs and labels, while failing to generalize.
We empirically show that PosCUDA can achieve unlearnability while maintaining
the quality of the original audio datasets. Our proposed method is also robust
to different audio feature representations such as MFCC, raw audio and
different architectures such as transformers, convolutional networks etc.</div><div><a href='http://arxiv.org/abs/2401.02135v1'>2401.02135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15123v1")'>Large Language Model Guided Knowledge Distillation for Time Series
  Anomaly Detection</div>
<div id='2401.15123v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T09:51:07Z</div><div>Authors: Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, Wenchao Meng</div><div style='padding-top: 10px; width: 80ex'>Self-supervised methods have gained prominence in time series anomaly
detection due to the scarcity of available annotations. Nevertheless, they
typically demand extensive training data to acquire a generalizable
representation map, which conflicts with scenarios of a few available samples,
thereby limiting their performance. To overcome the limitation, we propose
\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly
detection approach where the student network is trained to mimic the features
of the large language model (LLM)-based teacher network that is pretrained on
large-scale datasets. During the testing phase, anomalies are detected when the
discrepancy between the features of the teacher and student networks is large.
To circumvent the student network from learning the teacher network's feature
of anomalous samples, we devise two key strategies. 1) Prototypical signals are
incorporated into the student network to consolidate the normal feature
extraction. 2) We use synthetic anomalies to enlarge the representation gap
between the two networks. AnomalyLLM demonstrates state-of-the-art performance
on 15 datasets, improving accuracy by at least 14.5\% in the UCR dataset.</div><div><a href='http://arxiv.org/abs/2401.15123v1'>2401.15123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11235v1")'>TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly
  Detection with Inexact Supervision</div>
<div id='2401.11235v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T14:15:04Z</div><div>Authors: Chen Liu, Shibo He, Haoyu Liu, Shizhong Li</div><div style='padding-top: 10px; width: 80ex'>Time series anomaly detection (TSAD) plays a vital role in various domains
such as healthcare, networks, and industry. Considering labels are crucial for
detection but difficult to obtain, we turn to TSAD with inexact supervision:
only series-level labels are provided during the training phase, while
point-level anomalies are predicted during the testing phase. Previous works
follow a traditional multi-instance learning (MIL) approach, which focuses on
encouraging high anomaly scores at individual time steps. However, time series
anomalies are not only limited to individual point anomalies, they can also be
collective anomalies, typically exhibiting abnormal patterns over subsequences.
To address the challenge of collective anomalies, in this paper, we propose a
tree-based MIL framework (TreeMIL). We first adopt an N-ary tree structure to
divide the entire series into multiple nodes, where nodes at different levels
represent subsequences with different lengths. Then, the subsequence features
are extracted to determine the presence of collective anomalies. Finally, we
calculate point-level anomaly scores by aggregating features from nodes at
different levels. Experiments conducted on seven public datasets and eight
baselines demonstrate that TreeMIL achieves an average 32.3% improvement in F1-
score compared to previous state-of-the-art methods. The code is available at
https://github.com/fly-orange/TreeMIL.</div><div><a href='http://arxiv.org/abs/2401.11235v1'>2401.11235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12362v1")'>DMAD: Dual Memory Bank for Real-World Anomaly Detection</div>
<div id='2403.12362v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:16:32Z</div><div>Authors: Jianlong Hu, Xu Chen, Zhenye Gan, Jinlong Peng, Shengchuan Zhang, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Liujuan Cao, Rongrong Ji</div><div style='padding-top: 10px; width: 80ex'>Training a unified model is considered to be more suitable for practical
industrial anomaly detection scenarios due to its generalization ability and
storage efficiency. However, this multi-class setting, which exclusively uses
normal data, overlooks the few but important accessible annotated anomalies in
the real world. To address the challenge of real-world anomaly detection, we
propose a new framework named Dual Memory bank enhanced representation learning
for Anomaly Detection (DMAD). This framework handles both unsupervised and
semi-supervised scenarios in a unified (multi-class) setting. DMAD employs a
dual memory bank to calculate feature distance and feature attention between
normal and abnormal patterns, thereby encapsulating knowledge about normal and
abnormal instances. This knowledge is then used to construct an enhanced
representation for anomaly score learning. We evaluated DMAD on the MVTec-AD
and VisA datasets. The results show that DMAD surpasses current
state-of-the-art methods, highlighting DMAD's capability in handling the
complexities of real-world anomaly detection scenarios.</div><div><a href='http://arxiv.org/abs/2403.12362v1'>2403.12362v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08975v1")'>Research and application of Transformer based anomaly detection model: A
  literature review</div>
<div id='2402.08975v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T06:39:54Z</div><div>Authors: Mingrui Ma, Lansheng Han, Chunjie Zhou</div><div style='padding-top: 10px; width: 80ex'>Transformer, as one of the most advanced neural network models in Natural
Language Processing (NLP), exhibits diverse applications in the field of
anomaly detection. To inspire research on Transformer-based anomaly detection,
this review offers a fresh perspective on the concept of anomaly detection. We
explore the current challenges of anomaly detection and provide detailed
insights into the operating principles of Transformer and its variants in
anomaly detection tasks. Additionally, we delineate various application
scenarios for Transformer-based anomaly detection models and discuss the
datasets and evaluation metrics employed. Furthermore, this review highlights
the key challenges in Transformer-based anomaly detection research and conducts
a comprehensive analysis of future research trends in this domain. The review
includes an extensive compilation of over 100 core references related to
Transformer-based anomaly detection. To the best of our knowledge, this is the
first comprehensive review that focuses on the research related to Transformer
in the context of anomaly detection. We hope that this paper can provide
detailed technical information to researchers interested in Transformer-based
anomaly detection tasks.</div><div><a href='http://arxiv.org/abs/2402.08975v1'>2402.08975v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07655v1")'>MLAD: A Unified Model for Multi-system Log Anomaly Detection</div>
<div id='2401.07655v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T12:51:13Z</div><div>Authors: Runqiang Zang, Hongcheng Guo, Jian Yang, Jiaheng Liu, Zhoujun Li, Tieqiao Zheng, Xu Shi, Liangfan Zheng, Bo Zhang</div><div style='padding-top: 10px; width: 80ex'>In spite of the rapid advancements in unsupervised log anomaly detection
techniques, the current mainstream models still necessitate specific training
for individual system datasets, resulting in costly procedures and limited
scalability due to dataset size, thereby leading to performance bottlenecks.
Furthermore, numerous models lack cognitive reasoning capabilities, posing
challenges in direct transferability to similar systems for effective anomaly
detection. Additionally, akin to reconstruction networks, these models often
encounter the "identical shortcut" predicament, wherein the majority of system
logs are classified as normal, erroneously predicting normal classes when
confronted with rare anomaly logs due to reconstruction errors.
  To address the aforementioned issues, we propose MLAD, a novel anomaly
detection model that incorporates semantic relational reasoning across multiple
systems. Specifically, we employ Sentence-bert to capture the similarities
between log sequences and convert them into highly-dimensional learnable
semantic vectors. Subsequently, we revamp the formulas of the Attention layer
to discern the significance of each keyword in the sequence and model the
overall distribution of the multi-system dataset through appropriate vector
space diffusion. Lastly, we employ a Gaussian mixture model to highlight the
uncertainty of rare words pertaining to the "identical shortcut" problem,
optimizing the vector space of the samples using the maximum expectation model.
Experiments on three real-world datasets demonstrate the superiority of MLAD.</div><div><a href='http://arxiv.org/abs/2401.07655v1'>2401.07655v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04749v1")'>LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection</div>
<div id='2401.04749v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T12:55:21Z</div><div>Authors: Hongcheng Guo, Jian Yang, Jiaheng Liu, Jiaqi Bai, Boyang Wang, Zhoujun Li, Tieqiao Zheng, Bo Zhang, Junran peng, Qi Tian</div><div style='padding-top: 10px; width: 80ex'>Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios. However, previous deep models merely focused on
extracting the semantics of log sequences in the same domain, leading to poor
generalization on multi-domain logs. To alleviate this issue, we propose a
unified Transformer-based framework for Log anomaly detection (LogFormer) to
improve the generalization ability across different domains, where we establish
a two-stage process including the pre-training and adapter-based tuning stage.
Specifically, our model is first pre-trained on the source domain to obtain
shared semantic knowledge of log data. Then, we transfer such knowledge to the
target domain via shared parameters. Besides, the Log-Attention module is
proposed to supplement the information ignored by the log-paring. The proposed
method is evaluated on three public and one real-world datasets. Experimental
results on multiple benchmarks demonstrate the effectiveness of our LogFormer
with fewer trainable parameters and lower training costs.</div><div><a href='http://arxiv.org/abs/2401.04749v1'>2401.04749v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03151v1")'>Semi-supervised learning via DQN for log anomaly detection</div>
<div id='2401.03151v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:04:13Z</div><div>Authors: Yingying He, Xiaobing Pei, Lihong Shen</div><div style='padding-top: 10px; width: 80ex'>Log anomaly detection plays a critical role in ensuring the security and
maintenance of modern software systems. At present, the primary approach for
detecting anomalies in log data is through supervised anomaly detection.
Nonetheless, existing supervised methods heavily rely on labeled data, which
can be frequently limited in real-world scenarios. In this paper, we propose a
semi-supervised log anomaly detection method that combines the DQN algorithm
from deep reinforcement learning, which is called DQNLog. DQNLog leverages a
small amount of labeled data and a large-scale unlabeled dataset, effectively
addressing the challenges of imbalanced data and limited labeling. This
approach not only learns known anomalies by interacting with an environment
biased towards anomalies but also discovers unknown anomalies by actively
exploring the unlabeled dataset. Additionally, DQNLog incorporates a
cross-entropy loss term to prevent model overestimation during Deep
Reinforcement Learning (DRL). Our evaluation on three widely-used datasets
demonstrates that DQNLog significantly improves recall rate and F1-score while
maintaining precision, validating its practicality.</div><div><a href='http://arxiv.org/abs/2401.03151v1'>2401.03151v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09209v2")'>LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection</div>
<div id='2403.09209v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T09:22:17Z</div><div>Authors: Xiangrui Cai, Yang Wang, Sihan Xu, Hao Li, Ying Zhang, Zheli Liu, Xiaojie Yuan</div><div style='padding-top: 10px; width: 80ex'>Enterprises and organizations are faced with potential threats from insider
employees that may lead to serious consequences. Previous studies on insider
threat detection (ITD) mainly focus on detecting abnormal users or abnormal
time periods (e.g., a week or a day). However, a user may have hundreds of
thousands of activities in the log, and even within a day there may exist
thousands of activities for a user, requiring a high investigation budget to
verify abnormal users or activities given the detection results. On the other
hand, existing works are mainly post-hoc methods rather than real-time
detection, which can not report insider threats in time before they cause loss.
In this paper, we conduct the first study towards real-time ITD at activity
level, and present a fine-grained and efficient framework LAN. Specifically,
LAN simultaneously learns the temporal dependencies within an activity sequence
and the relationships between activities across sequences with graph structure
learning. Moreover, to mitigate the data imbalance problem in ITD, we propose a
novel hybrid prediction loss, which integrates self-supervision signals from
normal activities and supervision signals from abnormal activities into a
unified loss for anomaly detection. We evaluate the performance of LAN on two
widely used datasets, i.e., CERT r4.2 and CERT r5.2. Extensive and comparative
experiments demonstrate the superiority of LAN, outperforming 9
state-of-the-art baselines by at least 9.92% and 6.35% in AUC for real-time ITD
on CERT r4.2 and r5.2, respectively. Moreover, LAN can be also applied to
post-hoc ITD, surpassing 8 competitive baselines by at least 7.70% and 4.03% in
AUC on two datasets. Finally, the ablation study, parameter analysis, and
compatibility analysis evaluate the impact of each module and hyper-parameter
in LAN. The source code can be obtained from https://github.com/Li1Neo/LAN.</div><div><a href='http://arxiv.org/abs/2403.09209v2'>2403.09209v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16412v1")'>TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis</div>
<div id='2402.16412v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T09:11:12Z</div><div>Authors: Sabera Talukder, Yisong Yue, Georgia Gkioxari</div><div style='padding-top: 10px; width: 80ex'>The field of general time series analysis has recently begun to explore
unified modeling, where a common architectural backbone can be retrained on a
specific task for a specific dataset. In this work, we approach unification
from a complementary vantage point: unification across tasks and domains. To
this end, we explore the impact of discrete, learnt, time series data
representations that enable generalist, cross-domain training. Our method,
TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer
architecture that embeds time series data from varying domains using a discrete
vectorized representation learned in a self-supervised manner. TOTEM works
across multiple tasks and domains with minimal to no tuning. We study the
efficacy of TOTEM with an extensive evaluation on 17 real world time series
datasets across 3 tasks. We evaluate both the specialist (i.e., training a
model on each domain) and generalist (i.e., training a single model on many
domains) settings, and show that TOTEM matches or outperforms previous best
methods on several popular benchmarks. The code can be found at:
https://github.com/SaberaTalukder/TOTEM.</div><div><a href='http://arxiv.org/abs/2402.16412v1'>2402.16412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07300v1")'>Taming Pre-trained LLMs for Generalised Time Series Forecasting via
  Cross-modal Knowledge Distillation</div>
<div id='2403.07300v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T04:04:38Z</div><div>Authors: Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia</div><div style='padding-top: 10px; width: 80ex'>Multivariate time series forecasting has recently gained great success with
the rapid growth of deep learning models. However, existing approaches usually
train models from scratch using limited temporal data, preventing their
generalization. Recently, with the surge of the Large Language Models (LLMs),
several works have attempted to introduce LLMs into time series forecasting.
Despite promising results, these methods directly take time series as the input
to LLMs, ignoring the inherent modality gap between temporal and text data. In
this work, we propose a novel Large Language Models and time series alignment
framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time
series forecasting challenge. Based on cross-modal knowledge distillation, the
proposed method exploits both input-agnostic static knowledge and
input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers
the forecasting model with favorable performance as well as strong
generalization abilities. Extensive experiments demonstrate the proposed method
establishes a new state of the art for both long- and short-term forecasting.
Code is available at \url{https://github.com/Hank0626/LLaTA}.</div><div><a href='http://arxiv.org/abs/2403.07300v1'>2403.07300v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16808v2")'>Encoding Temporal Statistical-space Priors via Augmented Representation</div>
<div id='2401.16808v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T08:11:36Z</div><div>Authors: Insu Choi, Woosung Koh, Gimin Kang, Yuntae Jang, Woo Chang Kim</div><div style='padding-top: 10px; width: 80ex'>Modeling time series data remains a pervasive issue as the temporal dimension
is inherent to numerous domains. Despite significant strides in time series
forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and
lack of data continue challenging practitioners. In response, we leverage a
simple representation augmentation technique to overcome these challenges. Our
augmented representation acts as a statistical-space prior encoded at each time
step. In response, we name our method Statistical-space Augmented
Representation (SSAR). The underlying high-dimensional data-generating process
inspires our representation augmentation. We rigorously examine the empirical
generalization performance on two data sets with two downstream temporal
learning algorithms. Our approach significantly beats all five up-to-date
baselines. Moreover, the highly modular nature of our approach can easily be
applied to various settings. Lastly, fully-fledged theoretical perspectives are
available throughout the writing for a clear and rigorous understanding.</div><div><a href='http://arxiv.org/abs/2401.16808v2'>2401.16808v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15773v1")'>Evaluation of k-means time series clustering based on z-normalization
  and NP-Free</div>
<div id='2401.15773v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T21:23:13Z</div><div>Authors: Ming-Chang Lee, Jia-Chun Lin, Volker Stolz</div><div style='padding-top: 10px; width: 80ex'>Despite the widespread use of k-means time series clustering in various
domains, there exists a gap in the literature regarding its comprehensive
evaluation with different time series normalization approaches. This paper
seeks to fill this gap by conducting a thorough performance evaluation of
k-means time series clustering on real-world open-source time series datasets.
The evaluation focuses on two distinct normalization techniques:
z-normalization and NP-Free. The former is one of the most commonly used
normalization approach for time series. The latter is a real-time time series
representation approach, which can serve as a time series normalization
approach. The primary objective of this paper is to assess the impact of these
two normalization techniques on k-means time series clustering in terms of its
clustering quality. The experiments employ the silhouette score, a
well-established metric for evaluating the quality of clusters in a dataset. By
systematically investigating the performance of k-means time series clustering
with these two normalization techniques, this paper addresses the current gap
in k-means time series clustering evaluation and contributes valuable insights
to the development of time series clustering.</div><div><a href='http://arxiv.org/abs/2401.15773v1'>2401.15773v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11044v1")'>Advancing multivariate time series similarity assessment: an integrated
  computational approach</div>
<div id='2403.11044v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T23:52:25Z</div><div>Authors: Franck Tonle, Henri Tonnang, Milliam Ndadji, Maurice Tchendji, Armand Nzeukou, Kennedy Senagi, Saliou Niassy</div><div style='padding-top: 10px; width: 80ex'>Data mining, particularly the analysis of multivariate time series data,
plays a crucial role in extracting insights from complex systems and supporting
informed decision-making across diverse domains. However, assessing the
similarity of multivariate time series data presents several challenges,
including dealing with large datasets, addressing temporal misalignments, and
the need for efficient and comprehensive analytical frameworks. To address all
these challenges, we propose a novel integrated computational approach known as
Multivariate Time series Alignment and Similarity Assessment (MTASA). MTASA is
built upon a hybrid methodology designed to optimize time series alignment,
complemented by a multiprocessing engine that enhances the utilization of
computational resources. This integrated approach comprises four key
components, each addressing essential aspects of time series similarity
assessment, thereby offering a comprehensive framework for analysis. MTASA is
implemented as an open-source Python library with a user-friendly interface,
making it accessible to researchers and practitioners. To evaluate the
effectiveness of MTASA, we conducted an empirical study focused on assessing
agroecosystem similarity using real-world environmental data. The results from
this study highlight MTASA's superiority, achieving approximately 1.5 times
greater accuracy and twice the speed compared to existing state-of-the-art
integrated frameworks for multivariate time series similarity assessment. It is
hoped that MTASA will significantly enhance the efficiency and accessibility of
multivariate time series analysis, benefitting researchers and practitioners
across various domains. Its capabilities in handling large datasets, addressing
temporal misalignments, and delivering accurate results make MTASA a valuable
tool for deriving insights and aiding decision-making processes in complex
systems.</div><div><a href='http://arxiv.org/abs/2403.11044v1'>2403.11044v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07646v1")'>Multifractal-spectral features enhance classification of anomalous
  diffusion</div>
<div id='2401.07646v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T12:42:15Z</div><div>Authors: Henrik Seckler, Ralf Metzler, Damian G. Kelty-Stephen, Madhur Mangalam</div><div style='padding-top: 10px; width: 80ex'>Anomalous diffusion processes pose a unique challenge in classification and
characterization. Previously (Mangalam et al., 2023, Physical Review Research
5, 023144), we established a framework for understanding anomalous diffusion
using multifractal formalism. The present study delves into the potential of
multifractal spectral features for effectively distinguishing anomalous
diffusion trajectories from five widely used models: fractional Brownian
motion, scaled Brownian motion, continuous time random walk, annealed transient
time motion, and L\'evy walk. To accomplish this, we generate extensive
datasets comprising $10^6$ trajectories from these five anomalous diffusion
models and extract multiple multifractal spectra from each trajectory. Our
investigation entails a thorough analysis of neural network performance,
encompassing features derived from varying numbers of spectra. Furthermore, we
explore the integration of multifractal spectra into traditional feature
datasets, enabling us to assess their impact comprehensively. To ensure a
statistically meaningful comparison, we categorize features into concept groups
and train neural networks using features from each designated group. Notably,
several feature groups demonstrate similar levels of accuracy, with the highest
performance observed in groups utilizing moving-window characteristics and
$p$-variation features. Multifractal spectral features, particularly those
derived from three spectra involving different timescales and cutoffs, closely
follow, highlighting their robust discriminatory potential. Remarkably, a
neural network exclusively trained on features from a single multifractal
spectrum exhibits commendable performance, surpassing other feature groups. Our
findings underscore the diverse and potent efficacy of multifractal spectral
features in enhancing classification of anomalous diffusion.</div><div><a href='http://arxiv.org/abs/2401.07646v1'>2401.07646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07066v2")'>Classification of Volatile Organic Compounds by Differential Mobility
  Spectrometry Based on Continuity of Alpha Curves</div>
<div id='2401.07066v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T12:56:04Z</div><div>Authors: Anton Rauhameri, Angelo Robiños, Osmo Anttalainen, Timo Salpavaara, Jussi Rantala, Veikko Surakka, Pasi Kallio, Antti Vehkaoja, Philipp Müller</div><div style='padding-top: 10px; width: 80ex'>Background: Classification of volatile organic compounds (VOCs) is of
interest in many fields. Examples include but are not limited to medicine,
detection of explosives, and food quality control. Measurements collected with
electronic noses can be used for classification and analysis of VOCs. One type
of electronic noses that has seen considerable development in recent years is
Differential Mobility Spectrometry (DMS). DMS yields measurements that are
visualized as dispersion plots that contain traces, also known as alpha curves.
Current methods used for analyzing DMS dispersion plots do not usually utilize
the information stored in the continuity of these traces, which suggests that
alternative approaches should be investigated.
  Results: In this work, for the first time, dispersion plots were interpreted
as a series of measurements evolving sequentially. Thus, it was hypothesized
that time-series classification algorithms can be effective for classification
and analysis of dispersion plots. An extensive dataset of 900 dispersion plots
for five chemicals measured at five flow rates and two concentrations was
collected. The data was used to analyze the classification performance of six
algorithms. According to our hypothesis, the highest classification accuracy of
88\% was achieved by a Long-Short Term Memory neural network, which supports
our hypothesis.
  Significance: A new concept for approaching classification tasks of
dispersion plots is presented and compared with other well-known classification
algorithms. This creates a new angle of view for analysis and classification of
the dispersion plots. In addition, a new dataset of dispersion plots is openly
shared to public.</div><div><a href='http://arxiv.org/abs/2401.07066v2'>2401.07066v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01902v1")'>EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting
  of Honeybee Time Series</div>
<div id='2402.01902v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T21:05:56Z</div><div>Authors: Mst. Shamima Hossain, Christos Faloutsos, Boris Baer, Hyoseung Kim, Vassilis J. Tsotras</div><div style='padding-top: 10px; width: 80ex'>Honeybees are vital for pollination and food production. Among many factors,
extreme temperature (e.g., due to climate change) is particularly dangerous for
bee health. Anticipating such extremities would allow beekeepers to take early
preventive action. Thus, given sensor (temperature) time series data from
beehives, how can we find patterns and do forecasting? Forecasting is crucial
as it helps spot unexpected behavior and thus issue warnings to the beekeepers.
In that case, what are the right models for forecasting? ARIMA, RNNs, or
something else?
  We propose the EBV (Electronic Bee-Veterinarian) method, which has the
following desirable properties: (i) principled: it is based on a) diffusion
equations from physics and b) control theory for feedback-loop controllers;
(ii) effective: it works well on multiple, real-world time sequences, (iii)
explainable: it needs only a handful of parameters (e.g., bee strength) that
beekeepers can easily understand and trust, and (iv) scalable: it performs
linearly in time. We applied our method to multiple real-world time sequences,
and found that it yields accurate forecasting (up to 49% improvement in RMSE
compared to baselines), and segmentation. Specifically, discontinuities
detected by EBV mostly coincide with domain expert's opinions, showcasing our
approach's potential and practical feasibility. Moreover, EBV is scalable and
fast, taking about 20 minutes on a stock laptop for reconstructing two months
of sensor data.</div><div><a href='http://arxiv.org/abs/2402.01902v1'>2402.01902v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08687v1")'>Fuzzy clustering of circular time series based on a new dependence
  measure with applications to wind data</div>
<div id='2402.08687v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T12:21:57Z</div><div>Authors: Ángel López-Oriona, Ying Sun, Rosa M. Crujeiras</div><div style='padding-top: 10px; width: 80ex'>Time series clustering is an essential machine learning task with
applications in many disciplines. While the majority of the methods focus on
time series taking values on the real line, very few works consider time series
defined on the unit circle, although the latter objects frequently arise in
many applications. In this paper, the problem of clustering circular time
series is addressed. To this aim, a distance between circular series is
introduced and used to construct a clustering procedure. The metric relies on a
new measure of serial dependence considering circular arcs, thus taking
advantage of the directional character inherent to the series range. Since the
dynamics of the series may vary over the time, we adopt a fuzzy approach, which
enables the procedure to locate each series into several clusters with
different membership degrees. The resulting clustering algorithm is able to
group series generated from similar stochastic processes, reaching accurate
results with series coming from a broad variety of models. An extensive
simulation study shows that the proposed method outperforms several alternative
techniques, besides being computationally efficient. Two interesting
applications involving time series of wind direction in Saudi Arabia highlight
the potential of the proposed approach.</div><div><a href='http://arxiv.org/abs/2402.08687v1'>2402.08687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08943v1")'>Evaluating DTW Measures via a Synthesis Framework for Time-Series Data</div>
<div id='2402.08943v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T05:08:47Z</div><div>Authors: Kishansingh Rajput, Duong Binh Nguyen, Guoning Chen</div><div style='padding-top: 10px; width: 80ex'>Time-series data originate from various applications that describe specific
observations or quantities of interest over time. Their analysis often involves
the comparison across different time-series data sequences, which in turn
requires the alignment of these sequences. Dynamic Time Warping (DTW) is the
standard approach to achieve an optimal alignment between two temporal signals.
Different variations of DTW have been proposed to address various needs for
signal alignment or classifications. However, a comprehensive evaluation of
their performance in these time-series data processing tasks is lacking. Most
DTW measures perform well on certain types of time-series data without a clear
explanation of the reason. To address that, we propose a synthesis framework to
model the variation between two time-series data sequences for comparison. Our
synthesis framework can produce a realistic initial signal and deform it with
controllable variations that mimic real-world scenarios. With this synthesis
framework, we produce a large number of time-series sequence pairs with
different but known variations, which are used to assess the performance of a
number of well-known DTW measures for the tasks of alignment and
classification. We report their performance on different variations and suggest
the proper DTW measure to use based on the type of variations between two
time-series sequences. This is the first time such a guideline is presented for
selecting a proper DTW measure. To validate our conclusion, we apply our
findings to real-world applications, i.e., the detection of the formation top
for the oil and gas industry and the pattern search in streamlines for flow
visualization.</div><div><a href='http://arxiv.org/abs/2402.08943v1'>2402.08943v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01124v1")'>Explainable Adaptive Tree-based Model Selection for Time Series
  Forecasting</div>
<div id='2401.01124v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T09:40:02Z</div><div>Authors: Matthias Jakobs, Amal Saadallah</div><div style='padding-top: 10px; width: 80ex'>Tree-based models have been successfully applied to a wide variety of tasks,
including time series forecasting. They are increasingly in demand and widely
accepted because of their comparatively high level of interpretability.
However, many of them suffer from the overfitting problem, which limits their
application in real-world decision-making. This problem becomes even more
severe in online-forecasting settings where time series observations are
incrementally acquired, and the distributions from which they are drawn may
keep changing over time. In this context, we propose a novel method for the
online selection of tree-based models using the TreeSHAP explainability method
in the task of time series forecasting. We start with an arbitrary set of
different tree-based models. Then, we outline a performance-based ranking with
a coherent design to make TreeSHAP able to specialize the tree-based
forecasters across different regions in the input time series. In this
framework, adequate model selection is performed online, adaptively following
drift detection in the time series. In addition, explainability is supported on
three levels, namely online input importance, model selection, and model output
explanation. An extensive empirical study on various real-world datasets
demonstrates that our method achieves excellent or on-par results in comparison
to the state-of-the-art approaches as well as several baselines.</div><div><a href='http://arxiv.org/abs/2401.01124v1'>2401.01124v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01343v1")'>Shapelet-based Model-agnostic Counterfactual Local Explanations for Time
  Series Classification</div>
<div id='2402.01343v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:57:53Z</div><div>Authors: Qi Huang, Wei Chen, Thomas Bäck, Niki van Stein</div><div style='padding-top: 10px; width: 80ex'>In this work, we propose a model-agnostic instance-based post-hoc
explainability method for time series classification. The proposed algorithm,
namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual
explanations for arbitrary time series classifiers. We validate the proposed
method on several real-world univariate time series classification tasks from
the UCR Time Series Archive. The results indicate that the counterfactual
instances generated by Time-CF when compared to state-of-the-art methods,
demonstrate better performance in terms of four explainability metrics:
closeness, sensibility, plausibility, and sparsity.</div><div><a href='http://arxiv.org/abs/2402.01343v1'>2402.01343v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09489v1")'>PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies</div>
<div id='2401.09489v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T20:13:46Z</div><div>Authors: Audrey Der, Chin-Chia Michael Yeh, Yan Zheng, Junpeng Wang, Zhongfang Zhuang, Liang Wang, Wei Zhang, Eamonn J. Keogh</div><div style='padding-top: 10px; width: 80ex'>In recent years there has been significant progress in time series anomaly
detection. However, after detecting an (perhaps tentative) anomaly, can we
explain it? Such explanations would be useful to triage anomalies. For example,
in an oil refinery, should we respond to an anomaly by dispatching a hydraulic
engineer, or an intern to replace the battery on a sensor? There have been some
parallel efforts to explain anomalies, however many proposed techniques produce
explanations that are indirect, and often seem more complex than the anomaly
they seek to explain. Our review of the literature/checklists/user-manuals used
by frontline practitioners in various domains reveals an interesting
near-universal commonality. Most practitioners discuss, explain and report
anomalies in the following format: The anomaly would be like normal data A, if
not for the corruption B. The reader will appreciate that is a type of
counterfactual explanation. In this work we introduce a domain agnostic
counterfactual explanation technique to produce explanations for time series
anomalies. As we will show, our method can produce both visual and text-based
explanations that are objectively correct, intuitive and in many circumstances,
directly actionable.</div><div><a href='http://arxiv.org/abs/2401.09489v1'>2401.09489v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01245v1")'>AcME-AD: Accelerated Model Explanations for Anomaly Detection</div>
<div id='2403.01245v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T16:11:58Z</div><div>Authors: Valentina Zaccaria, David Dandolo, Chiara Masiero, Gian Antonio Susto</div><div style='padding-top: 10px; width: 80ex'>Pursuing fast and robust interpretability in Anomaly Detection is crucial,
especially due to its significance in practical applications. Traditional
Anomaly Detection methods excel in outlier identification but are often
black-boxes, providing scant insights into their decision-making process. This
lack of transparency compromises their reliability and hampers their adoption
in scenarios where comprehending the reasons behind anomaly detection is vital.
At the same time, getting explanations quickly is paramount in practical
scenarios. To bridge this gap, we present AcME-AD, a novel approach rooted in
Explainable Artificial Intelligence principles, designed to clarify Anomaly
Detection models for tabular data. AcME-AD transcends the constraints of
model-specific or resource-heavy explainability techniques by delivering a
model-agnostic, efficient solution for interoperability. It offers local
feature importance scores and a what-if analysis tool, shedding light on the
factors contributing to each anomaly, thus aiding root cause analysis and
decision-making. This paper elucidates AcME-AD's foundation, its benefits over
existing methods, and validates its effectiveness with tests on both synthetic
and real datasets. AcME-AD's implementation and experiment replication code is
accessible in a public repository.</div><div><a href='http://arxiv.org/abs/2403.01245v1'>2403.01245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10903v2")'>DTOR: Decision Tree Outlier Regressor to explain anomalies</div>
<div id='2403.10903v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T11:38:31Z</div><div>Authors: Riccardo Crupi, Daniele Regoli, Alessandro Damiano Sabatino, Immacolata Marano, Massimiliano Brinis, Luca Albertazzi, Andrea Cirillo, Andrea Claudio Cosentini</div><div style='padding-top: 10px; width: 80ex'>Explaining outliers occurrence and mechanism of their occurrence can be
extremely important in a variety of domains. Malfunctions, frauds, threats, in
addition to being correctly identified, oftentimes need a valid explanation in
order to effectively perform actionable counteracts. The ever more widespread
use of sophisticated Machine Learning approach to identify anomalies make such
explanations more challenging. We present the Decision Tree Outlier Regressor
(DTOR), a technique for producing rule-based explanations for individual data
points by estimating anomaly scores generated by an anomaly detection model.
This is accomplished by first applying a Decision Tree Regressor, which
computes the estimation score, and then extracting the relative path associated
with the data point score. Our results demonstrate the robustness of DTOR even
in datasets with a large number of features. Additionally, in contrast to other
rule-based approaches, the generated rules are consistently satisfied by the
points to be explained. Furthermore, our evaluation metrics indicate comparable
performance to Anchors in outlier explanation tasks, with reduced execution
time.</div><div><a href='http://arxiv.org/abs/2403.10903v2'>2403.10903v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16388v2")'>Uncertainty Quantification in Anomaly Detection with Cross-Conformal
  $p$-Values</div>
<div id='2402.16388v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T08:22:40Z</div><div>Authors: Oliver Hennhöfer, Christine Preisach</div><div style='padding-top: 10px; width: 80ex'>Given the growing significance of reliable, trustworthy, and explainable
machine learning, the requirement of uncertainty quantification for anomaly
detection systems has become increasingly important. In this context,
effectively controlling Type I error rates ($\alpha$) without compromising the
statistical power ($1-\beta$) of these systems can build trust and reduce costs
related to false discoveries, particularly when follow-up procedures are
expensive. Leveraging the principles of conformal prediction emerges as a
promising approach for providing respective statistical guarantees by
calibrating a model's uncertainty. This work introduces a novel framework for
anomaly detection, termed cross-conformal anomaly detection, building upon
well-known cross-conformal methods designed for prediction tasks. With that, it
addresses a natural research gap by extending previous works in the context of
inductive conformal anomaly detection, relying on the split-conformal approach
for model calibration. Drawing on insights from conformal prediction, we
demonstrate that the derived methods for calculating cross-conformal $p$-values
strike a practical compromise between statistical efficiency (full-conformal)
and computational efficiency (split-conformal) for uncertainty-quantified
anomaly detection on benchmark datasets.</div><div><a href='http://arxiv.org/abs/2402.16388v2'>2402.16388v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11532v1")'>Out-of-Distribution Detection Should Use Conformal Prediction (and
  Vice-versa?)</div>
<div id='2403.11532v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T07:35:25Z</div><div>Authors: Paul Novello, Joseba Dalmau, Léo Andeol</div><div style='padding-top: 10px; width: 80ex'>Research on Out-Of-Distribution (OOD) detection focuses mainly on building
scores that efficiently distinguish OOD data from In Distribution (ID) data. On
the other hand, Conformal Prediction (CP) uses non-conformity scores to
construct prediction sets with probabilistic coverage guarantees. In this work,
we propose to use CP to better assess the efficiency of OOD scores.
Specifically, we emphasize that in standard OOD benchmark settings, evaluation
metrics can be overly optimistic due to the finite sample size of the test
dataset. Based on the work of (Bates et al., 2022), we define new conformal
AUROC and conformal FRP@TPR95 metrics, which are corrections that provide
probabilistic conservativeness guarantees on the variability of these metrics.
We show the effect of these corrections on two reference OOD and anomaly
detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al.,
2022). We also show that the benefits of using OOD together with CP apply the
other way around by using OOD scores as non-conformity scores, which results in
improving upon current CP methods. One of the key messages of these
contributions is that since OOD is concerned with designing scores and CP with
interpreting these scores, the two fields may be inherently intertwined.</div><div><a href='http://arxiv.org/abs/2403.11532v1'>2403.11532v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10368v1")'>Conformal Predictions for Probabilistically Robust Scalable Machine
  Learning Classification</div>
<div id='2403.10368v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T14:59:24Z</div><div>Authors: Alberto Carlevaro, Teodoro Alamo Cantarero, Fabrizio Dabbene, Maurizio Mongelli</div><div style='padding-top: 10px; width: 80ex'>Conformal predictions make it possible to define reliable and robust learning
algorithms. But they are essentially a method for evaluating whether an
algorithm is good enough to be used in practice. To define a reliable learning
framework for classification from the very beginning of its design, the concept
of scalable classifier was introduced to generalize the concept of classical
classifier by linking it to statistical order theory and probabilistic learning
theory. In this paper, we analyze the similarities between scalable classifiers
and conformal predictions by introducing a new definition of a score function
and defining a special set of input variables, the conformal safety set, which
can identify patterns in the input space that satisfy the error coverage
guarantee, i.e., that the probability of observing the wrong (possibly unsafe)
label for points belonging to this set is bounded by a predefined $\varepsilon$
error level. We demonstrate the practical implications of this framework
through an application in cybersecurity for identifying DNS tunneling attacks.
Our work contributes to the development of probabilistically robust and
reliable machine learning models.</div><div><a href='http://arxiv.org/abs/2403.10368v1'>2403.10368v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12672v1")'>Improving Interpretability of Scores in Anomaly Detection Based on
  Gaussian-Bernoulli Restricted Boltzmann Machine</div>
<div id='2403.12672v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T12:13:52Z</div><div>Authors: Kaiji Sekimoto, Muneki Yasuda</div><div style='padding-top: 10px; width: 80ex'>Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for
semi-supervised anomaly detection, where they are trained using only normal
data points. In GBRBM-based anomaly detection, normal and anomalous data are
classified based on a score that is identical to an energy function of the
marginal GBRBM. However, the classification threshold is difficult to set to an
appropriate value, as this score cannot be interpreted. In this study, we
propose a measure that improves score's interpretability based on its
cumulative distribution, and establish a guideline for setting the threshold
using the interpretable measure. The results of numerical experiments show that
the guideline is reasonable when setting the threshold solely using normal data
points. Moreover, because identifying the measure involves computationally
infeasible evaluation of the minimum score value, we also propose an evaluation
method for the minimum score based on simulated annealing, which is widely used
for optimization problems. The proposed evaluation method was also validated
using numerical experiments.</div><div><a href='http://arxiv.org/abs/2403.12672v1'>2403.12672v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13349v1")'>Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified
  Anomaly Detection</div>
<div id='2403.13349v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T07:21:37Z</div><div>Authors: Xincheng Yao, Ruoqi Li, Zefeng Qian, Lu Wang, Chongyang Zhang</div><div style='padding-top: 10px; width: 80ex'>Unified anomaly detection (AD) is one of the most challenges for anomaly
detection, where one unified model is trained with normal samples from multiple
classes with the objective to detect anomalies in these classes. For such a
challenging task, popular normalizing flow (NF) based AD methods may fall into
a "homogeneous mapping" issue,where the NF-based AD models are biased to
generate similar latent representations for both normal and abnormal features,
and thereby lead to a high missing rate of anomalies. In this paper, we propose
a novel Hierarchical Gaussian mixture normalizing flow modeling method for
accomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists
of two key components: inter-class Gaussian mixture modeling and intra-class
mixed class centers learning. Compared to the previous NF-based AD methods, the
hierarchical Gaussian mixture modeling approach can bring stronger
representation capability to the latent space of normalizing flows, so that
even complex multi-class distribution can be well represented and learned in
the latent space. In this way, we can avoid mapping different class
distributions into the same single Gaussian prior, thus effectively avoiding or
mitigating the "homogeneous mapping" issue. We further indicate that the more
distinguishable different class centers, the more conducive to avoiding the
bias issue. Thus, we further propose a mutual information maximization loss for
better structuring the latent feature space. We evaluate our method on four
real-world AD benchmarks, where we can significantly improve the previous
NF-based AD methods and also outperform the SOTA unified AD methods.</div><div><a href='http://arxiv.org/abs/2403.13349v1'>2403.13349v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07460v1")'>Experimental Comparison of Ensemble Methods and Time-to-Event Analysis
  Models Through Integrated Brier Score and Concordance Index</div>
<div id='2403.07460v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T09:57:45Z</div><div>Authors: Camila Fernandez, Chung Shue Chen, Chen Pierre Gaillard, Alonso Silva</div><div style='padding-top: 10px; width: 80ex'>Time-to-event analysis is a branch of statistics that has increased in
popularity during the last decades due to its many application fields, such as
predictive maintenance, customer churn prediction and population lifetime
estimation. In this paper, we review and compare the performance of several
prediction models for time-to-event analysis. These consist of semi-parametric
and parametric statistical models, in addition to machine learning approaches.
Our study is carried out on three datasets and evaluated in two different
scores (the integrated Brier score and concordance index). Moreover, we show
how ensemble methods, which surprisingly have not yet been much studied in
time-to-event analysis, can improve the prediction accuracy and enhance the
robustness of the prediction performance. We conclude the analysis with a
simulation experiment in which we evaluate the factors influencing the
performance ranking of the methods using both scores.</div><div><a href='http://arxiv.org/abs/2403.07460v1'>2403.07460v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08890v1")'>Predicting the Emergence of Solar Active Regions Using Machine Learning</div>
<div id='2402.08890v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T01:41:50Z</div><div>Authors: Spiridon Kasapis, Irina N. Kitiashvili, Alexander G. Kosovichev, John T. Stefan, Bhairavi Apte</div><div style='padding-top: 10px; width: 80ex'>To create early warning capabilities for upcoming Space Weather disturbances,
we have selected a dataset of 61 emerging active regions, which allows us to
identify characteristic features in the evolution of acoustic power density to
predict continuum intensity emergence. For our study, we have utilized Doppler
shift and continuum intensity observations from the Helioseismic and Magnetic
Imager (HMI) onboard the Solar Dynamics Observatory (SDO). The local tracking
of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to
trace the evolution of active regions starting from the pre-emergence state. We
have developed a machine learning model to capture the acoustic power flux
density variations associated with upcoming magnetic flux emergence. The
trained Long Short-Term Memory (LSTM) model is able to predict 5 hours ahead
whether, in a given area of the solar surface, continuum intensity values will
decrease. The performed study allows us to investigate the potential of the
machine learning approach to predict the emergence of active regions using
acoustic power maps as input.</div><div><a href='http://arxiv.org/abs/2402.08890v1'>2402.08890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11185v1")'>Minimally Supervised Topological Projections of Self-Organizing Maps for
  Phase of Flight Identification</div>
<div id='2402.11185v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T04:05:01Z</div><div>Authors: Zimeng Lyu, Pujan Thapa, Travis Desell</div><div style='padding-top: 10px; width: 80ex'>Identifying phases of flight is important in the field of general aviation,
as knowing which phase of flight data is collected from aircraft flight data
recorders can aid in the more effective detection of safety or hazardous
events. General aviation flight data for phase of flight identification is
usually per-second data, comes on a large scale, and is class imbalanced. It is
expensive to manually label the data and training classification models usually
faces class imbalance problems. This work investigates the use of a novel
method for minimally supervised self-organizing maps (MS-SOMs) which utilize
nearest neighbor majority votes in the SOM U-matrix for class estimation.
Results show that the proposed method can reach or exceed a naive SOM approach
which utilized a full data file of labeled data, with only 30 labeled
datapoints per class. Additionally, the minimally supervised SOM is
significantly more robust to the class imbalance of the phase of flight data.
These results highlight how little data is required for effective phase of
flight identification.</div><div><a href='http://arxiv.org/abs/2402.11185v1'>2402.11185v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16285v1")'>A Comparison of Deep Learning Models for Proton Background Rejection
  with the AMS Electromagnetic Calorimeter</div>
<div id='2402.16285v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T04:06:05Z</div><div>Authors: Raheem Karim Hashmani, Emre Akbaş, Melahat Bilge Demirköz</div><div style='padding-top: 10px; width: 80ex'>The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector
onboard the International Space Station containing six different subdetectors.
The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are
used to separate electrons/positrons from the abundant cosmic-ray proton
background.
  The positron flux measured in space by AMS falls with a power law which
unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several
theoretical models try to explain these phenomena, and a purer measurement of
positrons at higher energies is needed to help test them. The currently used
methods to reject the proton background at high energies involve extrapolating
shower features from the ECAL to use as inputs for boosted decision tree and
likelihood classifiers. We present a new approach for particle identification
with the AMS ECAL using deep learning (DL). By taking the energy deposition
within all the ECAL cells as an input and treating them as pixels in an
image-like format, we train an MLP, a CNN, and multiple ResNets and
Convolutional vision Transformers (CvTs) as shower classifiers.
  Proton rejection performance is evaluated using Monte Carlo (MC) events and
ISS data separately. For MC, using events with a reconstructed energy between
0.2 - 2 TeV, at 90% electron accuracy, the proton rejection power of our CvT
model is more than 5 times that of the other DL models. Similarly, for ISS data
with a reconstructed energy between 50 - 70 GeV, the proton rejection power of
our CvT model is more than 2.5 times that of the other DL models.</div><div><a href='http://arxiv.org/abs/2402.16285v1'>2402.16285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16981v1")'>Selection of gamma events from IACT images with deep learning methods</div>
<div id='2401.16981v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T13:07:24Z</div><div>Authors: E. O. Gres, A. P. Kryukov, A. P. Demichev, J. J. Dubenskaya, S. P. Polyakov, A. A. Vlaskina, D. P. Zhurov</div><div style='padding-top: 10px; width: 80ex'>Imaging Atmospheric Cherenkov Telescopes (IACTs) of gamma ray observatory
TAIGA detect the Extesnive Air Showers (EASs) originating from the cosmic or
gamma rays interactions with the atmosphere. Thereby, telescopes obtain images
of the EASs. The ability to segregate gamma rays images from the hadronic
cosmic ray background is one of the main features of this type of detectors.
However, in actual IACT observations simultaneous observation of the background
and the source of gamma ray is needed. This observation mode (called wobbling)
modifies images of events, which affects the quality of selection by neural
networks.
  Thus, in this work, the results of the application of neural networks (NN)
for image classification task on Monte Carlo (MC) images of TAIGA-IACTs are
presented. The wobbling mode is considered together with the image adaptation
for adequate analysis by NNs. Simultaneously, we explore several neural network
structures that classify events both directly from images or through Hillas
parameters extracted from images. In addition, by employing NNs, MC simulation
data are used to evaluate the quality of the segregation of rare gamma events
with the account of all necessary image modifications.</div><div><a href='http://arxiv.org/abs/2401.16981v1'>2401.16981v1</a></div>
</div></div>
    <div><a href="arxiv_26.html">Prev (26)</a></div>
    <div><a href="arxiv_28.html">Next (28)</a></div>
    