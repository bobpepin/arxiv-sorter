
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14551v1")'>Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling</div>
<div id='2403.14551v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T16:52:01Z</div><div>Authors: Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas</div><div style='padding-top: 10px; width: 80ex'>Today's most accurate language models are trained on orders of magnitude more
language data than human language learners receive - but with no supervision
from other sensory modalities that play a crucial role in human learning. Can
we make LMs' representations and predictions more accurate (and more
human-like) with more ecologically plausible supervision? This paper describes
LexiContrastive Grounding (LCG), a grounded language learning procedure that
leverages visual supervision to improve textual representations.
LexiContrastive Grounding combines a next token prediction strategy with a
contrastive visual grounding objective, focusing on early-layer representations
that encode lexical information. Across multiple word-learning and
sentence-understanding benchmarks, LexiContrastive Grounding not only
outperforms standard language-only models in learning efficiency, but also
improves upon vision-and-language learning procedures including CLIP, GIT,
Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves
perplexity by around 5% on multiple language modeling tasks. This work
underscores the potential of incorporating visual grounding into language
models, aligning more closely with the multimodal nature of human language
acquisition.</div><div><a href='http://arxiv.org/abs/2403.14551v1'>2403.14551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17811v1")'>TruthX: Alleviating Hallucinations by Editing Large Language Models in
  Truthful Space</div>
<div id='2402.17811v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T14:45:04Z</div><div>Authors: Shaolei Zhang, Tian Yu, Yang Feng</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks. However, they sometimes suffer from producing hallucinations,
particularly in cases where they may generate untruthful responses despite
possessing the correct knowledge. In this paper, we propose TruthX, an
inference-time method to elicit the truthfulness of LLMs by editing their
internal representations in truthful space. TruthX employs an auto-encoder to
map LLM's representations into semantic and truthful latent spaces
respectively, and applies contrastive learning to identify a truthful editing
direction within the truthful space. During inference, by editing LLM's
internal representations in truthful space, TruthX effectively enhances the
truthfulness of LLMs. Experiments show that TruthX effectively improves the
truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark.
Further analyses suggest that the truthful space acquired by TruthX plays a
pivotal role in controlling LLM to produce truthful or hallucinatory responses.</div><div><a href='http://arxiv.org/abs/2402.17811v1'>2402.17811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13542v1")'>ARL2: Aligning Retrievers for Black-box Large Language Models via
  Self-guided Adaptive Relevance Labeling</div>
<div id='2402.13542v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T05:41:34Z</div><div>Authors: Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang</div><div style='padding-top: 10px; width: 80ex'>Retrieval-augmented generation enhances large language models (LLMs) by
incorporating relevant information from external knowledge sources. This
enables LLMs to adapt to specific domains and mitigate hallucinations in
knowledge-intensive tasks. However, existing retrievers are often misaligned
with LLMs due to their separate training processes and the black-box nature of
LLMs. To address this challenge, we propose ARL2, a retriever learning
technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and
score relevant evidence, enabling learning the retriever from robust LLM
supervision. Furthermore, ARL2 uses an adaptive self-training strategy for
curating high-quality and diverse relevance data, which can effectively reduce
the annotation cost. Extensive experiments demonstrate the effectiveness of
ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared
to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer
learning capabilities and strong zero-shot generalization abilities. Our code
will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.</div><div><a href='http://arxiv.org/abs/2402.13542v1'>2402.13542v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01858v1")'>Explaining latent representations of generative models with large
  multimodal models</div>
<div id='2402.01858v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:28:33Z</div><div>Authors: Mengdan Zhu, Zhenke Liu, Bo Pan, Abhinav Angirekula, Liang Zhao</div><div style='padding-top: 10px; width: 80ex'>Learning interpretable representations of data generative latent factors is
an important topic for the development of artificial intelligence. With the
rise of the large multimodal model, it can align images with text to generate
answers. In this work, we propose a framework to comprehensively explain each
latent factor in the generative models using a large multimodal model. We
further measure the uncertainty of our generated explanations, quantitatively
evaluate the performance of explanation generation among multiple large
multimodal models, and qualitatively visualize the variations of each latent
factor to learn the disentanglement effects of different generative models on
explanations. Finally, we discuss the explanatory capabilities and limitations
of state-of-the-art large multimodal models.</div><div><a href='http://arxiv.org/abs/2402.01858v1'>2402.01858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12070v1")'>Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated
  Text</div>
<div id='2401.12070v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:09:47Z</div><div>Authors: Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein</div><div style='padding-top: 10px; width: 80ex'>Detecting text generated by modern large language models is thought to be
hard, as both LLMs and humans can exhibit a wide range of complex behaviors.
However, we find that a score based on contrasting two closely related language
models is highly accurate at separating human-generated and machine-generated
text. Based on this mechanism, we propose a novel LLM detector that only
requires simple calculations using a pair of pre-trained LLMs. The method,
called Binoculars, achieves state-of-the-art accuracy without any training
data. It is capable of spotting machine text from a range of modern LLMs
without any model-specific modifications. We comprehensively evaluate
Binoculars on a number of text sources and in varied situations. Over a wide
range of document types, Binoculars detects over 90% of generated samples from
ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being
trained on any ChatGPT data.</div><div><a href='http://arxiv.org/abs/2401.12070v1'>2401.12070v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01355v1")'>a-DCF: an architecture agnostic metric with application to
  spoofing-robust speaker verification</div>
<div id='2403.01355v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T00:58:27Z</div><div>Authors: Hye-jin Shim, Jee-weon Jung, Tomi Kinnunen, Nicholas Evans, Jean-Francois Bonastre, Itshak Lapidot</div><div style='padding-top: 10px; width: 80ex'>Spoofing detection is today a mainstream research topic. Standard metrics can
be applied to evaluate the performance of isolated spoofing detection solutions
and others have been proposed to support their evaluation when they are
combined with speaker detection. These either have well-known deficiencies or
restrict the architectural approach to combine speaker and spoof detectors. In
this paper, we propose an architecture-agnostic detection cost function
(a-DCF). A generalisation of the original DCF used widely for the assessment of
automatic speaker verification (ASV), the a-DCF is designed for the evaluation
of spoofing-robust ASV. Like the DCF, the a-DCF reflects the cost of decisions
in a Bayes risk sense, with explicitly defined class priors and detection cost
model. We demonstrate the merit of the a-DCF through the benchmarking
evaluation of architecturally-heterogeneous spoofing-robust ASV solutions.</div><div><a href='http://arxiv.org/abs/2403.01355v1'>2403.01355v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07384v1")'>Exploring Perceptual Limitation of Multimodal Large Language Models</div>
<div id='2402.07384v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T03:04:42Z</div><div>Authors: Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, Maosong Sun</div><div style='padding-top: 10px; width: 80ex'>Multimodal Large Language Models (MLLMs) have recently shown remarkable
perceptual capability in answering visual questions, however, little is known
about the limits of their perception. In particular, while prior works have
provided anecdotal evidence of MLLMs' sensitivity to object size, this
phenomenon and its underlying causes have not been explored comprehensively. In
this work, we quantitatively study the perception of small visual objects in
several state-of-the-art MLLMs and reveal a pervasive limitation in answering
questions about small objects in images. Next, we identify four independent
factors that can contribute to this limitation -- object quality, size,
distractors, and location -- and conduct controlled intervention studies to
measure the effect of each factor on MLLMs' perception. In particular, we find
that lower object quality and smaller object size can both independently reduce
MLLMs' ability to answer visual questions. More surprisingly, we find that the
location of the object in the image and the presence of visual distractors can
also significantly reduce MLLMs' question answering accuracy. Our study
provides a better understanding of the perceptual limitation of MLLMs and
contributes new evaluation protocols for analyzing the perception of future
MLLMs. To facilitate further investigations, we release our code and data.</div><div><a href='http://arxiv.org/abs/2402.07384v1'>2402.07384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02469v1")'>Vision-Language Models for Medical Report Generation and Visual Question
  Answering: A Review</div>
<div id='2403.02469v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T20:29:51Z</div><div>Authors: Iryna Hartsock, Ghulam Rasool</div><div style='padding-top: 10px; width: 80ex'>Medical vision-language models (VLMs) combine computer vision and natural
language processing to analyze visual and textual medical data. Our paper
reviews recent advancements in developing VLMs specialized for healthcare,
focusing on models designed for medical report generation and visual question
answering. We provide background on natural language processing and computer
vision, explaining how techniques from both fields are integrated into VLMs to
enable learning from multimodal data. Key areas we address include the
exploration of medical vision-language datasets, in-depth analyses of
architectures and pre-training strategies employed in recent noteworthy medical
VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs'
performance in medical report generation and visual question answering. We also
highlight current challenges and propose future directions, including enhancing
clinical validity and addressing patient privacy concerns. Overall, our review
summarizes recent progress in developing VLMs to harness multimodal medical
data for improved healthcare applications.</div><div><a href='http://arxiv.org/abs/2403.02469v1'>2403.02469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10153v1")'>Improving Medical Multi-modal Contrastive Learning with Expert
  Annotations</div>
<div id='2403.10153v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T09:54:04Z</div><div>Authors: Yogesh Kumar, Pekka Marttinen</div><div style='padding-top: 10px; width: 80ex'>We introduce eCLIP, an enhanced version of the CLIP model that integrates
expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key
challenges in contrastive multi-modal medical imaging analysis, notably data
scarcity and the "modality gap" -- a significant disparity between image and
text embeddings that diminishes the quality of representations and hampers
cross-modal interoperability. eCLIP integrates a heatmap processor and
leverages mixup augmentation to efficiently utilize the scarce expert
annotations, thus boosting the model's learning effectiveness. eCLIP is
designed to be generally applicable to any variant of CLIP without requiring
any modifications of the core architecture. Through detailed evaluations across
several tasks, including zero-shot inference, linear probing, cross-modal
retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using
a frozen Large Language Model, eCLIP showcases consistent improvements in
embedding quality. The outcomes reveal enhanced alignment and uniformity,
affirming eCLIP's capability to harness high-quality annotations for enriched
multi-modal analysis in the medical imaging domain.</div><div><a href='http://arxiv.org/abs/2403.10153v1'>2403.10153v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13311v1")'>ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in
  Large Multimodal Models</div>
<div id='2401.13311v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T09:07:11Z</div><div>Authors: Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in AI have led to the development of large multimodal
models (LMMs) capable of processing complex tasks involving joint reasoning
over text and visual content in the image (e.g., navigating maps in public
places). This paper introduces ConTextual, a novel benchmark comprising
instructions designed explicitly to evaluate LMMs' ability to perform
context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse
real-world scenarios (e.g., time-reading, navigation, shopping and more)
demanding a deeper understanding of the interactions between textual and visual
elements. Our findings reveal a significant performance gap of 30.8% between
the best-performing LMM, GPT-4V(ision), and human capabilities using human
evaluation indicating substantial room for improvement in context-sensitive
text-rich visual reasoning. Notably, while GPT-4V excelled in abstract
categories like meme and quote interpretation, its overall performance still
lagged behind humans. In addition to human evaluations, we also employed
automatic evaluation metrics using GPT-4, uncovering similar trends in
performance disparities. We also perform a fine-grained evaluation across
diverse visual contexts and provide qualitative analysis which provides a
robust framework for future advancements in the LMM design.
https://con-textual.github.io/</div><div><a href='http://arxiv.org/abs/2401.13311v1'>2401.13311v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13649v1")'>VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web
  Tasks</div>
<div id='2401.13649v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T18:35:21Z</div><div>Authors: Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried</div><div style='padding-top: 10px; width: 80ex'>Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.</div><div><a href='http://arxiv.org/abs/2401.13649v1'>2401.13649v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03610v1")'>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal
  LLM Agents</div>
<div id='2402.03610v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T00:53:27Z</div><div>Authors: Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, Yang You</div><div style='padding-top: 10px; width: 80ex'>Owing to recent advancements, Large Language Models (LLMs) can now be
deployed as agents for increasingly complex decision-making applications in
areas including robotics, gaming, and API integration. However, reflecting past
experiences in current decision-making processes, an innate human behavior,
continues to pose significant challenges. Addressing this, we propose
Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage
past experiences corresponding to the current situation and context, thereby
enhancing agents' planning capabilities. RAP distinguishes itself by being
versatile: it excels in both text-only and multimodal environments, making it
suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's
effectiveness, where it achieves SOTA performance in textual scenarios and
notably enhances multimodal LLM agents' performance for embodied tasks. These
results highlight RAP's potential in advancing the functionality and
applicability of LLM agents in complex, real-world applications.</div><div><a href='http://arxiv.org/abs/2402.03610v1'>2402.03610v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08025v2")'>Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using
  Self-Imagination</div>
<div id='2401.08025v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T00:46:29Z</div><div>Authors: Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, Eric Nyberg</div><div style='padding-top: 10px; width: 80ex'>The potential of Vision-Language Models (VLMs) often remains underutilized in
handling complex text-based problems, particularly when these problems could
benefit from visual representation. Resonating with humans' ability to solve
complex text-based problems by (1) creating a visual diagram from the problem
and (2) deducing what steps they need to take to solve it, we propose
Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a
structured representation of the question using HTML, then render the HTML as
an image, and finally use the same VLM to answer the question using both the
question and the image. Our approach does not require any additional training
data or training. We evaluate our approach on three mathematics tasks and nine
general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI
PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on
all math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the
majority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.</div><div><a href='http://arxiv.org/abs/2401.08025v2'>2401.08025v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08743v1")'>MMToM-QA: Multimodal Theory of Mind Question Answering</div>
<div id='2401.08743v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:59:24Z</div><div>Authors: Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum, Tianmin Shu</div><div style='padding-top: 10px; width: 80ex'>Theory of Mind (ToM), the ability to understand people's minds, is an
essential ingredient for developing machines with human-level social
intelligence. Recent machine learning models, particularly large language
models, seem to show some aspects of ToM understanding. However, existing ToM
benchmarks use unimodal datasets - either video or text. Human ToM, on the
other hand, is more than video or text understanding. People can flexibly
reason about another person's mind based on conceptual representations (e.g.,
goals, beliefs, plans) extracted from any available data, which can include
visual cues, linguistic narratives, or both. To address this, we introduce a
multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA
comprehensively evaluates machine ToM both on multimodal data and on different
kinds of unimodal data about a person's activity in a household environment. To
engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian
Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified
representations from multimodal data and utilizes language models for scalable
Bayesian inverse planning. We conducted a systematic comparison of human
performance, BIP-ALM, and state-of-the-art models, including GPT-4. The
experiments demonstrate that large language models and large multimodal models
still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising
results, by leveraging the power of both model-based mental inference and
language models.</div><div><a href='http://arxiv.org/abs/2401.08743v1'>2401.08743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12424v3")'>Tables as Images? Exploring the Strengths and Limitations of LLMs on
  Multimodal Representations of Tabular Data</div>
<div id='2402.12424v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T16:34:50Z</div><div>Authors: Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, Rada Mihalcea</div><div style='padding-top: 10px; width: 80ex'>In this paper, we investigate the effectiveness of various LLMs in
interpreting tabular data through different prompting strategies and data
formats. Our analysis extends across six benchmarks for table-related tasks
such as question-answering and fact-checking. We introduce for the first time
the assessment of LLMs' performance on image-based table representations.
Specifically, we compare five text-based and three image-based table
representations, demonstrating the influence of representation and prompting on
LLM performance. Our study provides insights into the effective use of LLMs on
table-related tasks.</div><div><a href='http://arxiv.org/abs/2402.12424v3'>2402.12424v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14003v1")'>Multi-Modal Hallucination Control by Visual Information Grounding</div>
<div id='2403.14003v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T22:05:18Z</div><div>Authors: Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto</div><div style='padding-top: 10px; width: 80ex'>Generative Vision-Language Models (VLMs) are prone to generate
plausible-sounding textual answers that, however, are not always grounded in
the input image. We investigate this phenomenon, usually referred to as
"hallucination" and show that it stems from an excessive reliance on the
language prior. In particular, we show that as more tokens are generated, the
reliance on the visual prompt decreases, and this behavior strongly correlates
with the emergence of hallucinations. To reduce hallucinations, we introduce
Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for
prompt amplification. M3ID amplifies the influence of the reference image over
the language prior, hence favoring the generation of tokens with higher mutual
information with the visual prompt. M3ID can be applied to any pre-trained
autoregressive VLM at inference time without necessitating further training and
with minimal computational overhead. If training is an option, we show that
M3ID can be paired with Direct Preference Optimization (DPO) to improve the
model's reliance on the prompt image without requiring any labels. Our
empirical findings show that our algorithms maintain the fluency and linguistic
capabilities of pre-trained VLMs while reducing hallucinations by mitigating
visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and
M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by
25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as
POPE by 21% and 24%.</div><div><a href='http://arxiv.org/abs/2403.14003v1'>2403.14003v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06890v1")'>An Axiomatic Approach to Model-Agnostic Concept Explanations</div>
<div id='2401.06890v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T20:53:35Z</div><div>Authors: Zhili Feng, Michal Moshkovitz, Dotan Di Castro, J. Zico Kolter</div><div style='padding-top: 10px; width: 80ex'>Concept explanation is a popular approach for examining how
human-interpretable concepts impact the predictions of a model. However, most
existing methods for concept explanations are tailored to specific models. To
address this issue, this paper focuses on model-agnostic measures.
Specifically, we propose an approach to concept explanations that satisfy three
natural axioms: linearity, recursivity, and similarity. We then establish
connections with previous concept explanation methods, offering insight into
their varying semantic meanings. Experimentally, we demonstrate the utility of
the new method by applying it in different scenarios: for model selection,
optimizer selection, and model improvement using a kind of prompt editing for
zero-shot vision language models.</div><div><a href='http://arxiv.org/abs/2401.06890v1'>2401.06890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17221v1")'>MouSi: Poly-Visual-Expert Vision-Language Models</div>
<div id='2401.17221v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T18:09:11Z</div><div>Authors: Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, Ming Zhang, Caishuang Huang, Rui Zheng, Zhiheng Xi, Yuhao Zhou, Shihan Dou, Junjie Ye, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang</div><div style='padding-top: 10px; width: 80ex'>Current large vision-language models (VLMs) often encounter challenges such
as insufficient capabilities of a single visual component and excessively long
visual tokens. These issues can limit the model's effectiveness in accurately
interpreting complex visual information and over-lengthy contextual
information. Addressing these challenges is crucial for enhancing the
performance and applicability of VLMs. This paper proposes the use of ensemble
experts technique to synergizes the capabilities of individual visual encoders,
including those skilled in image-text matching, OCR, image segmentation, etc.
This technique introduces a fusion network to unify the processing of outputs
from different visual experts, while bridging the gap between image encoders
and pre-trained LLMs. In addition, we explore different positional encoding
schemes to alleviate the waste of positional encoding caused by lengthy image
feature sequences, effectively addressing the issue of position overflow and
length limitations. For instance, in our implementation, this technique
significantly reduces the positional occupancy in models like SAM, from a
substantial 4096 to a more efficient and manageable 64 or even down to 1.
Experimental results demonstrate that VLMs with multiple experts exhibit
consistently superior performance over isolated visual encoders and mark a
significant performance boost as more experts are integrated. We have
open-sourced the training code used in this report. All of these resources can
be found on our project website.</div><div><a href='http://arxiv.org/abs/2401.17221v1'>2401.17221v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07417v1")'>An Empirical Study Into What Matters for Calibrating Vision-Language
  Models</div>
<div id='2402.07417v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T05:44:10Z</div><div>Authors: Weijie Tu, Weijian Deng, Dylan Campbell, Stephen Gould, Tom Gedeon</div><div style='padding-top: 10px; width: 80ex'>Vision--Language Models (VLMs) have emerged as the dominant approach for
zero-shot recognition, adept at handling diverse scenarios and significant
distribution changes. However, their deployment in risk-sensitive areas
requires a deeper understanding of their uncertainty estimation capabilities, a
relatively uncharted area. In this study, we explore the calibration properties
of VLMs across different architectures, datasets, and training strategies. In
particular, we analyze the uncertainty estimation performance of VLMs when
calibrated in one domain, label set or hierarchy level, and tested in a
different one. Our findings reveal that while VLMs are not inherently
calibrated for uncertainty, temperature scaling significantly and consistently
improves calibration, even across shifts in distribution and changes in label
set. Moreover, VLMs can be calibrated with a very small set of examples.
Through detailed experimentation, we highlight the potential applications and
importance of our insights, aiming for more reliable and effective use of VLMs
in critical, real-world scenarios.</div><div><a href='http://arxiv.org/abs/2402.07417v1'>2402.07417v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13771v1")'>Describe-and-Dissect: Interpreting Neurons in Vision Networks with
  Language Models</div>
<div id='2403.13771v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:33:02Z</div><div>Authors: Nicholas Bai, Rahul A. Iyer, Tuomas Oikarinen, Tsui-Wei Weng</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose Describe-and-Dissect (DnD), a novel method to
describe the roles of hidden neurons in vision networks. DnD utilizes recent
advancements in multimodal deep learning to produce complex natural language
descriptions, without the need for labeled training data or a predefined set of
concepts to choose from. Additionally, DnD is training-free, meaning we don't
train any new models and can easily leverage more capable general purpose
models in the future. We have conducted extensive qualitative and quantitative
analysis to show that DnD outperforms prior work by providing higher quality
neuron descriptions. Specifically, our method on average provides the highest
quality labels and is more than 2 times as likely to be selected as the best
explanation for a neuron than the best baseline.</div><div><a href='http://arxiv.org/abs/2403.13771v1'>2403.13771v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17793v1")'>A Surprising Failure? Multimodal LLMs and the NLVR Challenge</div>
<div id='2402.17793v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:37:18Z</div><div>Authors: Anne Wu, Kianté Brantley, Yoav Artzi</div><div style='padding-top: 10px; width: 80ex'>This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and
the open-source model IDEFICS -- on the compositional natural language vision
reasoning task NLVR. Given a human-written sentence paired with a synthetic
image, this task requires the model to determine the truth value of the
sentence with respect to the image. Despite the strong performance demonstrated
by these models, we observe they perform poorly on NLVR, which was constructed
to require compositional and spatial reasoning, and to be robust for semantic
and systematic biases.</div><div><a href='http://arxiv.org/abs/2402.17793v1'>2402.17793v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01862v1")'>A Vision Check-up for Language Models</div>
<div id='2401.01862v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T18:09:33Z</div><div>Authors: Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba</div><div style='padding-top: 10px; width: 80ex'>What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.</div><div><a href='http://arxiv.org/abs/2401.01862v1'>2401.01862v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01399v1")'>A Probabilistic Model to explain Self-Supervised Representation Learning</div>
<div id='2402.01399v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:31:17Z</div><div>Authors: Alice Bizeul, Bernhard Schölkopf, Carl Allen</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) learns representations by leveraging an
auxiliary unsupervised task, such as classifying semantically related samples,
e.g. different data augmentations or modalities. Of the many approaches to SSL,
contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for
learning representations that achieve downstream performance close to that of
supervised learning. However, a theoretical understanding of the mechanism
behind these methods eludes. We propose a generative latent variable model for
the data and show that several families of discriminative self-supervised
algorithms, including contrastive methods, approximately induce its latent
structure over representations, providing a unifying theoretical framework. We
also justify links to mutual information and the use of a projection head.
Fitting our model generatively, as SimVE, improves performance over previous
VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows
the gap to discriminative methods on _content_ classification and, as our
analysis predicts, outperforms them where _style_ information is required,
taking a step toward task-agnostic representations.</div><div><a href='http://arxiv.org/abs/2402.01399v1'>2402.01399v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16803v2")'>PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset</div>
<div id='2401.16803v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T07:50:32Z</div><div>Authors: Arhan Jain, Alec Bunn, Austin Pham, TJ Tsai</div><div style='padding-top: 10px; width: 80ex'>This article motivates, describes, and presents the PBSCSR dataset for
studying composer style recognition of piano sheet music. Our overarching goal
was to create a dataset for studying composer style recognition that is "as
accessible as MNIST and as challenging as ImageNet". To achieve this goal, we
use a previously proposed feature representation of sheet music called a
bootleg score, which encodes the position of noteheads relative to the staff
lines. Using this representation, we sample fixed-length bootleg score
fragments from piano sheet music images on IMSLP. The dataset itself contains
40,000 62x64 bootleg score images for a 9-way classification task, 100,000
62x64 bootleg score images for a 100-way classification task, and 29,310
unlabeled variable-length bootleg score images for pretraining. The labeled
data is presented in a form that mirrors MNIST images, in order to make it
extremely easy to visualize, manipulate, and train models in an efficient
manner. Additionally, we include relevant metadata to allow access to the
underlying raw sheet music images and other related data on IMSLP. We describe
several research tasks that could be studied with the dataset, including
variations of composer style recognition in a few-shot or zero-shot setting.
For tasks that have previously proposed models, we release code and baseline
results for future works to compare against. We also discuss open research
questions that the PBSCSR data is especially well suited to facilitate research
on and areas of fruitful exploration in future work.</div><div><a href='http://arxiv.org/abs/2401.16803v2'>2401.16803v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13725v1")'>Sparse and Structured Hopfield Networks</div>
<div id='2402.13725v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T11:35:45Z</div><div>Authors: Saul Santos, Vlad Niculae, Daniel McNamee, Andre F. T. Martins</div><div style='padding-top: 10px; width: 80ex'>Modern Hopfield networks have enjoyed recent interest due to their connection
to attention in transformers. Our paper provides a unified framework for sparse
Hopfield networks by establishing a link with Fenchel-Young losses. The result
is a new family of Hopfield-Fenchel-Young energies whose update rules are
end-to-end differentiable sparse transformations. We reveal a connection
between loss margins, sparsity, and exact memory retrieval. We further extend
this framework to structured Hopfield networks via the SparseMAP
transformation, which can retrieve pattern associations instead of a single
pattern. Experiments on multiple instance learning and text rationalization
demonstrate the usefulness of our approach.</div><div><a href='http://arxiv.org/abs/2402.13725v1'>2402.13725v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02456v1")'>Discovering More Effective Tensor Network Structure Search Algorithms
  via Large Language Models (LLMs)</div>
<div id='2402.02456v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T12:06:13Z</div><div>Authors: Junhua Zeng, Guoxu Zhou, Chao Li, Zhun Sun, Qibin Zhao</div><div style='padding-top: 10px; width: 80ex'>Tensor network structure search (TN-SS), aiming at searching for suitable
tensor network (TN) structures in representing high-dimensional problems,
largely promotes the efficacy of TN in various machine learning applications.
Nonetheless, finding a satisfactory TN structure using existing algorithms
remains challenging. To develop more effective algorithms and avoid the human
labor-intensive development process, we explore the knowledge embedded in large
language models (LLMs) for the automatic design of TN-SS algorithms. Our
approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting
system that operates in an evolutionary-like manner. The experimental results,
derived from real-world data, demonstrate that GPTN-SS can effectively leverage
the insights gained from existing methods to develop novel TN-SS algorithms
that achieve a better balance between exploration and exploitation. These
algorithms exhibit superior performance in searching the high-quality TN
structures for natural image compression and model parameters compression while
also demonstrating generalizability in their performance.</div><div><a href='http://arxiv.org/abs/2402.02456v1'>2402.02456v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19411v1")'>PaECTER: Patent-level Representation Learning using Citation-informed
  Transformers</div>
<div id='2402.19411v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:09:03Z</div><div>Authors: Mainak Ghosh, Sebastian Erhardt, Michael E. Rose, Erik Buunk, Dietmar Harhoff</div><div style='padding-top: 10px; width: 80ex'>PaECTER is a publicly available, open-source document-level encoder specific
for patents. We fine-tune BERT for Patents with examiner-added citation
information to generate numerical representations for patent documents. PaECTER
performs better in similarity tasks than current state-of-the-art models used
in the patent domain. More specifically, our model outperforms the next-best
patent specific pre-trained language model (BERT for Patents) on our patent
citation prediction test dataset on two different rank evaluation metrics.
PaECTER predicts at least one most similar patent at a rank of 1.32 on average
when compared against 25 irrelevant patents. Numerical representations
generated by PaECTER from patent text can be used for downstream tasks such as
classification, tracing knowledge flows, or semantic similarity search.
Semantic similarity search is especially relevant in the context of prior art
search for both inventors and patent examiners. PaECTER is available on Hugging
Face.</div><div><a href='http://arxiv.org/abs/2402.19411v1'>2402.19411v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13797v1")'>Bridge the Modality and Capacity Gaps in Vision-Language Model Selection</div>
<div id='2403.13797v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:54:58Z</div><div>Authors: Chao Yi, De-Chuan Zhan, Han-Jia Ye</div><div style='padding-top: 10px; width: 80ex'>Vision Language Models (VLMs) excel in zero-shot image classification by
pairing images with textual category names. The expanding variety of
Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for
specific tasks. Thus, a promising zero-shot image classification strategy is
selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely
on the text data of the target dataset without access to the dataset's images.
In this paper, we analyze two inherent challenges in assessing the ability of a
VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in
VLM's embeddings across two different modalities, making text a less reliable
substitute for images; and the "Capability Gap" -- the discrepancy between the
VLM's overall ranking and its ranking for target dataset, hindering direct
prediction of a model's dataset-specific performance from its general
performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the
negative impact of these two gaps. SWAB first adopts optimal transport to
capture the relevance between open-source datasets and target dataset with a
transportation matrix. It then uses this matrix to transfer useful statistics
of VLMs from open-source datasets to the target dataset for bridging those two
gaps and enhancing the VLM's capacity estimation for VLM selection. Experiments
across various VLMs and image classification datasets validate SWAB's
effectiveness.</div><div><a href='http://arxiv.org/abs/2403.13797v1'>2403.13797v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12151v1")'>Fusing Domain-Specific Content from Large Language Models into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification</div>
<div id='2403.12151v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T18:08:44Z</div><div>Authors: Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis Theodore Patkos, Antonis Argyros, Dimitris Plexousakis</div><div style='padding-top: 10px; width: 80ex'>Domain-specific knowledge can significantly contribute to addressing a wide
variety of vision tasks. However, the generation of such knowledge entails
considerable human labor and time costs. This study investigates the potential
of Large Language Models (LLMs) in generating and providing domain-specific
information through semantic embeddings. To achieve this, an LLM is integrated
into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors
in the context of the Vision-based Zero-shot Object State Classification task.
We thoroughly examine the behavior of the LLM through an extensive ablation
study. Our findings reveal that the integration of LLM-based embeddings, in
combination with general-purpose pre-trained embeddings, leads to substantial
performance improvements. Drawing insights from this ablation study, we conduct
a comparative analysis against competing models, thereby highlighting the
state-of-the-art performance achieved by the proposed approach.</div><div><a href='http://arxiv.org/abs/2403.12151v1'>2403.12151v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01482v1")'>Incorporating Geo-Diverse Knowledge into Prompting for Increased
  Geographical Robustness in Object Recognition</div>
<div id='2401.01482v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T01:11:16Z</div><div>Authors: Kyle Buettner, Sina Malakouti, Xiang Lorraine Li, Adriana Kovashka</div><div style='padding-top: 10px; width: 80ex'>Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.</div><div><a href='http://arxiv.org/abs/2401.01482v1'>2401.01482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05224v1")'>Do Vision and Language Encoders Represent the World Similarly?</div>
<div id='2401.05224v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T15:51:39Z</div><div>Authors: Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Mohamed El Amine Seddik, Karttikeya Mangalam, Noel E. O'Connor</div><div style='padding-top: 10px; width: 80ex'>Aligned text-image encoders such as CLIP have become the de facto model for
vision-language tasks. Furthermore, modality-specific encoders achieve
impressive performances in their respective domains. This raises a central
question: does an alignment exist between uni-modal vision and language
encoders since they fundamentally represent the same physical world? Analyzing
the latent spaces structure of vision and language models on image-caption
benchmarks using the Centered Kernel Alignment (CKA), we find that the
representation spaces of unaligned and aligned encoders are semantically
similar. In the absence of statistical similarity in aligned encoders like
CLIP, we show that a possible matching of unaligned encoders exists without any
training. We frame this as a seeded graph-matching problem exploiting the
semantic similarity between graphs and propose two methods - a Fast Quadratic
Assignment Problem optimization, and a novel localized CKA metric-based
matching/retrieval. We demonstrate the effectiveness of this on several
downstream tasks including cross-lingual, cross-domain caption matching and
image classification.</div><div><a href='http://arxiv.org/abs/2401.05224v1'>2401.05224v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16347v1")'>Cross-Modal Coordination Across a Diverse Set of Input Modalities</div>
<div id='2401.16347v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:53:25Z</div><div>Authors: Jorge Sánchez, Rodrigo Laguna</div><div style='padding-top: 10px; width: 80ex'>Cross-modal retrieval is the task of retrieving samples of a given modality
by using queries of a different one. Due to the wide range of practical
applications, the problem has been mainly focused on the vision and language
case, e.g. text to image retrieval, where models like CLIP have proven
effective in solving such tasks. The dominant approach to learning such
coordinated representations consists of projecting them onto a common space
where matching views stay close and those from non-matching pairs are pushed
away from each other. Although this cross-modal coordination has been applied
also to other pairwise combinations, extending it to an arbitrary number of
diverse modalities is a problem that has not been fully explored in the
literature. In this paper, we propose two different approaches to the problem.
The first is based on an extension of the CLIP contrastive objective to an
arbitrary number of input modalities, while the second departs from the
contrastive formulation and tackles the coordination problem by regressing the
cross-modal similarities towards a target that reflects two simple and
intuitive constraints of the cross-modal retrieval task. We run experiments on
two different datasets, over different combinations of input modalities and
show that the approach is not only simple and effective but also allows for
tackling the retrieval problem in novel ways. Besides capturing a more diverse
set of pair-wise interactions, we show that we can use the learned
representations to improve retrieval performance by combining the embeddings
from two or more such modalities.</div><div><a href='http://arxiv.org/abs/2401.16347v1'>2401.16347v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04650v2")'>Context-Based Multimodal Fusion</div>
<div id='2403.04650v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T16:50:25Z</div><div>Authors: Bilal Faye, Hanane Azzag, Mustapha Lebbah, Djamel Bouchaffra</div><div style='padding-top: 10px; width: 80ex'>The fusion models, which effectively combine information from different
sources, are widely used in solving multimodal tasks. However, they have
significant limitations related to aligning data distributions across different
modalities. This challenge can lead to inconsistencies and difficulties in
learning robust representations. Alignment models, while specifically
addressing this issue, often require training "from scratch" with large
datasets to achieve optimal results, which can be costly in terms of resources
and time. To overcome these limitations, we propose an innovative model called
Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and
data distribution alignment. In CBMF, each modality is represented by a
specific context vector, fused with the embedding of each modality. This
enables the use of large pre-trained models that can be frozen, reducing the
computational and training data requirements. Additionally, the network learns
to differentiate embeddings of different modalities through fusion with context
and aligns data distributions using a contrastive approach for self-supervised
learning. Thus, CBMF offers an effective and economical solution for solving
complex multimodal tasks.</div><div><a href='http://arxiv.org/abs/2403.04650v2'>2403.04650v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13537v1")'>What explains the success of cross-modal fine-tuning with ORCA?</div>
<div id='2403.13537v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T12:14:54Z</div><div>Authors: Paloma García-de-Herreros, Vagrant Gautam, Philipp Slusallek, Dietrich Klakow, Marius Mosbach</div><div style='padding-top: 10px; width: 80ex'>ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,
i.e., applying pre-trained transformer models to modalities beyond their
training data. The technique consists primarily of training an embedder and
fine-tuning the embedder and model. Despite its high performance on a variety
of downstream tasks, we do not understand precisely how each of these
components contribute to ORCA's success. Therefore, we run a series of
ablations and find that embedder training does not help 2D tasks at all,
contrary to what the original paper posits. In 1D tasks, some amount of
embedder training is necessary but more is not better. In 4 out of 6 datasets
we experiment with, it is model fine-tuning that makes the biggest difference.
Through our ablations and baselines, we contribute a better understanding of
the individual components of ORCA.</div><div><a href='http://arxiv.org/abs/2403.13537v1'>2403.13537v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14289v1")'>TinyLLaVA: A Framework of Small-scale Large Multimodal Models</div>
<div id='2402.14289v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T05:05:30Z</div><div>Authors: Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, Lei Huang</div><div style='padding-top: 10px; width: 80ex'>We present the TinyLLaVA framework that provides a unified perspective in
designing and analyzing the small-scale Large Multimodal Models (LMMs). We
empirically study the effects of different vision encoders, connection modules,
language models, training data and training recipes. Our extensive experiments
showed that better quality of data combined with better training recipes,
smaller LMMs can consistently achieve on-par performances compared to bigger
LMMs. Under our framework, we train a family of small-scale LMMs. Our best
model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B
models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as
baselines for future research in terms of data scaling, training setups and
model selections. Our model weights and codes will be made public.</div><div><a href='http://arxiv.org/abs/2402.14289v1'>2402.14289v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15422v2")'>A Survey on Data Augmentation in Large Model Era</div>
<div id='2401.15422v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T14:19:33Z</div><div>Authors: Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, Yuan Wu</div><div style='padding-top: 10px; width: 80ex'>Large models, encompassing large language and diffusion models, have shown
exceptional promise in approximating human-level intelligence, garnering
significant interest from both academic and industrial spheres. However, the
training of these large models necessitates vast quantities of high-quality
data, and with continuous updates to these models, the existing reservoir of
high-quality data may soon be depleted. This challenge has catalyzed a surge in
research focused on data augmentation methods. Leveraging large models, these
data augmentation techniques have outperformed traditional approaches. This
paper offers an exhaustive review of large model-driven data augmentation
methods, adopting a comprehensive perspective. We begin by establishing a
classification of relevant studies into three main categories: image
augmentation, text augmentation, and paired data augmentation. Following this,
we delve into various data post-processing techniques pertinent to large
model-based data augmentation. Our discussion then expands to encompass the
array of applications for these data augmentation methods within natural
language processing, computer vision, and audio signal processing. We proceed
to evaluate the successes and limitations of large model-based data
augmentation across different scenarios. Concluding our review, we highlight
prospective challenges and avenues for future exploration in the field of data
augmentation. Our objective is to furnish researchers with critical insights,
ultimately contributing to the advancement of more sophisticated large models.
We consistently maintain the related open-source materials at:
https://github.com/MLGroup-JLU/LLM-data-aug-survey.</div><div><a href='http://arxiv.org/abs/2401.15422v2'>2401.15422v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14895v1")'>Data Augmentation is Dead, Long Live Data Augmentation</div>
<div id='2402.14895v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:42:37Z</div><div>Authors: Frédéric Piedboeuf, Philippe Langlais</div><div style='padding-top: 10px; width: 80ex'>Textual data augmentation (DA) is a prolific field of study where novel
techniques to create artificial data are regularly proposed, and that has
demonstrated great efficiency on small data settings, at least for text
classification tasks. In this paper, we challenge those results, showing that
classical data augmentation is simply a way of performing better fine-tuning,
and that spending more time fine-tuning before applying data augmentation
negates its effect. This is a significant contribution as it answers several
questions that were left open in recent years, namely~: which DA technique
performs best (all of them as long as they generate data close enough to the
training set as to not impair training) and why did DA show positive results
(facilitates training of network). We furthermore show that zero and few-shot
data generation via conversational agents such as ChatGPT or LLama2 can
increase performances, concluding that this form of data augmentation does
still work, even if classical methods do not.</div><div><a href='http://arxiv.org/abs/2402.14895v1'>2402.14895v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02302v2")'>Beyond Specialization: Assessing the Capabilities of MLLMs in Age and
  Gender Estimation</div>
<div id='2403.02302v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T18:32:12Z</div><div>Authors: Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh</div><div style='padding-top: 10px; width: 80ex'>Multimodal Large Language Models (MLLMs) have recently gained immense
popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as
open-source ones such as LLaVA, are essentially general-purpose models and are
applied to solve a wide variety of tasks, including those in computer vision.
These neural networks possess such strong general knowledge and reasoning
abilities that they have proven capable of working even on tasks for which they
were not specifically trained. We compared the capabilities of the most
powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task
of age and gender estimation with our state-of-the-art specialized model,
MiVOLO. We also updated MiVOLO and provide details and new metrics in this
article. This comparison has yielded some interesting results and insights
about the strengths and weaknesses of the participating models. Furthermore, we
attempted various ways to fine-tune the ShareGPT4V model for this specific
task, aiming to achieve state-of-the-art results in this particular challenge.
Although such a model would not be practical in production, as it is incredibly
expensive compared to a specialized model like MiVOLO, it could be very useful
in some tasks, like data annotation.</div><div><a href='http://arxiv.org/abs/2403.02302v2'>2403.02302v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15096v1")'>Multimodal Transformer With a Low-Computational-Cost Guarantee</div>
<div id='2402.15096v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T05:09:35Z</div><div>Authors: Sungjin Park, Edward Choi</div><div style='padding-top: 10px; width: 80ex'>Transformer-based models have significantly improved performance across a
range of multimodal understanding tasks, such as visual question answering and
action recognition. However, multimodal Transformers significantly suffer from
a quadratic complexity of the multi-head attention with the input sequence
length, especially as the number of modalities increases. To address this, we
introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal
attention mechanism that aims to reduce computational cost during training and
inference with minimal performance loss. Specifically, by assigning different
multimodal attention patterns to each attention head, LoCoMT can flexibly
control multimodal signals and theoretically ensures a reduced computational
cost compared to existing multimodal Transformer variants. Experimental results
on two multimodal datasets, namely Audioset and MedVidCL demonstrate that
LoCoMT not only reduces GFLOPs but also matches or even outperforms established
models.</div><div><a href='http://arxiv.org/abs/2402.15096v1'>2402.15096v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18603v4")'>MMSR: Symbolic Regression is a Multimodal Task</div>
<div id='2402.18603v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T08:29:42Z</div><div>Authors: Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan Hao, Su Wei, Yusong Deng</div><div style='padding-top: 10px; width: 80ex'>Mathematical formulas are the crystallization of human wisdom in exploring
the laws of nature for thousands of years. Describing the complex laws of
nature with a concise mathematical formula is a constant pursuit of scientists
and a great challenge for artificial intelligence. This field is called
symbolic regression. Symbolic regression was originally formulated as a
combinatorial optimization problem, and GP and reinforcement learning
algorithms were used to solve it. However, GP is sensitive to hyperparameters,
and these two types of algorithms are inefficient. To solve this problem,
researchers treat the mapping from data to expressions as a translation
problem. And the corresponding large-scale pre-trained model is introduced.
However, the data and expression skeletons do not have very clear word
correspondences as the two languages do. Instead, they are more like two
modalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.
The SR problem is solved as a pure multimodal problem, and contrastive learning
is also introduced in the training process for modal alignment to facilitate
later modal feature fusion. It is worth noting that in order to better promote
the modal feature fusion, we adopt the strategy of training contrastive
learning loss and other losses at the same time, which only needs one-step
training, instead of training contrastive learning loss first and then training
other losses. Because our experiments prove training together can make the
feature extraction module and feature fusion module running-in better.
Experimental results show that compared with multiple large-scale pre-training
baselines, MMSR achieves the most advanced results on multiple mainstream
datasets including SRBench.</div><div><a href='http://arxiv.org/abs/2402.18603v4'>2402.18603v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08017v1")'>Lumos : Empowering Multimodal LLMs with Scene Text Recognition</div>
<div id='2402.08017v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:27:26Z</div><div>Authors: Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar</div><div style='padding-top: 10px; width: 80ex'>We introduce Lumos, the first end-to-end multimodal question-answering system
with text understanding capabilities. At the core of Lumos is a Scene Text
Recognition (STR) component that extracts text from first person point-of-view
images, the output of which is used to augment input to a Multimodal Large
Language Model (MM-LLM). While building Lumos, we encountered numerous
challenges related to STR quality, overall latency, and model inference. In
this paper, we delve into those challenges, and discuss the system
architecture, design choices, and modeling techniques employed to overcome
these obstacles. We also provide a comprehensive evaluation for each component,
showcasing high quality and efficiency.</div><div><a href='http://arxiv.org/abs/2402.08017v1'>2402.08017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12267v2")'>Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data
  Quality over Quantity</div>
<div id='2403.12267v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T21:32:58Z</div><div>Authors: Siddharth Joshi, Arnav Jain, Ali Payani, Baharan Mirzasoleiman</div><div style='padding-top: 10px; width: 80ex'>Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption
datasets learns representations that can achieve remarkable zero-shot
generalization. However, such models require a massive amount of pre-training
data. Improving the quality of the pre-training data has been shown to be much
more effective in improving CLIP's performance than increasing its volume.
Nevertheless, finding small subsets of training data that provably generalize
the best has remained an open question. In this work, we propose the first
theoretically rigorous data selection method for CLIP. We show that subsets
that closely preserve the cross-covariance of the images and captions of the
full data provably achieve a superior generalization performance. Our extensive
experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that
subsets found by \method\ achieve over 2.7x and 1.4x the accuracy of the next
best baseline on ImageNet and its shifted versions. Moreover, we show that our
subsets obtain 1.5x the average accuracy across 11 downstream datasets, of the
next best baseline. The code is available at:
https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip.</div><div><a href='http://arxiv.org/abs/2403.12267v2'>2403.12267v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08567v1")'>Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal
  Data</div>
<div id='2401.08567v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:52:27Z</div><div>Authors: Yuhui Zhang, Elaine Sui, Serena Yeung-Levy</div><div style='padding-top: 10px; width: 80ex'>Building cross-modal applications is challenging due to limited paired
multi-modal data. Recent works have shown that leveraging a pre-trained
multi-modal contrastive representation space enables cross-modal tasks to be
learned from uni-modal data. This is based on the assumption that contrastive
optimization makes embeddings from different modalities interchangeable.
However, this assumption is under-explored due to the poorly understood
geometry of the multi-modal contrastive space, where a modality gap exists. In
our study, we provide a theoretical explanation of this space's geometry and
introduce a three-step method, $C^3$ (Connect, Collapse, Corrupt), to bridge
the modality gap, enhancing the interchangeability of embeddings. Our $C^3$
method significantly improves cross-modal learning from uni-modal data,
achieving state-of-the-art results on zero-shot image / audio / video
captioning and text-to-image generation.</div><div><a href='http://arxiv.org/abs/2401.08567v1'>2401.08567v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14405v2")'>Multimodal Pathway: Improve Transformers with Irrelevant Data from Other
  Modalities</div>
<div id='2401.14405v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:59:58Z</div><div>Authors: Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue</div><div style='padding-top: 10px; width: 80ex'>We propose to improve transformers of a specific modality with irrelevant
data from other modalities, e.g., improve an ImageNet model with audio or point
cloud datasets. We would like to highlight that the data samples of the target
modality are irrelevant to the other modalities, which distinguishes our method
from other works utilizing paired (e.g., CLIP) or interleaved data of different
modalities. We propose a methodology named Multimodal Pathway - given a target
modality and a transformer designed for it, we use an auxiliary transformer
trained with data of another modality and construct pathways to connect
components of the two models so that data of the target modality can be
processed by both models. In this way, we utilize the universal
sequence-to-sequence modeling abilities of transformers obtained from two
modalities. As a concrete implementation, we use a modality-specific tokenizer
and task-specific head as usual but utilize the transformer blocks of the
auxiliary model via a proposed method named Cross-Modal Re-parameterization,
which exploits the auxiliary weights without any inference costs. On the image,
point cloud, video, and audio recognition tasks, we observe significant and
consistent performance improvements with irrelevant data from other modalities.
The code and models are available at https://github.com/AILab-CVC/M2PT.</div><div><a href='http://arxiv.org/abs/2401.14405v2'>2401.14405v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07187v1")'>UPS: Towards Foundation Models for PDE Solving via Cross-Modal
  Adaptation</div>
<div id='2403.07187v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T22:00:39Z</div><div>Authors: Junhong Shen, Tanya Marwah, Ameet Talwalkar</div><div style='padding-top: 10px; width: 80ex'>We introduce UPS (Unified PDE Solver), an effective and data-efficient
approach to solve diverse spatiotemporal PDEs defined over various domains,
dimensions, and resolutions. UPS unifies different PDEs into a consistent
representation space and processes diverse collections of PDE data using a
unified network architecture that combines LLMs with domain-specific neural
operators. We train the network via a two-stage cross-modal adaptation process,
leveraging ideas of modality alignment and multi-task learning. By adapting
from pretrained LLMs and exploiting text-form meta information, we are able to
use considerably fewer training samples than previous methods while obtaining
strong empirical results. UPS outperforms existing baselines, often by a large
margin, on a wide range of 1D and 2D datasets in PDEBench, achieving
state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable
of few-shot transfer to different PDE families, coefficients, and resolutions.</div><div><a href='http://arxiv.org/abs/2403.07187v1'>2403.07187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09502v1")'>EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning</div>
<div id='2403.09502v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:44:19Z</div><div>Authors: Jongsuk Kim, Hyeongkeun Lee, Kyeongha Rho, Junmo Kim, Joon Son Chung</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in self-supervised audio-visual representation learning
have demonstrated its potential to capture rich and comprehensive
representations. However, despite the advantages of data augmentation verified
in many learning methods, audio-visual learning has struggled to fully harness
these benefits, as augmentations can easily disrupt the correspondence between
input pairs. To address this limitation, we introduce EquiAV, a novel framework
that leverages equivariance for audio-visual contrastive learning. Our approach
begins with extending equivariance to audio-visual learning, facilitated by a
shared attention-based transformation predictor. It enables the aggregation of
features from diverse augmentations into a representative embedding, providing
robust supervision. Notably, this is achieved with minimal computational
overhead. Extensive ablation studies and qualitative results verify the
effectiveness of our method. EquiAV outperforms previous works across various
audio-visual benchmarks.</div><div><a href='http://arxiv.org/abs/2403.09502v1'>2403.09502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12613v1")'>Analysis of Using Sigmoid Loss for Contrastive Learning</div>
<div id='2402.12613v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T00:34:58Z</div><div>Authors: Chungpa Lee, Joonhwan Chang, Jy-yong Sohn</div><div style='padding-top: 10px; width: 80ex'>Contrastive learning has emerged as a prominent branch of self-supervised
learning for several years. Especially, CLIP, which applies contrastive
learning to large sets of captioned images, has garnered significant attention.
Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid
loss instead of the standard InfoNCE loss. SigLIP achieves the performance
comparable to CLIP in a more efficient manner by eliminating the need for a
global view. However, theoretical understanding of using the sigmoid loss in
contrastive learning is underexplored. In this paper, we provide a theoretical
analysis of using the sigmoid loss in contrastive learning, in the perspective
of the geometric structure of learned embeddings. First, we propose the
double-Constant Embedding Model (CCEM), a framework for parameterizing various
well-known embedding structures by a single variable. Interestingly, the
proposed CCEM is proven to contain the optimal embedding with respect to the
sigmoid loss. Second, we mathematically analyze the optimal embedding
minimizing the sigmoid loss for contrastive learning. The optimal embedding
ranges from simplex equiangular-tight-frame to antipodal structure, depending
on the temperature parameter used in the sigmoid loss. Third, our experimental
results on synthetic datasets coincide with the theoretical results on the
optimal embedding structures.</div><div><a href='http://arxiv.org/abs/2402.12613v1'>2402.12613v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01306v1")'>ICC: Quantifying Image Caption Concreteness for Multimodal Dataset
  Curation</div>
<div id='2403.01306v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T20:36:10Z</div><div>Authors: Moran Yanuka, Morris Alper, Hadar Averbuch-Elor, Raja Giryes</div><div style='padding-top: 10px; width: 80ex'>Web-scale training on paired text-image data is becoming increasingly central
to multimodal learning, but is challenged by the highly noisy nature of
datasets in the wild. Standard data filtering approaches succeed in removing
mismatched text-image pairs, but permit semantically related but highly
abstract or subjective text. These approaches lack the fine-grained ability to
isolate the most concrete samples that provide the strongest signal for
learning in a noisy dataset. In this work, we propose a new metric, image
caption concreteness, that evaluates caption text without an image reference to
measure its concreteness and relevancy for use in multimodal learning. Our
approach leverages strong foundation models for measuring visual-semantic
information loss in multimodal representations. We demonstrate that this
strongly correlates with human evaluation of concreteness in both single-word
and sentence-level texts. Moreover, we show that curation using ICC complements
existing approaches: It succeeds in selecting the highest quality samples from
multimodal web-scale datasets to allow for efficient training in
resource-constrained settings.</div><div><a href='http://arxiv.org/abs/2403.01306v1'>2403.01306v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02506v1")'>Differentially Private Representation Learning via Image Captioning</div>
<div id='2403.02506v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T21:52:25Z</div><div>Authors: Tom Sander, Yaodong Yu, Maziar Sanjabi, Alain Durmus, Yi Ma, Kamalika Chaudhuri, Chuan Guo</div><div style='padding-top: 10px; width: 80ex'>Differentially private (DP) machine learning is considered the gold-standard
solution for training a model from sensitive data while still preserving
privacy. However, a major barrier to achieving this ideal is its sub-optimal
privacy-accuracy trade-off, which is particularly visible in DP representation
learning. Specifically, it has been shown that under modest privacy budgets,
most models learn representations that are not significantly better than
hand-crafted features. In this work, we show that effective DP representation
learning can be done via image captioning and scaling up to internet-scale
multimodal datasets. Through a series of engineering tricks, we successfully
train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch
using a reasonable amount of computation, and obtaining unprecedented
high-quality image features that can be used in a variety of downstream vision
and vision-language tasks. For example, under a privacy budget of
$\varepsilon=8$, a linear classifier trained on top of learned DP-Cap features
attains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTA
of 56.5%. Our work challenges the prevailing sentiment that high-utility DP
representation learning cannot be achieved by training from scratch.</div><div><a href='http://arxiv.org/abs/2403.02506v1'>2403.02506v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10090v1")'>PICS: Pipeline for Image Captioning and Search</div>
<div id='2402.10090v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T03:08:21Z</div><div>Authors: Grant Rosario, David Noever</div><div style='padding-top: 10px; width: 80ex'>The growing volume of digital images necessitates advanced systems for
efficient categorization and retrieval, presenting a significant challenge in
database management and information retrieval. This paper introduces PICS
(Pipeline for Image Captioning and Search), a novel approach designed to
address the complexities inherent in organizing large-scale image repositories.
PICS leverages the advancements in Large Language Models (LLMs) to automate the
process of image captioning, offering a solution that transcends traditional
manual annotation methods. The approach is rooted in the understanding that
meaningful, AI-generated captions can significantly enhance the searchability
and accessibility of images in large databases. By integrating sentiment
analysis into the pipeline, PICS further enriches the metadata, enabling
nuanced searches that extend beyond basic descriptors. This methodology not
only simplifies the task of managing vast image collections but also sets a new
precedent for accuracy and efficiency in image retrieval. The significance of
PICS lies in its potential to transform image database systems, harnessing the
power of machine learning and natural language processing to meet the demands
of modern digital asset management.</div><div><a href='http://arxiv.org/abs/2402.10090v1'>2402.10090v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14973v1")'>GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data</div>
<div id='2402.14973v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T21:22:04Z</div><div>Authors: Lele Cao, Valentin Buchner, Zineb Senane, Fangkai Yang</div><div style='padding-top: 10px; width: 80ex'>Multimodal Large Language Models (MLLMs) are commonly evaluated using costly
annotated multimodal benchmarks. However, these benchmarks often struggle to
keep pace with the rapidly advancing requirements of MLLM evaluation. We
propose GenCeption, a novel and annotation-free MLLM evaluation framework that
merely requires unimodal data to assess inter-modality semantic coherence and
inversely reflects the models' inclination to hallucinate. Analogous to the
popular DrawCeption game, GenCeption initiates with a non-textual sample and
undergoes a series of iterative description and generation steps. Semantic
drift across iterations is quantified using the GC@T metric. Our empirical
findings validate GenCeption's efficacy, showing strong correlations with
popular MLLM benchmarking results. GenCeption may be extended to mitigate
training data contamination by utilizing ubiquitous, previously unseen unimodal
data.</div><div><a href='http://arxiv.org/abs/2402.14973v1'>2402.14973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03190v3")'>Unified Hallucination Detection for Multimodal Large Language Models</div>
<div id='2402.03190v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:56:11Z</div><div>Authors: Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen</div><div style='padding-top: 10px; width: 80ex'>Despite significant strides in multimodal tasks, Multimodal Large Language
Models (MLLMs) are plagued by the critical issue of hallucination. The reliable
detection of such hallucinations in MLLMs has, therefore, become a vital aspect
of model evaluation and the safeguarding of practical application deployment.
Prior research in this domain has been constrained by a narrow focus on
singular tasks, an inadequate range of hallucination categories addressed, and
a lack of detailed granularity. In response to these challenges, our work
expands the investigative horizons of hallucination detection. We present a
novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate
the evaluation of advancements in hallucination detection methods.
Additionally, we unveil a novel unified multimodal hallucination detection
framework, UNIHD, which leverages a suite of auxiliary tools to validate the
occurrence of hallucinations robustly. We demonstrate the effectiveness of
UNIHD through meticulous evaluation and comprehensive analysis. We also provide
strategic insights on the application of specific tools for addressing various
categories of hallucinations.</div><div><a href='http://arxiv.org/abs/2402.03190v3'>2402.03190v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02889v2")'>In Search of Truth: An Interrogation Approach to Hallucination Detection</div>
<div id='2403.02889v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:50:01Z</div><div>Authors: Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, Noam Koenigstein</div><div style='padding-top: 10px; width: 80ex'>Despite the many advances of Large Language Models (LLMs) and their
unprecedented rapid evolution, their impact and integration into every facet of
our daily lives is limited due to various reasons. One critical factor
hindering their widespread adoption is the occurrence of hallucinations, where
LLMs invent answers that sound realistic, yet drift away from factual truth. In
this paper, we present a novel method for detecting hallucinations in large
language models, which tackles a critical issue in the adoption of these models
in various real-world scenarios. Through extensive evaluations across multiple
datasets and LLMs, including Llama-2, we study the hallucination levels of
various recent LLMs and demonstrate the effectiveness of our method to
automatically detect them. Notably, we observe up to 62% hallucinations for
Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy
(B-ACC) of 87%, all without relying on external knowledge.</div><div><a href='http://arxiv.org/abs/2403.02889v2'>2403.02889v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04696v1")'>Fact-Checking the Output of Large Language Models via Token-Level
  Uncertainty Quantification</div>
<div id='2403.04696v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T17:44:17Z</div><div>Authors: Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are notorious for hallucinating, i.e., producing
erroneous claims in their output. Such hallucinations can be dangerous, as
occasional factual inaccuracies in the generated text might be obscured by the
rest of the output being generally factual, making it extremely hard for the
users to spot them. Current services that leverage LLMs usually do not provide
any means for detecting unreliable generations. Here, we aim to bridge this
gap. In particular, we propose a novel fact-checking and hallucination
detection pipeline based on token-level uncertainty quantification. Uncertainty
scores leverage information encapsulated in the output of a neural network or
its layers to detect unreliable predictions, and we show that they can be used
to fact-check the atomic claims in the LLM output. Moreover, we present a novel
token-level uncertainty quantification method that removes the impact of
uncertainty about what claim to generate on the current step and what surface
form to use. Our method Claim Conditioned Probability (CCP) measures only the
uncertainty of particular claim value expressed by the model. Experiments on
the task of biography generation demonstrate strong improvements for CCP
compared to the baselines for six different LLMs and three languages. Human
evaluation reveals that the fact-checking pipeline based on uncertainty
quantification is competitive with a fact-checking tool that leverages external
knowledge.</div><div><a href='http://arxiv.org/abs/2403.04696v1'>2403.04696v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12566v2")'>GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence</div>
<div id='2402.12566v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T21:45:55Z</div><div>Authors: Kundan Krishna, Sanjana Ramprasad, Prakhar Gupta, Byron C. Wallace, Zachary C. Lipton, Jeffrey P. Bigham</div><div style='padding-top: 10px; width: 80ex'>LLMs can generate factually incorrect statements even when provided access to
reference documents. Such errors can be dangerous in high-stakes applications
(e.g., document-grounded QA for healthcare or finance). We present GenAudit --
a tool intended to assist fact-checking LLM responses for document-grounded
tasks. GenAudit suggests edits to the LLM response by revising or removing
claims that are not supported by the reference document, and also presents
evidence from the reference for facts that do appear to have support. We train
models to execute these tasks, and design an interactive interface to present
suggested edits and evidence to users. Comprehensive evaluation by human raters
shows that GenAudit can detect errors in 8 different LLM outputs when
summarizing documents from diverse domains. To ensure that most errors are
flagged by the system, we propose a method that can increase the error recall
while minimizing impact on precision. We release our tool (GenAudit) and
fact-checking model for public use.</div><div><a href='http://arxiv.org/abs/2402.12566v2'>2402.12566v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15422v1")'>A Data-Centric Approach To Generate Faithful and High Quality Patient
  Summaries with Large Language Models</div>
<div id='2402.15422v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:32:28Z</div><div>Authors: Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica Agrawal, David Sontag, Xiaoyi Jiang</div><div style='padding-top: 10px; width: 80ex'>Patients often face difficulties in understanding their hospitalizations,
while healthcare workers have limited resources to provide explanations. In
this work, we investigate the potential of large language models to generate
patient summaries based on doctors' notes and study the effect of training data
on the faithfulness and quality of the generated summaries. To this end, we
develop a rigorous labeling protocol for hallucinations, and have two medical
experts annotate 100 real-world summaries and 100 generated summaries. We show
that fine-tuning on hallucination-free data effectively reduces hallucinations
from 2.60 to 1.55 per summary for Llama 2, while preserving relevant
information. Although the effect is still present, it is much smaller for GPT-4
when prompted with five examples (0.70 to 0.40). We also conduct a qualitative
evaluation using hallucination-free and improved training data. GPT-4 shows
very good results even in the zero-shot setting. We find that common
quantitative metrics do not correlate well with faithfulness and quality.
Finally, we test GPT-4 for automatic hallucination detection, which yields
promising results.</div><div><a href='http://arxiv.org/abs/2402.15422v1'>2402.15422v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13369v1")'>Clinical information extraction for Low-resource languages with Few-shot
  learning using Pre-trained language models and Prompting</div>
<div id='2403.13369v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T08:01:33Z</div><div>Authors: Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab, Christina Kiriakou, Nicolas Geis, Christoph Dieterich, Anette Frank</div><div style='padding-top: 10px; width: 80ex'>Automatic extraction of medical information from clinical documents poses
several challenges: high costs of required clinical expertise, limited
interpretability of model predictions, restricted computational resources and
privacy regulations. Recent advances in domain-adaptation and prompting methods
showed promising results with minimal training data using lightweight masked
language models, which are suited for well-established interpretability
methods. We are first to present a systematic evaluation of these methods in a
low-resource setting, by performing multi-class section classification on
German doctor's letters. We conduct extensive class-wise evaluations supported
by Shapley values, to validate the quality of our small training data set and
to ensure the interpretability of model predictions. We demonstrate that a
lightweight, domain-adapted pretrained model, prompted with just 20 shots,
outperforms a traditional classification model by 30.5% accuracy. Our results
serve as a process-oriented guideline for clinical information extraction
projects working with low-resource.</div><div><a href='http://arxiv.org/abs/2403.13369v1'>2403.13369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01713v2")'>Prompting Large Language Models for Zero-Shot Clinical Prediction with
  Structured Longitudinal Electronic Health Record Data</div>
<div id='2402.01713v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T20:14:50Z</div><div>Authors: Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin Liao, Ewen M. Harrison, Liantao Ma, Chengwei Pan</div><div style='padding-top: 10px; width: 80ex'>The inherent complexity of structured longitudinal Electronic Health Records
(EHR) data poses a significant challenge when integrated with Large Language
Models (LLMs), which are traditionally tailored for natural language
processing. Motivated by the urgent need for swift decision-making during new
disease outbreaks, where traditional predictive models often fail due to a lack
of historical data, this research investigates the adaptability of LLMs, like
GPT-4, to EHR data. We particularly focus on their zero-shot capabilities,
which enable them to make predictions in scenarios in which they haven't been
explicitly trained. In response to the longitudinal, sparse, and
knowledge-infused nature of EHR data, our prompting approach involves taking
into account specific EHR characteristics such as units and reference ranges,
and employing an in-context learning strategy that aligns with clinical
contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets
demonstrate that with our elaborately designed prompting framework, LLMs can
improve prediction performance in key tasks such as mortality, length-of-stay,
and 30-day readmission by about 35\%, surpassing ML models in few-shot
settings. Our research underscores the potential of LLMs in enhancing clinical
decision-making, especially in urgent healthcare situations like the outbreak
of emerging diseases with no labeled data. The code is publicly available at
https://github.com/yhzhu99/llm4healthcare for reproducibility.</div><div><a href='http://arxiv.org/abs/2402.01713v2'>2402.01713v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10965v2")'>Generalization in Healthcare AI: Evaluation of a Clinical Large Language
  Model</div>
<div id='2402.10965v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T06:24:52Z</div><div>Authors: Salman Rahman, Lavender Yao Jiang, Saadia Gabriel, Yindalon Aphinyanaphongs, Eric Karl Oermann, Rumi Chunara</div><div style='padding-top: 10px; width: 80ex'>Advances in large language models (LLMs) provide new opportunities in
healthcare for improved patient care, clinical decision-making, and enhancement
of physician and administrator workflows. However, the potential of these
models importantly depends on their ability to generalize effectively across
clinical environments and populations, a challenge often underestimated in
early development. To better understand reasons for these challenges and inform
mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s
clinical notes, analyzing its performance on 30-day all-cause readmission
prediction focusing on variability across hospitals and patient
characteristics. We found poorer generalization particularly in hospitals with
fewer samples, among patients with government and unspecified insurance, the
elderly, and those with high comorbidities. To understand reasons for lack of
generalization, we investigated sample sizes for fine-tuning, note content
(number of words per note), patient characteristics (comorbidity level, age,
insurance type, borough), and health system aspects (hospital, all-cause 30-day
readmission, and mortality rates). We used descriptive statistics and
supervised classification to identify features. We found that, along with
sample size, patient age, number of comorbidities, and the number of words in
notes are all important factors related to generalization. Finally, we compared
local fine-tuning (hospital specific), instance-based augmented fine-tuning and
cluster-based fine-tuning for improving generalization. Among these, local
fine-tuning proved most effective, increasing AUC by 0.25% to 11.74% (most
helpful in settings with limited data). Overall, this study provides new
insights for enhancing the deployment of large language models in the
societally important domain of healthcare, and improving their performance for
broader populations.</div><div><a href='http://arxiv.org/abs/2402.10965v2'>2402.10965v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10951v1")'>DAEDRA: A language model for predicting outcomes in passive
  pharmacovigilance reporting</div>
<div id='2402.10951v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T16:48:45Z</div><div>Authors: Chris von Csefalvay</div><div style='padding-top: 10px; width: 80ex'>Over the recent years, the emergence of large language models (LLMs) has
given rise to a proliferation of domain-specific models that are intended to
reflect the particularities of linguistic context and content as a correlate of
the originating domain. This paper details the conception, design, training and
evaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes
(mortality, ER attendance and hospitalisation) in adverse event reports
elicited through passive reporting (PR). While PR is a highly cost-efficient
way of eliciting information from a wide and diverse audience -- typically
including not only physicians and healthcare providers but also patients,
family members and other lay stakeholders --, this diversity makes PR corpora
difficult to analyse. Generic language models may not capture the complex
clinical dimensions while specific clinical or biomedical models may not
perform well on lay reports. To evaluate the utility of a subdomain-specific
language model, an adaptive training approach was adapted, wherein base
language model candidates were evaluated on a subset of the corpus, and the
best performer was trained on the entire corpus. This yielded a small but
significant improvement in $F_1$ (+1%), precision (+2.5%) and recall (+3.8%),
at a relatively low training cost and a single-day training time.
Subdomain-specific LLMs continue to be viable options for better results when
analysing highly specialised corpora.</div><div><a href='http://arxiv.org/abs/2402.10951v1'>2402.10951v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13887v1")'>A comparative study of zero-shot inference with large language models
  and supervised modeling in breast cancer pathology classification</div>
<div id='2401.13887v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T02:05:31Z</div><div>Authors: Madhumita Sushil, Travis Zack, Divneet Mandair, Zhiwei Zheng, Ahmed Wali, Yan-Ning Yu, Yuwei Quan, Atul J. Butte</div><div style='padding-top: 10px; width: 80ex'>Although supervised machine learning is popular for information extraction
from clinical notes, creating large annotated datasets requires extensive
domain expertise and is time-consuming. Meanwhile, large language models (LLMs)
have demonstrated promising transfer learning capability. In this study, we
explored whether recent LLMs can reduce the need for large-scale data
annotations. We curated a manually-labeled dataset of 769 breast cancer
pathology reports, labeled with 13 categories, to compare zero-shot
classification capability of the GPT-4 model and the GPT-3.5 model with
supervised classification performance of three model architectures: random
forests classifier, long short-term memory networks with attention (LSTM-Att),
and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either
significantly better than or as well as the best supervised model, the LSTM-Att
model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance
between labels, the differences were more prominent. Frequent sources of GPT-4
errors included inferences from multiple samples and complex task design. On
complex tasks where large annotated datasets cannot be easily collected, LLMs
can reduce the burden of large-scale data labeling. However, if the use of LLMs
is prohibitive, the use of simpler supervised models with large annotated
datasets can provide comparable results. LLMs demonstrated the potential to
speed up the execution of clinical NLP studies by reducing the need for
curating large annotated datasets. This may result in an increase in the
utilization of NLP-based variables and outcomes in observational clinical
studies.</div><div><a href='http://arxiv.org/abs/2401.13887v1'>2401.13887v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16038v1")'>Deep Learning Approaches for Improving Question Answering Systems in
  Hepatocellular Carcinoma Research</div>
<div id='2402.16038v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T09:32:17Z</div><div>Authors: Shuning Huo, Yafei Xiang, Hanyi Yu, Mengran Zhu, Yulu Gong</div><div style='padding-top: 10px; width: 80ex'>In recent years, advancements in natural language processing (NLP) have been
fueled by deep learning techniques, particularly through the utilization of
powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,
trained on vast amounts of data, have revolutionized language understanding and
generation. These pre-trained models serve as robust bases for various tasks
including semantic understanding, intelligent writing, and reasoning, paving
the way for a more generalized form of artificial intelligence. NLP, as a vital
application of AI, aims to bridge the gap between humans and computers through
natural language interaction. This paper delves into the current landscape and
future prospects of large-scale model-based NLP, focusing on the
question-answering systems within this domain. Practical cases and developments
in artificial intelligence-driven question-answering systems are analyzed to
foster further exploration and research in the realm of large-scale NLP.</div><div><a href='http://arxiv.org/abs/2402.16038v1'>2402.16038v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03791v1")'>KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing
  Patient Data with Knowledge Graphs</div>
<div id='2403.03791v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:37:22Z</div><div>Authors: Ruoqi Liu, Lingfei Wu, Ping Zhang</div><div style='padding-top: 10px; width: 80ex'>Treatment effect estimation (TEE) is the task of determining the impact of
various treatments on patient outcomes. Current TEE methods fall short due to
reliance on limited labeled data and challenges posed by sparse and
high-dimensional observational patient data. To address the challenges, we
introduce a novel pre-training and fine-tuning framework, KG-TREAT, which
synergizes large-scale observational patient data with biomedical knowledge
graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs
dual-focus KGs and integrates a deep bi-level attention synergy method for
in-depth information fusion, enabling distinct encoding of treatment-covariate
and outcome-covariate relationships. KG-TREAT also incorporates two
pre-training tasks to ensure a thorough grounding and contextualization of
patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's
superiority over existing methods, with an average improvement of 7% in Area
under the ROC Curve (AUC) and 9% in Influence Function-based Precision of
Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated
treatment effects is further affirmed by alignment with established randomized
clinical trial findings.</div><div><a href='http://arxiv.org/abs/2403.03791v1'>2403.03791v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18046v1")'>Data augmentation method for modeling health records with applications
  to clopidogrel treatment failure detection</div>
<div id='2402.18046v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T04:47:32Z</div><div>Authors: Sunwoong Choi, Samuel Kim</div><div style='padding-top: 10px; width: 80ex'>We present a novel data augmentation method to address the challenge of data
scarcity in modeling longitudinal patterns in Electronic Health Records (EHR)
of patients using natural language processing (NLP) algorithms. The proposed
method generates augmented data by rearranging the orders of medical records
within a visit where the order of elements are not obvious, if any. Applying
the proposed method to the clopidogrel treatment failure detection task enabled
up to 5.3% absolute improvement in terms of ROC-AUC (from 0.908 without
augmentation to 0.961 with augmentation) when it was used during the
pre-training procedure. It was also shown that the augmentation helped to
improve performance during fine-tuning procedures, especially when the amount
of labeled training data is limited.</div><div><a href='http://arxiv.org/abs/2402.18046v1'>2402.18046v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13448v1")'>ED-Copilot: Reduce Emergency Department Wait Time with Language Model
  Diagnostic Assistance</div>
<div id='2402.13448v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T00:49:42Z</div><div>Authors: Liwen Sun, Abhineet Agarwal, Aaron Kornblith, Bin Yu, Chenyan Xiong</div><div style='padding-top: 10px; width: 80ex'>In the emergency department (ED), patients undergo triage and multiple
laboratory tests before diagnosis. This process is time-consuming, and causes
ED crowding which significantly impacts patient mortality, medical errors,
staff burnout, etc. This work proposes (time) cost-effective diagnostic
assistance that explores the potential of artificial intelligence (AI) systems
in assisting ED clinicians to make time-efficient and accurate diagnoses. Using
publicly available patient data, we collaborate with ED clinicians to curate
MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in
suggesting laboratory tests that minimize ED wait times, while correctly
predicting critical outcomes such as death. We develop ED-Copilot which
sequentially suggests patient-specific laboratory tests and makes diagnostic
predictions. ED-Copilot uses a pre-trained bio-medical language model to encode
patient information and reinforcement learning to minimize ED wait time and
maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist,
ED-Copilot improves prediction accuracy over baselines while halving average
wait time from four hours to two hours. Ablation studies demonstrate the
importance of model scale and use of a bio-medical language model. Further
analyses reveal the necessity of personalized laboratory test suggestions for
diagnosing patients with severe cases, as well as the potential of ED-Copilot
in providing ED clinicians with informative laboratory test recommendations.
Our code is available at https://github.com/cxcscmu/ED-Copilot.</div><div><a href='http://arxiv.org/abs/2402.13448v1'>2402.13448v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03597v1")'>Identifying Reasons for Contraceptive Switching from Real-World Data
  Using Large Language Models</div>
<div id='2402.03597v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T00:14:53Z</div><div>Authors: Brenda Y. Miao, Christopher YK Williams, Ebenezer Chinedu-Eneh, Travis Zack, Emily Alsentzer, Atul J. Butte, Irene Y. Chen</div><div style='padding-top: 10px; width: 80ex'>Prescription contraceptives play a critical role in supporting women's
reproductive health. With nearly 50 million women in the United States using
contraceptives, understanding the factors that drive contraceptives selection
and switching is of significant interest. However, many factors related to
medication switching are often only captured in unstructured clinical notes and
can be difficult to extract. Here, we evaluate the zero-shot abilities of a
recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft
Azure API), to identify reasons for switching between classes of contraceptives
from the UCSF Information Commons clinical notes dataset. We demonstrate that
GPT-4 can accurately extract reasons for contraceptive switching, outperforming
baseline BERT-based models with microF1 scores of 0.849 and 0.881 for
contraceptive start and stop extraction, respectively. Human evaluation of
GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal
hallucinations. Using extracted reasons, we identified patient preference,
adverse events, and insurance as key reasons for switching using unsupervised
topic modeling approaches. Notably, we also showed using our approach that
"weight gain/mood change" and "insurance coverage" are disproportionately found
as reasons for contraceptive switching in specific demographic populations. Our
code and supplemental data are available at
https://github.com/BMiao10/contraceptive-switching.</div><div><a href='http://arxiv.org/abs/2402.03597v1'>2402.03597v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18419v1")'>Can GPT Improve the State of Prior Authorization via Guideline Based
  Automated Question Answering?</div>
<div id='2402.18419v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:39:53Z</div><div>Authors: Shubham Vatsal, Ayush Singh, Shabnam Tafreshi</div><div style='padding-top: 10px; width: 80ex'>Health insurance companies have a defined process called prior authorization
(PA) which is a health plan cost-control process that requires doctors and
other healthcare professionals to get clearance in advance from a health plan
before performing a particular procedure on a patient in order to be eligible
for payment coverage. For health insurance companies, approving PA requests for
patients in the medical domain is a time-consuming and challenging task. One of
those key challenges is validating if a request matches up to certain criteria
such as age, gender, etc. In this work, we evaluate whether GPT can validate
numerous key factors, in turn helping health plans reach a decision drastically
faster. We frame it as a question answering task, prompting GPT to answer a
question from patient electronic health record. We experiment with different
conventional prompting techniques as well as introduce our own novel prompting
technique. Moreover, we report qualitative assessment by humans on the natural
language generation outputs from our approach. Results show that our method
achieves superior performance with the mean weighted F1 score of 0.61 as
compared to its standard counterparts.</div><div><a href='http://arxiv.org/abs/2402.18419v1'>2402.18419v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00788v1")'>PRECISE Framework: GPT-based Text For Improved Readability, Reliability,
  and Understandability of Radiology Reports For Patient-Centered Care</div>
<div id='2403.00788v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:26:31Z</div><div>Authors: Satvik Tripathi, Liam Mutter, Meghana Muppuri, Suhani Dheer, Emiliano Garza-Frias, Komal Awan, Aakash Jha, Michael Dezube, Azadeh Tabari, Christopher P. Bridge, Dania Daye</div><div style='padding-top: 10px; width: 80ex'>This study introduces and evaluates the PRECISE framework, utilizing OpenAI's
GPT-4 to enhance patient engagement by providing clearer and more accessible
chest X-ray reports at a sixth-grade reading level. The framework was tested on
500 reports, demonstrating significant improvements in readability,
reliability, and understandability. Statistical analyses confirmed the
effectiveness of the PRECISE approach, highlighting its potential to foster
patient-centric care delivery in healthcare decision-making.</div><div><a href='http://arxiv.org/abs/2403.00788v1'>2403.00788v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14367v1")'>Genie: Achieving Human Parity in Content-Grounded Datasets Generation</div>
<div id='2401.14367v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:14:57Z</div><div>Authors: Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf Toledo, Eyal Shnarch, Leshem Choshen</div><div style='padding-top: 10px; width: 80ex'>The lack of high-quality data for content-grounded generation tasks has been
identified as a major obstacle to advancing these tasks. To address this gap,
we propose Genie, a novel method for automatically generating high-quality
content-grounded data. It consists of three stages: (a) Content Preparation,
(b) Generation: creating task-specific examples from the content (e.g.,
question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure
the quality and faithfulness of the generated data. We showcase this
methodology by generating three large-scale synthetic data, making wishes, for
Long-Form Question-Answering (LFQA), summarization, and information extraction.
In a human evaluation, our generated data was found to be natural and of high
quality. Furthermore, we compare models trained on our data with models trained
on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for
Summarization. We show that our models are on par with or outperforming models
trained on human-generated data and consistently outperforming them in
faithfulness. Finally, we applied our method to create LFQA data within the
medical domain and compared a model trained on it with models trained on other
domains.</div><div><a href='http://arxiv.org/abs/2401.14367v1'>2401.14367v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16200v1")'>IR2: Information Regularization for Information Retrieval</div>
<div id='2402.16200v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T21:25:06Z</div><div>Authors: Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Weili Cao, Ramamohan Paturi, Leon Bergen</div><div style='padding-top: 10px; width: 80ex'>Effective information retrieval (IR) in settings with limited training data,
particularly for complex queries, remains a challenging task. This paper
introduces IR2, Information Regularization for Information Retrieval, a
technique for reducing overfitting during synthetic data generation. This
approach, representing a novel application of regularization techniques in
synthetic data creation for IR, is tested on three recent IR tasks
characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.
Experimental results indicate that our regularization techniques not only
outperform previous synthetic query generation methods on the tasks considered
but also reduce cost by up to 50%. Furthermore, this paper categorizes and
explores three regularization methods at different stages of the query
synthesis pipeline-input, prompt, and output-each offering varying degrees of
performance improvement compared to models where no regularization is applied.
This provides a systematic approach for optimizing synthetic data generation in
data-limited, complex-query IR scenarios. All code, prompts and synthetic data
are available at
https://github.com/Info-Regularization/Information-Regularization.</div><div><a href='http://arxiv.org/abs/2402.16200v1'>2402.16200v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16358v1")'>An Integrated Data Processing Framework for Pretraining Foundation
  Models</div>
<div id='2402.16358v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:22:51Z</div><div>Authors: Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao</div><div style='padding-top: 10px; width: 80ex'>The ability of the foundation models heavily relies on large-scale, diverse,
and high-quality pretraining data. In order to improve data quality,
researchers and practitioners often have to manually curate datasets from
difference sources and develop dedicated data cleansing pipeline for each data
repository. Lacking a unified data processing framework, this process is
repetitive and cumbersome. To mitigate this issue, we propose a data processing
framework that integrates a Processing Module which consists of a series of
operators at different granularity levels, and an Analyzing Module which
supports probing and evaluation of the refined data. The proposed framework is
easy to use and highly flexible. In this demo paper, we first introduce how to
use this framework with some example use cases and then demonstrate its
effectiveness in improving the data quality with an automated evaluation with
ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code
and demonstration videos are accessible on GitHub.</div><div><a href='http://arxiv.org/abs/2402.16358v1'>2402.16358v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00234v1")'>Are Generative AI systems Capable of Supporting Information Needs of
  Patients?</div>
<div id='2402.00234v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T23:24:37Z</div><div>Authors: Shreya Rajagopal, Subhashis Hazarika, Sookyung Kim, Yan-ming Chiou, Jae Ho Sohn, Hari Subramonyam, Shiwali Mohan</div><div style='padding-top: 10px; width: 80ex'>Patients managing a complex illness such as cancer face a complex information
challenge where they not only must learn about their illness but also how to
manage it. Close interaction with healthcare experts (radiologists,
oncologists) can improve patient learning and thereby, their disease outcome.
However, this approach is resource intensive and takes expert time away from
other critical tasks. Given the recent advancements in Generative AI models
aimed at improving the healthcare system, our work investigates whether and how
generative visual question answering systems can responsibly support patient
information needs in the context of radiology imaging data. We conducted a
formative need-finding study in which participants discussed chest computed
tomography (CT) scans and associated radiology reports of a fictitious close
relative with a cardiothoracic radiologist. Using thematic analysis of the
conversation between participants and medical experts, we identified commonly
occurring themes across interactions, including clarifying medical terminology,
locating the problems mentioned in the report in the scanned image,
understanding disease prognosis, discussing the next diagnostic steps, and
comparing treatment options. Based on these themes, we evaluated two
state-of-the-art generative visual language models against the radiologist's
responses. Our results reveal variability in the quality of responses generated
by the models across various themes. We highlight the importance of
patient-facing generative AI systems to accommodate a diverse range of
conversational themes, catering to the real-world informational needs of
patients.</div><div><a href='http://arxiv.org/abs/2402.00234v1'>2402.00234v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05654v1")'>Towards Conversational Diagnostic AI</div>
<div id='2401.05654v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T04:25:06Z</div><div>Authors: Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong Cheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, Alan Karthikesalingam, Vivek Natarajan</div><div style='padding-top: 10px; width: 80ex'>At the heart of medicine lies the physician-patient dialogue, where skillful
history-taking paves the way for accurate diagnosis, effective management, and
enduring trust. Artificial Intelligence (AI) systems capable of diagnostic
dialogue could increase accessibility, consistency, and quality of care.
However, approximating clinicians' expertise is an outstanding grand challenge.
Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large
Language Model (LLM) based AI system optimized for diagnostic dialogue.
  AMIE uses a novel self-play based simulated environment with automated
feedback mechanisms for scaling learning across diverse disease conditions,
specialties, and contexts. We designed a framework for evaluating
clinically-meaningful axes of performance including history-taking, diagnostic
accuracy, management reasoning, communication skills, and empathy. We compared
AMIE's performance to that of primary care physicians (PCPs) in a randomized,
double-blind crossover study of text-based consultations with validated patient
actors in the style of an Objective Structured Clinical Examination (OSCE). The
study included 149 case scenarios from clinical providers in Canada, the UK,
and India, 20 PCPs for comparison with AMIE, and evaluations by specialist
physicians and patient actors. AMIE demonstrated greater diagnostic accuracy
and superior performance on 28 of 32 axes according to specialist physicians
and 24 of 26 axes according to patient actors. Our research has several
limitations and should be interpreted with appropriate caution. Clinicians were
limited to unfamiliar synchronous text-chat which permits large-scale
LLM-patient interactions but is not representative of usual clinical practice.
While further research is required before AMIE could be translated to
real-world settings, the results represent a milestone towards conversational
diagnostic AI.</div><div><a href='http://arxiv.org/abs/2401.05654v1'>2401.05654v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06088v1")'>Autocompletion of Chief Complaints in the Electronic Health Records
  using Large Language Models</div>
<div id='2401.06088v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:06:30Z</div><div>Authors: K M Sajjadul Islam, Ayesha Siddika Nipu, Praveen Madiraju, Priya Deshpande</div><div style='padding-top: 10px; width: 80ex'>The Chief Complaint (CC) is a crucial component of a patient's medical record
as it describes the main reason or concern for seeking medical care. It
provides critical information for healthcare providers to make informed
decisions about patient care. However, documenting CCs can be time-consuming
for healthcare providers, especially in busy emergency departments. To address
this issue, an autocompletion tool that suggests accurate and well-formatted
phrases or sentences for clinical notes can be a valuable resource for triage
nurses. In this study, we utilized text generation techniques to develop
machine learning models using CC data. In our proposed work, we train a Long
Short-Term Memory (LSTM) model and fine-tune three different variants of
Biomedical Generative Pretrained Transformers (BioGPT), namely
microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.
Additionally, we tune a prompt by incorporating exemplar CC sentences,
utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on
the perplexity score, modified BERTScore, and cosine similarity score. The
results show that BioGPT-Large exhibits superior performance compared to the
other models. It consistently achieves a remarkably low perplexity score of
1.65 when generating CC, whereas the baseline LSTM model achieves the best
perplexity score of 170. Further, we evaluate and assess the proposed models'
performance and the outcome of GPT-4.0. Our study demonstrates that utilizing
LLMs such as BioGPT, leads to the development of an effective autocompletion
tool for generating CC documentation in healthcare settings.</div><div><a href='http://arxiv.org/abs/2401.06088v1'>2401.06088v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05720v1")'>A Benchmark of Domain-Adapted Large Language Models for Generating Brief
  Hospital Course Summaries</div>
<div id='2403.05720v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T23:17:55Z</div><div>Authors: Asad Aali, Dave Van Veen, Yamin Ishraq Arefeen, Jason Hom, Christian Bluethgen, Eduardo Pontes Reis, Sergios Gatidis, Namuun Clifford, Joseph Daws, Arash S. Tehrani, Jangwon Kim, Akshay S. Chaudhari</div><div style='padding-top: 10px; width: 80ex'>Brief hospital course (BHC) summaries are common clinical documents generated
by summarizing clinical notes. While large language models (LLMs) depict
remarkable capabilities in automating real-world tasks, their capabilities for
healthcare applications such as BHC synthesis have not been shown. To enable
the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark
consisting of a pre-processed dataset extracted from MIMIC-IV notes,
encapsulating clinical note, and brief hospital course (BHC) pairs. We assess
the performance of two general-purpose LLMs and three healthcare-adapted LLMs
to improve BHC synthesis from clinical notes. Using clinical notes as input for
generating BHCs, we apply prompting-based (using in-context learning) and
fine-tuning-based adaptation strategies to three open-source LLMs
(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,
GPT-4). We quantitatively evaluate the performance of these LLMs across varying
context-length inputs using conventional natural language similarity metrics.
We further perform a qualitative study where five diverse clinicians blindly
compare clinician-written BHCs and two LLM-generated BHCs for 30 samples across
metrics of comprehensiveness, conciseness, factual correctness, and fluency.
Overall, we present a new benchmark and pre-processed dataset for using LLMs in
BHC synthesis from clinical notes. We observe high-quality summarization
performance for both in-context proprietary and fine-tuned open-source LLMs
using both quantitative metrics and a qualitative clinical reader study. We
propose our work as a benchmark to motivate future works to adapt and assess
the performance of LLMs in BHC synthesis.</div><div><a href='http://arxiv.org/abs/2403.05720v1'>2403.05720v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06659v1")'>Zero-Shot ECG Classification with Multimodal Learning and Test-time
  Clinical Knowledge Enhancement</div>
<div id='2403.06659v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T12:28:55Z</div><div>Authors: Che Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, Rossella Arcucci</div><div style='padding-top: 10px; width: 80ex'>Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for
detecting cardiac arrhythmic diseases in clinical practice. While ECG
Self-supervised Learning (eSSL) methods show promise in representation learning
from unannotated ECG data, they often overlook the clinical knowledge that can
be found in reports. This oversight and the requirement for annotated samples
for downstream tasks limit eSSL's versatility. In this work, we address these
issues with the Multimodal ECG Representation Learning (MERL}) framework.
Through multimodal learning on ECG records and associated reports, MERL is
capable of performing zero-shot ECG classification with text prompts,
eliminating the need for training data in downstream tasks. At test time, we
propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach,
which uses Large Language Models (LLMs) to exploit external expert-verified
clinical knowledge databases, generating more descriptive prompts and reducing
hallucinations in LLM-generated content to boost zero-shot classification.
Based on MERL, we perform the first benchmark across six public ECG datasets,
showing the superior performance of MERL compared against eSSL methods.
Notably, MERL achieves an average AUC score of 75.2% in zero-shot
classification (without training data), 3.2% higher than linear probed eSSL
methods with 10\% annotated training data, averaged across all six datasets.</div><div><a href='http://arxiv.org/abs/2403.06659v1'>2403.06659v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04945v2")'>Electrocardiogram Instruction Tuning for Report Generation</div>
<div id='2403.04945v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T23:20:56Z</div><div>Authors: Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang</div><div style='padding-top: 10px; width: 80ex'>Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool
for cardiac conditions monitoring, are crucial in assisting clinicians. Recent
studies have concentrated on classifying cardiac conditions using ECG data but
have overlooked ECG report generation, which is not only time-consuming but
also requires clinical expertise. To automate ECG report generation and ensure
its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT)
framework, the \textit{first} attempt to tackle ECG report generation with LLMs
and multimodal instructions. To facilitate future research, we establish a
benchmark to evaluate MEIT with various LLMs backbones across two large-scale
ECG datasets. Our approach uniquely aligns the representations of the ECG
signal and the report, and we conduct extensive experiments to benchmark MEIT
with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results
underscore the superior performance of instruction-tuned LLMs, showcasing their
proficiency in quality report generation, zero-shot capabilities, and
resilience to signal perturbation. These findings emphasize the efficacy of our
MEIT framework and its potential for real-world clinical application.</div><div><a href='http://arxiv.org/abs/2403.04945v2'>2403.04945v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04938v1")'>Advancing ECG Diagnosis Using Reinforcement Learning on Global Waveform
  Variations Related to P Wave and PR Interval</div>
<div id='2401.04938v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T05:28:42Z</div><div>Authors: Rumsha Fatima, Shahzad Younis, Faraz Shaikh, Hamna Imran, Haseeb Sultan, Shahzad Rasool, Mehak Rafiq</div><div style='padding-top: 10px; width: 80ex'>The reliable diagnosis of cardiac conditions through electrocardiogram (ECG)
analysis critically depends on accurately detecting P waves and measuring the
PR interval. However, achieving consistent and generalizable diagnoses across
diverse populations presents challenges due to the inherent global variations
observed in ECG signals. This paper is focused on applying the Q learning
reinforcement algorithm to the various ECG datasets available in the
PhysioNet/Computing in Cardiology Challenge (CinC). Five ECG beats, including
Normal Sinus Rhythm, Atrial Flutter, Atrial Fibrillation, 1st Degree
Atrioventricular Block, and Left Atrial Enlargement, are included to study
variations of P waves and PR Interval on Lead II and Lead V1. Q-Agent
classified 71,672 beat samples in 8,867 patients with an average accuracy of
90.4% and only 9.6% average hamming loss over misclassification. The average
classification time at the 100th episode containing around 40,000 samples is
0.04 seconds. An average training reward of 344.05 is achieved at an alpha,
gamma, and SoftMax temperature rate of 0.001, 0.9, and 0.1, respectively.</div><div><a href='http://arxiv.org/abs/2401.04938v1'>2401.04938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07309v1")'>Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM
  Framework with Mortality Classifier and Transformer</div>
<div id='2403.07309v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T04:36:41Z</div><div>Authors: Dipesh Tamboli, Jiayu Chen, Kiran Pranesh Jotheeswaran, Denny Yu, Vaneet Aggarwal</div><div style='padding-top: 10px; width: 80ex'>Sepsis, a life-threatening condition triggered by the body's exaggerated
response to infection, demands urgent intervention to prevent severe
complications. Existing machine learning methods for managing sepsis struggle
in offline scenarios, exhibiting suboptimal performance with survival rates
below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with
Positive and Negative Demonstrations for Sequential Decision-Making" framework
utilizing an innovative transformer-based model and a feedback reinforcer to
replicate expert actions while considering individual patient characteristics.
A mortality classifier with 96.7\% accuracy guides treatment decisions towards
positive outcomes. The POSNEGDM framework significantly improves patient
survival, saving 97.39% of patients, outperforming established machine learning
algorithms (Decision Transformer and Behavioral Cloning) with survival rates of
33.4% and 43.5%, respectively. Additionally, ablation studies underscore the
critical role of the transformer-based decision maker and the integration of a
mortality classifier in enhancing overall survival rates. In summary, our
proposed approach presents a promising avenue for enhancing sepsis treatment
outcomes, contributing to improved patient care and reduced healthcare costs.</div><div><a href='http://arxiv.org/abs/2403.07309v1'>2403.07309v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06814v1")'>ε-Neural Thompson Sampling of Deep Brain Stimulation for
  Parkinson Disease Treatment</div>
<div id='2403.06814v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:33:40Z</div><div>Authors: Hao-Lun Hsu, Qitong Gao, Miroslav Pajic</div><div style='padding-top: 10px; width: 80ex'>Deep Brain Stimulation (DBS) stands as an effective intervention for
alleviating the motor symptoms of Parkinson's disease (PD). Traditional
commercial DBS devices are only able to deliver fixed-frequency periodic pulses
to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS).
However, they in general suffer from energy inefficiency and side effects, such
as speech impairment. Recent research has focused on adaptive DBS (aDBS) to
resolve the limitations of cDBS. Specifically, reinforcement learning (RL)
based approaches have been developed to adapt the frequencies of the stimuli in
order to achieve both energy efficiency and treatment efficacy. However, RL
approaches in general require significant amount of training data and
computational resources, making it intractable to integrate RL policies into
real-time embedded systems as needed in aDBS. In contrast, contextual
multi-armed bandits (CMAB) in general lead to better sample efficiency compared
to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we
define the context as the signals capturing irregular neuronal firing
activities in the BG regions (i.e., beta-band power spectral density), while
each arm signifies the (discretized) pulse frequency of the stimulation.
Moreover, an {\epsilon}-exploring strategy is introduced on top of the classic
Thompson sampling method, leading to an algorithm called {\epsilon}-Neural
Thompson sampling ({\epsilon}-NeuralTS), such that the learned CMAB policy can
better balance exploration and exploitation of the BG environment. The
{\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that
captures the neuronal activities in PD patients' brains. The results show that
our method outperforms both existing cDBS methods and CMAB baselines.</div><div><a href='http://arxiv.org/abs/2403.06814v1'>2403.06814v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15700v1")'>CoRelation: Boosting Automatic ICD Coding Through Contextualized Code
  Relation Learning</div>
<div id='2402.15700v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T03:25:28Z</div><div>Authors: Junyu Luo, Xiaochen Wang, Jiaqi Wang, Aofei Chang, Yaqing Wang, Fenglong Ma</div><div style='padding-top: 10px; width: 80ex'>Automatic International Classification of Diseases (ICD) coding plays a
crucial role in the extraction of relevant information from clinical notes for
proper recording and billing. One of the most important directions for boosting
the performance of automatic ICD coding is modeling ICD code relations.
However, current methods insufficiently model the intricate relationships among
ICD codes and often overlook the importance of context in clinical notes. In
this paper, we propose a novel approach, a contextualized and flexible
framework, to enhance the learning of ICD code representations. Our approach,
unlike existing methods, employs a dependent learning paradigm that considers
the context of clinical notes in modeling all possible code relations. We
evaluate our approach on six public ICD coding datasets and the experimental
results demonstrate the effectiveness of our approach compared to
state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2402.15700v1'>2402.15700v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11894v1")'>From explainable to interpretable deep learning for natural language
  processing in healthcare: how far from reality?</div>
<div id='2403.11894v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:53:33Z</div><div>Authors: Guangming Huang, Yunfei Long, Yingya Li, Giorgos Papanastasiou</div><div style='padding-top: 10px; width: 80ex'>Deep learning (DL) has substantially enhanced healthcare research by
addressing various natural language processing (NLP) tasks. Yet, the increasing
complexity of DL-based NLP methods necessitates transparent model
interpretability, or at least explainability, for reliable decision-making.
This work presents a thorough scoping review on explainable and interpretable
DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial
Intelligence) was introduced to distinguish XAI from IAI. Methods were further
categorized based on their functionality (model-, input-, output-based) and
scope (local, global). Our analysis shows that attention mechanisms were the
most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The
major challenges identified are that most XIAI do not explore "global" modeling
processes, the lack of best practices, and the unmet need for systematic
evaluation and benchmarks. Important opportunities were raised such as using
"attention" to enhance multi-modal XIAI for personalized medicine and combine
DL with causal reasoning. Our discussion encourages the integration of XIAI in
LLMs and domain-specific smaller models. Our review can stimulate further
research and benchmarks toward improving inherent IAI and engaging complex NLP
in healthcare.</div><div><a href='http://arxiv.org/abs/2403.11894v1'>2403.11894v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09762v1")'>Emotional Intelligence Through Artificial Intelligence : NLP and Deep
  Learning in the Analysis of Healthcare Texts</div>
<div id='2403.09762v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:58:13Z</div><div>Authors: Prashant Kumar Nag, Amit Bhagat, R. Vishnu Priya, Deepak kumar Khare</div><div style='padding-top: 10px; width: 80ex'>This manuscript presents a methodical examination of the utilization of
Artificial Intelligence in the assessment of emotions in texts related to
healthcare, with a particular focus on the incorporation of Natural Language
Processing and deep learning technologies. We scrutinize numerous research
studies that employ AI to augment sentiment analysis, categorize emotions, and
forecast patient outcomes based on textual information derived from clinical
narratives, patient feedback on medications, and online health discussions. The
review demonstrates noteworthy progress in the precision of algorithms used for
sentiment classification, the prognostic capabilities of AI models for
neurodegenerative diseases, and the creation of AI-powered systems that offer
support in clinical decision-making. Remarkably, the utilization of AI
applications has exhibited an enhancement in personalized therapy plans by
integrating patient sentiment and contributing to the early identification of
mental health disorders. There persist challenges, which encompass ensuring the
ethical application of AI, safeguarding patient confidentiality, and addressing
potential biases in algorithmic procedures. Nevertheless, the potential of AI
to revolutionize healthcare practices is unmistakable, offering a future where
healthcare is not only more knowledgeable and efficient but also more
empathetic and centered around the needs of patients. This investigation
underscores the transformative influence of AI on healthcare, delivering a
comprehensive comprehension of its role in examining emotional content in
healthcare texts and highlighting the trajectory towards a more compassionate
approach to patient care. The findings advocate for a harmonious synergy
between AI's analytical capabilities and the human aspects of healthcare.</div><div><a href='http://arxiv.org/abs/2403.09762v1'>2403.09762v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01067v1")'>Assessing Patient Eligibility for Inspire Therapy through Machine
  Learning and Deep Learning Models</div>
<div id='2402.01067v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T23:53:12Z</div><div>Authors: Mohsena Chowdhury, Tejas Vyas, Rahul Alapati, Andrés M Bur, Guanghui Wang</div><div style='padding-top: 10px; width: 80ex'>Inspire therapy is an FDA-approved internal neurostimulation treatment for
obstructive sleep apnea. However, not all patients respond to this therapy,
posing a challenge even for experienced otolaryngologists to determine
candidacy. This paper makes the first attempt to leverage both machine learning
and deep learning techniques in discerning patient responsiveness to Inspire
therapy using medical data and videos captured through Drug-Induced Sleep
Endoscopy (DISE), an essential procedure for Inspire therapy. To achieve this,
we gathered and annotated three datasets from 127 patients. Two of these
datasets comprise endoscopic videos focused on the Base of the Tongue and
Velopharynx. The third dataset composes the patient's clinical information. By
utilizing these datasets, we benchmarked and compared the performance of six
deep learning models and five classical machine learning algorithms. The
results demonstrate the potential of employing machine learning and deep
learning techniques to determine a patient's eligibility for Inspire therapy,
paving the way for future advancements in this field.</div><div><a href='http://arxiv.org/abs/2402.01067v1'>2402.01067v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00707v1")'>Non-Exchangeable Conformal Language Generation with Nearest Neighbors</div>
<div id='2402.00707v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:04:04Z</div><div>Authors: Dennis Ulmer, Chrysoula Zerva, André F. T. Martins</div><div style='padding-top: 10px; width: 80ex'>Quantifying uncertainty in automatically generated text is important for
letting humans check potential hallucinations and making systems more reliable.
Conformal prediction is an attractive framework to provide predictions imbued
with statistical guarantees, however, its application to text generation is
challenging since any i.i.d. assumptions are not realistic. In this paper, we
bridge this gap by leveraging recent results on non-exchangeable conformal
prediction, which still ensures bounds on coverage. The result,
non-exchangeable conformal nucleus sampling, is a novel extension of the
conformal prediction framework to generation based on nearest neighbors. Our
method can be used post-hoc for an arbitrary model without extra training and
supplies token-level, calibrated prediction sets equipped with statistical
guarantees. Experiments in machine translation and language modeling show
encouraging results in generation quality. By also producing tighter prediction
sets with good coverage, we thus give a more theoretically principled way to
perform sampling with conformal guarantees.</div><div><a href='http://arxiv.org/abs/2402.00707v1'>2402.00707v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08086v1")'>Text-centric Alignment for Multi-Modality Learning</div>
<div id='2402.08086v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:07:43Z</div><div>Authors: Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, Shou-De Lin</div><div style='padding-top: 10px; width: 80ex'>This research paper addresses the challenge of modality mismatch in
multimodal learning, where the modalities available during inference differ
from those available at training. We propose the Text-centric Alignment for
Multi-Modality Learning (TAMML) approach, an innovative method that utilizes
Large Language Models (LLMs) with in-context learning and foundation models to
enhance the generalizability of multimodal systems under these conditions. By
leveraging the unique properties of text as a unified semantic space, TAMML
demonstrates significant improvements in handling unseen, diverse, and
unpredictable modality combinations. TAMML not only adapts to varying
modalities but also maintains robust performance, showcasing the potential of
foundation models in overcoming the limitations of traditional fixed-modality
frameworks in embedding representations. This study contributes to the field by
offering a flexible, effective solution for real-world applications where
modality availability is dynamic and uncertain.</div><div><a href='http://arxiv.org/abs/2402.08086v1'>2402.08086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08825v1")'>AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant
  Reviews and Images on Social Media</div>
<div id='2401.08825v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T20:57:36Z</div><div>Authors: Alessandro Gambetti, Qiwei Han</div><div style='padding-top: 10px; width: 80ex'>Online reviews in the form of user-generated content (UGC) significantly
impact consumer decision-making. However, the pervasive issue of not only human
fake content but also machine-generated content challenges UGC's reliability.
Recent advances in Large Language Models (LLMs) may pave the way to fabricate
indistinguishable fake generated content at a much lower cost. Leveraging
OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a
multi-modal dataset of 20,144 restaurant review-image pairs divided into
authentic and machine-generated. We explore unimodal and multimodal detection
models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from
readability and photographic theories to score reviews and images,
respectively, demonstrating their utility as hand-crafted features in scalable
and interpretable detection models, with comparable performance. The paper
contributes by open-sourcing the dataset and releasing fake review detectors,
recommending its use in unimodal and multimodal fake review detection tasks,
and evaluating linguistic and visual features in synthetic versus authentic
data.</div><div><a href='http://arxiv.org/abs/2401.08825v1'>2401.08825v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01832v1")'>SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?</div>
<div id='2402.01832v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:59:58Z</div><div>Authors: Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem</div><div style='padding-top: 10px; width: 80ex'>We present SynthCLIP, a novel framework for training CLIP models with
entirely synthetic text-image pairs, significantly departing from previous
methods relying on real data. Leveraging recent text-to-image (TTI) generative
networks and large language models (LLM), we are able to generate synthetic
datasets of images and corresponding captions at any scale, with no human
intervention. With training at scale, SynthCLIP achieves performance comparable
to CLIP models trained on real datasets. We also introduce SynthCI-30M, a
purely synthetic dataset comprising 30 million captioned images. Our code,
trained models, and generated data are released at
https://github.com/hammoudhasan/SynthCLIP</div><div><a href='http://arxiv.org/abs/2402.01832v1'>2402.01832v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12481v1")'>TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer</div>
<div id='2403.12481v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T06:36:42Z</div><div>Authors: Eunjee Choi, Jong-Kook Kim</div><div style='padding-top: 10px; width: 80ex'>Detecting fake news has received a lot of attention. Many previous methods
concatenate independently encoded unimodal data, ignoring the benefits of
integrated multimodal information. Also, the absence of specialized feature
extraction for text and images further limits these methods. This paper
introduces an end-to-end model called TT-BLIP that applies the bootstrapping
language-image pretraining for unified vision-language understanding and
generation (BLIP) for three types of information: BERT and
BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for
images, and bidirectional BLIP encoders for multimodal information. The
Multimodal Tri-Transformer fuses tri-modal features using three types of
multi-head attention mechanisms, ensuring integrated modalities for enhanced
representations and improved multimodal data analysis. The experiments are
performed using two fake news datasets, Weibo and Gossipcop. The results
indicate TT-BLIP outperforms the state-of-the-art models.</div><div><a href='http://arxiv.org/abs/2403.12481v1'>2403.12481v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06194v1")'>CrisisKAN: Knowledge-infused and Explainable Multimodal Attention
  Network for Crisis Event Classification</div>
<div id='2401.06194v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T13:22:38Z</div><div>Authors: Shubham Gupta, Nandini Saini, Suman Kundu, Debasis Das</div><div style='padding-top: 10px; width: 80ex'>Pervasive use of social media has become the emerging source for real-time
information (like images, text, or both) to identify various events. Despite
the rapid growth of image and text-based event classification, the
state-of-the-art (SOTA) models find it challenging to bridge the semantic gap
between features of image and text modalities due to inconsistent encoding.
Also, the black-box nature of models fails to explain the model's outcomes for
building trust in high-stakes situations such as disasters, pandemic.
Additionally, the word limit imposed on social media posts can potentially
introduce bias towards specific events. To address these issues, we proposed
CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention
Network that entails images and texts in conjunction with external knowledge
from Wikipedia to classify crisis events. To enrich the context-specific
understanding of textual information, we integrated Wikipedia knowledge using
proposed wiki extraction algorithm. Along with this, a guided cross-attention
module is implemented to fill the semantic gap in integrating visual and
textual data. In order to ensure reliability, we employ a model-specific
approach called Gradient-weighted Class Activation Mapping (Grad-CAM) that
provides a robust explanation of the predictions of the proposed model. The
comprehensive experiments conducted on the CrisisMMD dataset yield in-depth
analysis across various crisis-specific tasks and settings. As a result,
CrisisKAN outperforms existing SOTA methodologies and provides a novel view in
the domain of explainable multimodal event classification.</div><div><a href='http://arxiv.org/abs/2401.06194v1'>2401.06194v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03043v1")'>SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach</div>
<div id='2402.03043v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:29:54Z</div><div>Authors: Mohammad N. S. Jahromi, Satya. M. Muddamsetty, Asta Sofie Stage Jarlner, Anna Murphy Høgenhaug, Thomas Gammeltoft-Hansen, Thomas B. Moeslund</div><div style='padding-top: 10px; width: 80ex'>Explainable AI (XAI) aids in deciphering 'black-box' models. While several
methods have been proposed and evaluated primarily in the image domain, the
exploration of explainability in the text domain remains a growing research
area. In this paper, we delve into the applicability of XAI methods for the
text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU)
XAI method, recognized for its superior capability in localizing entire salient
regions in image-based classification is extended to textual data. The extended
method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to
generate heatmaps at a granular, word-based level, thereby providing
explanations that highlight contextually significant textual elements crucial
for model predictions. Given the absence of a unified standard for assessing
XAI methods, this study applies a holistic three-tiered comprehensive
evaluation framework: Functionally-Grounded, Human-Grounded and
Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT
across various experiments. We find that, in sentiment analysis task of a movie
review dataset, SIDU-TXT excels in both functionally and human-grounded
evaluations, demonstrating superior performance through quantitative and
qualitative analyses compared to benchmarks like Grad-CAM and LIME. In the
application-grounded evaluation within the sensitive and complex legal domain
of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable
performances, each with its own set of strengths and weaknesses. However, both
methods fall short of entirely fulfilling the sophisticated criteria of expert
expectations, highlighting the imperative need for additional research in XAI
methods suitable for such domains.</div><div><a href='http://arxiv.org/abs/2402.03043v1'>2402.03043v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15017v1")'>Towards Few-Shot Adaptation of Foundation Models via Multitask
  Finetuning</div>
<div id='2402.15017v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T23:29:42Z</div><div>Authors: Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang</div><div style='padding-top: 10px; width: 80ex'>Foundation models have emerged as a powerful tool for many AI problems.
Despite the tremendous success of foundation models, effective adaptation to
new tasks, particularly those with limited labels, remains an open question and
lacks theoretical understanding. An emerging solution with recent success in
vision and NLP involves finetuning a foundation model on a selection of
relevant tasks, before its adaptation to a target task with limited labeled
samples. In this paper, we study the theoretical justification of this
multitask finetuning approach. Our theoretical analysis reveals that with a
diverse set of related tasks, this multitask finetuning leads to reduced error
in the target task, in comparison to directly adapting the same pretrained
model. We quantify the relationship between finetuning tasks and target tasks
by diversity and consistency metrics, and further propose a practical task
selection algorithm. We substantiate our theoretical claims with extensive
empirical evidence. Further, we present results affirming our task selection
algorithm adeptly chooses related finetuning tasks, providing advantages to the
model performance on target tasks. We believe our study shed new light on the
effective adaptation of foundation models to new tasks that lack abundant
labels. Our code is available at
https://github.com/OliverXUZY/Foudation-Model_Multitask.</div><div><a href='http://arxiv.org/abs/2402.15017v1'>2402.15017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10696v1")'>On the low-shot transferability of [V]-Mamba</div>
<div id='2403.10696v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T21:29:33Z</div><div>Authors: Diganta Misra, Jay Gala, Antonio Orvieto</div><div style='padding-top: 10px; width: 80ex'>The strength of modern large-scale neural networks lies in their ability to
efficiently adapt to new tasks with few examples. Although extensive research
has investigated the transferability of Vision Transformers (ViTs) to various
downstream tasks under diverse constraints, this study shifts focus to explore
the transfer learning potential of [V]-Mamba. We compare its performance with
ViTs across different few-shot data budgets and efficient transfer methods. Our
analysis yields three key insights into [V]-Mamba's few-shot transfer
performance: (a) [V]-Mamba demonstrates superior or equivalent few-shot
learning capabilities compared to ViTs when utilizing linear probing (LP) for
transfer, (b) Conversely, [V]-Mamba exhibits weaker or similar few-shot
learning performance compared to ViTs when employing visual prompting (VP) as
the transfer method, and (c) We observe a weak positive correlation between the
performance gap in transfer via LP and VP and the scale of the [V]-Mamba model.
This preliminary analysis lays the foundation for more comprehensive studies
aimed at furthering our understanding of the capabilities of [V]-Mamba variants
and their distinctions from ViTs.</div><div><a href='http://arxiv.org/abs/2403.10696v1'>2403.10696v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00534v1")'>A Manifold Representation of the Key in Vision Transformers</div>
<div id='2402.00534v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T12:01:43Z</div><div>Authors: Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad</div><div style='padding-top: 10px; width: 80ex'>Vision Transformers implement multi-head self-attention (MSA) via stacking
multiple attention blocks. The query, key, and value are often intertwined and
generated within those blocks via a single, shared linear transformation. This
paper explores the concept of disentangling the key from the query and value,
and adopting a manifold representation for the key. Our experiments reveal that
decoupling and endowing the key with a manifold structure can enhance the model
performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy,
while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K
dataset, with eight charts in the manifold key. Our approach also yields
positive results in object detection and instance segmentation tasks on the
COCO dataset. Through detailed ablation studies, we establish that these
performance gains are not merely due to the simplicity of adding more
parameters and computations. Future research may investigate strategies for
cutting the budget of such representations and aim for further performance
improvements based on our findings.</div><div><a href='http://arxiv.org/abs/2402.00534v1'>2402.00534v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15293v1")'>SkipViT: Speeding Up Vision Transformers with a Token-Level Skip
  Connection</div>
<div id='2401.15293v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T04:24:49Z</div><div>Authors: Foozhan Ataiefard, Walid Ahmed, Habib Hajimolahoseini, Saina Asani, Farnoosh Javadi, Mohammad Hassanpour, Omar Mohamed Awad, Austin Wen, Kangling Liu, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>Vision transformers are known to be more computationally and data-intensive
than CNN models. These transformer models such as ViT, require all the input
image tokens to learn the relationship among them. However, many of these
tokens are not informative and may contain irrelevant information such as
unrelated background or unimportant scenery. These tokens are overlooked by the
multi-head self-attention (MHSA), resulting in many redundant and unnecessary
computations in MHSA and the feed-forward network (FFN). In this work, we
propose a method to optimize the amount of unnecessary interactions between
unimportant tokens by separating and sending them through a different low-cost
computational path. Our method does not add any parameters to the ViT model and
aims to find the best trade-off between training throughput and achieving a 0%
loss in the Top-1 accuracy of the final model. Our experimental results on
training ViT-small from scratch show that SkipViT is capable of effectively
dropping 55% of the tokens while gaining more than 13% training throughput and
maintaining classification accuracy at the level of the baseline model on
Huawei Ascend910A.</div><div><a href='http://arxiv.org/abs/2401.15293v1'>2401.15293v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15004v1")'>ParFormer: Vision Transformer Baseline with Parallel Local Global Token
  Mixer and Convolution Attention Patch Embedding</div>
<div id='2403.15004v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T07:32:21Z</div><div>Authors: Novendra Setyawan, Ghufron Wahyu Kurniawan, Chi-Chia Sun, Jun-Wei Hsieh, Hui-Kai Su, Wen-Kai Kuo</div><div style='padding-top: 10px; width: 80ex'>This work presents ParFormer as an enhanced transformer architecture that
allows the incorporation of different token mixers into a single stage, hence
improving feature extraction capabilities. Integrating both local and global
data allows for precise representation of short- and long-range spatial
relationships without the need for computationally intensive methods such as
shifting windows. Along with the parallel token mixer encoder, We offer the
Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard
patch embedding to improve token mixer extraction with a convolutional
attention module. Our comprehensive evaluation demonstrates that our ParFormer
outperforms CNN-based and state-of-the-art transformer-based architectures in
image classification and several complex tasks such as object recognition. The
proposed CAPE has been demonstrated to benefit the overall MetaFormer
architecture, even while utilizing the Identity Mapping Token Mixer, resulting
in a 0.5\% increase in accuracy. The ParFormer models outperformed ConvNeXt and
Swin Transformer for the pure convolution and transformer model in accuracy.
Furthermore, our model surpasses the current leading hybrid transformer by
reaching competitive Top-1 scores in the ImageNet-1K classification test.
Specifically, our model variants with 11M, 23M, and 34M parameters achieve
scores of 80.4\%, 82.1\%, and 83.1\%, respectively. Code:
https://github.com/novendrastywn/ParFormer-CAPE-2024</div><div><a href='http://arxiv.org/abs/2403.15004v1'>2403.15004v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02411v1")'>NiNformer: A Network in Network Transformer with Token Mixing Generated
  Gating Function</div>
<div id='2403.02411v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:08:20Z</div><div>Authors: Abdullah Nazhat Abdullah, Tarkan Aydin</div><div style='padding-top: 10px; width: 80ex'>The Attention mechanism is the main component of the Transformer
architecture, and since its introduction, it has led to significant
advancements in Deep Learning that span many domains and multiple tasks. The
Attention Mechanism was utilized in Computer Vision as the Vision Transformer
ViT, and its usage has expanded into many tasks in the vision domain, such as
classification, segmentation, object detection, and image generation. While
this mechanism is very expressive and capable, it comes with the drawback of
being computationally expensive and requiring datasets of considerable size for
effective optimization. To address these shortcomings, many designs have been
proposed in the literature to reduce the computational burden and alleviate the
data size requirements. Examples of such attempts in the vision domain are the
MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper
introduces a new computational block as an alternative to the standard ViT
block that reduces the compute burdens by replacing the normal Attention layers
with a Network in Network structure that enhances the static approach of the
MLP Mixer with a dynamic system of learning an element-wise gating function by
a token mixing process. Extensive experimentation shows that the proposed
design provides better performance than the baseline architectures on multiple
datasets applied in the image classification task of the vision domain.</div><div><a href='http://arxiv.org/abs/2403.02411v1'>2403.02411v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13298v1")'>Rotary Position Embedding for Vision Transformer</div>
<div id='2403.13298v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T04:47:13Z</div><div>Authors: Byeongho Heo, Song Park, Dongyoon Han, Sangdoo Yun</div><div style='padding-top: 10px; width: 80ex'>Rotary Position Embedding (RoPE) performs remarkably on language models,
especially for length extrapolation of Transformers. However, the impacts of
RoPE on computer vision domains have been underexplored, even though RoPE
appears capable of enhancing Vision Transformer (ViT) performance in a way
similar to the language domain. This study provides a comprehensive analysis of
RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D
vision data. The analysis reveals that RoPE demonstrates impressive
extrapolation performance, i.e., maintaining precision while increasing image
resolution at inference. It eventually leads to performance improvement for
ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study
provides thorough guidelines to apply RoPE into ViT, promising improved
backbone performance with minimal extra computational overhead. Our code and
pre-trained models are available at https://github.com/naver-ai/rope-vit</div><div><a href='http://arxiv.org/abs/2403.13298v1'>2403.13298v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03251v1")'>CLIP Can Understand Depth</div>
<div id='2402.03251v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:09:33Z</div><div>Authors: Dunam Kim, Seokju Lee</div><div style='padding-top: 10px; width: 80ex'>Recent studies on generalizing CLIP for monocular depth estimation reveal
that CLIP pre-trained on web-crawled data is inefficient for deriving proper
similarities between image patches and depth-related prompts. In this paper, we
adapt CLIP for meaningful quality of monocular depth estimation with dense
prediction, without fine-tuning its original vision-language alignment. By
jointly training a compact deconvolutional decoder with a tiny learnable
embedding matrix named mirror, as a static prompt for its text encoder, CLIP is
enabled to understand depth. With this approach, our model exhibits impressive
performance matching several previous state-of-the-art vision-only models on
the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth
estimation model with a large margin. Experiments on temporal depth consistency
and spatial continuity demonstrate that the prior knowledge of CLIP can be
effectively refined by our proposed framework. Furthermore, an ablation study
on mirror proves that the resulting model estimates depth utilizing knowledge
not only from the image encoder but also text encoder despite not being given
any prompt written in a human way. This research demonstrates that through
minimal adjustments, the prior knowledge of vision-language foundation models,
such as CLIP, can be generalized even to domains where learning during
pretraining is challenging. We facilitate future works focused on methods to
adjust suboptimal prior knowledge of vision-language models using non-human
language prompts, achieving performance on par with task-specific
state-of-the-art methodologies.</div><div><a href='http://arxiv.org/abs/2402.03251v1'>2402.03251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02377v1")'>NOAH: Learning Pairwise Object Category Attentions for Image
  Classification</div>
<div id='2402.02377v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T07:19:40Z</div><div>Authors: Chao Li, Aojun Zhou, Anbang Yao</div><div style='padding-top: 10px; width: 80ex'>A modern deep neural network (DNN) for image classification tasks typically
consists of two parts: a backbone for feature extraction, and a head for
feature encoding and class predication. We observe that the head structures of
mainstream DNNs adopt a similar feature encoding pipeline, exploiting global
feature dependencies while disregarding local ones. In this paper, we revisit
the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that
relies on a new form of dot-product attention called pairwise object category
attention (POCA), efficiently exploiting spatially dense category-specific
attentions to augment classification performance. NOAH introduces a neat
combination of feature split, transform and merge operations to learn POCAs at
local to global scales. As a drop-in design, NOAH can be easily used to replace
existing heads of various types of DNNs, improving classification performance
while maintaining similar model efficiency. We validate the effectiveness of
NOAH on ImageNet classification benchmark with 25 DNN architectures spanning
convolutional neural networks, vision transformers and multi-layer perceptrons.
In general, NOAH is able to significantly improve the performance of
lightweight DNNs, e.g., showing 3.14\%|5.3\%|1.9\% top-1 accuracy improvement
to MobileNetV2 (0.5x)|Deit-Tiny (0.5x)|gMLP-Tiny (0.5x). NOAH also generalizes
well when applied to medium-size and large-size DNNs. We further show that NOAH
retains its efficacy on other popular multi-class and multi-label image
classification benchmarks as well as in different training regimes, e.g.,
showing 3.6\%|1.1\% mAP improvement to large ResNet101|ViT-Large on MS-COCO
dataset. Project page: https://github.com/OSVAI/NOAH.</div><div><a href='http://arxiv.org/abs/2402.02377v1'>2402.02377v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12109v1")'>GCAM: Gaussian and causal-attention model of food fine-grained
  recognition</div>
<div id='2403.12109v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T03:39:54Z</div><div>Authors: Guohang Zhuang, Yue Hu, Tianxing Yan, JiaZhan Gao</div><div style='padding-top: 10px; width: 80ex'>Currently, most food recognition relies on deep learning for category
classification. However, these approaches struggle to effectively distinguish
between visually similar food samples, highlighting the pressing need to
address fine-grained issues in food recognition. To mitigate these challenges,
we propose the adoption of a Gaussian and causal-attention model for
fine-grained object recognition.In particular, we train to obtain Gaussian
features over target regions, followed by the extraction of fine-grained
features from the objects, thereby enhancing the feature mapping capabilities
of the target regions. To counteract data drift resulting from uneven data
distributions, we employ a counterfactual reasoning approach. By using
counterfactual interventions, we analyze the impact of the learned image
attention mechanism on network predictions, enabling the network to acquire
more useful attention weights for fine-grained image recognition. Finally, we
design a learnable loss strategy to balance training stability across various
modules, ultimately improving the accuracy of the final target recognition. We
validate our approach on four relevant datasets, demonstrating its excellent
performance across these four datasets.We experimentally show that GCAM
surpasses state-of-the-art methods on the ETH-FOOD101, UECFOOD256, and
Vireo-FOOD172 datasets. Furthermore, our approach also achieves
state-of-the-art performance on the CUB-200 dataset.</div><div><a href='http://arxiv.org/abs/2403.12109v1'>2403.12109v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14270v1")'>Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship
  Detection</div>
<div id='2403.14270v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T10:15:57Z</div><div>Authors: Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer</div><div style='padding-top: 10px; width: 80ex'>Visual relationship detection aims to identify objects and their
relationships in images. Prior methods approach this task by adding separate
relationship modules or decoders to existing object detection architectures.
This separation increases complexity and hinders end-to-end training, which
limits performance. We propose a simple and highly efficient decoder-free
architecture for open-vocabulary visual relationship detection. Our model
consists of a Transformer-based image encoder that represents objects as tokens
and models their relationships implicitly. To extract relationship information,
we introduce an attention mechanism that selects object pairs likely to form a
relationship. We provide a single-stage recipe to train this model on a mixture
of object and relationship detection data. Our approach achieves
state-of-the-art relationship detection performance on Visual Genome and on the
large-vocabulary GQA benchmark at real-time inference speeds. We provide
analyses of zero-shot performance, ablations, and real-world qualitative
examples.</div><div><a href='http://arxiv.org/abs/2403.14270v1'>2403.14270v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03994v1")'>Video Relationship Detection Using Mixture of Experts</div>
<div id='2403.03994v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T19:08:34Z</div><div>Authors: Ala Shaabana, Zahra Gharaee, Paul Fieguth</div><div style='padding-top: 10px; width: 80ex'>Machine comprehension of visual information from images and videos by neural
networks faces two primary challenges. Firstly, there exists a computational
and inference gap in connecting vision and language, making it difficult to
accurately determine which object a given agent acts on and represent it
through language. Secondly, classifiers trained by a single, monolithic neural
network often lack stability and generalization. To overcome these challenges,
we introduce MoE-VRD, a novel approach to visual relationship detection
utilizing a mixture of experts. MoE-VRD identifies language triplets in the
form of &lt; subject, predicate, object&gt; tuples to extract relationships from
visual processing. Leveraging recent advancements in visual relationship
detection, MoE-VRD addresses the requirement for action recognition in
establishing relationships between subjects (acting) and objects (being acted
upon). In contrast to single monolithic networks, MoE-VRD employs multiple
small models as experts, whose outputs are aggregated. Each expert in MoE-VRD
specializes in visual relationship learning and object tagging. By utilizing a
sparsely-gated mixture of experts, MoE-VRD enables conditional computation and
significantly enhances neural network capacity without increasing computational
complexity. Our experimental results demonstrate that the conditional
computation capabilities and scalability of the mixture-of-experts approach
lead to superior performance in visual relationship detection compared to
state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.03994v1'>2403.03994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10831v1")'>Understanding Video Transformers via Universal Concept Discovery</div>
<div id='2401.10831v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T17:27:21Z</div><div>Authors: Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov</div><div style='padding-top: 10px; width: 80ex'>This paper studies the problem of concept-based interpretability of
transformer representations for videos. Concretely, we seek to explain the
decision-making process of video transformers based on high-level,
spatiotemporal concepts that are automatically discovered. Prior research on
concept-based interpretability has concentrated solely on image-level tasks.
Comparatively, video models deal with the added temporal dimension, increasing
complexity and posing challenges in identifying dynamic concepts over time. In
this work, we systematically address these challenges by introducing the first
Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose
an efficient approach for unsupervised identification of units of video
transformer representations - concepts, and ranking their importance to the
output of a model. The resulting concepts are highly interpretable, revealing
spatio-temporal reasoning mechanisms and object-centric representations in
unstructured video models. Performing this analysis jointly over a diverse set
of supervised and self-supervised representations, we discover that some of
these mechanism are universal in video transformers. Finally, we demonstrate
that VTCDcan be used to improve model performance for fine-grained tasks.</div><div><a href='http://arxiv.org/abs/2401.10831v1'>2401.10831v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00272v1")'>Dual Pose-invariant Embeddings: Learning Category and Object-specific
  Discriminative Representations for Recognition and Retrieval</div>
<div id='2403.00272v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T04:20:13Z</div><div>Authors: Rohan Sarkar, Avinash Kak</div><div style='padding-top: 10px; width: 80ex'>In the context of pose-invariant object recognition and retrieval, we
demonstrate that it is possible to achieve significant improvements in
performance if both the category-based and the object-identity-based embeddings
are learned simultaneously during training. In hindsight, that sounds intuitive
because learning about the categories is more fundamental than learning about
the individual objects that correspond to those categories. However, to the
best of what we know, no prior work in pose-invariant learning has demonstrated
this effect. This paper presents an attention-based dual-encoder architecture
with specially designed loss functions that optimize the inter- and intra-class
distances simultaneously in two different embedding spaces, one for the
category embeddings and the other for the object-level embeddings. The loss
functions we have proposed are pose-invariant ranking losses that are designed
to minimize the intra-class distances and maximize the inter-class distances in
the dual representation spaces. We demonstrate the power of our approach with
three challenging multi-view datasets, ModelNet-40, ObjectPI, and FG3D. With
our dual approach, for single-view object recognition, we outperform the
previous best by 20.0% on ModelNet40, 2.0% on ObjectPI, and 46.5% on FG3D. On
the other hand, for single-view object retrieval, we outperform the previous
best by 33.7% on ModelNet40, 18.8% on ObjectPI, and 56.9% on FG3D.</div><div><a href='http://arxiv.org/abs/2403.00272v1'>2403.00272v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02340v2")'>Learning Semantic Proxies from Visual Prompts for Parameter-Efficient
  Fine-Tuning in Deep Metric Learning</div>
<div id='2402.02340v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T04:42:05Z</div><div>Authors: Li Ren, Chen Chen, Liqiang Wang, Kien Hua</div><div style='padding-top: 10px; width: 80ex'>Deep Metric Learning (DML) has long attracted the attention of the machine
learning community as a key objective. Existing solutions concentrate on
fine-tuning the pre-trained models on conventional image datasets. As a result
of the success of recent pre-trained models trained from larger-scale datasets,
it is challenging to adapt the model to the DML tasks in the local data domain
while retaining the previously gained knowledge. In this paper, we investigate
parameter-efficient methods for fine-tuning the pre-trained model for DML
tasks. In particular, we propose a novel and effective framework based on
learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT).
Based on the conventional proxy-based DML paradigm, we augment the proxy by
incorporating the semantic information from the input image and the ViT, in
which we optimize the visual prompts for each class. We demonstrate that our
new approximations with semantic information are superior to representative
capabilities, thereby improving metric learning performance. We conduct
extensive experiments to demonstrate that our proposed framework is effective
and efficient by evaluating popular DML benchmarks. In particular, we
demonstrate that our fine-tuning method achieves comparable or even better
performance than recent state-of-the-art full fine-tuning works of DML while
tuning only a small percentage of total parameters.</div><div><a href='http://arxiv.org/abs/2402.02340v2'>2402.02340v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09976v1")'>AD3: Implicit Action is the Key for World Models to Distinguish the
  Diverse Visual Distractors</div>
<div id='2403.09976v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T02:46:19Z</div><div>Authors: Yucen Wang, Shenghua Wan, Le Gan, Shuai Feng, De-Chuan Zhan</div><div style='padding-top: 10px; width: 80ex'>Model-based methods have significantly contributed to distinguishing
task-irrelevant distractors for visual control. However, prior research has
primarily focused on heterogeneous distractors like noisy background videos,
leaving homogeneous distractors that closely resemble controllable agents
largely unexplored, which poses significant challenges to existing methods. To
tackle this problem, we propose Implicit Action Generator (IAG) to learn the
implicit actions of visual distractors, and present a new algorithm named
implicit Action-informed Diverse visual Distractors Distinguisher (AD3), that
leverages the action inferred by IAG to train separated world models. Implicit
actions effectively capture the behavior of background distractors, aiding in
distinguishing the task-irrelevant components, and the agent can optimize the
policy within the task-relevant state space. Our method achieves superior
performance on various visual control tasks featuring both heterogeneous and
homogeneous distractors. The indispensable role of implicit actions learned by
IAG is also empirically validated.</div><div><a href='http://arxiv.org/abs/2403.09976v1'>2403.09976v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00092v1")'>Episodic-free Task Selection for Few-shot Learning</div>
<div id='2402.00092v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T10:52:15Z</div><div>Authors: Tao Zhang</div><div style='padding-top: 10px; width: 80ex'>Episodic training is a mainstream training strategy for few-shot learning. In
few-shot scenarios, however, this strategy is often inferior to some
non-episodic training strategy, e. g., Neighbourhood Component Analysis (NCA),
which challenges the principle that training conditions must match testing
conditions. Thus, a question is naturally asked: How to search for
episodic-free tasks for better few-shot learning? In this work, we propose a
novel meta-training framework beyond episodic training. In this framework,
episodic tasks are not used directly for training, but for evaluating the
effectiveness of some selected episodic-free tasks from a task set that are
performed for training the meta-learners. The selection criterion is designed
with the affinity, which measures the degree to which loss decreases when
executing the target tasks after training with the selected tasks. In
experiments, the training task set contains some promising types, e. g.,
contrastive learning and classification, and the target few-shot tasks are
achieved with the nearest centroid classifiers on the miniImageNet,
tiered-ImageNet and CIFAR-FS datasets. The experimental results demonstrate the
effectiveness of our approach.</div><div><a href='http://arxiv.org/abs/2402.00092v1'>2402.00092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03038v1")'>Automatic Combination of Sample Selection Strategies for Few-Shot
  Learning</div>
<div id='2402.03038v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:23:43Z</div><div>Authors: Branislav Pecher, Ivan Srba, Maria Bielikova, Joaquin Vanschoren</div><div style='padding-top: 10px; width: 80ex'>In few-shot learning, such as meta-learning, few-shot fine-tuning or
in-context learning, the limited number of samples used to train a model have a
significant impact on the overall success. Although a large number of sample
selection strategies exist, their impact on the performance of few-shot
learning is not extensively known, as most of them have been so far evaluated
in typical supervised settings only. In this paper, we thoroughly investigate
the impact of 20 sample selection strategies on the performance of 5 few-shot
learning approaches over 8 image and 6 text datasets. In addition, we propose a
new method for automatic combination of sample selection strategies (ACSESS)
that leverages the strengths and complementary information of the individual
strategies. The experimental results show that our method consistently
outperforms the individual selection strategies, as well as the recently
proposed method for selecting support examples for in-context learning. We also
show a strong modality, dataset and approach dependence for the majority of
strategies as well as their dependence on the number of shots - demonstrating
that the sample selection strategies play a significant role for lower number
of shots, but regresses to random selection at higher number of shots.</div><div><a href='http://arxiv.org/abs/2402.03038v1'>2402.03038v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06469v2")'>Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning</div>
<div id='2401.06469v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T09:31:17Z</div><div>Authors: Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, Rui Yan</div><div style='padding-top: 10px; width: 80ex'>In this paper, by treating in-context learning (ICL) as a meta-optimization
process, we explain why LLMs are sensitive to the order of ICL examples. This
understanding leads us to the development of Batch-ICL, an effective,
efficient, and order-agnostic inference algorithm for ICL. Differing from the
standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot
forward computations and aggregates the resulting meta-gradients. These
aggregated meta-gradients are then applied to the forward computation of a
zero-shot query to generate the final prediction. This batch processing
approach renders the LLM agnostic to the order of ICL examples. Through
extensive experiments and analysis, we demonstrate that Batch-ICL consistently
outperforms most permutations of ICL examples. In some cases, it even exceeds
the performance of the best order for standard ICL, all while reducing the
computational resources required. Furthermore, we develop a novel variant of
Batch-ICL featuring multiple "epochs" of meta-optimization. This variant
implicitly explores permutations of ICL examples, further enhancing ICL
performance.</div><div><a href='http://arxiv.org/abs/2401.06469v2'>2401.06469v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01264v1")'>Direct side information learning for zero-shot regression</div>
<div id='2402.01264v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:36:06Z</div><div>Authors: Miriam Fdez-Díaz, Elena Montañés, José Ramón Quevedo</div><div style='padding-top: 10px; width: 80ex'>Zero-shot learning provides models for targets for which instances are not
available, commonly called unobserved targets. The availability of target side
information becomes crucial in this context in order to properly induce models
for these targets. The literature is plenty of strategies to cope with this
scenario, but specifically designed on the basis of a zero-shot classification
scenario, mostly in computer vision and image classification, but they are
either not applicable or easily extensible for a zero-shot regression framework
for which a continuos value is required to be predicted rather than a label. In
fact, there is a considerable lack of methods for zero-shot regression in the
literature. Two approaches for zero-shot regression that work in a two-phase
procedure were recently proposed. They first learn the observed target models
through a classical regression learning ignoring the target side information.
Then, they aggregate those observed target models afterwards exploiting the
target side information and the models for the unobserved targets are induced.
Despite both have shown quite good performance because of the different
treatment they grant to the common features and to the side information, they
exploit features and side information separately, avoiding a global
optimization for providing the unobserved target models. The proposal of this
paper is a novel method that jointly takes features and side information in a
one-phase learning process, but treating side information properly and in a
more deserving way than as common features. A specific kernel that properly
merges features and side information is proposed for this purpose resulting in
a novel approach that exhibits better performance over both artificial and real
datasets.</div><div><a href='http://arxiv.org/abs/2402.01264v1'>2402.01264v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02444v1")'>BECLR: Batch Enhanced Contrastive Few-Shot Learning</div>
<div id='2402.02444v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:52:43Z</div><div>Authors: Stylianos Poulakakis-Daktylidis, Hadi Jamali-Rad</div><div style='padding-top: 10px; width: 80ex'>Learning quickly from very few labeled samples is a fundamental attribute
that separates machines and humans in the era of deep representation learning.
Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding
the reliance on annotations at training time. Intrigued by the success of
contrastive learning approaches in the realm of U-FSL, we structurally approach
their shortcomings in both pretraining and downstream inference stages. We
propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly
separable latent representation space for enhancing positive sampling at the
pretraining phase and infusing implicit class-level insights into unsupervised
contrastive learning. We then tackle the, somehow overlooked yet critical,
issue of sample bias at the few-shot inference stage. We propose an iterative
Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate
that it efficiently addresses the problem, especially in low-shot scenarios
where FSL approaches suffer the most from sample bias. We later on discuss that
DyCE and OpTA are two intertwined pieces of a novel end-to-end approach (we
coin as BECLR), constructively magnifying each other's impact. We then present
a suite of extensive quantitative and qualitative experimentation to
corroborate that BECLR sets a new state-of-the-art across ALL existing U-FSL
benchmarks (to the best of our knowledge), and significantly outperforms the
best of the current baselines (codebase available at:
https://github.com/stypoumic/BECLR).</div><div><a href='http://arxiv.org/abs/2402.02444v1'>2402.02444v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13987v1")'>Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</div>
<div id='2401.13987v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T07:05:42Z</div><div>Authors: Naeem Paeedeh, Mahardhika Pratama, Muhammad Anwar Ma'sum, Wolfgang Mayer, Zehong Cao, Ryszard Kowlczyk</div><div style='padding-top: 10px; width: 80ex'>Most few-shot learning works rely on the same domain assumption between the
base and the target tasks, hindering their practical applications. This paper
proposes an adaptive transformer network (ADAPTER), a simple but effective
solution for cross-domain few-shot learning where there exist large domain
shifts between the base task and the target task. ADAPTER is built upon the
idea of bidirectional cross-attention to learn transferable features between
the two domains. The proposed architecture is trained with DINO to produce
diverse, and less biased features to avoid the supervision collapse problem.
Furthermore, the label smoothing approach is proposed to improve the
consistency and reliability of the predictions by also considering the
predicted labels of the close samples in the embedding space. The performance
of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it
outperforms prior arts with significant margins.</div><div><a href='http://arxiv.org/abs/2401.13987v1'>2401.13987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18599v1")'>Meta-Tasks: An alternative view on Meta-Learning Regularization</div>
<div id='2402.18599v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:15:40Z</div><div>Authors: Mohammad Rostami, Atik Faysal, Huaxia Wang, Avimanyu Sahoo, Ryan Antle</div><div style='padding-top: 10px; width: 80ex'>Few-shot learning (FSL) is a challenging machine learning problem due to a
scarcity of labeled data. The ability to generalize effectively on both novel
and training tasks is a significant barrier to FSL. This paper proposes a novel
solution that can generalize to both training and novel tasks while also
utilizing unlabeled samples. The method refines the embedding model before
updating the outer loop using unsupervised techniques as ``meta-tasks''. The
experimental results show that our proposed method performs well on novel and
training tasks, with faster and better convergence, lower generalization, and
standard deviation error, indicating its potential for practical applications
in FSL. The experimental results show that the proposed method outperforms
prototypical networks by 3.9%.</div><div><a href='http://arxiv.org/abs/2402.18599v1'>2402.18599v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18605v1")'>FORML: A Riemannian Hessian-free Method for Meta-learning with
  Orthogonality Constraint</div>
<div id='2402.18605v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T10:57:30Z</div><div>Authors: Hadi Tabealhojeh, Soumava Kumar Roy, Peyman Adibi, Hossein Karshenas</div><div style='padding-top: 10px; width: 80ex'>Meta-learning problem is usually formulated as a bi-level optimization in
which the task-specific and the meta-parameters are updated in the inner and
outer loops of optimization, respectively. However, performing the optimization
in the Riemannian space, where the parameters and meta-parameters are located
on Riemannian manifolds is computationally intensive. Unlike the Euclidean
methods, the Riemannian backpropagation needs computing the second-order
derivatives that include backward computations through the Riemannian operators
such as retraction and orthogonal projection. This paper introduces a
Hessian-free approach that uses a first-order approximation of derivatives on
the Stiefel manifold. Our method significantly reduces the computational load
and memory footprint. We show how using a Stiefel fully-connected layer that
enforces orthogonality constraint on the parameters of the last classification
layer as the head of the backbone network, strengthens the representation reuse
of the gradient-based meta-learning methods. Our experimental results across
various few-shot learning datasets, demonstrate the superiority of our proposed
method compared to the state-of-the-art methods, especially MAML, its Euclidean
counterpart.</div><div><a href='http://arxiv.org/abs/2402.18605v1'>2402.18605v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12486v1")'>NTK-Guided Few-Shot Class Incremental Learning</div>
<div id='2403.12486v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T06:43:46Z</div><div>Authors: Jingren Liu, Zhong Ji, Yanwei Pang, YunLong Yu</div><div style='padding-top: 10px; width: 80ex'>While anti-amnesia FSCIL learners often excel in incremental sessions, they
tend to prioritize mitigating knowledge attrition over harnessing the model's
potential for knowledge acquisition. In this paper, we delve into the
foundations of model generalization in FSCIL through the lens of the Neural
Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal
NTK convergence and NTK-related generalization error, serving as the
theoretical bedrock for exceptional generalization. To attain globally optimal
NTK convergence, we employ a meta-learning mechanism grounded in mathematical
principles to guide the optimization process within an expanded network.
Furthermore, to reduce the NTK-related generalization error, we commence from
the foundational level, optimizing the relevant factors constituting its
generalization loss. Specifically, we initiate self-supervised pre-training on
the base session to shape the initial network weights. Then they are carefully
refined through curricular alignment, followed by the application of dual NTK
regularization tailored specifically for both convolutional and linear layers.
Through the combined effects of these measures, our network acquires robust NTK
properties, significantly enhancing its foundational generalization. On popular
FSCIL benchmark datasets, our NTK-FSCIL surpasses contemporary state-of-the-art
approaches, elevating end-session accuracy by 2.9% to 8.7%.</div><div><a href='http://arxiv.org/abs/2403.12486v1'>2403.12486v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03292v2")'>Zero-shot Object-Level OOD Detection with Context-Aware Inpainting</div>
<div id='2402.03292v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:50:27Z</div><div>Authors: Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian Q. Weinberger, Dung D. Le</div><div style='padding-top: 10px; width: 80ex'>Machine learning algorithms are increasingly provided as black-box cloud
services or pre-trained models, without access to their training data. This
motivates the problem of zero-shot out-of-distribution (OOD) detection.
Concretely, we aim to detect OOD objects that do not belong to the classifier's
label set but are erroneously classified as in-distribution (ID) objects. Our
approach, RONIN, uses an off-the-shelf diffusion model to replace detected
objects with inpainting. RONIN conditions the inpainting process with the
predicted ID label, drawing the input object closer to the in-distribution
domain. As a result, the reconstructed object is very close to the original in
the ID cases and far in the OOD cases, allowing RONIN to effectively
distinguish ID and OOD samples. Throughout extensive experiments, we
demonstrate that RONIN achieves competitive results compared to previous
approaches across several datasets, both in zero-shot and non-zero-shot
settings.</div><div><a href='http://arxiv.org/abs/2402.03292v2'>2402.03292v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03472v1")'>Boosting Meta-Training with Base Class Information for Few-Shot Learning</div>
<div id='2403.03472v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T05:13:23Z</div><div>Authors: Weihao Jiang, Guodong Liu, Di He, Kun He</div><div style='padding-top: 10px; width: 80ex'>Few-shot learning, a challenging task in machine learning, aims to learn a
classifier adaptable to recognize new, unseen classes with limited labeled
examples. Meta-learning has emerged as a prominent framework for few-shot
learning. Its training framework is originally a task-level learning method,
such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a
recently proposed training paradigm called Meta-Baseline, which consists of
sequential pre-training and meta-training stages, gains state-of-the-art
performance. However, as a non-end-to-end training method, indicating the
meta-training stage can only begin after the completion of pre-training,
Meta-Baseline suffers from higher training cost and suboptimal performance due
to the inherent conflicts of the two training stages. To address these
limitations, we propose an end-to-end training paradigm consisting of two
alternative loops. In the outer loop, we calculate cross entropy loss on the
entire training set while updating only the final linear layer. In the inner
loop, we employ the original meta-learning training mode to calculate the loss
and incorporate gradients from the outer loss to guide the parameter updates.
This training paradigm not only converges quickly but also outperforms existing
baselines, indicating that information from the overall training set and the
meta-learning training paradigm could mutually reinforce one another. Moreover,
being model-agnostic, our framework achieves significant performance gains,
surpassing the baseline systems by approximate 1%.</div><div><a href='http://arxiv.org/abs/2403.03472v1'>2403.03472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14392v1")'>A Bag of Tricks for Few-Shot Class-Incremental Learning</div>
<div id='2403.14392v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T13:33:00Z</div><div>Authors: Shuvendu Roy, Chunjong Park, Aldi Fahrezi, Ali Etemad</div><div style='padding-top: 10px; width: 80ex'>We present a bag of tricks framework for few-shot class-incremental learning
(FSCIL), which is a challenging form of continual learning that involves
continuous adaptation to new tasks with limited samples. FSCIL requires both
stability and adaptability, i.e., preserving proficiency in previously learned
tasks while learning new ones. Our proposed bag of tricks brings together eight
key and highly influential techniques that improve stability, adaptability, and
overall performance under a unified framework for FSCIL. We organize these
tricks into three categories: stability tricks, adaptability tricks, and
training tricks. Stability tricks aim to mitigate the forgetting of previously
learned classes by enhancing the separation between the embeddings of learned
classes and minimizing interference when learning new ones. On the other hand,
adaptability tricks focus on the effective learning of new classes. Finally,
training tricks improve the overall performance without compromising stability
or adaptability. We perform extensive experiments on three benchmark datasets,
CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed
framework. Our detailed analysis shows that our approach substantially improves
both stability and adaptability, establishing a new state-of-the-art by
outperforming prior works in the area. We believe our method provides a go-to
solution and establishes a robust baseline for future research in this area.</div><div><a href='http://arxiv.org/abs/2403.14392v1'>2403.14392v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01201v1")'>Few-Shot Class-Incremental Learning with Prior Knowledge</div>
<div id='2402.01201v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T08:05:35Z</div><div>Authors: Wenhao Jiang, Duo Li, Menghan Hu, Guangtao Zhai, Xiaokang Yang, Xiao-Ping Zhang</div><div style='padding-top: 10px; width: 80ex'>To tackle the issues of catastrophic forgetting and overfitting in few-shot
class-incremental learning (FSCIL), previous work has primarily concentrated on
preserving the memory of old knowledge during the incremental phase. The role
of pre-trained model in shaping the effectiveness of incremental learning is
frequently underestimated in these studies. Therefore, to enhance the
generalization ability of the pre-trained model, we propose Learning with Prior
Knowledge (LwPK) by introducing nearly free prior knowledge from a few
unlabeled data of subsequent incremental classes. We cluster unlabeled
incremental class samples to produce pseudo-labels, then jointly train these
with labeled base class samples, effectively allocating embedding space for
both old and new class data. Experimental results indicate that LwPK
effectively enhances the model resilience against catastrophic forgetting, with
theoretical analysis based on empirical risk minimization and class distance
measurement corroborating its operational principles. The source code of LwPK
is publicly available at: \url{https://github.com/StevenJ308/LwPK}.</div><div><a href='http://arxiv.org/abs/2402.01201v1'>2402.01201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09857v1")'>Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive
  Prompt</div>
<div id='2403.09857v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T20:34:53Z</div><div>Authors: Chenxi Liu, Zhenyi Wang, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang</div><div style='padding-top: 10px; width: 80ex'>Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn
new classes with scarce samples while preserving knowledge of old ones.
Existing FSCIL methods usually fine-tune the entire backbone, leading to
overfitting and hindering the potential to learn new classes. On the other
hand, recent prompt-based CIL approaches alleviate forgetting by training
prompts with sufficient data in each task. In this work, we propose a novel
framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages
task-invariant prompts to capture shared knowledge by reducing specific
information from the attention aspect. Additionally, self-adaptive
task-specific prompts in ASP provide specific information and transfer
knowledge from old classes to new classes with an Information Bottleneck
learning objective. In summary, ASP prevents overfitting on base task and does
not require enormous data in few-shot incremental tasks. Extensive experiments
on three benchmark datasets validate that ASP consistently outperforms
state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning
new classes and mitigating forgetting.</div><div><a href='http://arxiv.org/abs/2403.09857v1'>2403.09857v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13522v1")'>REAL: Representation Enhanced Analytic Learning for Exemplar-free
  Class-incremental Learning</div>
<div id='2403.13522v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T11:48:10Z</div><div>Authors: Run He, Huiping Zhuang, Di Fang, Yizhu Chen, Kai Tong, Cen Chen</div><div style='padding-top: 10px; width: 80ex'>Exemplar-free class-incremental learning (EFCIL) aims to mitigate
catastrophic forgetting in class-incremental learning without available
historical data. Compared with its counterpart (replay-based CIL) that stores
historical samples, the EFCIL suffers more from forgetting issues under the
exemplar-free constraint. In this paper, inspired by the recently developed
analytic learning (AL) based CIL, we propose a representation enhanced analytic
learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining
(DS-BPT) and a representation enhancing distillation (RED) process to enhance
the representation of the extractor. The DS-BPT pretrains model in streams of
both supervised learning and self-supervised contrastive learning (SSCL) for
base knowledge extraction. The RED process distills the supervised knowledge to
the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that
converts the CIL to a recursive least-square problem. Our method addresses the
issue of insufficient discriminability in representations of unseen data caused
by a frozen backbone in the existing AL-based CIL. Empirical results on various
datasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that
our REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or
even more superior performance compared with the replay-based methods.</div><div><a href='http://arxiv.org/abs/2403.13522v1'>2403.13522v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03535v1")'>Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and
  Applications</div>
<div id='2403.03535v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:29:45Z</div><div>Authors: Minyang Hu, Hong Chang, Zong Guo, Bingpeng Ma, Shiguan Shan, Xilin Chen</div><div style='padding-top: 10px; width: 80ex'>Few-shot learning (FSL) aims to learn novel tasks with very few labeled
samples by leveraging experience from \emph{related} training tasks. In this
paper, we try to understand FSL by delving into two key questions: (1) How to
quantify the relationship between \emph{training} and \emph{novel} tasks? (2)
How does the relationship affect the \emph{adaptation difficulty} on novel
tasks for different models? To answer the two questions, we introduce Task
Attribute Distance (TAD) built upon attributes as a metric to quantify the task
relatedness. Unlike many existing metrics, TAD is model-agnostic, making it
applicable to different FSL models. Then, we utilize TAD metric to establish a
theoretical connection between task relatedness and task adaptation difficulty.
By deriving the generalization error bound on a novel task, we discover how TAD
measures the adaptation difficulty on novel tasks for FSL models. To validate
our TAD metric and theoretical findings, we conduct experiments on three
benchmarks. Our experimental results confirm that TAD metric effectively
quantifies the task relatedness and reflects the adaptation difficulty on novel
tasks for various FSL methods, even if some of them do not learn attributes
explicitly or human-annotated attributes are not available. Finally, we present
two applications of the proposed TAD metric: data augmentation and test-time
intervention, which further verify its effectiveness and general applicability.
The source code is available at https://github.com/hu-my/TaskAttributeDistance.</div><div><a href='http://arxiv.org/abs/2403.03535v1'>2403.03535v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16876v1")'>Zero-shot Classification using Hyperdimensional Computing</div>
<div id='2401.16876v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T10:29:31Z</div><div>Authors: Samuele Ruffino, Geethan Karunaratne, Michael Hersche, Luca Benini, Abu Sebastian, Abbas Rahimi</div><div style='padding-top: 10px; width: 80ex'>Classification based on Zero-shot Learning (ZSL) is the ability of a model to
classify inputs into novel classes on which the model has not previously seen
any training examples. Providing an auxiliary descriptor in the form of a set
of attributes describing the new classes involved in the ZSL-based
classification is one of the favored approaches to solving this challenging
task. In this work, inspired by Hyperdimensional Computing (HDC), we propose
the use of stationary binary codebooks of symbol-like distributed
representations inside an attribute encoder to compactly represent a
computationally simple end-to-end trainable model, which we name
Hyperdimensional Computing Zero-shot Classifier~(HDC-ZSC). It consists of a
trainable image encoder, an attribute encoder based on HDC, and a similarity
kernel. We show that HDC-ZSC can be used to first perform zero-shot attribute
extraction tasks and, can later be repurposed for Zero-shot Classification
tasks with minimal architectural changes and minimal model retraining. HDC-ZSC
achieves Pareto optimal results with a 63.8% top-1 classification accuracy on
the CUB-200 dataset by having only 26.6 million trainable parameters. Compared
to two other state-of-the-art non-generative approaches, HDC-ZSC achieves 4.3%
and 9.9% better accuracy, while they require more than 1.85x and 1.72x
parameters compared to HDC-ZSC, respectively.</div><div><a href='http://arxiv.org/abs/2401.16876v1'>2401.16876v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09974v1")'>GET: Unlocking the Multi-modal Potential of CLIP for Generalized
  Category Discovery</div>
<div id='2403.09974v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T02:40:13Z</div><div>Authors: Enguang Wang, Zhimao Peng, Zhengyuan Xie, Xialei Liu, Ming-Ming Cheng</div><div style='padding-top: 10px; width: 80ex'>Given unlabelled datasets containing both old and new categories, generalized
category discovery (GCD) aims to accurately discover new classes while
correctly classifying old classes, leveraging the class concepts learned from
labeled samples. Current GCD methods only use a single visual modality of
information, resulting in poor classification of visually similar classes.
Though certain classes are visually confused, their text information might be
distinct, motivating us to introduce text information into the GCD task.
However, the lack of class names for unlabelled data makes it impractical to
utilize text information. To tackle this challenging problem, in this paper, we
propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings
for unlabelled samples. Specifically, our TES leverages the property that CLIP
can generate aligned vision-language features, converting visual embeddings
into tokens of the CLIP's text encoder to generate pseudo text embeddings.
Besides, we employ a dual-branch framework, through the joint learning and
instance consistency of different modality branches, visual and semantic
information mutually enhance each other, promoting the interaction and fusion
of visual and text embedding space. Our method unlocks the multi-modal
potentials of CLIP and outperforms the baseline methods by a large margin on
all GCD benchmarks, achieving new state-of-the-art. The code will be released
at \url{https://github.com/enguangW/GET}.</div><div><a href='http://arxiv.org/abs/2403.09974v1'>2403.09974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.15389v1")'>DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from
  Partially Annotated Data</div>
<div id='2403.15389v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:59:58Z</div><div>Authors: Hanrong Ye, Dan Xu</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been an increased interest in the practical problem of
learning multiple dense scene understanding tasks from partially annotated
data, where each training sample is only labeled for a subset of the tasks. The
missing of task labels in training leads to low-quality and noisy predictions,
as can be observed from state-of-the-art methods. To tackle this issue, we
reformulate the partially-labeled multi-task dense prediction as a pixel-level
denoising problem, and propose a novel multi-task denoising diffusion framework
coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to
model a potential noisy distribution in the task prediction or feature maps and
generate rectified outputs for different tasks. To exploit multi-task
consistency in denoising, we further introduce a Multi-Task Conditioning
strategy, which can implicitly utilize the complementary nature of the tasks to
help learn the unlabeled tasks, leading to an improvement in the denoising
performance of the different tasks. Extensive quantitative and qualitative
experiments demonstrate that the proposed multi-task denoising diffusion model
can significantly improve multi-task prediction maps, and outperform the
state-of-the-art methods on three challenging multi-task benchmarks, under two
different partial-labeling evaluation settings. The code is available at
https://prismformore.github.io/diffusionmtl/.</div><div><a href='http://arxiv.org/abs/2403.15389v1'>2403.15389v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12382v1")'>Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising</div>
<div id='2403.12382v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:47:33Z</div><div>Authors: Jintong Hu, Bin Xia, Bingchen Li, Wenming Yang</div><div style='padding-top: 10px; width: 80ex'>Deep learning-based denoiser has been the focus of recent development on
image denoising. In the past few years, there has been increasing interest in
developing self-supervised denoising networks that only require noisy images,
without the need for clean ground truth for training. However, a performance
gap remains between current self-supervised methods and their supervised
counterparts. Additionally, these methods commonly depend on assumptions about
noise characteristics, thereby constraining their applicability in real-world
scenarios. Inspired by the properties of the Frobenius norm expansion, we
discover that incorporating a trace term reduces the optimization goal
disparity between self-supervised and supervised methods, thereby enhancing the
performance of self-supervised learning. To exploit this insight, we propose a
trace-constraint loss function and design the low-trace adaptation Noise2Noise
(LoTA-N2N) model that bridges the gap between self-supervised and supervised
learning. Furthermore, we have discovered that several existing self-supervised
denoising frameworks naturally fall within the proposed trace-constraint loss
as subcases. Extensive experiments conducted on natural and confocal image
datasets indicate that our method achieves state-of-the-art performance within
the realm of zero-shot self-supervised image denoising approaches, without
relying on any assumptions regarding the noise.</div><div><a href='http://arxiv.org/abs/2403.12382v1'>2403.12382v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12340v1")'>Contrastive Learning and Cycle Consistency-based Transductive Transfer
  Learning for Target Annotation</div>
<div id='2401.12340v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T20:08:57Z</div><div>Authors: Shoaib Meraj Sami, Md Mahedi Hasan, Nasser M. Nasrabadi, Raghuveer Rao</div><div style='padding-top: 10px; width: 80ex'>Annotating automatic target recognition (ATR) is a highly challenging task,
primarily due to the unavailability of labeled data in the target domain.
Hence, it is essential to construct an optimal target domain classifier by
utilizing the labeled information of the source domain images. The transductive
transfer learning (TTL) method that incorporates a CycleGAN-based unpaired
domain translation network has been previously proposed in the literature for
effective ATR annotation. Although this method demonstrates great potential for
ATR, it severely suffers from lower annotation performance, higher Fr\'echet
Inception Distance (FID) score, and the presence of visual artifacts in the
synthetic images. To address these issues, we propose a hybrid contrastive
learning base unpaired domain translation (H-CUT) network that achieves a
significantly lower FID score. It incorporates both attention and entropy to
emphasize the domain-specific region, a noisy feature mixup module to generate
high variational synthetic negative patches, and a modulated noise contrastive
estimation (MoNCE) loss to reweight all negative patches using optimal
transport for better performance. Our proposed contrastive learning and
cycle-consistency-based TTL (C3TTL) framework consists of two H-CUT networks
and two classifiers. It simultaneously optimizes cycle-consistency, MoNCE, and
identity losses. In C3TTL, two H-CUT networks have been employed through a
bijection mapping to feed the reconstructed source domain images into a
pretrained classifier to guide the optimal target domain classifier. Extensive
experimental analysis conducted on three ATR datasets demonstrates that the
proposed C3TTL method is effective in annotating civilian and military
vehicles, as well as ship targets.</div><div><a href='http://arxiv.org/abs/2401.12340v1'>2401.12340v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11816v2")'>Learning the Unlearned: Mitigating Feature Suppression in Contrastive
  Learning</div>
<div id='2402.11816v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T04:13:33Z</div><div>Authors: Jihai Zhang, Xiang Lan, Xiaoye Qu, Yu Cheng, Mengling Feng, Bryan Hooi</div><div style='padding-top: 10px; width: 80ex'>Self-Supervised Contrastive Learning has proven effective in deriving
high-quality representations from unlabeled data. However, a major challenge
that hinders both unimodal and multimodal contrastive learning is feature
suppression, a phenomenon where the trained model captures only a limited
portion of the information from the input data while overlooking other
potentially valuable content. This issue often leads to indistinguishable
representations for visually similar but semantically different inputs,
adversely affecting downstream task performance, particularly those requiring
rigorous semantic comprehension. To address this challenge, we propose a novel
model-agnostic Multistage Contrastive Learning (MCL) framework. Unlike standard
contrastive learning which inherently captures one single biased feature
distribution, MCL progressively learns previously unlearned features through
feature-aware negative sampling at each stage, where the negative samples of an
anchor are exclusively selected from the cluster it was assigned to in
preceding stages. Meanwhile, MCL preserves the previously well-learned features
by cross-stage representation integration, integrating features across all
stages to form final representations. Our comprehensive evaluation demonstrates
MCL's effectiveness and superiority across both unimodal and multimodal
contrastive learning, spanning a range of model architectures from ResNet to
Vision Transformers (ViT). Remarkably, in tasks where the original CLIP model
has shown limitations, MCL dramatically enhances performance, with improvements
up to threefold on specific attributes in the recently proposed MMVP benchmark.</div><div><a href='http://arxiv.org/abs/2402.11816v2'>2402.11816v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06289v1")'>Understanding and Mitigating Human-Labelling Errors in Supervised
  Contrastive Learning</div>
<div id='2403.06289v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T19:05:12Z</div><div>Authors: Zijun Long, Lipeng Zhuang, George Killick, Richard McCreadie, Gerardo Aragon Camarasa, Paul Henderson</div><div style='padding-top: 10px; width: 80ex'>Human-annotated vision datasets inevitably contain a fraction of human
mislabelled examples. While the detrimental effects of such mislabelling on
supervised learning are well-researched, their influence on Supervised
Contrastive Learning (SCL) remains largely unexplored. In this paper, we show
that human-labelling errors not only differ significantly from synthetic label
errors, but also pose unique challenges in SCL, different to those in
traditional supervised learning methods. Specifically, our results indicate
they adversely impact the learning process in the ~99% of cases when they occur
as false positive samples. Existing noise-mitigating methods primarily focus on
synthetic label errors and tackle the unrealistic setting of very high
synthetic noise rates (40-80%), but they often underperform on common image
datasets due to overfitting. To address this issue, we introduce a novel SCL
objective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is
designed to mitigate the effects of real-world mislabelled examples, typically
characterized by much lower noise rates (&lt;5%). We demonstrate that SCL-RHE
consistently outperforms state-of-the-art representation learning and
noise-mitigating methods across various vision benchmarks, by offering improved
resilience against human-labelling errors.</div><div><a href='http://arxiv.org/abs/2403.06289v1'>2403.06289v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14555v1")'>Revisiting Active Learning in the Era of Vision Foundation Models</div>
<div id='2401.14555v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T22:50:39Z</div><div>Authors: Sanket Rajan Gupte, Josiah Aklilu, Jeffrey J. Nirschl, Serena Yeung-Levy</div><div style='padding-top: 10px; width: 80ex'>Foundation vision or vision-language models are trained on large unlabeled or
noisy data and learn robust representations that can achieve impressive zero-
or few-shot performance on diverse tasks. Given these properties, they are a
natural fit for active learning (AL), which aims to maximize labeling
efficiency, but the full potential of foundation models has not been explored
in the context of AL, specifically in the low-budget regime. In this work, we
evaluate how foundation models influence three critical components of effective
AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling,
and 3) the trade-off between representative and uncertainty sampling. We
systematically study how the robust representations of foundation models
(DINOv2, OpenCLIP) challenge existing findings in active learning. Our
observations inform the principled construction of a new simple and elegant AL
strategy that balances uncertainty estimated via dropout with sample diversity.
We extensively test our strategy on many challenging image classification
benchmarks, including natural images as well as out-of-domain biomedical images
that are relatively understudied in the AL literature. Source code will be made
available.</div><div><a href='http://arxiv.org/abs/2401.14555v1'>2401.14555v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02110v1")'>Composite Active Learning: Towards Multi-Domain Active Learning with
  Theoretical Guarantees</div>
<div id='2402.02110v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T10:22:18Z</div><div>Authors: Guang-Yuan Hao, Hengguan Huang, Haotian Wang, Jie Gao, Hao Wang</div><div style='padding-top: 10px; width: 80ex'>Active learning (AL) aims to improve model performance within a fixed
labeling budget by choosing the most informative data points to label. Existing
AL focuses on the single-domain setting, where all data come from the same
domain (e.g., the same dataset). However, many real-world tasks often involve
multiple domains. For example, in visual recognition, it is often desirable to
train an image classifier that works across different environments (e.g.,
different backgrounds), where images from each environment constitute one
domain. Such a multi-domain AL setting is challenging for prior methods because
they (1) ignore the similarity among different domains when assigning labeling
budget and (2) fail to handle distribution shift of data across different
domains. In this paper, we propose the first general method, dubbed composite
active learning (CAL), for multi-domain AL. Our approach explicitly considers
the domain-level and instance-level information in the problem; CAL first
assigns domain-level budgets according to domain-level importance, which is
estimated by optimizing an upper error bound that we develop; with the
domain-level budgets, CAL then leverages a certain instance-level query
strategy to select samples to label from each domain. Our theoretical analysis
shows that our method achieves a better error bound compared to current AL
methods. Our empirical results demonstrate that our approach significantly
outperforms the state-of-the-art AL methods on both synthetic and real-world
multi-domain datasets. Code is available at
https://github.com/Wang-ML-Lab/multi-domain-active-learning.</div><div><a href='http://arxiv.org/abs/2402.02110v1'>2402.02110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09918v1")'>Attention-based Class-Conditioned Alignment for Multi-Source Domain
  Adaptive Object Detection</div>
<div id='2403.09918v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T23:31:41Z</div><div>Authors: Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger</div><div style='padding-top: 10px; width: 80ex'>Domain adaptation methods for object detection (OD) strive to mitigate the
impact of distribution shifts by promoting feature alignment across source and
target domains. Multi-source domain adaptation (MSDA) allows leveraging
multiple annotated source datasets, and unlabeled target data to improve the
accuracy and robustness of the detection model. Most state-of-the-art MSDA
methods for OD perform feature alignment in a class-agnostic manner. This is
challenging since the objects have unique modal information due to variations
in object appearance across domains. A recent prototype-based approach proposed
a class-wise alignment, yet it suffers from error accumulation due to noisy
pseudo-labels which can negatively affect adaptation with imbalanced data. To
overcome these limitations, we propose an attention-based class-conditioned
alignment scheme for MSDA that aligns instances of each object category across
domains. In particular, an attention module coupled with an adversarial domain
classifier allows learning domain-invariant and class-specific instance
representations. Experimental results on multiple benchmarking MSDA datasets
indicate that our method outperforms the state-of-the-art methods and is robust
to class imbalance. Our code is available at https://github.com/imatif17/ACIA.</div><div><a href='http://arxiv.org/abs/2403.09918v1'>2403.09918v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03094v2")'>Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object
  Detector</div>
<div id='2402.03094v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:25:32Z</div><div>Authors: Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan, Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang</div><div style='padding-top: 10px; width: 80ex'>This paper studies the challenging cross-domain few-shot object detection
(CD-FSOD), aiming to develop an accurate object detector for novel domains with
minimal labeled examples. While transformer-based open-set detectors, such as
DE-ViT, show promise in traditional few-shot object detection, their
generalization to CD-FSOD remains unclear: 1) can such open-set detection
methods easily generalize to CD-FSOD? 2) If not, how can models be enhanced
when facing huge domain gaps? To answer the first question, we employ measures
including style, inter-class variance (ICV), and indefinable boundaries (IB) to
understand the domain gap. Based on these measures, we establish a new
benchmark named CD-FSOD to evaluate object detection methods, revealing that
most of the current approaches fail to generalize across domains. Technically,
we observe that the performance decline is associated with our proposed
measures: style, ICV, and IB. Consequently, we propose several novel modules to
address these issues. First, the learnable instance features align initial
fixed instances with target categories, enhancing feature distinctiveness.
Second, the instance reweighting module assigns higher importance to
high-quality instances with slight IB. Third, the domain prompter encourages
features resilient to different styles by synthesizing imaginary domains
without altering semantic contents. These techniques collectively contribute to
the development of the Cross-Domain Vision Transformer for CD-FSOD (CD-ViTO),
significantly improving upon the base DE-ViT. Experimental results validate the
efficacy of our model. All datasets, codes, and models will be released to the
community.</div><div><a href='http://arxiv.org/abs/2402.03094v2'>2402.03094v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11220v3")'>CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object
  Detection under Unknown Degradations</div>
<div id='2403.11220v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T13:43:10Z</div><div>Authors: Yuwei Zhang, Yan Wu, Yanming Liu, Xinyue Peng</div><div style='padding-top: 10px; width: 80ex'>Object detection methods under known single degradations have been
extensively investigated. However, existing approaches require prior knowledge
of the degradation type and train a separate model for each, limiting their
practical applications in unpredictable environments. To address this
challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,
CPA-Enhancer, for object detection under unknown degradations. Specifically,
CPA-Enhancer progressively adapts its enhancement strategy under the
step-by-step guidance of CoT prompts, that encode degradation-related
information. To the best of our knowledge, it's the first work that exploits
CoT prompting for object detection tasks. Overall, CPA-Enhancer is a
plug-and-play enhancement model that can be integrated into any generic
detectors to achieve substantial gains on degraded images, without knowing the
degradation type priorly. Experimental results demonstrate that CPA-Enhancer
not only sets the new state of the art for object detection but also boosts the
performance of other downstream vision tasks under unknown degradations.</div><div><a href='http://arxiv.org/abs/2403.11220v3'>2403.11220v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14797v1")'>Preventing Catastrophic Forgetting through Memory Networks in Continuous
  Detection</div>
<div id='2403.14797v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T19:20:29Z</div><div>Authors: Gaurav Bhatt, James Ross, Leonid Sigal</div><div style='padding-top: 10px; width: 80ex'>Modern pre-trained architectures struggle to retain previous information
while undergoing continuous fine-tuning on new tasks. Despite notable progress
in continual classification, systems designed for complex vision tasks such as
detection or segmentation still struggle to attain satisfactory performance. In
this work, we introduce a memory-based detection transformer architecture to
adapt a pre-trained DETR-style detector to new tasks while preserving knowledge
from previous tasks. We propose a novel localized query function for efficient
information retrieval from memory units, aiming to minimize forgetting.
Furthermore, we identify a fundamental challenge in continual detection
referred to as background relegation. This arises when object categories from
earlier tasks reappear in future tasks, potentially without labels, leading
them to be implicitly treated as background. This is an inevitable issue in
continual detection or segmentation. The introduced continual optimization
technique effectively tackles this challenge. Finally, we assess the
performance of our proposed system on continual detection benchmarks and
demonstrate that our approach surpasses the performance of existing
state-of-the-art resulting in 5-7% improvements on MS-COCO and PASCAL-VOC on
the task of continual detection.</div><div><a href='http://arxiv.org/abs/2403.14797v1'>2403.14797v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15505v1")'>Co-Supervised Learning: Improving Weak-to-Strong Generalization with
  Hierarchical Mixture of Experts</div>
<div id='2402.15505v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:56:11Z</div><div>Authors: Yuejiang Liu, Alexandre Alahi</div><div style='padding-top: 10px; width: 80ex'>Steering the behavior of a strong model pre-trained on internet-scale data
can be difficult due to the scarcity of competent supervisors. Recent studies
reveal that, despite supervisory noises, a strong student model may surpass its
weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of
such weak-to-strong generalization remains limited, especially in the presence
of large capability gaps. In this paper, we propose to address this challenge
by harnessing a diverse set of specialized teachers, instead of a single
generalist one, that collectively supervises the strong student. Our approach
resembles the classical hierarchical mixture of experts, with two components
tailored for co-supervision: (i) we progressively alternate student training
and teacher assignment, leveraging the growth of the strong student to identify
plausible supervisions; (ii) we conservatively enforce teacher-student and
local-global consistency, leveraging their dependencies to reject potential
annotation noises. We validate the proposed method through visual recognition
tasks on the OpenAI weak-to-strong benchmark and additional multi-domain
datasets. Our code is available at \url{https://github.com/yuejiangliu/csl}.</div><div><a href='http://arxiv.org/abs/2402.15505v1'>2402.15505v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07685v1")'>Contrastive Multiple Instance Learning for Weakly Supervised Person ReID</div>
<div id='2402.07685v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T14:48:31Z</div><div>Authors: Jacob Tyo, Zachary C. Lipton</div><div style='padding-top: 10px; width: 80ex'>The acquisition of large-scale, precisely labeled datasets for person
re-identification (ReID) poses a significant challenge. Weakly supervised ReID
has begun to address this issue, although its performance lags behind fully
supervised methods. In response, we introduce Contrastive Multiple Instance
Learning (CMIL), a novel framework tailored for more effective weakly
supervised ReID. CMIL distinguishes itself by requiring only a single model and
no pseudo labels while leveraging contrastive losses -- a technique that has
significantly enhanced traditional ReID performance yet is absent in all prior
MIL-based approaches. Through extensive experiments and analysis across three
datasets, CMIL not only matches state-of-the-art performance on the large-scale
SYSU-30k dataset with fewer assumptions but also consistently outperforms all
baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification
dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an
extension of the MUDD dataset featuring naturally occurring weak labels from
the real-world application at PerformancePhoto.co. All our code and data are
accessible at
https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.</div><div><a href='http://arxiv.org/abs/2402.07685v1'>2402.07685v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12236v1")'>Improving Generalization via Meta-Learning on Hard Samples</div>
<div id='2403.12236v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T20:33:44Z</div><div>Authors: Nishant Jain, Arun S. Suggala, Pradeep Shenoy</div><div style='padding-top: 10px; width: 80ex'>Learned reweighting (LRW) approaches to supervised learning use an
optimization criterion to assign weights for training instances, in order to
maximize performance on a representative validation dataset. We pose and
formalize the problem of optimized selection of the validation set used in LRW
training, to improve classifier generalization. In particular, we show that
using hard-to-classify instances in the validation set has both a theoretical
connection to, and strong empirical evidence of generalization. We provide an
efficient algorithm for training this meta-optimized model, as well as a simple
train-twice heuristic for careful comparative study. We demonstrate that LRW
with easy validation data performs consistently worse than LRW with hard
validation data, establishing the validity of our meta-optimization problem.
Our proposed algorithm outperforms a wide range of baselines on a range of
datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M,
CAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet. We also show
that using naturally hard examples for validation (Imagenet-R / Imagenet-A) in
LRW training for Imagenet improves performance on both clean and naturally hard
test instances by 1-2%. Secondary analyses show that using hard validation data
in an LRW framework improves margins on test data, hinting at the mechanism
underlying our empirical gains. We believe this work opens up new research
directions for the meta-optimization of meta-learning in a supervised learning
context.</div><div><a href='http://arxiv.org/abs/2403.12236v1'>2403.12236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10771v1")'>A Probabilistic Approach for Alignment with Human Comparisons</div>
<div id='2403.10771v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T02:19:21Z</div><div>Authors: Junyu Cao, Mohsen Bayati</div><div style='padding-top: 10px; width: 80ex'>A growing trend involves integrating human knowledge into learning
frameworks, leveraging subtle human feedback to refine AI models. Despite these
advances, no comprehensive theoretical framework describing the specific
conditions under which human comparisons improve the traditional supervised
fine-tuning process has been developed. To bridge this gap, this paper studies
the effective use of human comparisons to address limitations arising from
noisy data and high-dimensional models. We propose a two-stage "Supervised Fine
Tuning+Human Comparison" (SFT+HC) framework connecting machine learning with
human feedback through a probabilistic bisection approach. The two-stage
framework first learns low-dimensional representations from noisy-labeled data
via an SFT procedure, and then uses human comparisons to improve the model
alignment. To examine the efficacy of the alignment phase, we introduce a novel
concept termed the "label-noise-to-comparison-accuracy" (LNCA) ratio. This
paper theoretically identifies the conditions under which the "SFT+HC"
framework outperforms pure SFT approach, leveraging this ratio to highlight the
advantage of incorporating human evaluators in reducing sample complexity. We
validate that the proposed conditions for the LNCA ratio are met in a case
study conducted via an Amazon Mechanical Turk experiment.</div><div><a href='http://arxiv.org/abs/2403.10771v1'>2403.10771v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10190v1")'>Perceptual Quality-based Model Training under Annotator Label
  Uncertainty</div>
<div id='2403.10190v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T10:52:18Z</div><div>Authors: Chen Zhou, Mohit Prabhushankar, Ghassan AlRegib</div><div style='padding-top: 10px; width: 80ex'>Annotators exhibit disagreement during data labeling, which can be termed as
annotator label uncertainty. Annotator label uncertainty manifests in
variations of labeling quality. Training with a single low-quality annotation
per sample induces model reliability degradations. In this work, we first
examine the effects of annotator label uncertainty in terms of the model's
generalizability and prediction uncertainty. We observe that the model's
generalizability and prediction uncertainty degrade with the presence of
low-quality noisy labels. Meanwhile, our evaluation of existing uncertainty
estimation algorithms indicates their incapability in response to annotator
label uncertainty. To mitigate performance degradation, prior methods show that
training models with labels collected from multiple independent annotators can
enhance generalizability. However, they require massive annotations. Hence, we
introduce a novel perceptual quality-based model training framework to
objectively generate multiple labels for model training to enhance reliability,
while avoiding massive annotations. Specifically, we first select a subset of
samples with low perceptual quality scores ranked by statistical regularities
of visual signals. We then assign de-aggregated labels to each sample in this
subset to obtain a training set with multiple labels. Our experiments and
analysis demonstrate that training with the proposed framework alleviates the
degradation of generalizability and prediction uncertainty caused by annotator
label uncertainty.</div><div><a href='http://arxiv.org/abs/2403.10190v1'>2403.10190v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14107v1")'>Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement</div>
<div id='2401.14107v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:43:35Z</div><div>Authors: Aaqib Saeed, Dimitris Spathis, Jungwoo Oh, Edward Choi, Ali Etemad</div><div style='padding-top: 10px; width: 80ex'>Wearable technologies enable continuous monitoring of various health metrics,
such as physical activity, heart rate, sleep, and stress levels. A key
challenge with wearable data is obtaining quality labels. Unlike modalities
like video where the videos themselves can be effectively used to label objects
or events, wearable data do not contain obvious cues about the physical
manifestation of the users and usually require rich metadata. As a result,
label noise can become an increasingly thorny issue when labeling such data. In
this paper, we propose a novel solution to address noisy label learning,
entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially
learns a seed model using weak labels. Next, it fine-tunes the seed model using
a handful of expert corrections. Finally, it achieves better generalizability
and robustness by merging the seed and fine-tuned models via weighted parameter
averaging. We evaluate our approach on four challenging tasks and datasets, and
compare it against eight competitive baselines designed to deal with noisy
labels. We show that FHLR achieves significantly better performance when
learning from noisy labels and achieves state-of-the-art by a large margin,
with up to 19% accuracy improvement under symmetric and asymmetric noise.
Notably, we find that FHLR is particularly robust to increased label noise,
unlike prior works that suffer from severe performance degradation. Our work
not only achieves better generalization in high-stakes health sensing
benchmarks but also sheds light on how noise affects commonly-used models.</div><div><a href='http://arxiv.org/abs/2401.14107v1'>2401.14107v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14922v1")'>CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR</div>
<div id='2403.14922v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T02:50:42Z</div><div>Authors: Minghui Qiu, Yandao Huang, Lin Chen, Lu Wang, Kaishun Wu</div><div style='padding-top: 10px; width: 80ex'>In recent years, emerging research on mobile sensing has led to novel
scenarios that enhance daily life for humans, but dynamic usage conditions
often result in performance degradation when systems are deployed in real-world
settings. Existing solutions typically employ one-off adaptation schemes based
on neural networks, which struggle to ensure robustness against uncertain
drifting conditions in human-centric sensing scenarios. In this paper, we
propose CODA, a COst-efficient Domain Adaptation mechanism for mobile sensing
that addresses real-time drifts from the data distribution perspective with
active learning theory, ensuring cost-efficient adaptation directly on the
device. By incorporating a clustering loss and importance-weighted active
learning algorithm, CODA retains the relationship between different clusters
during cost-effective instance-level updates, preserving meaningful structure
within the data distribution. We also showcase its generalization by seamlessly
integrating it with Neural Network-based solutions for Human Activity
Recognition tasks. Through meticulous evaluations across diverse datasets,
including phone-based, watch-based, and integrated sensor-based sensing tasks,
we demonstrate the feasibility and potential of online adaptation with CODA.
The promising results achieved by CODA, even without learnable parameters, also
suggest the possibility of realizing unobtrusive adaptation through specific
application designs with sufficient feedback.</div><div><a href='http://arxiv.org/abs/2403.14922v1'>2403.14922v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02255v1")'>Balancing Continual Learning and Fine-tuning for Human Activity
  Recognition</div>
<div id='2401.02255v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T13:11:43Z</div><div>Authors: Chi Ian Tang, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Akhil Mathur, Cecilia Mascolo</div><div style='padding-top: 10px; width: 80ex'>Wearable-based Human Activity Recognition (HAR) is a key task in
human-centric machine learning due to its fundamental understanding of human
behaviours. Due to the dynamic nature of human behaviours, continual learning
promises HAR systems that are tailored to users' needs. However, because of the
difficulty in collecting labelled data with wearable sensors, existing
approaches that focus on supervised continual learning have limited
applicability, while unsupervised continual learning methods only handle
representation learning while delaying classifier training to a later stage.
This work explores the adoption and adaptation of CaSSLe, a continual
self-supervised learning model, and Kaizen, a semi-supervised continual
learning model that balances representation learning and down-stream
classification, for the task of wearable-based HAR. These schemes re-purpose
contrastive learning for knowledge retention and, Kaizen combines that with
self-training in a unified scheme that can leverage unlabelled and labelled
data for continual learning. In addition to comparing state-of-the-art
self-supervised continual learning schemes, we further investigated the
importance of different loss terms and explored the trade-off between knowledge
retention and learning from new tasks. In particular, our extensive evaluation
demonstrated that the use of a weighting factor that reflects the ratio between
learned and new classes achieves the best overall trade-off in continual
learning.</div><div><a href='http://arxiv.org/abs/2401.02255v1'>2401.02255v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05477v1")'>Standardizing Your Training Process for Human Activity Recognition
  Models: A Comprehensive Review in the Tunable Factors</div>
<div id='2401.05477v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T17:45:28Z</div><div>Authors: Yiran Huang, Haibin Zhao, Yexu Zhou, Till Riedel, Michael Beigl</div><div style='padding-top: 10px; width: 80ex'>In recent years, deep learning has emerged as a potent tool across a
multitude of domains, leading to a surge in research pertaining to its
application in the wearable human activity recognition (WHAR) domain. Despite
the rapid development, concerns have been raised about the lack of
standardization and consistency in the procedures used for experimental model
training, which may affect the reproducibility and reliability of research
results. In this paper, we provide an exhaustive review of contemporary deep
learning research in the field of WHAR and collate information pertaining to
the training procedure employed in various studies. Our findings suggest that a
major trend is the lack of detail provided by model training protocols.
Besides, to gain a clearer understanding of the impact of missing descriptions,
we utilize a control variables approach to assess the impact of key tunable
components (e.g., optimization techniques and early stopping criteria) on the
inter-subject generalization capabilities of HAR models. With insights from the
analyses, we define a novel integrated training procedure tailored to the WHAR
model. Empirical results derived using five well-known \ac{whar} benchmark
datasets and three classical HAR model architectures demonstrate the
effectiveness of our proposed methodology: in particular, there is a
significant improvement in macro F1 leave one subject out cross-validation
performance.</div><div><a href='http://arxiv.org/abs/2401.05477v1'>2401.05477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18296v1")'>Comparative Analysis of XGBoost and Minirocket Algortihms for Human
  Activity Recognition</div>
<div id='2402.18296v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T12:41:06Z</div><div>Authors: Celal Alagoz</div><div style='padding-top: 10px; width: 80ex'>Human Activity Recognition (HAR) has been extensively studied, with recent
emphasis on the implementation of advanced Machine Learning (ML) and Deep
Learning (DL) algorithms for accurate classification. This study investigates
the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and
MiniRocket, in the realm of HAR using data collected from smartphone sensors.
The experiments are conducted on a dataset obtained from the UCI repository,
comprising accelerometer and gyroscope signals captured from 30 volunteers
performing various activities while wearing a smartphone. The dataset undergoes
preprocessing, including noise filtering and feature extraction, before being
utilized for training and testing the classifiers. Monte Carlo cross-validation
is employed to evaluate the models' robustness. The findings reveal that both
XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as
0.99 in activity classification. XGBoost exhibits a slightly superior
performance compared to MiniRocket. Notably, both algorithms surpass the
performance of other ML and DL algorithms reported in the literature for HAR
tasks. Additionally, the study compares the computational efficiency of the two
algorithms, revealing XGBoost's advantage in terms of training time.
Furthermore, the performance of MiniRocket, which achieves accuracy and F1
values of 0.94, and an AUC value of 0.96 using raw data and utilizing only one
channel from the sensors, highlights the potential of directly leveraging
unprocessed signals. It also suggests potential advantages that could be gained
by utilizing sensor fusion or channel fusion techniques. Overall, this research
sheds light on the effectiveness and computational characteristics of XGBoost
and MiniRocket in HAR tasks, providing insights for future studies in activity
recognition using smartphone sensor data.</div><div><a href='http://arxiv.org/abs/2402.18296v1'>2402.18296v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12790v1")'>From Movements to Metrics: Evaluating Explainable AI Methods in
  Skeleton-Based Human Activity Recognition</div>
<div id='2402.12790v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T07:58:04Z</div><div>Authors: Kimji N. Pellano, Inga Strümke, Espen Alexander F. Ihlen</div><div style='padding-top: 10px; width: 80ex'>The advancement of deep learning in human activity recognition (HAR) using 3D
skeleton data is critical for applications in healthcare, security, sports, and
human-computer interaction. This paper tackles a well-known gap in the field,
which is the lack of testing in the applicability and reliability of XAI
evaluation metrics in the skeleton-based HAR domain. We have tested established
XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM)
and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this
problem. The study also introduces a perturbation method that respects human
biomechanical constraints to ensure realistic variations in human movement. Our
findings indicate that \textit{faithfulness} may not be a reliable metric in
certain contexts, such as with the EfficientGCN model. Conversely, stability
emerges as a more dependable metric when there is slight input data
perturbations. CAM and Grad-CAM are also found to produce almost identical
explanations, leading to very similar XAI metric performance. This calls for
the need for more diversified metrics and new XAI methods applied in
skeleton-based HAR.</div><div><a href='http://arxiv.org/abs/2402.12790v1'>2402.12790v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14400v1")'>Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks</div>
<div id='2402.14400v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:34:48Z</div><div>Authors: Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta, Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos</div><div style='padding-top: 10px; width: 80ex'>Reliable methods for the neurodevelopmental assessment of infants are
essential for early detection of medical issues that may need prompt
interventions. Spontaneous motor activity, or `kinetics', is shown to provide a
powerful surrogate measure of upcoming neurodevelopment. However, its
assessment is by and large qualitative and subjective, focusing on visually
identified, age-specific gestures. Here, we follow an alternative approach,
predicting infants' neurodevelopmental maturation based on data-driven
evaluation of individual motor patterns. We utilize 3D video recordings of
infants processed with pose-estimation to extract spatio-temporal series of
anatomical landmarks, and apply adaptive graph convolutional networks to
predict the actual age. We show that our data-driven approach achieves
improvement over traditional machine learning baselines based on manually
engineered features.</div><div><a href='http://arxiv.org/abs/2402.14400v1'>2402.14400v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05426v1")'>CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in
  Human Activity Recognition</div>
<div id='2401.05426v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T22:04:40Z</div><div>Authors: Mengxi Liu, Zimin Zhao, Daniel Geißler, Bo Zhou, Sungho Suh, Paul Lukowicz</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in Artificial Neural Networks have significantly improved
human activity recognition using multiple time-series sensors. While employing
numerous sensors with high-frequency sampling rates usually improves the
results, it often leads to data inefficiency and unnecessary expansion of the
ANN, posing a challenge for their practical deployment on edge devices.
Addressing these issues, our work introduces a pragmatic framework for
data-efficient utilization in HAR tasks, considering the optimization of both
sensor modalities and sampling rate simultaneously. Central to our approach are
the designed trainable parameters, termed 'Weight Scores,' which assess the
significance of each sensor modality and sampling rate during the training
phase. These scores guide the sensor modalities and sampling rate selection.
The pruning method allows users to make a trade-off between computational
budgets and performance by selecting the sensor modalities and sampling rates
according to the weight score ranking. We tested our framework's effectiveness
in optimizing sensor modality and sampling rate selection using three public
HAR benchmark datasets. The results show that the sensor and sampling rate
combination selected via CoSS achieves similar classification performance to
configurations using the highest sampling rate with all sensors but at a
reduced hardware cost.</div><div><a href='http://arxiv.org/abs/2401.05426v1'>2401.05426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09434v1")'>Disentangling Imperfect: A Wavelet-Infused Multilevel Heterogeneous
  Network for Human Activity Recognition in Flawed Wearable Sensor Data</div>
<div id='2402.09434v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T06:08:49Z</div><div>Authors: Mengna Liu, Dong Xiang, Xu Cheng, Xiufeng Liu, Dalin Zhang, Shengyong Chen, Christian S. Jensen</div><div style='padding-top: 10px; width: 80ex'>The popularity and diffusion of wearable devices provides new opportunities
for sensor-based human activity recognition that leverages deep learning-based
algorithms. Although impressive advances have been made, two major challenges
remain. First, sensor data is often incomplete or noisy due to sensor placement
and other issues as well as data transmission failure, calling for imputation
of missing values, which also introduces noise. Second, human activity has
multi-scale characteristics. Thus, different groups of people and even the same
person may behave differently under different circumstances. To address these
challenges, we propose a multilevel heterogeneous neural network, called MHNN,
for sensor data analysis. We utilize multilevel discrete wavelet decomposition
to extract multi-resolution features from sensor data. This enables
distinguishing signals with different frequencies, thereby suppressing noise.
As the components resulting from the decomposition are heterogeneous, we equip
the proposed model with heterogeneous feature extractors that enable the
learning of multi-scale features. Due to the complementarity of these features,
we also include a cross aggregation module for enhancing their interactions. An
experimental study using seven publicly available datasets offers evidence that
MHNN can outperform other cutting-edge models and offers evidence of robustness
to missing values and noise. An ablation study confirms the importance of each
module.</div><div><a href='http://arxiv.org/abs/2402.09434v1'>2402.09434v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10748v1")'>A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power
  Arrhythmia Classification on Microcontrollers</div>
<div id='2402.10748v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T15:14:16Z</div><div>Authors: Paola Busia, Matteo Antonio Scrugli, Victor Jean-Baptiste Jung, Luca Benini, Paolo Meloni</div><div style='padding-top: 10px; width: 80ex'>Wearable systems for the long-term monitoring of cardiovascular diseases are
becoming widespread and valuable assets in diagnosis and therapy. A promising
approach for real-time analysis of the electrocardiographic (ECG) signal and
the detection of heart conditions, such as arrhythmia, is represented by the
transformer machine learning model. Transformers are powerful models for the
classification of time series, although efficient implementation in the
wearable domain raises significant design challenges, to combine adequate
accuracy and a suitable complexity. In this work, we present a tiny transformer
model for the analysis of the ECG signal, requiring only 6k parameters and
reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia
classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit
integer inference as required for efficient execution on low-power
microcontroller-based devices. We explored an augmentation-based training
approach for improving the robustness against electrode motion artifacts noise,
resulting in a worst-case post-deployment performance assessment of 98.36%
accuracy. Suitability for wearable monitoring solutions is finally demonstrated
through efficient deployment on the parallel ultra-low-power GAP9 processor,
where inference execution requires 4.28ms and 0.09mJ.</div><div><a href='http://arxiv.org/abs/2402.10748v1'>2402.10748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09264v3")'>UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers</div>
<div id='2402.09264v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:51:28Z</div><div>Authors: Hong Jia, Young D. Kwon, Dong Ma, Nhat Pham, Lorena Qendro, Tam Vu, Cecilia Mascolo</div><div style='padding-top: 10px; width: 80ex'>Traditional machine learning techniques are prone to generating inaccurate
predictions when confronted with shifts in the distribution of data between the
training and testing phases. This vulnerability can lead to severe
consequences, especially in applications such as mobile healthcare. Uncertainty
estimation has the potential to mitigate this issue by assessing the
reliability of a model's output. However, existing uncertainty estimation
techniques often require substantial computational resources and memory, making
them impractical for implementation on microcontrollers (MCUs). This limitation
hinders the feasibility of many important on-device wearable event detection
(WED) applications, such as heart attack detection.
  In this paper, we present UR2M, a novel Uncertainty and Resource-aware event
detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware
WED based on evidential theory for accurate event detection and reliable
uncertainty estimation; (ii) introduce a cascade ML framework to achieve
efficient model inference via early exits, by sharing shallower model layers
among different event models; (iii) optimize the deployment of the model and
MCU library for system efficiency. We conducted extensive experiments and
compared UR2M to traditional uncertainty baselines using three wearable
datasets. Our results demonstrate that UR2M achieves up to 864% faster
inference speed, 857% energy-saving for uncertainty estimation, 55% memory
saving on two popular MCUs, and a 22% improvement in uncertainty quantification
performance.
  UR2M can be deployed on a wide range of MCUs, significantly expanding
real-time and reliable WED applications.</div><div><a href='http://arxiv.org/abs/2402.09264v3'>2402.09264v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09867v1")'>Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs</div>
<div id='2402.09867v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T10:50:42Z</div><div>Authors: Zain Taufique, Muhammad Awais Bin Altaf, Antonio Miele, Pasi Liljeberg, Anil Kanduri</div><div style='padding-top: 10px; width: 80ex'>Electroencephalography (EEG) recordings are analyzed using battery-powered
wearable devices to monitor brain activities and neurological disorders. These
applications require long and continuous processing to generate feasible
results. However, wearable devices are constrained with limited energy and
computation resources, owing to their small sizes for practical use cases.
Embedded heterogeneous multi-core platforms (HMPs) can provide better
performance within limited energy budgets for EEG applications. Error
resilience of the EEG application pipeline can be exploited further to maximize
the performance and energy gains with HMPs. However, disciplined tuning of
approximation on embedded HMPs requires a thorough exploration of the
accuracy-performance-power trade-off space. In this work, we characterize the
error resilience of three EEG applications, including Epileptic Seizure
Detection, Sleep Stage Classification, and Stress Detection on the real-world
embedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial
evaluation of power-performance-accuracy trade-offs of EEG applications at
different approximation, power, and performance levels to provide insights into
the disciplined tuning of approximation in EEG applications on embedded
platforms.</div><div><a href='http://arxiv.org/abs/2402.09867v1'>2402.09867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12783v1")'>A Review of Deep Learning Methods for Photoplethysmography Data</div>
<div id='2401.12783v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:11:29Z</div><div>Authors: Guangkun Nie, Jiabao Zhu, Gongzheng Tang, Deyun Zhang, Shijia Geng, Qinghao Zhao, Shenda Hong</div><div style='padding-top: 10px; width: 80ex'>Photoplethysmography (PPG) is a highly promising device due to its advantages
in portability, user-friendly operation, and non-invasive capabilities to
measure a wide range of physiological information. Recent advancements in deep
learning have demonstrated remarkable outcomes by leveraging PPG signals for
tasks related to personal health management and other multifaceted
applications. In this review, we systematically reviewed papers that applied
deep learning models to process PPG data between January 1st of 2017 and July
31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed
from three key perspectives: tasks, models, and data. We finally extracted 193
papers where different deep learning frameworks were used to process PPG
signals. Based on the tasks addressed in these papers, we categorized them into
two major groups: medical-related, and non-medical-related. The medical-related
tasks were further divided into seven subgroups, including blood pressure
analysis, cardiovascular monitoring and diagnosis, sleep health, mental health,
respiratory monitoring and analysis, blood glucose analysis, as well as others.
The non-medical-related tasks were divided into four subgroups, which encompass
signal processing, biometric identification, electrocardiogram reconstruction,
and human activity recognition. In conclusion, significant progress has been
made in the field of using deep learning methods to process PPG data recently.
This allows for a more thorough exploration and utilization of the information
contained in PPG signals. However, challenges remain, such as limited quantity
and quality of publicly available databases, a lack of effective validation in
real-world scenarios, and concerns about the interpretability, scalability, and
complexity of deep learning models. Moreover, there are still emerging research
areas that require further investigation.</div><div><a href='http://arxiv.org/abs/2401.12783v1'>2401.12783v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10582v1")'>How Suboptimal is Training rPPG Models with Videos and Targets from
  Different Body Sites?</div>
<div id='2403.10582v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:20:21Z</div><div>Authors: Björn Braun, Daniel McDuff, Christian Holz</div><div style='padding-top: 10px; width: 80ex'>Remote camera measurement of the blood volume pulse via photoplethysmography
(rPPG) is a compelling technology for scalable, low-cost, and accessible
assessment of cardiovascular information. Neural networks currently provide the
state-of-the-art for this task and supervised training or fine-tuning is an
important step in creating these models. However, most current models are
trained on facial videos using contact PPG measurements from the fingertip as
targets/ labels. One of the reasons for this is that few public datasets to
date have incorporated contact PPG measurements from the face. Yet there is
copious evidence that the PPG signals at different sites on the body have very
different morphological features. Is training a facial video rPPG model using
contact measurements from another site on the body suboptimal? Using a recently
released unique dataset with synchronized contact PPG and video measurements
from both the hand and face, we can provide precise and quantitative answers to
this question. We obtain up to 40 % lower mean squared errors between the
waveforms of the predicted and the ground truth PPG signals using
state-of-the-art neural models when using PPG signals from the forehead
compared to using PPG signals from the fingertip. We also show qualitatively
that the neural models learn to predict the morphology of the ground truth PPG
signal better when trained on the forehead PPG signals. However, while models
trained from the forehead PPG produce a more faithful waveform, models trained
from a finger PPG do still learn the dominant frequency (i.e., the heart rate)
well.</div><div><a href='http://arxiv.org/abs/2403.10582v1'>2403.10582v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05469v1")'>Robust CNN-based Respiration Rate Estimation for Smartwatch PPG and IMU</div>
<div id='2401.05469v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T15:15:46Z</div><div>Authors: Kianoosh Kazemi, Iman Azimi, Pasi Liljeberg, Amir M. Rahmani</div><div style='padding-top: 10px; width: 80ex'>Respiratory rate (RR) serves as an indicator of various medical conditions,
such as cardiovascular diseases and sleep disorders. These RR estimation
methods were mostly designed for finger-based PPG collected from subjects in
stationary situations (e.g., in hospitals). In contrast to finger-based PPG
signals, wrist-based PPG are more susceptible to noise, particularly in their
low frequency range, which includes respiratory information. Therefore, the
existing methods struggle to accurately extract RR when PPG data are collected
from wrist area under free-living conditions. The increasing popularity of
smartwatches, equipped with various sensors including PPG, has prompted the
need for a robust RR estimation method. In this paper, we propose a
convolutional neural network-based approach to extract RR from PPG,
accelerometer, and gyroscope signals captured via smartwatches. Our method,
including a dilated residual inception module and 1D convolutions, extract the
temporal information from the signals, enabling RR estimation. Our method is
trained and tested using data collected from 36 subjects under free-living
conditions for one day using Samsung Gear Sport watches. For evaluation, we
compare the proposed method with four state-of-the-art RR estimation methods.
The RR estimates are compared with RR references obtained from a chest-band
device. The results show that our method outperforms the existing methods with
the Mean-Absolute-Error and Root-Mean-Square-Error of 1.85 and 2.34, while the
best results obtained by the other methods are 2.41 and 3.29, respectively.
Moreover, compared to the other methods, the absolute error distribution of our
method was narrow (with the lowest median), indicating a higher level of
agreement between the estimated and reference RR values.</div><div><a href='http://arxiv.org/abs/2401.05469v1'>2401.05469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13820v1")'>Identity information based on human magnetocardiography signals</div>
<div id='2403.13820v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T17:18:40Z</div><div>Authors: Pengju Zhang, Chenxi Sun, Jianwei Zhang, Hong Guo</div><div style='padding-top: 10px; width: 80ex'>We have developed an individual identification system based on
magnetocardiography (MCG) signals captured using optically pumped magnetometers
(OPMs). Our system utilizes pattern recognition to analyze the signals obtained
at different positions on the body, by scanning the matrices composed of MCG
signals with a 2*2 window. In order to make use of the spatial information of
MCG signals, we transform the signals from adjacent small areas into four
channels of a dataset. We further transform the data into time-frequency
matrices using wavelet transforms and employ a convolutional neural network
(CNN) for classification. As a result, our system achieves an accuracy rate of
97.04% in identifying individuals. This finding indicates that the MCG signal
holds potential for use in individual identification systems, offering a
valuable tool for personalized healthcare management.</div><div><a href='http://arxiv.org/abs/2403.13820v1'>2403.13820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17780v1")'>Constraint Latent Space Matters: An Anti-anomalous Waveform
  Transformation Solution from Photoplethysmography to Arterial Blood Pressure</div>
<div id='2402.17780v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T02:31:35Z</div><div>Authors: Cheng Bian, Xiaoyu Li, Qi Bi, Guangpu Zhu, Jiegeng Lyu, Weile Zhang, Yelei Li, Zijing Zeng</div><div style='padding-top: 10px; width: 80ex'>Arterial blood pressure (ABP) holds substantial promise for proactive
cardiovascular health management. Notwithstanding its potential, the invasive
nature of ABP measurements confines their utility primarily to clinical
environments, limiting their applicability for continuous monitoring beyond
medical facilities. The conversion of photoplethysmography (PPG) signals into
ABP equivalents has garnered significant attention due to its potential in
revolutionizing cardiovascular disease management. Recent strides in PPG-to-ABP
prediction encompass the integration of generative and discriminative models.
Despite these advances, the efficacy of these models is curtailed by the latent
space shift predicament, stemming from alterations in PPG data distribution
across disparate hardware and individuals, potentially leading to distorted ABP
waveforms. To tackle this problem, we present an innovative solution named the
Latent Space Constraint Transformer (LSCT), leveraging a quantized codebook to
yield robust latent spaces by employing multiple discretizing bases. To
facilitate improved reconstruction, the Correlation-boosted Attention Module
(CAM) is introduced to systematically query pertinent bases on a global scale.
Furthermore, to enhance expressive capacity, we propose the Multi-Spectrum
Enhancement Knowledge (MSEK), which fosters local information flow within the
channels of latent code and provides additional embedding for reconstruction.
Through comprehensive experimentation on both publicly available datasets and a
private downstream task dataset, the proposed approach demonstrates noteworthy
performance enhancements compared to existing methods. Extensive ablation
studies further substantiate the effectiveness of each introduced module.</div><div><a href='http://arxiv.org/abs/2402.17780v1'>2402.17780v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05452v1")'>Cuff-less Arterial Blood Pressure Waveform Synthesis from Single-site
  PPG using Transformer &amp; Frequency-domain Learning</div>
<div id='2401.05452v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T17:12:37Z</div><div>Authors: Muhammad Ahmad Tahir, Ahsan Mehmood, Muhammad Mahboob Ur Rahman, Muhammad Wasim Nawaz, Kashif Riaz, Qammer H. Abbasi</div><div style='padding-top: 10px; width: 80ex'>We propose two novel purpose-built deep learning (DL) models for synthesis of
the arterial blood pressure (ABP) waveform in a cuff-less manner, using a
single-site photoplethysmography (PPG) signal. We utilize the public UCI
dataset on cuff-less blood pressure (CLBP) estimation to train and evaluate our
DL models. Firstly, we implement a transformer model that incorporates
positional encoding, multi-head attention, layer normalization, and dropout
techniques, and synthesizes the ABP waveform with a mean absolute error (MAE)
of 14. Secondly, we implement a frequency-domain (FD) learning approach where
we first obtain the discrete cosine transform (DCT) coefficients of the PPG and
ABP signals corresponding to two cardiac cycles, and then learn a
linear/non-linear (L/NL) regression between them. We learn that the FD L/NL
regression model outperforms the transformer model by achieving an MAE of 11.87
and 8.01, for diastolic blood pressure (DBP) and systolic blood pressure (SBP),
respectively. Our FD L/NL regression model also fulfills the AAMI criterion of
utilizing data from more than 85 subjects, and achieves grade B by the BHS
criterion.</div><div><a href='http://arxiv.org/abs/2401.05452v1'>2401.05452v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18886v1")'>BP-DeepONet: A new method for cuffless blood pressure estimation using
  the physcis-informed DeepONet</div>
<div id='2402.18886v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T06:11:21Z</div><div>Authors: Lingfeng Li, Xue-Cheng Tai, Raymond Chan</div><div style='padding-top: 10px; width: 80ex'>Cardiovascular diseases (CVDs) are the leading cause of death worldwide, with
blood pressure serving as a crucial indicator. Arterial blood pressure (ABP)
waveforms provide continuous pressure measurements throughout the cardiac cycle
and offer valuable diagnostic insights. Consequently, there is a significant
demand for non-invasive and cuff-less methods to measure ABP waveforms
continuously. Accurate prediction of ABP waveforms can also improve the
estimation of mean blood pressure, an essential cardiovascular health
characteristic.
  This study proposes a novel framework based on the physics-informed DeepONet
approach to predict ABP waveforms. Unlike previous methods, our approach
requires the predicted ABP waveforms to satisfy the Navier-Stokes equation with
a time-periodic condition and a Windkessel boundary condition. Notably, our
framework is the first to predict ABP waveforms continuously, both with
location and time, within the part of the artery that is being simulated.
Furthermore, our method only requires ground truth data at the outlet boundary
and can handle periodic conditions with varying periods. Incorporating the
Windkessel boundary condition in our solution allows for generating natural
physical reflection waves, which closely resemble measurements observed in
real-world cases. Moreover, accurately estimating the hyper-parameters in the
Navier-Stokes equation for our simulations poses a significant challenge. To
overcome this obstacle, we introduce the concept of meta-learning, enabling the
neural networks to learn these parameters during the training process.</div><div><a href='http://arxiv.org/abs/2402.18886v1'>2402.18886v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12323v1")'>Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional
  Computing on Embedded Devices</div>
<div id='2403.12323v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:32:08Z</div><div>Authors: Manuel E. Segura, Pere Verges, Justin Tian Jin Chen, Ramesh Arangott, Angela Kristine Garcia, Laura Garcia Reynoso, Alexandru Nicolau, Tony Givargis, Sergio Gago-Masague</div><div style='padding-top: 10px; width: 80ex'>Alcohol consumption has a significant impact on individuals' health, with
even more pronounced consequences when consumption becomes excessive. One
approach to promoting healthier drinking habits is implementing just-in-time
interventions, where timely notifications indicating intoxication are sent
during heavy drinking episodes. However, the complexity or invasiveness of an
intervention mechanism may deter an individual from using them in practice.
Previous research tackled this challenge using collected motion data and
conventional Machine Learning (ML) algorithms to classify heavy drinking
episodes, but with impractical accuracy and computational efficiency for mobile
devices. Consequently, we have elected to use Hyperdimensional Computing (HDC)
to design a just-in-time intervention approach that is practical for
smartphones, smart wearables, and IoT deployment. HDC is a framework that has
proven results in processing real-time sensor data efficiently. This approach
offers several advantages, including low latency, minimal power consumption,
and high parallelism. We explore various HDC encoding designs and combine them
with various HDC learning models to create an optimal and feasible approach for
mobile devices. Our findings indicate an accuracy rate of 89\%, which
represents a substantial 12\% improvement over the current state-of-the-art.</div><div><a href='http://arxiv.org/abs/2403.12323v1'>2403.12323v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00177v1")'>Non-Invasive Medical Digital Twins using Physics-Informed
  Self-Supervised Learning</div>
<div id='2403.00177v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T23:04:42Z</div><div>Authors: Keying Kuang, Frances Dean, Jack B. Jedlicki, David Ouyang, Anthony Philippakis, David Sontag, Ahmed M. Alaa</div><div style='padding-top: 10px; width: 80ex'>A digital twin is a virtual replica of a real-world physical phenomena that
uses mathematical modeling to characterize and simulate its defining features.
By constructing digital twins for disease processes, we can perform in-silico
simulations that mimic patients' health conditions and counterfactual outcomes
under hypothetical interventions in a virtual setting. This eliminates the need
for invasive procedures or uncertain treatment decisions. In this paper, we
propose a method to identify digital twin model parameters using only
noninvasive patient health data. We approach the digital twin modeling as a
composite inverse problem, and observe that its structure resembles pretraining
and finetuning in self-supervised learning (SSL). Leveraging this, we introduce
a physics-informed SSL algorithm that initially pretrains a neural network on
the pretext task of solving the physical model equations. Subsequently, the
model is trained to reconstruct low-dimensional health measurements from
noninvasive modalities while being constrained by the physical equations
learned in pretraining. We apply our method to identify digital twins of
cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate
its utility in unsupervised disease detection and in-silico clinical trials.</div><div><a href='http://arxiv.org/abs/2403.00177v1'>2403.00177v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17783v1")'>BagStacking: An Integrated Ensemble Learning Approach for Freezing of
  Gait Detection in Parkinson's Disease</div>
<div id='2402.17783v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T04:13:48Z</div><div>Authors: Seffi Cohen, Lior Rokach</div><div style='padding-top: 10px; width: 80ex'>This paper introduces BagStacking, a novel ensemble learning method designed
to enhance the detection of Freezing of Gait (FOG) in Parkinson's Disease (PD)
by using a lower-back sensor to track acceleration. Building on the principles
of bagging and stacking, BagStacking aims to achieve the variance reduction
benefit of bagging's bootstrap sampling while also learning sophisticated
blending through stacking. The method involves training a set of base models on
bootstrap samples from the training data, followed by a meta-learner trained on
the base model outputs and true labels to find an optimal aggregation scheme.
The experimental evaluation demonstrates significant improvements over other
state-of-the-art machine learning methods on the validation set. Specifically,
BagStacking achieved a MAP score of 0.306, outperforming LightGBM (0.234) and
classic Stacking (0.286). Additionally, the run-time of BagStacking was
measured at 3828 seconds, illustrating an efficient approach compared to
Regular Stacking's 8350 seconds. BagStacking presents a promising direction for
handling the inherent variability in FOG detection data, offering a robust and
scalable solution to improve patient care in PD.</div><div><a href='http://arxiv.org/abs/2402.17783v1'>2402.17783v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10288v1")'>CLAN: A Contrastive Learning based Novelty Detection Framework for Human
  Activity Recognition</div>
<div id='2401.10288v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T03:57:36Z</div><div>Authors: Hyunju Kim, Dongman Lee</div><div style='padding-top: 10px; width: 80ex'>In ambient assisted living, human activity recognition from time series
sensor data mainly focuses on predefined activities, often overlooking new
activity patterns. We propose CLAN, a two-tower contrastive learning-based
novelty detection framework with diverse types of negative pairs for human
activity recognition. It is tailored to challenges with human activity
characteristics, including the significance of temporal and frequency features,
complex activity dynamics, shared features across activities, and sensor
modality variations. The framework aims to construct invariant representations
of known activity robust to the challenges. To generate suitable negative
pairs, it selects data augmentation methods according to the temporal and
frequency characteristics of each dataset. It derives the key representations
against meaningless dynamics by contrastive and classification losses-based
representation learning and score function-based novelty detection that
accommodate dynamic numbers of the different types of augmented samples. The
proposed two-tower model extracts the representations in terms of time and
frequency, mutually enhancing expressiveness for distinguishing between new and
known activities, even when they share common features. Experiments on four
real-world human activity datasets show that CLAN surpasses the best
performance of existing novelty detection methods, improving by 8.3%, 13.7%,
and 53.3% in AUROC, balanced accuracy, and FPR@TPR0.95 metrics respectively.</div><div><a href='http://arxiv.org/abs/2401.10288v1'>2401.10288v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09445v1")'>iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition</div>
<div id='2402.09445v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T16:53:50Z</div><div>Authors: Mengxi Liu, Vitor Fortes Rey, Yu Zhang, Lala Shakti Swarup Ray, Bo Zhou, Paul Lukowicz</div><div style='padding-top: 10px; width: 80ex'>Automatic and precise fitness activity recognition can be beneficial in
aspects from promoting a healthy lifestyle to personalized preventative
healthcare. While IMUs are currently the prominent fitness tracking modality,
through iMove, we show bio-impedence can help improve IMU-based fitness
tracking through sensor fusion and contrastive learning.To evaluate our
methods, we conducted an experiment including six upper body fitness activities
performed by ten subjects over five days to collect synchronized data from
bio-impedance across two wrists and IMU on the left wrist.The contrastive
learning framework uses the two modalities to train a better IMU-only
classification model, where bio-impedance is only required at the training
phase, by which the average Macro F1 score with the input of a single IMU was
improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU
baseline model. We have also shown how bio-impedance can improve human activity
recognition (HAR) directly through sensor fusion, reaching an average Macro F1
score of 89.57 \% (two modalities required for both training and inference)
even if Bio-impedance alone has an average macro F1 score of 75.36 \%, which is
outperformed by IMU alone. In addition, similar results were obtained in an
extended study on lower body fitness activity classification, demonstrating the
generalisability of our approach.Our findings underscore the potential of
sensor fusion and contrastive learning as valuable tools for advancing fitness
activity recognition, with bio-impedance playing a pivotal role in augmenting
the capabilities of IMU-based systems.</div><div><a href='http://arxiv.org/abs/2402.09445v1'>2402.09445v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02910v2")'>DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage
  Temporal Convolutional Network</div>
<div id='2402.02910v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:25:45Z</div><div>Authors: Meng Shang, Lenore Dedeyne, Jolan Dupont, Laura Vercauteren, Nadjia Amini, Laurence Lapauw, Evelien Gielen, Sabine Verschueren, Carolina Varon, Walter De Raedt, Bart Vanrumste</div><div style='padding-top: 10px; width: 80ex'>The Otago Exercise Program (OEP) represents a crucial rehabilitation
initiative tailored for older adults, aimed at enhancing balance and strength.
Despite previous efforts utilizing wearable sensors for OEP recognition,
existing studies have exhibited limitations in terms of accuracy and
robustness. This study addresses these limitations by employing a single
waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among
community-dwelling older adults in their daily lives. A cohort of 36 older
adults participated in laboratory settings, supplemented by an additional 7
older adults recruited for at-home assessments. The study proposes a Dual-Scale
Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level
sequence-to-sequence classification, incorporating them in one loss function.
In the first stage, the model focuses on recognizing each repetition of the
exercises (micro labels). Subsequent stages extend the recognition to encompass
the complete range of exercises (macro labels). The DS-MS-TCN model surpasses
existing state-of-the-art deep learning models, achieving f1-scores exceeding
80% and Intersection over Union (IoU) f1-scores surpassing 60% for all four
exercises evaluated. Notably, the model outperforms the prior study utilizing
the sliding window technique, eliminating the need for post-processing stages
and window size tuning. To our knowledge, we are the first to present a novel
perspective on enhancing Human Activity Recognition (HAR) systems through the
recognition of each repetition of activities.</div><div><a href='http://arxiv.org/abs/2402.02910v2'>2402.02910v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02772v1")'>Rehabilitation Exercise Quality Assessment through Supervised
  Contrastive Learning with Hard and Soft Negatives</div>
<div id='2403.02772v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:38:25Z</div><div>Authors: Mark Karlov, Ali Abedi, Shehroz S. Khan</div><div style='padding-top: 10px; width: 80ex'>Exercise-based rehabilitation programs have proven to be effective in
enhancing the quality of life and reducing mortality and rehospitalization
rates. AI-driven virtual rehabilitation, which allows patients to independently
complete exercises at home, utilizes AI algorithms to analyze exercise data,
providing feedback to patients and updating clinicians on their progress. These
programs commonly prescribe a variety of exercise types, leading to a distinct
challenge in rehabilitation exercise assessment datasets: while abundant in
overall training samples, these datasets often have a limited number of samples
for each individual exercise type. This disparity hampers the ability of
existing approaches to train generalizable models with such a small sample size
per exercise. Addressing this issue, our paper introduces a novel supervised
contrastive learning framework with hard and soft negative samples that
effectively utilizes the entire dataset to train a single model applicable to
all exercise types. This model, with a Spatial-Temporal Graph Convolutional
Network (ST-GCN) architecture, demonstrated enhanced generalizability across
exercises and a decrease in overall complexity. Through extensive experiments
on three publicly available rehabilitation exercise assessment datasets, the
University of Idaho-Physical Rehabilitation Movement Data (UI-PRMD),
IntelliRehabDS (IRDS), and KInematic assessment of MOvement and clinical scores
for remote monitoring of physical REhabilitation (KIMORE), our method has shown
to surpass existing methods, setting a new benchmark in rehabilitation exercise
assessment accuracy.</div><div><a href='http://arxiv.org/abs/2403.02772v1'>2403.02772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05595v1")'>Comparison of gait phase detection using traditional machine learning
  and deep learning techniques</div>
<div id='2403.05595v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T10:05:09Z</div><div>Authors: Farhad Nazari, Navid Mohajer, Darius Nahavandi, Abbas Khosravi</div><div style='padding-top: 10px; width: 80ex'>Human walking is a complex activity with a high level of cooperation and
interaction between different systems in the body. Accurate detection of the
phases of the gait in real-time is crucial to control lower-limb assistive
devices like exoskeletons and prostheses. There are several ways to detect the
walking gait phase, ranging from cameras and depth sensors to the sensors
attached to the device itself or the human body. Electromyography (EMG) is one
of the input methods that has captured lots of attention due to its precision
and time delay between neuromuscular activity and muscle movement. This study
proposes a few Machine Learning (ML) based models on lower-limb EMG data for
human walking. The proposed models are based on Gaussian Naive Bayes (NB),
Decision Tree (DT), Random Forest (RF), Linear Discriminant Analysis (LDA) and
Deep Convolutional Neural Networks (DCNN). The traditional ML models are
trained on hand-crafted features or their reduced components using Principal
Component Analysis (PCA). On the contrary, the DCNN model utilises
convolutional layers to extract features from raw data. The results show up to
75% average accuracy for traditional ML models and 79% for Deep Learning (DL)
model. The highest achieved accuracy in 50 trials of the training DL model is
89.5%.</div><div><a href='http://arxiv.org/abs/2403.05595v1'>2403.05595v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07926v1")'>Value Prediction for Spatiotemporal Gait Data Using Deep Learning</div>
<div id='2403.07926v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:30:13Z</div><div>Authors: Ryan Cavanagh, Jelena Trajkovic, Wenlu Zhang, I-Hung Khoo, Vennila Krishnan</div><div style='padding-top: 10px; width: 80ex'>Human gait has been commonly used for the diagnosis and evaluation of medical
conditions and for monitoring the progress during treatment and rehabilitation.
The use of wearable sensors that capture pressure or motion has yielded
techniques that analyze the gait data to aid recovery, identify activity
performed, or identify individuals. Deep learning, usually employing
classification, has been successfully utilized in a variety of applications
such as computer vision, biomedical imaging analysis, and natural language
processing. We expand the application of deep learning to value prediction of
time-series of spatiotemporal gait data. Moreover, we explore several deep
learning architectures (Recurrent Neural Networks (RNN) and RNN combined with
Convolutional Neural Networks (CNN)) to make short- and long-distance
predictions using two different experimental setups. Our results show that
short-distance prediction has an RMSE as low as 0.060675, and long-distance
prediction RMSE as low as 0.106365. Additionally, the results show that the
proposed deep learning models are capable of predicting the entire trial when
trained and validated using the trials from the same participant. The proposed,
customized models, used with value prediction open possibilities for additional
applications, such as fall prediction, in-home progress monitoring, aiding of
exoskeleton movement, and authentication.</div><div><a href='http://arxiv.org/abs/2403.07926v1'>2403.07926v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09761v1")'>A Framework For Gait-Based User Demography Estimation Using Inertial
  Sensors</div>
<div id='2402.09761v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T07:23:34Z</div><div>Authors: Chinmay Prakash Swami</div><div style='padding-top: 10px; width: 80ex'>Human gait has been shown to provide crucial motion cues for various
applications. Recognizing patterns in human gait has been widely adopted in
various application areas such as security, virtual reality gaming, medical
rehabilitation, and ailment identification. Furthermore, wearable inertial
sensors have been widely used for not only recording gait but also to predict
users' demography. Machine Learning techniques such as deep learning, combined
with inertial sensor signals, have shown promising results in recognizing
patterns in human gait and estimate users' demography. However, the black-box
nature of such deep learning models hinders the researchers from uncovering the
reasons behind the model's predictions. Therefore, we propose leveraging deep
learning and Layer-Wise Relevance Propagation (LRP) to identify the important
variables that play a vital role in identifying the users' demography such as
age and gender. To assess the efficacy of this approach we train a deep neural
network model on a large sensor-based gait dataset consisting of 745 subjects
to identify users' age and gender. Using LRP we identify the variables relevant
for characterizing the gait patterns. Thus, we enable interpretation of
non-linear ML models which are experts in identifying the users' demography
based on inertial signals. We believe this approach can not only provide
clinicians information about the gait parameters relevant to age and gender but
also can be expanded to analyze and diagnose gait disorders.</div><div><a href='http://arxiv.org/abs/2402.09761v1'>2402.09761v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16649v1")'>Using Motion Forecasting for Behavior-Based Virtual Reality (VR)
  Authentication</div>
<div id='2401.16649v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T00:43:41Z</div><div>Authors: Mingjun Li, Natasha Kholgade Banerjee, Sean Banerjee</div><div style='padding-top: 10px; width: 80ex'>Task-based behavioral biometric authentication of users interacting in
virtual reality (VR) environments enables seamless continuous authentication by
using only the motion trajectories of the person's body as a unique signature.
Deep learning-based approaches for behavioral biometrics show high accuracy
when using complete or near complete portions of the user trajectory, but show
lower performance when using smaller segments from the start of the task. Thus,
any systems designed with existing techniques are vulnerable while waiting for
future segments of motion trajectories to become available. In this work, we
present the first approach that predicts future user behavior using
Transformer-based forecasting and using the forecasted trajectory to perform
user authentication. Our work leverages the notion that given the current
trajectory of a user in a task-based environment we can predict the future
trajectory of the user as they are unlikely to dramatically shift their
behavior since it would preclude the user from successfully completing their
task goal. Using the publicly available 41-subject ball throwing dataset of
Miller et al. we show improvement in user authentication when using forecasted
data. When compared to no forecasting, our approach reduces the authentication
equal error rate (EER) by an average of 23.85% and a maximum reduction of
36.14%.</div><div><a href='http://arxiv.org/abs/2401.16649v1'>2401.16649v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07684v1")'>Towards a Foundation Model for Brain Age Prediction using coVariance
  Neural Networks</div>
<div id='2402.07684v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T14:46:31Z</div><div>Authors: Saurabh Sihag, Gonzalo Mateos, Alejandro Ribeiro</div><div style='padding-top: 10px; width: 80ex'>Brain age is the estimate of biological age derived from neuroimaging
datasets using machine learning algorithms. Increasing brain age with respect
to chronological age can reflect increased vulnerability to neurodegeneration
and cognitive decline. In this paper, we study NeuroVNN, based on coVariance
neural networks, as a paradigm for foundation model for the brain age
prediction application. NeuroVNN is pre-trained as a regression model on
healthy population to predict chronological age using cortical thickness
features and fine-tuned to estimate brain age in different neurological
contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age
and has a `scale-free' characteristic that allows its transference to datasets
curated according to any arbitrary brain atlas. Our results demonstrate that
NeuroVNN can extract biologically plausible brain age estimates in different
populations, as well as transfer successfully to datasets of dimensionalities
distinct from that for the dataset used to train NeuroVNN.</div><div><a href='http://arxiv.org/abs/2402.07684v1'>2402.07684v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16443v1")'>Evaluating Deep Networks for Detecting User Familiarity with VR from
  Hand Interactions</div>
<div id='2401.16443v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T19:15:24Z</div><div>Authors: Mingjun Li, Numan Zafar, Natasha Kholgade Banerjee, Sean Banerjee</div><div style='padding-top: 10px; width: 80ex'>As VR devices become more prevalent in the consumer space, VR applications
are likely to be increasingly used by users unfamiliar with VR. Detecting the
familiarity level of a user with VR as an interaction medium provides the
potential of providing on-demand training for acclimatization and prevents the
user from being burdened by the VR environment in accomplishing their tasks. In
this work, we present preliminary results of using deep classifiers to conduct
automatic detection of familiarity with VR by using hand tracking of the user
as they interact with a numeric passcode entry panel to unlock a VR door. We
use a VR door as we envision it to the first point of entry to collaborative
virtual spaces, such as meeting rooms, offices, or clinics. Users who are
unfamiliar with VR will have used their hands to open doors with passcode entry
panels in the real world. Thus, while the user may not be familiar with VR,
they would be familiar with the task of opening the door. Using a pilot dataset
consisting of 7 users familiar with VR, and 7 not familiar with VR, we acquire
highest accuracy of 88.03\% when 6 test users, 3 familiar and 3 not familiar,
are evaluated with classifiers trained using data from the remaining 8 users.
Our results indicate potential for using user movement data to detect
familiarity for the simple yet important task of secure passcode-based access.</div><div><a href='http://arxiv.org/abs/2401.16443v1'>2401.16443v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06994v1")'>Physics Sensor Based Deep Learning Fall Detection System</div>
<div id='2403.06994v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T07:50:06Z</div><div>Authors: Zeyuan Qu, Tiange Huang, Yuxin Ji, Yongjun Li</div><div style='padding-top: 10px; width: 80ex'>Fall detection based on embedded sensor is a practical and popular research
direction in recent years. In terms of a specific application: fall detection
methods based upon physics sensors such as [gyroscope and accelerator] have
been exploited using traditional hand crafted features and feed them in machine
learning models like Markov chain or just threshold based classification
methods. In this paper, we build a complete system named TSFallDetect including
data receiving device based on embedded sensor, mobile deep-learning model
deploying platform, and a simple server, which will be used to gather models
and data for future expansion. On the other hand, we exploit the sequential
deep-learning methods to address this falling motion prediction problem based
on data collected by inertial and film pressure sensors. We make a empirical
study based on existing datasets and our datasets collected from our system
separately, which shows that the deep-learning model has more potential
advantage than other traditional methods, and we proposed a new deep-learning
model based on the time series data to predict the fall, and it may be superior
to other sequential models in this particular field.</div><div><a href='http://arxiv.org/abs/2403.06994v1'>2403.06994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06569v2")'>Enhancing Joint Motion Prediction for Individuals with Limb Loss Through
  Model Reprogramming</div>
<div id='2403.06569v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:10:45Z</div><div>Authors: Sharmita Dey, Sarath R. Nair</div><div style='padding-top: 10px; width: 80ex'>Mobility impairment caused by limb loss is a significant challenge faced by
millions of individuals worldwide. The development of advanced assistive
technologies, such as prosthetic devices, has the potential to greatly improve
the quality of life for amputee patients. A critical component in the design of
such technologies is the accurate prediction of reference joint motion for the
missing limb. However, this task is hindered by the scarcity of joint motion
data available for amputee patients, in contrast to the substantial quantity of
data from able-bodied subjects. To overcome this, we leverage deep learning's
reprogramming property to repurpose well-trained models for a new goal without
altering the model parameters. With only data-level manipulation, we adapt
models originally designed for able-bodied people to forecast joint motion in
amputees. The findings in this study have significant implications for
advancing assistive tech and amputee mobility.</div><div><a href='http://arxiv.org/abs/2403.06569v2'>2403.06569v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12397v1")'>Multi-class Temporal Logic Neural Networks</div>
<div id='2402.12397v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:22:29Z</div><div>Authors: Danyang Li, Roberto Tron</div><div style='padding-top: 10px; width: 80ex'>Time-series data can represent the behaviors of autonomous systems, such as
drones and self-driving cars. The problem of binary and multi-class
classification has received a lot of attention in this field. Neural networks
represent a popular approach to classifying data; However, they lack
interpretability, which poses a significant challenge in extracting meaningful
information from them. Signal Temporal Logic (STL) is a formalism to describe
the properties of timed behaviors. We propose a method that combines all of the
above: neural networks that represent STL specifications for multi-class
classification of time-series data. We offer two key contributions: 1) We
introduce a notion of margin for multi-class classification, and 2) we
introduce the use of STL-based attributes for enhancing the interpretability of
the results. We evaluate our method on two datasets and compare with
state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2402.12397v1'>2402.12397v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03714v1")'>Advancing Location-Invariant and Device-Agnostic Motion Activity
  Recognition on Wearable Devices</div>
<div id='2402.03714v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T05:10:00Z</div><div>Authors: Rebecca Adaimi, Abdelkareem Bedri, Jun Gong, Richard Kang, Joanna Arreaza-Taylor, Gerri-Michelle Pascual, Michael Ralph, Gierad Laput</div><div style='padding-top: 10px; width: 80ex'>Wearable sensors have permeated into people's lives, ushering impactful
applications in interactive systems and activity recognition. However,
practitioners face significant obstacles when dealing with sensing
heterogeneities, requiring custom models for different platforms. In this
paper, we conduct a comprehensive evaluation of the generalizability of motion
models across sensor locations. Our analysis highlights this challenge and
identifies key on-body locations for building location-invariant models that
can be integrated on any device. For this, we introduce the largest
multi-location activity dataset (N=50, 200 cumulative hours), which we make
publicly available. We also present deployable on-device motion models reaching
91.41% frame-level F1-score from a single model irrespective of sensor
placements. Lastly, we investigate cross-location data synthesis, aiming to
alleviate the laborious data collection tasks by synthesizing data in one
location given data from another. These contributions advance our vision of
low-barrier, location-invariant activity recognition systems, catalyzing
research in HCI and ubiquitous computing.</div><div><a href='http://arxiv.org/abs/2402.03714v1'>2402.03714v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09452v1")'>Data Distribution Dynamics in Real-World WiFi-Based Patient Activity
  Monitoring for Home Healthcare</div>
<div id='2402.09452v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T08:58:53Z</div><div>Authors: Mahathir Monjur, Jia Liu, Jingye Xu, Yuntong Zhang, Xiaomeng Wang, Chengdong Li, Hyejin Park, Wei Wang, Karl Shieh, Sirajum Munir, Jing Wang, Lixin Song, Shahriar Nirjon</div><div style='padding-top: 10px; width: 80ex'>This paper examines the application of WiFi signals for real-world monitoring
of daily activities in home healthcare scenarios. While the state-of-the-art of
WiFi-based activity recognition is promising in lab environments, challenges
arise in real-world settings due to environmental, subject, and system
configuration variables, affecting accuracy and adaptability. The research
involved deploying systems in various settings and analyzing data shifts. It
aims to guide realistic development of robust, context-aware WiFi sensing
systems for elderly care. The findings suggest a shift in WiFi-based activity
sensing, bridging the gap between academic research and practical applications,
enhancing life quality through technology.</div><div><a href='http://arxiv.org/abs/2402.09452v1'>2402.09452v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05538v1")'>Multi-objective Feature Selection in Remote Health Monitoring
  Applications</div>
<div id='2401.05538v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T20:27:43Z</div><div>Authors: Le Ngu Nguyen, Constantino Álvarez Casado, Manuel Lage Cañellas, Anirban Mukherjee, Nhi Nguyen, Dinesh Babu Jayagopi, Miguel Bordallo López</div><div style='padding-top: 10px; width: 80ex'>Radio frequency (RF) signals have facilitated the development of non-contact
human monitoring tasks, such as vital signs measurement, activity recognition,
and user identification. In some specific scenarios, an RF signal analysis
framework may prioritize the performance of one task over that of others. In
response to this requirement, we employ a multi-objective optimization approach
inspired by biological principles to select discriminative features that
enhance the accuracy of breathing patterns recognition while simultaneously
impeding the identification of individual users. This approach is validated
using a novel vital signs dataset consisting of 50 subjects engaged in four
distinct breathing patterns. Our findings indicate a remarkable result: a
substantial divergence in accuracy between breathing recognition and user
identification. As a complementary viewpoint, we present a contrariwise result
to maximize user identification accuracy and minimize the system's capacity for
breathing activity recognition.</div><div><a href='http://arxiv.org/abs/2401.05538v1'>2401.05538v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02447v1")'>User authentication system based on human exhaled breath physics</div>
<div id='2401.02447v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T19:36:13Z</div><div>Authors: Mukesh Karunanethy, Rahul Tripathi, Mahesh V Panchagnula, Raghunathan Rengaswamy</div><div style='padding-top: 10px; width: 80ex'>This work, in a pioneering approach, attempts to build a biometric system
that works purely based on the fluid mechanics governing exhaled breath. We
test the hypothesis that the structure of turbulence in exhaled human breath
can be exploited to build biometric algorithms. This work relies on the idea
that the extrathoracic airway is unique for every individual, making the
exhaled breath a biomarker. Methods including classical multi-dimensional
hypothesis testing approach and machine learning models are employed in
building user authentication algorithms, namely user confirmation and user
identification. A user confirmation algorithm tries to verify whether a user is
the person they claim to be. A user identification algorithm tries to identify
a user's identity with no prior information available. A dataset of exhaled
breath time series samples from 94 human subjects was used to evaluate the
performance of these algorithms. The user confirmation algorithms performed
exceedingly well for the given dataset with over $97\%$ true confirmation rate.
The machine learning based algorithm achieved a good true confirmation rate,
reiterating our understanding of why machine learning based algorithms
typically outperform classical hypothesis test based algorithms. The user
identification algorithm performs reasonably well with the provided dataset
with over $50\%$ of the users identified as being within two possible suspects.
We show surprisingly unique turbulent signatures in the exhaled breath that
have not been discovered before. In addition to discussions on a novel
biometric system, we make arguments to utilise this idea as a tool to gain
insights into the morphometric variation of extrathoracic airway across
individuals. Such tools are expected to have future potential in the area of
personalised medicines.</div><div><a href='http://arxiv.org/abs/2401.02447v1'>2401.02447v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00889v1")'>Time-bound Contextual Bio-ID Generation for Minimalist Wearables</div>
<div id='2403.00889v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T11:55:37Z</div><div>Authors: Adiba Orzikulova, Diana A. Vasile, Fahim Kawsar, Chulhong Min</div><div style='padding-top: 10px; width: 80ex'>As wearable devices become increasingly miniaturized and powerful, a new
opportunity arises for instant and dynamic device-to-device collaboration and
human-to-device interaction. However, this progress presents a unique
challenge: these minimalist wearables lack inherent mechanisms for real-time
authentication, posing significant risks to data privacy and overall security.
To address this, we introduce Proteus that realizes an innovative concept of
time-bound contextual bio-IDs, which are generated from on-device sensor data
and embedded into a common latent space. These bio-IDs act as a time-bound
unique user identifier that can be used to identify the wearer in a certain
context. Proteus enables dynamic and contextual device collaboration as well as
robust human-to-device interaction. Our evaluations demonstrate the
effectiveness of our method, particularly in the context of minimalist
wearables.</div><div><a href='http://arxiv.org/abs/2403.00889v1'>2403.00889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08962v1")'>DOO-RE: A dataset of ambient sensors in a meeting room for activity
  recognition</div>
<div id='2401.08962v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T04:21:04Z</div><div>Authors: Hyunju Kim, Geon Kim, Taehoon Lee, Kisoo Kim, Dongman Lee</div><div style='padding-top: 10px; width: 80ex'>With the advancement of IoT technology, recognizing user activities with
machine learning methods is a promising way to provide various smart services
to users. High-quality data with privacy protection is essential for deploying
such services in the real world. Data streams from surrounding ambient sensors
are well suited to the requirement. Existing ambient sensor datasets only
support constrained private spaces and those for public spaces have yet to be
explored despite growing interest in research on them. To meet this need, we
build a dataset collected from a meeting room equipped with ambient sensors.
The dataset, DOO-RE, includes data streams from various ambient sensor types
such as Sound and Projector. Each sensor data stream is segmented into activity
units and multiple annotators provide activity labels through a
cross-validation annotation process to improve annotation quality. We finally
obtain 9 types of activities. To our best knowledge, DOO-RE is the first
dataset to support the recognition of both single and group activities in a
real meeting room with reliable annotations.</div><div><a href='http://arxiv.org/abs/2401.08962v1'>2401.08962v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05557v1")'>Re-thinking Human Activity Recognition with Hierarchy-aware Label
  Relationship Modeling</div>
<div id='2403.05557v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T12:23:21Z</div><div>Authors: Jingwei Zuo, Hakim Hacid</div><div style='padding-top: 10px; width: 80ex'>Human Activity Recognition (HAR) has been studied for decades, from data
collection, learning models, to post-processing and result interpretations.
However, the inherent hierarchy in the activities remains relatively
under-explored, despite its significant impact on model performance and
interpretation. In this paper, we propose H-HAR, by rethinking the HAR tasks
from a fresh perspective by delving into their intricate global label
relationships. Rather than building multiple classifiers separately for
multi-layered activities, we explore the efficacy of a flat model enhanced with
graph-based label relationship modeling. Being hierarchy-aware, the graph-based
label modeling enhances the fundamental HAR model, by incorporating intricate
label relationships into the model. We validate the proposal with a multi-label
classifier on complex human activity data. The results highlight the advantages
of the proposal, which can be vertically integrated into advanced HAR models to
further enhance their performances.</div><div><a href='http://arxiv.org/abs/2403.05557v1'>2403.05557v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06586v1")'>ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity
  Recognition Models</div>
<div id='2403.06586v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:32:23Z</div><div>Authors: Luca Arrotta, Claudio Bettini, Gabriele Civitarese, Michele Fiori</div><div style='padding-top: 10px; width: 80ex'>Context-aware Human Activity Recognition (HAR) is a hot research area in
mobile computing, and the most effective solutions in the literature are based
on supervised deep learning models. However, the actual deployment of these
systems is limited by the scarcity of labeled data that is required for
training. Neuro-Symbolic AI (NeSy) provides an interesting research direction
to mitigate this issue, by infusing common-sense knowledge about human
activities and the contexts in which they can be performed into HAR deep
learning classifiers. Existing NeSy methods for context-aware HAR rely on
knowledge encoded in logic-based models (e.g., ontologies) whose design,
implementation, and maintenance to capture new activities and contexts require
significant human engineering efforts, technical knowledge, and domain
expertise. Recent works show that pre-trained Large Language Models (LLMs)
effectively encode common-sense knowledge about human activities. In this work,
we propose ContextGPT: a novel prompt engineering approach to retrieve from
LLMs common-sense knowledge about the relationship between human activities and
the context in which they are performed. Unlike ontologies, ContextGPT requires
limited human effort and expertise. An extensive evaluation carried out on two
public datasets shows how a NeSy model obtained by infusing common-sense
knowledge from ContextGPT is effective in data scarcity scenarios, leading to
similar (and sometimes better) recognition rates than logic-based approaches
with a fraction of the effort.</div><div><a href='http://arxiv.org/abs/2403.06586v1'>2403.06586v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01242v1")'>Augmenting Automation: Intent-Based User Instruction Classification with
  Machine Learning</div>
<div id='2403.01242v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T16:06:03Z</div><div>Authors: Lochan Basyal, Bijay Gaudel</div><div style='padding-top: 10px; width: 80ex'>Electric automation systems offer convenience and efficiency in controlling
electrical circuits and devices. Traditionally, these systems rely on
predefined commands for control, limiting flexibility and adaptability. In this
paper, we propose a novel approach to augment automation by introducing
intent-based user instruction classification using machine learning techniques.
Our system represents user instructions as intents, allowing for dynamic
control of electrical circuits without relying on predefined commands. Through
a machine learning model trained on a labeled dataset of user instructions, our
system classifies intents from user input, enabling a more intuitive and
adaptable control scheme. We present the design and implementation of our
intent-based electric automation system, detailing the development of the
machine learning model for intent classification. Experimental results
demonstrate the effectiveness of our approach in enhancing user experience and
expanding the capabilities of electric automation systems. Our work contributes
to the advancement of smart technologies by providing a more seamless
interaction between users and their environments.</div><div><a href='http://arxiv.org/abs/2403.01242v1'>2403.01242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.00964v1")'>Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human
  Activity Recognition</div>
<div id='2401.00964v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T22:27:59Z</div><div>Authors: Julian Strohmayer, Martin Kampel</div><div style='padding-top: 10px; width: 80ex'>The recognition of human activities based on WiFi Channel State Information
(CSI) enables contactless and visual privacy-preserving sensing in indoor
environments. However, poor model generalization, due to varying environmental
conditions and sensing hardware, is a well-known problem in this space. To
address this issue, in this work, data augmentation techniques commonly used in
image-based learning are applied to WiFi CSI to investigate their effects on
model generalization performance in cross-scenario and cross-system settings.
In particular, we focus on the generalization between line-of-sight (LOS) and
non-line-of-sight (NLOS) through-wall scenarios, as well as on the
generalization between different antenna systems, which remains under-explored.
We collect and make publicly available a dataset of CSI amplitude spectrograms
of human activities. Utilizing this data, an ablation study is conducted in
which activity recognition models based on the EfficientNetV2 architecture are
trained, allowing us to assess the effects of each augmentation on model
generalization performance. The gathered results show that specific
combinations of simple data augmentation techniques applied to CSI amplitude
data can significantly improve cross-scenario and cross-system generalization.</div><div><a href='http://arxiv.org/abs/2401.00964v1'>2401.00964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01388v1")'>Directional Antenna Systems for Long-Range Through-Wall Human Activity
  Recognition</div>
<div id='2401.01388v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T22:35:22Z</div><div>Authors: Julian Strohmayer, Martin Kampel</div><div style='padding-top: 10px; width: 80ex'>WiFi Channel State Information (CSI)-based human activity recognition (HAR)
enables contactless, long-range sensing in spatially constrained environments
while preserving visual privacy. However, despite the presence of numerous
WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of
sensing hardware options. Variants of the Espressif ESP32 have emerged as
potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this
work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for
their ability to facilitate long-range through-wall HAR. Two promising systems
are proposed, one of which combines the ESP32-S3 with a directional biquad
antenna. This combination represents, to the best of our knowledge, the first
demonstration of such a system in WiFi-based HAR. The second system relies on
the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves
directionality through a plane reflector. In a comprehensive evaluation of
line-of-sight (LOS) and non-line-of-sight (NLOS) HAR performance, both systems
are deployed in an office environment spanning a distance of 18 meters across
five rooms. In this experimental setup, the Wallhack1.8k dataset, comprising
1806 CSI amplitude spectrograms of human activities, is collected and made
publicly available. Based on Wallhack1.8k, we train activity recognition models
using the EfficientNetV2 architecture to assess system performance in LOS and
NLOS scenarios. For the core NLOS activity recognition problem, the biquad
antenna and PIFA-based systems achieve accuracies of 92.0$\pm$3.5 and
86.8$\pm$4.7, respectively, demonstrating the feasibility of long-range
through-wall HAR with the proposed systems.</div><div><a href='http://arxiv.org/abs/2401.01388v1'>2401.01388v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17738v1")'>Harnessing Smartwatch Microphone Sensors for Cough Detection and
  Classification</div>
<div id='2401.17738v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T10:58:59Z</div><div>Authors: Pranay Jaiswal, Haroon R. Lone</div><div style='padding-top: 10px; width: 80ex'>This study investigates the potential of using smartwatches with built-in
microphone sensors for monitoring coughs and detecting various cough types. We
conducted a study involving 32 participants and collected 9 hours of audio data
in a controlled manner. Afterward, we processed this data using a structured
approach, resulting in 223 positive cough samples. We further improved the
dataset through augmentation techniques and employed a specialized 1D CNN
model. This model achieved an impressive accuracy rate of 98.49% while
non-walking and 98.2% while walking, showing smartwatches can detect cough.
Moreover, our research successfully identified four distinct types of coughs
using clustering techniques.</div><div><a href='http://arxiv.org/abs/2401.17738v1'>2401.17738v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01229v1")'>REWIND Dataset: Privacy-preserving Speaking Status Segmentation from
  Multimodal Body Movement Signals in the Wild</div>
<div id='2403.01229v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T15:14:58Z</div><div>Authors: Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura Cabrera-Quiros, Hayley Hung</div><div style='padding-top: 10px; width: 80ex'>Recognizing speaking in humans is a central task towards understanding social
interactions. Ideally, speaking would be detected from individual voice
recordings, as done previously for meeting scenarios. However, individual voice
recordings are hard to obtain in the wild, especially in crowded mingling
scenarios due to cost, logistics, and privacy concerns. As an alternative,
machine learning models trained on video and wearable sensor data make it
possible to recognize speech by detecting its related gestures in an
unobtrusive, privacy-preserving way. These models themselves should ideally be
trained using labels obtained from the speech signal. However, existing
mingling datasets do not contain high quality audio recordings. Instead,
speaking status annotations have often been inferred by human annotators from
video, without validation of this approach against audio-based ground truth. In
this paper we revisit no-audio speaking status estimation by presenting the
first publicly available multimodal dataset with high-quality individual speech
recordings of 33 subjects in a professional networking event. We present three
baselines for no-audio speaking status segmentation: a) from video, b) from
body acceleration (chest-worn accelerometer), c) from body pose tracks. In all
cases we predict a 20Hz binary speaking status signal extracted from the audio,
a time resolution not available in previous datasets. In addition to providing
the signals and ground truth necessary to evaluate a wide range of speaking
status detection methods, the availability of audio in REWIND makes it suitable
for cross-modality studies not feasible with previous mingling datasets.
Finally, our flexible data consent setup creates new challenges for multimodal
systems under missing modalities.</div><div><a href='http://arxiv.org/abs/2403.01229v1'>2403.01229v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05849v1")'>Inferring Intentions to Speak Using Accelerometer Data In-the-Wild</div>
<div id='2401.05849v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T11:38:21Z</div><div>Authors: Litian Li, Jord Molhoek, Jing Zhou</div><div style='padding-top: 10px; width: 80ex'>Humans have good natural intuition to recognize when another person has
something to say. It would be interesting if an AI can also recognize
intentions to speak. Especially in scenarios when an AI is guiding a group
discussion, this can be a useful skill. This work studies the inference of
successful and unsuccessful intentions to speak from accelerometer data. This
is chosen because it is privacy-preserving and feasible for in-the-wild
settings since it can be placed in a smart badge. Data from a real-life social
networking event is used to train a machine-learning model that aims to infer
intentions to speak. A subset of unsuccessful intention-to-speak cases in the
data is annotated. The model is trained on the successful intentions to speak
and evaluated on both the successful and unsuccessful cases. In conclusion,
there is useful information in accelerometer data, but not enough to reliably
capture intentions to speak. For example, posture shifts are correlated with
intentions to speak, but people also often shift posture without having an
intention to speak, or have an intention to speak without shifting their
posture. More modalities are likely needed to reliably infer intentions to
speak.</div><div><a href='http://arxiv.org/abs/2401.05849v1'>2401.05849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01703v3")'>A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver
  Interaction in Los Angeles</div>
<div id='2402.01703v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T19:56:20Z</div><div>Authors: Benjamin A. T. Grahama, Lauren Brown, Georgios Chochlakis, Morteza Dehghani, Raquel Delerme, Brittany Friedman, Ellie Graeden, Preni Golazizian, Rajat Hebbar, Parsa Hejabi, Aditya Kommineni, Mayagüez Salinas, Michael Sierra-Arévalo, Jackson Trager, Nicholas Weller, Shrikanth Narayanan</div><div style='padding-top: 10px; width: 80ex'>Interactions between the government officials and civilians affect public
wellbeing and the state legitimacy that is necessary for the functioning of
democratic society. Police officers, the most visible and contacted agents of
the state, interact with the public more than 20 million times a year during
traffic stops. Today, these interactions are regularly recorded by body-worn
cameras (BWCs), which are lauded as a means to enhance police accountability
and improve police-public interactions. However, the timely analysis of these
recordings is hampered by a lack of reliable automated tools that can enable
the analysis of these complex and contested police-public interactions. This
article proposes an approach to developing new multi-perspective, multimodal
machine learning (ML) tools to analyze the audio, video, and transcript
information from this BWC footage. Our approach begins by identifying the
aspects of communication most salient to different stakeholders, including both
community members and police officers. We move away from modeling approaches
built around the existence of a single ground truth and instead utilize new
advances in soft labeling to incorporate variation in how different observers
perceive the same interactions. We argue that this inclusive approach to the
conceptualization and design of new ML tools is broadly applicable to the study
of communication and development of analytic tools across domains of human
interaction, including education, medicine, and the workplace.</div><div><a href='http://arxiv.org/abs/2402.01703v3'>2402.01703v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03040v1")'>AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident
  Analysis</div>
<div id='2401.03040v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T19:33:21Z</div><div>Authors: Kebin Wu, Wenbin Li, Xiaofei Xiao</div><div style='padding-top: 10px; width: 80ex'>Traffic accident analysis is pivotal for enhancing public safety and
developing road regulations. Traditional approaches, although widely used, are
often constrained by manual analysis processes, subjective decisions, uni-modal
outputs, as well as privacy issues related to sensitive data. This paper
introduces the idea of AccidentGPT, a foundation model of traffic accident
analysis, which incorporates multi-modal input data to automatically
reconstruct the accident process video with dynamics details, and furthermore
provide multi-task analysis with multi-modal outputs. The design of the
AccidentGPT is empowered with a multi-modality prompt with feedback for
task-oriented adaptability, a hybrid training schema to leverage labelled and
unlabelled data, and a edge-cloud split configuration for data privacy. To
fully realize the functionalities of this model, we proposes several research
opportunities. This paper serves as the stepping stone to fill the gaps in
traditional approaches of traffic accident analysis and attract the research
community attention for automatic, objective, and privacy-preserving traffic
accident analysis.</div><div><a href='http://arxiv.org/abs/2401.03040v1'>2401.03040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17601v1")'>Advancing sleep detection by modelling weak label sets: A novel weakly
  supervised learning approach</div>
<div id='2402.17601v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T15:30:01Z</div><div>Authors: Matthias Boeker, Vajira Thambawita, Michael Riegler, Pål Halvorsen, Hugo L. Hammer</div><div style='padding-top: 10px; width: 80ex'>Understanding sleep and activity patterns plays a crucial role in physical
and mental health. This study introduces a novel approach for sleep detection
using weakly supervised learning for scenarios where reliable ground truth
labels are unavailable. The proposed method relies on a set of weak labels,
derived from the predictions generated by conventional sleep detection
algorithms. Introducing a novel approach, we suggest a novel generalised
non-linear statistical model in which the number of weak sleep labels is
modelled as outcome of a binomial distribution. The probability of sleep in the
binomial distribution is linked to the outcomes of neural networks trained to
detect sleep based on actigraphy. We show that maximizing the likelihood
function of the model, is equivalent to minimizing the soft cross-entropy loss.
Additionally, we explored the use of the Brier score as a loss function for
weak labels. The efficacy of the suggested modelling framework was demonstrated
using the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trained
on the soft cross-entropy outperformed conventional sleep detection algorithms,
other neural network architectures and loss functions in accuracy and model
calibration. This research not only advances sleep detection techniques in
scenarios where ground truth data is scarce but also contributes to the broader
field of weakly supervised learning by introducing innovative approach in
modelling sets of weak labels.</div><div><a href='http://arxiv.org/abs/2402.17601v1'>2402.17601v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17788v1")'>Multimodal Sleep Apnea Detection with Missing or Noisy Modalities</div>
<div id='2402.17788v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T16:29:36Z</div><div>Authors: Hamed Fayyaz, Abigail Strang, Niharika S. D'Souza, Rahmatollah Beheshti</div><div style='padding-top: 10px; width: 80ex'>Polysomnography (PSG) is a type of sleep study that records multimodal
physiological signals and is widely used for purposes such as sleep staging and
respiratory event detection. Conventional machine learning methods assume that
each sleep study is associated with a fixed set of observed modalities and that
all modalities are available for each sample. However, noisy and missing
modalities are a common issue in real-world clinical settings. In this study,
we propose a comprehensive pipeline aiming to compensate for the missing or
noisy modalities when performing sleep apnea detection. Unlike other existing
studies, our proposed model works with any combination of available modalities.
Our experiments show that the proposed model outperforms other state-of-the-art
approaches in sleep apnea detection using various subsets of available data and
different levels of noise, and maintains its high performance (AUROC&gt;0.9) even
in the presence of high levels of noise or missingness. This is especially
relevant in settings where the level of noise and missingness is high (such as
pediatric or outside-of-clinic scenarios).</div><div><a href='http://arxiv.org/abs/2402.17788v1'>2402.17788v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14827v1")'>Optimizing Uterine Synchronization Analysis in Pregnancy and Labor
  through Window Selection and Node Optimization</div>
<div id='2402.14827v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T17:59:12Z</div><div>Authors: Kamil Bader El Dine, Noujoud Nader, Mohamad Khalil, Catherine Marque</div><div style='padding-top: 10px; width: 80ex'>Preterm labor (PL) has globally become the leading cause of death in children
under the age of 5 years. To address this problem, this paper will provide a
new approach by analyzing the EHG signals, which are recorded on the abdomen of
the mother during labor and pregnancy. The EHG signal reflects the electrical
activity that induces the mechanical contraction of the myometrium. Because
EHGs are known to be non-stationary signals, and because we anticipate
connectivity to alter during contraction, we applied the windowing approach on
real signals to help us identify the best windows and the best nodes with the
most significant data to be used for classification. The suggested pipeline
includes i) divide the 16 EHG signals that are recorded from the abdomen of
pregnant women in N windows; ii) apply the connectivity matrices on each
window; iii) apply the Graph theory-based measures on the connectivity matrices
on each window; iv) apply the consensus Matrix on each window in order to
retrieve the best windows and the best nodes. Following that, several neural
network and machine learning methods are applied to the best windows and best
nodes to categorize pregnancy and labor contractions, based on the different
input parameters (connectivity method alone, connectivity method plus graph
parameters, best nodes, all nodes, best windows, all windows). Results showed
that the best nodes are nodes 8, 9, 10, 11, and 12; while the best windows are
2, 4, and 5. The classification results obtained by using only these best nodes
are better than when using the whole nodes. The results are always better when
using the full burst, whatever the chosen nodes. Thus, the windowing approach
proved to be an innovative technique that can improve the differentiation
between labor and pregnancy EHG signals.</div><div><a href='http://arxiv.org/abs/2402.14827v1'>2402.14827v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07180v2")'>MAGNETO: Edge AI for Human Activity Recognition -- Privacy and
  Personalization</div>
<div id='2402.07180v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T12:29:16Z</div><div>Authors: Jingwei Zuo, George Arvanitakis, Mthandazo Ndhlovu, Hakim Hacid</div><div style='padding-top: 10px; width: 80ex'>Human activity recognition (HAR) is a well-established field, significantly
advanced by modern machine learning (ML) techniques. While companies have
successfully integrated HAR into consumer products, they typically rely on a
predefined activity set, which limits personalizations at the user level (edge
devices). Despite advancements in Incremental Learning for updating models with
new data, this often occurs on the Cloud, necessitating regular data transfers
between cloud and edge devices, thus leading to data privacy issues. In this
paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the
Cloud to the Edge. MAGNETO allows incremental human activity learning directly
on the Edge devices, without any data exchange with the Cloud. This enables
strong privacy guarantees, low processing latency, and a high degree of
personalization for users. In particular, we demonstrate MAGNETO in an Android
device, validating the whole pipeline from data collection to result
visualization.</div><div><a href='http://arxiv.org/abs/2402.07180v2'>2402.07180v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06866v1")'>Health-LLM: Large Language Models for Health Prediction via Wearable
  Sensor Data</div>
<div id='2401.06866v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T19:40:11Z</div><div>Authors: Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, Hae Won Park</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are capable of many natural language tasks, yet
they are far from perfect. In health applications, grounding and interpreting
domain-specific and non-linguistic data is important. This paper investigates
the capacity of LLMs to deliver multi-modal health predictions based on
contextual information (e.g. user demographics, health knowledge) and
physiological data (e.g. resting heart rate, sleep minutes). We present a
comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting
and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps,
GLOBEM, AW_FB, MIT-BIH &amp; MIMIC-III). Our experiments cover thirteen consumer
health prediction tasks in mental health, activity, metabolic, sleep, and
cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable
performance to larger models (GPT-3.5 and GPT-4), achieving the best
performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness
of context enhancement strategies, and generalization capability of the
fine-tuned models across training datasets and the size of training samples.
Notably, we observe that our context enhancement can yield up to 23.8%
improvement in performance. While constructing contextually rich prompts
(combining user context, health knowledge and temporal information) exhibits
synergistic improvement, the inclusion of health knowledge context in prompts
significantly enhances overall performance.</div><div><a href='http://arxiv.org/abs/2401.06866v1'>2401.06866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01133v1")'>Evaluating Large Language Models as Virtual Annotators for Time-series
  Physical Sensing Data</div>
<div id='2403.01133v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T08:29:08Z</div><div>Authors: Aritra Hota, Soumyajit Chatterjee, Sandip Chakraborty</div><div style='padding-top: 10px; width: 80ex'>Traditional human-in-the-loop-based annotation for time-series data like
inertial data often requires access to alternate modalities like video or audio
from the environment. These alternate sources provide the necessary information
to the human annotator, as the raw numeric data is often too obfuscated even
for an expert. However, this traditional approach has many concerns surrounding
overall cost, efficiency, storage of additional modalities, time, scalability,
and privacy. Interestingly, recent large language models (LLMs) are also
trained with vast amounts of publicly available alphanumeric data, which allows
them to comprehend and perform well on tasks beyond natural language
processing. Naturally, this opens up a potential avenue to explore LLMs as
virtual annotators where the LLMs will be directly provided the raw sensor data
for annotation instead of relying on any alternate modality. Naturally, this
could mitigate the problems of the traditional human-in-the-loop approach.
Motivated by this observation, we perform a detailed study in this paper to
assess whether the state-of-the-art (SOTA) LLMs can be used as virtual
annotators for labeling time-series physical sensing data. To perform this in a
principled manner, we segregate the study into two major phases. In the first
phase, we investigate the challenges an LLM like GPT-4 faces in comprehending
raw sensor data. Considering the observations from phase 1, in the next phase,
we investigate the possibility of encoding the raw sensor data using SOTA SSL
approaches and utilizing the projected time-series data to get annotations from
the LLM. Detailed evaluation with four benchmark HAR datasets shows that
SSL-based encoding and metric-based guidance allow the LLM to make more
reasonable decisions and provide accurate annotations without requiring
computationally expensive fine-tuning or sophisticated prompt engineering.</div><div><a href='http://arxiv.org/abs/2403.01133v1'>2403.01133v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06201v1")'>Are You Being Tracked? Discover the Power of Zero-Shot Trajectory
  Tracing with LLMs!</div>
<div id='2403.06201v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T12:50:35Z</div><div>Authors: Huanqi Yang, Sijie Ji, Rucheng Wu, Weitao Xu</div><div style='padding-top: 10px; width: 80ex'>There is a burgeoning discussion around the capabilities of Large Language
Models (LLMs) in acting as fundamental components that can be seamlessly
incorporated into Artificial Intelligence of Things (AIoT) to interpret complex
trajectories. This study introduces LLMTrack, a model that illustrates how LLMs
can be leveraged for Zero-Shot Trajectory Recognition by employing a novel
single-prompt technique that combines role-play and think step-by-step
methodologies with unprocessed Inertial Measurement Unit (IMU) data. We
evaluate the model using real-world datasets designed to challenge it with
distinct trajectories characterized by indoor and outdoor scenarios. In both
test scenarios, LLMTrack not only meets but exceeds the performance benchmarks
set by traditional machine learning approaches and even contemporary
state-of-the-art deep learning models, all without the requirement of training
on specialized datasets. The results of our research suggest that, with
strategically designed prompts, LLMs can tap into their extensive knowledge
base and are well-equipped to analyze raw sensor data with remarkable
effectiveness.</div><div><a href='http://arxiv.org/abs/2403.06201v1'>2403.06201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01748v2")'>Large Multi-Modal Models (LMMs) as Universal Foundation Models for
  AI-Native Wireless Systems</div>
<div id='2402.01748v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T00:21:41Z</div><div>Authors: Shengzhe Xu, Christo Kurisummoottil Thomas, Omar Hashash, Nikhil Muralidhar, Walid Saad, Naren Ramakrishnan</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) and foundation models have been recently touted
as a game-changer for 6G systems. However, recent efforts on LLMs for wireless
networks are limited to a direct application of existing language models that
were designed for natural language processing (NLP) applications. To address
this challenge and create wireless-centric foundation models, this paper
presents a comprehensive vision on how to design universal foundation models
that are tailored towards the deployment of artificial intelligence (AI)-native
networks. Diverging from NLP-based foundation models, the proposed framework
promotes the design of large multi-modal models (LMMs) fostered by three key
capabilities: 1) processing of multi-modal sensing data, 2) grounding of
physical symbol representations in real-world wireless systems using causal
reasoning and retrieval-augmented generation (RAG), and 3) enabling
instructibility from the wireless environment feedback to facilitate dynamic
network adaptation thanks to logical and mathematical reasoning facilitated by
neuro-symbolic AI. In essence, these properties enable the proposed LMM
framework to build universal capabilities that cater to various cross-layer
networking tasks and alignment of intents across different domains. Preliminary
results from experimental evaluation demonstrate the efficacy of grounding
using RAG in LMMs, and showcase the alignment of LMMs with wireless system
designs. Furthermore, the enhanced rationale exhibited in the responses to
mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the
logical and mathematical reasoning capabilities inherent in LMMs. Building on
those results, we present a sequel of open questions and challenges for LMMs.
We then conclude with a set of recommendations that ignite the path towards
LMM-empowered AI-native systems.</div><div><a href='http://arxiv.org/abs/2402.01748v2'>2402.01748v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16858v1")'>Pragmatic Goal-Oriented Communications under Semantic-Effectiveness
  Channel Errors</div>
<div id='2402.16858v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:43:47Z</div><div>Authors: Tomás Hüttebräucker, Mohamed Sana, Emilio Calvanese Strinati</div><div style='padding-top: 10px; width: 80ex'>In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and
goal-oriented communication strategies becomes imperative. This integration
will enable sensing, transmission, and processing of exclusively pertinent task
data, ensuring conveyed information possesses understandable, pragmatic
semantic significance, aligning with destination needs and goals. Without
doubt, no communication is error free. Within this context, besides errors
stemming from typical wireless communication dynamics, potential distortions
between transmitter-intended and receiver-interpreted meanings can emerge due
to limitations in semantic processing capabilities, as well as language and
knowledge representation disparities between transmitters and receivers. The
main contribution of this paper is two-fold. First, it proposes and details a
novel mathematical modeling of errors stemming from language mismatches at both
semantic and effectiveness levels. Second, it provides a novel algorithmic
solution to counteract these types of errors which leverages optimal transport
theory. Our numerical results show the potential of the proposed mechanism to
compensate for language mismatches, thereby enhancing the attainability of
reliable communication under noisy communication environments.</div><div><a href='http://arxiv.org/abs/2402.16858v1'>2402.16858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13841v1")'>Integrating Wearable Sensor Data and Self-reported Diaries for
  Personalized Affect Forecasting</div>
<div id='2403.13841v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T17:24:38Z</div><div>Authors: Zhongqi Yang, Yuning Wang, Ken S. Yamashita, Maryam Sabah, Elahe Khatibi, Iman Azimi, Nikil Dutt, Jessica L. Borelli, Amir M. Rahmani</div><div style='padding-top: 10px; width: 80ex'>Emotional states, as indicators of affect, are pivotal to overall health,
making their accurate prediction before onset crucial. Current studies are
primarily centered on immediate short-term affect detection using data from
wearable and mobile devices. These studies typically focus on objective sensory
measures, often neglecting other forms of self-reported information like
diaries and notes. In this paper, we propose a multimodal deep learning model
for affect status forecasting. This model combines a transformer encoder with a
pre-trained language model, facilitating the integrated analysis of objective
metrics and self-reported diaries. To validate our model, we conduct a
longitudinal study, enrolling college students and monitoring them over a year,
to collect an extensive dataset including physiological, environmental, sleep,
metabolic, and physical activity parameters, alongside open-ended textual
diaries provided by the participants. Our results demonstrate that the proposed
model achieves predictive accuracy of 82.50% for positive affect and 82.76% for
negative affect, a full week in advance. The effectiveness of our model is
further elevated by its explainability.</div><div><a href='http://arxiv.org/abs/2403.13841v1'>2403.13841v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11113v2")'>SleepNet: Attention-Enhanced Robust Sleep Prediction using Dynamic
  Social Networks</div>
<div id='2401.11113v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T04:38:34Z</div><div>Authors: Maryam Khalid, Elizabeth B. Klerman, Andrew W. Mchill, Andrew J. K. Phillips, Akane Sano</div><div style='padding-top: 10px; width: 80ex'>Sleep behavior significantly impacts health and acts as an indicator of
physical and mental well-being. Monitoring and predicting sleep behavior with
ubiquitous sensors may therefore assist in both sleep management and tracking
of related health conditions. While sleep behavior depends on, and is reflected
in the physiology of a person, it is also impacted by external factors such as
digital media usage, social network contagion, and the surrounding weather. In
this work, we propose SleepNet, a system that exploits social contagion in
sleep behavior through graph networks and integrates it with physiological and
phone data extracted from ubiquitous mobile and wearable devices for predicting
next-day sleep labels about sleep duration. Our architecture overcomes the
limitations of large-scale graphs containing connections irrelevant to sleep
behavior by devising an attention mechanism. The extensive experimental
evaluation highlights the improvement provided by incorporating social networks
in the model. Additionally, we conduct robustness analysis to demonstrate the
system's performance in real-life conditions. The outcomes affirm the stability
of SleepNet against perturbations in input data. Further analyses emphasize the
significance of network topology in prediction performance revealing that users
with higher eigenvalue centrality are more vulnerable to data perturbations.</div><div><a href='http://arxiv.org/abs/2401.11113v2'>2401.11113v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02905v1")'>H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network
  Framework for Discovery of Multi-Modal Physiological Responses</div>
<div id='2401.02905v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T17:05:33Z</div><div>Authors: Haidong Gu, Nathan Gaw, Yinan Wang, Chancellor Johnstone, Christine Beauchene, Sophia Yuditskaya, Hrishikesh Rao, Chun-An Chou</div><div style='padding-top: 10px; width: 80ex'>Discovering human cognitive and emotional states using multi-modal
physiological signals draws attention across various research applications.
Physiological responses of the human body are influenced by human cognition and
commonly used to analyze cognitive states. From a network science perspective,
the interactions of these heterogeneous physiological modalities in a graph
structure may provide insightful information to support prediction of cognitive
states. However, there is no clue to derive exact connectivity between
heterogeneous modalities and there exists a hierarchical structure of
sub-modalities. Existing graph neural networks are designed to learn on
non-hierarchical homogeneous graphs with pre-defined graph structures; they
failed to learn from hierarchical, multi-modal physiological data without a
pre-defined graph structure. To this end, we propose a hierarchical
heterogeneous graph generative network (H2G2-Net) that automatically learns a
graph structure without domain knowledge, as well as a powerful representation
on the hierarchical heterogeneous graph in an end-to-end fashion. We validate
the proposed method on the CogPilot dataset that consists of multi-modal
physiological signals. Extensive experiments demonstrate that our proposed
method outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.</div><div><a href='http://arxiv.org/abs/2401.02905v1'>2401.02905v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01138v1")'>Graph Neural Networks in EEG-based Emotion Recognition: A Survey</div>
<div id='2402.01138v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T04:30:58Z</div><div>Authors: Chenyu Liu, Xinliang Zhou, Yihao Wu, Ruizhi Yang, Liming Zhai, Ziyu Jia, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>Compared to other modalities, EEG-based emotion recognition can intuitively
respond to the emotional patterns in the human brain and, therefore, has become
one of the most concerning tasks in the brain-computer interfaces field. Since
dependencies within brain regions are closely related to emotion, a significant
trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion
recognition. However, brain region dependencies in emotional EEG have
physiological bases that distinguish GNNs in this field from those in other
time series fields. Besides, there is neither a comprehensive review nor
guidance for constructing GNNs in EEG-based emotion recognition. In the survey,
our categorization reveals the commonalities and differences of existing
approaches under a unified framework of graph construction. We analyze and
categorize methods from three stages in the framework to provide clear guidance
on constructing GNNs in EEG-based emotion recognition. In addition, we discuss
several open challenges and future directions, such as Temporal full-connected
graph and Graph condensation.</div><div><a href='http://arxiv.org/abs/2402.01138v1'>2402.01138v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05711v1")'>Dynamic Indoor Fingerprinting Localization based on Few-Shot
  Meta-Learning with CSI Images</div>
<div id='2401.05711v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T07:26:32Z</div><div>Authors: Jiyu Jiao, Xiaojun Wang, Chenpei Han, Yuhua Huang, Yizhuo Zhang</div><div style='padding-top: 10px; width: 80ex'>While fingerprinting localization is favored for its effectiveness, it is
hindered by high data acquisition costs and the inaccuracy of static
database-based estimates. Addressing these issues, this letter presents an
innovative indoor localization method using a data-efficient meta-learning
algorithm. This approach, grounded in the ``Learning to Learn'' paradigm of
meta-learning, utilizes historical localization tasks to improve adaptability
and learning efficiency in dynamic indoor environments. We introduce a
task-weighted loss to enhance knowledge transfer within this framework. Our
comprehensive experiments confirm the method's robustness and superiority over
current benchmarks, achieving a notable 23.13\% average gain in Mean Euclidean
Distance, particularly effective in scenarios with limited CSI data.</div><div><a href='http://arxiv.org/abs/2401.05711v1'>2401.05711v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14810v1")'>GeneOH Diffusion: Towards Generalizable Hand-Object Interaction
  Denoising via Denoising Diffusion</div>
<div id='2402.14810v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:59:21Z</div><div>Authors: Xueyi Liu, Li Yi</div><div style='padding-top: 10px; width: 80ex'>In this work, we tackle the challenging problem of denoising hand-object
interactions (HOI). Given an erroneous interaction sequence, the objective is
to refine the incorrect hand trajectory to remove interaction artifacts for a
perceptually realistic sequence. This challenge involves intricate interaction
noise, including unnatural hand poses and incorrect hand-object relations,
alongside the necessity for robust generalization to new interactions and
diverse noise patterns. We tackle those challenges through a novel approach,
GeneOH Diffusion, incorporating two key designs: an innovative contact-centric
HOI representation named GeneOH and a new domain-generalizable denoising
scheme. The contact-centric representation GeneOH informatively parameterizes
the HOI process, facilitating enhanced generalization across various HOI
scenarios. The new denoising scheme consists of a canonical denoising model
trained to project noisy data samples from a whitened noise space to a clean
data manifold and a "denoising via diffusion" strategy which can handle input
trajectories with various noise patterns by first diffusing them to align with
the whitened noise space and cleaning via the canonical denoiser. Extensive
experiments on four benchmarks with significant domain variations demonstrate
the superior effectiveness of our method. GeneOH Diffusion also shows promise
for various downstream applications. Project website:
https://meowuu7.github.io/GeneOH-Diffusion/.</div><div><a href='http://arxiv.org/abs/2402.14810v1'>2402.14810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12689v3")'>Energy-based Automated Model Evaluation</div>
<div id='2401.12689v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T11:54:09Z</div><div>Authors: Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao</div><div style='padding-top: 10px; width: 80ex'>The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE's validity, together with its
superiority compared with prior approaches. We also prove MDE's versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels. Code and data are
available: https://github.com/pengr/Energy_AutoEval</div><div><a href='http://arxiv.org/abs/2401.12689v3'>2401.12689v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15666v1")'>Universal Model in Online Customer Service</div>
<div id='2402.15666v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T00:41:16Z</div><div>Authors: Shu-Ting Pi, Cheng-Ping Hsieh, Qun Liu, Yuying Zhu</div><div style='padding-top: 10px; width: 80ex'>Building machine learning models can be a time-consuming process that often
takes several months to implement in typical business scenarios. To ensure
consistent model performance and account for variations in data distribution,
regular retraining is necessary. This paper introduces a solution for improving
online customer service in e-commerce by presenting a universal model for
predict-ing labels based on customer questions, without requiring training. Our
novel approach involves using machine learning techniques to tag customer
questions in transcripts and create a repository of questions and corresponding
labels. When a customer requests assistance, an information retrieval model
searches the repository for similar questions, and statistical analysis is used
to predict the corresponding label. By eliminating the need for individual
model training and maintenance, our approach reduces both the model development
cycle and costs. The repository only requires periodic updating to maintain
accuracy.</div><div><a href='http://arxiv.org/abs/2402.15666v1'>2402.15666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.18919v2")'>Decompose-and-Compose: A Compositional Approach to Mitigating Spurious
  Correlation</div>
<div id='2402.18919v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T07:24:24Z</div><div>Authors: Fahimeh Hosseini Noohdani, Parsa Hosseini, Aryan Yazdan Parast, Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah</div><div style='padding-top: 10px; width: 80ex'>While standard Empirical Risk Minimization (ERM) training is proven effective
for image classification on in-distribution data, it fails to perform well on
out-of-distribution samples. One of the main sources of distribution shift for
image classification is the compositional nature of images. Specifically, in
addition to the main object or component(s) determining the label, some other
image components usually exist, which may lead to the shift of input
distribution between train and test environments. More importantly, these
components may have spurious correlations with the label. To address this
issue, we propose Decompose-and-Compose (DaC), which improves robustness to
correlation shift by a compositional approach based on combining elements of
images. Based on our observations, models trained with ERM usually highly
attend to either the causal components or the components having a high spurious
correlation with the label (especially in datapoints on which models have a
high confidence). In fact, according to the amount of spurious correlation and
the easiness of classification based on the causal or non-causal components,
the model usually attends to one of these more (on samples with high
confidence). Following this, we first try to identify the causal components of
images using class activation maps of models trained with ERM. Afterward, we
intervene on images by combining them and retraining the model on the augmented
data, including the counterfactual ones. Along with its high interpretability,
this work proposes a group-balancing method by intervening on images without
requiring group labels or information regarding the spurious features during
training. The method has an overall better worst group accuracy compared to
previous methods with the same amount of supervision on the group labels in
correlation shift.</div><div><a href='http://arxiv.org/abs/2402.18919v2'>2402.18919v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13870v1")'>ExMap: Leveraging Explainability Heatmaps for Unsupervised Group
  Robustness to Spurious Correlations</div>
<div id='2403.13870v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T14:47:28Z</div><div>Authors: Rwiddhi Chakraborty, Adrian Sletten, Michael Kampffmeyer</div><div style='padding-top: 10px; width: 80ex'>Group robustness strategies aim to mitigate learned biases in deep learning
models that arise from spurious correlations present in their training
datasets. However, most existing methods rely on the access to the label
distribution of the groups, which is time-consuming and expensive to obtain. As
a result, unsupervised group robustness strategies are sought. Based on the
insight that a trained model's classification strategies can be inferred
accurately based on explainability heatmaps, we introduce ExMap, an
unsupervised two stage mechanism designed to enhance group robustness in
traditional classifiers. ExMap utilizes a clustering module to infer
pseudo-labels based on a model's explainability heatmaps, which are then used
during training in lieu of actual labels. Our empirical studies validate the
efficacy of ExMap - We demonstrate that it bridges the performance gap with its
supervised counterparts and outperforms existing partially supervised and
unsupervised methods. Additionally, ExMap can be seamlessly integrated with
existing group robustness learning strategies. Finally, we demonstrate its
potential in tackling the emerging issue of multiple shortcut
mitigation\footnote{Code available at \url{https://github.com/rwchakra/exmap}}.</div><div><a href='http://arxiv.org/abs/2403.13870v1'>2403.13870v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08732v2")'>Bayes Conditional Distribution Estimation for Knowledge Distillation
  Based on Conditional Mutual Information</div>
<div id='2401.08732v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T16:01:37Z</div><div>Authors: Linfeng Ye, Shayan Mohajer Hamidi, Renhao Tan, En-Hui Yang</div><div style='padding-top: 10px; width: 80ex'>It is believed that in knowledge distillation (KD), the role of the teacher
is to provide an estimate for the unknown Bayes conditional probability
distribution (BCPD) to be used in the student training process. Conventionally,
this estimate is obtained by training the teacher using maximum log-likelihood
(MLL) method. To improve this estimate for KD, in this paper we introduce the
concept of conditional mutual information (CMI) into the estimation of BCPD and
propose a novel estimator called the maximum CMI (MCMI) method. Specifically,
in MCMI estimation, both the log-likelihood and CMI of the teacher are
simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is
further shown that maximizing the teacher's CMI value allows the teacher to
capture more contextual information in an image cluster. Via conducting a
thorough set of experiments, we show that by employing a teacher trained via
MCMI estimation rather than one trained via MLL estimation in various
state-of-the-art KD frameworks, the student's classification accuracy
consistently increases, with the gain of up to 3.32\%. This suggests that the
teacher's BCPD estimate provided by MCMI method is more accurate than that
provided by MLL method. In addition, we show that such improvements in the
student's accuracy are more drastic in zero-shot and few-shot settings.
Notably, the student's accuracy increases with the gain of up to 5.72\% when
5\% of the training samples are available to the student (few-shot), and
increases from 0\% to as high as 84\% for an omitted class (zero-shot). The
code is available at \url{https://github.com/iclr2024mcmi/ICLRMCMI}.</div><div><a href='http://arxiv.org/abs/2401.08732v2'>2401.08732v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08690v1")'>Contrastive Learning with Negative Sampling Correction</div>
<div id='2401.08690v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T11:18:18Z</div><div>Authors: Lu Wang, Chao Du, Pu Zhao, Chuan Luo, Zhangchi Zhu, Bo Qiao, Wei Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</div><div style='padding-top: 10px; width: 80ex'>As one of the most effective self-supervised representation learning methods,
contrastive learning (CL) relies on multiple negative pairs to contrast against
each positive pair. In the standard practice of contrastive learning, data
augmentation methods are utilized to generate both positive and negative pairs.
While existing works have been focusing on improving the positive sampling, the
negative sampling process is often overlooked. In fact, the generated negative
samples are often polluted by positive samples, which leads to a biased loss
and performance degradation. To correct the negative sampling bias, we propose
a novel contrastive learning method named Positive-Unlabeled Contrastive
Learning (PUCL). PUCL treats the generated negative samples as unlabeled
samples and uses information from positive samples to correct bias in
contrastive loss. We prove that the corrected loss used in PUCL only incurs a
negligible bias compared to the unbiased contrastive loss. PUCL can be applied
to general contrastive learning problems and outperforms state-of-the-art
methods on various image and graph classification tasks. The code of PUCL is in
the supplementary file.</div><div><a href='http://arxiv.org/abs/2401.08690v1'>2401.08690v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01448v1")'>ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label
  Visual Classification</div>
<div id='2401.01448v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T22:15:20Z</div><div>Authors: Ahmad Sajedi, Samir Khaki, Yuri A. Lawryshyn, Konstantinos N. Plataniotis</div><div style='padding-top: 10px; width: 80ex'>Multi-label image classification presents a challenging task in many domains,
including computer vision and medical imaging. Recent advancements have
introduced graph-based and transformer-based methods to improve performance and
capture label dependencies. However, these methods often include complex
modules that entail heavy computation and lack interpretability. In this paper,
we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel
framework to address these challenges in multi-label image classification
tasks. Our simple yet effective approach employs supervised contrastive
learning, in which samples that share enough labels with an anchor image based
on a decision threshold are introduced as a positive set. This structure
captures label dependencies by pulling positive pair embeddings together and
pushing away negative samples that fall below the threshold. We enhance
representation learning by incorporating a mixture density network into
contrastive learning and generating Gaussian mixture distributions to explore
the epistemic uncertainty of the feature encoder. We validate the effectiveness
of our framework through experimentation with datasets from the computer vision
and medical imaging domains. Our method outperforms the existing
state-of-the-art methods while achieving a low computational footprint on both
datasets. Visualization analyses also demonstrate that ProbMCL-learned
classifiers maintain a meaningful semantic topology.</div><div><a href='http://arxiv.org/abs/2401.01448v1'>2401.01448v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05458v1")'>CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic
  Dimensionality Guidance</div>
<div id='2401.05458v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T08:10:59Z</div><div>Authors: Dongyu Zhang, Ruofan Hu, Elke Rundensteiner</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks (DNNs) have advanced many machine learning tasks, but
their performance is often harmed by noisy labels in real-world data.
Addressing this, we introduce CoLafier, a novel approach that uses Local
Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier
consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized
classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a
sample's features and its label to predict the label - which allows it to
produce an enhanced internal representation. We observe that LID scores
computed from this representation effectively distinguish between correct and
incorrect labels across various noise scenarios. In contrast to LID-dis,
LID-gen, functioning as a regular classifier, operates solely on the sample's
features. During training, CoLafier utilizes two augmented views per instance
to feed both subnets. CoLafier considers the LID scores from the two views as
produced by LID-dis to assign weights in an adapted loss function for both
subnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels.
LID-dis then processes these pseudo-labels along with two views to derive LID
scores. Finally, these LID scores along with the differences in predictions
from the two subnets guide the label update decisions. This dual-view and
dual-subnet approach enhances the overall reliability of the framework. Upon
completion of the training, we deploy the LID-gen subnet of CoLafier as the
final classification model. CoLafier demonstrates improved prediction accuracy,
surpassing existing methods, particularly under severe label noise. For more
details, see the code at https://github.com/zdy93/CoLafier.</div><div><a href='http://arxiv.org/abs/2401.05458v1'>2401.05458v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16549v2")'>Deep Learning for Multi-Label Learning: A Comprehensive Survey</div>
<div id='2401.16549v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T20:37:03Z</div><div>Authors: Adane Nega Tarekegn, Mohib Ullah, Faouzi Alaya Cheikh</div><div style='padding-top: 10px; width: 80ex'>Multi-label learning is a rapidly growing research area that aims to predict
multiple labels from a single input data point. In the era of big data, tasks
involving multi-label classification (MLC) or ranking present significant and
intricate challenges, capturing considerable attention in diverse domains.
Inherent difficulties in MLC include dealing with high-dimensional data,
addressing label correlations, and handling partial labels, for which
conventional methods prove ineffective. Recent years have witnessed a notable
increase in adopting deep learning (DL) techniques to address these challenges
more effectively in MLC. Notably, there is a burgeoning effort to harness the
robust learning capabilities of DL for improved modelling of label dependencies
and other challenges in MLC. However, it is noteworthy that comprehensive
studies specifically dedicated to DL for multi-label learning are limited.
Thus, this survey aims to thoroughly review recent progress in DL for
multi-label learning, along with a summary of open research problems in MLC.
The review consolidates existing research efforts in DL for MLC,including deep
neural networks, transformers, autoencoders, and convolutional and recurrent
architectures. Finally, the study presents a comparative analysis of the
existing methods to provide insightful observations and stimulate future
research directions in this domain.</div><div><a href='http://arxiv.org/abs/2401.16549v2'>2401.16549v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07603v1")'>ProPML: Probability Partial Multi-label Learning</div>
<div id='2403.07603v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T12:40:23Z</div><div>Authors: Łukasz Struski, Adam Pardyl, Jacek Tabor, Bartosz Zieliński</div><div style='padding-top: 10px; width: 80ex'>Partial Multi-label Learning (PML) is a type of weakly supervised learning
where each training instance corresponds to a set of candidate labels, among
which only some are true. In this paper, we introduce \our{}, a novel
probabilistic approach to this problem that extends the binary cross entropy to
the PML setup. In contrast to existing methods, it does not require suboptimal
disambiguation and, as such, can be applied to any deep architecture.
Furthermore, experiments conducted on artificial and real-world datasets
indicate that \our{} outperforms existing approaches, especially for high noise
in a candidate set.</div><div><a href='http://arxiv.org/abs/2403.07603v1'>2403.07603v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12559v1")'>Confidence Self-Calibration for Multi-Label Class-Incremental Learning</div>
<div id='2403.12559v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T09:14:52Z</div><div>Authors: Kaile Du, Yifan Zhou, Fan Lyu, Yuyang Li, Chen Lu, Guangcan Liu</div><div style='padding-top: 10px; width: 80ex'>The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL)
arises when only the new classes are labeled during training, while past and
future labels remain unavailable. This issue leads to a proliferation of
false-positive errors due to erroneously high confidence multi-label
predictions, exacerbating catastrophic forgetting within the disjoint label
space. In this paper, we aim to refine multi-label confidence calibration in
MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for
label relationship calibration, we introduce a class-incremental graph
convolutional network that bridges the isolated label spaces by constructing
learnable, dynamically extended label relationship graph. Then, for confidence
calibration, we present a max-entropy regularization for each multi-label
increment, facilitating confidence self-calibration through the penalization of
over-confident output distributions. Our approach attains new state-of-the-art
results in MLCIL tasks on both MS-COCO and PASCAL VOC datasets, with the
calibration of label confidences confirmed through our methodology.</div><div><a href='http://arxiv.org/abs/2403.12559v1'>2403.12559v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05945v1")'>Eliminating Information Leakage in Hard Concept Bottleneck Models with
  Supervised, Hierarchical Concept Learning</div>
<div id='2402.05945v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T03:50:58Z</div><div>Authors: Ao Sun, Yuanyuan Yuan, Pingchuan Ma, Shuai Wang</div><div style='padding-top: 10px; width: 80ex'>Concept Bottleneck Models (CBMs) aim to deliver interpretable and
interventionable predictions by bridging features and labels with
human-understandable concepts. While recent CBMs show promising potential, they
suffer from information leakage, where unintended information beyond the
concepts (either when concepts are represented with probabilities or binary
states) are leaked to the subsequent label prediction. Consequently, distinct
classes are falsely classified via indistinguishable concepts, undermining the
interpretation and intervention of CBMs.
  This paper alleviates the information leakage issue by introducing label
supervision in concept predication and constructing a hierarchical concept set.
Accordingly, we propose a new paradigm of CBMs, namely SupCBM, which achieves
label predication via predicted concepts and a deliberately-designed
intervention matrix. SupCBM focuses on concepts that are mostly relevant to the
predicted label and only distinguishes classes when different concepts are
presented. Our evaluations show that SupCBM outperforms SOTA CBMs over diverse
datasets. It also manifests better generality across different backbone models.
With proper quantification of information leakage in different CBMs, we
demonstrate that SupCBM significantly reduces the information leakage.</div><div><a href='http://arxiv.org/abs/2402.05945v1'>2402.05945v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16734v1")'>Investigating the Robustness of Vision Transformers against Label Noise
  in Medical Image Classification</div>
<div id='2402.16734v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T16:53:23Z</div><div>Authors: Bidur Khanal, Prashant Shrestha, Sanskar Amgain, Bishesh Khanal, Binod Bhattarai, Cristian A. Linte</div><div style='padding-top: 10px; width: 80ex'>Label noise in medical image classification datasets significantly hampers
the training of supervised deep learning methods, undermining their
generalizability. The test performance of a model tends to decrease as the
label noise rate increases. Over recent years, several methods have been
proposed to mitigate the impact of label noise in medical image classification
and enhance the robustness of the model. Predominantly, these works have
employed CNN-based architectures as the backbone of their classifiers for
feature extraction. However, in recent years, Vision Transformer (ViT)-based
backbones have replaced CNNs, demonstrating improved performance and a greater
ability to learn more generalizable features, especially when the dataset is
large. Nevertheless, no prior work has rigorously investigated how
transformer-based backbones handle the impact of label noise in medical image
classification. In this paper, we investigate the architectural robustness of
ViT against label noise and compare it to that of CNNs. We use two medical
image classification datasets -- COVID-DU-Ex, and NCT-CRC-HE-100K -- both
corrupted by injecting label noise at various rates. Additionally, we show that
pretraining is crucial for ensuring ViT's improved robustness against label
noise in supervised training.</div><div><a href='http://arxiv.org/abs/2402.16734v1'>2402.16734v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07990v1")'>How does self-supervised pretraining improve robustness against noisy
  labels across various medical image classification datasets?</div>
<div id='2401.07990v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T22:29:23Z</div><div>Authors: Bidur Khanal, Binod Bhattarai, Bishesh Khanal, Cristian Linte</div><div style='padding-top: 10px; width: 80ex'>Noisy labels can significantly impact medical image classification,
particularly in deep learning, by corrupting learned features. Self-supervised
pretraining, which doesn't rely on labeled data, can enhance robustness against
noisy labels. However, this robustness varies based on factors like the number
of classes, dataset complexity, and training size. In medical images, subtle
inter-class differences and modality-specific characteristics add complexity.
Previous research hasn't comprehensively explored the interplay between
self-supervised learning and robustness against noisy labels in medical image
classification, considering all these factors. In this study, we address three
key questions: i) How does label noise impact various medical image
classification datasets? ii) Which types of medical image datasets are more
challenging to learn and more affected by label noise? iii) How do different
self-supervised pretraining methods enhance robustness across various medical
image datasets? Our results show that DermNet, among five datasets (Fetal
plane, DermNet, COVID-DU-Ex, MURA, NCT-CRC-HE-100K), is the most challenging
but exhibits greater robustness against noisy labels. Additionally, contrastive
learning stands out among the eight self-supervised methods as the most
effective approach to enhance robustness against noisy labels.</div><div><a href='http://arxiv.org/abs/2401.07990v1'>2401.07990v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08865v3")'>The Effect of Intrinsic Dataset Properties on Generalization: Unraveling
  Learning Differences Between Natural and Medical Images</div>
<div id='2401.08865v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T22:36:23Z</div><div>Authors: Nicholas Konz, Maciej A. Mazurowski</div><div style='padding-top: 10px; width: 80ex'>This paper investigates discrepancies in how neural networks learn from
different imaging domains, which are commonly overlooked when adopting computer
vision techniques from the domain of natural images to other specialized
domains such as medical images. Recent works have found that the generalization
error of a trained network typically increases with the intrinsic dimension
($d_{data}$) of its training set. Yet, the steepness of this relationship
varies significantly between medical (radiological) and natural imaging
domains, with no existing theoretical explanation. We address this gap in
knowledge by establishing and empirically validating a generalization scaling
law with respect to $d_{data}$, and propose that the substantial scaling
discrepancy between the two considered domains may be at least partially
attributed to the higher intrinsic ``label sharpness'' ($K_\mathcal{F}$) of
medical imaging datasets, a metric which we propose. Next, we demonstrate an
additional benefit of measuring the label sharpness of a training set: it is
negatively correlated with the trained model's adversarial robustness, which
notably leads to models for medical images having a substantially higher
vulnerability to adversarial attack. Finally, we extend our $d_{data}$
formalism to the related metric of learned representation intrinsic dimension
($d_{repr}$), derive a generalization scaling law with respect to $d_{repr}$,
and show that $d_{data}$ serves as an upper bound for $d_{repr}$. Our
theoretical results are supported by thorough experiments with six models and
eleven natural and medical imaging datasets over a range of training set sizes.
Our findings offer insights into the influence of intrinsic dataset properties
on generalization, representation learning, and robustness in deep neural
networks. Code link: https://github.com/mazurowski-lab/intrinsic-properties</div><div><a href='http://arxiv.org/abs/2401.08865v3'>2401.08865v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16005v1")'>Adversarial-Robust Transfer Learning for Medical Imaging via Domain
  Assimilation</div>
<div id='2402.16005v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T06:39:15Z</div><div>Authors: Xiaohui Chen, Tie Luo</div><div style='padding-top: 10px; width: 80ex'>In the field of Medical Imaging, extensive research has been dedicated to
leveraging its potential in uncovering critical diagnostic features in
patients. Artificial Intelligence (AI)-driven medical diagnosis relies on
sophisticated machine learning and deep learning models to analyze, detect, and
identify diseases from medical images. Despite the remarkable performance of
these models, characterized by high accuracy, they grapple with trustworthiness
issues. The introduction of a subtle perturbation to the original image
empowers adversaries to manipulate the prediction output, redirecting it to
other targeted or untargeted classes. Furthermore, the scarcity of publicly
available medical images, constituting a bottleneck for reliable training, has
led contemporary algorithms to depend on pretrained models grounded on a large
set of natural images -- a practice referred to as transfer learning. However,
a significant {\em domain discrepancy} exists between natural and medical
images, which causes AI models resulting from transfer learning to exhibit
heightened {\em vulnerability} to adversarial attacks. This paper proposes a
{\em domain assimilation} approach that introduces texture and color adaptation
into transfer learning, followed by a texture preservation component to
suppress undesired distortion. We systematically analyze the performance of
transfer learning in the face of various adversarial attacks under different
data modalities, with the overarching goal of fortifying the model's robustness
and security in medical imaging tasks. The results demonstrate high
effectiveness in reducing attack efficacy, contributing toward more trustworthy
transfer learning in biomedical applications.</div><div><a href='http://arxiv.org/abs/2402.16005v1'>2402.16005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08768v1")'>Adversarially Robust Feature Learning for Breast Cancer Diagnosis</div>
<div id='2402.08768v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T20:02:34Z</div><div>Authors: Degan Hao, Dooman Arefan, Margarita Zuley, Wendie Berg, Shandong Wu</div><div style='padding-top: 10px; width: 80ex'>Adversarial data can lead to malfunction of deep learning applications. It is
essential to develop deep learning models that are robust to adversarial data
while accurate on standard, clean data. In this study, we proposed a novel
adversarially robust feature learning (ARFL) method for a real-world
application of breast cancer diagnosis. ARFL facilitates adversarial training
using both standard data and adversarial data, where a feature correlation
measure is incorporated as an objective function to encourage learning of
robust features and restrain spurious features. To show the effects of ARFL in
breast cancer diagnosis, we built and evaluated diagnosis models using two
independent clinically collected breast imaging datasets, comprising a total of
9,548 mammogram images. We performed extensive experiments showing that our
method outperformed several state-of-the-art methods and that our method can
enhance safer breast cancer diagnosis against adversarial attacks in clinical
settings.</div><div><a href='http://arxiv.org/abs/2402.08768v1'>2402.08768v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06798v1")'>Dynamic Perturbation-Adaptive Adversarial Training on Medical Image
  Classification</div>
<div id='2403.06798v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:16:20Z</div><div>Authors: Shuai Li, Xiaoguang Ma, Shancheng Jiang, Lu Meng</div><div style='padding-top: 10px; width: 80ex'>Remarkable successes were made in Medical Image Classification (MIC)
recently, mainly due to wide applications of convolutional neural networks
(CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity
with raw data, raising serious concerns on network robustness. Although
adversarial training (AT), in responding to malevolent AEs, was recognized as
an effective approach to improve robustness, it was challenging to overcome
generalization decline of networks caused by the AT. In this paper, in order to
reserve high generalization while improving robustness, we proposed a dynamic
perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a
dynamic learning environment to generate adaptive data-level perturbations and
provided a dynamically updated criterion by loss information collections to
handle the disadvantage of fixed perturbation sizes in conventional AT methods
and the dependence on external transference. Comprehensive testing on
dermatology HAM10000 dataset showed that the DPAAT not only achieved better
robustness improvement and generalization preservation but also significantly
enhanced mean average precision and interpretability on various CNNs,
indicating its great potential as a generic adversarial training method on the
MIC.</div><div><a href='http://arxiv.org/abs/2403.06798v1'>2403.06798v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08692v1")'>Inference Stage Denoising for Undersampled MRI Reconstruction</div>
<div id='2402.08692v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T12:50:10Z</div><div>Authors: Yuyang Xue, Chen Qin, Sotirios A. Tsaftaris</div><div style='padding-top: 10px; width: 80ex'>Reconstruction of magnetic resonance imaging (MRI) data has been positively
affected by deep learning. A key challenge remains: to improve generalisation
to distribution shifts between the training and testing data. Most approaches
aim to address this via inductive design or data augmentation. However, they
can be affected by misleading data, e.g. random noise, and cases where the
inference stage data do not match assumptions in the modelled shifts. In this
work, by employing a conditional hyperparameter network, we eliminate the need
of augmentation, yet maintain robust performance under various levels of
Gaussian noise. We demonstrate that our model withstands various input noise
levels while producing high-definition reconstructions during the test stage.
Moreover, we present a hyperparameter sampling strategy that accelerates the
convergence of training. Our proposed method achieves the highest accuracy and
image quality in all settings compared to baseline methods.</div><div><a href='http://arxiv.org/abs/2402.08692v1'>2402.08692v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05256v1")'>DuDoUniNeXt: Dual-domain unified hybrid model for single and
  multi-contrast undersampled MRI reconstruction</div>
<div id='2403.05256v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T12:26:48Z</div><div>Authors: Ziqi Gao, Yue Zhang, Xinwen Liu, Kaiyan Li, S. Kevin Zhou</div><div style='padding-top: 10px; width: 80ex'>Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to
incorporate a reference image of auxiliary modality to guide the reconstruction
process of the target modality. Known MC reconstruction methods perform well
with a fully sampled reference image, but usually exhibit inferior performance,
compared to single-contrast (SC) methods, when the reference image is missing
or of low quality. To address this issue, we propose DuDoUniNeXt, a unified
dual-domain MRI reconstruction network that can accommodate to scenarios
involving absent, low-quality, and high-quality reference images. DuDoUniNeXt
adopts a hybrid backbone that combines CNN and ViT, enabling specific
adjustment of image domain and k-space reconstruction. Specifically, an
adaptive coarse-to-fine feature fusion module (AdaC2F) is devised to
dynamically process the information from reference images of varying qualities.
Besides, a partially shared shallow feature extractor (PaSS) is proposed, which
uses shared and distinct parameters to handle consistent and discrepancy
information among contrasts. Experimental results demonstrate that the proposed
model surpasses state-of-the-art SC and MC models significantly. Ablation
studies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS,
and the dual-domain unified learning scheme.</div><div><a href='http://arxiv.org/abs/2403.05256v1'>2403.05256v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03551v1")'>Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for
  Gaussian Denoising for the Downstream Task of Image Enhancement</div>
<div id='2403.03551v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:51:09Z</div><div>Authors: Tim Selig, Thomas März, Martin Storath, Andreas Weinmann</div><div style='padding-top: 10px; width: 80ex'>Computed Tomography (CT) is a widely used medical imaging modality, and as it
is based on ionizing radiation, it is desirable to minimize the radiation dose.
However, a reduced radiation dose comes with reduced image quality, and
reconstruction from low-dose CT (LDCT) data is still a challenging task which
is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for
LDCT reconstruction, many state-of-the-art methods use pipelines involving
UNet-type architectures. Specifically the top ranking method, ItNet, employs a
three-stage process involving filtered backprojection (FBP), a UNet trained on
CT data, and an iterative refinement step. In this paper, we propose a less
complex two-stage method. The first stage also employs FBP, while the novelty
lies in the training strategy for the second stage, characterized as the CT
image enhancement stage. The crucial point of our approach is that the neural
network is pretrained on a distinctly different pretraining task with non-CT
data, namely Gaussian noise removal on a variety of natural grayscale images
(photographs). We then fine-tune this network for the downstream task of CT
image enhancement using pairs of LDCT images and corresponding normal-dose CT
images (NDCT). Despite being notably simpler than the state-of-the-art, as the
pretraining did not depend on domain-specific CT data and no further iterative
refinement step was necessary, the proposed two-stage method achieves
competitive results. The proposed method achieves a shared top ranking in the
LoDoPaB-CT challenge and a first position with respect to the SSIM metric.</div><div><a href='http://arxiv.org/abs/2403.03551v1'>2403.03551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16039v1")'>Data-Driven Filter Design in FBP: Transforming CT Reconstruction with
  Trainable Fourier Series</div>
<div id='2401.16039v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T10:47:37Z</div><div>Authors: Yipeng Sun, Linda-Sophie Schneider, Fuxin Fan, Mareike Thies, Mingxuan Gu, Siyuan Mei, Yuzhong Zhou, Siming Bayer, Andreas Maier</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce a Fourier series-based trainable filter for
computed tomography (CT) reconstruction within the filtered backprojection
(FBP) framework. This method overcomes the limitation in noise reduction,
inherent in conventional FBP methods, by optimizing Fourier series coefficients
to construct the filter. This method enables robust performance across
different resolution scales and maintains computational efficiency with minimal
increment for the trainable parameters compared to other deep learning
frameworks. Additionally, we propose Gaussian edge-enhanced (GEE) loss function
that prioritizes the $L_1$ norm of high-frequency magnitudes, effectively
countering the blurring problems prevalent in mean squared error (MSE)
approaches. The model's foundation in the FBP algorithm ensures excellent
interpretability, as it relies on a data-driven filter with all other
parameters derived through rigorous mathematical procedures. Designed as a
plug-and-play solution, our Fourier series-based filter can be easily
integrated into existing CT reconstruction models, making it a versatile tool
for a wide range of practical applications. Our research presents a robust and
scalable method that expands the utility of FBP in both medical and scientific
imaging.</div><div><a href='http://arxiv.org/abs/2401.16039v1'>2401.16039v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03539v1")'>Gadolinium dose reduction for brain MRI using conditional deep learning</div>
<div id='2403.03539v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:35:29Z</div><div>Authors: Thomas Pinetz, Erich Kobler, Robert Haase, Julian A. Luetkens, Mathias Meetschen, Johannes Haubold, Cornelius Deuschl, Alexander Radbruch, Katerina Deike, Alexander Effland</div><div style='padding-top: 10px; width: 80ex'>Recently, deep learning (DL)-based methods have been proposed for the
computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate
adverse side effects while preserving diagnostic value. Currently, the two main
challenges for these approaches are the accurate prediction of contrast
enhancement and the synthesis of realistic images. In this work, we address
both challenges by utilizing the contrast signal encoded in the subtraction
images of pre-contrast and post-contrast image pairs. To avoid the synthesis of
any noise or artifacts and solely focus on contrast signal extraction and
enhancement from low-dose subtraction images, we train our DL model using
noise-free standard-dose subtraction images as targets. As a result, our model
predicts the contrast enhancement signal only; thereby enabling synthesization
of images beyond the standard dose. Furthermore, we adapt the embedding idea of
recent diffusion-based models to condition our model on physical parameters
affecting the contrast enhancement behavior. We demonstrate the effectiveness
of our approach on synthetic and real datasets using various scanners, field
strengths, and contrast agents.</div><div><a href='http://arxiv.org/abs/2403.03539v1'>2403.03539v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13890v1")'>Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion
  Models</div>
<div id='2403.13890v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T18:01:57Z</div><div>Authors: Richard Osuala, Daniel Lang, Preeti Verma, Smriti Joshi, Apostolia Tsirikoglou, Grzegorz Skorupko, Kaisar Kushibar, Lidia Garrucho, Walter H. L. Pinaya, Oliver Diaz, Julia Schnabel, Karim Lekadir</div><div style='padding-top: 10px; width: 80ex'>Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow
to localize tumors and observe their contrast kinetics, which is essential for
cancer characterization and respective treatment decision-making. However,
contrast agent administration is not only associated with adverse health risks,
but also restricted for patients during pregnancy, and for those with kidney
malfunction, or other adverse reactions. With contrast uptake as key biomarker
for lesion malignancy, cancer recurrence risk, and treatment response, it
becomes pivotal to reduce the dependency on intravenous contrast agent
administration. To this end, we propose a multi-conditional latent diffusion
model capable of acquisition time-conditioned image synthesis of DCE-MRI
temporal sequences. To evaluate medical image synthesis, we additionally
propose and validate the Fr\'echet radiomics distance as an image quality
measure based on biomarker variability between synthetic and real imaging data.
Our results demonstrate our method's ability to generate realistic
multi-sequence fat-saturated breast DCE-MRI and uncover the emerging potential
of deep learning based contrast kinetics simulation. We publicly share our
accessible codebase at https://github.com/RichardObi/ccnet.</div><div><a href='http://arxiv.org/abs/2403.13890v1'>2403.13890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01076v1")'>DoseGNN: Improving the Performance of Deep Learning Models in Adaptive
  Dose-Volume Histogram Prediction through Graph Neural Networks</div>
<div id='2402.01076v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T00:28:19Z</div><div>Authors: Zehao Dong, Yixin Chen, Tianyu Zhao</div><div style='padding-top: 10px; width: 80ex'>Dose-Volume Histogram (DVH) prediction is fundamental in radiation therapy
that facilitate treatment planning, dose evaluation, plan comparison and etc.
It helps to increase the ability to deliver precise and effective radiation
treatments while managing potential toxicities to healthy tissues as needed to
reduce the risk of complications. This paper extends recently disclosed
research findings presented on AAPM (AAPM 65th Annual Meeting $\&amp;$ Exhibition)
and includes necessary technique details. The objective is to design efficient
deep learning models for DVH prediction on general radiotherapy platform
equipped with high performance CBCT system, where input CT images and target
dose images to predict may have different origins, spacing and sizes. Deep
learning models widely-adopted in DVH prediction task are evaluated on the
novel radiotherapy platform, and graph neural networks (GNNs) are shown to be
the ideal architecture to construct a plug-and-play framework to improve
predictive performance of base deep learning models in the adaptive setting.</div><div><a href='http://arxiv.org/abs/2402.01076v1'>2402.01076v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00816v1")'>GLIMPSE: Generalized Local Imaging with MLPs</div>
<div id='2401.00816v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T17:15:42Z</div><div>Authors: AmirEhsan Khorashadizadeh, Valentin Debarnot, Tianlin Liu, Ivan Dokmanić</div><div style='padding-top: 10px; width: 80ex'>Deep learning is the current de facto state of the art in tomographic
imaging. A common approach is to feed the result of a simple inversion, for
example the backprojection, to a convolutional neural network (CNN) which then
computes the reconstruction. Despite strong results on 'in-distribution' test
data similar to the training data, backprojection from sparse-view data
delocalizes singularities, so these approaches require a large receptive field
to perform well. As a consequence, they overfit to certain global structures
which leads to poor generalization on out-of-distribution (OOD) samples.
Moreover, their memory complexity and training time scale unfavorably with
image resolution, making them impractical for application at realistic clinical
resolutions, especially in 3D: a standard U-Net requires a substantial 140GB of
memory and 2600 seconds per epoch on a research-grade GPU when training on
1024x1024 images. In this paper, we introduce GLIMPSE, a local processing
neural network for computed tomography which reconstructs a pixel value by
feeding only the measurements associated with the neighborhood of the pixel to
a simple MLP. While achieving comparable or better performance with successful
CNNs like the U-Net on in-distribution test data, GLIMPSE significantly
outperforms them on OOD samples while maintaining a memory footprint almost
independent of image resolution; 5GB memory suffices to train on 1024x1024
images. Further, we built GLIMPSE to be fully differentiable, which enables
feats such as recovery of accurate projection angles if they are out of
calibration.</div><div><a href='http://arxiv.org/abs/2401.00816v1'>2401.00816v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15939v1")'>Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI</div>
<div id='2402.15939v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T23:56:15Z</div><div>Authors: Zi Wang, Min Xiao, Yirong Zhou, Chengyan Wang, Naiming Wu, Yi Li, Yiwen Gong, Shufu Chang, Yinyin Chen, Liuhong Zhu, Jianjun Zhou, Congbo Cai, He Wang, Di Guo, Guang Yang, Xiaobo Qu</div><div style='padding-top: 10px; width: 80ex'>Dynamic magnetic resonance imaging (MRI) plays an indispensable role in
cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled
but the image reconstruction poses a great challenge of high-dimensional
processing. This challenge leads to necessitate extensive training data in many
deep learning reconstruction methods. This work proposes a novel and efficient
approach, leveraging a dimension-reduced separable learning scheme that excels
even with highly limited training data. We further integrate it with
spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning
network (DeepSSL), which unrolls an iteration process of a reconstruction model
with both temporal low-rankness and spatial sparsity. Intermediate outputs are
visualized to provide insights into the network's behavior and enhance its
interpretability. Extensive results on cardiac cine datasets show that the
proposed DeepSSL is superior to the state-of-the-art methods visually and
quantitatively, while reducing the demand for training cases by up to 75%. And
its preliminary adaptability to cardiac patients has been verified through
experienced radiologists' and cardiologists' blind reader study. Additionally,
DeepSSL also benefits for achieving the downstream task of cardiac segmentation
with higher accuracy and shows robustness in prospective real-time cardiac MRI.</div><div><a href='http://arxiv.org/abs/2402.15939v1'>2402.15939v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05024v1")'>A Probabilistic Hadamard U-Net for MRI Bias Field Correction</div>
<div id='2403.05024v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T04:02:34Z</div><div>Authors: Xin Zhu, Hongyi Pan, Yury Velichko, Adam B. Murphy, Ashley Ross, Baris Turkbey, Ahmet Enis Cetin, Ulas Bagci</div><div style='padding-top: 10px; width: 80ex'>Magnetic field inhomogeneity correction remains a challenging task in MRI
analysis. Most established techniques are designed for brain MRI by supposing
that image intensities in the identical tissue follow a uniform distribution.
Such an assumption cannot be easily applied to other organs, especially those
that are small in size and heterogeneous in texture (large variations in
intensity), such as the prostate. To address this problem, this paper proposes
a probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field
correction. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the
low-frequency scalar field, multiplied by the original input to obtain the
prototypical corrected image. HU-Net converts the input image from the time
domain into the frequency domain via Hadamard transform. In the frequency
domain, high-frequency components are eliminated using the trainable filter
(scaling layer), hard-thresholding layer, and sparsity penalty. Next, a
conditional variational autoencoder is used to encode possible bias
field-corrected variants into a low-dimensional latent space. Random samples
drawn from latent space are then incorporated with a prototypical corrected
image to generate multiple plausible images. Experimental results demonstrate
the effectiveness of PHU-Net in correcting bias-field in prostate MRI with a
fast inference speed. It has also been shown that prostate MRI segmentation
accuracy improves with the high-quality corrected images from PHU-Net. The code
will be available in the final version of this manuscript.</div><div><a href='http://arxiv.org/abs/2403.05024v1'>2403.05024v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04484v1")'>Source Matters: Source Dataset Impact on Model Robustness in Medical
  Imaging</div>
<div id='2403.04484v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T13:36:15Z</div><div>Authors: Dovile Juodelyte, Yucheng Lu, Amelia Jiménez-Sánchez, Sabrina Bottazzi, Enzo Ferrante, Veronika Cheplygina</div><div style='padding-top: 10px; width: 80ex'>Transfer learning has become an essential part of medical imaging
classification algorithms, often leveraging ImageNet weights. However, the
domain shift from natural to medical images has prompted alternatives such as
RadImageNet, often demonstrating comparable classification performance.
However, it remains unclear whether the performance gains from transfer
learning stem from improved generalization or shortcut learning. To address
this, we investigate potential confounders -- whether synthetic or sampled from
the data -- across two publicly available chest X-ray and CT datasets. We show
that ImageNet and RadImageNet achieve comparable classification performance,
yet ImageNet is much more prone to overfitting to confounders. We recommend
that researchers using ImageNet-pretrained models reexamine their model
robustness by conducting similar experiments. Our code and experiments are
available at https://github.com/DovileDo/source-matters.</div><div><a href='http://arxiv.org/abs/2403.04484v1'>2403.04484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15534v1")'>DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in
  Chest X-Ray Studies</div>
<div id='2402.15534v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T20:51:37Z</div><div>Authors: Abhieet Parida, Daniel Capellan-Martin, Sara Atito, Muhammad Awais, Maria J. Ledesma-Carbayo, Marius G. Linguraru, Syed Muhammad Anwar</div><div style='padding-top: 10px; width: 80ex'>Chest X-Ray (CXR) is a widely used clinical imaging modality and has a
pivotal role in the diagnosis and prognosis of various lung and heart related
conditions. Conventional automated clinical diagnostic tool design strategies
relying on radiology reads and supervised learning, entail the cumbersome
requirement of high quality annotated training data. To address this challenge,
self-supervised pre-training has proven to outperform supervised pre-training
in numerous downstream vision tasks, representing a significant breakthrough in
the field. However, medical imaging pre-training significantly differs from
pre-training with natural images (e.g., ImageNet) due to unique attributes of
clinical images. In this context, we introduce Diverse Concept Modeling
(DiCoM), a novel self-supervised training paradigm that leverages a student
teacher framework for learning diverse concepts and hence effective
representation of the CXR data. Hence, expanding beyond merely modeling a
single primary label within an image, instead, effectively harnessing the
information from all the concepts inherent in the CXR. The pre-trained model is
subsequently fine-tuned to address diverse domain-specific tasks. Our proposed
paradigm consistently demonstrates robust performance across multiple
downstream tasks on multiple datasets, highlighting the success and
generalizability of the pre-training strategy. To establish the efficacy of our
methods we analyze both the power of learned representations and the speed of
convergence (SoC) of our models. For diverse data and tasks, DiCoM is able to
achieve in most cases better results compared to other state-of-the-art
pre-training strategies. This when combined with the higher SoC and
generalization capabilities positions DiCoM to be established as a foundation
model for CXRs, a widely used imaging modality.</div><div><a href='http://arxiv.org/abs/2402.15534v1'>2402.15534v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14815v1")'>Demographic Bias of Expert-Level Vision-Language Foundation Models in
  Medical Imaging</div>
<div id='2402.14815v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:59:53Z</div><div>Authors: Yuzhe Yang, Yujia Liu, Xin Liu, Avanti Gulhane, Domenico Mastrodicasa, Wei Wu, Edward J Wang, Dushyant W Sahani, Shwetak Patel</div><div style='padding-top: 10px; width: 80ex'>Advances in artificial intelligence (AI) have achieved expert-level
performance in medical imaging applications. Notably, self-supervised
vision-language foundation models can detect a broad spectrum of pathologies
without relying on explicit training annotations. However, it is crucial to
ensure that these AI models do not mirror or amplify human biases, thereby
disadvantaging historically marginalized groups such as females or Black
patients. The manifestation of such biases could systematically delay essential
medical care for certain patient subgroups. In this study, we investigate the
algorithmic fairness of state-of-the-art vision-language foundation models in
chest X-ray diagnosis across five globally-sourced datasets. Our findings
reveal that compared to board-certified radiologists, these foundation models
consistently underdiagnose marginalized groups, with even higher rates seen in
intersectional subgroups, such as Black female patients. Such demographic
biases present over a wide range of pathologies and demographic attributes.
Further analysis of the model embedding uncovers its significant encoding of
demographic information. Deploying AI systems with these biases in medical
imaging can intensify pre-existing care disparities, posing potential
challenges to equitable healthcare access and raising ethical questions about
their clinical application.</div><div><a href='http://arxiv.org/abs/2402.14815v1'>2402.14815v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05606v1")'>A Concept-based Interpretable Model for the Diagnosis of Choroid
  Neoplasias using Multimodal Data</div>
<div id='2403.05606v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T07:15:53Z</div><div>Authors: Yifan Wu, Yang Liu, Yue Yang, Michael S. Yao, Wenli Yang, Xuehui Shi, Lihong Yang, Dongjun Li, Yueming Liu, James C. Gee, Xuan Yang, Wenbin Wei, Shi Gu</div><div style='padding-top: 10px; width: 80ex'>Diagnosing rare diseases presents a common challenge in clinical practice,
necessitating the expertise of specialists for accurate identification. The
advent of machine learning offers a promising solution, while the development
of such technologies is hindered by the scarcity of data on rare conditions and
the demand for models that are both interpretable and trustworthy in a clinical
context. Interpretable AI, with its capacity for human-readable outputs, can
facilitate validation by clinicians and contribute to medical education. In the
current work, we focus on choroid neoplasias, the most prevalent form of eye
cancer in adults, albeit rare with 5.1 per million. We built the so-far largest
dataset consisting of 750 patients, incorporating three distinct imaging
modalities collected from 2004 to 2022. Our work introduces a concept-based
interpretable model that distinguishes between three types of choroidal tumors,
integrating insights from domain experts via radiological reports. Remarkably,
this model not only achieves an F1 score of 0.91, rivaling that of black-box
models, but also boosts the diagnostic accuracy of junior doctors by 42%. This
study highlights the significant potential of interpretable machine learning in
improving the diagnosis of rare diseases, laying a groundwork for future
breakthroughs in medical AI that could tackle a wider array of complex health
scenarios.</div><div><a href='http://arxiv.org/abs/2403.05606v1'>2403.05606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15905v1")'>Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer
  Classification</div>
<div id='2402.15905v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T21:03:30Z</div><div>Authors: Ashfiqun Mustari, Rushmia Ahmed, Afsara Tasnim, Jakia Sultana Juthi, G M Shahariar</div><div style='padding-top: 10px; width: 80ex'>This paper proposes an efficient system for classifying cervical cancer cells
using pre-trained convolutional neural networks (CNNs). We first fine-tune five
pre-trained CNNs and minimize the overall cost of misclassification by
prioritizing accuracy for certain classes that have higher associated costs or
importance. To further enhance the performance of the models, supervised
contrastive learning is included to make the models more adept at capturing
important features and patterns. Extensive experimentation are conducted to
evaluate the proposed system on the SIPaKMeD dataset. The experimental results
demonstrate the effectiveness of the developed system, achieving an accuracy of
97.29%. To make our system more trustworthy, we have employed several
explainable AI techniques to interpret how the models reached a specific
decision. The implementation of the system can be found at -
https://github.com/isha-67/CervicalCancerStudy.</div><div><a href='http://arxiv.org/abs/2402.15905v1'>2402.15905v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.14255v1")'>Interpretable Solutions for Breast Cancer Diagnosis with Grammatical
  Evolution and Data Augmentation</div>
<div id='2401.14255v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T15:45:28Z</div><div>Authors: Yumnah Hasan, Allan de Lima, Fatemeh Amerehi, Darian Reyes Fernandez de Bulnes, Patrick Healy, Conor Ryan</div><div style='padding-top: 10px; width: 80ex'>Medical imaging diagnosis increasingly relies on Machine Learning (ML)
models. This is a task that is often hampered by severely imbalanced datasets,
where positive cases can be quite rare. Their use is further compromised by
their limited interpretability, which is becoming increasingly important. While
post-hoc interpretability techniques such as SHAP and LIME have been used with
some success on so-called black box models, the use of inherently
understandable models makes such endeavors more fruitful. This paper addresses
these issues by demonstrating how a relatively new synthetic data generation
technique, STEM, can be used to produce data to train models produced by
Grammatical Evolution (GE) that are inherently understandable. STEM is a
recently introduced combination of the Synthetic Minority Oversampling
Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously
been successfully used to tackle both between class and within class imbalance
issues. We test our technique on the Digital Database for Screening Mammography
(DDSM) and the Wisconsin Breast Cancer (WBC) datasets and compare Area Under
the Curve (AUC) results with an ensemble of the top three performing
classifiers from a set of eight standard ML classifiers with varying degrees of
interpretability. We demonstrate that the GE-derived models present the best
AUC while still maintaining interpretable solutions.</div><div><a href='http://arxiv.org/abs/2401.14255v1'>2401.14255v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12394v1")'>Improving Model's Interpretability and Reliability using Biomarkers</div>
<div id='2402.12394v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T20:19:28Z</div><div>Authors: Gautam Rajendrakumar Gare, Tom Fox, Beam Chansangavej, Amita Krishnan, Ricardo Luis Rodriguez, Bennett P deBoisblanc, Deva Kannan Ramanan, John Michael Galeotti</div><div style='padding-top: 10px; width: 80ex'>Accurate and interpretable diagnostic models are crucial in the
safety-critical field of medicine. We investigate the interpretability of our
proposed biomarker-based lung ultrasound diagnostic pipeline to enhance
clinicians' diagnostic capabilities. The objective of this study is to assess
whether explanations from a decision tree classifier, utilizing biomarkers, can
improve users' ability to identify inaccurate model predictions compared to
conventional saliency maps. Our findings demonstrate that decision tree
explanations, based on clinically established biomarkers, can assist clinicians
in detecting false positives, thus improving the reliability of diagnostic
models in medicine.</div><div><a href='http://arxiv.org/abs/2402.12394v1'>2402.12394v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11504v1")'>MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray
  Self-Supervised Representation Learning</div>
<div id='2403.11504v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T06:19:37Z</div><div>Authors: Azad Singh, Vandan Gorade, Deepak Mishra</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) is potentially useful in reducing the need for
manual annotation and making deep learning models accessible for medical image
analysis tasks. By leveraging the representations learned from unlabeled data,
self-supervised models perform well on tasks that require little to no
fine-tuning. However, for medical images, like chest X-rays, which are
characterized by complex anatomical structures and diverse clinical conditions,
there arises a need for representation learning techniques that can encode
fine-grained details while preserving the broader contextual information. In
this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration
for Chest X-ray Self-Supervised Representation Learning), an approach to
capture rich representations in the form of embeddings from chest X-ray images.
Central to our approach is a novel multi-level variance and covariance
exploration strategy that empowers the model to detect diagnostically
meaningful patterns while reducing redundancy effectively. By enhancing the
variance and covariance of the learned embeddings, MLVICX promotes the
retention of critical medical insights by adapting both global and local
contextual details. We demonstrate the performance of MLVICX in advancing
self-supervised chest X-ray representation learning through comprehensive
experiments. The performance enhancements we observe across various downstream
tasks highlight the significance of the proposed approach in enhancing the
utility of chest X-ray embeddings for precision medical diagnosis and
comprehensive image analysis. For pertaining, we used the NIH-Chest X-ray
dataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR,
RSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more
than 3% performance gains over SOTA SSL approaches in various downstream tasks.</div><div><a href='http://arxiv.org/abs/2403.11504v1'>2403.11504v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00912v1")'>Can we Constrain Concept Bottleneck Models to Learn Semantically
  Meaningful Input Features?</div>
<div id='2402.00912v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T10:18:43Z</div><div>Authors: Jack Furby, Daniel Cunnington, Dave Braines, Alun Preece</div><div style='padding-top: 10px; width: 80ex'>Concept Bottleneck Models (CBMs) are considered inherently interpretable
because they first predict a set of human-defined concepts before using these
concepts to predict the output of a downstream task. For inherent
interpretability to be fully realised, and ensure trust in a model's output, we
need to guarantee concepts are predicted based on semantically mapped input
features. For example, one might expect the pixels representing a broken bone
in an image to be used for the prediction of a fracture. However, current
literature indicates this is not the case, as concept predictions are often
mapped to irrelevant input features. We hypothesise that this occurs when
concept annotations are inaccurate or how input features should relate to
concepts is unclear. In general, the effect of dataset labelling on concept
representations in CBMs remains an understudied area. Therefore, in this paper,
we examine how CBMs learn concepts from datasets with fine-grained concept
annotations. We demonstrate that CBMs can learn concept representations with
semantic mapping to input features by removing problematic concept
correlations, such as two concepts always appearing together. To support our
evaluation, we introduce a new synthetic image dataset based on a playing cards
domain, which we hope will serve as a benchmark for future CBM research. For
validation, we provide empirical evidence on a real-world dataset of chest
X-rays, to demonstrate semantically meaningful concepts can be learned in
real-world applications.</div><div><a href='http://arxiv.org/abs/2402.00912v1'>2402.00912v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13544v1")'>Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?</div>
<div id='2401.13544v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T16:02:14Z</div><div>Authors: Ričards Marcinkevičs, Sonia Laguna, Moritz Vandenhirtz, Julia E. Vogt</div><div style='padding-top: 10px; width: 80ex'>Recently, interpretable machine learning has re-explored concept bottleneck
models (CBM), comprising step-by-step prediction of the high-level concepts
from the raw features and the target variable from the predicted concepts. A
compelling advantage of this model class is the user's ability to intervene on
the predicted concept values, affecting the model's downstream output. In this
work, we introduce a method to perform such concept-based interventions on
already-trained neural networks, which are not interpretable by design, given
an annotated validation set. Furthermore, we formalise the model's
intervenability as a measure of the effectiveness of concept-based
interventions and leverage this definition to fine-tune black-box models.
Empirically, we explore the intervenability of black-box classifiers on
synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning
improves intervention effectiveness and often yields better-calibrated
predictions. To showcase the practical utility of the proposed techniques, we
apply them to deep chest X-ray classifiers and show that fine-tuned black boxes
can be as intervenable and more performant than CBMs.</div><div><a href='http://arxiv.org/abs/2401.13544v1'>2401.13544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14142v2")'>Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept
  Intervention, and Probabilistic Interpretations</div>
<div id='2401.14142v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T12:46:37Z</div><div>Authors: Xinyue Xu, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li</div><div style='padding-top: 10px; width: 80ex'>Existing methods, such as concept bottleneck models (CBMs), have been
successful in providing concept-based interpretations for black-box deep
learning models. They typically work by predicting concepts given the input and
then predicting the final class label given the predicted concepts. However,
(1) they often fail to capture the high-order, nonlinear interaction between
concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not
help correct highly correlated concepts (e.g., "yellow belly"), leading to
suboptimal final accuracy; (2) they cannot naturally quantify the complex
conditional dependencies between different concepts and class labels (e.g., for
an image with the class label "Kentucky Warbler" and a concept "black bill",
what is the probability that the model correctly predicts another concept
"black crown"), therefore failing to provide deeper insight into how a
black-box model works. In response to these limitations, we propose
Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural
networks to define the joint energy of candidate (input, concept, class)
tuples. With such a unified interface, prediction, concept correction, and
conditional dependency quantification are then represented as conditional
probabilities, which are generated by composing different energy functions. Our
ECBMs address both limitations of existing CBMs, providing higher accuracy and
richer concept interpretations. Empirical results show that our approach
outperforms the state-of-the-art on real-world datasets.</div><div><a href='http://arxiv.org/abs/2401.14142v2'>2401.14142v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15721v2")'>A Study of Acquisition Functions for Medical Imaging Deep Active
  Learning</div>
<div id='2401.15721v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T18:09:02Z</div><div>Authors: Bonaventure F. P. Dossou</div><div style='padding-top: 10px; width: 80ex'>The Deep Learning revolution has enabled groundbreaking achievements in
recent years. From breast cancer detection to protein folding, deep learning
algorithms have been at the core of very important advancements. However, these
modern advancements are becoming more and more data-hungry, especially on
labeled data whose availability is scarce: this is even more prevalent in the
medical context. In this work, we show how active learning could be very
effective in data scarcity situations, where obtaining labeled data (or
annotation budget is very limited). We compare several selection criteria
(BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the
effect of acquired pool size on the model's performance. Our results suggest
that uncertainty is useful to the Melanoma detection task, and confirms the
hypotheses of the author of the paper of interest, that \textit{bald} performs
on average better than other acquisition functions. Our extended analyses
however revealed that all acquisition functions perform badly on the positive
(cancerous) samples, suggesting exploitation of class unbalance, which could be
crucial in real-world settings. We finish by suggesting future work directions
that would be useful to improve this current work. The code of our
implementation is open-sourced at
\url{https://github.com/bonaventuredossou/ece526_course_project}</div><div><a href='http://arxiv.org/abs/2401.15721v2'>2401.15721v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07595v2")'>Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and
  DINOv2 in Medical Imaging Classification</div>
<div id='2402.07595v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:49:08Z</div><div>Authors: Yuning Huang, Jingchen Zou, Lanxi Meng, Xin Yue, Qing Zhao, Jianqiang Li, Changwei Song, Gabriel Jimenez, Shaowu Li, Guanghui Fu</div><div style='padding-top: 10px; width: 80ex'>Medical image analysis frequently encounters data scarcity challenges.
Transfer learning has been effective in addressing this issue while conserving
computational resources. The recent advent of foundational models like the
DINOv2, which uses the vision transformer architecture, has opened new
opportunities in the field and gathered significant interest. However, DINOv2's
performance on clinical data still needs to be verified. In this paper, we
performed a glioma grading task using three clinical modalities of brain MRI
data. We compared the performance of various pre-trained deep learning models,
including those based on ImageNet and DINOv2, in a transfer learning context.
Our focus was on understanding the impact of the freezing mechanism on
performance. We also validated our findings on three other types of public
datasets: chest radiography, fundus radiography, and dermoscopy. Our findings
indicate that in our clinical dataset, DINOv2's performance was not as strong
as ImageNet-based pre-trained models, whereas in public datasets, DINOv2
generally outperformed other models, especially when using the frozen
mechanism. Similar performance was observed with various sizes of DINOv2 models
across different tasks. In summary, DINOv2 is viable for medical image
classification tasks, particularly with data resembling natural images.
However, its effectiveness may vary with data that significantly differs from
natural images such as MRI. In addition, employing smaller versions of the
model can be adequate for medical task, offering resource-saving benefits. Our
codes are available at https://github.com/GuanghuiFU/medical_DINOv2_eval.</div><div><a href='http://arxiv.org/abs/2402.07595v2'>2402.07595v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09471v1")'>Brain Tumor Radiogenomic Classification</div>
<div id='2401.09471v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T10:30:09Z</div><div>Authors: Amr Mohamed, Mahmoud Rabea, Aya Sameh, Ehab Kamal</div><div style='padding-top: 10px; width: 80ex'>The RSNA-MICCAI brain tumor radiogenomic classification challenge aimed to
predict MGMT biomarker status in glioblastoma through binary classification on
Multi parameter mpMRI scans: T1w, T1wCE, T2w and FLAIR. The dataset is splitted
into three main cohorts: training set, validation set which were used during
training, and the testing were only used during final evaluation. Images were
either in a DICOM format or in Png format. different architectures were used to
investigate the problem including the 3D version of Vision Transformer (ViT3D),
ResNet50, Xception and EfficientNet-B3. AUC was used as the main evaluation
metric and the results showed an advantage for both the ViT3D and the Xception
models achieving 0.6015 and 0.61745 respectively on the testing set. compared
to other results, our results proved to be valid given the complexity of the
task. further improvements can be made through exploring different strategies,
different architectures and more diverse datasets.</div><div><a href='http://arxiv.org/abs/2401.09471v1'>2401.09471v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03384v1")'>Survival and grade of the glioma prediction using transfer learning</div>
<div id='2402.03384v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:07:07Z</div><div>Authors: Santiago Valbuena Rubio, María Teresa García-Ordás, Oscar García-Olalla Olivera, Héctor Alaiz-Moretón, Maria-Inmaculada González-Alonso, José Alberto Benítez-Andrades</div><div style='padding-top: 10px; width: 80ex'>Glioblastoma is a highly malignant brain tumor with a life expectancy of only
3 to 6 months without treatment. Detecting and predicting its survival and
grade accurately are crucial. This study introduces a novel approach using
transfer learning techniques. Various pre-trained networks, including
EfficientNet, ResNet, VGG16, and Inception, were tested through exhaustive
optimization to identify the most suitable architecture. Transfer learning was
applied to fine-tune these models on a glioblastoma image dataset, aiming to
achieve two objectives: survival and tumor grade prediction.The experimental
results show 65% accuracy in survival prediction, classifying patients into
short, medium, or long survival categories. Additionally, the prediction of
tumor grade achieved an accuracy of 97%, accurately differentiating low-grade
gliomas (LGG) and high-grade gliomas (HGG). The success of the approach is
attributed to the effectiveness of transfer learning, surpassing the current
state-of-the-art methods. In conclusion, this study presents a promising method
for predicting the survival and grade of glioblastoma. Transfer learning
demonstrates its potential in enhancing prediction models, particularly in
scenarios with limited large datasets. These findings hold promise for
improving diagnostic and treatment approaches for glioblastoma patients.</div><div><a href='http://arxiv.org/abs/2402.03384v1'>2402.03384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15022v2")'>Applications of artificial intelligence in the analysis of
  histopathology images of gliomas: a review</div>
<div id='2401.15022v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T17:29:01Z</div><div>Authors: Jan-Philipp Redlich, Friedrich Feuerhake, Joachim Weis, Nadine S. Schaadt, Sarah Teuber-Hanselmann, Christoph Buck, Sabine Luttmann, Andrea Eberle, Stefan Nikolin, Arno Appenzeller, Andreas Portmann, André Homeyer</div><div style='padding-top: 10px; width: 80ex'>In recent years, the diagnosis of gliomas has become increasingly complex.
Analysis of glioma histopathology images using artificial intelligence (AI)
offers new opportunities to support diagnosis and outcome prediction. To give
an overview of the current state of research, this review examines 70 publicly
available research studies that have proposed AI-based methods for whole-slide
histopathology images of human gliomas, covering the diagnostic tasks of
subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and
survival prediction (27/70). All studies were reviewed with regard to
methodological aspects as well as clinical applicability. It was found that the
focus of current research is the assessment of hematoxylin and eosin-stained
tissue sections of adult-type diffuse gliomas. The majority of studies (49/70)
are based on the publicly available glioblastoma and low-grade glioma datasets
from The Cancer Genome Atlas (TCGA) and only a few studies employed other
datasets in isolation (10/70) or in addition to the TCGA datasets (11/70).
Current approaches mostly rely on convolutional neural networks (53/70) for
analyzing tissue at 20x magnification (30/70). A new field of research is the
integration of clinical data, omics data, or magnetic resonance imaging
(27/70). So far, AI-based methods have achieved promising results, but are not
yet used in real clinical settings. Future work should focus on the independent
validation of methods on larger, multi-site datasets with high-quality and
up-to-date clinical and molecular pathology annotations to demonstrate routine
applicability.</div><div><a href='http://arxiv.org/abs/2401.15022v2'>2401.15022v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00038v2")'>Detecting Brain Tumors through Multimodal Neural Networks</div>
<div id='2402.00038v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T13:06:52Z</div><div>Authors: Antonio Curci, Andrea Esposito</div><div style='padding-top: 10px; width: 80ex'>Tumors can manifest in various forms and in different areas of the human
body. Brain tumors are specifically hard to diagnose and treat because of the
complexity of the organ in which they develop. Detecting them in time can lower
the chances of death and facilitate the therapy process for patients. The use
of Artificial Intelligence (AI) and, more specifically, deep learning, has the
potential to significantly reduce costs in terms of time and resources for the
discovery and identification of tumors from images obtained through imaging
techniques. This research work aims to assess the performance of a multimodal
model for the classification of Magnetic Resonance Imaging (MRI) scans
processed as grayscale images. The results are promising, and in line with
similar works, as the model reaches an accuracy of around 98\%. We also
highlight the need for explainability and transparency to ensure human control
and safety.</div><div><a href='http://arxiv.org/abs/2402.00038v2'>2402.00038v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03302v2")'>Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical
  Images Using YOLOv8 and DeiT</div>
<div id='2401.03302v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T20:53:02Z</div><div>Authors: Seyed Mohammad Hossein Hashemi, Leila Safari, Amirhossein Dadashzade Taromi</div><div style='padding-top: 10px; width: 80ex'>In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.</div><div><a href='http://arxiv.org/abs/2401.03302v2'>2401.03302v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03891v1")'>Joint multi-task learning improves weakly-supervised biomarker
  prediction in computational pathology</div>
<div id='2403.03891v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:51:04Z</div><div>Authors: Omar S. M. El Nahhas, Georg Wölflein, Marta Ligero, Tim Lenz, Marko van Treeck, Firas Khader, Daniel Truhn, Jakob Nikolas Kather</div><div style='padding-top: 10px; width: 80ex'>Deep Learning (DL) can predict biomarkers directly from digitized cancer
histology in a weakly-supervised setting. Recently, the prediction of
continuous biomarkers through regression-based DL has seen an increasing
interest. Nonetheless, clinical decision making often requires a categorical
outcome. Consequently, we developed a weakly-supervised joint multi-task
Transformer architecture which has been trained and evaluated on four public
patient cohorts for the prediction of two key predictive biomarkers,
microsatellite instability (MSI) and homologous recombination deficiency (HRD),
trained with auxiliary regression tasks related to the tumor microenvironment.
Moreover, we perform a comprehensive benchmark of 16 approaches of task
balancing for weakly-supervised joint multi-task learning in computational
pathology. Using our novel approach, we improve over the state-of-the-art area
under the receiver operating characteristic by +7.7% and +4.1%, as well as
yielding better clustering of latent embeddings by +8% and +5% for the
prediction of MSI and HRD in external cohorts, respectively.</div><div><a href='http://arxiv.org/abs/2403.03891v1'>2403.03891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11375v1")'>Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival
  Outcome Prediction</div>
<div id='2403.11375v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T00:02:48Z</div><div>Authors: Hongxiao Wang, Yang Yang, Zhuo Zhao, Pengfei Gu, Nishchal Sapkota, Danny Z. Chen</div><div style='padding-top: 10px; width: 80ex'>For predicting cancer survival outcomes, standard approaches in clinical
research are often based on two main modalities: pathology images for observing
cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene
expressions. However, existing pathology-genomic multi-modal algorithms face
significant challenges: (1) Valuable biological insights regarding genes and
gene-gene interactions are frequently overlooked; (2) one modality often
dominates the optimization process, causing inadequate training for the other
modality. In this paper, we introduce a new multi-modal ``Path-GPTOmic"
framework for cancer survival outcome prediction. First, to extract valuable
biological insights, we regulate the embedding space of a foundation model,
scGPT, initially trained on single-cell RNA-seq data, making it adaptable for
bulk RNA-seq data. Second, to address the imbalance-between-modalities problem,
we propose a gradient modulation mechanism tailored to the Cox partial
likelihood loss for survival prediction. The contributions of the modalities
are dynamically monitored and adjusted during the training process, encouraging
that both modalities are sufficiently trained. Evaluated on two TCGA(The Cancer
Genome Atlas) datasets, our model achieves substantially improved survival
prediction accuracy.</div><div><a href='http://arxiv.org/abs/2403.11375v1'>2403.11375v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10603v1")'>SurvRNC: Learning Ordered Representations for Survival Prediction using
  Rank-N-Contrast</div>
<div id='2403.10603v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T18:00:11Z</div><div>Authors: Numan Saeed, Muhammad Ridzuan, Fadillah Adamsyah Maani, Hussain Alasmawi, Karthik Nandakumar, Mohammad Yaqub</div><div style='padding-top: 10px; width: 80ex'>Predicting the likelihood of survival is of paramount importance for
individuals diagnosed with cancer as it provides invaluable information
regarding prognosis at an early stage. This knowledge enables the formulation
of effective treatment plans that lead to improved patient outcomes. In the
past few years, deep learning models have provided a feasible solution for
assessing medical images, electronic health records, and genomic data to
estimate cancer risk scores. However, these models often fall short of their
potential because they struggle to learn regression-aware feature
representations. In this study, we propose Survival Rank-N Contrast (SurvRNC)
method, which introduces a loss function as a regularizer to obtain an ordered
representation based on the survival times. This function can handle censored
data and can be incorporated into any survival model to ensure that the learned
representation is ordinal. The model was extensively evaluated on a HEad \&amp;
NeCK TumOR (HECKTOR) segmentation and the outcome-prediction task dataset. We
demonstrate that using the SurvRNC method for training can achieve higher
performance on different deep survival models. Additionally, it outperforms
state-of-the-art methods by 3.6% on the concordance index. The code is publicly
available on https://github.com/numanai/SurvRNC</div><div><a href='http://arxiv.org/abs/2403.10603v1'>2403.10603v1</a></div>
</div></div>
    <div><a href="arxiv_15.html">Prev (15)</a></div>
    <div><a href="arxiv_17.html">Next (17)</a></div>
    