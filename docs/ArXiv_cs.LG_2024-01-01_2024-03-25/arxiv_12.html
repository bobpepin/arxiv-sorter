
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07005v1")'>Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines</div>
<div id='2403.07005v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T06:38:22Z</div><div>Authors: Xuejing Zheng, Chao Yu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the cooperative Multi-Agent Reinforcement Learning
(MARL) problems using Reward Machines (RMs) to specify the reward functions
such that the prior knowledge of high-level events in a task can be leveraged
to facilitate the learning efficiency. Unlike the existing work that RMs have
been incorporated into MARL for task decomposition and policy learning in
relatively simple domains or with an assumption of independencies among the
agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs
(MAHRM) that is capable of dealing with more complex scenarios when the events
among agents can occur concurrently and the agents are highly interdependent.
  MAHRM exploits the relationship of high-level events to decompose a task into
a hierarchy of simpler subtasks that are assigned to a small group of agents,
so as to reduce the overall computational complexity.
  Experimental results in three cooperative MARL domains show that MAHRM
outperforms other MARL methods using the same prior knowledge of high-level
events.</div><div><a href='http://arxiv.org/abs/2403.07005v1'>2403.07005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17978v2")'>Imagine, Initialize, and Explore: An Effective Exploration Method in
  Multi-Agent Reinforcement Learning</div>
<div id='2402.17978v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T01:45:01Z</div><div>Authors: Zeyang Liu, Lipeng Wan, Xinrui Yang, Zhuoran Chen, Xingyu Chen, Xuguang Lan</div><div style='padding-top: 10px; width: 80ex'>Effective exploration is crucial to discovering optimal strategies for
multi-agent reinforcement learning (MARL) in complex coordination tasks.
Existing methods mainly utilize intrinsic rewards to enable committed
exploration or use role-based learning for decomposing joint action spaces
instead of directly conducting a collective search in the entire
action-observation space. However, they often face challenges obtaining
specific joint action sequences to reach successful states in long-horizon
tasks. To address this limitation, we propose Imagine, Initialize, and Explore
(IIE), a novel method that offers a promising solution for efficient
multi-agent exploration in complex scenarios. IIE employs a transformer model
to imagine how the agents reach a critical state that can influence each
other's transition functions. Then, we initialize the environment at this state
using a simulator before the exploration phase. We formulate the imagination as
a sequence modeling problem, where the states, observations, prompts, actions,
and rewards are predicted autoregressively. The prompt consists of
timestep-to-go, return-to-go, influence value, and one-shot demonstration,
specifying the desired state and trajectory as well as guiding the action
generation. By initializing agents at the critical states, IIE significantly
increases the likelihood of discovering potentially important under-explored
regions. Despite its simplicity, empirical results demonstrate that our method
outperforms multi-agent exploration baselines on the StarCraft Multi-Agent
Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved
performance in the sparse-reward SMAC tasks and produces more effective
curricula over the initialized states than other generative methods, such as
CVAE-GAN and diffusion models.</div><div><a href='http://arxiv.org/abs/2402.17978v2'>2402.17978v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03741v1")'>SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent
  Reinforcement Learning Systems</div>
<div id='2402.03741v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:18:16Z</div><div>Authors: Oubo Ma, Yuwen Pu, Linkang Du, Yang Dai, Ruo Wang, Xiaolei Liu, Yingcai Wu, Shouling Ji</div><div style='padding-top: 10px; width: 80ex'>Recent advances in multi-agent reinforcement learning (MARL) have opened up
vast application prospects, including swarm control of drones, collaborative
manipulation by robotic arms, and multi-target encirclement. However, potential
security threats during the MARL deployment need more attention and thorough
investigation. Recent researches reveal that an attacker can rapidly exploit
the victim's vulnerabilities and generate adversarial policies, leading to the
victim's failure in specific tasks. For example, reducing the winning rate of a
superhuman-level Go AI to around 20%. They predominantly focus on two-player
competitive environments, assuming attackers possess complete global state
observation.
  In this study, we unveil, for the first time, the capability of attackers to
generate adversarial policies even when restricted to partial observations of
the victims in multi-agent competitive environments. Specifically, we propose a
novel black-box attack (SUB-PLAY), which incorporates the concept of
constructing multiple subgames to mitigate the impact of partial observability
and suggests the sharing of transitions among subpolicies to improve the
exploitative ability of attackers. Extensive evaluations demonstrate the
effectiveness of SUB-PLAY under three typical partial observability
limitations. Visualization results indicate that adversarial policies induce
significantly different activations of the victims' policy networks.
Furthermore, we evaluate three potential defenses aimed at exploring ways to
mitigate security threats posed by adversarial policies, providing constructive
recommendations for deploying MARL in competitive environments.</div><div><a href='http://arxiv.org/abs/2402.03741v1'>2402.03741v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13460v1")'>Multi-Agent Diagnostics for Robustness via Illuminated Diversity</div>
<div id='2401.13460v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T14:02:09Z</div><div>Authors: Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder, Tim Rockt√§schel</div><div style='padding-top: 10px; width: 80ex'>In the rapidly advancing field of multi-agent systems, ensuring robustness in
unfamiliar and adversarial settings is crucial. Notwithstanding their
outstanding performance in familiar environments, these systems often falter in
new situations due to overfitting during the training phase. This is especially
pronounced in settings where both cooperative and competitive behaviours are
present, encapsulating a dual nature of overfitting and generalisation
challenges. To address this issue, we present Multi-Agent Diagnostics for
Robustness via Illuminated Diversity (MADRID), a novel approach for generating
diverse adversarial scenarios that expose strategic vulnerabilities in
pre-trained multi-agent policies. Leveraging the concepts from open-ended
learning, MADRID navigates the vast space of adversarial settings, employing a
target policy's regret to gauge the vulnerabilities of these settings. We
evaluate the effectiveness of MADRID on the 11vs11 version of Google Research
Football, one of the most complex environments for multi-agent reinforcement
learning. Specifically, we employ MADRID for generating a diverse array of
adversarial settings for TiZero, the state-of-the-art approach which "masters"
the game through 45 days of training on a large-scale distributed
infrastructure. We expose key shortcomings in TiZero's tactical
decision-making, underlining the crucial importance of rigorous evaluation in
multi-agent systems.</div><div><a href='http://arxiv.org/abs/2401.13460v1'>2401.13460v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11114v1")'>Phasic Diversity Optimization for Population-Based Reinforcement
  Learning</div>
<div id='2403.11114v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T06:41:09Z</div><div>Authors: Jingcheng Jiang, Haiyin Piao, Yu Fu, Yihang Hao, Chuanlu Jiang, Ziqi Wei, Xin Yang</div><div style='padding-top: 10px; width: 80ex'>Reviewing the previous work of diversity Rein-forcement Learning,diversity is
often obtained via an augmented loss function,which requires a balance between
reward and diversity.Generally,diversity optimization algorithms use
Multi-armed Bandits algorithms to select the coefficient in the pre-defined
space. However, the dynamic distribution of reward signals for MABs or the
conflict between quality and diversity limits the performance of these methods.
We introduce the Phasic Diversity Optimization (PDO) algorithm, a
Population-Based Training framework that separates reward and diversity
training into distinct phases instead of optimizing a multi-objective function.
In the auxiliary phase, agents with poor performance diversified via
determinants will not replace the better agents in the archive. The decoupling
of reward and diversity allows us to use an aggressive diversity optimization
in the auxiliary phase without performance degradation. Furthermore, we
construct a dogfight scenario for aerial agents to demonstrate the practicality
of the PDO algorithm. We introduce two implementations of PDO archive and
conduct tests in the newly proposed adversarial dogfight and MuJoCo
simulations. The results show that our proposed algorithm achieves better
performance than baselines.</div><div><a href='http://arxiv.org/abs/2403.11114v1'>2403.11114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03172v1")'>Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning
  with Goal Imagination</div>
<div id='2403.03172v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:07:34Z</div><div>Authors: Liangzhou Wang, Kaiwen Zhu, Fengming Zhu, Xinghu Yao, Shujie Zhang, Deheng Ye, Haobo Fu, Qiang Fu, Wei Yang</div><div style='padding-top: 10px; width: 80ex'>Reaching consensus is key to multi-agent coordination. To accomplish a
cooperative task, agents need to coherently select optimal joint actions to
maximize the team reward. However, current cooperative multi-agent
reinforcement learning (MARL) methods usually do not explicitly take consensus
into consideration, which may cause miscoordination problem. In this paper, we
propose a model-based consensus mechanism to explicitly coordinate multiple
agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides
agents to reach consensus with an Imagined common goal. The common goal is an
achievable state with high value, which is obtained by sampling from the
distribution of future states. We directly model this distribution with a
self-supervised generative model, thus alleviating the "curse of dimensinality"
problem induced by multi-agent multi-step policy rollout commonly used in
model-based methods. We show that such efficient consensus mechanism can guide
all agents cooperatively reaching valuable future states. Results on
Multi-agent Particle-Environments and Google Research Football environment
demonstrate the superiority of MAGI in both sample efficiency and performance.</div><div><a href='http://arxiv.org/abs/2403.03172v1'>2403.03172v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08184v1")'>Enabling Multi-Agent Transfer Reinforcement Learning via Scenario
  Independent Representation</div>
<div id='2402.08184v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T02:48:18Z</div><div>Authors: Ayesha Siddika Nipu, Siming Liu, Anthony Harris</div><div style='padding-top: 10px; width: 80ex'>Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in
tackling complex tasks that require collaboration and competition among agents
in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch
is arduous and may not always be feasible, particularly for MASs with a large
number of interactive agents due to the extensive sample complexity. Therefore,
reusing knowledge gained from past experiences or other agents could
efficiently accelerate the learning process and upscale MARL algorithms. In
this study, we introduce a novel framework that enables transfer learning for
MARL through unifying various state spaces into fixed-size inputs that allow
one unified deep-learning policy viable in different scenarios within a MAS. We
evaluated our approach in a range of scenarios within the StarCraft Multi-Agent
Challenge (SMAC) environment, and the findings show significant enhancements in
multi-agent learning performance using maneuvering skills learned from other
scenarios compared to agents learning from scratch. Furthermore, we adopted
Curriculum Transfer Learning (CTL), enabling our deep learning policy to
progressively acquire knowledge and skills across pre-designed homogeneous
learning scenarios organized by difficulty levels. This process promotes inter-
and intra-agent knowledge transfer, leading to high multi-agent learning
performance in more complicated heterogeneous scenarios.</div><div><a href='http://arxiv.org/abs/2402.08184v1'>2402.08184v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07890v1")'>MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement
  Learning</div>
<div id='2402.07890v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:53:20Z</div><div>Authors: Ayesha Siddika Nipu, Siming Liu, Anthony Harris</div><div style='padding-top: 10px; width: 80ex'>Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.</div><div><a href='http://arxiv.org/abs/2402.07890v1'>2402.07890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02097v1")'>Settling Decentralized Multi-Agent Coordinated Exploration by Novelty
  Sharing</div>
<div id='2402.02097v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T09:35:25Z</div><div>Authors: Haobin Jiang, Ziluo Ding, Zongqing Lu</div><div style='padding-top: 10px; width: 80ex'>Exploration in decentralized cooperative multi-agent reinforcement learning
faces two challenges. One is that the novelty of global states is unavailable,
while the novelty of local observations is biased. The other is how agents can
explore in a coordinated way. To address these challenges, we propose MACE, a
simple yet effective multi-agent coordinated exploration method. By
communicating only local novelty, agents can take into account other agents'
local novelty to approximate the global novelty. Further, we newly introduce
weighted mutual information to measure the influence of one agent's action on
other agents' accumulated novelty. We convert it as an intrinsic reward in
hindsight to encourage agents to exert more influence on other agents'
exploration and boost coordinated exploration. Empirically, we show that MACE
achieves superior performance in three multi-agent environments with sparse
rewards.</div><div><a href='http://arxiv.org/abs/2402.02097v1'>2402.02097v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04934v1")'>Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A
  Survey</div>
<div id='2401.04934v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T05:07:42Z</div><div>Authors: Jiechuan Jiang, Kefan Su, Zongqing Lu</div><div style='padding-top: 10px; width: 80ex'>Cooperative multi-agent reinforcement learning is a powerful tool to solve
many real-world cooperative tasks, but restrictions of real-world applications
may require training the agents in a fully decentralized manner. Due to the
lack of information about other agents, it is challenging to derive algorithms
that can converge to the optimal joint policy in a fully decentralized setting.
Thus, this research area has not been thoroughly studied. In this paper, we
seek to systematically review the fully decentralized methods in two settings:
maximizing a shared reward of all agents and maximizing the sum of individual
rewards of all agents, and discuss open questions and future research
directions.</div><div><a href='http://arxiv.org/abs/2401.04934v1'>2401.04934v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15059v1")'>Fully Independent Communication in Multi-Agent Reinforcement Learning</div>
<div id='2401.15059v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T18:42:01Z</div><div>Authors: Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu</div><div style='padding-top: 10px; width: 80ex'>Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.</div><div><a href='http://arxiv.org/abs/2401.15059v1'>2401.15059v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06750v1")'>Generalising Multi-Agent Cooperation through Task-Agnostic Communication</div>
<div id='2403.06750v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T14:20:13Z</div><div>Authors: Dulhan Jayalath, Steven Morad, Amanda Prorok</div><div style='padding-top: 10px; width: 80ex'>Existing communication methods for multi-agent reinforcement learning (MARL)
in cooperative multi-robot problems are almost exclusively task-specific,
training new communication strategies for each unique task. We address this
inefficiency by introducing a communication strategy applicable to any task
within a given environment. We pre-train the communication strategy without
task-specific reward guidance in a self-supervised manner using a set
autoencoder. Our objective is to learn a fixed-size latent Markov state from a
variable number of agent observations. Under mild assumptions, we prove that
policies using our latent representations are guaranteed to converge, and upper
bound the value error introduced by our Markov state approximation. Our method
enables seamless adaptation to novel tasks without fine-tuning the
communication strategy, gracefully supports scaling to more agents than present
during training, and detects out-of-distribution events in an environment.
Empirical results on diverse MARL scenarios validate the effectiveness of our
approach, surpassing task-specific communication strategies in unseen tasks.
Our implementation of this work is available at
https://github.com/proroklab/task-agnostic-comms.</div><div><a href='http://arxiv.org/abs/2403.06750v1'>2403.06750v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06535v1")'>Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning</div>
<div id='2403.06535v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T09:21:11Z</div><div>Authors: Shuo Tang, Rui Ye, Chenxin Xu, Xiaowen Dong, Siheng Chen, Yanfeng Wang</div><div style='padding-top: 10px; width: 80ex'>Decentralized and lifelong-adaptive multi-agent collaborative learning aims
to enhance collaboration among multiple agents without a central server, with
each agent solving varied tasks over time. To achieve efficient collaboration,
agents should: i) autonomously identify beneficial collaborative relationships
in a decentralized manner; and ii) adapt to dynamically changing task
observations. In this paper, we propose DeLAMA, a decentralized multi-agent
lifelong collaborative learning algorithm with dynamic collaboration graphs. To
promote autonomous collaboration relationship learning, we propose a
decentralized graph structure learning algorithm, eliminating the need for
external priors. To facilitate adaptation to dynamic tasks, we design a memory
unit to capture the agents' accumulated learning history and knowledge, while
preserving finite storage consumption. To further augment the system's
expressive capabilities and computational efficiency, we apply algorithm
unrolling, leveraging the advantages of both mathematical optimization and
neural networks. This allows the agents to `learn to collaborate' through the
supervision of training tasks. Our theoretical analysis verifies that
inter-agent collaboration is communication efficient under a small number of
communication rounds. The experimental results verify its ability to facilitate
the discovery of collaboration strategies and adaptation to dynamic learning
scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in
classification accuracy. We expect our work can serve as a foundational
technique to facilitate future works towards an intelligent, decentralized, and
dynamic multi-agent system. Code is available at
https://github.com/ShuoTang123/DeLAMA.</div><div><a href='http://arxiv.org/abs/2403.06535v1'>2403.06535v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15259v1")'>Open Ad Hoc Teamwork with Cooperative Game Theory</div>
<div id='2402.15259v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T11:04:33Z</div><div>Authors: Jianhong Wang, Yang Li, Yuan Zhang, Wei Pan, Samuel Kaski</div><div style='padding-top: 10px; width: 80ex'>Ad hoc teamwork poses a challenging problem, requiring the design of an agent
to collaborate with teammates without prior coordination or joint training.
Open ad hoc teamwork further complicates this challenge by considering
environments with a changing number of teammates, referred to as open teams.
The state-of-the-art solution to this problem is graph-based policy learning
(GPL), leveraging the generalizability of graph neural networks to handle an
unrestricted number of agents and effectively address open teams. GPL's
performance is superior to other methods, but its joint Q-value representation
presents challenges for interpretation, hindering further development of this
research line and applicability. In this paper, we establish a new theory to
give an interpretation for the joint Q-value representation employed in GPL,
from the perspective of cooperative game theory. Building on our theory, we
propose a novel algorithm based on GPL framework, to complement the critical
features that facilitate learning, but overlooked in GPL. Through experiments,
we demonstrate the correctness of our theory by comparing the performance of
the resulting algorithm with GPL in dynamic team compositions.</div><div><a href='http://arxiv.org/abs/2402.15259v1'>2402.15259v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09984v1")'>Symmetry-Breaking Augmentations for Ad Hoc Teamwork</div>
<div id='2402.09984v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:49:28Z</div><div>Authors: Ravi Hammond, Dustin Craggs, Mingyu Guo, Jakob Foerster, Ian Reid</div><div style='padding-top: 10px; width: 80ex'>In many collaborative settings, artificial intelligence (AI) agents must be
able to adapt to new teammates that use unknown or previously unobserved
strategies. While often simple for humans, this can be challenging for AI
agents. For example, if an AI agent learns to drive alongside others (a
training set) that only drive on one side of the road, it may struggle to adapt
this experience to coordinate with drivers on the opposite side, even if their
behaviours are simply flipped along the left-right symmetry. To address this we
introduce symmetry-breaking augmentations (SBA), which increases diversity in
the behaviour of training teammates by applying a symmetry-flipping operation.
By learning a best-response to the augmented set of teammates, our agent is
exposed to a wider range of behavioural conventions, improving performance when
deployed with novel teammates. We demonstrate this experimentally in two
settings, and show that our approach improves upon previous ad hoc teamwork
results in the challenging card game Hanabi. We also propose a general metric
for estimating symmetry-dependency amongst a given set of policies.</div><div><a href='http://arxiv.org/abs/2402.09984v1'>2402.09984v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10006v1")'>Graph Enhanced Reinforcement Learning for Effective Group Formation in
  Collaborative Problem Solving</div>
<div id='2403.10006v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T04:04:40Z</div><div>Authors: Zheng Fang, Fucai Ke, Jae Young Han, Zhijie Feng, Toby Cai</div><div style='padding-top: 10px; width: 80ex'>This study addresses the challenge of forming effective groups in
collaborative problem-solving environments. Recognizing the complexity of human
interactions and the necessity for efficient collaboration, we propose a novel
approach leveraging graph theory and reinforcement learning. Our methodology
involves constructing a graph from a dataset where nodes represent
participants, and edges signify the interactions between them. We conceptualize
each participant as an agent within a reinforcement learning framework, aiming
to learn an optimal graph structure that reflects effective group dynamics.
Clustering techniques are employed to delineate clear group structures based on
the learned graph. Our approach provides theoretical solutions based on
evaluation metrics and graph measurements, offering insights into potential
improvements in group effectiveness and reductions in conflict incidences. This
research contributes to the fields of collaborative work and educational
psychology by presenting a data-driven, analytical approach to group formation.
It has practical implications for organizational team building, classroom
settings, and any collaborative scenario where group dynamics are crucial. The
study opens new avenues for exploring the application of graph theory and
reinforcement learning in social and behavioral sciences, highlighting the
potential for empirical validation in future work.</div><div><a href='http://arxiv.org/abs/2403.10006v1'>2403.10006v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10794v1")'>Diffusion-Reinforcement Learning Hierarchical Motion Planning in
  Adversarial Multi-agent Games</div>
<div id='2403.10794v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T03:53:55Z</div><div>Authors: Zixuan Wu, Sean Ye, Manisha Natarajan, Matthew C. Gombolay</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning- (RL-)based motion planning has recently shown the
potential to outperform traditional approaches from autonomous navigation to
robot manipulation. In this work, we focus on a motion planning task for an
evasive target in a partially observable multi-agent adversarial
pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to
various applications, such as search and rescue operations and surveillance
robots, where robots must effectively plan their actions to gather intelligence
or accomplish mission tasks while avoiding detection or capture themselves. We
propose a hierarchical architecture that integrates a high-level diffusion
model to plan global paths responsive to environment data while a low-level RL
algorithm reasons about evasive versus global path-following behavior. Our
approach outperforms baselines by 51.2% by leveraging the diffusion model to
guide the RL algorithm for more efficient exploration and improves the
explanability and predictability.</div><div><a href='http://arxiv.org/abs/2403.10794v1'>2403.10794v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02914v1")'>A unified uncertainty-aware exploration: Combining epistemic and
  aleatory uncertainty</div>
<div id='2401.02914v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T17:39:00Z</div><div>Authors: Parvin Malekzadeh, Ming Hou, Konstantinos N. Plataniotis</div><div style='padding-top: 10px; width: 80ex'>Exploration is a significant challenge in practical reinforcement learning
(RL), and uncertainty-aware exploration that incorporates the quantification of
epistemic and aleatory uncertainty has been recognized as an effective
exploration strategy. However, capturing the combined effect of aleatory and
epistemic uncertainty for decision-making is difficult. Existing works estimate
aleatory and epistemic uncertainty separately and consider the composite
uncertainty as an additive combination of the two. Nevertheless, the additive
formulation leads to excessive risk-taking behavior, causing instability. In
this paper, we propose an algorithm that clarifies the theoretical connection
between aleatory and epistemic uncertainty, unifies aleatory and epistemic
uncertainty estimation, and quantifies the combined effect of both
uncertainties for a risk-sensitive exploration. Our method builds on a novel
extension of distributional RL that estimates a parameterized return
distribution whose parameters are random variables encoding epistemic
uncertainty. Experimental results on tasks with exploration and risk challenges
show that our method outperforms alternative approaches.</div><div><a href='http://arxiv.org/abs/2401.02914v1'>2401.02914v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18910v1")'>DIGIC: Domain Generalizable Imitation Learning by Causal Discovery</div>
<div id='2402.18910v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T07:09:01Z</div><div>Authors: Yang Chen, Yitao Liang, Zhouchen Lin</div><div style='padding-top: 10px; width: 80ex'>Causality has been combined with machine learning to produce robust
representations for domain generalization. Most existing methods of this type
require massive data from multiple domains to identify causal features by
cross-domain variations, which can be expensive or even infeasible and may lead
to misidentification in some cases. In this work, we make a different attempt
by leveraging the demonstration data distribution to discover the causal
features for a domain generalizable policy. We design a novel framework, called
DIGIC, to identify the causal features by finding the direct cause of the
expert action from the demonstration data distribution via causal discovery.
Our framework can achieve domain generalizable imitation learning with only
single-domain data and serve as a complement for cross-domain variation-based
methods under non-structural assumptions on the underlying causal models. Our
empirical study in various control tasks shows that the proposed framework
evidently improves the domain generalization performance and has comparable
performance to the expert in the original domain simultaneously.</div><div><a href='http://arxiv.org/abs/2402.18910v1'>2402.18910v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02576v1")'>t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual
  Learning in Decision Making</div>
<div id='2401.02576v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T23:44:35Z</div><div>Authors: William Yue, Bo Liu, Peter Stone</div><div style='padding-top: 10px; width: 80ex'>Deep generative replay has emerged as a promising approach for continual
learning in decision-making tasks. This approach addresses the problem of
catastrophic forgetting by leveraging the generation of trajectories from
previously encountered tasks to augment the current dataset. However, existing
deep generative replay methods for continual learning rely on autoregressive
models, which suffer from compounding errors in the generated trajectories. In
this paper, we propose a simple, scalable, and non-autoregressive method for
continual learning in decision-making tasks using a generative model that
generates task samples conditioned on the trajectory timestep. We evaluate our
method on Continual World benchmarks and find that our approach achieves
state-of-the-art performance on the average success rate metric among continual
learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .</div><div><a href='http://arxiv.org/abs/2401.02576v1'>2401.02576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07928v1")'>Abstracted Trajectory Visualization for Explainability in Reinforcement
  Learning</div>
<div id='2402.07928v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T21:17:44Z</div><div>Authors: Yoshiki Takagi, Roderick Tabalba, Nurit Kirshenbaum, Jason Leigh</div><div style='padding-top: 10px; width: 80ex'>Explainable AI (XAI) has demonstrated the potential to help reinforcement
learning (RL) practitioners to understand how RL models work. However, XAI for
users who do not have RL expertise (non-RL experts), has not been studied
sufficiently. This results in a difficulty for the non-RL experts to
participate in the fundamental discussion of how RL models should be designed
for an incoming society where humans and AI coexist. Solving such a problem
would enable RL experts to communicate with the non-RL experts in producing
machine learning solutions that better fit our society. We argue that
abstracted trajectories, that depicts transitions between the major states of
the RL model, will be useful for non-RL experts to build a mental model of the
agents. Our early results suggest that by leveraging a visualization of the
abstracted trajectories, users without RL expertise are able to infer the
behavior patterns of RL.</div><div><a href='http://arxiv.org/abs/2402.07928v1'>2402.07928v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03141v1")'>Boosting Long-Delayed Reinforcement Learning with Auxiliary
  Short-Delayed Task</div>
<div id='2402.03141v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:11:03Z</div><div>Authors: Qingyuan Wu, Simon Sinong Zhan, Yixuan Wang, Chung-Wei Lin, Chen Lv, Qi Zhu, Chao Huang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning is challenging in delayed scenarios, a common
real-world situation where observations and interactions occur with delays.
State-of-the-art (SOTA) state-augmentation techniques either suffer from the
state-space explosion along with the delayed steps, or performance degeneration
in stochastic environments. To address these challenges, our novel
Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary
short-delayed task to accelerate the learning on a long-delayed task without
compromising the performance in stochastic environments. Specifically, AD-RL
learns the value function in the short-delayed task and then employs it with
the bootstrapping and policy improvement techniques in the long-delayed task.
We theoretically show that this can greatly reduce the sample complexity
compared to directly learning on the original long-delayed task. On
deterministic and stochastic benchmarks, our method remarkably outperforms the
SOTAs in both sample efficiency and policy performance.</div><div><a href='http://arxiv.org/abs/2402.03141v1'>2402.03141v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00673v2")'>Snapshot Reinforcement Learning: Leveraging Prior Trajectories for
  Efficiency</div>
<div id='2403.00673v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T17:05:22Z</div><div>Authors: Yanxiao Zhao, Yangge Qian, Tianyi Wang, Jingyang Shan, Xiaolin Qin</div><div style='padding-top: 10px; width: 80ex'>Deep reinforcement learning (DRL) algorithms require substantial samples and
computational resources to achieve higher performance, which restricts their
practical application and poses challenges for further development. Given the
constraint of limited resources, it is essential to leverage existing
computational work (e.g., learned policies, samples) to enhance sample
efficiency and reduce the computational resource consumption of DRL algorithms.
Previous works to leverage existing computational work require intrusive
modifications to existing algorithms and models, designed specifically for
specific algorithms, lacking flexibility and universality. In this paper, we
present the Snapshot Reinforcement Learning (SnapshotRL) framework, which
enhances sample efficiency by simply altering environments, without making any
modifications to algorithms and models. By allowing student agents to choose
states in teacher trajectories as the initial state to sample, SnapshotRL can
effectively utilize teacher trajectories to assist student agents in training,
allowing student agents to explore a larger state space at the early training
phase. We propose a simple and effective SnapshotRL baseline algorithm, S3RL,
which integrates well with existing DRL algorithms. Our experiments demonstrate
that integrating S3RL with TD3, SAC, and PPO algorithms on the MuJoCo benchmark
significantly improves sample efficiency and average return, without extra
samples and additional computational resources.</div><div><a href='http://arxiv.org/abs/2403.00673v2'>2403.00673v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07710v1")'>Go-Explore for Residential Energy Management</div>
<div id='2401.07710v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T14:26:44Z</div><div>Authors: Junlin Lu, Patrick Mannion, Karl Mason</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning is commonly applied in residential energy management,
particularly for optimizing energy costs. However, RL agents often face
challenges when dealing with deceptive and sparse rewards in the energy control
domain, especially with stochastic rewards. In such situations, thorough
exploration becomes crucial for learning an optimal policy. Unfortunately, the
exploration mechanism can be misled by deceptive reward signals, making
thorough exploration difficult. Go-Explore is a family of algorithms which
combines planning methods and reinforcement learning methods to achieve
efficient exploration. We use the Go-Explore algorithm to solve the cost-saving
task in residential energy management problems and achieve an improvement of up
to 19.84\% compared to the well-known reinforcement learning algorithms.</div><div><a href='http://arxiv.org/abs/2401.07710v1'>2401.07710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11947v1")'>Explainable Reinforcement Learning-based Home Energy Management Systems
  using Differentiable Decision Trees</div>
<div id='2403.11947v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:40:41Z</div><div>Authors: Gargya Gokhale, Bert Claessens, Chris Develder</div><div style='padding-top: 10px; width: 80ex'>With the ongoing energy transition, demand-side flexibility has become an
important aspect of the modern power grid for providing grid support and
allowing further integration of sustainable energy sources. Besides traditional
sources, the residential sector is another major and largely untapped source of
flexibility, driven by the increased adoption of solar PV, home batteries, and
EVs. However, unlocking this residential flexibility is challenging as it
requires a control framework that can effectively manage household energy
consumption, and maintain user comfort while being readily scalable across
different, diverse houses. We aim to address this challenging problem and
introduce a reinforcement learning-based approach using differentiable decision
trees. This approach integrates the scalability of data-driven reinforcement
learning with the explainability of (differentiable) decision trees. This leads
to a controller that can be easily adapted across different houses and provides
a simple control policy that can be explained to end-users, further improving
user acceptance. As a proof-of-concept, we analyze our method using a home
energy management problem, comparing its performance with commercially
available rule-based baseline and standard neural network-based RL controllers.
Through this preliminary study, we show that the performance of our proposed
method is comparable to standard RL-based controllers, outperforming baseline
controllers by ~20% in terms of daily cost savings while being straightforward
to explain.</div><div><a href='http://arxiv.org/abs/2403.11947v1'>2403.11947v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11907v1")'>Distill2Explain: Differentiable decision trees for explainable
  reinforcement learning in energy application controllers</div>
<div id='2403.11907v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:09:49Z</div><div>Authors: Gargya Gokhale, Seyed Soroush Karimi Madahi, Bert Claessens, Chris Develder</div><div style='padding-top: 10px; width: 80ex'>Demand-side flexibility is gaining importance as a crucial element in the
energy transition process. Accounting for about 25% of final energy consumption
globally, the residential sector is an important (potential) source of energy
flexibility. However, unlocking this flexibility requires developing a control
framework that (1) easily scales across different houses, (2) is easy to
maintain, and (3) is simple to understand for end-users. A potential control
framework for such a task is data-driven control, specifically model-free
reinforcement learning (RL). Such RL-based controllers learn a good control
policy by interacting with their environment, learning purely based on data and
with minimal human intervention. Yet, they lack explainability, which hampers
user acceptance. Moreover, limited hardware capabilities of residential assets
forms a hurdle (e.g., using deep neural networks). To overcome both those
challenges, we propose a novel method to obtain explainable RL policies by
using differentiable decision trees. Using a policy distillation approach, we
train these differentiable decision trees to mimic standard RL-based
controllers, leading to a decision tree-based control policy that is
data-driven and easy to explain. As a proof-of-concept, we examine the
performance and explainability of our proposed approach in a battery-based home
energy management system to reduce energy costs. For this use case, we show
that our proposed approach can outperform baseline rule-based policies by about
20-25%, while providing simple, explainable control policies. We further
compare these explainable policies with standard RL policies and examine the
performance trade-offs associated with this increased explainability.</div><div><a href='http://arxiv.org/abs/2403.11907v1'>2403.11907v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00172v1")'>Go Beyond Black-box Policies: Rethinking the Design of Learning Agent
  for Interpretable and Verifiable HVAC Control</div>
<div id='2403.00172v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T22:42:23Z</div><div>Authors: Zhiyu An, Xianzhong Ding, Wan Du</div><div style='padding-top: 10px; width: 80ex'>Recent research has shown the potential of Model-based Reinforcement Learning
(MBRL) to enhance energy efficiency of Heating, Ventilation, and Air
Conditioning (HVAC) systems. However, existing methods rely on black-box
thermal dynamics models and stochastic optimizers, lacking reliability
guarantees and posing risks to occupant health. In this work, we overcome the
reliability bottleneck by redesigning HVAC controllers using decision trees
extracted from existing thermal dynamics models and historical data. Our
decision tree-based policies are deterministic, verifiable, interpretable, and
more energy-efficient than current MBRL methods. First, we introduce a novel
verification criterion for RL agents in HVAC control based on domain knowledge.
Second, we develop a policy extraction procedure that produces a verifiable
decision tree policy. We found that the high dimensionality of the thermal
dynamics model input hinders the efficiency of policy extraction. To tackle the
dimensionality challenge, we leverage importance sampling conditioned on
historical data distributions, significantly improving policy extraction
efficiency. Lastly, we present an offline verification algorithm that
guarantees the reliability of a control policy. Extensive experiments show that
our method saves 68.4% more energy and increases human comfort gain by 14.8%
compared to the state-of-the-art method, in addition to an 1127x reduction in
computation overhead. Our code and data are available at
https://github.com/ryeii/Veri_HVAC</div><div><a href='http://arxiv.org/abs/2403.00172v1'>2403.00172v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05737v1")'>An experimental evaluation of Deep Reinforcement Learning algorithms for
  HVAC control</div>
<div id='2401.05737v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T08:40:26Z</div><div>Authors: Antonio Manjavacas, Alejandro Campoy-Nieves, Javier Jim√©nez-Raboso, Miguel Molina-Solana, Juan G√≥mez-Romero</div><div style='padding-top: 10px; width: 80ex'>Heating, Ventilation, and Air Conditioning (HVAC) systems are a major driver
of energy consumption in commercial and residential buildings. Recent studies
have shown that Deep Reinforcement Learning (DRL) algorithms can outperform
traditional reactive controllers. However, DRL-based solutions are generally
designed for ad hoc setups and lack standardization for comparison. To fill
this gap, this paper provides a critical and reproducible evaluation, in terms
of comfort and energy consumption, of several state-of-the-art DRL algorithms
for HVAC control. The study examines the controllers' robustness, adaptability,
and trade-off between optimization goals by using the Sinergym framework. The
results obtained confirm the potential of DRL algorithms, such as SAC and TD3,
in complex scenarios and reveal several challenges related to generalization
and incremental learning.</div><div><a href='http://arxiv.org/abs/2401.05737v1'>2401.05737v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14092v1")'>Carbon Footprint Reduction for Sustainable Data Centers in Real-Time</div>
<div id='2403.14092v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T02:59:56Z</div><div>Authors: Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh Babu</div><div style='padding-top: 10px; width: 80ex'>As machine learning workloads significantly increase energy consumption,
sustainable data centers with low carbon emissions are becoming a top priority
for governments and corporations worldwide. This requires a paradigm shift in
optimizing power consumption in cooling and IT loads, shifting flexible loads
based on the availability of renewable energy in the power grid, and leveraging
battery storage from the uninterrupted power supply in data centers, using
collaborative agents. The complex association between these optimization
strategies and their dependencies on variable external factors like weather and
the power grid carbon intensity makes this a hard problem. Currently, a
real-time controller to optimize all these goals simultaneously in a dynamic
real-world setting is lacking. We propose a Data Center Carbon Footprint
Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that
optimizes data centers for the multiple objectives of carbon footprint
reduction, energy consumption, and energy cost. The results show that the
DC-CFR MARL agents effectively resolved the complex interdependencies in
optimizing cooling, load shifting, and energy storage in real-time for various
locations under real-world dynamic weather and grid carbon intensity
conditions. DC-CFR significantly outperformed the industry standard ASHRAE
controller with a considerable reduction in carbon emissions (14.5%), energy
usage (14.4%), and energy cost (13.7%) when evaluated over one year across
multiple geographical regions.</div><div><a href='http://arxiv.org/abs/2403.14092v1'>2403.14092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09499v2")'>A Reinforcement Learning Approach to Dairy Farm Battery Management using
  Q Learning</div>
<div id='2403.09499v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T15:42:26Z</div><div>Authors: Nawazish Ali, Abdul Wahid, Rachael Shaw, Karl Mason</div><div style='padding-top: 10px; width: 80ex'>Dairy farming consumes a significant amount of energy, making it an
energy-intensive sector within agriculture. Integrating renewable energy
generation into dairy farming could help address this challenge. Effective
battery management is important for integrating renewable energy generation.
Managing battery charging and discharging poses significant challenges because
of fluctuations in electrical consumption, the intermittent nature of renewable
energy generation, and fluctuations in energy prices. Artificial Intelligence
(AI) has the potential to significantly improve the use of renewable energy in
dairy farming, however, there is limited research conducted in this particular
domain. This research considers Ireland as a case study as it works towards
attaining its 2030 energy strategy centered on the utilization of renewable
sources. This study proposes a Q-learning-based algorithm for scheduling
battery charging and discharging in a dairy farm setting. This research also
explores the effect of the proposed algorithm by adding wind generation data
and considering additional case studies. The proposed algorithm reduces the
cost of imported electricity from the grid by 13.41\%, peak demand by 2\%, and
24.49\% when utilizing wind generation. These results underline how
reinforcement learning is highly effective in managing batteries in the dairy
farming sector.</div><div><a href='http://arxiv.org/abs/2403.09499v2'>2403.09499v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15324v1")'>Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method
  and Its Application to Energy Network</div>
<div id='2402.15324v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:43:15Z</div><div>Authors: Jianhong Wang</div><div style='padding-top: 10px; width: 80ex'>Multi-agent reinforcement learning is an area of rapid advancement in
artificial intelligence and machine learning. One of the important questions to
be answered is how to conduct credit assignment in a multi-agent system. There
have been many schemes designed to conduct credit assignment by multi-agent
reinforcement learning algorithms. Although these credit assignment schemes
have been proved useful in improving the performance of multi-agent
reinforcement learning, most of them are designed heuristically without a
rigorous theoretic basis and therefore infeasible to understand how agents
cooperate. In this thesis, we aim at investigating the foundation of credit
assignment in multi-agent reinforcement learning via cooperative game theory.
We first extend a game model called convex game and a payoff distribution
scheme called Shapley value in cooperative game theory to Markov decision
process, named as Markov convex game and Markov Shapley value respectively. We
represent a global reward game as a Markov convex game under the grand
coalition. As a result, Markov Shapley value can be reasonably used as a credit
assignment scheme in the global reward game. Markov Shapley value possesses the
following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii)
reflecting the contribution and (iv) symmetry, which form the fair credit
assignment. Based on Markov Shapley value, we propose three multi-agent
reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO. Furthermore,
we extend Markov convex game to partial observability to deal with the
partially observable problems, named as partially observable Markov convex
game. In application, we evaluate SQDDPG and SMFPPO on the real-world problem
in energy networks.</div><div><a href='http://arxiv.org/abs/2402.15324v1'>2402.15324v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13947v2")'>Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy
  Trading</div>
<div id='2401.13947v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T05:05:55Z</div><div>Authors: Chen Feng, Andrew L. Liu</div><div style='padding-top: 10px; width: 80ex'>Utilizing distributed renewable and energy storage resources in local
distribution networks via peer-to-peer (P2P) energy trading has long been
touted as a solution to improve energy systems' resilience and sustainability.
Consumers and prosumers (those who have energy generation resources), however,
do not have the expertise to engage in repeated P2P trading, and the
zero-marginal costs of renewables present challenges in determining fair market
prices. To address these issues, we propose multi-agent reinforcement learning
(MARL) frameworks to help automate consumers' bidding and management of their
solar PV and energy storage resources, under a specific P2P clearing mechanism
that utilizes the so-called supply-demand ratio. In addition, we show how the
MARL frameworks can integrate physical network constraints to realize voltage
control, hence ensuring physical feasibility of the P2P energy trading and
paving way for real-world implementations.</div><div><a href='http://arxiv.org/abs/2401.13947v2'>2401.13947v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12235v1")'>Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot
  Adaption via Contextual Meta Graph Reinforcement Learning</div>
<div id='2401.12235v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T13:58:46Z</div><div>Authors: Bairong Deng, Tao Yu, Zhenning Pan, Xuehan Zhang, Yufeng Wu, Qiaoyi Ding</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning is an emerging approaches to facilitate multi-stage
sequential decision-making problems. This paper studies a real-time multi-stage
stochastic power dispatch considering multivariate uncertainties. Current
researches suffer from low generalization and practicality, that is, the
learned dispatch policy can only handle a specific dispatch scenario, its
performance degrades significantly if actual samples and training samples are
inconsistent. To fill these gaps, a novel contextual meta graph reinforcement
learning (Meta-GRL) for a highly generalized multi-stage optimal dispatch
policy is proposed. Specifically, a more general contextual Markov decision
process (MDP) and scalable graph representation are introduced to achieve a
more generalized multi-stage stochastic power dispatch modeling. An upper
meta-learner is proposed to encode context for different dispatch scenarios and
learn how to achieve dispatch task identification while the lower policy
learner learns context-specified dispatch policy. After sufficient offline
learning, this approach can rapidly adapt to unseen and undefined scenarios
with only a few updations of the hypothesis judgments generated by the
meta-learner. Numerical comparisons with state-of-the-art policies and
traditional reinforcement learning verify the optimality, efficiency,
adaptability, and scalability of the proposed Meta-GRL.</div><div><a href='http://arxiv.org/abs/2401.12235v1'>2401.12235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06952v1")'>Reinforcement Learning for Scalable Train Timetable Rescheduling with
  Graph Representation</div>
<div id='2401.06952v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T02:14:35Z</div><div>Authors: Peng Yue, Yaochu Jin, Xuewu Dai, Zhenhua Feng, Dongliang Cui</div><div style='padding-top: 10px; width: 80ex'>Train timetable rescheduling (TTR) aims to promptly restore the original
operation of trains after unexpected disturbances or disruptions. Currently,
this work is still done manually by train dispatchers, which is challenging to
maintain performance under various problem instances. To mitigate this issue,
this study proposes a reinforcement learning-based approach to TTR, which makes
the following contributions compared to existing work. First, we design a
simple directed graph to represent the TTR problem, enabling the automatic
extraction of informative states through graph neural networks. Second, we
reformulate the construction process of TTR's solution, not only decoupling the
decision model from the problem size but also ensuring the generated scheme's
feasibility. Third, we design a learning curriculum for our model to handle the
scenarios with different levels of delay. Finally, a simple local search method
is proposed to assist the learned decision model, which can significantly
improve solution quality with little additional computation cost, further
enhancing the practical value of our method. Extensive experimental results
demonstrate the effectiveness of our method. The learned decision model can
achieve better performance for various problems with varying degrees of train
delay and different scales when compared to handcrafted rules and
state-of-the-art solvers.</div><div><a href='http://arxiv.org/abs/2401.06952v1'>2401.06952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12455v1")'>Multi-agent deep reinforcement learning with centralized training and
  decentralized execution for transportation infrastructure management</div>
<div id='2401.12455v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T02:52:36Z</div><div>Authors: M. Saifullah, K. G. Papakonstantinou, C. P. Andriotis, S. M. Stoffels</div><div style='padding-top: 10px; width: 80ex'>We present a multi-agent Deep Reinforcement Learning (DRL) framework for
managing large transportation infrastructure systems over their life-cycle.
Life-cycle management of such engineering systems is a computationally
intensive task, requiring appropriate sequential inspection and maintenance
decisions able to reduce long-term risks and costs, while dealing with
different uncertainties and constraints that lie in high-dimensional spaces. To
date, static age- or condition-based maintenance methods and risk-based or
periodic inspection plans have mostly addressed this class of optimization
problems. However, optimality, scalability, and uncertainty limitations are
often manifested under such approaches. The optimization problem in this work
is cast in the framework of constrained Partially Observable Markov Decision
Processes (POMDPs), which provides a comprehensive mathematical basis for
stochastic sequential decision settings with observation uncertainties, risk
considerations, and limited resources. To address significantly large state and
action spaces, a Deep Decentralized Multi-agent Actor-Critic (DDMAC) DRL method
with Centralized Training and Decentralized Execution (CTDE), termed as
DDMAC-CTDE is developed. The performance strengths of the DDMAC-CTDE method are
demonstrated in a generally representative and realistic example application of
an existing transportation network in Virginia, USA. The network includes
several bridge and pavement components with nonstationary degradation,
agency-imposed constraints, and traffic delay and risk considerations. Compared
to traditional management policies for transportation networks, the proposed
DDMAC-CTDE method vastly outperforms its counterparts. Overall, the proposed
algorithmic framework provides near optimal solutions for transportation
infrastructure management under real-world constraints and complexities.</div><div><a href='http://arxiv.org/abs/2401.12455v1'>2401.12455v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17967v1")'>Imitation-regularized Optimal Transport on Networks: Provable Robustness
  and Application to Logistics Planning</div>
<div id='2402.17967v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T01:19:42Z</div><div>Authors: Koshi Oishi, Yota Hashizume, Tomohiko Jimbo, Hirotaka Kaji, Kenji Kashima</div><div style='padding-top: 10px; width: 80ex'>Network systems form the foundation of modern society, playing a critical
role in various applications. However, these systems are at significant risk of
being adversely affected by unforeseen circumstances, such as disasters.
Considering this, there is a pressing need for research to enhance the
robustness of network systems. Recently, in reinforcement learning, the
relationship between acquiring robustness and regularizing entropy has been
identified. Additionally, imitation learning is used within this framework to
reflect experts' behavior. However, there are no comprehensive studies on the
use of a similar imitation framework for optimal transport on networks.
Therefore, in this study, imitation-regularized optimal transport (I-OT) on
networks was investigated. It encodes prior knowledge on the network by
imitating a given prior distribution. The I-OT solution demonstrated robustness
in terms of the cost defined on the network. Moreover, we applied the I-OT to a
logistics planning problem using real data. We also examined the imitation and
apriori risk information scenarios to demonstrate the usefulness and
implications of the proposed method.</div><div><a href='http://arxiv.org/abs/2402.17967v1'>2402.17967v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09550v1")'>Dataset Clustering for Improved Offline Policy Learning</div>
<div id='2402.09550v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T20:01:41Z</div><div>Authors: Qiang Wang, Yixin Deng, Francisco Roldan Sanchez, Keru Wang, Kevin McGuinness, Noel O'Connor, Stephen J. Redmond</div><div style='padding-top: 10px; width: 80ex'>Offline policy learning aims to discover decision-making policies from
previously-collected datasets without additional online interactions with the
environment. As the training dataset is fixed, its quality becomes a crucial
determining factor in the performance of the learned policy. This paper studies
a dataset characteristic that we refer to as multi-behavior, indicating that
the dataset is collected using multiple policies that exhibit distinct
behaviors. In contrast, a uni-behavior dataset would be collected solely using
one policy. We observed that policies learned from a uni-behavior dataset
typically outperform those learned from multi-behavior datasets, despite the
uni-behavior dataset having fewer examples and less diversity. Therefore, we
propose a behavior-aware deep clustering approach that partitions
multi-behavior datasets into several uni-behavior subsets, thereby benefiting
downstream policy learning. Our approach is flexible and effective; it can
adaptively estimate the number of clusters while demonstrating high clustering
accuracy, achieving an average Adjusted Rand Index of 0.987 across various
continuous control task datasets. Finally, we present improved policy learning
examples using dataset clustering and discuss several potential scenarios where
our approach might benefit the offline policy learning community.</div><div><a href='http://arxiv.org/abs/2402.09550v1'>2402.09550v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05006v1")'>Provable Multi-Party Reinforcement Learning with Diverse Human Feedback</div>
<div id='2403.05006v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T03:05:11Z</div><div>Authors: Huiying Zhong, Zhun Deng, Weijie J. Su, Zhiwei Steven Wu, Linjun Zhang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning with human feedback (RLHF) is an emerging paradigm to
align models with human preferences. Typically, RLHF aggregates preferences
from multiple individuals who have diverse viewpoints that may conflict with
each other. Our work \textit{initiates} the theoretical study of multi-party
RLHF that explicitly models the diverse preferences of multiple individuals. We
show how traditional RLHF approaches can fail since learning a single reward
function cannot capture and balance the preferences of multiple individuals. To
overcome such limitations, we incorporate meta-learning to learn multiple
preferences and adopt different social welfare functions to aggregate the
preferences across multiple parties. We focus on the offline learning setting
and establish sample complexity bounds, along with efficiency and fairness
guarantees, for optimizing diverse social welfare functions such as Nash,
Utilitarian, and Leximin welfare functions. Our results show a separation
between the sample complexities of multi-party RLHF and traditional
single-party RLHF. Furthermore, we consider a reward-free setting, where each
individual's preference is no longer consistent with a reward model, and give
pessimistic variants of the von Neumann Winner based on offline preference
data. Taken together, our work showcases the advantage of multi-party RLHF but
also highlights its more demanding statistical complexity.</div><div><a href='http://arxiv.org/abs/2403.05006v1'>2403.05006v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10946v1")'>The Fallacy of Minimizing Local Regret in the Sequential Task Setting</div>
<div id='2403.10946v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T15:29:22Z</div><div>Authors: Ziping Xu, Kelly W. Zhang, Susan A. Murphy</div><div style='padding-top: 10px; width: 80ex'>In the realm of Reinforcement Learning (RL), online RL is often
conceptualized as an optimization problem, where an algorithm interacts with an
unknown environment to minimize cumulative regret. In a stationary setting,
strong theoretical guarantees, like a sublinear ($\sqrt{T}$) regret bound, can
be obtained, which typically implies the convergence to an optimal policy and
the cessation of exploration. However, these theoretical setups often
oversimplify the complexities encountered in real-world RL implementations,
where tasks arrive sequentially with substantial changes between tasks and the
algorithm may not be allowed to adaptively learn within certain tasks. We study
the changes beyond the outcome distributions, encompassing changes in the
reward designs (mappings from outcomes to rewards) and the permissible policy
spaces. Our results reveal the fallacy of myopically minimizing regret within
each task: obtaining optimal regret rates in the early tasks may lead to worse
rates in the subsequent ones, even when the outcome distributions stay the
same. To realize the optimal cumulative regret bound across all the tasks, the
algorithm has to overly explore in the earlier tasks. This theoretical insight
is practically significant, suggesting that due to unanticipated changes (e.g.,
rapid technological development or human-in-the-loop involvement) between
tasks, the algorithm needs to explore more than it would in the usual
stationary setting within each task. Such implication resonates with the common
practice of using clipped policies in mobile health clinical trials and
maintaining a fixed rate of $\epsilon$-greedy exploration in robotic learning.</div><div><a href='http://arxiv.org/abs/2403.10946v1'>2403.10946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16463v1")'>Learning to Schedule Online Tasks with Bandit Feedback</div>
<div id='2402.16463v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T10:11:28Z</div><div>Authors: Yongxin Xu, Shangshang Wang, Hengquan Guo, Xin Liu, Ziyu Shao</div><div style='padding-top: 10px; width: 80ex'>Online task scheduling serves an integral role for task-intensive
applications in cloud computing and crowdsourcing. Optimal scheduling can
enhance system performance, typically measured by the reward-to-cost ratio,
under some task arrival distribution. On one hand, both reward and cost are
dependent on task context (e.g., evaluation metric) and remain black-box in
practice. These render reward and cost hard to model thus unknown before
decision making. On the other hand, task arrival behaviors remain sensitive to
factors like unpredictable system fluctuation whereby a prior estimation or the
conventional assumption of arrival distribution (e.g., Poisson) may fail. This
implies another practical yet often neglected challenge, i.e., uncertain task
arrival distribution. Towards effective scheduling under a stationary
environment with various uncertainties, we propose a double-optimistic learning
based Robbins-Monro (DOL-RM) algorithm. Specifically, DOL-RM integrates a
learning module that incorporates optimistic estimation for reward-to-cost
ratio and a decision module that utilizes the Robbins-Monro method to
implicitly learn task arrival distribution while making scheduling decisions.
Theoretically, DOL-RM achieves convergence gap and no regret learning with a
sub-linear regret of $O(T^{3/4})$, which is the first result for online task
scheduling under uncertain task arrival distribution and unknown reward and
cost. Our numerical results in a synthetic experiment and a real-world
application demonstrate the effectiveness of DOL-RM in achieving the best
cumulative reward-to-cost ratio compared with other state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2402.16463v1'>2402.16463v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09961v1")'>Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through
  Dynamic Shift Extensions: A Deep Reinforcement Learning Approach</div>
<div id='2402.09961v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:15:51Z</div><div>Authors: Zead Saleh, Ahmad Al Hanbali, Ahmad Baubaid</div><div style='padding-top: 10px; width: 80ex'>Crowdsourced delivery platforms face complex scheduling challenges to match
couriers and customer orders. We consider two types of crowdsourced couriers,
namely, committed and occasional couriers, each with different compensation
schemes. Crowdsourced delivery platforms usually schedule committed courier
shifts based on predicted demand. Therefore, platforms may devise an offline
schedule for committed couriers before the planning period. However, due to the
unpredictability of demand, there are instances where it becomes necessary to
make online adjustments to the offline schedule. In this study, we focus on the
problem of dynamically adjusting the offline schedule through shift extensions
for committed couriers. This problem is modeled as a sequential decision
process. The objective is to maximize platform profit by determining the shift
extensions of couriers and the assignments of requests to couriers. To solve
the model, a Deep Q-Network (DQN) learning approach is developed. Comparing
this model with the baseline policy where no extensions are allowed
demonstrates the benefits that platforms can gain from allowing shift
extensions in terms of reward, reduced lost order costs, and lost requests.
Additionally, sensitivity analysis showed that the total extension compensation
increases in a nonlinear manner with the arrival rate of requests, and in a
linear manner with the arrival rate of occasional couriers. On the compensation
sensitivity, the results showed that the normal scenario exhibited the highest
average number of shift extensions and, consequently, the fewest average number
of lost requests. These findings serve as evidence of the successful learning
of such dynamics by the DQN algorithm.</div><div><a href='http://arxiv.org/abs/2402.09961v1'>2402.09961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08321v1")'>Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret
  with Adversarial Robustness in Partial Monitoring</div>
<div id='2402.08321v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T09:34:22Z</div><div>Authors: Taira Tsuchiya, Shinji Ito, Junya Honda</div><div style='padding-top: 10px; width: 80ex'>Partial monitoring is a generic framework of online decision-making problems
with limited observations. To make decisions from such limited observations, it
is necessary to find an appropriate distribution for exploration. Recently, a
powerful approach for this purpose, exploration by optimization (ExO), was
proposed, which achieves the optimal bounds in adversarial environments with
follow-the-regularized-leader for a wide range of online decision-making
problems. However, a naive application of ExO in stochastic environments
significantly degrades regret bounds. To resolve this problem in locally
observable games, we first establish a novel framework and analysis for ExO
with a hybrid regularizer. This development allows us to significantly improve
the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which
achieves nearly optimal bounds both in stochastic and adversarial environments.
In particular, we derive a stochastic regret bound of $O(\sum_{a \neq a^*} k^2
m^2 \log T / \Delta_a)$, where $k$, $m$, and $T$ are the numbers of actions,
observations and rounds, $a^*$ is an optimal action, and $\Delta_a$ is the
suboptimality gap for action $a$. This bound is roughly $\Theta(k^2 \log T)$
times smaller than existing BOBW bounds. In addition, for globally observable
games, we provide a new BOBW algorithm with the first $O(\log T)$ stochastic
bound.</div><div><a href='http://arxiv.org/abs/2402.08321v1'>2402.08321v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08576v2")'>Regret Minimization in Stackelberg Games with Side Information</div>
<div id='2402.08576v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:24:57Z</div><div>Authors: Keegan Harris, Zhiwei Steven Wu, Maria-Florina Balcan</div><div style='padding-top: 10px; width: 80ex'>In its most basic form, a Stackelberg game is a two-player game in which a
leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg
games are perhaps one of the biggest success stories of algorithmic game theory
over the last decade, as algorithms for playing in Stackelberg games have been
deployed in many real-world domains including airport security, anti-poaching
efforts, and cyber-crime prevention. However, these algorithms often fail to
take into consideration the additional information available to each player
(e.g. traffic patterns, weather conditions, network congestion), a salient
feature of reality which may significantly affect both players' optimal
strategies. We formalize such settings as Stackelberg games with side
information, in which both players observe an external context before playing.
The leader then commits to a (possibly context-dependent) strategy, and the
follower best-responds to both the leader's strategy and the context. We focus
on the online setting in which a sequence of followers arrive over time, and
the context may change from round-to-round. In sharp contrast to the
non-contextual version, we show that it is impossible for the leader to achieve
good performance (measured by regret) in the full adversarial setting (i.e.,
when both the context and the follower are chosen by an adversary). However, it
turns out that a little bit of randomness goes a long way. Motivated by our
impossibility result, we show that no-regret learning is possible in two
natural relaxations: the setting in which the sequence of followers is chosen
stochastically and the sequence of contexts is adversarial, and the setting in
which the sequence of contexts is stochastic and the sequence of followers is
chosen by an adversary.</div><div><a href='http://arxiv.org/abs/2402.08576v2'>2402.08576v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17736v2")'>Learning-Based Algorithms for Graph Searching Problems</div>
<div id='2402.17736v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:12:58Z</div><div>Authors: Adela Frances DePavia, Erasmo Tani, Ali Vakilian</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of graph searching with prediction recently
introduced by Banerjee et al. (2022). In this problem, an agent, starting at
some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a
hidden goal node $g$ while minimizing the total distance travelled. We study a
setting in which at any node $v$, the agent receives a noisy estimate of the
distance from $v$ to $g$. We design algorithms for this search task on unknown
graphs. We establish the first formal guarantees on unknown weighted graphs and
provide lower bounds showing that the algorithms we propose have optimal or
nearly-optimal dependence on the prediction error. Further, we perform
numerical experiments demonstrating that in addition to being robust to
adversarial error, our algorithms perform well in typical instances in which
the error is stochastic. Finally, we provide alternative simpler performance
bounds on the algorithms of Banerjee et al. (2022) for the case of searching on
a known graph, and establish new lower bounds for this setting.</div><div><a href='http://arxiv.org/abs/2402.17736v2'>2402.17736v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12673v1")'>Beyond Worst-case Attacks: Robust RL with Adaptive Defense via
  Non-dominated Policies</div>
<div id='2402.12673v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T02:45:20Z</div><div>Authors: Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang</div><div style='padding-top: 10px; width: 80ex'>In light of the burgeoning success of reinforcement learning (RL) in diverse
real-world applications, considerable focus has been directed towards ensuring
RL policies are robust to adversarial attacks during test time. Current
approaches largely revolve around solving a minimax problem to prepare for
potential worst-case scenarios. While effective against strong attacks, these
methods often compromise performance in the absence of attacks or the presence
of only weak attacks. To address this, we study policy robustness under the
well-accepted state-adversarial attack model, extending our focus beyond only
worst-case attacks. We first formalize this task at test time as a regret
minimization problem and establish its intrinsic hardness in achieving
sublinear regret when the baseline policy is from a general continuous policy
class, $\Pi$. This finding prompts us to \textit{refine} the baseline policy
class $\Pi$ prior to test time, aiming for efficient adaptation within a finite
policy class $\Tilde{\Pi}$, which can resort to an adversarial bandit
subroutine. In light of the importance of a small, finite $\Tilde{\Pi}$, we
propose a novel training-time algorithm to iteratively discover
\textit{non-dominated policies}, forming a near-optimal and minimal
$\Tilde{\Pi}$, thereby ensuring both robustness and test-time efficiency.
Empirical validation on the Mujoco corroborates the superiority of our approach
in terms of natural and robust performance, as well as adaptability to various
attack scenarios.</div><div><a href='http://arxiv.org/abs/2402.12673v1'>2402.12673v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18781v3")'>Conjectural Online Learning with First-order Beliefs in Asymmetric
  Information Stochastic Games</div>
<div id='2402.18781v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T01:07:29Z</div><div>Authors: Tao Li, Kim Hammar, Rolf Stadler, Quanyan Zhu</div><div style='padding-top: 10px; width: 80ex'>Asymmetric information stochastic games (\textsc{aisg}s) arise in many
complex socio-technical systems, such as cyber-physical systems and IT
infrastructures. Existing computational methods for \textsc{aisg}s are
primarily offline and can not adapt to equilibrium deviations. Further, current
methods are limited to special classes of \textsc{aisg}s to avoid belief
hierarchies. To address these limitations, we propose conjectural online
learning (\textsc{col}), an online method for generic \textsc{aisg}s.
\textsc{col} uses a forecaster-actor-critic (\textsc{fac}) architecture where
subjective forecasts are used to conjecture the opponents' strategies within a
lookahead horizon, and Bayesian learning is used to calibrate the conjectures.
To adapt strategies to nonstationary environments, \textsc{col} uses online
rollout with cost function approximation (actor-critic). We prove that the
conjectures produced by \textsc{col} are asymptotically consistent with the
information feedback in the sense of a relaxed Bayesian consistency. We also
prove that the empirical strategy profile induced by \textsc{col} converges to
the Berk-Nash equilibrium, a solution concept characterizing rationality under
subjectivity. Experimental results from an intrusion response use case
demonstrate \textsc{col}'s superiority over state-of-the-art reinforcement
learning methods against nonstationary attacks.</div><div><a href='http://arxiv.org/abs/2402.18781v3'>2402.18781v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12499v1")'>Automated Security Response through Online Learning with Adaptive
  Conjectures</div>
<div id='2402.12499v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:06:15Z</div><div>Authors: Kim Hammar, Tao Li, Rolf Stadler, Quanyan Zhu</div><div style='padding-top: 10px; width: 80ex'>We study automated security response for an IT infrastructure and formulate
the interaction between an attacker and a defender as a partially observed,
non-stationary game. We relax the standard assumption that the game model is
correctly specified and consider that each player has a probabilistic
conjecture about the model, which may be misspecified in the sense that the
true model has probability 0. This formulation allows us to capture uncertainty
about the infrastructure and the intents of the players. To learn effective
game strategies online, we design a novel method where a player iteratively
adapts its conjecture using Bayesian learning and updates its strategy through
rollout. We prove that the conjectures converge to best fits, and we provide a
bound on the performance improvement that rollout enables with a conjectured
model. To characterize the steady state of the game, we propose a variant of
the Berk-Nash equilibrium. We present our method through an advanced persistent
threat use case. Simulation studies based on testbed measurements show that our
method produces effective security strategies that adapt to a changing
environment. We also find that our method enables faster convergence than
current reinforcement learning techniques.</div><div><a href='http://arxiv.org/abs/2402.12499v1'>2402.12499v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07154v1")'>Discovering Command and Control Channels Using Reinforcement Learning</div>
<div id='2401.07154v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T20:03:11Z</div><div>Authors: Cheng Wang, Akshay Kakkar, Christopher Redino, Abdul Rahman, Ajinsyam S, Ryan Clark, Daniel Radke, Tyler Cody, Lanxiao Huang, Edward Bowen</div><div style='padding-top: 10px; width: 80ex'>Command and control (C2) paths for issuing commands to malware are sometimes
the only indicators of its existence within networks. Identifying potential C2
channels is often a manually driven process that involves a deep understanding
of cyber tradecraft. Efforts to improve discovery of these channels through
using a reinforcement learning (RL) based approach that learns to automatically
carry out C2 attack campaigns on large networks, where multiple defense layers
are in place serves to drive efficiency for network operators. In this paper,
we model C2 traffic flow as a three-stage process and formulate it as a Markov
decision process (MDP) with the objective to maximize the number of valuable
hosts whose data is exfiltrated. The approach also specifically models payload
and defense mechanisms such as firewalls which is a novel contribution. The
attack paths learned by the RL agent can in turn help the blue team identify
high-priority vulnerabilities and develop improved defense strategies. The
method is evaluated on a large network with more than a thousand hosts and the
results demonstrate that the agent can effectively learn attack paths while
avoiding firewalls.</div><div><a href='http://arxiv.org/abs/2401.07154v1'>2401.07154v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00027v1")'>A Quick Framework for Evaluating Worst Robustness of Complex Networks</div>
<div id='2403.00027v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T16:32:47Z</div><div>Authors: Wenjun Jiang, Peiyan Li, Tianlong Fan, Ting Li, Chuan-fu Zhang, Tao Zhang, Zong-fu Luo</div><div style='padding-top: 10px; width: 80ex'>Robustness is pivotal for comprehending, designing, optimizing, and
rehabilitating networks, with simulation attacks being the prevailing
evaluation method. Simulation attacks are often time-consuming or even
impractical, however, a more crucial yet persistently overlooked drawback is
that any attack strategy merely provides a potential paradigm of
disintegration. The key concern is: in the worst-case scenario or facing the
most severe attacks, what is the limit of robustness, referred to as ``Worst
Robustness'', for a given system? Understanding a system's worst robustness is
imperative for grasping its reliability limits, accurately evaluating
protective capabilities, and determining associated design and security
maintenance costs. To address these challenges, we introduce the concept of
Most Destruction Attack (MDA) based on the idea of knowledge stacking. MDA is
employed to assess the worst robustness of networks, followed by the
application of an adapted CNN algorithm for rapid worst robustness prediction.
We establish the logical validity of MDA and highlight the exceptional
performance of the adapted CNN algorithm in predicting the worst robustness
across diverse network topologies, encompassing both model and empirical
networks.</div><div><a href='http://arxiv.org/abs/2403.00027v1'>2403.00027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12946v1")'>Sample Complexity of Offline Distributionally Robust Linear Markov
  Decision Processes</div>
<div id='2403.12946v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:48:42Z</div><div>Authors: He Wang, Laixi Shi, Yuejie Chi</div><div style='padding-top: 10px; width: 80ex'>In offline reinforcement learning (RL), the absence of active exploration
calls for attention on the model robustness to tackle the sim-to-real gap,
where the discrepancy between the simulated and deployed environments can
significantly undermine the performance of the learned policy. To endow the
learned policy with robustness in a sample-efficient manner in the presence of
high-dimensional state-action space, this paper considers the sample complexity
of distributionally robust linear Markov decision processes (MDPs) with an
uncertainty set characterized by the total variation distance using offline
data. We develop a pessimistic model-based algorithm and establish its sample
complexity bound under minimal data coverage assumptions, which outperforms
prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We
further improve the performance guarantee of the proposed algorithm by
incorporating a carefully-designed variance estimator.</div><div><a href='http://arxiv.org/abs/2403.12946v1'>2403.12946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15399v1")'>Distributionally Robust Off-Dynamics Reinforcement Learning: Provable
  Efficiency with Linear Function Approximation</div>
<div id='2402.15399v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:01:44Z</div><div>Authors: Zhishuai Liu, Pan Xu</div><div style='padding-top: 10px; width: 80ex'>We study off-dynamics Reinforcement Learning (RL), where the policy is
trained on a source domain and deployed to a distinct target domain. We aim to
solve this problem via online distributionally robust Markov decision processes
(DRMDPs), where the learning algorithm actively interacts with the source
domain while seeking the optimal performance under the worst possible dynamics
that is within an uncertainty set of the source domain's transition kernel. We
provide the first study on online DRMDPs with function approximation for
off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity,
even when the nominal transition kernel is linear, leading to error
propagation. By designing a $d$-rectangular uncertainty set using the total
variation distance, we remove this additional nonlinearity and bypass the error
propagation. We then introduce DR-LSVI-UCB, the first provably efficient online
DRMDP algorithm for off-dynamics RL with function approximation, and establish
a polynomial suboptimality bound that is independent of the state and action
space sizes. Our work makes the first step towards a deeper understanding of
the provable efficiency of online DRMDPs with linear function approximation.
Finally, we substantiate the performance and robustness of DR-LSVI-UCB through
different numerical experiments.</div><div><a href='http://arxiv.org/abs/2402.15399v1'>2402.15399v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15781v1")'>Analysis of Off-Policy Multi-Step TD-Learning with Linear Function
  Approximation</div>
<div id='2402.15781v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T10:42:50Z</div><div>Authors: Donghwan Lee</div><div style='padding-top: 10px; width: 80ex'>This paper analyzes multi-step TD-learning algorithms within the `deadly
triad' scenario, characterized by linear function approximation, off-policy
learning, and bootstrapping. In particular, we prove that n-step TD-learning
algorithms converge to a solution as the sampling horizon n increases
sufficiently. The paper is divided into two parts. In the first part, we
comprehensively examine the fundamental properties of their model-based
deterministic counterparts, including projected value iteration, gradient
descent algorithms, and the control theoretic approach, which can be viewed as
prototype deterministic algorithms whose analysis plays a pivotal role in
understanding and developing their model-free reinforcement learning
counterparts. In particular, we prove that these algorithms converge to
meaningful solutions when n is sufficiently large. Based on these findings, two
n-step TD-learning algorithms are proposed and analyzed, which can be seen as
the model-free reinforcement learning counterparts of the gradient and control
theoretic algorithms.</div><div><a href='http://arxiv.org/abs/2402.15781v1'>2402.15781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09621v1")'>Minimax Optimal and Computationally Efficient Algorithms for
  Distributionally Robust Offline Reinforcement Learning</div>
<div id='2403.09621v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:55:10Z</div><div>Authors: Zhishuai Liu, Pan Xu</div><div style='padding-top: 10px; width: 80ex'>Distributionally robust offline reinforcement learning (RL), which seeks
robust policy training against environment perturbation by modeling dynamics
uncertainty, calls for function approximations when facing large state-action
spaces. However, the consideration of dynamics uncertainty introduces essential
nonlinearity and computational burden, posing unique challenges for analyzing
and practically employing function approximation. Focusing on a basic setting
where the nominal model and perturbed models are linearly parameterized, we
propose minimax optimal and computationally efficient algorithms realizing
function approximation and initiate the study on instance-dependent
suboptimality analysis in the context of robust offline RL. Our results uncover
that function approximation in robust offline RL is essentially distinct from
and probably harder than that in standard offline RL. Our algorithms and
theoretical results crucially depend on a variety of new techniques, involving
a novel function approximation mechanism incorporating variance information, a
new procedure of suboptimality and estimation uncertainty decomposition, a
quantification of the robust value function shrinkage, and a meticulously
designed family of hard instances, which might be of independent interest.</div><div><a href='http://arxiv.org/abs/2403.09621v1'>2403.09621v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12216v1")'>Mitigating Covariate Shift in Misspecified Regression with Applications
  to Reinforcement Learning</div>
<div id='2401.12216v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:59:12Z</div><div>Authors: Philip Amortila, Tongyi Cao, Akshay Krishnamurthy</div><div style='padding-top: 10px; width: 80ex'>A pervasive phenomenon in machine learning applications is distribution
shift, where training and deployment conditions for a machine learning model
differ. As distribution shift typically results in a degradation in
performance, much attention has been devoted to algorithmic interventions that
mitigate these detrimental effects. In this paper, we study the effect of
distribution shift in the presence of model misspecification, specifically
focusing on $L_{\infty}$-misspecified regression and adversarial covariate
shift, where the regression target remains fixed while the covariate
distribution changes arbitrarily. We show that empirical risk minimization, or
standard least squares regression, can result in undesirable misspecification
amplification where the error due to misspecification is amplified by the
density ratio between the training and testing distributions. As our main
result, we develop a new algorithm -- inspired by robust optimization
techniques -- that avoids this undesirable behavior, resulting in no
misspecification amplification while still obtaining optimal statistical rates.
As applications, we use this regression procedure to obtain new guarantees in
offline and online reinforcement learning with misspecification and establish
new separations between previously studied structural conditions and notions of
coverage.</div><div><a href='http://arxiv.org/abs/2401.12216v1'>2401.12216v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18159v1")'>Provable Risk-Sensitive Distributional Reinforcement Learning with
  General Function Approximation</div>
<div id='2402.18159v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T08:43:18Z</div><div>Authors: Yu Chen, Xiangcheng Zhang, Siwei Wang, Longbo Huang</div><div style='padding-top: 10px; width: 80ex'>In the realm of reinforcement learning (RL), accounting for risk is crucial
for making decisions under uncertainty, particularly in applications where
safety and reliability are paramount. In this paper, we introduce a general
framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL),
with static Lipschitz Risk Measures (LRM) and general function approximation.
Our framework covers a broad class of risk-sensitive RL, and facilitates
analysis of the impact of estimation functions on the effectiveness of RSRL
strategies and evaluation of their sample complexity. We design two innovative
meta-algorithms: \texttt{RS-DisRL-M}, a model-based strategy for model-based
function approximation, and \texttt{RS-DisRL-V}, a model-free approach for
general value function approximation. With our novel estimation techniques via
Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in
distributional RL with augmented Markov Decision Process (MDP), we derive the
first $\widetilde{\mathcal{O}}(\sqrt{K})$ dependency of the regret upper bound
for RSRL with static LRM, marking a pioneering contribution towards
statistically efficient algorithms in this domain.</div><div><a href='http://arxiv.org/abs/2402.18159v1'>2402.18159v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06323v1")'>Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to
  Standard RL</div>
<div id='2403.06323v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T21:45:12Z</div><div>Authors: Kaiwen Wang, Dawen Liang, Nathan Kallus, Wen Sun</div><div style='padding-top: 10px; width: 80ex'>We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized
Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk
(CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov
Decision Process (MDP), we propose two general meta-algorithms via reductions
to standard RL: one based on optimistic algorithms and another based on policy
optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL
theory with entropic risk or CVaR. Under discrete rewards, our optimistic
theory also certifies the first RSRL regret bounds for MDPs with bounded
coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy
optimization meta-algorithm enjoys both global convergence and local
improvement guarantees in a novel metric that lower bounds the true OCE risk.
Finally, we instantiate our framework with PPO, construct an MDP, and show that
it learns the optimal risk-sensitive policy while prior algorithms provably
fail.</div><div><a href='http://arxiv.org/abs/2403.06323v1'>2403.06323v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18149v1")'>Provably Efficient Partially Observable Risk-Sensitive Reinforcement
  Learning with Hindsight Observation</div>
<div id='2402.18149v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T08:24:06Z</div><div>Authors: Tonghe Zhang, Yu Chen, Longbo Huang</div><div style='padding-top: 10px; width: 80ex'>This work pioneers regret analysis of risk-sensitive reinforcement learning
in partially observable environments with hindsight observation, addressing a
gap in theoretical exploration. We introduce a novel formulation that
integrates hindsight observations into a Partially Observable Markov Decision
Process (POMDP) framework, where the goal is to optimize accumulated reward
under the entropic risk measure. We develop the first provably efficient RL
algorithm tailored for this setting. We also prove by rigorous analysis that
our algorithm achieves polynomial regret
$\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$,
which outperforms or matches existing upper bounds when the model degenerates
to risk-neutral or fully observable settings. We adopt the method of
change-of-measure and develop a novel analytical tool of beta vectors to
streamline mathematical derivations. These techniques are of particular
interest to the theoretical study of reinforcement learning.</div><div><a href='http://arxiv.org/abs/2402.18149v1'>2402.18149v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11841v1")'>Pessimistic Causal Reinforcement Learning with Mediators for Confounded
  Offline Data</div>
<div id='2403.11841v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:51:19Z</div><div>Authors: Danyang Wang, Chengchun Shi, Shikai Luo, Will Wei Sun</div><div style='padding-top: 10px; width: 80ex'>In real-world scenarios, datasets collected from randomized experiments are
often constrained by size, due to limitations in time and budget. As a result,
leveraging large observational datasets becomes a more attractive option for
achieving high-quality policy learning. However, most existing offline
reinforcement learning (RL) methods depend on two key
assumptions--unconfoundedness and positivity--which frequently do not hold in
observational data contexts. Recognizing these challenges, we propose a novel
policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the
mediator variable based on front-door criterion to remove the confounding bias;
additionally, we adopt the pessimistic principle to address the distributional
shift between the action distributions induced by candidate policies, and the
behavior policy that generates the observational data. Our key observation is
that, by incorporating auxiliary variables that mediate the effect of actions
on system dynamics, it is sufficient to learn a lower bound of the mediator
distribution function, instead of the Q-function, to partially mitigate the
issue of distributional shift. This insight significantly simplifies our
algorithm, by circumventing the challenging task of sequential uncertainty
quantification for the estimated Q-function. Moreover, we provide theoretical
guarantees for the algorithms we propose, and demonstrate their efficacy
through simulations, as well as real-world experiments utilizing offline
datasets from a leading ride-hailing platform.</div><div><a href='http://arxiv.org/abs/2403.11841v1'>2403.11841v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15188v1")'>Parameter-Free Algorithms for Performative Regret Minimization under
  Decision-Dependent Distributions</div>
<div id='2402.15188v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:36:28Z</div><div>Authors: Sungwoo Park, Junyeop Kwon, Byeongnoh Kim, Suhyun Chae, Jeeyong Lee, Dabeen Lee</div><div style='padding-top: 10px; width: 80ex'>This paper studies performative risk minimization, a formulation of
stochastic optimization under decision-dependent distributions. We consider the
general case where the performative risk can be non-convex, for which we
develop efficient parameter-free optimistic optimization-based methods. Our
algorithms significantly improve upon the existing Lipschitz bandit-based
method in many aspects. In particular, our framework does not require knowledge
about the sensitivity parameter of the distribution map and the Lipshitz
constant of the loss function. This makes our framework practically favorable,
together with the efficient optimistic optimization-based tree-search
mechanism. We provide experimental results that demonstrate the numerical
superiority of our algorithms over the existing method and other black-box
optimistic optimization methods.</div><div><a href='http://arxiv.org/abs/2402.15188v1'>2402.15188v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08929v1")'>Second Order Methods for Bandit Optimization and Control</div>
<div id='2402.08929v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T04:03:38Z</div><div>Authors: Arun Suggala, Y. Jennifer Sun, Praneeth Netrapalli, Elad Hazan</div><div style='padding-top: 10px; width: 80ex'>Bandit convex optimization (BCO) is a general framework for online decision
making under uncertainty. While tight regret bounds for general convex losses
have been established, existing algorithms achieving these bounds have
prohibitive computational costs for high dimensional data.
  In this paper, we propose a simple and practical BCO algorithm inspired by
the online Newton step algorithm. We show that our algorithm achieves optimal
(in terms of horizon) regret bounds for a large class of convex functions that
we call $\kappa$-convex. This class contains a wide range of practically
relevant loss functions including linear, quadratic, and generalized linear
models. In addition to optimal regret, this method is the most efficient known
algorithm for several well-studied applications including bandit logistic
regression.
  Furthermore, we investigate the adaptation of our second-order bandit
algorithm to online convex optimization with memory. We show that for loss
functions with a certain affine structure, the extended algorithm attains
optimal regret. This leads to an algorithm with optimal regret for bandit
LQR/LQG problems under a fully adversarial noise model, thereby resolving an
open question posed in \citep{gradu2020non} and \citep{sun2023optimal}.
  Finally, we show that the more general problem of BCO with (non-affine)
memory is harder. We derive a $\tilde{\Omega}(T^{2/3})$ regret lower bound,
even under the assumption of smooth and quadratic losses.</div><div><a href='http://arxiv.org/abs/2402.08929v1'>2402.08929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09278v1")'>Adaptive Regret for Bandits Made Possible: Two Queries Suffice</div>
<div id='2401.09278v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T15:32:04Z</div><div>Authors: Zhou Lu, Qiuyi Zhang, Xinyi Chen, Fred Zhang, David Woodruff, Elad Hazan</div><div style='padding-top: 10px; width: 80ex'>Fast changing states or volatile environments pose a significant challenge to
online optimization, which needs to perform rapid adaptation under limited
observation. In this paper, we give query and regret optimal bandit algorithms
under the strict notion of strongly adaptive regret, which measures the maximum
regret over any contiguous interval $I$. Due to its worst-case nature, there is
an almost-linear $\Omega(|I|^{1-\epsilon})$ regret lower bound, when only one
query per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just
two queries per round, we give Strongly Adaptive Bandit Learner (StABL) that
achieves $\tilde{O}(\sqrt{n|I|})$ adaptive regret for multi-armed bandits with
$n$ arms. The bound is tight and cannot be improved in general. Our algorithm
leverages a multiplicative update scheme of varying stepsizes and a carefully
chosen observation distribution to control the variance. Furthermore, we extend
our results and provide optimal algorithms in the bandit convex optimization
setting. Finally, we empirically demonstrate the superior performance of our
algorithms under volatile environments and for downstream tasks, such as
algorithm selection for hyperparameter optimization.</div><div><a href='http://arxiv.org/abs/2401.09278v1'>2401.09278v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01632v2")'>Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown
  Hyperparameters Of Any Type</div>
<div id='2402.01632v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:52:16Z</div><div>Authors: Juliusz Ziomek, Masaki Adachi, Michael A. Osborne</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimisation requires fitting a Gaussian process model, which in
turn requires specifying hyperparameters - most of the theoretical literature
assumes those hyperparameters are known. The commonly used maximum likelihood
estimator for hyperparameters of the Gaussian process is consistent only if the
data fills the space uniformly, which does not have to be the case in Bayesian
optimisation. Since no guarantees exist regarding the correctness of
hyperparameter estimation, and those hyperparameters can significantly affect
the Gaussian process fit, theoretical analysis of Bayesian optimisation with
unknown hyperparameters is very challenging. Previously proposed algorithms
with the no-regret property were only able to handle the special case of
unknown lengthscales, reproducing kernel Hilbert space norm and applied only to
the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the
first algorithm enjoying the no-regret property in the case of unknown
hyperparameters of arbitrary form, and which supports both Bayesian and
frequentist settings. Our proof idea is novel and can easily be extended to
other variants of Bayesian optimisation. We show this by extending our
algorithm to the adversarially robust optimisation setting under unknown
hyperparameters. Finally, we empirically evaluate our algorithm on a set of toy
problems and show that it can outperform the maximum likelihood estimator.</div><div><a href='http://arxiv.org/abs/2402.01632v2'>2402.01632v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04764v1")'>Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a
  provably efficient algorithm for batch Bayesian Optimization</div>
<div id='2403.04764v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:58:26Z</div><div>Authors: Zhaolin Ren, Na Li</div><div style='padding-top: 10px; width: 80ex'>This paper presents a new approach for batch Bayesian Optimization (BO),
where the sampling takes place by minimizing a Thompson Sampling approximation
of a regret to uncertainty ratio. Our objective is able to coordinate the
actions chosen in each batch in a way that minimizes redundancy between points
whilst focusing on points with high predictive means or high uncertainty. We
provide high-probability theoretical guarantees on the regret of our algorithm.
Finally, numerically, we demonstrate that our method attains state-of-the-art
performance on a range of nonconvex test functions, where it outperforms
several competitive benchmark batch BO algorithms by an order of magnitude on
average.</div><div><a href='http://arxiv.org/abs/2403.04764v1'>2403.04764v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16811v1")'>Stopping Bayesian Optimization with Probabilistic Regret Bounds</div>
<div id='2402.16811v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:34:58Z</div><div>Authors: James T. Wilson</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimization is a popular framework for efficiently finding
high-quality solutions to difficult problems based on limited prior
information. As a rule, these algorithms operate by iteratively choosing what
to try next until some predefined budget has been exhausted. We investigate
replacing this de facto stopping rule with an $(\epsilon, \delta)$-criterion:
stop when a solution has been found whose value is within $\epsilon &gt; 0$ of the
optimum with probability at least $1 - \delta$ under the model. Given access to
the prior distribution of problems, we show how to verify this condition in
practice using a limited number of draws from the posterior. For Gaussian
process priors, we prove that Bayesian optimization with the proposed criterion
stops in finite time and returns a point that satisfies the $(\epsilon,
\delta)$-criterion under mild assumptions. These findings are accompanied by
extensive empirical results which demonstrate the strengths and weaknesses of
this approach.</div><div><a href='http://arxiv.org/abs/2402.16811v1'>2402.16811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01981v2")'>Beyond Regrets: Geometric Metrics for Bayesian Optimization</div>
<div id='2401.01981v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T20:59:52Z</div><div>Authors: Jungtaek Kim</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimization is a principled optimization strategy for a black-box
objective function. It shows its effectiveness in a wide variety of real-world
applications such as scientific discovery and experimental design. In general,
the performance of Bayesian optimization is reported through regret-based
metrics such as instantaneous, simple, and cumulative regrets. These metrics
only rely on function evaluations, so that they do not consider geometric
relationships between query points and global solutions, or query points
themselves. Notably, they cannot discriminate if multiple global solutions are
successfully found. Moreover, they do not evaluate Bayesian optimization's
abilities to exploit and explore a search space given. To tackle these issues,
we propose four new geometric metrics, i.e., precision, recall, average degree,
and average distance. These metrics allow us to compare Bayesian optimization
algorithms considering the geometry of both query points and global optima, or
query points. However, they are accompanied by an extra parameter, which needs
to be carefully determined. We therefore devise the parameter-free forms of the
respective metrics by integrating out the additional parameter. Finally, we
empirically validate that our proposed metrics can provide more delicate
interpretation of Bayesian optimization algorithms, on top of assessment via
the conventional metrics.</div><div><a href='http://arxiv.org/abs/2401.01981v2'>2401.01981v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00898v1")'>The Algorithm Configuration Problem</div>
<div id='2403.00898v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T17:29:34Z</div><div>Authors: Gabriele Iommazzo, Claudia D'Ambrosio, Antonio Frangioni, Leo Liberti</div><div style='padding-top: 10px; width: 80ex'>The field of algorithmic optimization has significantly advanced with the
development of methods for the automatic configuration of algorithmic
parameters. This article delves into the Algorithm Configuration Problem,
focused on optimizing parametrized algorithms for solving specific instances of
decision/optimization problems. We present a comprehensive framework that not
only formalizes the Algorithm Configuration Problem, but also outlines
different approaches for its resolution, leveraging machine learning models and
heuristic strategies. The article categorizes existing methodologies into
per-instance and per-problem approaches, distinguishing between offline and
online strategies for model construction and deployment. By synthesizing these
approaches, we aim to provide a clear pathway for both understanding and
addressing the complexities inherent in algorithm configuration.</div><div><a href='http://arxiv.org/abs/2403.00898v1'>2403.00898v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12745v1")'>On the Utility of Probing Trajectories for Algorithm-Selection</div>
<div id='2401.12745v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T13:23:59Z</div><div>Authors: Quentin Renau, Emma Hart</div><div style='padding-top: 10px; width: 80ex'>Machine-learning approaches to algorithm-selection typically take data
describing an instance as input. Input data can take the form of features
derived from the instance description or fitness landscape, or can be a direct
representation of the instance itself, i.e. an image or textual description.
Regardless of the choice of input, there is an implicit assumption that
instances that are similar will elicit similar performance from algorithm, and
that a model is capable of learning this relationship. We argue that viewing
algorithm-selection purely from an instance perspective can be misleading as it
fails to account for how an algorithm `views' similarity between instances. We
propose a novel `algorithm-centric' method for describing instances that can be
used to train models for algorithm-selection: specifically, we use short
probing trajectories calculated by applying a solver to an instance for a very
short period of time. The approach is demonstrated to be promising, providing
comparable or better results to computationally expensive landscape-based
feature-based approaches. Furthermore, projecting the trajectories into a
2-dimensional space illustrates that functions that are similar from an
algorithm-perspective do not necessarily correspond to the accepted
categorisation of these functions from a human perspective.</div><div><a href='http://arxiv.org/abs/2401.12745v1'>2401.12745v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01579v1")'>An Invariant Information Geometric Method for High-Dimensional Online
  Optimization</div>
<div id='2401.01579v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T07:06:26Z</div><div>Authors: Zhengfei Zhang, Yunyue Wei, Yanan Sui</div><div style='padding-top: 10px; width: 80ex'>Sample efficiency is crucial in optimization, particularly in black-box
scenarios characterized by expensive evaluations and zeroth-order feedback.
When computing resources are plentiful, Bayesian optimization is often favored
over evolution strategies. In this paper, we introduce a full invariance
oriented evolution strategies algorithm, derived from its corresponding
framework, that effectively rivals the leading Bayesian optimization method in
tasks with dimensions at the upper limit of Bayesian capability. Specifically,
we first build the framework InvIGO that fully incorporates historical
information while retaining the full invariant and computational complexity. We
then exemplify InvIGO on multi-dimensional Gaussian, which gives an invariant
and scalable optimizer SynCMA . The theoretical behavior and advantages of our
algorithm over other Gaussian-based evolution strategies are further analyzed.
Finally, We benchmark SynCMA against leading algorithms in Bayesian
optimization and evolution strategies on various high dimension tasks, in
cluding Mujoco locomotion tasks, rover planning task and synthetic functions.
In all scenarios, SynCMA demonstrates great competence, if not dominance, over
other algorithms in sample efficiency, showing the underdeveloped potential of
property oriented evolution strategies.</div><div><a href='http://arxiv.org/abs/2401.01579v1'>2401.01579v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15731v1")'>Clustering in Dynamic Environments: A Framework for Benchmark Dataset
  Generation With Heterogeneous Changes</div>
<div id='2402.15731v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T05:49:27Z</div><div>Authors: Danial Yazdani, Juergen Branke, Mohammad Sadegh Khorshidi, Mohammad Nabi Omidvar, Xiaodong Li, Amir H. Gandomi, Xin Yao</div><div style='padding-top: 10px; width: 80ex'>Clustering in dynamic environments is of increasing importance, with broad
applications ranging from real-time data analysis and online unsupervised
learning to dynamic facility location problems. While meta-heuristics have
shown promising effectiveness in static clustering tasks, their application for
tracking optimal clustering solutions or robust clustering over time in dynamic
environments remains largely underexplored. This is partly due to a lack of
dynamic datasets with diverse, controllable, and realistic dynamic
characteristics, hindering systematic performance evaluations of clustering
algorithms in various dynamic scenarios. This deficiency leads to a gap in our
understanding and capability to effectively design algorithms for clustering in
dynamic environments. To bridge this gap, this paper introduces the Dynamic
Dataset Generator (DDG). DDG features multiple dynamic Gaussian components
integrated with a range of heterogeneous, local, and global changes. These
changes vary in spatial and temporal severity, patterns, and domain of
influence, providing a comprehensive tool for simulating a wide range of
dynamic scenarios.</div><div><a href='http://arxiv.org/abs/2402.15731v1'>2402.15731v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00678v1")'>Real Evaluations Tractability using Continuous Goal-Directed Actions in
  Smart City Applications</div>
<div id='2402.00678v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:38:21Z</div><div>Authors: Raul Fernandez-Fernandez, Juan G. Victores, David Estevez, Carlos Balaguer</div><div style='padding-top: 10px; width: 80ex'>One of the most important challenges of Smart City Applications is to adapt
the system to interact with non-expert users. Robot imitation frameworks aim to
simplify and reduce times of robot programming by allowing users to program
directly through demonstrations. In classical frameworks, actions are modeled
using joint or Cartesian space trajectories. Other features, such as visual
ones, are not always well represented with these pure geometrical approaches.
Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as
it encodes actions as changes of any feature that can be extracted from the
environment. As a consequence of this, the robot joint trajectories for
execution must be fully computed to comply with this feature-agnostic encoding.
This is achieved using Evolutionary Algorithms (EA), which usually requires too
many evaluations to perform this evolution step in the actual robot. Current
strategies involve performing evaluations in a simulation, transferring the
final joint trajectory to the actual robot. Smart City applications involve
working in highly dynamic and complex environments, where having a precise
model is not always achievable. Our goal is to study the tractability of
performing these evaluations directly in a real-world scenario. Two different
approaches to reduce the number of evaluations using EA, are proposed and
compared. In the first approach, Particle Swarm Optimization (PSO)-based
methods have been studied and compared within CGDA: naive PSO, Fitness
Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO
(AFFG-PSO). The second approach studied the introduction of geometrical and
velocity constraints within CGDA. The effects of both approaches were analyzed
and compared in the wax and paint actions, two CGDA commonly studied use cases.
Results from this paper depict an important reduction in the number of
evaluations.</div><div><a href='http://arxiv.org/abs/2402.00678v1'>2402.00678v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12794v1")'>Autonomous Reality Modelling for Cultural Heritage Sites employing
  cooperative quadrupedal robots and unmanned aerial vehicles</div>
<div id='2402.12794v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T08:08:07Z</div><div>Authors: Nikolaos Giakoumidis, Christos-Nikolaos Anagnostopoulos</div><div style='padding-top: 10px; width: 80ex'>Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners,
mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has
become the prevalent practice for 3D Reality Modeling and digitization of
large-scale monuments of Cultural Heritage (CH). In practice, this process is
heavily related to the expertise of the surveying team, handling the laborious
planning and time-consuming execution of the 3D mapping process that is
tailored to the specific requirements and constraints of each site. To minimize
human intervention, this paper introduces a novel methodology for autonomous 3D
Reality Modeling for CH monuments by employing au-tonomous biomimetic
quadrupedal robotic agents and UAVs equipped with the appropriate sensors.
These autonomous robotic agents carry out the 3D RM process in a systematic and
repeatable ap-proach. The outcomes of this automated process may find
applications in digital twin platforms, facilitating secure monitoring and
management of cultural heritage sites and spaces, in both indoor and outdoor
environments.</div><div><a href='http://arxiv.org/abs/2402.12794v1'>2402.12794v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15997v2")'>Cieran: Designing Sequential Colormaps via In-Situ Active Preference
  Learning</div>
<div id='2402.15997v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T05:45:36Z</div><div>Authors: Matt-Heun Hong, Zachary N. Sunberg, Danielle Albers Szafir</div><div style='padding-top: 10px; width: 80ex'>Quality colormaps can help communicate important data patterns. However,
finding an aesthetically pleasing colormap that looks "just right" for a given
scenario requires significant design and technical expertise. We introduce
Cieran, a tool that allows any data analyst to rapidly find quality colormaps
while designing charts within Jupyter Notebooks. Our system employs an active
preference learning paradigm to rank expert-designed colormaps and create new
ones from pairwise comparisons, allowing analysts who are novices in color
design to tailor colormaps to their data context. We accomplish this by
treating colormap design as a path planning problem through the CIELAB
colorspace with a context-specific reward model. In an evaluation with twelve
scientists, we found that Cieran effectively modeled user preferences to rank
colormaps and leveraged this model to create new quality designs. Our work
shows the potential of active preference learning for supporting efficient
visualization design optimization.</div><div><a href='http://arxiv.org/abs/2402.15997v2'>2402.15997v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05041v1")'>Learning to Configure Mathematical Programming Solvers by Mathematical
  Programming</div>
<div id='2401.05041v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T10:02:01Z</div><div>Authors: Gabriele Iommazzo, Claudia D'Ambrosio, Antonio Frangioni, Leo Liberti</div><div style='padding-top: 10px; width: 80ex'>We discuss the issue of finding a good mathematical programming solver
configuration for a particular instance of a given problem, and we propose a
two-phase approach to solve it. In the first phase we learn the relationships
between the instance, the configuration and the performance of the configured
solver on the given instance. A specific difficulty of learning a good solver
configuration is that parameter settings may not all be independent; this
requires enforcing (hard) constraints, something that many widely used
supervised learning methods cannot natively achieve. We tackle this issue in
the second phase of our approach, where we use the learnt information to
construct and solve an optimization problem having an explicit representation
of the dependency/consistency constraints on the configuration parameter
settings. We discuss computational results for two different instantiations of
this approach on a unit commitment problem arising in the short-term planning
of hydro valleys. We use logistic regression as the supervised learning
methodology and consider CPLEX as the solver of interest.</div><div><a href='http://arxiv.org/abs/2401.05041v1'>2401.05041v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02328v1")'>Data-driven algorithm design using neural networks with applications to
  branch-and-cut</div>
<div id='2402.02328v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T03:03:27Z</div><div>Authors: Hongyu Cheng, Sammy Khalife, Barbara Fiedorowicz, Amitabh Basu</div><div style='padding-top: 10px; width: 80ex'>Data-driven algorithm design is a paradigm that uses statistical and machine
learning techniques to select from a class of algorithms for a computational
problem an algorithm that has the best expected performance with respect to
some (unknown) distribution on the instances of the problem. We build upon
recent work in this line of research by introducing the idea where, instead of
selecting a single algorithm that has the best performance, we allow the
possibility of selecting an algorithm based on the instance to be solved. In
particular, given a representative sample of instances, we learn a neural
network that maps an instance of the problem to the most appropriate algorithm
{\em for that instance}. We formalize this idea and derive rigorous sample
complexity bounds for this learning problem, in the spirit of recent work in
data-driven algorithm design. We then apply this approach to the problem of
making good decisions in the branch-and-cut framework for mixed-integer
optimization (e.g., which cut to add?). In other words, the neural network will
take as input a mixed-integer optimization instance and output a decision that
will result in a small branch-and-cut tree for that instance. Our computational
results provide evidence that our particular way of using neural networks for
cut selection can make a significant impact in reducing branch-and-cut tree
sizes, compared to previous data-driven approaches.</div><div><a href='http://arxiv.org/abs/2402.02328v1'>2402.02328v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10811v1")'>Simulation Based Bayesian Optimization</div>
<div id='2401.10811v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:56:11Z</div><div>Authors: Roi Naveiro, Becky Tang</div><div style='padding-top: 10px; width: 80ex'>Bayesian Optimization (BO) is a powerful method for optimizing black-box
functions by combining prior knowledge with ongoing function evaluations. BO
constructs a probabilistic surrogate model of the objective function given the
covariates, which is in turn used to inform the selection of future evaluation
points through an acquisition function. For smooth continuous search spaces,
Gaussian Processes (GPs) are commonly used as the surrogate model as they offer
analytical access to posterior predictive distributions, thus facilitating the
computation and optimization of acquisition functions. However, in complex
scenarios involving optimizations over categorical or mixed covariate spaces,
GPs may not be ideal.
  This paper introduces Simulation Based Bayesian Optimization (SBBO) as a
novel approach to optimizing acquisition functions that only requires
\emph{sampling-based} access to posterior predictive distributions. SBBO allows
the use of surrogate probabilistic models tailored for combinatorial spaces
with discrete variables. Any Bayesian model in which posterior inference is
carried out through Markov chain Monte Carlo can be selected as the surrogate
model in SBBO. In applications involving combinatorial optimization, we
demonstrate empirically the effectiveness of SBBO method using various choices
of surrogate models.</div><div><a href='http://arxiv.org/abs/2401.10811v1'>2401.10811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16373v1")'>Bayesian optimization as a flexible and efficient design framework for
  sustainable process systems</div>
<div id='2401.16373v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:12:32Z</div><div>Authors: Joel A. Paulson, Calvin Tsay</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimization (BO) is a powerful technology for optimizing noisy
expensive-to-evaluate black-box functions, with a broad range of real-world
applications in science, engineering, economics, manufacturing, and beyond. In
this paper, we provide an overview of recent developments, challenges, and
opportunities in BO for design of next-generation process systems. After
describing several motivating applications, we discuss how advanced BO methods
have been developed to more efficiently tackle important problems in these
applications. We conclude the paper with a summary of challenges and
opportunities related to improving the quality of the probabilistic model, the
choice of internal optimization procedure used to select the next sample point,
and the exploitation of problem structure to improve sample efficiency.</div><div><a href='http://arxiv.org/abs/2401.16373v1'>2401.16373v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17343v1")'>Enhanced Bayesian Optimization via Preferential Modeling of Abstract
  Properties</div>
<div id='2402.17343v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T09:23:13Z</div><div>Authors: Arun Kumar A V, Alistair Shilton, Sunil Gupta, Santu Rana, Stewart Greenhill, Svetha Venkatesh</div><div style='padding-top: 10px; width: 80ex'>Experimental (design) optimization is a key driver in designing and
discovering new products and processes. Bayesian Optimization (BO) is an
effective tool for optimizing expensive and black-box experimental design
processes. While Bayesian optimization is a principled data-driven approach to
experimental optimization, it learns everything from scratch and could greatly
benefit from the expertise of its human (domain) experts who often reason about
systems at different abstraction levels using physical properties that are not
necessarily directly measured (or measurable). In this paper, we propose a
human-AI collaborative Bayesian framework to incorporate expert preferences
about unmeasured abstract properties into the surrogate modeling to further
boost the performance of BO. We provide an efficient strategy that can also
handle any incorrect/misleading expert bias in preferential judgments. We
discuss the convergence behavior of our proposed framework. Our experimental
results involving synthetic functions and real-world datasets show the
superiority of our method against the baselines.</div><div><a href='http://arxiv.org/abs/2402.17343v1'>2402.17343v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17704v1")'>Transfer Learning Bayesian Optimization to Design Competitor DNA
  Molecules for Use in Diagnostic Assays</div>
<div id='2402.17704v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:30:33Z</div><div>Authors: Ruby Sedgwick, John P. Goertz, Molly M. Stevens, Ruth Misener, Mark van der Wilk</div><div style='padding-top: 10px; width: 80ex'>With the rise in engineered biomolecular devices, there is an increased need
for tailor-made biological sequences. Often, many similar biological sequences
need to be made for a specific application meaning numerous, sometimes
prohibitively expensive, lab experiments are necessary for their optimization.
This paper presents a transfer learning design of experiments workflow to make
this development feasible. By combining a transfer learning surrogate model
with Bayesian optimization, we show how the total number of experiments can be
reduced by sharing information between optimization tasks. We demonstrate the
reduction in the number of experiments using data from the development of DNA
competitors for use in an amplification-based diagnostic assay. We use
cross-validation to compare the predictive accuracy of different transfer
learning models, and then compare the performance of the models for both single
objective and penalized optimization tasks.</div><div><a href='http://arxiv.org/abs/2402.17704v1'>2402.17704v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04837v1")'>Cell reprogramming design by transfer learning of functional
  transcriptional networks</div>
<div id='2403.04837v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T19:00:02Z</div><div>Authors: Thomas P. Wytock, Adilson E. Motter</div><div style='padding-top: 10px; width: 80ex'>Recent developments in synthetic biology, next-generation sequencing, and
machine learning provide an unprecedented opportunity to rationally design new
disease treatments based on measured responses to gene perturbations and drugs
to reprogram cells. The main challenges to seizing this opportunity are the
incomplete knowledge of the cellular network and the combinatorial explosion of
possible interventions, both of which are insurmountable by experiments. To
address these challenges, we develop a transfer learning approach to control
cell behavior that is pre-trained on transcriptomic data associated with human
cell fates, thereby generating a model of the network dynamics that can be
transferred to specific reprogramming goals. The approach combines
transcriptional responses to gene perturbations to minimize the difference
between a given pair of initial and target transcriptional states. We
demonstrate our approach's versatility by applying it to a microarray dataset
comprising &gt;9,000 microarrays across 54 cell types and 227 unique
perturbations, and an RNASeq dataset consisting of &gt;10,000 sequencing runs
across 36 cell types and 138 perturbations. Our approach reproduces known
reprogramming protocols with an AUROC of 0.91 while innovating over existing
methods by pre-training an adaptable model that can be tailored to specific
reprogramming transitions. We show that the number of gene perturbations
required to steer from one fate to another increases with decreasing
developmental relatedness and that fewer genes are needed to progress along
developmental paths than to regress. These findings establish a
proof-of-concept for our approach to computationally design control strategies
and provide insights into how gene regulatory networks govern phenotype.</div><div><a href='http://arxiv.org/abs/2403.04837v1'>2403.04837v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03006v1")'>On the development of a practical Bayesian optimisation algorithm for
  expensive experiments and simulations with changing environmental conditions</div>
<div id='2402.03006v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:46:04Z</div><div>Authors: Mike Diessner, Kevin J. Wilson, Richard D. Whalley</div><div style='padding-top: 10px; width: 80ex'>Experiments in engineering are typically conducted in controlled environments
where parameters can be set to any desired value. This assumes that the same
applies in a real-world setting -- an assumption that is often incorrect as
many experiments are influenced by uncontrollable environmental conditions such
as temperature, humidity and wind speed. When optimising such experiments, the
focus should lie on finding optimal values conditionally on these
uncontrollable variables. This article extends Bayesian optimisation to the
optimisation of systems in changing environments that include controllable and
uncontrollable parameters. The extension fits a global surrogate model over all
controllable and environmental variables but optimises only the controllable
parameters conditional on measurements of the uncontrollable variables. The
method is validated on two synthetic test functions and the effects of the
noise level, the number of the environmental parameters, the parameter
fluctuation, the variability of the uncontrollable parameters, and the
effective domain size are investigated. ENVBO, the proposed algorithm resulting
from this investigation, is applied to a wind farm simulator with eight
controllable and one environmental parameter. ENVBO finds solutions for the
full domain of the environmental variable that outperforms results from
optimisation algorithms that only focus on a fixed environmental value in all
but one case while using a fraction of their evaluation budget. This makes the
proposed approach very sample-efficient and cost-effective. An off-the-shelf
open-source version of ENVBO is available via the NUBO Python package.</div><div><a href='http://arxiv.org/abs/2402.03006v1'>2402.03006v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03816v1")'>Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box
  Simulators with Noise Parameters</div>
<div id='2403.03816v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:03:37Z</div><div>Authors: John Joshua Miller, Simon Mak</div><div style='padding-top: 10px; width: 80ex'>The optimization of a black-box simulator over control parameters
$\mathbf{x}$ arises in a myriad of scientific applications. In such
applications, the simulator often takes the form
$f(\mathbf{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ are parameters
that are uncertain in practice. Robust optimization aims to optimize the
objective $\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$, where
$\boldsymbol{\Theta} \sim \mathcal{P}$ is a random variable that models
uncertainty on $\boldsymbol{\theta}$. For this, existing black-box methods
typically employ a two-stage approach for selecting the next point
$(\mathbf{x},\boldsymbol{\theta})$, where $\mathbf{x}$ and
$\boldsymbol{\theta}$ are optimized separately via different acquisition
functions. As such, these approaches do not employ a joint acquisition over
$(\mathbf{x},\boldsymbol{\theta})$, and thus may fail to fully exploit
control-to-noise interactions for effective robust optimization. To address
this, we propose a new Bayesian optimization method called Targeted Variance
Reduction (TVR). The TVR leverages a novel joint acquisition function over
$(\mathbf{x},\boldsymbol{\theta})$, which targets variance reduction on the
objective within the desired region of improvement. Under a Gaussian process
surrogate on $f$, the TVR acquisition can be evaluated in closed form, and
reveals an insightful exploration-exploitation-precision trade-off for robust
black-box optimization. The TVR can further accommodate a broad class of
non-Gaussian distributions on $\mathcal{P}$ via a careful integration of
normalizing flows. We demonstrate the improved performance of TVR over the
state-of-the-art in a suite of numerical experiments and an application to the
robust design of automobile brake discs under operational uncertainty.</div><div><a href='http://arxiv.org/abs/2403.03816v1'>2403.03816v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06816v1")'>Efficient first-order algorithms for large-scale, non-smooth maximum
  entropy models with application to wildfire science</div>
<div id='2403.06816v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:33:55Z</div><div>Authors: Gabriel P. Langlois, Jatan Buch, J√©r√¥me Darbon</div><div style='padding-top: 10px; width: 80ex'>Maximum entropy (Maxent) models are a class of statistical models that use
the maximum entropy principle to estimate probability distributions from data.
Due to the size of modern data sets, Maxent models need efficient optimization
algorithms to scale well for big data applications. State-of-the-art algorithms
for Maxent models, however, were not originally designed to handle big data
sets; these algorithms either rely on technical devices that may yield
unreliable numerical results, scale poorly, or require smoothness assumptions
that many practical Maxent models lack. In this paper, we present novel
optimization algorithms that overcome the shortcomings of state-of-the-art
algorithms for training large-scale, non-smooth Maxent models. Our proposed
first-order algorithms leverage the Kullback-Leibler divergence to train
large-scale and non-smooth Maxent models efficiently. For Maxent models with
discrete probability distribution of $n$ elements built from samples, each
containing $m$ features, the stepsize parameters estimation and iterations in
our algorithms scale on the order of $O(mn)$ operations and can be trivially
parallelized. Moreover, the strong $\ell_{1}$ convexity of the
Kullback--Leibler divergence allows for larger stepsize parameters, thereby
speeding up the convergence rate of our algorithms. To illustrate the
efficiency of our novel algorithms, we consider the problem of estimating
probabilities of fire occurrences as a function of ecological features in the
Western US MTBS-Interagency wildfire data set. Our numerical results show that
our algorithms outperform the state of the arts by one order of magnitude and
yield results that agree with physical models of wildfire occurrence and
previous statistical analyses of wildfire drivers.</div><div><a href='http://arxiv.org/abs/2403.06816v1'>2403.06816v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09492v1")'>Uncertainty-Aware Calibration of a Hot-Wire Anemometer With Gaussian
  Process Regression</div>
<div id='2401.09492v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T22:01:24Z</div><div>Authors: Rub√©n Antonio Garc√≠a-Ruiz, Jos√© Luis Blanco-Claraco, Javier L√≥pez-Mart√≠nez, √Ångel Jes√∫s Callej√≥n-Ferre</div><div style='padding-top: 10px; width: 80ex'>Expensive ultrasonic anemometers are usually required to measure wind speed
accurately. The aim of this work is to overcome the loss of accuracy of a low
cost hot-wire anemometer caused by the changes of air temperature, by means of
a probabilistic calibration using Gaussian Process Regression. Gaussian Process
Regression is a non-parametric, Bayesian, and supervised learning method
designed to make predictions of an unknown target variable as a function of one
or more known input variables. Our approach is validated against real datasets,
obtaining a good performance in inferring the actual wind speed values. By
performing, before its real use in the field, a calibration of the hot-wire
anemometer taking into account air temperature, permits that the wind speed can
be estimated for the typical range of ambient temperatures, including a
grounded uncertainty estimation for each speed measure.</div><div><a href='http://arxiv.org/abs/2401.09492v1'>2401.09492v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11125v1")'>Machine learning-based system reliability analysis with Gaussian Process
  Regression</div>
<div id='2403.11125v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T07:17:07Z</div><div>Authors: Lisang Zhou, Ziqian Luo, Xueting Pan</div><div style='padding-top: 10px; width: 80ex'>Machine learning-based reliability analysis methods have shown great
advancements for their computational efficiency and accuracy. Recently, many
efficient learning strategies have been proposed to enhance the computational
performance. However, few of them explores the theoretical optimal learning
strategy. In this article, we propose several theorems that facilitates such
exploration. Specifically, cases that considering and neglecting the
correlations among the candidate design samples are well elaborated. Moreover,
we prove that the well-known U learning function can be reformulated to the
optimal learning function for the case neglecting the Kriging correlation. In
addition, the theoretical optimal learning strategy for sequential multiple
training samples enrichment is also mathematically explored through the
Bayesian estimate with the corresponding lost functions. Simulation results
show that the optimal learning strategy considering the Kriging correlation
works better than that neglecting the Kriging correlation and other
state-of-the art learning functions from the literatures in terms of the
reduction of number of evaluations of performance function. However, the
implementation needs to investigate very large computational resource.</div><div><a href='http://arxiv.org/abs/2403.11125v1'>2403.11125v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13666v1")'>Measurement Uncertainty: Relating the uncertainties of physical and
  virtual measurements</div>
<div id='2402.13666v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:56:56Z</div><div>Authors: Simon Cramer, Tobias M√ºller, Robert H. Schmitt</div><div style='padding-top: 10px; width: 80ex'>In the context of industrially mass-manufactured products, quality management
is based on physically inspecting a small sample from a large batch and
reasoning about the batch's quality conformance. When complementing physical
inspections with predictions from machine learning models, it is crucial that
the uncertainty of the prediction is known. Otherwise, the application of
established quality management concepts is not legitimate. Deterministic
(machine learning) models lack quantification of their predictive uncertainty
and are therefore unsuitable. Probabilistic (machine learning) models provide a
predictive uncertainty along with the prediction. However, a concise
relationship is missing between the measurement uncertainty of physical
inspections and the predictive uncertainty of probabilistic models in their
application in quality management. Here, we show how the predictive uncertainty
of probabilistic (machine learning) models is related to the measurement
uncertainty of physical inspections. This enables the use of probabilistic
models for virtual inspections and integrates them into existing quality
management concepts. Thus, we can provide a virtual measurement for any quality
characteristic based on the process data and achieve a 100 percent inspection
rate. In the field of Predictive Quality, the virtual measurement is of great
interest. Based on our results, physical inspections with a low sampling rate
can be accompanied by virtual measurements that allow an inspection rate of 100
percent. We add substantial value, especially to complex process chains, as
faulty products/parts are identified promptly and upcoming process steps can be
aborted.</div><div><a href='http://arxiv.org/abs/2402.13666v1'>2402.13666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07160v1")'>PASOA- PArticle baSed Bayesian Optimal Adaptive design</div>
<div id='2402.07160v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T11:11:39Z</div><div>Authors: Jacopo Iollo, Christophe Heinkel√©, Pierre Alliez, Florence Forbes</div><div style='padding-top: 10px; width: 80ex'>We propose a new procedure named PASOA, for Bayesian experimental design,
that performs sequential design optimization by simultaneously providing
accurate estimates of successive posterior distributions for parameter
inference. The sequential design process is carried out via a contrastive
estimation principle, using stochastic optimization and Sequential Monte Carlo
(SMC) samplers to maximise the Expected Information Gain (EIG). As larger
information gains are obtained for larger distances between successive
posterior distributions, this EIG objective may worsen classical SMC
performance. To handle this issue, tempering is proposed to have both a large
information gain and an accurate SMC sampling, that we show is crucial for
performance. This novel combination of stochastic optimization and tempered SMC
allows to jointly handle design optimization and parameter inference. We
provide a proof that the obtained optimal design estimators benefit from some
consistency property. Numerical experiments confirm the potential of the
approach, which outperforms other recent existing procedures.</div><div><a href='http://arxiv.org/abs/2402.07160v1'>2402.07160v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07868v1")'>Nesting Particle Filters for Experimental Design in Dynamical Systems</div>
<div id='2402.07868v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:29:17Z</div><div>Authors: Sahel Iqbal, Adrien Corenflos, Simo S√§rkk√§, Hany Abdulsamad</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a novel approach to Bayesian Experimental Design
(BED) for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested
sequential Monte Carlo (SMC) estimator of the expected information gain and
embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform
gradient-based policy optimization. This is in contrast to recent approaches
that rely on biased estimators of the expected information gain (EIG) to
amortize the cost of experiments by learning a design policy in advance.
Numerical validation on a set of dynamical systems showcases the efficacy of
our method in comparison to other state-of-the-art strategies.</div><div><a href='http://arxiv.org/abs/2402.07868v1'>2402.07868v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02111v1")'>Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo
  is All you Need</div>
<div id='2402.02111v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T10:24:30Z</div><div>Authors: Shangda Yang, Vitaly Zankin, Maximilian Balandat, Stefan Scherer, Kevin Carlberg, Neil Walton, Kody J. H. Law</div><div style='padding-top: 10px; width: 80ex'>We leverage multilevel Monte Carlo (MLMC) to improve the performance of
multi-step look-ahead Bayesian optimization (BO) methods that involve nested
expectations and maximizations. The complexity rate of naive Monte Carlo
degrades for nested operations, whereas MLMC is capable of achieving the
canonical Monte Carlo convergence rate for this type of problem, independently
of dimension and without any smoothness assumptions. Our theoretical study
focuses on the approximation improvements for one- and two-step look-ahead
acquisition functions, but, as we discuss, the approach is generalizable in
various ways, including beyond the context of BO. Findings are verified
numerically and the benefits of MLMC for BO are illustrated on several
benchmark examples. Code is available here
https://github.com/Shangda-Yang/MLMCBO.</div><div><a href='http://arxiv.org/abs/2402.02111v1'>2402.02111v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09346v1")'>High Confidence Level Inference is Almost Free using Parallel Stochastic
  Optimization</div>
<div id='2401.09346v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T17:11:45Z</div><div>Authors: Wanrong Zhu, Zhipeng Lou, Ziyang Wei, Wei Biao Wu</div><div style='padding-top: 10px; width: 80ex'>Uncertainty quantification for estimation through stochastic optimization
solutions in an online setting has gained popularity recently. This paper
introduces a novel inference method focused on constructing confidence
intervals with efficient computation and fast convergence to the nominal level.
Specifically, we propose to use a small number of independent multi-runs to
acquire distribution information and construct a t-based confidence interval.
Our method requires minimal additional computation and memory beyond the
standard updating of estimates, making the inference process almost cost-free.
We provide a rigorous theoretical guarantee for the confidence interval,
demonstrating that the coverage is approximately exact with an explicit
convergence rate and allowing for high confidence level inference. In
particular, a new Gaussian approximation result is developed for the online
estimators to characterize the coverage properties of our confidence intervals
in terms of relative errors. Additionally, our method also allows for
leveraging parallel computing to further accelerate calculations using multiple
cores. It is easy to implement and can be integrated with existing stochastic
algorithms without the need for complicated modifications.</div><div><a href='http://arxiv.org/abs/2401.09346v1'>2401.09346v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17037v1")'>Bayesian Optimization with Noise-Free Observations: Improved Regret
  Bounds via Random Exploration</div>
<div id='2401.17037v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T14:16:06Z</div><div>Authors: Hwanwoo Kim, Daniel Sanz-Alonso</div><div style='padding-top: 10px; width: 80ex'>This paper studies Bayesian optimization with noise-free observations. We
introduce new algorithms rooted in scattered data approximation that rely on a
random exploration step to ensure that the fill-distance of query points decays
at a near-optimal rate. Our algorithms retain the ease of implementation of the
classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly
match those conjectured in arXiv:2002.05096, hence solving a COLT open problem.
Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian
optimization strategies in several examples.</div><div><a href='http://arxiv.org/abs/2401.17037v1'>2401.17037v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07694v1")'>Stochastic optimization with arbitrary recurrent data sampling</div>
<div id='2401.07694v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T14:04:50Z</div><div>Authors: William G. Powell, Hanbaek Lyu</div><div style='padding-top: 10px; width: 80ex'>For obtaining optimal first-order convergence guarantee for stochastic
optimization, it is necessary to use a recurrent data sampling algorithm that
samples every data point with sufficient frequency. Most commonly used data
sampling algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed
recurrent under mild assumptions. In this work, we show that for a particular
class of stochastic optimization algorithms, we do not need any other property
(e.g., independence, exponential mixing, and reshuffling) than recurrence in
data sampling algorithms to guarantee the optimal rate of first-order
convergence. Namely, using regularized versions of Minimization by Incremental
Surrogate Optimization (MISO), we show that for non-convex and possibly
non-smooth objective functions, the expected optimality gap converges at an
optimal rate $O(n^{-1/2})$ under general recurrent sampling schemes.
Furthermore, the implied constant depends explicitly on the `speed of
recurrence', measured by the expected amount of time to visit a given data
point either averaged (`target time') or supremized (`hitting time') over the
current location. We demonstrate theoretically and empirically that convergence
can be accelerated by selecting sampling algorithms that cover the data set
most effectively. We discuss applications of our general framework to
decentralized optimization and distributed non-negative matrix factorization.</div><div><a href='http://arxiv.org/abs/2401.07694v1'>2401.07694v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06325v1")'>Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo</div>
<div id='2401.06325v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T02:33:57Z</div><div>Authors: Xunpeng Huang, Difan Zou, Hanze Dong, Yian Ma, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>To sample from a general target distribution $p_*\propto e^{-f_*}$ beyond the
isoperimetric condition, Huang et al. (2023) proposed to perform sampling
through reverse diffusion, giving rise to Diffusion-based Monte Carlo (DMC).
Specifically, DMC follows the reverse SDE of a diffusion process that
transforms the target distribution to the standard Gaussian, utilizing a
non-parametric score estimation. However, the original DMC algorithm
encountered high gradient complexity, resulting in an exponential dependency on
the error tolerance $\epsilon$ of the obtained samples. In this paper, we
demonstrate that the high complexity of DMC originates from its redundant
design of score estimation, and proposed a more efficient algorithm, called
RS-DMC, based on a novel recursive score estimation method. In particular, we
first divide the entire diffusion process into multiple segments and then
formulate the score estimation step (at any time step) as a series of
interconnected mean estimation and sampling subproblems accordingly, which are
correlated in a recursive manner. Importantly, we show that with a proper
design of the segment decomposition, all sampling subproblems will only need to
tackle a strongly log-concave distribution, which can be very efficient to
solve using the Langevin-based samplers with a provably rapid convergence rate.
As a result, we prove that the gradient complexity of RS-DMC only has a
quasi-polynomial dependency on $\epsilon$, which significantly improves
exponential gradient complexity in Huang et al. (2023). Furthermore, under
commonly used dissipative conditions, our algorithm is provably much faster
than the popular Langevin-based algorithms. Our algorithm design and
theoretical framework illuminate a novel direction for addressing sampling
problems, which could be of broader applicability in the community.</div><div><a href='http://arxiv.org/abs/2401.06325v1'>2401.06325v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17886v2")'>Zeroth-Order Sampling Methods for Non-Log-Concave Distributions:
  Alleviating Metastability by Denoising Diffusion</div>
<div id='2402.17886v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:00:00Z</div><div>Authors: Ye He, Kevin Rojas, Molei Tao</div><div style='padding-top: 10px; width: 80ex'>This paper considers the problem of sampling from non-logconcave
distribution, based on queries of its unnormalized density. It first describes
a framework, Diffusion Monte Carlo (DMC), based on the simulation of a
denoising diffusion process with its score function approximated by a generic
Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle
is the assumed access to samples that generate a Monte Carlo score estimator.
Then we provide an implementation of this oracle, based on rejection sampling,
and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte
Carlo (ZOD-MC). We provide convergence analyses by first constructing a general
framework, i.e. a performance guarantee for DMC, without assuming the target
distribution to be log-concave or satisfying any isoperimetric inequality. Then
we prove that ZOD-MC admits an inverse polynomial dependence on the desired
sampling accuracy, albeit still suffering from the curse of dimensionality.
Consequently, for low dimensional distributions, ZOD-MC is a very efficient
sampler, with performance exceeding latest samplers, including
also-denoising-diffusion-based RDMC and RS-DMC. Last, we experimentally
demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between
modes or discontinuity in non-convex potential.</div><div><a href='http://arxiv.org/abs/2402.17886v2'>2402.17886v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13901v1")'>Non-asymptotic Convergence of Discrete-time Diffusion Models: New
  Approach and Improved Rate</div>
<div id='2402.13901v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:11:47Z</div><div>Authors: Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness Shroff</div><div style='padding-top: 10px; width: 80ex'>The denoising diffusion model emerges recently as a powerful generative
technique that converts noise into data. Theoretical convergence guarantee has
been mainly studied for continuous-time diffusion models, and has been obtained
for discrete-time diffusion models only for distributions with bounded support
in the literature. In this paper, we establish the convergence guarantee for
substantially larger classes of distributions under discrete-time diffusion
models and further improve the convergence rate for distributions with bounded
support. In particular, we first establish the convergence rates for both
smooth and general (possibly non-smooth) distributions having finite second
moment. We then specialize our results to a number of interesting classes of
distributions with explicit parameter dependencies, including distributions
with Lipschitz scores, Gaussian mixture distributions, and distributions with
bounded support. We further propose a novel accelerated sampler and show that
it improves the convergence rates of the corresponding regular sampler by
orders of magnitude with respect to all system parameters. For distributions
with bounded support, our result improves the dimensional dependence of the
previous convergence rate by orders of magnitude. Our study features a novel
analysis technique that constructs tilting factor representation of the
convergence error and exploits Tweedie's formula for handling Taylor expansion
power terms.</div><div><a href='http://arxiv.org/abs/2402.13901v1'>2402.13901v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02957v1")'>On the Asymptotic Mean Square Error Optimality of Diffusion
  Probabilistic Models</div>
<div id='2403.02957v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:25:44Z</div><div>Authors: Benedikt Fesl, Benedikt B√∂ck, Florian Strasser, Michael Baur, Michael Joham, Wolfgang Utschick</div><div style='padding-top: 10px; width: 80ex'>Diffusion probabilistic models (DPMs) have recently shown great potential for
denoising tasks. Despite their practical utility, there is a notable gap in
their theoretical understanding. This paper contributes novel theoretical
insights by rigorously proving the asymptotic convergence of a specific DPM
denoising strategy to the mean square error (MSE)-optimal conditional mean
estimator (CME) over a large number of diffusion steps. The studied DPM-based
denoiser shares the training procedure of DPMs but distinguishes itself by
forwarding only the conditional mean during the reverse inference process after
training. We highlight the unique perspective that DPMs are composed of an
asymptotically optimal denoiser while simultaneously inheriting a powerful
generator by switching re-sampling in the reverse process on and off. The
theoretical findings are validated by numerical results.</div><div><a href='http://arxiv.org/abs/2403.02957v1'>2403.02957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08667v1")'>Target Score Matching</div>
<div id='2402.08667v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:48:28Z</div><div>Authors: Valentin De Bortoli, Michael Hutchinson, Peter Wirnsberger, Arnaud Doucet</div><div style='padding-top: 10px; width: 80ex'>Denoising Score Matching estimates the score of a noised version of a target
distribution by minimizing a regression loss and is widely used to train the
popular class of Denoising Diffusion Models. A well known limitation of
Denoising Score Matching, however, is that it yields poor estimates of the
score at low noise levels. This issue is particularly unfavourable for problems
in the physical sciences and for Monte Carlo sampling tasks for which the score
of the clean original target is known. Intuitively, estimating the score of a
slightly noised version of the target should be a simple task in such cases. In
this paper, we address this shortcoming and show that it is indeed possible to
leverage knowledge of the target score. We present a Target Score Identity and
corresponding Target Score Matching regression loss which allows us to obtain
score estimates admitting favourable properties at low noise levels.</div><div><a href='http://arxiv.org/abs/2402.08667v1'>2402.08667v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18491v1")'>Dynamical Regimes of Diffusion Models</div>
<div id='2402.18491v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:19:26Z</div><div>Authors: Giulio Biroli, Tony Bonnaire, Valentin de Bortoli, Marc M√©zard</div><div style='padding-top: 10px; width: 80ex'>Using statistical physics methods, we study generative diffusion models in
the regime where the dimension of space and the number of data are large, and
the score function has been trained optimally. Our analysis reveals three
distinct dynamical regimes during the backward generative diffusion process.
The generative dynamics, starting from pure noise, encounters first a
'speciation' transition where the gross structure of data is unraveled, through
a mechanism similar to symmetry breaking in phase transitions. It is followed
at later time by a 'collapse' transition where the trajectories of the dynamics
become attracted to one of the memorized data points, through a mechanism which
is similar to the condensation in a glass phase. For any dataset, the
speciation time can be found from a spectral analysis of the correlation
matrix, and the collapse time can be found from the estimation of an 'excess
entropy' in the data. The dependence of the collapse time on the dimension and
number of data provides a thorough characterization of the curse of
dimensionality for diffusion models. Analytical solutions for simple models
like high-dimensional Gaussian mixtures substantiate these findings and provide
a theoretical framework, while extensions to more complex scenarios and
numerical validations with real datasets confirm the theoretical predictions.</div><div><a href='http://arxiv.org/abs/2402.18491v1'>2402.18491v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01434v1")'>Conditioning non-linear and infinite-dimensional diffusion processes</div>
<div id='2402.01434v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:26:32Z</div><div>Authors: Elizabeth Louise Baker, Gefan Yang, Michael L. Severinsen, Christy Anna Hipsley, Stefan Sommer</div><div style='padding-top: 10px; width: 80ex'>Generative diffusion models and many stochastic models in science and
engineering naturally live in infinite dimensions before discretisation. To
incorporate observed data for statistical and learning tasks, one needs to
condition on observations. While recent work has treated conditioning linear
processes in infinite dimensions, conditioning non-linear processes in infinite
dimensions has not been explored. This paper conditions function valued
stochastic processes without prior discretisation. To do so, we use an
infinite-dimensional version of Girsanov's theorem to condition a
function-valued stochastic process, leading to a stochastic differential
equation (SDE) for the conditioned process involving the score. We apply this
technique to do time series analysis for shapes of organisms in evolutionary
biology, where we discretise via the Fourier basis and then learn the
coefficients of the score function with score matching methods.</div><div><a href='http://arxiv.org/abs/2402.01434v1'>2402.01434v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12231v1")'>Diffusion Tempering Improves Parameter Estimation with Probabilistic
  Integrators for Ordinary Differential Equations</div>
<div id='2402.12231v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:36:36Z</div><div>Authors: Jonas Beck, Nathanael Bosch, Michael Deistler, Kyra L. Kadhim, Jakob H. Macke, Philipp Hennig, Philipp Berens</div><div style='padding-top: 10px; width: 80ex'>Ordinary differential equations (ODEs) are widely used to describe dynamical
systems in science, but identifying parameters that explain experimental
measurements is challenging. In particular, although ODEs are differentiable
and would allow for gradient-based parameter optimization, the nonlinear
dynamics of ODEs often lead to many local minima and extreme sensitivity to
initial conditions. We therefore propose diffusion tempering, a novel
regularization technique for probabilistic numerical methods which improves
convergence of gradient-based parameter optimization in ODEs. By iteratively
reducing a noise parameter of the probabilistic integrator, the proposed method
converges more reliably to the true parameters. We demonstrate that our method
is effective for dynamical systems of different complexity and show that it
obtains reliable parameter estimates for a Hodgkin-Huxley model with a
practically relevant number of parameters.</div><div><a href='http://arxiv.org/abs/2402.12231v1'>2402.12231v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07447v1")'>Ab-initio variational wave functions for the time-dependent
  many-electron Schr√∂dinger equation</div>
<div id='2403.07447v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T09:37:22Z</div><div>Authors: Jannes Nys, Gabriel Pescia, Giuseppe Carleo</div><div style='padding-top: 10px; width: 80ex'>Describing the dynamics of many-electron quantum systems is crucial for
applications such as predicting electronic structures in quantum chemistry, the
properties of condensed matter systems, and the behaviors of complex materials.
However, the real-time evolution of non-equilibrium quantum electronic systems
poses a significant challenge for theoretical and computational approaches, due
to the system's exploration of a vast configuration space. This work introduces
a variational approach for fermionic time-dependent wave functions, surpassing
mean-field approximations by capturing many-body correlations. The proposed
methodology involves parameterizing the time-evolving quantum state, enabling
the approximation of the state's evolution. To account for electron
correlations, we employ time-dependent Jastrow factors and backflow
transformations. We also show that we can incorporate neural networks to
parameterize these functions. The time-dependent variational Monte Carlo
technique is employed to efficiently compute the optimal time-dependent
parameters. The approach is demonstrated in three distinct systems: the
solvable harmonic interaction model, the dynamics of a diatomic molecule in
intense laser fields, and a quenched quantum dot. In all cases, we show clear
signatures of many-body correlations in the dynamics not captured by mean-field
methods. The results showcase the ability of our variational approach to
accurately capture the time evolution of quantum states, providing insight into
the quantum dynamics of interacting electronic systems, beyond the capabilities
of mean-field.</div><div><a href='http://arxiv.org/abs/2403.07447v1'>2403.07447v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01036v1")'>Fisher information dissipation for time inhomogeneous stochastic
  differential equations</div>
<div id='2402.01036v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T21:49:50Z</div><div>Authors: Qi Feng, Xinzhe Zuo, Wuchen Li</div><div style='padding-top: 10px; width: 80ex'>We provide a Lyapunov convergence analysis for time-inhomogeneous variable
coefficient stochastic differential equations (SDEs). Three typical examples
include overdamped, irreversible drift, and underdamped Langevin dynamics. We
first formula the probability transition equation of Langevin dynamics as a
modified gradient flow of the Kullback-Leibler divergence in the probability
space with respect to time-dependent optimal transport metrics. This
formulation contains both gradient and non-gradient directions depending on a
class of time-dependent target distribution. We then select a time-dependent
relative Fisher information functional as a Lyapunov functional. We develop a
time-dependent Hessian matrix condition, which guarantees the convergence of
the probability density function of the SDE. We verify the proposed conditions
for several time-inhomogeneous Langevin dynamics. For the overdamped Langevin
dynamics, we prove the $O(t^{-1/2})$ convergence in $L^1$ distance for the
simulated annealing dynamics with a strongly convex potential function. For the
irreversible drift Langevin dynamics, we prove an improved convergence towards
the target distribution in an asymptotic regime. We also verify the convergence
condition for the underdamped Langevin dynamics. Numerical examples demonstrate
the convergence results for the time-dependent Langevin dynamics.</div><div><a href='http://arxiv.org/abs/2402.01036v1'>2402.01036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03852v1")'>Accelerating Convergence of Score-Based Diffusion Models, Provably</div>
<div id='2403.03852v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:02:39Z</div><div>Authors: Gen Li, Yu Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, Yuxin Chen</div><div style='padding-top: 10px; width: 80ex'>Score-based diffusion models, while achieving remarkable empirical
performance, often suffer from low sampling speed, due to extensive function
evaluations needed during the sampling phase. Despite a flurry of recent
activities towards speeding up diffusion generative modeling in practice,
theoretical underpinnings for acceleration techniques remain severely limited.
In this paper, we design novel training-free algorithms to accelerate popular
deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our
accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the
number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our
accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the
rate $O(1/\sqrt{T})$ for the DDPM sampler. The design of our algorithms
leverages insights from higher-order approximation, and shares similar
intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory
accommodates $\ell_2$-accurate score estimates, and does not require
log-concavity or smoothness on the target distribution.</div><div><a href='http://arxiv.org/abs/2403.03852v1'>2403.03852v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13115v1")'>Contractive Diffusion Probabilistic Models</div>
<div id='2401.13115v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T21:51:51Z</div><div>Authors: Wenpin Tang, Hanyang Zhao</div><div style='padding-top: 10px; width: 80ex'>Diffusion probabilistic models (DPMs) have emerged as a promising technology
in generative modeling. The success of DPMs relies on two ingredients: time
reversal of Markov diffusion processes and score matching. Most existing work
implicitly assumes that score matching is close to perfect, while this
assumption is questionable. In view of possibly unguaranteed score matching, we
propose a new criterion -- the contraction of backward sampling in the design
of DPMs. This leads to a novel class of contractive DPMs (CDPMs), including
contractive Ornstein-Uhlenbeck (OU) processes and contractive sub-variance
preserving (sub-VP) stochastic differential equations (SDEs). The key insight
is that the contraction in the backward process narrows score matching errors,
as well as discretization error. Thus, the proposed CDPMs are robust to both
sources of error. Our proposal is supported by theoretical results, and is
corroborated by experiments. Notably, contractive sub-VP shows the best
performance among all known SDE-based DPMs on the CIFAR-10 dataset.</div><div><a href='http://arxiv.org/abs/2401.13115v1'>2401.13115v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17958v1")'>Convergence Analysis for General Probability Flow ODEs of Diffusion
  Models in Wasserstein Distances</div>
<div id='2401.17958v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T16:07:44Z</div><div>Authors: Xuefeng Gao, Lingjiong Zhu</div><div style='padding-top: 10px; width: 80ex'>Score-based generative modeling with probability flow ordinary differential
equations (ODEs) has achieved remarkable success in a variety of applications.
While various fast ODE-based samplers have been proposed in the literature and
employed in practice, the theoretical understandings about convergence
properties of the probability flow ODE are still quite limited. In this paper,
we provide the first non-asymptotic convergence analysis for a general class of
probability flow ODE samplers in 2-Wasserstein distance, assuming accurate
score estimates. We then consider various examples and establish results on the
iteration complexity of the corresponding ODE-based samplers.</div><div><a href='http://arxiv.org/abs/2401.17958v1'>2401.17958v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13530v2")'>Continuous-time Riemannian SGD and SVRG Flows on Wasserstein
  Probabilistic Space</div>
<div id='2401.13530v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T15:35:44Z</div><div>Authors: Mingyang Yi, Bohan Wang</div><div style='padding-top: 10px; width: 80ex'>Recently, optimization on the Riemannian manifold has provided new insights
to the optimization community. In this regard, the manifold taken as the
probability measure metric space equipped with the second-order Wasserstein
distance is of particular interest, since optimization on it can be linked to
practical sampling processes. In general, the oracle (continuous) optimization
method on Wasserstein space is Riemannian gradient flow (i.e., Langevin
dynamics when minimizing KL divergence). In this paper, we aim to enrich the
continuous optimization methods in the Wasserstein space by extending the
gradient flow into the stochastic gradient descent (SGD) flow and stochastic
variance reduction gradient (SVRG) flow. The two flows on Euclidean space are
standard stochastic optimization methods, while their Riemannian counterparts
are not explored yet. By leveraging the structures in Wasserstein space, we
construct a stochastic differential equation (SDE) to approximate the discrete
dynamics of desired stochastic methods in the corresponded random vector space.
Then, the flows of probability measures are naturally obtained by applying
Fokker-Planck equation to such SDE. Furthermore, the convergence rates of the
proposed Riemannian stochastic flows are proven, and they match the results in
Euclidean space.</div><div><a href='http://arxiv.org/abs/2401.13530v2'>2401.13530v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12012v1")'>Convergence of Kinetic Langevin Monte Carlo on Lie groups</div>
<div id='2403.12012v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:50:20Z</div><div>Authors: Lingkai Kong, Molei Tao</div><div style='padding-top: 10px; width: 80ex'>Explicit, momentum-based dynamics for optimizing functions defined on Lie
groups was recently constructed, based on techniques such as variational
optimization and left trivialization. We appropriately add tractable noise to
the optimization dynamics to turn it into a sampling dynamics, leveraging the
advantageous feature that the momentum variable is Euclidean despite that the
potential function lives on a manifold. We then propose a Lie-group MCMC
sampler, by delicately discretizing the resulting kinetic-Langevin-type
sampling dynamics. The Lie group structure is exactly preserved by this
discretization. Exponential convergence with explicit convergence rate for both
the continuous dynamics and the discrete sampler are then proved under W2
distance. Only compactness of the Lie group and geodesically L-smoothness of
the potential function are needed. To the best of our knowledge, this is the
first convergence result for kinetic Langevin on curved spaces, and also the
first quantitative result that requires no convexity or, at least not
explicitly, any common relaxation such as isoperimetry.</div><div><a href='http://arxiv.org/abs/2403.12012v1'>2403.12012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08711v2")'>Correction to "Wasserstein distance estimates for the distributions of
  numerical approximations to ergodic stochastic differential equations"</div>
<div id='2402.08711v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:31:55Z</div><div>Authors: Daniel Paulin, Peter A. Whalley</div><div style='padding-top: 10px; width: 80ex'>A method for analyzing non-asymptotic guarantees of numerical discretizations
of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and
Zygalakis in ``Wasserstein distance estimates for the distributions of
numerical approximations to ergodic stochastic differential equations". They
analyze the UBU integrator which is strong order two and only requires one
gradient evaluation per step, resulting in desirable non-asymptotic guarantees,
in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance
of $\epsilon &gt; 0$ in Wasserstein-2 distance away from the target distribution.
However, there is a mistake in the local error estimates in Sanz-Serna and
Zygalakis (2021), in particular, a stronger assumption is needed to achieve
these complexity estimates. This note reconciles the theory with the dimension
dependence observed in practice in many applications of interest.</div><div><a href='http://arxiv.org/abs/2402.08711v2'>2402.08711v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06281v1")'>Demystifying Variational Diffusion Models</div>
<div id='2401.06281v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T22:37:37Z</div><div>Authors: Fabio De Sousa Ribeiro, Ben Glocker</div><div style='padding-top: 10px; width: 80ex'>Despite the growing popularity of diffusion models, gaining a deep
understanding of the model class remains somewhat elusive for the uninitiated
in non-equilibrium statistical physics. With that in mind, we present what we
believe is a more straightforward introduction to diffusion models using
directed graphical modelling and variational Bayesian principles, which imposes
relatively fewer prerequisites on the average reader. Our exposition
constitutes a comprehensive technical review spanning from foundational
concepts like deep latent variable models to recent advances in continuous-time
diffusion-based modelling, highlighting theoretical connections between model
classes along the way. We provide additional mathematical insights that were
omitted in the seminal works whenever possible to aid in understanding, while
avoiding the introduction of new notation. We envision this article serving as
a useful educational supplement for both researchers and practitioners in the
area, and we welcome feedback and contributions from the community at
https://github.com/biomedia-mira/demystifying-diffusion.</div><div><a href='http://arxiv.org/abs/2401.06281v1'>2401.06281v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02739v2")'>Denoising Diffusion Variational Inference: Diffusion Models as
  Expressive Variational Posteriors</div>
<div id='2401.02739v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T10:27:44Z</div><div>Authors: Top Piriyakulkij, Yingheng Wang, Volodymyr Kuleshov</div><div style='padding-top: 10px; width: 80ex'>We propose denoising diffusion variational inference (DDVI), an approximate
inference algorithm for latent variable models which relies on diffusion models
as flexible variational posteriors. Specifically, our method introduces an
expressive class of approximate posteriors with auxiliary latent variables that
perform diffusion in latent space by reversing a user-specified noising
process. We fit these models by optimizing a lower bound on the marginal
likelihood inspired by the wake-sleep algorithm. Our method is easy to
implement (it fits a regularized extension of the ELBO), is compatible with
black-box variational inference, and outperforms alternative classes of
approximate posteriors based on normalizing flows or adversarial networks. It
increases the expressivity of flow-based methods via non-invertible deep
recurrent architectures and avoids the instability of adversarial methods. We
use DDVI on a motivating task in biology -- inferring latent ancestry from
human genomes -- and we find that it outperforms strong baselines on the
Thousand Genomes dataset.</div><div><a href='http://arxiv.org/abs/2401.02739v2'>2401.02739v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01263v2")'>A Differentiable Partially Observable Generalized Linear Model with
  Forward-Backward Message Passing</div>
<div id='2402.01263v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:34:49Z</div><div>Authors: Chengrui Li, Weihan Li, Yule Wang, Anqi Wu</div><div style='padding-top: 10px; width: 80ex'>The partially observable generalized linear model (POGLM) is a powerful tool
for understanding neural connectivity under the assumption of existing hidden
neurons. With spike trains only recorded from visible neurons, existing works
use variational inference to learn POGLM meanwhile presenting the difficulty of
learning this latent variable model. There are two main issues: (1) the sampled
Poisson hidden spike count hinders the use of the pathwise gradient estimator
in VI; and (2) the existing design of the variational model is neither
expressive nor time-efficient, which further affects the performance. For (1),
we propose a new differentiable POGLM, which enables the pathwise gradient
estimator, better than the score function gradient estimator used in existing
works. For (2), we propose the forward-backward message-passing sampling scheme
for the variational model. Comprehensive experiments show that our
differentiable POGLMs with our forward-backward message passing produce a
better performance on one synthetic and two real-world datasets. Furthermore,
our new method yields more interpretable parameters, underscoring its
significance in neuroscience.</div><div><a href='http://arxiv.org/abs/2402.01263v2'>2402.01263v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16755v1")'>Diffusion model for relational inference</div>
<div id='2401.16755v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T05:25:02Z</div><div>Authors: Shuhan Zheng, Ziqiang Li, Kantaro Fujiwara, Gouhei Tanaka</div><div style='padding-top: 10px; width: 80ex'>Dynamical behaviors of complex interacting systems, including brain
activities, financial price movements, and physical collective phenomena, are
associated with underlying interactions between the system's components. The
issue of uncovering interaction relations in such systems using observable
dynamics is called relational inference. In this study, we propose a Diffusion
model for Relational Inference (DiffRI), inspired by a self-supervised method
for probabilistic time series imputation. DiffRI learns to infer the
probability of the presence of connections between components through
conditional diffusion modeling. Experiments on both simulated and quasi-real
datasets show that DiffRI is highly competent compared with other
state-of-the-art models in discovering ground truth interactions in an
unsupervised manner. Our code will be made public soon.</div><div><a href='http://arxiv.org/abs/2401.16755v1'>2401.16755v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08095v2")'>Convergence Analysis of Discrete Diffusion Model: Exact Implementation
  through Uniformization</div>
<div id='2402.08095v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:26:52Z</div><div>Authors: Hongrui Chen, Lexing Ying</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have achieved huge empirical success in data generation
tasks. Recently, some efforts have been made to adapt the framework of
diffusion models to discrete state space, providing a more natural approach for
modeling intrinsically discrete data, such as language and graphs. This is
achieved by formulating both the forward noising process and the corresponding
reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we
investigate the theoretical properties of the discrete diffusion model.
Specifically, we introduce an algorithm leveraging the uniformization of
continuous Markov chains, implementing transitions on random time points. Under
reasonable assumptions on the learning of the discrete score function, we
derive Total Variation distance and KL divergence guarantees for sampling from
any distribution on a hypercube. Our results align with state-of-the-art
achievements for diffusion models in $\mathbb{R}^d$ and further underscore the
advantages of discrete diffusion models in comparison to the $\mathbb{R}^d$
setting.</div><div><a href='http://arxiv.org/abs/2402.08095v2'>2402.08095v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06279v2")'>Fine-tuning of diffusion models via stochastic control: entropy
  regularization and beyond</div>
<div id='2403.06279v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T18:13:22Z</div><div>Authors: Wenpin Tang</div><div style='padding-top: 10px; width: 80ex'>This paper aims to develop and provide a rigorous treatment to the problem of
entropy regularized fine-tuning in the context of continuous-time diffusion
models, which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024).
The idea is to use stochastic control for sample generation, where the entropy
regularizer is introduced to mitigate reward collapse. We also show how the
analysis can be extended to fine-tuning involving a general $f$-divergence
regularizer.</div><div><a href='http://arxiv.org/abs/2403.06279v2'>2403.06279v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16359v2")'>Feedback Efficient Online Fine-Tuning of Diffusion Models</div>
<div id='2402.16359v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:24:32Z</div><div>Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Sergey Levine, Tommaso Biancalani</div><div style='padding-top: 10px; width: 80ex'>Diffusion models excel at modeling complex data distributions, including
those of images, proteins, and small molecules. However, in many cases, our
goal is to model parts of the distribution that maximize certain properties:
for example, we may want to generate images with high aesthetic quality, or
molecules with high bioactivity. It is natural to frame this as a reinforcement
learning (RL) problem, in which the objective is to fine-tune a diffusion model
to maximize a reward function that corresponds to some property. Even with
access to online queries of the ground-truth reward function, efficiently
discovering high-reward samples can be challenging: they might have a low
probability in the initial distribution, and there might be many infeasible
samples that do not even have a well-defined reward (e.g., unnatural images or
physically impossible molecules). In this work, we propose a novel
reinforcement learning procedure that efficiently explores on the manifold of
feasible samples. We present a theoretical analysis providing a regret
guarantee, as well as empirical validation across three domains: images,
biological sequences, and molecules.</div><div><a href='http://arxiv.org/abs/2402.16359v2'>2402.16359v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15194v2")'>Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized
  Control</div>
<div id='2402.15194v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:54:42Z</div><div>Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, Sergey Levine</div><div style='padding-top: 10px; width: 80ex'>Diffusion models excel at capturing complex data distributions, such as those
of natural images and proteins. While diffusion models are trained to represent
the distribution in the training dataset, we often are more concerned with
other properties, such as the aesthetic quality of the generated images or the
functional properties of generated proteins. Diffusion models can be finetuned
in a goal-directed way by maximizing the value of some reward function (e.g.,
the aesthetic quality of an image). However, these approaches may lead to
reduced sample diversity, significant deviations from the training data
distribution, and even poor sample quality due to the exploitation of an
imperfect reward function. The last issue often occurs when the reward function
is a learned model meant to approximate a ground-truth "genuine" reward, as is
the case in many practical applications. These challenges, collectively termed
"reward collapse," pose a substantial obstacle. To address this reward
collapse, we frame the finetuning problem as entropy-regularized control
against the pretrained diffusion model, i.e., directly optimizing
entropy-enhanced rewards with neural SDEs. We present theoretical and empirical
evidence that demonstrates our framework is capable of efficiently generating
diverse samples with high genuine rewards, mitigating the overoptimization of
imperfect reward models.</div><div><a href='http://arxiv.org/abs/2402.15194v2'>2402.15194v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11968v1")'>Unveil Conditional Diffusion Models with Classifier-free Guidance: A
  Sharp Statistical Theory</div>
<div id='2403.11968v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:08:24Z</div><div>Authors: Hengyu Fu, Zhuoran Yang, Mengdi Wang, Minshuo Chen</div><div style='padding-top: 10px; width: 80ex'>Conditional diffusion models serve as the foundation of modern image
synthesis and find extensive application in fields like computational biology
and reinforcement learning. In these applications, conditional diffusion models
incorporate various conditional information, such as prompt input, to guide the
sample generation towards desired properties. Despite the empirical success,
theory of conditional diffusion models is largely missing. This paper bridges
this gap by presenting a sharp statistical theory of distribution estimation
using conditional diffusion models. Our analysis yields a sample complexity
bound that adapts to the smoothness of the data distribution and matches the
minimax lower bound. The key to our theoretical development lies in an
approximation result for the conditional score function, which relies on a
novel diffused Taylor approximation technique. Moreover, we demonstrate the
utility of our statistical theory in elucidating the performance of conditional
diffusion models across diverse applications, including model-based transition
kernel estimation in reinforcement learning, solving inverse problems, and
reward conditioned sample generation.</div><div><a href='http://arxiv.org/abs/2403.11968v1'>2403.11968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13219v1")'>Diffusion Model for Data-Driven Black-Box Optimization</div>
<div id='2403.13219v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T00:41:12Z</div><div>Authors: Zihao Li, Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Yinyu Ye, Minshuo Chen, Mengdi Wang</div><div style='padding-top: 10px; width: 80ex'>Generative AI has redefined artificial intelligence, enabling the creation of
innovative content and customized solutions that drive business practices into
a new era of efficiency and creativity. In this paper, we focus on diffusion
models, a powerful generative AI technology, and investigate their potential
for black-box optimization over complex structured variables. Consider the
practical scenario where one wants to optimize some structured design in a
high-dimensional space, based on massive unlabeled data (representing design
variables) and a small labeled dataset. We study two practical types of labels:
1) noisy measurements of a real-valued reward function and 2) human preference
based on pairwise comparisons. The goal is to generate new designs that are
near-optimal and preserve the designed latent structures. Our proposed method
reformulates the design optimization problem into a conditional sampling
problem, which allows us to leverage the power of diffusion models for modeling
complex distributions. In particular, we propose a reward-directed conditional
diffusion model, to be trained on the mixed data, for sampling a near-optimal
solution conditioned on high predicted rewards. Theoretically, we establish
sub-optimality error bounds for the generated designs. The sub-optimality gap
nearly matches the optimal guarantee in off-policy bandits, demonstrating the
efficiency of reward-directed diffusion models for black-box optimization.
Moreover, when the data admits a low-dimensional latent subspace structure, our
model efficiently generates high-fidelity designs that closely respect the
latent structure. We provide empirical experiments validating our model in
decision-making and content-creation tasks.</div><div><a href='http://arxiv.org/abs/2403.13219v1'>2403.13219v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07487v1")'>Score-based Diffusion Models via Stochastic Differential Equations -- a
  Technical Tutorial</div>
<div id='2402.07487v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T08:52:35Z</div><div>Authors: Wenpin Tang, Hanyang Zhao</div><div style='padding-top: 10px; width: 80ex'>This is an expository article on the score-based diffusion models, with a
particular focus on the formulation via stochastic differential equations
(SDE). After a gentle introduction, we discuss the two pillars in the diffusion
modeling -- sampling and score matching, which encompass the SDE/ODE sampling,
score matching efficiency, the consistency model, and reinforcement learning.
Short proofs are given to illustrate the main idea of the stated results. The
article is primarily for introducing the beginners to the field, and
practitioners may also find some analysis useful in designing new models or
algorithms.</div><div><a href='http://arxiv.org/abs/2402.07487v1'>2402.07487v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10028v1")'>Diffusion Models Meet Contextual Bandits with Large Action Spaces</div>
<div id='2402.10028v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T15:48:55Z</div><div>Authors: Imad Aouali</div><div style='padding-top: 10px; width: 80ex'>Efficient exploration is a key challenge in contextual bandits due to the
large size of their action space, where uninformed exploration can result in
computational and statistical inefficiencies. Fortunately, the rewards of
actions are often correlated and this can be leveraged to explore them
efficiently. In this work, we capture such correlations using pre-trained
diffusion models; upon which we design diffusion Thompson sampling (dTS). Both
theoretical and algorithmic foundations are developed for dTS, and empirical
evaluation also shows its favorable performance.</div><div><a href='http://arxiv.org/abs/2402.10028v1'>2402.10028v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03008v2")'>Diffusive Gibbs Sampling</div>
<div id='2402.03008v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:47:41Z</div><div>Authors: Wenlin Chen, Mingtian Zhang, Brooks Paige, Jos√© Miguel Hern√°ndez-Lobato, David Barber</div><div style='padding-top: 10px; width: 80ex'>The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods
for multi-modal distributions presents a significant challenge in practical
applications such as Bayesian inference and molecular dynamics. Addressing
this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of
sampling methods designed for effective sampling from distributions
characterized by distant and disconnected modes. DiGS integrates recent
developments in diffusion models, leveraging Gaussian convolution to create an
auxiliary noisy distribution that bridges isolated modes in the original space
and applying Gibbs sampling to alternately draw samples from both spaces. Our
approach exhibits a better mixing property for sampling multi-modal
distributions than state-of-the-art methods such as parallel tempering. We
demonstrate that our sampler attains substantially improved results across
various tasks, including mixtures of Gaussians, Bayesian neural networks and
molecular dynamics.</div><div><a href='http://arxiv.org/abs/2402.03008v2'>2402.03008v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14404v1")'>Physics-Informed Diffusion Models</div>
<div id='2403.14404v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T13:52:55Z</div><div>Authors: Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann</div><div style='padding-top: 10px; width: 80ex'>Generative models such as denoising diffusion models are quickly advancing
their ability to approximate highly complex data distributions. They are also
increasingly leveraged in scientific machine learning, where samples from the
implied data distribution are expected to adhere to specific governing
equations. We present a framework to inform denoising diffusion models on
underlying constraints on such generated samples during model training. Our
approach improves the alignment of the generated samples with the imposed
constraints and significantly outperforms existing methods without affecting
inference speed. Additionally, our findings suggest that incorporating such
constraints during training provides a natural regularization against
overfitting. Our framework is easy to implement and versatile in its
applicability for imposing equality and inequality constraints as well as
auxiliary optimization objectives.</div><div><a href='http://arxiv.org/abs/2403.14404v1'>2403.14404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07808v1")'>Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation</div>
<div id='2402.07808v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:13:02Z</div><div>Authors: Julius Vetter, Guy Moss, Cornelius Schr√∂der, Richard Gao, Jakob H. Macke</div><div style='padding-top: 10px; width: 80ex'>Scientific modeling applications often require estimating a distribution of
parameters consistent with a dataset of observations - an inference task also
known as source distribution estimation. This problem can be ill-posed,
however, since many different source distributions might produce the same
distribution of data-consistent simulations. To make a principled choice among
many equally valid sources, we propose an approach which targets the maximum
entropy distribution, i.e., prioritizes retaining as much uncertainty as
possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein
distance to measure the discrepancy between the dataset and simulations - and
thus suitable for simulators with intractable likelihoods. We benchmark our
method on several tasks, and show that it can recover source distributions with
substantially higher entropy without sacrificing the fidelity of the
simulations. Finally, to demonstrate the utility of our approach, we infer
source distributions for parameters of the Hodgkin-Huxley neuron model from
experimental datasets with thousands of measurements. In summary, we propose a
principled framework for inferring unique source distributions of scientific
simulator parameters while retaining as much uncertainty as possible.</div><div><a href='http://arxiv.org/abs/2402.07808v1'>2402.07808v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02080v1")'>Energy based diffusion generator for efficient sampling of Boltzmann
  distributions</div>
<div id='2401.02080v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T06:03:46Z</div><div>Authors: Yan Wang, Ling Guo, Hao Wu, Tao Zhou</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel sampler called the energy based diffusion generator for
generating samples from arbitrary target distributions. The sampling model
employs a structure similar to a variational autoencoder, utilizing a decoder
to transform latent variables from a simple distribution into random variables
approximating the target distribution, and we design an encoder based on the
diffusion model. Leveraging the powerful modeling capacity of the diffusion
model for complex distributions, we can obtain an accurate variational estimate
of the Kullback-Leibler divergence between the distributions of the generated
samples and the target. Moreover, we propose a decoder based on generalized
Hamiltonian dynamics to further enhance sampling performance. Through empirical
evaluation, we demonstrate the effectiveness of our method across various
complex distribution functions, showcasing its superiority compared to existing
methods.</div><div><a href='http://arxiv.org/abs/2401.02080v1'>2401.02080v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06183v1")'>An Improved Analysis of Langevin Algorithms with Prior Diffusion for
  Non-Log-Concave Sampling</div>
<div id='2403.06183v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T11:50:34Z</div><div>Authors: Xunpeng Huang, Hanze Dong, Difan Zou, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>Understanding the dimension dependency of computational complexity in
high-dimensional sampling problem is a fundamental problem, both from a
practical and theoretical perspective. Compared with samplers with unbiased
stationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA),
biased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in
low-accuracy cases just because a lower dimension dependency in their
complexities. Along this line, Freund et al. (2022) suggest that the modified
Langevin algorithm with prior diffusion is able to converge dimension
independently for strongly log-concave target distributions. Nonetheless, it
remains open whether such property establishes for more general cases. In this
paper, we investigate the prior diffusion technique for the target
distributions satisfying log-Sobolev inequality (LSI), which covers a much
broader class of distributions compared to the strongly log-concave ones. In
particular, we prove that the modified Langevin algorithm can also obtain the
dimension-independent convergence of KL divergence with different step size
schedules. The core of our proof technique is a novel construction of an
interpolating SDE, which significantly helps to conduct a more accurate
characterization of the discrete updates of the overdamped Langevin dynamics.
Our theoretical analysis demonstrates the benefits of prior diffusion for a
broader class of target distributions and provides new insights into developing
faster sampling algorithms.</div><div><a href='http://arxiv.org/abs/2403.06183v1'>2403.06183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17870v1")'>Stochastic Approximation with Biased MCMC for Expectation Maximization</div>
<div id='2402.17870v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T20:10:03Z</div><div>Authors: Samuel Gruffaz, Kyurae Kim, Alain Oliviero Durmus, Jacob R. Gardner</div><div style='padding-top: 10px; width: 80ex'>The expectation maximization (EM) algorithm is a widespread method for
empirical Bayesian inference, but its expectation step (E-step) is often
intractable. Employing a stochastic approximation scheme with Markov chain
Monte Carlo (MCMC) can circumvent this issue, resulting in an algorithm known
as MCMC-SAEM. While theoretical guarantees for MCMC-SAEM have previously been
established, these results are restricted to the case where asymptotically
unbiased MCMC algorithms are used. In practice, MCMC-SAEM is often run with
asymptotically biased MCMC, for which the consequences are theoretically less
understood. In this work, we fill this gap by analyzing the asymptotics and
non-asymptotics of SAEM with biased MCMC steps, particularly the effect of
bias. We also provide numerical experiments comparing the Metropolis-adjusted
Langevin algorithm (MALA), which is asymptotically unbiased, and the unadjusted
Langevin algorithm (ULA), which is asymptotically biased, on synthetic and real
datasets. Experimental results show that ULA is more stable with respect to the
choice of Langevin stepsize and can sometimes result in faster convergence.</div><div><a href='http://arxiv.org/abs/2402.17870v1'>2402.17870v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14434v2")'>Parallelized Midpoint Randomization for Langevin Monte Carlo</div>
<div id='2402.14434v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T10:26:46Z</div><div>Authors: Lu Yu, Arnak Dalalyan</div><div style='padding-top: 10px; width: 80ex'>We explore the sampling problem within the framework where parallel
evaluations of the gradient of the log-density are feasible. Our investigation
focuses on target distributions characterized by smooth and strongly
log-concave densities. We revisit the parallelized randomized midpoint method
and employ proof techniques recently developed for analyzing its purely
sequential version. Leveraging these techniques, we derive upper bounds on the
Wasserstein distance between the sampling and target densities. These bounds
quantify the runtime improvement achieved by utilizing parallel processing
units, which can be considerable.</div><div><a href='http://arxiv.org/abs/2402.14434v2'>2402.14434v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12727v1")'>Diffusion Posterior Sampling is Computationally Intractable</div>
<div id='2402.12727v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T05:28:13Z</div><div>Authors: Shivam Gupta, Ajil Jalal, Aditya Parulekar, Eric Price, Zhiyang Xun</div><div style='padding-top: 10px; width: 80ex'>Diffusion models are a remarkably effective way of learning and sampling from
a distribution $p(x)$. In posterior sampling, one is also given a measurement
model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x
\mid y)$. Posterior sampling is useful for tasks such as inpainting,
super-resolution, and MRI reconstruction, so a number of recent works have
given algorithms to heuristically approximate it; but none are known to
converge to the correct distribution in polynomial time.
  In this paper we show that posterior sampling is \emph{computationally
intractable}: under the most basic assumption in cryptography -- that one-way
functions exist -- there are instances for which \emph{every} algorithm takes
superpolynomial time, even though \emph{unconditional} sampling is provably
fast. We also show that the exponential-time rejection sampling algorithm is
essentially optimal under the stronger plausible assumption that there are
one-way functions that take exponential time to invert.</div><div><a href='http://arxiv.org/abs/2402.12727v1'>2402.12727v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17943v1")'>Sequential transport maps using SoS density estimation and
  $Œ±$-divergences</div>
<div id='2402.17943v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T23:52:58Z</div><div>Authors: Benjamin Zanger, Tiangang Cui, Martin Schreiber, Olivier Zahm</div><div style='padding-top: 10px; width: 80ex'>Transport-based density estimation methods are receiving growing interest
because of their ability to efficiently generate samples from the approximated
density. We further invertigate the sequential transport maps framework
proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of
composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first
estimating an intermediate density of moderate complexity, and then by
computing the exact KR map from a reference density to the precomputed
approximate density. In our work, we explore the use of Sum-of-Squares (SoS)
densities and $\alpha$-divergences for approximating the intermediate
densities. Combining SoS densities with $\alpha$-divergence interestingly
yields convex optimization problems which can be efficiently solved using
semidefinite programming. The main advantage of $\alpha$-divergences is to
enable working with unnormalized densities, which provides benefits both
numerically and theoretically. In particular, we provide two new convergence
analyses of the sequential transport maps: one based on a triangle-like
inequality and the second on information geometric properties of
$\alpha$-divergences for unnormalizied densities. The choice of intermediate
densities is also crucial for the efficiency of the method. While tempered (or
annealed) densities are the state-of-the-art, we introduce diffusion-based
intermediate densities which permits to approximate densities known from
samples only. Such intermediate densities are well-established in machine
learning for generative modeling. Finally we propose and try different
low-dimensional maps (or lazy maps) for dealing with high-dimensional problems
and numerically demonstrate our methods on several benchmarks, including
Bayesian inference problems and unsupervised learning task.</div><div><a href='http://arxiv.org/abs/2402.17943v1'>2402.17943v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02977v2")'>Variational Flow Models: Flowing in Your Style</div>
<div id='2402.02977v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:58:29Z</div><div>Authors: Kien Do, Duc Kieu, Toan Nguyen, Dang Nguyen, Hung Le, Dung Nguyen, Thin Nguyen</div><div style='padding-top: 10px; width: 80ex'>We introduce a variational inference interpretation for models of "posterior
flows" - generalizations of "probability flows" to a broader class of
stochastic processes not necessarily diffusion processes. We coin the resulting
models as "Variational Flow Models". Additionally, we propose a systematic
training-free method to transform the posterior flow of a "linear" stochastic
process characterized by the equation Xt = at * X0 + st * X1 into a straight
constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation
facilitates fast sampling along the original posterior flow without training a
new model of the SC flow. The flexibility of our approach allows us to extend
our transformation to inter-convert two posterior flows from distinct "linear"
stochastic processes. Moreover, we can easily integrate high-order numerical
solvers into the transformed SC flow, further enhancing sampling accuracy and
efficiency. Rigorous theoretical analysis and extensive experimental results
substantiate the advantages of our framework.</div><div><a href='http://arxiv.org/abs/2402.02977v2'>2402.02977v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09429v2")'>VISA: Variational Inference with Sequential Sample-Average
  Approximations</div>
<div id='2403.09429v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T14:20:22Z</div><div>Authors: Heiko Zimmermann, Christian A. Naesseth, Jan-Willem van de Meent</div><div style='padding-top: 10px; width: 80ex'>We present variational inference with sequential sample-average approximation
(VISA), a method for approximate inference in computationally intensive models,
such as those based on numerical simulations. VISA extends importance-weighted
forward-KL variational inference by employing a sequence of sample-average
approximations, which are considered valid inside a trust region. This makes it
possible to reuse model evaluations across multiple gradient steps, thereby
reducing computational cost. We perform experiments on high-dimensional
Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate
that VISA can achieve comparable approximation accuracy to standard
importance-weighted forward-KL variational inference with computational savings
of a factor two or more for conservatively chosen learning rates.</div><div><a href='http://arxiv.org/abs/2403.09429v2'>2403.09429v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07454v2")'>Fast, accurate and lightweight sequential simulation-based inference
  using Gaussian locally linear mappings</div>
<div id='2403.07454v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T09:48:17Z</div><div>Authors: Henrik H√§ggstr√∂m, Pedro L. C. Rodrigues, Geoffroy Oudoumanessah, Florence Forbes, Umberto Picchini</div><div style='padding-top: 10px; width: 80ex'>Bayesian inference for complex models with an intractable likelihood can be
tackled using algorithms performing many calls to computer simulators. These
approaches are collectively known as "simulation-based inference" (SBI). Recent
SBI methods have made use of neural networks (NN) to provide approximate, yet
expressive constructs for the unavailable likelihood function and the posterior
distribution. However, they do not generally achieve an optimal trade-off
between accuracy and computational demand. In this work, we propose an
alternative that provides both approximations to the likelihood and the
posterior distribution, using structured mixtures of probability distributions.
Our approach produces accurate posterior inference when compared to
state-of-the-art NN-based SBI methods, while exhibiting a much smaller
computational footprint. We illustrate our results on several benchmark models
from the SBI literature.</div><div><a href='http://arxiv.org/abs/2403.07454v2'>2403.07454v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07355v3")'>Sampling from the Mean-Field Stationary Distribution</div>
<div id='2402.07355v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T01:04:39Z</div><div>Authors: Yunbum Kook, Matthew S. Zhang, Sinho Chewi, Murat A. Erdogdu, Mufan Bill Li</div><div style='padding-top: 10px; width: 80ex'>We study the complexity of sampling from the stationary distribution of a
mean-field SDE, or equivalently, the complexity of minimizing a functional over
the space of probability measures which includes an interaction term. Our main
insight is to decouple the two key aspects of this problem: (1) approximation
of the mean-field SDE via a finite-particle system, via uniform-in-time
propagation of chaos, and (2) sampling from the finite-particle stationary
distribution, via standard log-concave samplers. Our approach is conceptually
simpler and its flexibility allows for incorporating the state-of-the-art for
both algorithms and theory. This leads to improved guarantees in numerous
settings, including better guarantees for optimizing certain two-layer neural
networks in the mean-field regime.</div><div><a href='http://arxiv.org/abs/2402.07355v3'>2402.07355v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18839v2")'>Extended Flow Matching: a Method of Conditional Generation with
  Generalized Continuity Equation</div>
<div id='2402.18839v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T04:12:32Z</div><div>Authors: Noboru Isobe, Masanori Koyama, Kohei Hayashi, Kenji Fukumizu</div><div style='padding-top: 10px; width: 80ex'>The task of conditional generation is one of the most important applications
of generative models, and numerous methods have been developed to date based on
the celebrated diffusion models, with the guidance-based classifier-free method
taking the lead. However, the theory of the guidance-based method not only
requires the user to fine-tune the "guidance strength," but its target vector
field does not necessarily correspond to the conditional distribution used in
training. In this paper, we develop the theory of conditional generation based
on Flow Matching, a current strong contender of diffusion methods. Motivated by
the interpretation of a probability path as a distribution on path space, we
establish a novel theory of flow-based generation of conditional distribution
by employing the mathematical framework of generalized continuity equation
instead of the continuity equation in flow matching. This theory naturally
derives a method that aims to match the matrix field as opposed to the vector
field. Our framework ensures the continuity of the generated conditional
distribution through the existence of flow between conditional distributions.
We will present our theory through experiments and mathematical results.</div><div><a href='http://arxiv.org/abs/2402.18839v2'>2402.18839v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07846v1")'>Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow
  Matching on Assignment Manifolds</div>
<div id='2402.07846v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:56:52Z</div><div>Authors: Bastian Boll, Daniel Gonzalez-Alvarado, Christoph Schn√∂rr</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel generative model for discrete distributions
based on continuous normalizing flows on the submanifold of factorizing
discrete measures. Integration of the flow gradually assigns categories and
avoids issues of discretizing the latent continuous model like rounding, sample
truncation etc. General non-factorizing discrete distributions capable of
representing complex statistical dependencies of structured discrete data, can
be approximated by embedding the submanifold into a the meta-simplex of all
joint discrete distributions and data-driven averaging. Efficient training of
the generative model is demonstrated by matching the flow of geodesics of
factorizing discrete distributions. Various experiments underline the
approach's broad applicability.</div><div><a href='http://arxiv.org/abs/2402.07846v1'>2402.07846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13913v1")'>Spectral Clustering for Discrete Distributions</div>
<div id='2401.13913v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T03:17:03Z</div><div>Authors: Zixiao Wang, Dong Qiao, Jicong Fan</div><div style='padding-top: 10px; width: 80ex'>Discrete distribution clustering (D2C) was often solved by Wasserstein
barycenter methods. These methods are under a common assumption that clusters
can be well represented by barycenters, which may not hold in many real
applications. In this work, we propose a simple yet effective framework based
on spectral clustering and distribution affinity measures (e.g., maximum mean
discrepancy and Wasserstein distance) for D2C. To improve the scalability, we
propose to use linear optimal transport to construct affinity matrices
efficiently on large datasets. We provide theoretical guarantees for the
success of the proposed methods in clustering distributions. Experiments on
synthetic and real data show that our methods outperform the baselines largely
in terms of both clustering accuracy and computational efficiency.</div><div><a href='http://arxiv.org/abs/2401.13913v1'>2401.13913v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01302v1")'>A Unified Framework for Gradient-based Clustering of Distributed Data</div>
<div id='2402.01302v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:44:42Z</div><div>Authors: Aleksandar Armacki, Dragana Bajoviƒá, Du≈°an Jakovetiƒá, Soummya Kar</div><div style='padding-top: 10px; width: 80ex'>We develop a family of distributed clustering algorithms that work over
networks of users. In the proposed scenario, users contain a local dataset and
communicate only with their immediate neighbours, with the aim of finding a
clustering of the full, joint data. The proposed family, termed Distributed
Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$,
controling the proximity of users' center estimates, with $\mathcal{F}$
determining the clustering loss. Specialized to popular clustering losses like
$K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel
distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a
novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We
provide a unified analysis and establish several strong results, under mild
assumptions. First, the sequence of centers generated by the methods converges
to a well-defined notion of fixed point, under any center initialization and
value of $\rho$. Second, as $\rho$ increases, the family of fixed points
produced by DGC-$\mathcal{F}_\rho$ converges to a notion of consensus fixed
points. We show that consensus fixed points of DGC-$\mathcal{F}_{\rho}$ are
equivalent to fixed points of gradient clustering over the full data,
guaranteeing a clustering of the full data is produced. For the special case of
Bregman losses, we show that our fixed points converge to the set of Lloyd
points. Numerical experiments on real data confirm our theoretical findings and
demonstrate strong performance of the methods.</div><div><a href='http://arxiv.org/abs/2402.01302v1'>2402.01302v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01460v1")'>Deep Conditional Generative Learning: Model and Error Analysis</div>
<div id='2402.01460v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:52:10Z</div><div>Authors: Jinyuan Chang, Zhao Ding, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang</div><div style='padding-top: 10px; width: 80ex'>We introduce an Ordinary Differential Equation (ODE) based deep generative
method for learning a conditional distribution, named the Conditional Follmer
Flow. Starting from a standard Gaussian distribution, the proposed flow could
efficiently transform it into the target conditional distribution at time 1.
For effective implementation, we discretize the flow with Euler's method where
we estimate the velocity field nonparametrically using a deep neural network.
Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein
distance between the distribution of the learned samples and the target
distribution, providing the first comprehensive end-to-end error analysis for
conditional distribution learning via ODE flow. Our numerical experiments
showcase its effectiveness across a range of scenarios, from standard
nonparametric conditional density estimation problems to more intricate
challenges involving image data, illustrating its superiority over various
existing conditional density estimation methods.</div><div><a href='http://arxiv.org/abs/2402.01460v1'>2402.01460v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08847v1")'>Space-Time Bridge-Diffusion</div>
<div id='2402.08847v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T23:26:11Z</div><div>Authors: Hamidreza Behjoo, Michael Chertkov</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce a novel method for generating new synthetic
samples that are independent and identically distributed (i.i.d.) from
high-dimensional real-valued probability distributions, as defined implicitly
by a set of Ground Truth (GT) samples. Central to our method is the integration
of space-time mixing strategies that extend across temporal and spatial
dimensions. Our methodology is underpinned by three interrelated stochastic
processes designed to enable optimal transport from an easily tractable initial
probability distribution to the target distribution represented by the GT
samples: (a) linear processes incorporating space-time mixing that yield
Gaussian conditional probability densities, (b) their bridge-diffusion analogs
that are conditioned to the initial and final state vectors, and (c) nonlinear
stochastic processes refined through score-matching techniques. The crux of our
training regime involves fine-tuning the nonlinear model, and potentially the
linear models - to align closely with the GT data. We validate the efficacy of
our space-time diffusion approach with numerical experiments, laying the
groundwork for more extensive future theory and experiments to fully
authenticate the method, particularly providing a more efficient (possibly
simulation-free) inference.</div><div><a href='http://arxiv.org/abs/2402.08847v1'>2402.08847v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14544v1")'>Bayesian Optimization through Gaussian Cox Process Models for
  Spatio-temporal Data</div>
<div id='2401.14544v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T22:26:15Z</div><div>Authors: Yongsheng Mei, Mahdi Imani, Tian Lan</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimization (BO) has established itself as a leading strategy for
efficiently optimizing expensive-to-evaluate functions. Existing BO methods
mostly rely on Gaussian process (GP) surrogate models and are not applicable to
(doubly-stochastic) Gaussian Cox processes, where the observation process is
modulated by a latent intensity function modeled as a GP. In this paper, we
propose a novel maximum a posteriori inference of Gaussian Cox processes. It
leverages the Laplace approximation and change of kernel technique to transform
the problem into a new reproducing kernel Hilbert space, where it becomes more
tractable computationally. It enables us to obtain both a functional posterior
of the latent intensity function and the covariance of the posterior, thus
extending existing works that often focus on specific link functions or
estimating the posterior mean. Using the result, we propose a BO framework
based on the Gaussian Cox process model and further develop a Nystr\"om
approximation for efficient computation. Extensive evaluations on various
synthetic and real-world datasets demonstrate significant improvement over
state-of-the-art inference solutions for Gaussian Cox processes, as well as
effective BO with a wide range of acquisition functions designed through the
underlying Gaussian Cox process model.</div><div><a href='http://arxiv.org/abs/2401.14544v1'>2401.14544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17539v1")'>Enhancing Score-Based Sampling Methods with Ensembles</div>
<div id='2401.17539v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T01:51:29Z</div><div>Authors: Tobias Bischoff, Bryan Riel</div><div style='padding-top: 10px; width: 80ex'>We introduce ensembles within score-based sampling methods to develop
gradient-free approximate sampling techniques that leverage the collective
dynamics of particle ensembles to compute approximate reverse diffusion drifts.
We introduce the underlying methodology, emphasizing its relationship with
generative diffusion models and the previously introduced F\"ollmer sampler. We
demonstrate the efficacy of ensemble strategies through various examples,
ranging from low- to medium-dimensionality sampling problems, including
multi-modal and highly non-Gaussian probability distributions, and provide
comparisons to traditional methods like NUTS. Our findings highlight the
potential of ensemble strategies for modeling complex probability distributions
in situations where gradients are unavailable. Finally, we showcase its
application in the context of Bayesian inversion problems within the
geophysical sciences.</div><div><a href='http://arxiv.org/abs/2401.17539v1'>2401.17539v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08193v1")'>Gaussian Ensemble Belief Propagation for Efficient Inference in
  High-Dimensional Systems</div>
<div id='2402.08193v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T03:31:36Z</div><div>Authors: Dan MacKinlay, Russell Tsuchida, Dan Pagendam, Petra Kuhnert</div><div style='padding-top: 10px; width: 80ex'>Efficient inference in high-dimensional models remains a central challenge in
machine learning. This paper introduces the Gaussian Ensemble Belief
Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and
Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing
low-rank local messages in a graphical model structure. This combination
inherits favourable qualities from each method. Ensemble techniques allow GEnBP
to handle high-dimensional states, parameters and intricate, noisy, black-box
generation processes. The use of local messages in a graphical model structure
ensures that the approach is suited to distributed computing and can
efficiently handle complex dependence structures. GEnBP is particularly
advantageous when the ensemble size is considerably smaller than the inference
dimension. This scenario often arises in fields such as spatiotemporal
modelling, image processing and physical model inversion. GEnBP can be applied
to general problem structures, including jointly learning system parameters,
observation parameters, and latent state variables.</div><div><a href='http://arxiv.org/abs/2402.08193v1'>2402.08193v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01389v1")'>Fusion of Gaussian Processes Predictions with Monte Carlo Sampling</div>
<div id='2403.01389v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T04:21:21Z</div><div>Authors: Marzieh Ajirak, Daniel Waxman, Fernando Llorente, Petar M. Djuric</div><div style='padding-top: 10px; width: 80ex'>In science and engineering, we often work with models designed for accurate
prediction of variables of interest. Recognizing that these models are
approximations of reality, it becomes desirable to apply multiple models to the
same data and integrate their outcomes. In this paper, we operate within the
Bayesian paradigm, relying on Gaussian processes as our models. These models
generate predictive probability density functions (pdfs), and the objective is
to integrate them systematically, employing both linear and log-linear pooling.
We introduce novel approaches for log-linear pooling, determining
input-dependent weights for the predictive pdfs of the Gaussian processes. The
aggregation of the pdfs is realized through Monte Carlo sampling, drawing
samples of weights from their posterior. The performance of these methods, as
well as those based on linear pooling, is demonstrated using a synthetic
dataset.</div><div><a href='http://arxiv.org/abs/2403.01389v1'>2403.01389v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16683v1")'>Re-Envisioning Numerical Information Field Theory (NIFTy.re): A Library
  for Gaussian Processes and Variational Inference</div>
<div id='2402.16683v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T16:01:35Z</div><div>Authors: Gordian Edenhofer, Philipp Frank, Jakob Roth, Reimar H. Leike, Massin Guerdi, Lukas I. Scheel-Platz, Matteo Guardiani, Vincent Eberle, Margret Westerkamp, Torsten A. En√ülin</div><div style='padding-top: 10px; width: 80ex'>Imaging is the process of transforming noisy, incomplete data into a space
that humans can interpret. NIFTy is a Bayesian framework for imaging and has
already successfully been applied to many fields in astrophysics. Previous
design decisions held the performance and the development of methods in NIFTy
back. We present a rewrite of NIFTy, coined NIFTy.re, which reworks the
modeling principle, extends the inference strategies, and outsources much of
the heavy lifting to JAX. The rewrite dramatically accelerates models written
in NIFTy, lays the foundation for new types of inference machineries, improves
maintainability, and enables interoperability between NIFTy and the JAX machine
learning ecosystem.</div><div><a href='http://arxiv.org/abs/2402.16683v1'>2402.16683v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10797v2")'>BlackJAX: Composable Bayesian inference in JAX</div>
<div id='2402.10797v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:21:02Z</div><div>Authors: Alberto Cabezas, Adrien Corenflos, Junpeng Lao, R√©mi Louf, Antoine Carnec, Kaustubh Chaudhari, Reuben Cohn-Gordon, Jeremie Coullon, Wei Deng, Sam Duffield, Gerardo Dur√°n-Mart√≠n, Marcin Elantkowski, Dan Foreman-Mackey, Michele Gregori, Carlos Iguaran, Ravin Kumar, Martin Lysy, Kevin Murphy, Juan Camilo Orduz, Karm Patel, Xi Wang, Rob Zinkov</div><div style='padding-top: 10px; width: 80ex'>BlackJAX is a library implementing sampling and variational inference
algorithms commonly used in Bayesian computation. It is designed for ease of
use, speed, and modularity by taking a functional approach to the algorithms'
implementation. BlackJAX is written in Python, using JAX to compile and run
NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The
library integrates well with probabilistic programming languages by working
directly with the (un-normalized) target log density function. BlackJAX is
intended as a collection of low-level, composable implementations of basic
statistical 'atoms' that can be combined to perform well-defined Bayesian
inference, but also provides high-level routines for ease of use. It is
designed for users who need cutting-edge methods, researchers who want to
create complex sampling methods, and people who want to learn how these work.</div><div><a href='http://arxiv.org/abs/2402.10797v2'>2402.10797v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01393v2")'>Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and
  Stochastic root finding</div>
<div id='2401.01393v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T15:37:47Z</div><div>Authors: John Erik Fornaess, Mi Hu, Tuyen Trung Truong, Takayuki Watanabe</div><div style='padding-top: 10px; width: 80ex'>A new variant of Newton's method - named Backtracking New Q-Newton's method
(BNQN) - which has strong theoretical guarantee, is easy to implement, and has
good experimental performance, was recently introduced by the third author.
  Experiments performed previously showed some remarkable properties of the
basins of attractions for finding roots of polynomials and meromorphic
functions, with BNQN. In general, they look more smooth than that of Newton's
method.
  In this paper, we continue to experimentally explore in depth this remarkable
phenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link
poses a couple of challenging puzzles to be explained. Experiments also
indicate that BNQN is more robust against random perturbations than Newton's
method and Random Relaxed Newton's method.</div><div><a href='http://arxiv.org/abs/2401.01393v2'>2401.01393v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08001v1")'>Improvement and generalization of ABCD method with Bayesian inference</div>
<div id='2402.08001v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:05:27Z</div><div>Authors: Ezequiel Alvarez, Leandro Da Rold, Manuel Szewc, Alejandro Szynkman, Santiago A. Tanco, Tatiana Tarutina</div><div style='padding-top: 10px; width: 80ex'>To find New Physics or to refine our knowledge of the Standard Model at the
LHC is an enterprise that involves many factors. We focus on taking advantage
of available information and pour our effort in re-thinking the usual
data-driven ABCD method to improve it and to generalize it using Bayesian
Machine Learning tools. We propose that a dataset consisting of a signal and
many backgrounds is well described through a mixture model. Signal, backgrounds
and their relative fractions in the sample can be well extracted by exploiting
the prior knowledge and the dependence between the different observables at the
event-by-event level with Bayesian tools. We show how, in contrast to the ABCD
method, one can take advantage of understanding some properties of the
different backgrounds and of having more than two independent observables to
measure in each event. In addition, instead of regions defined through hard
cuts, the Bayesian framework uses the information of continuous distribution to
obtain soft-assignments of the events which are statistically more robust. To
compare both methods we use a toy problem inspired by $pp\to hh\to b\bar b b
\bar b$, selecting a reduced and simplified number of processes and analysing
the flavor of the four jets and the invariant mass of the jet-pairs, modeled
with simplified distributions. Taking advantage of all this information, and
starting from a combination of biased and agnostic priors, leads us to a very
good posterior once we use the Bayesian framework to exploit the data and the
mutual information of the observables at the event-by-event level. We show how,
in this simplified model, the Bayesian framework outperforms the ABCD method
sensitivity in obtaining the signal fraction in scenarios with $1\%$ and
$0.5\%$ true signal fractions in the dataset. We also show that the method is
robust against the absence of signal.</div><div><a href='http://arxiv.org/abs/2402.08001v1'>2402.08001v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10603v1")'>ZnTrack -- Data as Code</div>
<div id='2401.10603v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T10:21:27Z</div><div>Authors: Fabian Zills, Moritz Sch√§fer, Samuel Tovey, Johannes K√§stner, Christian Holm</div><div style='padding-top: 10px; width: 80ex'>The past decade has seen tremendous breakthroughs in computation and there is
no indication that this will slow any time soon. Machine learning, large-scale
computing resources, and increased industry focus have resulted in rising
investments in computer-driven solutions for data management, simulations, and
model generation. However, with this growth in computation has come an even
larger expansion of data and with it, complexity in data storage, sharing, and
tracking. In this work, we introduce ZnTrack, a Python-driven data versioning
tool. ZnTrack builds upon established version control systems to provide a
user-friendly and easy-to-use interface for tracking parameters in experiments,
designing workflows, and storing and sharing data. From this ability to reduce
large datasets to a simple Python script emerges the concept of Data as Code, a
core component of the work presented here and an undoubtedly important concept
as the age of computation continues to evolve. ZnTrack offers an open-source,
FAIR data compatible Python package to enable users to harness these concepts
of the future.</div><div><a href='http://arxiv.org/abs/2401.10603v1'>2401.10603v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09122v1")'>Mixed-Output Gaussian Process Latent Variable Models</div>
<div id='2402.09122v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T12:18:23Z</div><div>Authors: James Odgers, Chrysoula Kappatou, Ruth Misener, Sarah Filippi</div><div style='padding-top: 10px; width: 80ex'>This work develops a Bayesian non-parametric approach to signal separation
where the signals may vary according to latent variables. Our key contribution
is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate
the case where each data point comprises the weighted sum of a known number of
pure component signals, observed across several input locations. Our framework
allows the use of a range of priors for the weights of each observation. This
flexibility enables us to represent use cases including sum-to-one constraints
for estimating fractional makeup, and binary weights for classification. Our
contributions are particularly relevant to spectroscopy, where changing
conditions may cause the underlying pure component signals to vary from sample
to sample. To demonstrate the applicability to both spectroscopy and other
domains, we consider several applications: a near-infrared spectroscopy data
set with varying temperatures, a simulated data set for identifying flow
configuration through a pipe, and a data set for determining the type of rock
from its reflectance.</div><div><a href='http://arxiv.org/abs/2402.09122v1'>2402.09122v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17492v1")'>syren-halofit: A fast, interpretable, high-precision formula for the
  $Œõ$CDM nonlinear matter power spectrum</div>
<div id='2402.17492v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T13:18:00Z</div><div>Authors: Deaglan J. Bartlett, Benjamin D. Wandelt, Matteo Zennaro, Pedro G. Ferreira, Harry Desmond</div><div style='padding-top: 10px; width: 80ex'>Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$,
as a function of cosmological parameters and redshift is of fundamental
importance in cosmology. Analytic approximations provide an interpretable
solution, yet current approximations are neither fast nor accurate relative to
black-box numerical emulators. We use symbolic regression to obtain simple
analytic approximations to the nonlinear scale, $k_\sigma$, the effective
spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for
the halofit model. We then re-optimise the coefficients of halofit to fit a
wide range of cosmologies and redshifts. We then again exploit symbolic
regression to explore the space of analytic expressions to fit the residuals
between $P(k)$ and the optimised predictions of halofit. All methods are
validated against $N$-body simulations. Our symbolic expressions for
$k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squared fractional errors of
0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of
cosmologies. The re-optimised halofit parameters reduce the root mean squared
fractional error from 3% to below 2% for wavenumbers $k=9\times10^{-3}-9 \,
h{\rm Mpc^{-1}}$. We introduce syren-halofit (symbolic-regression-enhanced
halofit), an extension to halofit containing a short symbolic correction which
improves this error to 1%. Our method is 2350 and 3170 times faster than
current halofit and hmcode implementations, respectively, and 2680 and 64 times
faster than EuclidEmulator2 (which requires running class) and the BACCO
emulator. We obtain comparable accuracy to EuclidEmulator2 and the BACCO
emulator when tested on $N$-body simulations. Our work greatly increases the
speed and accuracy of symbolic approximations to $P(k)$, making them
significantly faster than their numerical counterparts without loss of
accuracy.</div><div><a href='http://arxiv.org/abs/2402.17492v1'>2402.17492v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06182v2")'>Prediction of Cellular Identities from Trajectory and Cell Fate
  Information</div>
<div id='2401.06182v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T03:28:13Z</div><div>Authors: Baiyang Dai, Jiamin Yang, Hari Shroff, Patrick La Riviere</div><div style='padding-top: 10px; width: 80ex'>Determining cell identities in imaging sequences is an important yet
challenging task. The conventional method for cell identification is via cell
tracking, which is complex and can be time-consuming. In this study, we propose
an innovative approach to cell identification during early $\textit{C.
elegans}$ embryogenesis using machine learning. Cell identification during
$\textit{C. elegans}$ embryogenesis would provide insights into neural
development with implications for higher organisms including humans. We
employed random forest, MLP, and LSTM models, and tested cell classification
accuracy on 3D time-lapse confocal datasets spanning the first 4 hours of
embryogenesis. By leveraging a small number of spatial-temporal features of
individual cells, including cell trajectory and cell fate information, our
models achieve an accuracy of over 91%, even with limited data. We also
determine the most important feature contributions and can interpret these
features in the context of biological knowledge. Our research demonstrates the
success of predicting cell identities in time-lapse imaging sequences directly
from simple spatio-temporal features.</div><div><a href='http://arxiv.org/abs/2401.06182v2'>2401.06182v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.19295v1")'>Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical
  Bayesian Modelling</div>
<div id='2402.19295v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:58:16Z</div><div>Authors: S. M. Smith, A. J. Hughes, T. A. Dardeno, L. A. Bull, N. Dervilis, K. Worden</div><div style='padding-top: 10px; width: 80ex'>Population-based structural health monitoring (PBSHM), aims to share
information between members of a population. An offshore wind (OW) farm could
be considered as a population of nominally-identical wind-turbine structures.
However, benign variations exist among members, such as geometry, sea-bed
conditions and temperature differences. These factors could influence
structural properties and therefore the dynamic response, making it more
difficult to detect structural problems via traditional SHM techniques. This
paper explores the use of a hierarchical Bayesian model to infer expected soil
stiffness distributions at both population and local levels, as a basis to
perform anomaly detection, in the form of scour, for new and existing turbines.
To do this, observations of natural frequency will be generated as though they
are from a small population of wind turbines. Differences between individual
observations will be introduced by postulating distributions over the soil
stiffness and measurement noise, as well as reducing soil depth (to represent
scour), in the case of anomaly detection.</div><div><a href='http://arxiv.org/abs/2402.19295v1'>2402.19295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17036v1")'>Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical
  Systems</div>
<div id='2402.17036v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T21:35:33Z</div><div>Authors: Rafael Anderka, Marc Peter Deisenroth, So Takao</div><div style='padding-top: 10px; width: 80ex'>Data assimilation (DA) methods use priors arising from differential equations
to robustly interpolate and extrapolate data. Popular techniques such as
ensemble methods that handle high-dimensional, nonlinear PDE priors focus
mostly on state estimation, however can have difficulty learning the parameters
accurately. On the other hand, machine learning based approaches can naturally
learn the state and parameters, but their applicability can be limited, or
produce uncertainties that are hard to interpret. Inspired by the Integrated
Nested Laplace Approximation (INLA) method in spatial statistics, we propose an
alternative approach to DA based on iteratively linearising the dynamical
model. This produces a Gaussian Markov random field at each iteration, enabling
one to use INLA to infer the state and parameters. Our approach can be used for
arbitrary nonlinear systems, while retaining interpretability, and is
furthermore demonstrated to outperform existing methods on the DA task. By
providing a more nuanced approach to handling nonlinear PDE priors, our
methodology offers improved accuracy and robustness in predictions, especially
where data sparsity is prevalent.</div><div><a href='http://arxiv.org/abs/2402.17036v1'>2402.17036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17570v2")'>Sparse Variational Contaminated Noise Gaussian Process Regression for
  Forecasting Geomagnetic Perturbations</div>
<div id='2402.17570v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T15:08:57Z</div><div>Authors: Daniel Iong, Matthew McAnear, Yuezhou Qu, Shasha Zou, Gabor Toth, Yang Chen</div><div style='padding-top: 10px; width: 80ex'>Gaussian Processes (GP) have become popular machine learning methods for
kernel based learning on datasets with complicated covariance structures. In
this paper, we present a novel extension to the GP framework using a
contaminated normal likelihood function to better account for heteroscedastic
variance and outlier noise. We propose a scalable inference algorithm based on
the Sparse Variational Gaussian Process (SVGP) method for fitting sparse
Gaussian process regression models with contaminated normal noise on large
datasets. We examine an application to geomagnetic ground perturbations, where
the state-of-art prediction model is based on neural networks. We show that our
approach yields shorter predictions intervals for similar coverage and accuracy
when compared to an artificial dense neural network baseline.</div><div><a href='http://arxiv.org/abs/2402.17570v2'>2402.17570v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10578v1")'>Generative Modelling of Stochastic Rotating Shallow Water Noise</div>
<div id='2403.10578v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T09:30:29Z</div><div>Authors: Dan Crisan, Oana Lang, Alexander Lobbe</div><div style='padding-top: 10px; width: 80ex'>In recent work, the authors have developed a generic methodology for
calibrating the noise in fluid dynamics stochastic partial differential
equations where the stochasticity was introduced to parametrize subgrid-scale
processes. The stochastic parameterization of sub-grid scale processes is
required in the estimation of uncertainty in weather and climate predictions,
to represent systematic model errors arising from subgrid-scale fluctuations.
The previous methodology used a principal component analysis (PCA) technique
based on the ansatz that the increments of the stochastic parametrization are
normally distributed.
  In this paper, the PCA technique is replaced by a generative model technique.
This enables us to avoid imposing additional constraints on the increments. The
methodology is tested on a stochastic rotating shallow water model with the
elevation variable of the model used as input data. The numerical simulations
show that the noise is indeed non-Gaussian. The generative modelling technology
gives good RMSE, CRPS score and forecast rank histogram results.</div><div><a href='http://arxiv.org/abs/2403.10578v1'>2403.10578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15645v1")'>Ensemble-Based Annealed Importance Sampling</div>
<div id='2401.15645v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T12:47:39Z</div><div>Authors: Haoxuan Chen, Lexing Ying</div><div style='padding-top: 10px; width: 80ex'>Sampling from a multimodal distribution is a fundamental and challenging
problem in computational science and statistics. Among various approaches
proposed for this task, one popular method is Annealed Importance Sampling
(AIS). In this paper, we propose an ensemble-based version of AIS by combining
it with population-based Monte Carlo methods to improve its efficiency. By
keeping track of an ensemble instead of a single particle along some
continuation path between the starting distribution and the target
distribution, we take advantage of the interaction within the ensemble to
encourage the exploration of undiscovered modes. Specifically, our main idea is
to utilize either the snooker algorithm or the genetic algorithm used in
Evolutionary Monte Carlo. We discuss how the proposed algorithm can be
implemented and derive a partial differential equation governing the evolution
of the ensemble under the continuous time and mean-field limit. We also test
the efficiency of the proposed algorithm on various continuous and discrete
distributions.</div><div><a href='http://arxiv.org/abs/2401.15645v1'>2401.15645v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10800v2")'>Estimation of AMOC transition probabilities using a machine learning
  based rare-event algorithm</div>
<div id='2401.10800v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:36:27Z</div><div>Authors: Val√©rian Jacques-Dumas, Ren√© M. van Westen, Henk A. Dijkstra</div><div style='padding-top: 10px; width: 80ex'>The Atlantic Meridional Overturning Circulation (AMOC) is an important
component of the global climate, known to be a tipping element, as it could
collapse under global warming. The main objective of this study is to compute
the probability that the AMOC collapses within a specified time window, using a
rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS).
However, the efficiency and accuracy of TAMS depend on the choice of the score
function. Although the definition of the optimal score function, called
``committor function" is known, it is impossible in general to compute it a
priori. Here, we combine TAMS with a Next-Generation Reservoir Computing
technique that estimates the committor function from the data generated by the
rare-event algorithm. We test this technique in a stochastic box model of the
AMOC for which two types of transition exist, the so-called F(ast)-transitions
and S(low)-transitions. Results for the F-transtions compare favorably with
those in the literature where a physically-informed score function was used. We
show that coupling a rare-event algorithm with machine learning allows for a
correct estimation of transition probabilities, transition times, and even
transition paths for a wide range of model parameters. We then extend these
results to the more difficult problem of S-transitions in the same model. In
both cases of F- and S-transitions, we also show how the Next-Generation
Reservoir Computing technique can be interpreted to retrieve an analytical
estimate of the committor function.</div><div><a href='http://arxiv.org/abs/2401.10800v2'>2401.10800v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.05966v1")'>Rethink Model Re-Basin and the Linear Mode Connectivity</div>
<div id='2402.05966v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:06:26Z</div><div>Authors: Xingyu Qu, Samuel Horvath</div><div style='padding-top: 10px; width: 80ex'>Recent studies suggest that with sufficiently wide models, most SGD solutions
can, up to permutation, converge into the same basin. This phenomenon, known as
the model re-basin regime, has significant implications for model averaging.
However, current re-basin strategies are limited in effectiveness due to a lack
of comprehensive understanding of underlying mechanisms. Addressing this gap,
our work revisits standard practices and uncovers the frequent inadequacies of
existing matching algorithms, which we show can be mitigated through proper
re-normalization. By introducing a more direct analytical approach, we expose
the interaction between matching algorithms and re-normalization processes.
This perspective not only clarifies and refines previous findings but also
facilitates novel insights. For instance, it connects the linear mode
connectivity to pruning, motivating a lightweight yet effective post-pruning
plug-in that can be directly merged with any existing pruning techniques. Our
implementation is available at https://github.com/XingyuQu/rethink-re-basin.</div><div><a href='http://arxiv.org/abs/2402.05966v1'>2402.05966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12242v1")'>Synthetic location trajectory generation using categorical diffusion
  models</div>
<div id='2402.12242v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:57:39Z</div><div>Authors: Simon Dirmeier, Ye Hong, Fernando Perez-Cruz</div><div style='padding-top: 10px; width: 80ex'>Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the
predominant generative models for the simulation of synthetic data, for
instance, for computer vision, audio, natural language processing, or
biomolecule generation. Here, we propose using DPMs for the generation of
synthetic individual location trajectories (ILTs) which are sequences of
variables representing physical locations visited by individuals. ILTs are of
major importance in mobility research to understand the mobility behavior of
populations and to ultimately inform political decision-making. We represent
ILTs as multi-dimensional categorical random variables and propose to model
their joint distribution using a continuous DPM by first applying the diffusion
process in a continuous unconstrained space and then mapping the continuous
variables into a discrete space. We demonstrate that our model can synthesize
realistic ILPs by comparing conditionally and unconditionally generated
sequences to real-world ILPs from a GNSS tracking data set which suggests the
potential use of our model for synthetic data generation, for example, for
benchmarking models used in mobility research.</div><div><a href='http://arxiv.org/abs/2402.12242v1'>2402.12242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05054v1")'>A Sinkhorn-type Algorithm for Constrained Optimal Transport</div>
<div id='2403.05054v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T05:01:43Z</div><div>Authors: Xun Tang, Holakou Rahmanian, Michael Shavlovsky, Kiran Koshy Thekumparampil, Tesi Xiao, Lexing Ying</div><div style='padding-top: 10px; width: 80ex'>Entropic optimal transport (OT) and the Sinkhorn algorithm have made it
practical for machine learning practitioners to perform the fundamental task of
calculating transport distance between statistical distributions. In this work,
we focus on a general class of OT problems under a combination of equality and
inequality constraints. We derive the corresponding entropy regularization
formulation and introduce a Sinkhorn-type algorithm for such constrained OT
problems supported by theoretical guarantees. We first bound the approximation
error when solving the problem through entropic regularization, which reduces
exponentially with the increase of the regularization parameter. Furthermore,
we prove a sublinear first-order convergence rate of the proposed Sinkhorn-type
algorithm in the dual space by characterizing the optimization procedure with a
Lyapunov function. To achieve fast and higher-order convergence under weak
entropy regularization, we augment the Sinkhorn-type algorithm with dynamic
regularization scheduling and second-order acceleration. Overall, this work
systematically combines recent theoretical and numerical advances in entropic
optimal transport with the constrained case, allowing practitioners to derive
approximate transport plans in complex scenarios.</div><div><a href='http://arxiv.org/abs/2403.05054v1'>2403.05054v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04259v2")'>Decentralized and Equitable Optimal Transport</div>
<div id='2403.04259v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T06:47:45Z</div><div>Authors: Ivan Lau, Shiqian Ma, C√©sar A. Uribe</div><div style='padding-top: 10px; width: 80ex'>This paper considers the decentralized (discrete) optimal transport (D-OT)
problem. In this setting, a network of agents seeks to design a transportation
plan jointly, where the cost function is the sum of privately held costs for
each agent. We reformulate the D-OT problem as a constraint-coupled
optimization problem and propose a single-loop decentralized algorithm with an
iteration complexity of O(1/{\epsilon}) that matches existing centralized
first-order approaches. Moreover, we propose the decentralized equitable
optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively
designing a transportation plan that minimizes transportation costs, agents
seek to ensure equity in their individual costs. The iteration complexity of
the proposed method to solve DE-OT is also O(1/{\epsilon}). This rate improves
existing centralized algorithms, where the best iteration complexity obtained
is O(1/{\epsilon}^2).</div><div><a href='http://arxiv.org/abs/2403.04259v2'>2403.04259v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.18012v1")'>Diffusion Models as Constrained Samplers for Optimization with Unknown
  Constraints</div>
<div id='2402.18012v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T03:09:12Z</div><div>Authors: Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De Bortol, Haorui Wang, Dongxia Wu, Aaron Ferber, Yi-An Ma, Carla P. Gomes, Chao Zhang</div><div style='padding-top: 10px; width: 80ex'>Addressing real-world optimization problems becomes particularly challenging
when analytic objective functions or constraints are unavailable. While
numerous studies have addressed the issue of unknown objectives, limited
research has focused on scenarios where feasibility constraints are not given
explicitly. Overlooking these constraints can lead to spurious solutions that
are unrealistic in practice. To deal with such unknown constraints, we propose
to perform optimization within the data manifold using diffusion models. To
constrain the optimization process to the data manifold, we reformulate the
original optimization problem as a sampling problem from the product of the
Boltzmann distribution defined by the objective function and the data
distribution learned by the diffusion model. To enhance sampling efficiency, we
propose a two-stage framework that begins with a guided diffusion process for
warm-up, followed by a Langevin dynamics stage for further correction.
Theoretical analysis shows that the initial stage results in a distribution
focused on feasible solutions, thereby providing a better initialization for
the later stage. Comprehensive experiments on a synthetic dataset, six
real-world black-box optimization datasets, and a multi-objective optimization
dataset show that our method achieves better or comparable performance with
previous state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2402.18012v1'>2402.18012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02190v1")'>Continuous Tensor Relaxation for Finding Diverse Solutions in
  Combinatorial Optimization Problems</div>
<div id='2402.02190v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T15:31:05Z</div><div>Authors: Yuma Ichikawa, Hiroaki Iwashita</div><div style='padding-top: 10px; width: 80ex'>Finding the best solution is the most common objective in combinatorial
optimization (CO) problems. However, a single solution may not be suitable in
practical scenarios, as the objective functions and constraints are only
approximations of original real-world situations. To tackle this, finding (i)
"heterogeneous solutions", diverse solutions with distinct characteristics, and
(ii) "penalty-diversified solutions", variations in constraint severity, are
natural directions. This strategy provides the flexibility to select a suitable
solution during post-processing. However, discovering these diverse solutions
is more challenging than identifying a single solution. To overcome this
challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA)
for unsupervised-learning-based CO solvers. CTRA addresses various problems
simultaneously by extending the continual relaxation approach, which transforms
discrete decision variables into continual tensors. This method finds
heterogeneous and penalty-diversified solutions through mutual interactions,
where the choice of one solution affects the other choices. Numerical
experiments show that CTRA enables UL-based solvers to find heterogeneous and
penalty-diversified solutions much faster than existing UL-based solvers.
Moreover, these experiments reveal that CTRA enhances the exploration ability.</div><div><a href='http://arxiv.org/abs/2402.02190v1'>2402.02190v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03207v1")'>Light and Optimal Schr√∂dinger Bridge Matching</div>
<div id='2402.03207v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:17:57Z</div><div>Authors: Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, Alexander Korotin</div><div style='padding-top: 10px; width: 80ex'>Schr\"odinger Bridges (SB) have recently gained the attention of the ML
community as a promising extension of classic diffusion models which is also
interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB
exploit the pervasive bridge matching procedures. Such procedures aim to
recover a stochastic process transporting the mass between distributions given
only a transport plan between them. In particular, given the EOT plan, these
procedures can be adapted to solve SB. This fact is heavily exploited by recent
works giving rives to matching-based SB solvers. The cornerstone here is
recovering the EOT plan: recent works either use heuristical approximations
(e.g., the minibatch OT) or establish iterative matching procedures which by
the design accumulate the error during the training. We address these
limitations and propose a novel procedure to learn SB which we call the
\textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal
parameterization of the diffusion process and provably recovers the SB process
\textbf{(a)} with a single bridge matching step and \textbf{(b)} with arbitrary
transport plan as the input. Furthermore, we show that the optimal bridge
matching objective coincides with the recently discovered energy-based modeling
(EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a
light solver (which we call LightSB-M) to implement optimal matching in
practice using the Gaussian mixture parameterization of the Schr\"odinger
potential. We experimentally showcase the performance of our solver in a range
of practical tasks. The code for the LightSB-M solver can be found at
\url{https://github.com/SKholkin/LightSB-Matching}.</div><div><a href='http://arxiv.org/abs/2402.03207v1'>2402.03207v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15602v1")'>Minimax Optimality of Score-based Diffusion Models: Beyond the Density
  Lower Bound Assumptions</div>
<div id='2402.15602v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T20:51:31Z</div><div>Authors: Kaihong Zhang, Heqi Yin, Feng Liang, Jingbo Liu</div><div style='padding-top: 10px; width: 80ex'>We study the asymptotic error of score-based diffusion model sampling in
large-sample scenarios from a non-parametric statistics perspective. We show
that a kernel-based score estimator achieves an optimal mean square error of
$\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$
for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and
$d$ represent the sample size and the dimension, $t$ is bounded above and below
by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As
a consequence, this yields an $\widetilde{O}\left(n^{-1/2}
t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the
distribution of the sample generated by the diffusion model under a mere
sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric
family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early
stopping strategy, we obtain that the diffusion model is nearly (up to log
factors) minimax optimal. This removes the crucial lower bound assumption on
$p_0$ in previous proofs of the minimax optimality of the diffusion model for
nonparametric families.</div><div><a href='http://arxiv.org/abs/2402.15602v1'>2402.15602v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15520v1")'>Oracle-Efficient Hybrid Online Learning with Unknown Distribution</div>
<div id='2401.15520v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T22:45:02Z</div><div>Authors: Changlong Wu, Jin Sima, Wojciech Szpankowski</div><div style='padding-top: 10px; width: 80ex'>We study the problem of oracle-efficient hybrid online learning when the
features are generated by an unknown i.i.d. process and the labels are
generated adversarially. Assuming access to an (offline) ERM oracle, we show
that there exists a computationally efficient online predictor that achieves a
regret upper bounded by $\tilde{O}(T^{\frac{3}{4}})$ for a finite-VC class, and
upper bounded by $\tilde{O}(T^{\frac{p+1}{p+2}})$ for a class with $\alpha$
fat-shattering dimension $\alpha^{-p}$. This provides the first known
oracle-efficient sublinear regret bounds for hybrid online learning with an
unknown feature generation process. In particular, it confirms a conjecture of
Lazaric and Munos (JCSS 2012). We then extend our result to the scenario of
shifting distributions with $K$ changes, yielding a regret of order
$\tilde{O}(T^{\frac{4}{5}}K^{\frac{1}{5}})$. Finally, we establish a regret of
$\tilde{O}((K^{\frac{2}{3}}(\log|\mathcal{H}|)^{\frac{1}{3}}+K)\cdot
T^{\frac{4}{5}})$ for the contextual $K$-armed bandits with a finite policy set
$\mathcal{H}$, i.i.d. generated contexts from an unknown distribution, and
adversarially generated costs.</div><div><a href='http://arxiv.org/abs/2401.15520v1'>2401.15520v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00315v1")'>Online Distribution Learning with Local Private Constraints</div>
<div id='2402.00315v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T03:56:48Z</div><div>Authors: Jin Sima, Changlong Wu, Olgica Milenkovic, Wojciech Szpankowski</div><div style='padding-top: 10px; width: 80ex'>We study the problem of online conditional distribution estimation with
\emph{unbounded} label sets under local differential privacy. Let $\mathcal{F}$
be a distribution-valued function class with unbounded label set. We aim at
estimating an \emph{unknown} function $f\in \mathcal{F}$ in an online fashion
so that at time $t$ when the context $\boldsymbol{x}_t$ is provided we can
generate an estimate of $f(\boldsymbol{x}_t)$ under KL-divergence knowing only
a privatized version of the true labels sampling from $f(\boldsymbol{x}_t)$.
The ultimate objective is to minimize the cumulative KL-risk of a finite
horizon $T$. We show that under $(\epsilon,0)$-local differential privacy of
the privatized labels, the KL-risk grows as
$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$ upto poly-logarithmic factors
where $K=|\mathcal{F}|$. This is in stark contrast to the
$\tilde{\Theta}(\sqrt{T\log K})$ bound demonstrated by Wu et al. (2023a) for
bounded label sets. As a byproduct, our results recover a nearly tight upper
bound for the hypothesis selection problem of gopi et al. (2020) established
only for the batch setting.</div><div><a href='http://arxiv.org/abs/2402.00315v1'>2402.00315v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09483v1")'>Oracle-Efficient Differentially Private Learning with Public Data</div>
<div id='2402.09483v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T23:40:50Z</div><div>Authors: Adam Block, Mark Bun, Rathin Desai, Abhishek Shetty, Steven Wu</div><div style='padding-top: 10px; width: 80ex'>Due to statistical lower bounds on the learnability of many function classes
under privacy constraints, there has been recent interest in leveraging public
data to improve the performance of private learning algorithms. In this model,
algorithms must always guarantee differential privacy with respect to the
private samples while also ensuring learning guarantees when the private data
distribution is sufficiently close to that of the public data. Previous work
has demonstrated that when sufficient public, unlabelled data is available,
private learning can be made statistically tractable, but the resulting
algorithms have all been computationally inefficient. In this work, we present
the first computationally efficient, algorithms to provably leverage public
data to learn privately whenever a function class is learnable non-privately,
where our notion of computational efficiency is with respect to the number of
calls to an optimization oracle for the function class. In addition to this
general result, we provide specialized algorithms with improved sample
complexities in the special cases when the function class is convex or when the
task is binary classification.</div><div><a href='http://arxiv.org/abs/2402.09483v1'>2402.09483v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03856v1")'>Public-data Assisted Private Stochastic Optimization: Power and
  Limitations</div>
<div id='2403.03856v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:06:11Z</div><div>Authors: Enayat Ullah, Michael Menart, Raef Bassily, Crist√≥bal Guzm√°n, Raman Arora</div><div style='padding-top: 10px; width: 80ex'>We study the limits and capability of public-data assisted differentially
private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic
convex optimization (SCO) with either labeled or unlabeled public data. For
complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has
excess risk
$\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon}
\big\} \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of
public samples, ${n_{\text{priv}}}$ is the number of private samples, and
$n={n_{\text{pub}}}+{n_{\text{priv}}}$. These lower bounds are established via
our new lower bounds for PA-DP mean estimation, which are of a similar form. Up
to constant factors, these lower bounds show that the simple strategy of either
treating all data as private or discarding the private data, is optimal. We
also study PA-DP supervised learning with \textit{unlabeled} public samples. In
contrast to our previous result, we here show novel methods for leveraging
public data in private supervised learning. For generalized linear models (GLM)
with unlabeled public data, we show an efficient algorithm which, given
$\tilde{O}({n_{\text{priv}}}\epsilon)$ unlabeled public samples, achieves the
dimension independent rate $\tilde{O}\big(\frac{1}{\sqrt{{n_{\text{priv}}}}} +
\frac{1}{\sqrt{{n_{\text{priv}}}\epsilon}}\big)$. We develop new lower bounds
for this setting which shows that this rate cannot be improved with more public
samples, and any fewer public samples leads to a worse rate. Finally, we
provide extensions of this result to general hypothesis classes with finite
fat-shattering dimension with applications to neural networks and non-Euclidean
geometries.</div><div><a href='http://arxiv.org/abs/2403.03856v1'>2403.03856v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19016v1")'>SPriFed-OMP: A Differentially Private Federated Learning Algorithm for
  Sparse Basis Recovery</div>
<div id='2402.19016v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T10:23:23Z</div><div>Authors: Ajinkya Kiran Mulay, Xiaojun Lin</div><div style='padding-top: 10px; width: 80ex'>Sparse basis recovery is a classical and important statistical learning
problem when the number of model dimensions $p$ is much larger than the number
of samples $n$. However, there has been little work that studies sparse basis
recovery in the Federated Learning (FL) setting, where the client data's
differential privacy (DP) must also be simultaneously protected. In particular,
the performance guarantees of existing DP-FL algorithms (such as DP-SGD) will
degrade significantly when $p \gg n$, and thus, they will fail to learn the
true underlying sparse model accurately. In this work, we develop a new
differentially private sparse basis recovery algorithm for the FL setting,
called SPriFed-OMP. SPriFed-OMP converts OMP (Orthogonal Matching Pursuit) to
the FL setting. Further, it combines SMPC (secure multi-party computation) and
DP to ensure that only a small amount of noise needs to be added in order to
achieve differential privacy. As a result, SPriFed-OMP can efficiently recover
the true sparse basis for a linear model with only $n = O(\sqrt{p})$ samples.
We further present an enhanced version of our approach, SPriFed-OMP-GRAD based
on gradient privatization, that improves the performance of SPriFed-OMP. Our
theoretical analysis and empirical results demonstrate that both SPriFed-OMP
and SPriFed-OMP-GRAD terminate in a small number of steps, and they
significantly outperform the previous state-of-the-art DP-FL solutions in terms
of the accuracy-privacy trade-off.</div><div><a href='http://arxiv.org/abs/2402.19016v1'>2402.19016v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13531v1")'>Private Gradient Descent for Linear Regression: Tighter Error Bounds and
  Instance-Specific Uncertainty Estimation</div>
<div id='2402.13531v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T04:58:41Z</div><div>Authors: Gavin Brown, Krishnamurthy Dvijotham, Georgina Evans, Daogao Liu, Adam Smith, Abhradeep Thakurta</div><div style='padding-top: 10px; width: 80ex'>We provide an improved analysis of standard differentially private gradient
descent for linear regression under the squared error loss. Under modest
assumptions on the input, we characterize the distribution of the iterate at
each time step.
  Our analysis leads to new results on the algorithm's accuracy: for a proper
fixed choice of hyperparameters, the sample complexity depends only linearly on
the dimension of the data. This matches the dimension-dependence of the
(non-private) ordinary least squares estimator as well as that of recent
private algorithms that rely on sophisticated adaptive gradient-clipping
schemes (Varshney et al., 2022; Liu et al., 2023).
  Our analysis of the iterates' distribution also allows us to construct
confidence intervals for the empirical optimizer which adapt automatically to
the variance of the algorithm on a particular data set. We validate our
theorems through experiments on synthetic data.</div><div><a href='http://arxiv.org/abs/2402.13531v1'>2402.13531v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11173v1")'>How to Make the Gradients Small Privately: Improved Rates for
  Differentially Private Non-Convex Optimization</div>
<div id='2402.11173v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T02:42:56Z</div><div>Authors: Andrew Lowy, Jonathan Ullman, Stephen J. Wright</div><div style='padding-top: 10px; width: 80ex'>We provide a simple and flexible framework for designing differentially
private algorithms to find approximate stationary points of non-convex loss
functions. Our framework is based on using a private approximate risk minimizer
to "warm start" another private algorithm for finding stationary points. We use
this framework to obtain improved, and sometimes optimal, rates for several
classes of non-convex loss functions. First, we obtain improved rates for
finding stationary points of smooth non-convex empirical loss functions.
Second, we specialize to quasar-convex functions, which generalize star-convex
functions and arise in learning dynamical systems and training some neural
nets. We achieve the optimal rate for this class. Third, we give an optimal
algorithm for finding stationary points of functions satisfying the
Kurdyka-Lojasiewicz (KL) condition. For example, over-parameterized neural
networks often satisfy this condition. Fourth, we provide new state-of-the-art
rates for stationary points of non-convex population loss functions. Fifth, we
obtain improved rates for non-convex generalized linear models. A modification
of our algorithm achieves nearly the same rates for second-order stationary
points of functions with Lipschitz Hessian, improving over the previous
state-of-the-art for each of the above problems.</div><div><a href='http://arxiv.org/abs/2402.11173v1'>2402.11173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00278v1")'>Shifted Interpolation for Differential Privacy</div>
<div id='2403.00278v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T04:50:04Z</div><div>Authors: Jinho Bok, Weijie Su, Jason M. Altschuler</div><div style='padding-top: 10px; width: 80ex'>Noisy gradient descent and its variants are the predominant algorithms for
differentially private machine learning. It is a fundamental question to
quantify their privacy leakage, yet tight characterizations remain open even in
the foundational setting of convex losses. This paper improves over previous
analyses by establishing (and refining) the "privacy amplification by
iteration" phenomenon in the unifying framework of $f$-differential
privacy--which tightly captures all aspects of the privacy loss and immediately
implies tighter privacy accounting in other notions of differential privacy,
e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the
construction of shifted interpolated processes that unravel the popular
shifted-divergences argument, enabling generalizations beyond divergence-based
relaxations of DP. Notably, this leads to the first exact privacy analysis in
the foundational setting of strongly convex optimization. Our techniques extend
to many settings: convex/strongly convex, constrained/unconstrained,
full/cyclic/stochastic batches, and all combinations thereof. As an immediate
corollary, we recover the $f$-DP characterization of the exponential mechanism
for strongly convex optimization in Gopi et al. (2022), and moreover extend
this result to more general settings.</div><div><a href='http://arxiv.org/abs/2403.00278v1'>2403.00278v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19437v1")'>Differentially Private Worst-group Risk Minimization</div>
<div id='2402.19437v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:38:20Z</div><div>Authors: Xinyu Zhou, Raef Bassily</div><div style='padding-top: 10px; width: 80ex'>We initiate a systematic study of worst-group risk minimization under
$(\epsilon, \delta)$-differential privacy (DP). The goal is to privately find a
model that approximately minimizes the maximal risk across $p$ sub-populations
(groups) with different distributions, where each group distribution is
accessed via a sample oracle. We first present a new algorithm that achieves
excess worst-group population risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} +
\sqrt{\frac{p}{K}})$, where $K$ is the total number of samples drawn from all
groups and $d$ is the problem dimension. Our rate is nearly optimal when each
distribution is observed via a fixed-size dataset of size $K/p$. Our result is
based on a new stability-based analysis for the generalization error. In
particular, we show that $\Delta$-uniform argument stability implies
$\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$ generalization error w.r.t. the
worst-group risk, where $n$ is the number of samples drawn from each sample
oracle. Next, we propose an algorithmic framework for worst-group population
risk minimization using any DP online convex optimization algorithm as a
subroutine. Hence, we give another excess risk bound of $\tilde{O}\left(
\sqrt{\frac{d^{1/2}}{\epsilon K}} +\sqrt{\frac{p}{K\epsilon^2}} \right)$.
Assuming the typical setting of $\epsilon=\Theta(1)$, this bound is more
favorable than our first bound in a certain range of $p$ as a function of $K$
and $d$. Finally, we study differentially private worst-group empirical risk
minimization in the offline setting, where each group distribution is observed
by a fixed-size dataset. We present a new algorithm with nearly optimal excess
risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon})$.</div><div><a href='http://arxiv.org/abs/2402.19437v1'>2402.19437v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16439v2")'>Distribution-Specific Auditing For Subgroup Fairness</div>
<div id='2401.16439v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T06:38:31Z</div><div>Authors: Daniel Hsu, Jizhou Huang, Brendan Juba</div><div style='padding-top: 10px; width: 80ex'>We study the problem of auditing classifiers with the notion of statistical
subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing
combinatorial subgroups fairness is as hard as agnostic learning. Essentially
all work on remedying statistical measures of discrimination against subgroups
assumes access to an oracle for this problem, despite the fact that no
efficient algorithms are known for it. If we assume the data distribution is
Gaussian, or even merely log-concave, then a recent line of work has discovered
efficient agnostic learning algorithms for halfspaces. Unfortunately, the
reduction of Kearns et al. was formulated in terms of weak, "distribution-free"
learning, and thus did not establish a connection for families such as
log-concave distributions.
  In this work, we give positive and negative results on auditing for Gaussian
distributions: On the positive side, we present an alternative approach to
leverage these advances in agnostic learning and thereby obtain the first
polynomial-time approximation scheme (PTAS) for auditing nontrivial
combinatorial subgroup fairness: we show how to audit statistical notions of
fairness over homogeneous halfspace subgroups when the features are Gaussian.
On the negative side, we find that under cryptographic assumptions, no
polynomial-time algorithm can guarantee any nontrivial auditing, even under
Gaussian feature distributions, for general halfspace subgroups.</div><div><a href='http://arxiv.org/abs/2401.16439v2'>2401.16439v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16778v1")'>On the Growth of Mistakes in Differentially Private Online Learning: A
  Lower Bound Perspective</div>
<div id='2402.16778v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T17:49:37Z</div><div>Authors: Daniil Dmitriev, Krist√≥f Szab√≥, Amartya Sanyal</div><div style='padding-top: 10px; width: 80ex'>In this paper, we provide lower bounds for Differentially Private (DP) Online
Learning algorithms. Our result shows that, for a broad class of
$(\varepsilon,\delta)$-DP online algorithms, for $T$ such that $\log T\leq O(1
/ \delta)$, the expected number of mistakes incurred by the algorithm grows as
$\Omega(\log \frac{T}{\delta})$. This matches the upper bound obtained by
Golowich and Livni (2021) and is in contrast to non-private online learning
where the number of mistakes is independent of $T$. To the best of our
knowledge, our work is the first result towards settling lower bounds for
DP-Online learning and partially addresses the open question in Sanyal and
Ramponi (2022).</div><div><a href='http://arxiv.org/abs/2402.16778v1'>2402.16778v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15246v1")'>Training Differentially Private Ad Prediction Models with Semi-Sensitive
  Features</div>
<div id='2401.15246v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T23:41:28Z</div><div>Authors: Lynn Chua, Qiliang Cui, Badih Ghazi, Charlie Harrison, Pritish Kamath, Walid Krichene, Ravi Kumar, Pasin Manurangsi, Krishna Giri Narra, Amer Sinha, Avinash Varadarajan, Chiyuan Zhang</div><div style='padding-top: 10px; width: 80ex'>Motivated by problems arising in digital advertising, we introduce the task
of training differentially private (DP) machine learning models with
semi-sensitive features. In this setting, a subset of the features is known to
the attacker (and thus need not be protected) while the remaining features as
well as the label are unknown to the attacker and should be protected by the DP
guarantee. This task interpolates between training the model with full DP
(where the label and all features should be protected) or with label DP (where
all the features are considered known, and only the label should be protected).
We present a new algorithm for training DP models with semi-sensitive features.
Through an empirical evaluation on real ads datasets, we demonstrate that our
algorithm surpasses in utility the baselines of (i) DP stochastic gradient
descent (DP-SGD) run on all features (known and unknown), and (ii) a label DP
algorithm run only on the known features (while discarding the unknown ones).</div><div><a href='http://arxiv.org/abs/2401.15246v1'>2401.15246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13041v1")'>Provable Privacy with Non-Private Pre-Processing</div>
<div id='2403.13041v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:54:49Z</div><div>Authors: Yaxi Hu, Amartya Sanyal, Bernhard Sch√∂lkopf</div><div style='padding-top: 10px; width: 80ex'>When analysing Differentially Private (DP) machine learning pipelines, the
potential privacy cost of data-dependent pre-processing is frequently
overlooked in privacy accounting. In this work, we propose a general framework
to evaluate the additional privacy cost incurred by non-private data-dependent
pre-processing algorithms. Our framework establishes upper bounds on the
overall privacy guarantees by utilising two new technical notions: a variant of
DP termed Smooth DP and the bounded sensitivity of the pre-processing
algorithms. In addition to the generic framework, we provide explicit overall
privacy guarantees for multiple data-dependent pre-processing algorithms, such
as data imputation, quantization, deduplication and PCA, when used in
combination with several DP algorithms. Notably, this framework is also simple
to implement, allowing direct integration into existing DP pipelines.</div><div><a href='http://arxiv.org/abs/2403.13041v1'>2403.13041v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14397v1")'>Closed-Form Bounds for DP-SGD against Record-level Inference</div>
<div id='2402.14397v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:26:16Z</div><div>Authors: Giovanni Cherubin, Boris K√∂pf, Andrew Paverd, Shruti Tople, Lukas Wutschitz, Santiago Zanella-B√©guelin</div><div style='padding-top: 10px; width: 80ex'>Machine learning models trained with differentially-private (DP) algorithms
such as DP-SGD enjoy resilience against a wide range of privacy attacks.
Although it is possible to derive bounds for some attacks based solely on an
$(\varepsilon,\delta)$-DP guarantee, meaningful bounds require a small enough
privacy budget (i.e., injecting a large amount of noise), which results in a
large loss in utility. This paper presents a new approach to evaluate the
privacy of machine learning models against specific record-level threats, such
as membership and attribute inference, without the indirection through DP. We
focus on the popular DP-SGD algorithm, and derive simple closed-form bounds.
Our proofs model DP-SGD as an information theoretic channel whose inputs are
the secrets that an attacker wants to infer (e.g., membership of a data record)
and whose outputs are the intermediate model parameters produced by iterative
optimization. We obtain bounds for membership inference that match
state-of-the-art techniques, whilst being orders of magnitude faster to
compute. Additionally, we present a novel data-dependent bound against
attribute inference. Our results provide a direct, interpretable, and practical
way to evaluate the privacy of trained models against specific inference
threats without sacrificing utility.</div><div><a href='http://arxiv.org/abs/2402.14397v1'>2402.14397v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09540v1")'>Why Does Differential Privacy with Large Epsilon Defend Against
  Practical Membership Inference Attacks?</div>
<div id='2402.09540v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T19:31:45Z</div><div>Authors: Andrew Lowy, Zhuohang Li, Jing Liu, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang</div><div style='padding-top: 10px; width: 80ex'>For small privacy parameter $\epsilon$, $\epsilon$-differential privacy (DP)
provides a strong worst-case guarantee that no membership inference attack
(MIA) can succeed at determining whether a person's data was used to train a
machine learning model. The guarantee of DP is worst-case because: a) it holds
even if the attacker already knows the records of all but one person in the
data set; and b) it holds uniformly over all data sets. In practical
applications, such a worst-case guarantee may be overkill: practical attackers
may lack exact knowledge of (nearly all of) the private data, and our data set
might be easier to defend, in some sense, than the worst-case data set. Such
considerations have motivated the industrial deployment of DP models with large
privacy parameter (e.g. $\epsilon \geq 7$), and it has been observed
empirically that DP with large $\epsilon$ can successfully defend against
state-of-the-art MIAs. Existing DP theory cannot explain these empirical
findings: e.g., the theoretical privacy guarantees of $\epsilon \geq 7$ are
essentially vacuous. In this paper, we aim to close this gap between theory and
practice and understand why a large DP parameter can prevent practical MIAs. To
tackle this problem, we propose a new privacy notion called practical
membership privacy (PMP). PMP models a practical attacker's uncertainty about
the contents of the private data. The PMP parameter has a natural
interpretation in terms of the success rate of a practical MIA on a given data
set. We quantitatively analyze the PMP parameter of two fundamental DP
mechanisms: the exponential mechanism and Gaussian mechanism. Our analysis
reveals that a large DP parameter often translates into a much smaller PMP
parameter, which guarantees strong privacy against practical MIAs. Using our
findings, we offer principled guidance for practitioners in choosing the DP
parameter.</div><div><a href='http://arxiv.org/abs/2402.09540v1'>2402.09540v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04867v1")'>Group Privacy Amplification and Unified Amplification by Subsampling for
  R√©nyi Differential Privacy</div>
<div id='2403.04867v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T19:36:05Z</div><div>Authors: Jan Schuchardt, Mihail Stoian, Arthur Kosmala, Stephan G√ºnnemann</div><div style='padding-top: 10px; width: 80ex'>Differential privacy (DP) has various desirable properties, such as
robustness to post-processing, group privacy, and amplification by subsampling,
which can be derived independently of each other. Our goal is to determine
whether stronger privacy guarantees can be obtained by considering multiple of
these properties jointly. To this end, we focus on the combination of group
privacy and amplification by subsampling. To provide guarantees that are
amenable to machine learning algorithms, we conduct our analysis in the
framework of R\'enyi-DP, which has more favorable composition properties than
$(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified
framework for deriving amplification by subsampling guarantees for R\'enyi-DP,
which represents the first such framework for a privacy accounting method and
is of independent interest. We find that it not only lets us improve upon and
generalize existing amplification results for R\'enyi-DP, but also derive
provably tight group privacy amplification guarantees stronger than existing
principles. These results establish the joint study of different DP properties
as a promising research direction.</div><div><a href='http://arxiv.org/abs/2403.04867v1'>2403.04867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07131v1")'>Resampling methods for Private Statistical Inference</div>
<div id='2402.07131v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T08:59:02Z</div><div>Authors: Karan Chadha, John Duchi, Rohith Kuditipudi</div><div style='padding-top: 10px; width: 80ex'>We consider the task of constructing confidence intervals with differential
privacy. We propose two private variants of the non-parametric bootstrap, which
privately compute the median of the results of multiple ``little'' bootstraps
run on partitions of the data and give asymptotic bounds on the coverage error
of the resulting confidence intervals. For a fixed differential privacy
parameter $\epsilon$, our methods enjoy the same error rates as that of the
non-private bootstrap to within logarithmic factors in the sample size $n$. We
empirically validate the performance of our methods for mean estimation, median
estimation, and logistic regression with both real and synthetic data. Our
methods achieve similar coverage accuracy to existing methods (and non-private
baselines) while providing notably shorter ($\gtrsim 10$ times) confidence
intervals than previous approaches.</div><div><a href='http://arxiv.org/abs/2402.07131v1'>2402.07131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15502v1")'>Differentially Private Bayesian Tests</div>
<div id='2401.15502v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T21:07:11Z</div><div>Authors: Abhisek Chakraborty, Saptati Datta</div><div style='padding-top: 10px; width: 80ex'>Differential privacy has emerged as an significant cornerstone in the realm
of scientific hypothesis testing utilizing confidential data. In reporting
scientific discoveries, Bayesian tests are widely adopted since they
effectively circumnavigate the key criticisms of P-values, namely, lack of
interpretability and inability to quantify evidence in support of the competing
hypotheses. We present a novel differentially private Bayesian hypotheses
testing framework that arise naturally under a principled data generative
mechanism, inherently maintaining the interpretability of the resulting
inferences. Furthermore, by focusing on differentially private Bayes factors
based on widely used test statistics, we circumvent the need to model the
complete data generative mechanism and ensure substantial computational
benefits. We also provide a set of sufficient conditions to establish results
on Bayes factor consistency under the proposed framework. The utility of the
devised technology is showcased via several numerical experiments.</div><div><a href='http://arxiv.org/abs/2401.15502v1'>2401.15502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07066v1")'>Differentially Private Range Queries with Correlated Input Perturbation</div>
<div id='2402.07066v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T23:42:05Z</div><div>Authors: Prathamesh Dharangutte, Jie Gao, Ruobin Gong, Guanyang Wang</div><div style='padding-top: 10px; width: 80ex'>This work proposes a class of locally differentially private mechanisms for
linear queries, in particular range queries, that leverages correlated input
perturbation to simultaneously achieve unbiasedness, consistency, statistical
transparency, and control over utility requirements in terms of accuracy
targets expressed either in certain query margins or as implied by the
hierarchical database structure. The proposed Cascade Sampling algorithm
instantiates the mechanism exactly and efficiently. Our bounds show that we
obtain near-optimal utility while being empirically competitive against output
perturbation methods.</div><div><a href='http://arxiv.org/abs/2402.07066v1'>2402.07066v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06273v1")'>Qrlew: Rewriting SQL into Differentially Private SQL</div>
<div id='2401.06273v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T22:01:35Z</div><div>Authors: Nicolas Grislain, Paul Roussel, Victoria de Sainte Agathe</div><div style='padding-top: 10px; width: 80ex'>This paper introduces Qrlew, an open source library that can parse SQL
queries into Relations -- an intermediate representation -- that keeps track of
rich data types, value ranges, and row ownership; so that they can easily be
rewritten into differentially-private equivalent and turned back into SQL
queries for execution in a variety of standard data stores.
  With Qrlew, a data practitioner can express their data queries in standard
SQL; the data owner can run the rewritten query without any technical
integration and with strong privacy guarantees on the output; and the query
rewriting can be operated by a privacy-expert who must be trusted by the owner,
but may belong to a separate organization.</div><div><a href='http://arxiv.org/abs/2401.06273v1'>2401.06273v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14577v2")'>An Algorithm for Streaming Differentially Private Data</div>
<div id='2401.14577v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T00:32:31Z</div><div>Authors: Girish Kumar, Thomas Strohmer, Roman Vershynin</div><div style='padding-top: 10px; width: 80ex'>Much of the research in differential privacy has focused on offline
applications with the assumption that all data is available at once. When these
algorithms are applied in practice to streams where data is collected over
time, this either violates the privacy guarantees or results in poor utility.
We derive an algorithm for differentially private synthetic streaming data
generation, especially curated towards spatial datasets. Furthermore, we
provide a general framework for online selective counting among a collection of
queries which forms a basis for many tasks such as query answering and
synthetic data generation. The utility of our algorithm is verified on both
real-world and simulated datasets.</div><div><a href='http://arxiv.org/abs/2401.14577v2'>2401.14577v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07218v1")'>SoK: Can Trajectory Generation Combine Privacy and Utility?</div>
<div id='2403.07218v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T00:25:14Z</div><div>Authors: Erik Buchholz, Alsharif Abuadbba, Shuo Wang, Surya Nepal, Salil S. Kanhere</div><div style='padding-top: 10px; width: 80ex'>While location trajectories represent a valuable data source for analyses and
location-based services, they can reveal sensitive information, such as
political and religious preferences. Differentially private publication
mechanisms have been proposed to allow for analyses under rigorous privacy
guarantees. However, the traditional protection schemes suffer from a limiting
privacy-utility trade-off and are vulnerable to correlation and reconstruction
attacks. Synthetic trajectory data generation and release represent a promising
alternative to protection algorithms. While initial proposals achieve
remarkable utility, they fail to provide rigorous privacy guarantees. This
paper proposes a framework for designing a privacy-preserving trajectory
publication approach by defining five design goals, particularly stressing the
importance of choosing an appropriate Unit of Privacy. Based on this framework,
we briefly discuss the existing trajectory protection approaches, emphasising
their shortcomings. This work focuses on the systematisation of the
state-of-the-art generative models for trajectories in the context of the
proposed framework. We find that no existing solution satisfies all
requirements. Thus, we perform an experimental study evaluating the
applicability of six sequential generative models to the trajectory domain.
Finally, we conclude that a generative trajectory model providing semantic
guarantees remains an open research question and propose concrete next steps
for future research.</div><div><a href='http://arxiv.org/abs/2403.07218v1'>2403.07218v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17823v2")'>Privacy-preserving data release leveraging optimal transport and
  particle gradient descent</div>
<div id='2401.17823v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T13:28:07Z</div><div>Authors: Konstantin Donhauser, Javier Abad, Neha Hulkund, Fanny Yang</div><div style='padding-top: 10px; width: 80ex'>We present a novel approach for differentially private data synthesis of
protected tabular datasets, a relevant task in highly sensitive domains such as
healthcare and government. Current state-of-the-art methods predominantly use
marginal-based approaches, where a dataset is generated from private estimates
of the marginals. In this paper, we introduce PrivPGD, a new generation method
for marginal-based private data synthesis, leveraging tools from optimal
transport and particle gradient descent. Our algorithm outperforms existing
methods on a large range of datasets while being highly scalable and offering
the flexibility to incorporate additional domain-specific constraints.</div><div><a href='http://arxiv.org/abs/2401.17823v2'>2401.17823v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05598v1")'>Privacy Amplification for the Gaussian Mechanism via Bounded Support</div>
<div id='2403.05598v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T21:22:07Z</div><div>Authors: Shengyuan Hu, Saeed Mahloujifar, Virginia Smith, Kamalika Chaudhuri, Chuan Guo</div><div style='padding-top: 10px; width: 80ex'>Data-dependent privacy accounting frameworks such as per-instance
differential privacy (pDP) and Fisher information loss (FIL) confer
fine-grained privacy guarantees for individuals in a fixed training dataset.
These guarantees can be desirable compared to vanilla DP in real world settings
as they tightly upper-bound the privacy leakage for a $\textit{specific}$
individual in an $\textit{actual}$ dataset, rather than considering worst-case
datasets. While these frameworks are beginning to gain popularity, to date,
there is a lack of private mechanisms that can fully leverage advantages of
data-dependent accounting. To bridge this gap, we propose simple modifications
of the Gaussian mechanism with bounded support, showing that they amplify
privacy guarantees under data-dependent accounting. Experiments on model
training with DP-SGD show that using bounded support Gaussian mechanisms can
provide a reduction of the pDP bound $\epsilon$ by as much as 30% without
negative effects on model utility.</div><div><a href='http://arxiv.org/abs/2403.05598v1'>2403.05598v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17127v1")'>Personalized Differential Privacy for Ridge Regression</div>
<div id='2401.17127v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T16:00:14Z</div><div>Authors: Krishna Acharya, Franziska Boenisch, Rakshit Naidu, Juba Ziani</div><div style='padding-top: 10px; width: 80ex'>The increased application of machine learning (ML) in sensitive domains
requires protecting the training data through privacy frameworks, such as
differential privacy (DP). DP requires to specify a uniform privacy level
$\varepsilon$ that expresses the maximum privacy loss that each data point in
the entire dataset is willing to tolerate. Yet, in practice, different data
points often have different privacy requirements. Having to set one uniform
privacy level is usually too restrictive, often forcing a learner to guarantee
the stringent privacy requirement, at a large cost to accuracy. To overcome
this limitation, we introduce our novel Personalized-DP Output Perturbation
method (PDP-OP) that enables to train Ridge regression models with individual
per data point privacy levels. We provide rigorous privacy proofs for our
PDP-OP as well as accuracy guarantees for the resulting model. This work is the
first to provide such theoretical accuracy guarantees when it comes to
personalized DP in machine learning, whereas previous work only provided
empirical evaluations. We empirically evaluate PDP-OP on synthetic and real
datasets and with diverse privacy distributions. We show that by enabling each
data point to specify their own privacy requirement, we can significantly
improve the privacy-accuracy trade-offs in DP. We also show that PDP-OP
outperforms the personalized privacy techniques of Jorgensen et al. (2015).</div><div><a href='http://arxiv.org/abs/2401.17127v1'>2401.17127v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04778v2")'>An Efficient Difference-of-Convex Solver for Privacy Funnel</div>
<div id='2403.04778v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T01:05:25Z</div><div>Authors: Teng-Hui Huang, Hesham El Gamal</div><div style='padding-top: 10px; width: 80ex'>We propose an efficient solver for the privacy funnel (PF) method, leveraging
its difference-of-convex (DC) structure. The proposed DC separation results in
a closed-form update equation, which allows straightforward application to both
known and unknown distribution settings. For known distribution case, we prove
the convergence (local stationary points) of the proposed non-greedy solver,
and empirically show that it outperforms the state-of-the-art approaches in
characterizing the privacy-utility trade-off. The insights of our DC approach
apply to unknown distribution settings where labeled empirical samples are
available instead. Leveraging the insights, our alternating minimization solver
satisfies the fundamental Markov relation of PF in contrast to previous
variational inference-based solvers. Empirically, we evaluate the proposed
solver with MNIST and Fashion-MNIST datasets. Our results show that under a
comparable reconstruction quality, an adversary suffers from higher prediction
error from clustering our compressed codes than that with the compared methods.
Most importantly, our solver is independent to private information in inference
phase contrary to the baselines.</div><div><a href='http://arxiv.org/abs/2403.04778v2'>2403.04778v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16926v1")'>On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing
  Problem</div>
<div id='2402.16926v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T11:43:01Z</div><div>Authors: Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg</div><div style='padding-top: 10px; width: 80ex'>We introduce a formal statistical definition for the problem of backdoor
detection in machine learning systems and use it to analyze the feasibility of
such problems, providing evidence for the utility and applicability of our
definition. The main contributions of this work are an impossibility result and
an achievability result for backdoor detection. We show a no-free-lunch
theorem, proving that universal (adversary-unaware) backdoor detection is
impossible, except for very small alphabet sizes. Thus, we argue, that backdoor
detection methods need to be either explicitly, or implicitly adversary-aware.
However, our work does not imply that backdoor detection cannot work in
specific scenarios, as evidenced by successful backdoor detection methods in
the scientific literature. Furthermore, we connect our definition to the
probably approximately correct (PAC) learnability of the out-of-distribution
detection problem.</div><div><a href='http://arxiv.org/abs/2402.16926v1'>2402.16926v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11119v1")'>Private PAC Learning May be Harder than Online Learning</div>
<div id='2402.11119v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T22:44:52Z</div><div>Authors: Mark Bun, Aloni Cohen, Rathin Desai</div><div style='padding-top: 10px; width: 80ex'>We continue the study of the computational complexity of differentially
private PAC learning and how it is situated within the foundations of machine
learning. A recent line of work uncovered a qualitative equivalence between the
private PAC model and Littlestone's mistake-bounded model of online learning,
in particular, showing that any concept class of Littlestone dimension $d$ can
be privately PAC learned using $\mathrm{poly}(d)$ samples. This raises the
natural question of whether there might be a generic conversion from online
learners to private PAC learners that also preserves computational efficiency.
  We give a negative answer to this question under reasonable cryptographic
assumptions (roughly, those from which it is possible to build
indistinguishability obfuscation for all circuits). We exhibit a concept class
that admits an online learner running in polynomial time with a polynomial
mistake bound, but for which there is no computationally-efficient
differentially private PAC learner. Our construction and analysis strengthens
and generalizes that of Bun and Zhandry (TCC 2016-A), who established such a
separation between private and non-private PAC learner.</div><div><a href='http://arxiv.org/abs/2402.11119v1'>2402.11119v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13848v1")'>Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule
  Lists</div>
<div id='2403.13848v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T10:44:22Z</div><div>Authors: Timoth√©e Ly, Julien Ferry, Marie-Jos√© Huguet, S√©bastien Gambs, Ulrich Aivodji</div><div style='padding-top: 10px; width: 80ex'>Differentially-private (DP) mechanisms can be embedded into the design of a
machine learningalgorithm to protect the resulting model against privacy
leakage, although this often comes with asignificant loss of accuracy. In this
paper, we aim at improving this trade-off for rule lists modelsby establishing
the smooth sensitivity of the Gini impurity and leveraging it to propose a DP
greedyrule list algorithm. In particular, our theoretical analysis and
experimental results demonstrate thatthe DP rule lists models integrating
smooth sensitivity have higher accuracy that those using otherDP frameworks
based on global sensitivity.</div><div><a href='http://arxiv.org/abs/2403.13848v1'>2403.13848v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13087v1")'>How Does Selection Leak Privacy: Revisiting Private Selection and
  Improved Results for Hyper-parameter Tuning</div>
<div id='2402.13087v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:29:49Z</div><div>Authors: Zihang Xiang, Chenglong Wang, Di Wang</div><div style='padding-top: 10px; width: 80ex'>We study the problem of guaranteeing Differential Privacy (DP) in
hyper-parameter tuning, a crucial process in machine learning involving the
selection of the best run from several. Unlike many private algorithms,
including the prevalent DP-SGD, the privacy implications of tuning remain
insufficiently understood. Recent works propose a generic private solution for
the tuning process, yet a fundamental question still persists: is the current
privacy bound for this solution tight?
  This paper contributes both positive and negative answers to this question.
Initially, we provide studies affirming the current privacy analysis is indeed
tight in a general sense. However, when we specifically study the
hyper-parameter tuning problem, such tightness no longer holds. This is first
demonstrated by applying privacy audit on the tuning process. Our findings
underscore a substantial gap between the current theoretical privacy bound and
the empirical bound derived even under the strongest audit setup.
  The gap found is not a fluke. Our subsequent study provides an improved
privacy result for private hyper-parameter tuning due to its distinct
properties. Our privacy results are also more generalizable compared to prior
analyses that are only easily applicable in specific setups.</div><div><a href='http://arxiv.org/abs/2402.13087v1'>2402.13087v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03295v1")'>Proper vs Improper Quantum PAC learning</div>
<div id='2403.03295v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T19:49:44Z</div><div>Authors: Ashwin Nayak, Pulkit Sinha</div><div style='padding-top: 10px; width: 80ex'>A basic question in the PAC model of learning is whether proper learning is
harder than improper learning. In the classical case, there are examples of
concept classes with VC dimension $d$ that have sample complexity
$\Omega\left(\frac d\epsilon\log\frac1\epsilon\right)$ for proper learning with
error $\epsilon$, while the complexity for improper learning is O$\!\left(\frac
d\epsilon\right)$. One such example arises from the Coupon Collector problem.
  Motivated by the efficiency of proper versus improper learning with quantum
samples, Arunachalam, Belovs, Childs, Kothari, Rosmanis, and de Wolf (TQC 2020)
studied an analogue, the Quantum Coupon Collector problem. Curiously, they
discovered that for learning size $k$ subsets of $[n]$ the problem has sample
complexity $\Theta(k\log\min\{k,n-k+1\})$, in contrast with the complexity of
$\Theta(k\log k)$ for Coupon Collector. This effectively negates the
possibility of a separation between the two modes of learning via the quantum
problem, and Arunachalam et al.\ posed the possibility of such a separation as
an open question.
  In this work, we first present an algorithm for the Quantum Coupon Collector
problem with sample complexity that matches the sharper lower bound of
$(1-o_k(1))k\ln\min\{k,n-k+1\}$ shown recently by Bab Hadiashar, Nayak, and
Sinha (IEEE TIT 2024), for the entire range of the parameter $k$. Next, we
devise a variant of the problem, the Quantum Padded Coupon Collector. We prove
that its sample complexity matches that of the classical Coupon Collector
problem for both modes of learning, thereby exhibiting the same asymptotic
separation between proper and improper quantum learning as mentioned above. The
techniques we develop in the process can be directly applied to any form of
padded quantum data. We hope that padding can more generally lift other forms
of classical learning behaviour to the quantum setting.</div><div><a href='http://arxiv.org/abs/2403.03295v1'>2403.03295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06230v1")'>LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding
  Linear Bandit Problem</div>
<div id='2403.06230v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T15:01:50Z</div><div>Authors: Yun-Ang Wu, Yun-Da Tsai, Shou-De Lin</div><div style='padding-top: 10px; width: 80ex'>In this study, we delve into the Thresholding Linear Bandit (TLB) problem, a
nuanced domain within stochastic Multi-Armed Bandit (MAB) problems, focusing on
maximizing decision accuracy against a linearly defined threshold under
resource constraints. We present LinearAPT, a novel algorithm designed for the
fixed budget setting of TLB, providing an efficient solution to optimize
sequential decision-making. This algorithm not only offers a theoretical upper
bound for estimated loss but also showcases robust performance on both
synthetic and real-world datasets. Our contributions highlight the
adaptability, simplicity, and computational efficiency of LinearAPT, making it
a valuable addition to the toolkit for addressing complex sequential
decision-making challenges.</div><div><a href='http://arxiv.org/abs/2403.06230v1'>2403.06230v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15073v1")'>Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation</div>
<div id='2402.15073v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T03:27:17Z</div><div>Authors: Duy Nguyen, Bao Nguyen, Viet Anh Nguyen</div><div style='padding-top: 10px; width: 80ex'>Algorithmic recourse recommends a cost-efficient action to a subject to
reverse an unfavorable machine learning classification decision. Most existing
methods in the literature generate recourse under the assumption of complete
knowledge about the cost function. In real-world practice, subjects could have
distinct preferences, leading to incomplete information about the underlying
cost function of the subject. This paper proposes a two-step approach
integrating preference learning into the recourse generation problem. In the
first step, we design a question-answering framework to refine the confidence
set of the Mahalanobis matrix cost of the subject sequentially. Then, we
generate recourse by utilizing two methods: gradient-based and graph-based
cost-adaptive recourse that ensures validity while considering the whole
confidence set of the cost matrix. The numerical evaluation demonstrates the
benefits of our approach over state-of-the-art baselines in delivering
cost-efficient recourse recommendations.</div><div><a href='http://arxiv.org/abs/2402.15073v1'>2402.15073v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09152v1")'>Improved Regret for Bandit Convex Optimization with Delayed Feedback</div>
<div id='2402.09152v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:08:26Z</div><div>Authors: Yuanyu Wan, Chang Yao, Mingli Song, Lijun Zhang</div><div style='padding-top: 10px; width: 80ex'>We investigate bandit convex optimization (BCO) with delayed feedback, where
only the loss value of the action is revealed under an arbitrary delay.
Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$
for this problem, where $d$ is the maximum delay, by simply feeding delayed
loss values to the classical bandit gradient descent (BGD) algorithm. In this
paper, we develop a novel algorithm to enhance the regret, which carefully
exploits the delayed bandit feedback via a blocking update mechanism. Our
analysis first reveals that the proposed algorithm can decouple the joint
effect of the delays and bandit feedback on the regret, and improve the regret
bound to $O(T^{3/4}+\sqrt{dT})$ for convex functions. Compared with the
previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the
non-delayed setting for a larger amount of delay, i.e., $d=O(\sqrt{T})$,
instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly
convex functions, and prove that the proposed algorithm can enjoy a better
regret bound of $O(T^{2/3}\log^{1/3}T+d\log T)$. Finally, we show that in a
special case with unconstrained action sets, it can be simply extended to
achieve a regret bound of $O(\sqrt{T\log T}+d\log T)$ for strongly convex and
smooth functions.</div><div><a href='http://arxiv.org/abs/2402.09152v1'>2402.09152v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07703v3")'>Online Sequential Decision-Making with Unknown Delays</div>
<div id='2402.07703v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:17:31Z</div><div>Authors: Ping Wu, Heyan Huang, Zhengyang Liu</div><div style='padding-top: 10px; width: 80ex'>In the field of online sequential decision-making, we address the problem
with delays utilizing the framework of online convex optimization (OCO), where
the feedback of a decision can arrive with an unknown delay. Unlike previous
research that is limited to Euclidean norm and gradient information, we propose
three families of delayed algorithms based on approximate solutions to handle
different types of received feedback. Our proposed algorithms are versatile and
applicable to universal norms. Specifically, we introduce a family of Follow
the Delayed Regularized Leader algorithms for feedback with full information on
the loss function, a family of Delayed Mirror Descent algorithms for feedback
with gradient information on the loss function and a family of Simplified
Delayed Mirror Descent algorithms for feedback with the value information of
the loss function's gradients at corresponding decision points. For each type
of algorithm, we provide corresponding regret bounds under cases of general
convexity and relative strong convexity, respectively. We also demonstrate the
efficiency of each algorithm under different norms through concrete examples.
Furthermore, our theoretical results are consistent with the current best
bounds when degenerated to standard settings.</div><div><a href='http://arxiv.org/abs/2402.07703v3'>2402.07703v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03256v2")'>Learning Best-in-Class Policies for the Predict-then-Optimize Framework</div>
<div id='2402.03256v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:14:28Z</div><div>Authors: Michael Huang, Vishal Gupta</div><div style='padding-top: 10px; width: 80ex'>We propose a novel family of decision-aware surrogate losses, called
Perturbation Gradient (PG) losses, for the predict-then-optimize framework.
These losses directly approximate the downstream decision loss and can be
optimized using off-the-shelf gradient-based methods. Importantly, unlike
existing surrogate losses, the approximation error of our PG losses vanishes as
the number of samples grows. This implies that optimizing our surrogate loss
yields a best-in-class policy asymptotically, even in misspecified settings.
This is the first such result in misspecified settings and we provide numerical
evidence confirming our PG losses substantively outperform existing proposals
when the underlying model is misspecified and the noise is not centrally
symmetric. Insofar as misspecification is commonplace in practice -- especially
when we might prefer a simpler, more interpretable model -- PG losses offer a
novel, theoretically justified, method for computationally tractable
decision-aware learning.</div><div><a href='http://arxiv.org/abs/2402.03256v2'>2402.03256v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07458v1")'>On the Distance from Calibration in Sequential Prediction</div>
<div id='2402.07458v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T07:37:19Z</div><div>Authors: Mingda Qiao, Letian Zheng</div><div style='padding-top: 10px; width: 80ex'>We study a sequential binary prediction setting where the forecaster is
evaluated in terms of the calibration distance, which is defined as the $L_1$
distance between the predicted values and the set of predictions that are
perfectly calibrated in hindsight. This is analogous to a calibration measure
recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the
offline setting. The calibration distance is a natural and intuitive measure of
deviation from perfect calibration, and satisfies a Lipschitz continuity
property which does not hold for many popular calibration measures, such as the
$L_1$ calibration error and its variants.
  We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$
calibration distance in expectation on an adversarially chosen sequence of $T$
binary outcomes. At the core of this upper bound is a structural result showing
that the calibration distance is accurately approximated by the lower
calibration distance, which is a continuous relaxation of the former. We then
show that an $O(\sqrt{T})$ lower calibration distance can be achieved via a
simple minimax argument and a reduction to online learning on a Lipschitz
class.
  On the lower bound side, an $\Omega(T^{1/3})$ calibration distance is shown
to be unavoidable, even when the adversary outputs a sequence of independent
random bits, and has an additional ability to early stop (i.e., to stop
producing random bits and output the same bit in the remaining steps).
Interestingly, without this early stopping, the forecaster can achieve a much
smaller calibration distance of $\mathrm{polylog}(T)$.</div><div><a href='http://arxiv.org/abs/2402.07458v1'>2402.07458v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11410v1")'>An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration</div>
<div id='2402.11410v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T00:53:05Z</div><div>Authors: Eshwar Ram Arunachaleswaran, Natalie Collina, Aaron Roth, Mirah Shi</div><div style='padding-top: 10px; width: 80ex'>Blasiok et al. [2023] proposed distance to calibration as a natural measure
of calibration error that unlike expected calibration error (ECE) is
continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument
establishing the existence of an online predictor that can obtain $O(\sqrt{T})$
distance to calibration in the adversarial setting, which is known to be
impossible for ECE. They leave as an open problem finding an explicit,
efficient algorithm. We resolve this problem and give an extremely simple,
efficient, deterministic algorithm that obtains distance to calibration error
at most $2\sqrt{T}$.</div><div><a href='http://arxiv.org/abs/2402.11410v1'>2402.11410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17104v1")'>Adversarial Perturbations of Physical Signals</div>
<div id='2402.17104v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T00:41:00Z</div><div>Authors: Robert L. Bassett, Austin Van Dellen, Anthony P. Austin</div><div style='padding-top: 10px; width: 80ex'>We investigate the vulnerability of computer-vision-based signal classifiers
to adversarial perturbations of their inputs, where the signals and
perturbations are subject to physical constraints. We consider a scenario in
which a source and interferer emit signals that propagate as waves to a
detector, which attempts to classify the source by analyzing the spectrogram of
the signal it receives using a pre-trained neural network. By solving
PDE-constrained optimization problems, we construct interfering signals that
cause the detector to misclassify the source even though the perturbations to
the spectrogram of the received signal are nearly imperceptible. Though such
problems can have millions of decision variables, we introduce methods to solve
them efficiently. Our experiments demonstrate that one can compute effective
and physically realizable adversarial perturbations for a variety of machine
learning models under various physical conditions.</div><div><a href='http://arxiv.org/abs/2402.17104v1'>2402.17104v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09802v1")'>Criterion collapse and loss distribution control</div>
<div id='2402.09802v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T08:58:58Z</div><div>Authors: Matthew J. Holland</div><div style='padding-top: 10px; width: 80ex'>In this work, we consider the notion of "criterion collapse," in which
optimization of one metric implies optimality in another, with a particular
focus on conditions for collapse into error probability minimizers under a wide
variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM)
to non-monotonic criteria underlying recent ascent-descent algorithms explored
in the literature (Flooding, SoftAD). We show how collapse in the context of
losses with a Bernoulli distribution goes far beyond existing results for CVaR
and DRO, then expand our scope to include surrogate losses, showing conditions
where monotonic criteria such as tilted ERM cannot avoid collapse, whereas
non-monotonic alternatives can.</div><div><a href='http://arxiv.org/abs/2402.09802v1'>2402.09802v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15411v1")'>Optimistic Information Directed Sampling</div>
<div id='2402.15411v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:19:32Z</div><div>Authors: Gergely Neu, Matteo Papini, Ludovic Schwartz</div><div style='padding-top: 10px; width: 80ex'>We study the problem of online learning in contextual bandit problems where
the loss function is assumed to belong to a known parametric function class. We
propose a new analytic framework for this setting that bridges the Bayesian
theory of information-directed sampling due to Russo and Van Roy (2018) and the
worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the
decision-estimation coefficient. Drawing from both lines of work, we propose a
algorithmic template called Optimistic Information-Directed Sampling and show
that it can achieve instance-dependent regret guarantees similar to the ones
achievable by the classic Bayesian IDS method, but with the major advantage of
not requiring any Bayesian assumptions. The key technical innovation of our
analysis is introducing an optimistic surrogate model for the regret and using
it to define a frequentist version of the Information Ratio of Russo and Van
Roy (2018), and a less conservative version of the Decision Estimation
Coefficient of Foster et al. (2021). Keywords: Contextual bandits,
information-directed sampling, decision estimation coefficient, first-order
regret bounds.</div><div><a href='http://arxiv.org/abs/2402.15411v1'>2402.15411v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03737v1")'>Differentially Private High Dimensional Bandits</div>
<div id='2402.03737v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:10:46Z</div><div>Authors: Apurv Shukla</div><div style='padding-top: 10px; width: 80ex'>We consider a high-dimensional stochastic contextual linear bandit problem
when the parameter vector is $s_{0}$-sparse and the decision maker is subject
to privacy constraints under both central and local models of differential
privacy. We present PrivateLASSO, a differentially private LASSO bandit
algorithm. PrivateLASSO is based on two sub-routines: (i) a sparse
hard-thresholding-based privacy mechanism and (ii) an episodic thresholding
rule for identifying the support of the parameter $\theta$. We prove minimax
private lower bounds and establish privacy and utility guarantees for
PrivateLASSO for the central model under standard assumptions.</div><div><a href='http://arxiv.org/abs/2402.03737v1'>2402.03737v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07728v1")'>CAS: A General Algorithm for Online Selective Conformal Prediction with
  FCR Control</div>
<div id='2403.07728v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T15:07:20Z</div><div>Authors: Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou</div><div style='padding-top: 10px; width: 80ex'>We study the problem of post-selection predictive inference in an online
fashion. To avoid devoting resources to unimportant units, a preliminary
selection of the current individual before reporting its prediction interval is
common and meaningful in online predictive tasks. Since the online selection
causes a temporal multiplicity in the selected prediction intervals, it is
important to control the real-time false coverage-statement rate (FCR) to
measure the averaged miscoverage error. We develop a general framework named
CAS (Calibration after Adaptive Selection) that can wrap around any prediction
model and online selection rule to output post-selection prediction intervals.
If the current individual is selected, we first perform an adaptive selection
on historical data to construct a calibration set, then output a conformal
prediction interval for the unobserved label. We provide tractable
constructions for the calibration set for popular online selection rules. We
proved that CAS can achieve an exact selection-conditional coverage guarantee
in the finite-sample and distribution-free regimes. For the decision-driven
selection rule, including most online multiple-testing procedures, CAS can
exactly control the real-time FCR below the target level without any
distributional assumptions. For the online selection with symmetric thresholds,
we establish the error bound for the control gap of FCR under mild
distributional assumptions. To account for the distribution shift in online
data, we also embed CAS into some recent dynamic conformal prediction methods
and examine the long-run FCR control. Numerical results on both synthetic and
real data corroborate that CAS can effectively control FCR around the target
level and yield more narrowed prediction intervals over existing baselines
across various settings.</div><div><a href='http://arxiv.org/abs/2403.07728v1'>2403.07728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11973v1")'>Bayesian Active Learning for Censored Regression</div>
<div id='2402.11973v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:19:01Z</div><div>Authors: Frederik Boe H√ºttel, Christoffer Riis, Filipe Rodrigues, Francisco C√¢mara Pereira</div><div style='padding-top: 10px; width: 80ex'>Bayesian active learning is based on information theoretical approaches that
focus on maximising the information that new observations provide to the model
parameters. This is commonly done by maximising the Bayesian Active Learning by
Disagreement (BALD) acquisitions function. However, we highlight that it is
challenging to estimate BALD when the new data points are subject to
censorship, where only clipped values of the targets are observed. To address
this, we derive the entropy and the mutual information for censored
distributions and derive the BALD objective for active learning in censored
regression ($\mathcal{C}$-BALD). We propose a novel modelling approach to
estimate the $\mathcal{C}$-BALD objective and use it for active learning in the
censored setting. Across a wide range of datasets and models, we demonstrate
that $\mathcal{C}$-BALD outperforms other Bayesian active learning methods in
censored regression.</div><div><a href='http://arxiv.org/abs/2402.11973v1'>2402.11973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01995v2")'>Online Uniform Risk Times Sampling: First Approximation Algorithms,
  Learning Augmentation with Full Confidence Interval Integration</div>
<div id='2402.01995v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T02:36:59Z</div><div>Authors: Xueqing Liu, Kyra Gan, Esmaeil Keyvanshokooh, Susan Murphy</div><div style='padding-top: 10px; width: 80ex'>In digital health, the strategy of allocating a limited treatment budget
across available risk times is crucial to reduce user fatigue. This strategy,
however, encounters a significant obstacle due to the unknown actual number of
risk times, a factor not adequately addressed by existing methods lacking
theoretical guarantees. This paper introduces, for the first time, the online
uniform risk times sampling problem within the approximation algorithm
framework. We propose two online approximation algorithms for this problem, one
with and one without learning augmentation, and provide rigorous theoretical
performance guarantees for them using competitive ratio analysis. We assess the
performance of our algorithms using both synthetic experiments and a real-world
case study on HeartSteps mobile applications.</div><div><a href='http://arxiv.org/abs/2402.01995v2'>2402.01995v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00158v2")'>Automated Efficient Estimation using Monte Carlo Efficient Influence
  Functions</div>
<div id='2403.00158v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T22:19:46Z</div><div>Authors: Raj Agrawal, Sam Witty, Andy Zane, Eli Bingham</div><div style='padding-top: 10px; width: 80ex'>Many practical problems involve estimating low dimensional statistical
quantities with high-dimensional models and datasets. Several approaches
address these estimation tasks based on the theory of influence functions, such
as debiased/double ML or targeted minimum loss estimation. This paper
introduces \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully
automated technique for approximating efficient influence functions that
integrates seamlessly with existing differentiable probabilistic programming
systems. MC-EIF automates efficient statistical estimation for a broad class of
models and target functionals that would previously require rigorous custom
analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF
achieve optimal $\sqrt{N}$ convergence rates. We show empirically that
estimators using MC-EIF are at parity with estimators using analytic EIFs.
Finally, we demonstrate a novel capstone example using MC-EIF for optimal
portfolio selection.</div><div><a href='http://arxiv.org/abs/2403.00158v2'>2403.00158v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08108v1")'>Finding Moving-Band Statistical Arbitrages via Convex-Concave
  Optimization</div>
<div id='2402.08108v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:56:16Z</div><div>Authors: Kasper Johansson, Thomas Schmelzer, Stephen Boyd</div><div style='padding-top: 10px; width: 80ex'>We propose a new method for finding statistical arbitrages that can contain
more assets than just the traditional pair. We formulate the problem as seeking
a portfolio with the highest volatility, subject to its price remaining in a
band and a leverage limit. This optimization problem is not convex, but can be
approximately solved using the convex-concave procedure, a specific sequential
convex programming method. We show how the method generalizes to finding
moving-band statistical arbitrages, where the price band midpoint varies over
time.</div><div><a href='http://arxiv.org/abs/2402.08108v1'>2402.08108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12934v1")'>Reward-Relevance-Filtered Linear Offline Reinforcement Learning</div>
<div id='2401.12934v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T17:42:17Z</div><div>Authors: Angela Zhou</div><div style='padding-top: 10px; width: 80ex'>This paper studies offline reinforcement learning with linear function
approximation in a setting with decision-theoretic, but not estimation
sparsity. The structural restrictions of the data-generating process presume
that the transitions factor into a sparse component that affects the reward and
could affect additional exogenous dynamics that do not affect the reward.
Although the minimally sufficient adjustment set for estimation of full-state
transition properties depends on the whole state, the optimal policy and
therefore state-action value function depends only on the sparse component: we
call this causal/decision-theoretic sparsity. We develop a method for
reward-filtering the estimation of the state-action value function to the
sparse component by a modification of thresholded lasso in least-squares policy
evaluation. We provide theoretical guarantees for our reward-filtered linear
fitted-Q-iteration, with sample complexity depending only on the size of the
sparse component.</div><div><a href='http://arxiv.org/abs/2401.12934v1'>2401.12934v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01900v1")'>Distributional Off-policy Evaluation with Bellman Residual Minimization</div>
<div id='2402.01900v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:59:29Z</div><div>Authors: Sungee Hong, Zhengling Qi, Raymond K. W. Wong</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of distributional off-policy evaluation which serves
as the foundation of many distributional reinforcement learning (DRL)
algorithms. In contrast to most existing works (that rely on supremum-extended
statistical distances such as supremum-Wasserstein distance), we study the
expectation-extended statistical distance for quantifying the distributional
Bellman residuals and show that it can upper bound the expected error of
estimating the return distribution. Based on this appealing property, by
extending the framework of Bellman residual minimization to DRL, we propose a
method called Energy Bellman Residual Minimizer (EBRM) to estimate the return
distribution. We establish a finite-sample error bound for the EBRM estimator
under the realizability assumption. Furthermore, we introduce a variant of our
method based on a multi-step bootstrapping procedure to enable multi-step
extension. By selecting an appropriate step level, we obtain a better error
bound for this variant of EBRM compared to a single-step EBRM, under some
non-realizability settings. Finally, we demonstrate the superior performance of
our method through simulation studies, comparing with several existing methods.</div><div><a href='http://arxiv.org/abs/2402.01900v1'>2402.01900v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02325v2")'>A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In
  Distributional Reinforcement Learning</div>
<div id='2401.02325v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T15:51:49Z</div><div>Authors: Parvin Malekzadeh, Konstantinos N. Plataniotis, Zissis Poulos, Zeyu Wang</div><div style='padding-top: 10px; width: 80ex'>Distributional Reinforcement Learning (RL) estimates return distribution
mainly by learning quantile values via minimizing the quantile Huber loss
function, entailing a threshold parameter often selected heuristically or via
hyperparameter search, which may not generalize well and can be suboptimal.
This paper introduces a generalized quantile Huber loss function derived from
Wasserstein distance (WD) calculation between Gaussian distributions, capturing
noise in predicted (current) and target (Bellman-updated) quantile values.
Compared to the classical quantile Huber loss, this innovative loss function
enhances robustness against outliers. Notably, the classical Huber loss
function can be seen as an approximation of our proposed loss, enabling
parameter adjustment by approximating the amount of noise in the data during
the learning process. Empirical tests on Atari games, a common application in
distributional RL, and a recent hedging strategy using distributional RL,
validate the effectiveness of our proposed loss function and its potential for
parameter adjustments in distributional RL. The implementation of the proposed
loss function is available here.</div><div><a href='http://arxiv.org/abs/2401.02325v2'>2401.02325v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03301v2")'>On Sample-Efficient Offline Reinforcement Learning: Data Diversity,
  Posterior Sampling, and Beyond</div>
<div id='2401.03301v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T20:52:04Z</div><div>Authors: Thanh Nguyen-Tang, Raman Arora</div><div style='padding-top: 10px; width: 80ex'>We seek to understand what facilitates sample-efficient learning from
historical datasets for sequential decision-making, a problem that is popularly
known as offline reinforcement learning (RL). Further, we are interested in
algorithms that enjoy sample efficiency while leveraging (value) function
approximation. In this paper, we address these fundamental questions by (i)
proposing a notion of data diversity that subsumes the previous notions of
coverage measures in offline RL and (ii) using this notion to {unify} three
distinct classes of offline RL algorithms based on version spaces (VS),
regularized optimization (RO), and posterior sampling (PS). We establish that
VS-based, RO-based, and PS-based algorithms, under standard assumptions,
achieve \emph{comparable} sample efficiency, which recovers the
state-of-the-art sub-optimality bounds for finite and linear model classes with
the standard assumptions. This result is surprising, given that the prior work
suggested an unfavorable sample complexity of the RO-based algorithm compared
to the VS-based algorithm, whereas posterior sampling is rarely considered in
offline RL due to its explorative nature. Notably, our proposed model-free
PS-based algorithm for offline RL is {novel}, with sub-optimality bounds that
are {frequentist} (i.e., worst-case) in nature.</div><div><a href='http://arxiv.org/abs/2401.03301v2'>2401.03301v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11877v1")'>Finite-Time Error Analysis of Online Model-Based Q-Learning with a
  Relaxed Sampling Model</div>
<div id='2402.11877v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T06:33:51Z</div><div>Authors: Han-Dong Lim, HyeAnn Lee, Donghwan Lee</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning has witnessed significant advancements, particularly
with the emergence of model-based approaches. Among these, $Q$-learning has
proven to be a powerful algorithm in model-free settings. However, the
extension of $Q$-learning to a model-based framework remains relatively
unexplored. In this paper, we delve into the sample complexity of $Q$-learning
when integrated with a model-based approach. Through theoretical analyses and
empirical evaluations, we seek to elucidate the conditions under which
model-based $Q$-learning excels in terms of sample efficiency compared to its
model-free counterpart.</div><div><a href='http://arxiv.org/abs/2402.11877v1'>2402.11877v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08201v1")'>Off-Policy Evaluation in Markov Decision Processes under Weak
  Distributional Overlap</div>
<div id='2402.08201v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T03:55:56Z</div><div>Authors: Mohammad Mehrabi, Stefan Wager</div><div style='padding-top: 10px; width: 80ex'>Doubly robust methods hold considerable promise for off-policy evaluation in
Markov decision processes (MDPs) under sequential ignorability: They have been
shown to converge as $1/\sqrt{T}$ with the horizon $T$, to be statistically
efficient in large samples, and to allow for modular implementation where
preliminary estimation tasks can be executed using standard reinforcement
learning techniques. Existing results, however, make heavy use of a strong
distributional overlap assumption whereby the stationary distributions of the
target policy and the data-collection policy are within a bounded factor of
each other -- and this assumption is typically only credible when the state
space of the MDP is bounded. In this paper, we re-visit the task of off-policy
evaluation in MDPs under a weaker notion of distributional overlap, and
introduce a class of truncated doubly robust (TDR) estimators which we find to
perform well in this setting. When the distribution ratio of the target and
data-collection policies is square-integrable (but not necessarily bounded),
our approach recovers the large-sample behavior previously established under
strong distributional overlap. When this ratio is not square-integrable, TDR is
still consistent but with a slower-than-$1/\sqrt{T}$; furthermore, this rate of
convergence is minimax over a class of MDPs defined only using mixing
conditions. We validate our approach numerically and find that, in our
experiments, appropriate truncation plays a major role in enabling accurate
off-policy evaluation when strong distributional overlap does not hold.</div><div><a href='http://arxiv.org/abs/2402.08201v1'>2402.08201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05811v2")'>Near Minimax-Optimal Distributional Temporal Difference Algorithms and
  The Freedman Inequality in Hilbert Spaces</div>
<div id='2403.05811v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T06:19:53Z</div><div>Authors: Yang Peng, Liangyu Zhang, Zhihua Zhang</div><div style='padding-top: 10px; width: 80ex'>Distributional reinforcement learning (DRL) has achieved empirical success in
various domains. One of the core tasks in the field of DRL is distributional
policy evaluation, which involves estimating the return distribution $\eta^\pi$
for a given policy $\pi$. The distributional temporal difference (TD) algorithm
has been accordingly proposed, which is an extension of the temporal difference
algorithm in the classic RL literature. In the tabular case,
\citet{rowland2018analysis} and \citet{rowland2023analysis} proved the
asymptotic convergence of two instances of distributional TD, namely
categorical temporal difference algorithm (CTD) and quantile temporal
difference algorithm (QTD), respectively. In this paper, we go a step further
and analyze the finite-sample performance of distributional TD. To facilitate
theoretical analysis, we propose a non-parametric distributional TD algorithm
(NTD). For a $\gamma$-discounted infinite-horizon tabular Markov decision
process, we show that for NTD we need
$\tilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)$ iterations
to achieve an $\varepsilon$-optimal estimator with high probability, when the
estimation error is measured by the $p$-Wasserstein distance. This sample
complexity bound is minimax optimal (up to logarithmic factors) in the case of
the $1$-Wasserstein distance. To achieve this, we establish a novel Freedman's
inequality in Hilbert spaces, which would be of independent interest. In
addition, we revisit CTD, showing that the same non-asymptotic convergence
bounds hold for CTD in the case of the $p$-Wasserstein distance.</div><div><a href='http://arxiv.org/abs/2403.05811v2'>2403.05811v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10738v1")'>Horizon-Free Regret for Linear Markov Decision Processes</div>
<div id='2403.10738v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T23:50:58Z</div><div>Authors: Zihan Zhang, Jason D. Lee, Yuxin Chen, Simon S. Du</div><div style='padding-top: 10px; width: 80ex'>A recent line of works showed regret bounds in reinforcement learning (RL)
can be (nearly) independent of planning horizon, a.k.a.~the horizon-free
bounds. However, these regret bounds only apply to settings where a polynomial
dependency on the size of transition model is allowed, such as tabular Markov
Decision Process (MDP) and linear mixture MDP. We give the first horizon-free
bound for the popular linear MDP setting where the size of the transition model
can be exponentially large or even uncountable. In contrast to prior works
which explicitly estimate the transition model and compute the inhomogeneous
value functions at different time steps, we directly estimate the value
functions and confidence sets. We obtain the horizon-free bound by: (1)
maintaining multiple weighted least square estimators for the value functions;
and (2) a structural lemma which shows the maximal total variation of the
inhomogeneous value functions is bounded by a polynomial factor of the feature
dimension.</div><div><a href='http://arxiv.org/abs/2403.10738v1'>2403.10738v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08998v1")'>Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic
  Shortest Path</div>
<div id='2402.08998v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T07:52:00Z</div><div>Authors: Qiwei Di, Jiafan He, Dongruo Zhou, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>We study the Stochastic Shortest Path (SSP) problem with a linear mixture
transition kernel, where an agent repeatedly interacts with a stochastic
environment and seeks to reach certain goal state while minimizing the
cumulative cost. Existing works often assume a strictly positive lower bound of
the cost function or an upper bound of the expected length for the optimal
policy. In this paper, we propose a new algorithm to eliminate these
restrictive assumptions. Our algorithm is based on extended value iteration
with a fine-grained variance-aware confidence set, where the variance is
estimated recursively from high-order moments. Our algorithm achieves an
$\tilde{\mathcal O}(dB_*\sqrt{K})$ regret bound, where $d$ is the dimension of
the feature mapping in the linear transition kernel, $B_*$ is the upper bound
of the total cumulative cost for the optimal policy, and $K$ is the number of
episodes. Our regret upper bound matches the $\Omega(dB_*\sqrt{K})$ lower bound
of linear mixture SSPs in Min et al. (2022), which suggests that our algorithm
is nearly minimax optimal.</div><div><a href='http://arxiv.org/abs/2402.08998v1'>2402.08998v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08406v1")'>Transition Constrained Bayesian Optimization via Markov Decision
  Processes</div>
<div id='2402.08406v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T12:11:40Z</div><div>Authors: Jose Pablo Folch, Calvin Tsay, Robert M Lee, Behrang Shafei, Weronika Ormaniec, Andreas Krause, Mark van der Wilk, Ruth Misener, Mojm√≠r Mutn√Ω</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimization is a methodology to optimize black-box functions.
Traditionally, it focuses on the setting where you can arbitrarily query the
search space. However, many real-life problems do not offer this flexibility;
in particular, the search space of the next query may depend on previous ones.
Example challenges arise in the physical sciences in the form of local movement
constraints, required monotonicity in certain variables, and transitions
influencing the accuracy of measurements. Altogether, such transition
constraints necessitate a form of planning. This work extends Bayesian
optimization via the framework of Markov Decision Processes, iteratively
solving a tractable linearization of our objective using reinforcement learning
to obtain a policy that plans ahead over long horizons. The resulting policy is
potentially history-dependent and non-Markovian. We showcase applications in
chemical reactor optimization, informative path planning, machine calibration,
and other synthetic examples.</div><div><a href='http://arxiv.org/abs/2402.08406v1'>2402.08406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01841v3")'>Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov
  Decision Processes</div>
<div id='2401.01841v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T17:19:54Z</div><div>Authors: Baiting Luo, Yunuo Zhang, Abhishek Dubey, Ayan Mukhopadhyay</div><div style='padding-top: 10px; width: 80ex'>A fundamental (and largely open) challenge in sequential decision-making is
dealing with non-stationary environments, where exogenous environmental
conditions change over time. Such problems are traditionally modeled as
non-stationary Markov decision processes (NSMDP). However, existing approaches
for decision-making in NSMDPs have two major shortcomings: first, they assume
that the updated environmental dynamics at the current time are known (although
future dynamics can change); and second, planning is largely pessimistic, i.e.,
the agent acts ``safely'' to account for the non-stationary evolution of the
environment. We argue that both these assumptions are invalid in practice --
updated environmental conditions are rarely known, and as the agent interacts
with the environment, it can learn about the updated dynamics and avoid being
pessimistic, at least in states whose dynamics it is confident about. We
present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree
Search (ADA-MCTS)} that addresses these challenges. We show that the agent can
learn the updated dynamics of the environment over time and then act as it
learns, i.e., if the agent is in a region of the state space about which it has
updated knowledge, it can avoid being pessimistic. To quantify ``updated
knowledge,'' we disintegrate the aleatoric and epistemic uncertainty in the
agent's updated belief and show how the agent can use these estimates for
decision-making. We compare the proposed approach with the multiple
state-of-the-art approaches in decision-making across multiple well-established
open-source problems and empirically show that our approach is faster and
highly adaptive without sacrificing safety.</div><div><a href='http://arxiv.org/abs/2401.01841v3'>2401.01841v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12417v1")'>On Predictive planning and counterfactual learning in active inference</div>
<div id='2403.12417v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T04:02:31Z</div><div>Authors: Aswin Paul, Takuya Isomura, Adeel Razi</div><div style='padding-top: 10px; width: 80ex'>Given the rapid advancement of artificial intelligence, understanding the
foundations of intelligent behaviour is increasingly important. Active
inference, regarded as a general theory of behaviour, offers a principled
approach to probing the basis of sophistication in planning and
decision-making. In this paper, we examine two decision-making schemes in
active inference based on 'planning' and 'learning from experience'.
Furthermore, we also introduce a mixed model that navigates the data-complexity
trade-off between these strategies, leveraging the strengths of both to
facilitate balanced decision-making. We evaluate our proposed model in a
challenging grid-world scenario that requires adaptability from the agent.
Additionally, our model provides the opportunity to analyze the evolution of
various parameters, offering valuable insights and contributing to an
explainable framework for intelligent decision-making.</div><div><a href='http://arxiv.org/abs/2403.12417v1'>2403.12417v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01586v2")'>TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent
  Constitution</div>
<div id='2402.01586v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T17:26:23Z</div><div>Authors: Wenyue Hua, Xianjun Yang, Zelong Li, Wei Cheng, Yongfeng Zhang</div><div style='padding-top: 10px; width: 80ex'>The emergence of LLM-based agents has garnered considerable attention, yet
their trustworthiness remains an under-explored area. As agents can directly
interact with the physical environment, their reliability and safety is
critical. This paper presents an Agent-Constitution-based agent framework,
TrustAgent, an initial investigation into improving the safety dimension of
trustworthiness in LLM-based agents. This framework consists of threefold
strategies: pre-planning strategy which injects safety knowledge to the model
prior to plan generation, in-planning strategy which bolsters safety during
plan generation, and post-planning strategy which ensures safety by
post-planning inspection. Through experimental analysis, we demonstrate how
these approaches can effectively elevate an LLM agent's safety by identifying
and preventing potential dangers. Furthermore, we explore the intricate
relationships between safety and helpfulness, and between the model's reasoning
ability and its efficacy as a safe agent. This paper underscores the imperative
of integrating safety awareness and trustworthiness into the design and
deployment of LLM-based agents, not only to enhance their performance but also
to ensure their responsible integration into human-centric environments. Data
and code are available at https://github.com/agiresearch/TrustAgent.</div><div><a href='http://arxiv.org/abs/2402.01586v2'>2402.01586v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19265v1")'>Learning Logic Specifications for Policy Guidance in POMDPs: an
  Inductive Logic Programming Approach</div>
<div id='2402.19265v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:36:01Z</div><div>Authors: Daniele Meli, Alberto Castellini, Alessandro Farinelli</div><div style='padding-top: 10px; width: 80ex'>Partially Observable Markov Decision Processes (POMDPs) are a powerful
framework for planning under uncertainty. They allow to model state uncertainty
as a belief probability distribution. Approximate solvers based on Monte Carlo
sampling show great success to relax the computational demand and perform
online planning. However, scaling to complex realistic domains with many
actions and long planning horizons is still a major challenge, and a key point
to achieve good performance is guiding the action-selection process with
domain-dependent policy heuristics which are tailored for the specific
application domain. We propose to learn high-quality heuristics from POMDP
traces of executions generated by any solver. We convert the belief-action
pairs to a logical semantics, and exploit data- and time-efficient Inductive
Logic Programming (ILP) to generate interpretable belief-based policy
specifications, which are then used as online heuristics. We evaluate
thoroughly our methodology on two notoriously challenging POMDP problems,
involving large action spaces and long planning horizons, namely, rocksample
and pocman. Considering different state-of-the-art online POMDP solvers,
including POMCP, DESPOT and AdaOPS, we show that learned heuristics expressed
in Answer Set Programming (ASP) yield performance superior to neural networks
and similar to optimal handcrafted task-specific heuristics within lower
computational time. Moreover, they well generalize to more challenging
scenarios not experienced in the training phase (e.g., increasing rocks and
grid size in rocksample, incrementing the size of the map and the aggressivity
of ghosts in pocman).</div><div><a href='http://arxiv.org/abs/2402.19265v1'>2402.19265v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07656v2")'>Learning Explainable and Better Performing Representations of POMDP
  Strategies</div>
<div id='2401.07656v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T12:52:56Z</div><div>Authors: Alexander Bork, Debraj Chakraborty, Kush Grover, Jan Kretinsky, Stefanie Mohr</div><div style='padding-top: 10px; width: 80ex'>Strategies for partially observable Markov decision processes (POMDP)
typically require memory. One way to represent this memory is via automata. We
present a method to learn an automaton representation of a strategy using a
modification of the L*-algorithm. Compared to the tabular representation of a
strategy, the resulting automaton is dramatically smaller and thus also more
explainable. Moreover, in the learning process, our heuristics may even improve
the strategy's performance. In contrast to approaches that synthesize an
automaton directly from the POMDP thereby solving it, our approach is
incomparably more scalable.</div><div><a href='http://arxiv.org/abs/2401.07656v2'>2401.07656v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12243v1")'>Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming
  for Policy Optimization in Mixed Discrete-Continuous MDPs</div>
<div id='2401.12243v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T07:12:57Z</div><div>Authors: Michael Gimelfarb, Ayal Taitler, Scott Sanner</div><div style='padding-top: 10px; width: 80ex'>We propose Constraint-Generation Policy Optimization (CGPO) for optimizing
policy parameters within compact and interpretable policy classes for mixed
discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able
to provide bounded policy error guarantees over an infinite range of initial
states for many DC-MDPs with expressive nonlinear dynamics, but it can also
provably derive optimal policies in cases where it terminates with zero error.
Furthermore, CGPO can generate worst-case state trajectories to diagnose policy
deficiencies and provide counterfactual explanations of optimal actions. To
achieve such results, CGPO proposes a bi-level mixed-integer nonlinear
optimization framework for optimizing policies within defined expressivity
classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint
generation methodology that adversarially generates worst-case state
trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can
obtain solutions with bounded optimality gap guarantees. We handle stochastic
transitions through explicit marginalization (where applicable) or
chance-constraints, providing high-probability policy performance guarantees.
We also present a road-map for understanding the computational complexities
associated with different expressivity classes of policy, reward, and
transition dynamics. We experimentally demonstrate the applicability of CGPO in
diverse domains, including inventory control, management of a system of water
reservoirs, and physics control. In summary, we provide a solution for deriving
structured, compact, and explainable policies with bounded performance
guarantees, enabling worst-case scenario generation and counterfactual policy
diagnostics.</div><div><a href='http://arxiv.org/abs/2401.12243v1'>2401.12243v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12015v1")'>An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart
  Target Tracking</div>
<div id='2402.12015v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T10:13:25Z</div><div>Authors: Yuhang Hao, Zengfu Wang, Jing Fu, Quan Pan</div><div style='padding-top: 10px; width: 80ex'>In solving the non-myopic radar scheduling for multiple smart target tracking
within an active and passive radar network, we need to consider both short-term
enhanced tracking performance and a higher probability of target maneuvering in
the future with active tracking. Acquiring the long-term tracking performance
while scheduling the beam resources of active and passive radars poses a
challenge. To address this challenge, we model this problem as a Markov
decision process consisting of parallel restless bandit processes. Each bandit
process is associated with a smart target, of which the estimation state
evolves according to different discrete dynamic models for different actions -
whether or not the target is being tracked. The discrete state is defined by
the dynamic mode. The problem exhibits the curse of dimensionality, where
optimal solutions are in general intractable. We resort to heuristics through
the famous restless multi-armed bandit techniques. It follows with efficient
scheduling policies based on the indices that are real numbers representing the
marginal rewards of taking different actions. For the inevitable practical case
with unknown transition matrices, we propose a new method that utilizes the
forward Sarsa and backward Q-learning to approximate the indices through
adapting the state-action value functions, or equivalently the Q-functions, and
propose a new policy, namely ISQ, aiming to maximize the long-term tracking
rewards. Numerical results demonstrate that the proposed ISQ policy outperforms
conventional Q-learning-based methods and rapidly converges to the well-known
Whittle index policy with revealed state transition models, which is considered
the benchmark.</div><div><a href='http://arxiv.org/abs/2402.12015v1'>2402.12015v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11175v1")'>Prior-dependent analysis of posterior sampling reinforcement learning
  with function approximation</div>
<div id='2403.11175v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T11:23:51Z</div><div>Authors: Yingru Li, Zhi-Quan Luo</div><div style='padding-top: 10px; width: 80ex'>This work advances randomized exploration in reinforcement learning (RL) with
function approximation modeled by linear mixture MDPs. We establish the first
prior-dependent Bayesian regret bound for RL with function approximation; and
refine the Bayesian regret analysis for posterior sampling reinforcement
learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log
T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the
planning horizon, and $T$ the total number of interactions. This signifies a
methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$
factor over the previous benchmark (Osband and Van Roy, 2014) specified to
linear mixture MDPs. Our approach, leveraging a value-targeted model learning
perspective, introduces a decoupling argument and a variance reduction
technique, moving beyond traditional analyses reliant on confidence sets and
concentration inequalities to formalize Bayesian regret bounds more
effectively.</div><div><a href='http://arxiv.org/abs/2403.11175v1'>2403.11175v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11062v2")'>A Simple Mixture Policy Parameterization for Improving Sample Efficiency
  of CVaR Optimization</div>
<div id='2403.11062v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T02:24:09Z</div><div>Authors: Yudong Luo, Yangchen Pan, Han Wang, Philip Torr, Pascal Poupart</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning algorithms utilizing policy gradients (PG) to optimize
Conditional Value at Risk (CVaR) face significant challenges with sample
inefficiency, hindering their practical applications. This inefficiency stems
from two main facts: a focus on tail-end performance that overlooks many
sampled trajectories, and the potential of gradient vanishing when the lower
tail of the return distribution is overly flat. To address these challenges, we
propose a simple mixture policy parameterization. This method integrates a
risk-neutral policy with an adjustable policy to form a risk-averse policy. By
employing this strategy, all collected trajectories can be utilized for policy
updating, and the issue of vanishing gradients is counteracted by stimulating
higher returns through the risk-neutral component, thus lifting the tail and
preventing flatness. Our empirical study reveals that this mixture
parameterization is uniquely effective across a variety of benchmark domains.
Specifically, it excels in identifying risk-averse CVaR policies in some Mujoco
environments where the traditional CVaR-PG fails to learn a reasonable policy.</div><div><a href='http://arxiv.org/abs/2403.11062v2'>2403.11062v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14703v1")'>On the Curses of Future and History in Future-dependent Value Functions
  for Off-policy Evaluation</div>
<div id='2402.14703v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:00:50Z</div><div>Authors: Yuheng Zhang, Nan Jiang</div><div style='padding-top: 10px; width: 80ex'>We study off-policy evaluation (OPE) in partially observable environments
with complex observations, with the goal of developing estimators whose
guarantee avoids exponential dependence on the horizon. While such estimators
exist for MDPs and POMDPs can be converted to history-based MDPs, their
estimation errors depend on the state-density ratio for MDPs which becomes
history ratios after conversion, an exponential object. Recently, Uehara et al.
(2022) proposed future-dependent value functions as a promising framework to
address this issue, where the guarantee for memoryless policies depends on the
density ratio over the latent state space. However, it also depends on the
boundedness of the future-dependent value function and other related
quantities, which we show could be exponential-in-length and thus erasing the
advantage of the method. In this paper, we discover novel coverage assumptions
tailored to the structure of POMDPs, such as outcome coverage and belief
coverage. These assumptions not only enable polynomial bounds on the
aforementioned quantities, but also lead to the discovery of new algorithms
with complementary properties.</div><div><a href='http://arxiv.org/abs/2402.14703v1'>2402.14703v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13821v1")'>Performance Improvement Bounds for Lipschitz Configurable Markov
  Decision Processes</div>
<div id='2402.13821v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T13:59:47Z</div><div>Authors: Alberto Maria Metelli</div><div style='padding-top: 10px; width: 80ex'>Configurable Markov Decision Processes (Conf-MDPs) have recently been
introduced as an extension of the traditional Markov Decision Processes (MDPs)
to model the real-world scenarios in which there is the possibility to
intervene in the environment in order to configure some of its parameters. In
this paper, we focus on a particular subclass of Conf-MDP that satisfies
regularity conditions, namely Lipschitz continuity. We start by providing a
bound on the Wasserstein distance between $\gamma$-discounted stationary
distributions induced by changing policy and configuration. This result
generalizes the already existing bounds both for Conf-MDPs and traditional
MDPs. Then, we derive a novel performance improvement lower bound.</div><div><a href='http://arxiv.org/abs/2402.13821v1'>2402.13821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05233v1")'>Taming "data-hungry" reinforcement learning? Stability in continuous
  state-action spaces</div>
<div id='2401.05233v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T16:01:08Z</div><div>Authors: Yaqi Duan, Martin J. Wainwright</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel framework for analyzing reinforcement learning (RL) in
continuous state-action spaces, and use it to prove fast rates of convergence
in both off-line and on-line settings. Our analysis highlights two key
stability properties, relating to how changes in value functions and/or
policies affect the Bellman operator and occupation measures. We argue that
these properties are satisfied in many continuous state-action Markov decision
processes, and demonstrate how they arise naturally when using linear function
approximation methods. Our analysis offers fresh perspectives on the roles of
pessimism and optimism in off-line and on-line RL, and highlights the
connection between off-line RL and transfer learning.</div><div><a href='http://arxiv.org/abs/2401.05233v1'>2401.05233v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07844v2")'>The ODE Method for Stochastic Approximation and Reinforcement Learning
  with Markovian Noise</div>
<div id='2401.07844v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T17:20:17Z</div><div>Authors: Shuze Liu, Shuhang Chen, Shangtong Zhang</div><div style='padding-top: 10px; width: 80ex'>Stochastic approximation is a class of algorithms that update a vector
iteratively, incrementally, and stochastically, including, e.g., stochastic
gradient descent and temporal difference learning. One fundamental challenge in
analyzing a stochastic approximation algorithm is to establish its stability,
i.e., to show that the stochastic vector iterates are bounded almost surely. In
this paper, we extend the celebrated Borkar-Meyn theorem for stability from the
Martingale difference noise setting to the Markovian noise setting, which
greatly improves its applicability in reinforcement learning, especially in
those off-policy reinforcement learning algorithms with linear function
approximation and eligibility traces. Central to our analysis is the
diminishing asymptotic rate of change of a few functions, which is implied by
both a form of strong law of large numbers and a commonly used V4 Lyapunov
drift condition and trivially holds if the Markov chain is finite and
irreducible.</div><div><a href='http://arxiv.org/abs/2401.07844v2'>2401.07844v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09339v2")'>Central Limit Theorem for Two-Timescale Stochastic Approximation with
  Markovian Noise: Theory and Applications</div>
<div id='2401.09339v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T17:01:08Z</div><div>Authors: Jie Hu, Vishwaraj Doshi, Do Young Eun</div><div style='padding-top: 10px; width: 80ex'>Two-timescale stochastic approximation (TTSA) is among the most general
frameworks for iterative stochastic algorithms. This includes well-known
stochastic optimization methods such as SGD variants and those designed for
bilevel or minimax problems, as well as reinforcement learning like the family
of gradient-based temporal difference (GTD) algorithms. In this paper, we
conduct an in-depth asymptotic analysis of TTSA under controlled Markovian
noise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA
influenced by the underlying Markov chain, which has not been addressed by
previous CLT results of TTSA only with Martingale difference noise. Building
upon our CLT, we expand its application horizon of efficient sampling
strategies from vanilla SGD to a wider TTSA context in distributed learning,
thus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT
result to deduce the statistical properties of GTD algorithms with nonlinear
function approximation using Markovian samples and show their identical
asymptotic performance, a perspective not evident from current finite-time
bounds.</div><div><a href='http://arxiv.org/abs/2401.09339v2'>2401.09339v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11800v1")'>Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling</div>
<div id='2402.11800v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T03:08:02Z</div><div>Authors: Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra</div><div style='padding-top: 10px; width: 80ex'>Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.</div><div><a href='http://arxiv.org/abs/2402.11800v1'>2402.11800v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12338v1")'>Stochastic Halpern iteration in normed spaces and applications to
  reinforcement learning</div>
<div id='2403.12338v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T01:07:35Z</div><div>Authors: Mario Bravo, Juan Pablo Contreras</div><div style='padding-top: 10px; width: 80ex'>We analyze the oracle complexity of the stochastic Halpern iteration with
variance reduction, where we aim to approximate fixed-points of nonexpansive
and contractive operators in a normed finite-dimensional space. We show that if
the underlying stochastic oracle is with uniformly bounded variance, our method
exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$,
improving recent rates established for the stochastic Krasnoselskii-Mann
iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$,
which applies to a wide range of algorithms, including all averaged iterations
even with minibatching. Using a suitable modification of our approach, we
derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in
which the operator is a $\gamma$-contraction. As an application, we propose new
synchronous algorithms for average reward and discounted reward Markov decision
processes. In particular, for the average reward, our method improves on the
best-known sample complexity.</div><div><a href='http://arxiv.org/abs/2403.12338v1'>2403.12338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15719v2")'>Rates of Convergence in the Central Limit Theorem for Markov Chains,
  with an Application to TD Learning</div>
<div id='2401.15719v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T17:57:39Z</div><div>Authors: R. Srikant</div><div style='padding-top: 10px; width: 80ex'>We prove a non-asymptotic central limit theorem for vector-valued martingale
differences using Stein's method, and use Poisson's equation to extend the
result to functions of Markov Chains. We then show that these results can be
applied to establish a non-asymptotic central limit theorem for Temporal
Difference (TD) learning with averaging.</div><div><a href='http://arxiv.org/abs/2401.15719v2'>2401.15719v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02650v1")'>Improving sample efficiency of high dimensional Bayesian optimization
  with MCMC</div>
<div id='2401.02650v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T05:56:42Z</div><div>Authors: Zeji Yi, Yunyue Wei, Chu Xin Cheng, Kaibo He, Yanan Sui</div><div style='padding-top: 10px; width: 80ex'>Sequential optimization methods are often confronted with the curse of
dimensionality in high-dimensional spaces. Current approaches under the
Gaussian process framework are still burdened by the computational complexity
of tracking Gaussian process posteriors and need to partition the optimization
problem into small regions to ensure exploration or assume an underlying
low-dimensional structure. With the idea of transiting the candidate points
towards more promising positions, we propose a new method based on Markov Chain
Monte Carlo to efficiently sample from an approximated posterior. We provide
theoretical guarantees of its convergence in the Gaussian process Thompson
sampling setting. We also show experimentally that both the Metropolis-Hastings
and the Langevin Dynamics version of our algorithm outperform state-of-the-art
methods in high-dimensional sequential optimization and reinforcement learning
benchmarks.</div><div><a href='http://arxiv.org/abs/2401.02650v1'>2401.02650v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00540v1")'>Epsilon-Greedy Thompson Sampling to Bayesian Optimization</div>
<div id='2403.00540v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:53:44Z</div><div>Authors: Bach Do, Ruda Zhang</div><div style='padding-top: 10px; width: 80ex'>Thompson sampling (TS) serves as a solution for addressing the
exploitation-exploration dilemma in Bayesian optimization (BO). While it
prioritizes exploration by randomly generating and maximizing sample paths of
Gaussian process (GP) posteriors, TS weakly manages its exploitation by
gathering information about the true objective function after each exploration
is performed. In this study, we incorporate the epsilon-greedy
($\varepsilon$-greedy) policy, a well-established selection strategy in
reinforcement learning, into TS to improve its exploitation. We first delineate
two extremes of TS applied for BO, namely the generic TS and a sample-average
TS. The former and latter promote exploration and exploitation, respectively.
We then use $\varepsilon$-greedy policy to randomly switch between the two
extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation,
and vice versa. We empirically show that $\varepsilon$-greedy TS with an
appropriate $\varepsilon$ is better than one of its two extremes and competes
with the other.</div><div><a href='http://arxiv.org/abs/2403.00540v1'>2403.00540v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17730v1")'>Markovletics: Methods and A Novel Application for Learning
  Continuous-Time Markov Chain Mixtures</div>
<div id='2402.17730v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:04:59Z</div><div>Authors: Fabian Spaeh, Charalampos E. Tsourakakis</div><div style='padding-top: 10px; width: 80ex'>Sequential data naturally arises from user engagement on digital platforms
like social media, music streaming services, and web navigation, encapsulating
evolving user preferences and behaviors through continuous information streams.
A notable unresolved query in stochastic processes is learning mixtures of
continuous-time Markov chains (CTMCs). While there is progress in learning
mixtures of discrete-time Markov chains with recovery guarantees
[GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored
challenges. The intrigue in CTMC mixtures stems from their potential to model
intricate continuous-time stochastic processes prevalent in various fields
including social media, finance, and biology.
  In this study, we introduce a novel framework for exploring CTMCs,
emphasizing the influence of observed trails' length and mixture parameters on
problem regimes, which demands specific algorithms. Through thorough
experimentation, we examine the impact of discretizing continuous-time trails
on the learnability of the continuous-time mixture, given that these processes
are often observed via discrete, resource-demanding observations. Our
comparative analysis with leading methods explores sample complexity and the
trade-off between the number of trails and their lengths, offering crucial
insights for method selection in different problem instances. We apply our
algorithms on an extensive collection of Lastfm's user-generated trails
spanning three years, demonstrating the capability of our algorithms to
differentiate diverse user preferences. We pioneer the use of CTMC mixtures on
a basketball passing dataset to unveil intricate offensive tactics of NBA
teams. This underscores the pragmatic utility and versatility of our proposed
framework. All results presented in this study are replicable, and we provide
the implementations to facilitate reproducibility.</div><div><a href='http://arxiv.org/abs/2402.17730v1'>2402.17730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12764v3")'>Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving
  $O(1/k)$ Finite-Sample Complexity</div>
<div id='2401.12764v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T13:44:15Z</div><div>Authors: Thinh T. Doan</div><div style='padding-top: 10px; width: 80ex'>This paper proposes to develop a new variant of the two-time-scale stochastic
approximation to find the roots of two coupled nonlinear operators, assuming
only noisy samples of these operators can be observed. Our key idea is to
leverage the classic Ruppert-Polyak averaging technique to dynamically estimate
the operators through their samples. The estimated values of these averaging
steps will then be used in the two-time-scale stochastic approximation updates
to find the desired solution. Our main theoretical result is to show that under
the strongly monotone condition of the underlying nonlinear operators the
mean-squared errors of the iterates generated by the proposed method converge
to zero at an optimal rate $O(1/k)$, where $k$ is the number of iterations. Our
result significantly improves the existing result of two-time-scale stochastic
approximation, where the best known finite-time convergence rate is
$O(1/k^{2/3})$. We illustrate this result by applying the proposed method to
develop new reinforcement learning algorithms with improved performance.</div><div><a href='http://arxiv.org/abs/2401.12764v3'>2401.12764v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02476v1")'>A Simple Finite-Time Analysis of TD Learning with Linear Function
  Approximation</div>
<div id='2403.02476v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T20:40:02Z</div><div>Authors: Aritra Mitra</div><div style='padding-top: 10px; width: 80ex'>We study the finite-time convergence of TD learning with linear function
approximation under Markovian sampling. Existing proofs for this setting either
assume a projection step in the algorithm to simplify the analysis, or require
a fairly intricate argument to ensure stability of the iterates. We ask:
\textit{Is it possible to retain the simplicity of a projection-based analysis
without actually performing a projection step in the algorithm?} Our main
contribution is to show this is possible via a novel two-step argument. In the
first step, we use induction to prove that under a standard choice of a
constant step-size $\alpha$, the iterates generated by TD learning remain
uniformly bounded in expectation. In the second step, we establish a recursion
that mimics the steady-state dynamics of TD learning up to a bounded
perturbation on the order of $O(\alpha^2)$ that captures the effect of
Markovian sampling. Combining these pieces leads to an overall approach that
considerably simplifies existing proofs. We conjecture that our inductive proof
technique will find applications in the analyses of more complex stochastic
approximation algorithms, and conclude by providing some examples of such
applications.</div><div><a href='http://arxiv.org/abs/2403.02476v1'>2403.02476v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13884v1")'>Constant Stepsize Q-learning: Distributional Convergence, Bias and
  Extrapolation</div>
<div id='2401.13884v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T02:01:53Z</div><div>Authors: Yixuan Zhang, Qiaomin Xie</div><div style='padding-top: 10px; width: 80ex'>Stochastic Approximation (SA) is a widely used algorithmic approach in
various fields, including optimization and reinforcement learning (RL). Among
RL algorithms, Q-learning is particularly popular due to its empirical success.
In this paper, we study asynchronous Q-learning with constant stepsize, which
is commonly used in practice for its fast convergence. By connecting the
constant stepsize Q-learning to a time-homogeneous Markov chain, we show the
distributional convergence of the iterates in Wasserstein distance and
establish its exponential convergence rate. We also establish a Central Limit
Theory for Q-learning iterates, demonstrating the asymptotic normality of the
averaged iterates. Moreover, we provide an explicit expansion of the asymptotic
bias of the averaged iterate in stepsize. Specifically, the bias is
proportional to the stepsize up to higher-order terms and we provide an
explicit expression for the linear coefficient. This precise characterization
of the bias allows the application of Richardson-Romberg (RR) extrapolation
technique to construct a new estimate that is provably closer to the optimal Q
function. Numerical results corroborate our theoretical finding on the
improvement of the RR extrapolation method.</div><div><a href='http://arxiv.org/abs/2401.13884v1'>2401.13884v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06366v1")'>Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach</div>
<div id='2403.06366v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T01:36:37Z</div><div>Authors: Narim Jeong, Donghwan Lee</div><div style='padding-top: 10px; width: 80ex'>Soft Q-learning is a variation of Q-learning designed to solve entropy
regularized Markov decision problems where an agent aims to maximize the
entropy regularized value function. Despite its empirical success, there have
been limited theoretical studies of soft Q-learning to date. This paper aims to
offer a novel and unified finite-time, control-theoretic analysis of soft
Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one
utilizing the log-sum-exp operator and the other employing the Boltzmann
operator. By using dynamical switching system models, we derive novel
finite-time error bounds for both soft Q-learning algorithms. We hope that our
analysis will deepen the current understanding of soft Q-learning by
establishing connections with switching system models and may even pave the way
for new frameworks in the finite-time analysis of other reinforcement learning
algorithms.</div><div><a href='http://arxiv.org/abs/2403.06366v1'>2403.06366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07136v1")'>On the Limited Representational Power of Value Functions and its Links
  to Statistical (In)Efficiency</div>
<div id='2403.07136v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:05:48Z</div><div>Authors: David Cheikhi, Daniel Russo</div><div style='padding-top: 10px; width: 80ex'>Identifying the trade-offs between model-based and model-free methods is a
central question in reinforcement learning. Value-based methods offer
substantial computational advantages and are sometimes just as statistically
efficient as model-based methods. However, focusing on the core problem of
policy evaluation, we show information about the transition dynamics may be
impossible to represent in the space of value functions. We explore this
through a series of case studies focused on structures that arises in many
important problems. In several, there is no information loss and value-based
methods are as statistically efficient as model based ones. In other
closely-related examples, information loss is severe and value-based methods
are severely outperformed. A deeper investigation points to the limitations of
the representational power as the driver of the inefficiency, as opposed to
failure in algorithm design.</div><div><a href='http://arxiv.org/abs/2403.07136v1'>2403.07136v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03590v1")'>Assessing the Impact of Distribution Shift on Reinforcement Learning
  Performance</div>
<div id='2402.03590v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T23:50:55Z</div><div>Authors: Ted Fujimoto, Joshua Suetterlein, Samrat Chatterjee, Auroop Ganguly</div><div style='padding-top: 10px; width: 80ex'>Research in machine learning is making progress in fixing its own
reproducibility crisis. Reinforcement learning (RL), in particular, faces its
own set of unique challenges. Comparison of point estimates, and plots that
show successful convergence to the optimal policy during training, may
obfuscate overfitting or dependence on the experimental setup. Although
researchers in RL have proposed reliability metrics that account for
uncertainty to better understand each algorithm's strengths and weaknesses, the
recommendations of past work do not assume the presence of out-of-distribution
observations. We propose a set of evaluation methods that measure the
robustness of RL algorithms under distribution shifts. The tools presented here
argue for the need to account for performance over time while the agent is
acting in its environment. In particular, we recommend time series analysis as
a method of observational RL evaluation. We also show that the unique
properties of RL and simulated dynamic environments allow us to make stronger
assumptions to justify the measurement of causal impact in our evaluations. We
then apply these tools to single-agent and multi-agent environments to show the
impact of introducing distribution shifts during test time. We present this
methodology as a first step toward rigorous RL evaluation in the presence of
distribution shifts.</div><div><a href='http://arxiv.org/abs/2402.03590v1'>2402.03590v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03046v1")'>Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement
  Learning</div>
<div id='2402.03046v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:32:00Z</div><div>Authors: Shengyi Huang, Quentin Gallou√©dec, Florian Felten, Antonin Raffin, Rousslan Fernand Julien Dossa, Yanxiao Zhao, Ryan Sullivan, Viktor Makoviychuk, Denys Makoviichuk, Mohamad H. Danesh, Cyril Roum√©gous, Jiayi Weng, Chufan Chen, Md Masudur Rahman, Jo√£o G. M. Ara√∫jo, Guorui Quan, Daniel Tan, Timo Klein, Rujikorn Charakorn, Mark Towers, Yann Berthelot, Kinal Mehta, Dipam Chakraborty, Arjun KG, Valentin Charraut, Chang Ye, Zichen Liu, Lucas N. Alegre, Alexander Nikulin, Xiao Hu, Tianlin Liu, Jongwook Choi, Brent Yi</div><div style='padding-top: 10px; width: 80ex'>In many Reinforcement Learning (RL) papers, learning curves are useful
indicators to measure the effectiveness of RL algorithms. However, the complete
raw data of the learning curves are rarely available. As a result, it is
usually necessary to reproduce the experiments from scratch, which can be
time-consuming and error-prone. We present Open RL Benchmark, a set of fully
tracked RL experiments, including not only the usual data such as episodic
return, but also all algorithm-specific and system metrics. Open RL Benchmark
is community-driven: anyone can download, use, and contribute to the data. At
the time of writing, more than 25,000 runs have been tracked, for a cumulative
duration of more than 8 years. Open RL Benchmark covers a wide range of RL
libraries and reference implementations. Special care is taken to ensure that
each experiment is precisely reproducible by providing not only the full
parameters, but also the versions of the dependencies used to generate it. In
addition, Open RL Benchmark comes with a command-line interface (CLI) for easy
fetching and generating figures to present the results. In this document, we
include two case studies to demonstrate the usefulness of Open RL Benchmark in
practice. To the best of our knowledge, Open RL Benchmark is the first RL
benchmark of its kind, and the authors hope that it will improve and facilitate
the work of researchers in the field.</div><div><a href='http://arxiv.org/abs/2402.03046v1'>2402.03046v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02423v1")'>Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement
  Learning with Diverse Human Feedback</div>
<div id='2402.02423v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:40:22Z</div><div>Authors: Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, Yan Zheng</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning with Human Feedback (RLHF) has received significant
attention for performing tasks without the need for costly manual reward design
by aligning human preferences. It is crucial to consider diverse human feedback
types and various learning methods in different environments. However,
quantifying progress in RLHF with diverse feedback is challenging due to the
lack of standardized annotation platforms and widely used unified benchmarks.
To bridge this gap, we introduce Uni-RLHF, a comprehensive system
implementation tailored for RLHF. It aims to provide a complete workflow from
real human feedback, fostering progress in the development of practical
problems. Uni-RLHF contains three packages: 1) a universal multi-feedback
annotation platform, 2) large-scale crowdsourced feedback datasets, and 3)
modular offline RLHF baseline implementations. Uni-RLHF develops a
user-friendly annotation interface tailored to various feedback types,
compatible with a wide range of mainstream RL environments. We then establish a
systematic pipeline of crowdsourced annotations, resulting in large-scale
annotated datasets comprising more than 15 million steps across 30+ popular
tasks. Through extensive experiments, the results in the collected datasets
demonstrate competitive performance compared to those from well-designed manual
rewards. We evaluate various design choices and offer insights into their
strengths and potential areas of improvement. We wish to build valuable
open-source platforms, datasets, and baselines to facilitate the development of
more robust and reliable RLHF solutions based on realistic human feedback. The
website is available at https://uni-rlhf.github.io/.</div><div><a href='http://arxiv.org/abs/2402.02423v1'>2402.02423v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15164v1")'>EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning
  Based Recommender Systems</div>
<div id='2402.15164v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T07:54:26Z</div><div>Authors: Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian Chen, Weizhi Ma, Min Zhang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly
recognized for their ability to improve long-term user engagement. Yet, the
field grapples with challenges such as the absence of accessible frameworks,
inconsistent evaluation standards, and the complexity of replicating prior
work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and
efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight,
diverse RL environments built on five widely-used public datasets, and is
equipped with comprehensive core modules that offer rich options to ease the
development of models. It establishes consistent evaluation criteria with a
focus on long-term impacts and introduces customized solutions for state
modeling and action representation tailored to recommender systems.
Additionally, we share valuable insights gained from extensive experiments with
current methods. EasyRL4Rec aims to facilitate the model development and
experimental process in the domain of RL-based RSs. The library is openly
accessible at https://github.com/chongminggao/EasyRL4Rec.</div><div><a href='http://arxiv.org/abs/2402.15164v1'>2402.15164v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05864v1")'>PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for
  Reinforcement Learning in Human-in-the-Loop Systems</div>
<div id='2403.05864v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T10:24:12Z</div><div>Authors: Mojtaba Taherisadr, Salma Elmalaki</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning (RL) has increasingly become a preferred method over
traditional rule-based systems in diverse human-in-the-loop (HITL) applications
due to its adaptability to the dynamic nature of human interactions. However,
integrating RL in such settings raises significant privacy concerns, as it
might inadvertently expose sensitive user information. Addressing this, our
paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy
through exploiting an early-exit approach designed explicitly for privacy
preservation in HITL environments. This approach dynamically adjusts the
tradeoff between privacy protection and system utility, tailoring its operation
to individual behavioral patterns and preferences. We mainly highlight the
challenge of dealing with the variable and evolving nature of human behavior,
which renders static privacy models ineffective. PAPER-HILT's effectiveness is
evaluated through its application in two distinct contexts: Smart Home
environments and Virtual Reality (VR) Smart Classrooms. The empirical results
demonstrate PAPER-HILT's capability to provide a personalized equilibrium
between user privacy and application utility, adapting effectively to
individual user needs and preferences. On average for both experiments, utility
(performance) drops by 24%, and privacy (state prediction) improves by 31%.</div><div><a href='http://arxiv.org/abs/2403.05864v1'>2403.05864v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11367v1")'>Multi Task Inverse Reinforcement Learning for Common Sense Reward</div>
<div id='2402.11367v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T19:49:00Z</div><div>Authors: Neta Glazer, Aviv Navon, Aviv Shamsian, Ethan Fetaya</div><div style='padding-top: 10px; width: 80ex'>One of the challenges in applying reinforcement learning in a complex
real-world environment lies in providing the agent with a sufficiently detailed
reward function. Any misalignment between the reward and the desired behavior
can result in unwanted outcomes. This may lead to issues like "reward hacking"
where the agent maximizes rewards by unintended behavior. In this work, we
propose to disentangle the reward into two distinct parts. A simple
task-specific reward, outlining the particulars of the task at hand, and an
unknown common-sense reward, indicating the expected behavior of the agent
within the environment. We then explore how this common-sense reward can be
learned from expert demonstrations. We first show that inverse reinforcement
learning, even when it succeeds in training an agent, does not learn a useful
reward function. That is, training a new agent with the learned reward does not
impair the desired behaviors. We then demonstrate that this problem can be
solved by training simultaneously on multiple tasks. That is, multi-task
inverse reinforcement learning can be applied to learn a useful reward
function.</div><div><a href='http://arxiv.org/abs/2402.11367v1'>2402.11367v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07411v1")'>Potential-Based Reward Shaping For Intrinsic Motivation</div>
<div id='2402.07411v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T05:12:09Z</div><div>Authors: Grant C. Forbes, Nitish Gupta, Leonardo Villalobos-Arias, Colin M. Potts, Arnav Jhala, David L. Roberts</div><div style='padding-top: 10px; width: 80ex'>Recently there has been a proliferation of intrinsic motivation (IM)
reward-shaping methods to learn in complex and sparse-reward environments.
These methods can often inadvertently change the set of optimal policies in an
environment, leading to suboptimal behavior. Previous work on mitigating the
risks of reward shaping, particularly through potential-based reward shaping
(PBRS), has not been applicable to many IM methods, as they are often complex,
trainable functions themselves, and therefore dependent on a wider set of
variables than the traditional reward functions that PBRS was developed for. We
present an extension to PBRS that we prove preserves the set of optimal
policies under a more general set of functions than has been previously proven.
We also present {\em Potential-Based Intrinsic Motivation} (PBIM), a method for
converting IM rewards into a potential-based form that is useable without
altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff
Walking environments, we demonstrate that PBIM successfully prevents the agent
from converging to a suboptimal policy and can speed up training.</div><div><a href='http://arxiv.org/abs/2402.07411v1'>2402.07411v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00344v1")'>Robustifying a Policy in Multi-Agent RL with Diverse Cooperative
  Behavior and Adversarial Style Sampling for Assistive Tasks</div>
<div id='2403.00344v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T08:15:18Z</div><div>Authors: Tayuki Osa, Tatsuya Harada</div><div style='padding-top: 10px; width: 80ex'>Autonomous assistance of people with motor impairments is one of the most
promising applications of autonomous robotic systems. Recent studies have
reported encouraging results using deep reinforcement learning (RL) in the
healthcare domain. Previous studies showed that assistive tasks can be
formulated as multi-agent RL, wherein there are two agents: a caregiver and a
care-receiver. However, policies trained in multi-agent RL are often sensitive
to the policies of other agents. In such a case, a trained caregiver's policy
may not work for different care-receivers. To alleviate this issue, we propose
a framework that learns a robust caregiver's policy by training it for diverse
care-receiver responses. In our framework, diverse care-receiver responses are
autonomously learned through trials and errors. In addition, to robustify the
care-giver's policy, we propose a strategy for sampling a care-receiver's
response in an adversarial manner during the training. We evaluated the
proposed method using tasks in an Assistive Gym. We demonstrate that policies
trained with a popular deep RL method are vulnerable to changes in policies of
other agents and that the proposed framework improves the robustness against
such changes.</div><div><a href='http://arxiv.org/abs/2403.00344v1'>2403.00344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10941v1")'>Crowd-PrefRL: Preference-Based Reward Learning from Crowds</div>
<div id='2401.10941v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T18:06:17Z</div><div>Authors: David Chhan, Ellen Novoseller, Vernon J. Lawhern</div><div style='padding-top: 10px; width: 80ex'>Preference-based reinforcement learning (RL) provides a framework to train
agents using human feedback through pairwise preferences over pairs of
behaviors, enabling agents to learn desired behaviors when it is difficult to
specify a numerical reward function. While this paradigm leverages human
feedback, it currently treats the feedback as given by a single human user.
Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of
users) in a robust manner remains a challenge, and the problem of training RL
agents using feedback from multiple human users remains understudied. In this
work, we introduce Crowd-PrefRL, a framework for performing preference-based RL
leveraging feedback from crowds. This work demonstrates the viability of
learning reward functions from preference feedback provided by crowds of
unknown expertise and reliability. Crowd-PrefRL not only robustly aggregates
the crowd preference feedback, but also estimates the reliability of each user
within the crowd using only the (noisy) crowdsourced preference comparisons.
Most importantly, we show that agents trained with Crowd-PrefRL outperform
agents trained with majority-vote preferences or preferences from any
individual user in most cases, especially when the spread of user error rates
among the crowd is large. Results further suggest that our method can identify
minority viewpoints within the crowd.</div><div><a href='http://arxiv.org/abs/2401.10941v1'>2401.10941v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15480v1")'>Social Interpretable Reinforcement Learning</div>
<div id='2401.15480v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T19:05:21Z</div><div>Authors: Leonardo Lucio Custode, Giovanni Iacca</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning (RL) bears the promise of being an enabling technology
for many applications. However, since most of the literature in the field is
currently focused on opaque models, the use of RL in high-stakes scenarios,
where interpretability is crucial, is still limited. Recently, some approaches
to interpretable RL, e.g., based on Decision Trees, have been proposed, but one
of the main limitations of these techniques is their training cost. To overcome
this limitation, we propose a new population-based method, called Social
Interpretable RL (SIRL), inspired by social learning principles, to improve
learning efficiency. Our method mimics a social learning process, where each
agent in a group learns to solve a given task based both on its own individual
experience as well as the experience acquired together with its peers. Our
approach is divided into two phases. In the \emph{collaborative phase}, all the
agents in the population interact with a shared instance of the environment,
where each agent observes the state and independently proposes an action. Then,
voting is performed to choose the action that will actually be performed in the
environment. In the \emph{individual phase}, each agent refines its individual
performance by interacting with its own instance of the environment. This
mechanism makes the agents experience a larger number of episodes while
simultaneously reducing the computational cost of the process. Our results on
six well-known benchmarks show that SIRL reaches state-of-the-art performance
w.r.t. the alternative interpretable methods from the literature.</div><div><a href='http://arxiv.org/abs/2401.15480v1'>2401.15480v1</a></div>
</div></div>
    <div><a href="arxiv_11.html">Prev (11)</a></div>
    <div><a href="arxiv_13.html">Next (13)</a></div>
    