
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00086v1")'>Retrosynthesis prediction enhanced by in-silico reaction data
  augmentation</div>
<div id='2402.00086v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T07:40:37Z</div><div>Authors: Xu Zhang, Yiming Mo, Wenguan Wang, Yi Yang</div><div style='padding-top: 10px; width: 80ex'>Recent advances in machine learning (ML) have expedited retrosynthesis
research by assisting chemists to design experiments more efficiently. However,
all ML-based methods consume substantial amounts of paired training data (i.e.,
chemical reaction: product-reactant(s) pair), which is costly to obtain.
Moreover, companies view reaction data as a valuable asset and restrict the
accessibility to researchers. These issues prevent the creation of more
powerful retrosynthesis models due to their data-driven nature. As a response,
we exploit easy-to-access unpaired data (i.e., one component of
product-reactant(s) pair) for generating in-silico paired data to facilitate
model training. Specifically, we present RetroWISE, a self-boosting framework
that employs a base model inferred from real paired data to perform in-silico
reaction generation and augmentation using unpaired data, ultimately leading to
a superior model. On three benchmark datasets, RetroWISE achieves the best
overall performance against state-of-the-art models (e.g., +8.6% top-1 accuracy
on the USPTO-50K test dataset). Moreover, it consistently improves the
prediction accuracy of rare transformations. These results show that Retro-
WISE overcomes the training bottleneck by in-silico reactions, thereby paving
the way toward more effective ML-based retrosynthesis models.</div><div><a href='http://arxiv.org/abs/2402.00086v1'>2402.00086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02765v1")'>G4-Attention: Deep Learning Model with Attention for predicting DNA
  G-Quadruplexes</div>
<div id='2403.02765v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:34:04Z</div><div>Authors: Shrimon Mukherjee, Pulakesh Pramanik, Partha Basuchowdhuri, Santanu Bhattacharya</div><div style='padding-top: 10px; width: 80ex'>G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary
structures, formed by the stacking arrangement of the guanine tetramers. They
are involved in a wide range of biological roles because of their exceptionally
unique and distinct structural characteristics. After the completion of the
human genome sequencing project, a lot of bioinformatic algorithms were
introduced to predict the active G4s regions \textit{in vitro} based on the
canonical G4 sequence elements, G-\textit{richness}, and G-\textit{skewness},
as well as the non-canonical sequence features. Recently, sequencing techniques
like G4-seq and G4-ChIP-seq were developed to map the G4s \textit{in vitro},
and \textit{in vivo} respectively at a few hundred base resolution.
Subsequently, several machine learning approaches were developed for predicting
the G4 regions using the existing databases. However, their prediction models
were simplistic, and the prediction accuracy was notably poor. In response,
here, we propose a novel convolutional neural network with Bi-LSTM and
attention layers, named G4-attention, to predict the G4 forming sequences with
improved accuracy. G4-attention achieves high accuracy and attains
state-of-the-art results in the G4 prediction task. Our model also predicts the
G4 regions accurately in the highly class-imbalanced datasets. In addition, the
developed model trained on the human genome dataset can be applied to any
non-human genome DNA sequences to predict the G4 formation propensities.</div><div><a href='http://arxiv.org/abs/2403.02765v1'>2403.02765v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16299v1")'>Enhancing Molecular Property Prediction with Auxiliary Learning and
  Task-Specific Adaptation</div>
<div id='2401.16299v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:00:28Z</div><div>Authors: Vishal Dey, Xia Ning</div><div style='padding-top: 10px; width: 80ex'>Pretrained Graph Neural Networks have been widely adopted for various
molecular property prediction tasks. Despite their ability to encode structural
and relational features of molecules, traditional fine-tuning of such
pretrained GNNs on the target task can lead to poor generalization. To address
this, we explore the adaptation of pretrained GNNs to the target task by
jointly training them with multiple auxiliary tasks. This could enable the GNNs
to learn both general and task-specific features, which may benefit the target
task. However, a major challenge is to determine the relatedness of auxiliary
tasks with the target task. To address this, we investigate multiple strategies
to measure the relevance of auxiliary tasks and integrate such tasks by
adaptively combining task gradients or by learning task weights via bi-level
optimization. Additionally, we propose a novel gradient surgery-based approach,
Rotation of Conflicting Gradients ($\mathtt{RCGrad}$), that learns to align
conflicting auxiliary task gradients through rotation. Our experiments with
state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed
methods, with improvements of up to 7.7% over fine-tuning. This suggests that
incorporating auxiliary tasks along with target task fine-tuning can be an
effective way to improve the generalizability of pretrained GNNs for molecular
property prediction.</div><div><a href='http://arxiv.org/abs/2401.16299v1'>2401.16299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15921v1")'>Pretraining Strategy for Neural Potentials</div>
<div id='2402.15921v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T22:32:34Z</div><div>Authors: Zehua Zhang, Zijie Li, Amir Barati Farimani</div><div style='padding-top: 10px; width: 80ex'>We propose a mask pretraining method for Graph Neural Networks (GNNs) to
improve their performance on fitting potential energy surfaces, particularly in
water systems. GNNs are pretrained by recovering spatial information related to
masked-out atoms from molecules, then transferred and finetuned on atomic
forcefields. Through such pretraining, GNNs learn meaningful prior about
structural and underlying physical information of molecule systems that are
useful for downstream tasks. From comprehensive experiments and ablation
studies, we show that the proposed method improves the accuracy and convergence
speed compared to GNNs trained from scratch or using other pretraining
techniques such as denoising. On the other hand, our pretraining method is
suitable for both energy-centric and force-centric GNNs. This approach
showcases its potential to enhance the performance and data efficiency of GNNs
in fitting molecular force fields.</div><div><a href='http://arxiv.org/abs/2402.15921v1'>2402.15921v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09549v1")'>Generalizing Denoising to Non-Equilibrium Structures Improves
  Equivariant Force Fields</div>
<div id='2403.09549v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T16:38:02Z</div><div>Authors: Yi-Lun Liao, Tess Smidt, Abhishek Das</div><div style='padding-top: 10px; width: 80ex'>Understanding the interactions of atoms such as forces in 3D atomistic
systems is fundamental to many applications like molecular dynamics and
catalyst design. However, simulating these interactions requires
compute-intensive ab initio calculations and thus results in limited data for
training neural networks. In this paper, we propose to use denoising
non-equilibrium structures (DeNS) as an auxiliary task to better leverage
training data and improve performance. For training with DeNS, we first corrupt
a 3D structure by adding noise to its 3D coordinates and then predict the
noise. Different from previous works on denoising, which are limited to
equilibrium structures, the proposed method generalizes denoising to a much
larger set of non-equilibrium structures. The main difference is that a
non-equilibrium structure does not correspond to local energy minima and has
non-zero forces, and therefore it can have many possible atomic positions
compared to an equilibrium structure. This makes denoising non-equilibrium
structures an ill-posed problem since the target of denoising is not uniquely
defined. Our key insight is to additionally encode the forces of the original
non-equilibrium structure to specify which non-equilibrium structure we are
denoising. Concretely, given a corrupted non-equilibrium structure and the
forces of the original one, we predict the non-equilibrium structure satisfying
the input forces instead of any arbitrary structures. Since DeNS requires
encoding forces, DeNS favors equivariant networks, which can easily incorporate
forces and other higher-order tensors in node embeddings. We study the
effectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17
datasets and demonstrate that DeNS can achieve new state-of-the-art results on
OC20 and OC22 and significantly improve training efficiency on MD17.</div><div><a href='http://arxiv.org/abs/2403.09549v1'>2403.09549v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02683v1")'>Geometric-Facilitated Denoising Diffusion Model for 3D Molecule
  Generation</div>
<div id='2401.02683v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T07:29:21Z</div><div>Authors: Can Xu, Haosen Wang, Weigang Wang, Pengfei Zheng, Hongyang Chen</div><div style='padding-top: 10px; width: 80ex'>Denoising diffusion models have shown great potential in multiple research
areas. Existing diffusion-based generative methods on de novo 3D molecule
generation face two major challenges. Since majority heavy atoms in molecules
allow connections to multiple atoms through single bonds, solely using
pair-wise distance to model molecule geometries is insufficient. Therefore, the
first one involves proposing an effective neural network as the denoising
kernel that is capable to capture complex multi-body interatomic relationships
and learn high-quality features. Due to the discrete nature of graphs,
mainstream diffusion-based methods for molecules heavily rely on predefined
rules and generate edges in an indirect manner. The second challenge involves
accommodating molecule generation to diffusion and accurately predicting the
existence of bonds. In our research, we view the iterative way of updating
molecule conformations in diffusion process is consistent with molecular
dynamics and introduce a novel molecule generation method named
Geometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,
we introduce a Dual-Track Transformer Network (DTN) to fully excevate global
spatial relationships and learn high quality representations which contribute
to accurate predictions of features and geometries. As for the second
challenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the
formation of bonds during the training period, instead of directly embedding
edges into the latent space. Comprehensive experiments on current benchmarks
demonstrate the superiority of GFMDiff.</div><div><a href='http://arxiv.org/abs/2401.02683v1'>2401.02683v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13858v1")'>Inverse Molecular Design with Multi-Conditional Diffusion Guidance</div>
<div id='2401.13858v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T23:45:31Z</div><div>Authors: Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang</div><div style='padding-top: 10px; width: 80ex'>Inverse molecular design with diffusion models holds great potential for
advancements in material and drug discovery. Despite success in unconditional
molecule generation, integrating multiple properties such as synthetic score
and gas permeability as condition constraints into diffusion models remains
unexplored. We introduce multi-conditional diffusion guidance. The proposed
Transformer-based denoising model has a condition encoder that learns the
representations of numerical and categorical conditions. The denoising model,
consisting of a structure encoder-decoder, is trained for denoising under the
representation of conditions. The diffusion process becomes graph-dependent to
accurately estimate graph-related noise in molecules, unlike the previous
models that focus solely on the marginal distributions of atoms or bonds. We
extensively validate our model for multi-conditional polymer and small molecule
generation. Results demonstrate our superiority across metrics from
distribution learning to condition control for molecular properties. An inverse
polymer design task for gas separation with feedback from domain experts
further demonstrates its practical utility.</div><div><a href='http://arxiv.org/abs/2401.13858v1'>2401.13858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13829v1")'>DecompOpt: Controllable and Decomposed Diffusion Models for
  Structure-based Molecular Optimization</div>
<div id='2403.13829v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T02:53:40Z</div><div>Authors: Xiangxin Zhou, Xiwei Cheng, Yuwei Yang, Yu Bao, Liang Wang, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>Recently, 3D generative models have shown promising performances in
structure-based drug design by learning to generate ligands given target
binding sites. However, only modeling the target-ligand distribution can hardly
fulfill one of the main goals in drug discovery -- designing novel ligands with
desired properties, e.g., high binding affinity, easily synthesizable, etc.
This challenge becomes particularly pronounced when the target-ligand pairs
used for training do not align with these desired properties. Moreover, most
existing methods aim at solving \textit{de novo} design task, while many
generative scenarios requiring flexible controllability, such as R-group
optimization and scaffold hopping, have received little attention. In this
work, we propose DecompOpt, a structure-based molecular optimization method
based on a controllable and decomposed diffusion model. DecompOpt presents a
new generation paradigm which combines optimization with conditional diffusion
models to achieve desired properties while adhering to the molecular grammar.
Additionally, DecompOpt offers a unified framework covering both \textit{de
novo} design and controllable generation. To achieve so, ligands are decomposed
into substructures which allows fine-grained control and local optimization.
Experiments show that DecompOpt can efficiently generate molecules with
improved properties than strong de novo baselines, and demonstrate great
potential in controllable generation tasks.</div><div><a href='http://arxiv.org/abs/2403.13829v1'>2403.13829v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14315v2")'>Structure-Based Drug Design via 3D Molecular Generative Pre-training and
  Sampling</div>
<div id='2402.14315v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T06:17:11Z</div><div>Authors: Yuwei Yang, Siqi Ouyang, Xueyu Hu, Mingyue Zheng, Hao Zhou, Lei Li</div><div style='padding-top: 10px; width: 80ex'>Structure-based drug design aims at generating high affinity ligands with
prior knowledge of 3D target structures. Existing methods either use
conditional generative model to learn the distribution of 3D ligands given
target binding sites, or iteratively modify molecules to optimize a
structure-based activity estimator. The former is highly constrained by data
quantity and quality, which leaves optimization-based approaches more promising
in practical scenario. However, existing optimization-based approaches choose
to edit molecules in 2D space, and use molecular docking to estimate the
activity using docking predicted 3D target-ligand complexes. The misalignment
between the action space and the objective hinders the performance of these
models, especially for those employ deep learning for acceleration. In this
work, we propose MolEdit3D to combine 3D molecular generation with optimization
frameworks. We develop a novel 3D graph editing model to generate molecules
using fragments, and pre-train this model on abundant 3D ligands for learning
target-independent properties. Then we employ a target-guided self-learning
strategy to improve target-related properties using self-sampled molecules.
MolEdit3D achieves state-of-the-art performance on majority of the evaluation
metrics, and demonstrate strong capability of capturing both target-dependent
and -independent properties.</div><div><a href='http://arxiv.org/abs/2402.14315v2'>2402.14315v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08703v1")'>A Survey of Generative AI for De Novo Drug Design: New Frontiers in
  Molecule and Protein Generation</div>
<div id='2402.08703v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:56:31Z</div><div>Authors: Xiangru Tang, Howard Dai, Elizabeth Knight, Fang Wu, Yunyang Li, Tianxiao Li, Mark Gerstein</div><div style='padding-top: 10px; width: 80ex'>Artificial intelligence (AI)-driven methods can vastly improve the
historically costly drug design process, with various generative models already
in widespread use. Generative models for de novo drug design, in particular,
focus on the creation of novel biological compounds entirely from scratch,
representing a promising future direction. Rapid development in the field,
combined with the inherent complexity of the drug design process, creates a
difficult landscape for new researchers to enter. In this survey, we organize
de novo drug design into two overarching themes: small molecule and protein
generation. Within each theme, we identify a variety of subtasks and
applications, highlighting important datasets, benchmarks, and model
architectures and comparing the performance of top models. We take a broad
approach to AI-driven drug design, allowing for both micro-level comparisons of
various methods within each subtask and macro-level observations across
different fields. We discuss parallel challenges and approaches between the two
applications and highlight future directions for AI-driven de novo drug design
as a whole. An organized repository of all covered sources is available at
https://github.com/gersteinlab/GenAI4Drug.</div><div><a href='http://arxiv.org/abs/2402.08703v1'>2402.08703v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13005v1")'>Leap: molecular synthesisability scoring with intermediates</div>
<div id='2403.13005v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T11:53:35Z</div><div>Authors: Antonia Calvi, Théophile Gaudin, Dominik Miketa, Dominique Sydow, Liam Wilbraham</div><div style='padding-top: 10px; width: 80ex'>Assessing whether a molecule can be synthesised is a primary task in drug
discovery. It enables computational chemists to filter for viable compounds or
bias molecular generative models. The notion of synthesisability is dynamic as
it evolves depending on the availability of key compounds. A common approach in
drug discovery involves exploring the chemical space surrounding
synthetically-accessible intermediates. This strategy improves the
synthesisability of the derived molecules due to the availability of key
intermediates. Existing synthesisability scoring methods such as SAScore,
SCScore and RAScore, cannot condition on intermediates dynamically. Our
approach, Leap, is a GPT-2 model trained on the depth, or longest linear path,
of predicted synthesis routes that allows information on the availability of
key intermediates to be included at inference time. We show that Leap surpasses
all other scoring methods by at least 5% on AUC score when identifying
synthesisable molecules, and can successfully adapt predicted scores when
presented with a relevant intermediate compound.</div><div><a href='http://arxiv.org/abs/2403.13005v1'>2403.13005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03960v1")'>Assessing the Extrapolation Capability of Template-Free Retrosynthesis
  Models</div>
<div id='2403.03960v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T00:48:17Z</div><div>Authors: Shuan Chen, Yousung Jung</div><div style='padding-top: 10px; width: 80ex'>Despite the acknowledged capability of template-free models in exploring
unseen reaction spaces compared to template-based models for retrosynthesis
prediction, their ability to venture beyond established boundaries remains
relatively uncharted. In this study, we empirically assess the extrapolation
capability of state-of-the-art template-free models by meticulously assembling
an extensive set of out-of-distribution (OOD) reactions. Our findings
demonstrate that while template-free models exhibit potential in predicting
precursors with novel synthesis rules, their top-10 exact-match accuracy in OOD
reactions is strikingly modest (&lt; 1%). Furthermore, despite the capability of
generating novel reactions, our investigation highlights a recurring issue
where more than half of the novel reactions predicted by template-free models
are chemically implausible. Consequently, we advocate for the future
development of template-free models that integrate considerations of chemical
feasibility when navigating unexplored regions of reaction space.</div><div><a href='http://arxiv.org/abs/2403.03960v1'>2403.03960v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10721v1")'>Generative Model for Constructing Reaction Path from Initial to Final
  States</div>
<div id='2401.10721v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T14:32:50Z</div><div>Authors: Akihide Hayashi, So Takamoto, Ju Li, Daisuke Okanohara</div><div style='padding-top: 10px; width: 80ex'>Mapping out reaction pathways and their corresponding activation barriers is
a significant aspect of molecular simulation. Given their inherent complexity
and nonlinearity, even generating a initial guess of these paths remains a
challenging problem. Presented in this paper is an innovative approach that
utilizes neural networks to generate initial guess for these reaction pathways.
The proposed method is initiated by inputting the coordinates of the initial
state, followed by progressive alterations to its structure. This iterative
process culminates in the generation of the approximate representation of the
reaction path and the coordinates of the final state. The application of this
method extends to complex reaction pathways illustrated by organic reactions.
Training was executed on the Transition1x dataset, an organic reaction pathway
dataset. The results revealed generation of reactions that bore substantial
similarities with the corresponding test data. The method's flexibility allows
for reactions to be generated either to conform to predetermined conditions or
in a randomized manner.</div><div><a href='http://arxiv.org/abs/2401.10721v1'>2401.10721v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08210v1")'>Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS</div>
<div id='2402.08210v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T04:19:06Z</div><div>Authors: Mohammad Ghazi Vakili, Christoph Gorgulla, AkshatKumar Nigam, Dmitry Bezrukov, Daniel Varoli, Alex Aliper, Daniil Polykovsky, Krishna M. Padmanabha Das, Jamie Snider, Anna Lyakisheva, Ardalan Hosseini Mansob, Zhong Yao, Lela Bitar, Eugene Radchenko, Xiao Ding, Jinxin Liu, Fanye Meng, Feng Ren, Yudong Cao, Igor Stagljar, Alán Aspuru-Guzik, Alex Zhavoronkov</div><div style='padding-top: 10px; width: 80ex'>The discovery of small molecules with therapeutic potential is a
long-standing challenge in chemistry and biology. Researchers have increasingly
leveraged novel computational techniques to streamline the drug development
process to increase hit rates and reduce the costs associated with bringing a
drug to market. To this end, we introduce a quantum-classical generative model
that seamlessly integrates the computational power of quantum algorithms
trained on a 16-qubit IBM quantum computer with the established reliability of
classical methods for designing small molecules. Our hybrid generative model
was applied to designing new KRAS inhibitors, a crucial target in cancer
therapy. We synthesized 15 promising molecules during our investigation and
subjected them to experimental testing to assess their ability to engage with
the target. Notably, among these candidates, two molecules, ISM061-018-2 and
ISM061-22, each featuring unique scaffolds, stood out by demonstrating
effective engagement with KRAS. ISM061-018-2 was identified as a broad-spectrum
KRAS inhibitor, exhibiting a binding affinity to KRAS-G12D at $1.4 \mu M$.
Concurrently, ISM061-22 exhibited specific mutant selectivity, displaying
heightened activity against KRAS G12R and Q61H mutants. To our knowledge, this
work shows for the first time the use of a quantum-generative model to yield
experimentally confirmed biological hits, showcasing the practical potential of
quantum-assisted drug discovery to produce viable therapeutics. Moreover, our
findings reveal that the efficacy of distribution learning correlates with the
number of qubits utilized, underlining the scalability potential of quantum
computing resources. Overall, we anticipate our results to be a stepping stone
towards developing more advanced quantum generative models in drug discovery.</div><div><a href='http://arxiv.org/abs/2402.08210v1'>2402.08210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07902v1")'>DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based
  Drug Design</div>
<div id='2403.07902v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T05:21:21Z</div><div>Authors: Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, Quanquan Gu</div><div style='padding-top: 10px; width: 80ex'>Designing 3D ligands within a target binding site is a fundamental task in
drug discovery. Existing structured-based drug design methods treat all ligand
atoms equally, which ignores different roles of atoms in the ligand for drug
design and can be less efficient for exploring the large drug-like molecule
space. In this paper, inspired by the convention in pharmaceutical practice, we
decompose the ligand molecule into two parts, namely arms and scaffold, and
propose a new diffusion model, DecompDiff, with decomposed priors over arms and
scaffold. In order to facilitate the decomposed generation and improve the
properties of the generated molecules, we incorporate both bond diffusion in
the model and additional validity guidance in the sampling phase. Extensive
experiments on CrossDocked2020 show that our approach achieves state-of-the-art
performance in generating high-affinity molecules while maintaining proper
molecular properties and conformational stability, with up to -8.39 Avg. Vina
Dock score and 24.5 Success Rate. The code is provided at
https://github.com/bytedance/DecompDiff</div><div><a href='http://arxiv.org/abs/2403.07902v1'>2403.07902v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18583v1")'>Binding-Adaptive Diffusion Models for Structure-Based Drug Design</div>
<div id='2402.18583v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T00:34:00Z</div><div>Authors: Zhilin Huang, Ling Yang, Zaixi Zhang, Xiangxin Zhou, Yu Bao, Xiawu Zheng, Yuwei Yang, Yu Wang, Wenming Yang</div><div style='padding-top: 10px; width: 80ex'>Structure-based drug design (SBDD) aims to generate 3D ligand molecules that
bind to specific protein targets. Existing 3D deep generative models including
diffusion models have shown great promise for SBDD. However, it is complex to
capture the essential protein-ligand interactions exactly in 3D space for
molecular generation. To address this problem, we propose a novel framework,
namely Binding-Adaptive Diffusion Models (BindDM). In BindDM, we adaptively
extract subcomplex, the essential part of binding sites responsible for
protein-ligand interactions. Then the selected protein-ligand subcomplex is
processed with SE(3)-equivariant neural networks, and transmitted back to each
atom of the complex for augmenting the target-aware 3D molecule diffusion
generation with binding interaction information. We iterate this hierarchical
complex-subcomplex process with cross-hierarchy interaction node for adequately
fusing global binding context between the complex and its corresponding
subcomplex. Empirical studies on the CrossDocked2020 dataset show BindDM can
generate molecules with more realistic 3D structures and higher binding
affinities towards the protein targets, with up to -5.92 Avg. Vina Score, while
maintaining proper molecular properties. Our code is available at
https://github.com/YangLing0818/BindDM</div><div><a href='http://arxiv.org/abs/2402.18583v1'>2402.18583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11459v2")'>Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion
  Bridge</div>
<div id='2402.11459v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T05:04:50Z</div><div>Authors: Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang Gao, Siyuan Li, Stan. Z. Li</div><div style='padding-top: 10px; width: 80ex'>Accurate prediction of protein-ligand binding structures, a task known as
molecular docking is crucial for drug design but remains challenging. While
deep learning has shown promise, existing methods often depend on holo-protein
structures (docked, and not accessible in realistic tasks) or neglect pocket
sidechain conformations, leading to limited practical utility and unrealistic
conformation predictions. To fill these gaps, we introduce an under-explored
task, named flexible docking to predict poses of ligand and pocket sidechains
simultaneously and introduce Re-Dock, a novel diffusion bridge generative model
extended to geometric manifolds. Specifically, we propose energy-to-geometry
mapping inspired by the Newton-Euler equation to co-model the binding energy
and conformations for reflecting the energy-constrained docking generative
process. Comprehensive experiments on designed benchmark datasets including
apo-dock and cross-dock demonstrate our model's superior effectiveness and
efficiency over current methods.</div><div><a href='http://arxiv.org/abs/2402.11459v2'>2402.11459v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17179v1")'>Dual-Space Optimization: Improved Molecule Sequence Design by Latent
  Prompt Transformer</div>
<div id='2402.17179v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T03:33:23Z</div><div>Authors: Deqian Kong, Yuhao Huang, Jianwen Xie, Edouardo Honig, Ming Xu, Shuanghong Xue, Pei Lin, Sanping Zhou, Sheng Zhong, Nanning Zheng, Ying Nian Wu</div><div style='padding-top: 10px; width: 80ex'>Designing molecules with desirable properties, such as drug-likeliness and
high binding affinities towards protein targets, is a challenging problem. In
this paper, we propose the Dual-Space Optimization (DSO) method that integrates
latent space sampling and data space selection to solve this problem. DSO
iteratively updates a latent space generative model and a synthetic dataset in
an optimization process that gradually shifts the generative model and the
synthetic data towards regions of desired property values. Our generative model
takes the form of a Latent Prompt Transformer (LPT) where the latent vector
serves as the prompt of a causal transformer. Our extensive experiments
demonstrate effectiveness of the proposed method, which sets new performance
benchmarks across single-objective, multi-objective and constrained molecule
design tasks.</div><div><a href='http://arxiv.org/abs/2402.17179v1'>2402.17179v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08708v1")'>Zero Shot Molecular Generation via Similarity Kernels</div>
<div id='2402.08708v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T17:53:44Z</div><div>Authors: Rokas Elijošius, Fabian Zills, Ilyes Batatia, Sam Walton Norwood, Dávid Péter Kovács, Christian Holm, Gábor Csányi</div><div style='padding-top: 10px; width: 80ex'>Generative modelling aims to accelerate the discovery of novel chemicals by
directly proposing structures with desirable properties. Recently, score-based,
or diffusion, generative models have significantly outperformed previous
approaches. Key to their success is the close relationship between the score
and physical force, allowing the use of powerful equivariant neural networks.
However, the behaviour of the learnt score is not yet well understood. Here, we
analyse the score by training an energy-based diffusion model for molecular
generation. We find that during the generation the score resembles a
restorative potential initially and a quantum-mechanical force at the end. In
between the two endpoints, it exhibits special properties that enable the
building of large molecules. Using insights from the trained model, we present
Similarity-based Molecular Generation (SiMGen), a new method for zero shot
molecular generation. SiMGen combines a time-dependent similarity kernel with
descriptors from a pretrained machine learning force field to generate
molecules without any further training. Our approach allows full control over
the molecular shape through point cloud priors and supports conditional
generation. We also release an interactive web tool that allows users to
generate structures with SiMGen online (https://zndraw.icp.uni-stuttgart.de).</div><div><a href='http://arxiv.org/abs/2402.08708v1'>2402.08708v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15864v1")'>Field-based Molecule Generation</div>
<div id='2402.15864v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T17:13:58Z</div><div>Authors: Alexandru Dumitrescu, Dani Korpela, Markus Heinonen, Yogesh Verma, Valerii Iakovlev, Vikas Garg, Harri Lähdesmäki</div><div style='padding-top: 10px; width: 80ex'>This work introduces FMG, a field-based model for drug-like molecule
generation. We show how the flexibility of this method provides crucial
advantages over the prevalent, point-cloud based methods, and achieves
competitive molecular stability generation. We tackle optical isomerism
(enantiomers), a previously omitted molecular property that is crucial for drug
safety and effectiveness, and thus account for all molecular geometry aspects.
We demonstrate how previous methods are invariant to a group of transformations
that includes enantiomer pairs, leading them invariant to the molecular R and S
configurations, while our field-based generative model captures this property.</div><div><a href='http://arxiv.org/abs/2402.15864v1'>2402.15864v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07925v2")'>Physics-informed generative model for drug-like molecule conformers</div>
<div id='2403.07925v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T17:11:08Z</div><div>Authors: David C. Williams, Neil Inala</div><div style='padding-top: 10px; width: 80ex'>We present a diffusion-based, generative model for conformer generation. Our
model is focused on the reproduction of bonded structure and is constructed
from the associated terms traditionally found in classical force fields to
ensure a physically relevant representation. Techniques in deep learning are
used to infer atom typing and geometric parameters from a training set.
Conformer sampling is achieved by taking advantage of recent advancements in
diffusion-based generation. By training on large, synthetic data sets of
diverse, drug-like molecules optimized with the semiempirical GFN2-xTB method,
high accuracy is achieved for bonded parameters, exceeding that of
conventional, knowledge-based methods. Results are also compared to
experimental structures from the Protein Databank (PDB) and Cambridge
Structural Database (CSD).</div><div><a href='http://arxiv.org/abs/2403.07925v2'>2403.07925v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16930v1")'>TrustMol: Trustworthy Inverse Molecular Design via Alignment with
  Molecular Dynamics</div>
<div id='2402.16930v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T15:14:38Z</div><div>Authors: Kevin Tirta Wijaya, Navid Ansari, Hans-Peter Seidel, Vahid Babaei</div><div style='padding-top: 10px; width: 80ex'>Data-driven generation of molecules with desired properties, also known as
inverse molecular design (IMD), has attracted significant attention in recent
years. Despite the significant progress in the accuracy and diversity of
solutions, existing IMD methods lag behind in terms of trustworthiness. The
root issue is that the design process of these methods is increasingly more
implicit and indirect, and this process is also isolated from the native
forward process (NFP), the ground-truth function that models the molecular
dynamics. Following this insight, we propose TrustMol, an IMD method built to
be trustworthy. For this purpose, TrustMol relies on a set of technical
novelties including a new variational autoencoder network. Moreover, we propose
a latent-property pairs acquisition method to effectively navigate the
complexities of molecular latent optimization, a process that seems intuitive
yet challenging due to the high-frequency and discontinuous nature of molecule
space. TrustMol also integrates uncertainty-awareness into molecular latent
optimization. These lead to improvements in both explainability and reliability
of the IMD process. We validate the trustworthiness of TrustMol through a wide
range of experiments.</div><div><a href='http://arxiv.org/abs/2402.16930v1'>2402.16930v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03425v1")'>Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for
  Text-Oriented Molecular Optimization</div>
<div id='2403.03425v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T03:15:25Z</div><div>Authors: Kaiwei Zhang, Yange Lin, Guangcheng Wu, Yuxiang Ren, Xuecang Zhang, Bo wang, Xiaoyu Zhang, Weitao Du</div><div style='padding-top: 10px; width: 80ex'>The integration of deep learning, particularly AI-Generated Content, with
high-quality data derived from ab initio calculations has emerged as a
promising avenue for transforming the landscape of scientific research.
However, the challenge of designing molecular drugs or materials that
incorporate multi-modality prior knowledge remains a critical and complex
undertaking. Specifically, achieving a practical molecular design necessitates
not only meeting the diversity requirements but also addressing structural and
textural constraints with various symmetries outlined by domain experts. In
this article, we present an innovative approach to tackle this inverse design
problem by formulating it as a multi-modality guidance generation/optimization
task. Our proposed solution involves a textural-structure alignment symmetric
diffusion framework for the implementation of molecular generation/optimization
tasks, namely 3DToMolo. 3DToMolo aims to harmonize diverse modalities, aligning
them seamlessly to produce molecular structures adhere to specified symmetric
structural and textural constraints by experts in the field. Experimental
trials across three guidance generation settings have shown a superior hit
generation performance compared to state-of-the-art methodologies. Moreover,
3DToMolo demonstrates the capability to generate novel molecules, incorporating
specified target substructures, without the need for prior knowledge. This work
not only holds general significance for the advancement of deep learning
methodologies but also paves the way for a transformative shift in molecular
design strategies. 3DToMolo creates opportunities for a more nuanced and
effective exploration of the vast chemical space, opening new frontiers in the
development of molecular entities with tailored properties and functionalities.</div><div><a href='http://arxiv.org/abs/2403.03425v1'>2403.03425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16302v1")'>Graph Diffusion Policy Optimization</div>
<div id='2402.16302v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T04:58:42Z</div><div>Authors: Yijing Liu, Chao Du, Tianyu Pang, Chongxuan Li, Wei Chen, Min Lin</div><div style='padding-top: 10px; width: 80ex'>Recent research has made significant progress in optimizing diffusion models
for specific downstream objectives, which is an important pursuit in fields
such as graph generation for drug design. However, directly applying these
models to graph diffusion presents challenges, resulting in suboptimal
performance. This paper introduces graph diffusion policy optimization (GDPO),
a novel approach to optimize graph diffusion models for arbitrary (e.g.,
non-differentiable) objectives using reinforcement learning. GDPO is based on
an eager policy gradient tailored for graph diffusion models, developed through
meticulous analysis and promising improved performance. Experimental results
show that GDPO achieves state-of-the-art performance in various graph
generation tasks with complex and diverse objectives. Code is available at
https://github.com/sail-sg/GDPO.</div><div><a href='http://arxiv.org/abs/2402.16302v1'>2402.16302v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17660v1")'>TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular
  Simulations</div>
<div id='2402.17660v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T16:27:06Z</div><div>Authors: Raul P. Pelaez, Guillem Simeon, Raimondas Galvelis, Antonio Mirarchi, Peter Eastman, Stefan Doerr, Philipp Thölke, Thomas E. Markland, Gianni De Fabritiis</div><div style='padding-top: 10px; width: 80ex'>Achieving a balance between computational speed, prediction accuracy, and
universal applicability in molecular simulations has been a persistent
challenge. This paper presents substantial advancements in the TorchMD-Net
software, a pivotal step forward in the shift from conventional force fields to
neural network-based potentials. The evolution of TorchMD-Net into a more
comprehensive and versatile framework is highlighted, incorporating
cutting-edge architectures such as TensorNet. This transformation is achieved
through a modular design approach, encouraging customized applications within
the scientific community. The most notable enhancement is a significant
improvement in computational efficiency, achieving a very remarkable
acceleration in the computation of energy and forces for TensorNet models, with
performance gains ranging from 2-fold to 10-fold over previous iterations.
Other enhancements include highly optimized neighbor search algorithms that
support periodic boundary conditions and the smooth integration with existing
molecular dynamics frameworks. Additionally, the updated version introduces the
capability to integrate physical priors, further enriching its application
spectrum and utility in research. The software is available at
https://github.com/torchmd/torchmd-net.</div><div><a href='http://arxiv.org/abs/2402.17660v1'>2402.17660v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15122v2")'>A Multi-Grained Symmetric Differential Equation Model for Learning
  Protein-Ligand Binding Dynamics</div>
<div id='2401.15122v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T09:35:17Z</div><div>Authors: Shengchao Liu, Weitao Du, Yanjing Li, Zhuoxinran Li, Vignesh Bhethanabotla, Nakul Rampal, Omar Yaghi, Christian Borgs, Anima Anandkumar, Hongyu Guo, Jennifer Chayes</div><div style='padding-top: 10px; width: 80ex'>In drug discovery, molecular dynamics (MD) simulation for protein-ligand
binding provides a powerful tool for predicting binding affinities, estimating
transport properties, and exploring pocket sites. There has been a long history
of improving the efficiency of MD simulations through better numerical methods
and, more recently, by utilizing machine learning (ML) methods. Yet, challenges
remain, such as accurate modeling of extended-timescale simulations. To address
this issue, we propose NeuralMD, the first ML surrogate that can facilitate
numerical MD and provide accurate simulations in protein-ligand binding. We
propose a principled approach that incorporates a novel physics-informed
multi-grained group symmetric framework. Specifically, we propose (1) a
BindingNet model that satisfies group symmetry using vector frames and captures
the multi-level protein-ligand interactions, and (2) an augmented neural
differential equation solver that learns the trajectory under Newtonian
mechanics. For the experiment, we design ten single-trajectory and three
multi-trajectory binding simulation tasks. We show the efficiency and
effectiveness of NeuralMD, with a 2000$\times$ speedup over standard numerical
MD simulation and outperforming all other ML approaches by up to 80% under the
stability metric. We further qualitatively show that NeuralMD reaches more
stable binding predictions compared to other machine learning methods.</div><div><a href='http://arxiv.org/abs/2401.15122v2'>2401.15122v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13984v1")'>Stability-Aware Training of Neural Network Interatomic Potentials with
  Differentiable Boltzmann Estimators</div>
<div id='2402.13984v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:12:07Z</div><div>Authors: Sanjeev Raja, Ishan Amin, Fabian Pedregosa, Aditi S. Krishnapriyan</div><div style='padding-top: 10px; width: 80ex'>Neural network interatomic potentials (NNIPs) are an attractive alternative
to ab-initio methods for molecular dynamics (MD) simulations. However, they can
produce unstable simulations which sample unphysical states, limiting their
usefulness for modeling phenomena occurring over longer timescales. To address
these challenges, we present Stability-Aware Boltzmann Estimator (StABlE)
Training, a multi-modal training procedure which combines conventional
supervised training from quantum-mechanical energies and forces with reference
system observables, to produce stable and accurate NNIPs. StABlE Training
iteratively runs MD simulations to seek out unstable regions, and corrects the
instabilities via supervision with a reference observable. The training
procedure is enabled by the Boltzmann Estimator, which allows efficient
computation of gradients required to train neural networks to system
observables, and can detect both global and local instabilities. We demonstrate
our methodology across organic molecules, tetrapeptides, and condensed phase
systems, along with using three modern NNIP architectures. In all three cases,
StABlE-trained models achieve significant improvements in simulation stability
and recovery of structural and dynamic observables. In some cases,
StABlE-trained models outperform conventional models trained on datasets 50
times larger. As a general framework applicable across NNIP architectures and
systems, StABlE Training is a powerful tool for training stable and accurate
NNIPs, particularly in the absence of large reference datasets.</div><div><a href='http://arxiv.org/abs/2402.13984v1'>2402.13984v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01195v1")'>Conditional Normalizing Flows for Active Learning of Coarse-Grained
  Molecular Representations</div>
<div id='2402.01195v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T07:44:26Z</div><div>Authors: Henrik Schopmans, Pascal Friederich</div><div style='padding-top: 10px; width: 80ex'>Efficient sampling of the Boltzmann distribution of molecular systems is a
long-standing challenge. Recently, instead of generating long molecular
dynamics simulations, generative machine learning methods such as normalizing
flows have been used to learn the Boltzmann distribution directly, without
samples. However, this approach is susceptible to mode collapse and thus often
does not explore the full configurational space. In this work, we address this
challenge by separating the problem into two levels, the fine-grained and
coarse-grained degrees of freedom. A normalizing flow conditioned on the
coarse-grained space yields a probabilistic connection between the two levels.
To explore the configurational space, we employ coarse-grained simulations with
active learning which allows us to update the flow and make all-atom potential
energy evaluations only when necessary. Using alanine dipeptide as an example,
we show that our methods obtain a speedup to molecular dynamics simulations of
approximately 15.9 to 216.2 compared to the speedup of 4.5 of the current
state-of-the-art machine learning approach.</div><div><a href='http://arxiv.org/abs/2402.01195v1'>2402.01195v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06936v1")'>Accelerated Sampling of Rare Events using a Neural Network Bias
  Potential</div>
<div id='2401.06936v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T00:11:02Z</div><div>Authors: Xinru Hua, Rasool Ahmad, Jose Blanchet, Wei Cai</div><div style='padding-top: 10px; width: 80ex'>In the field of computational physics and material science, the efficient
sampling of rare events occurring at atomic scale is crucial. It aids in
understanding mechanisms behind a wide range of important phenomena, including
protein folding, conformal changes, chemical reactions and materials diffusion
and deformation. Traditional simulation methods, such as Molecular Dynamics and
Monte Carlo, often prove inefficient in capturing the timescale of these rare
events by brute force. In this paper, we introduce a practical approach by
combining the idea of importance sampling with deep neural networks (DNNs) that
enhance the sampling of these rare events. In particular, we approximate the
variance-free bias potential function with DNNs which is trained to maximize
the probability of rare event transition under the importance potential
function. This method is easily scalable to high-dimensional problems and
provides robust statistical guarantees on the accuracy of the estimated
probability of rare event transition. Furthermore, our algorithm can actively
generate and learn from any successful samples, which is a novel improvement
over existing methods. Using a 2D system as a test bed, we provide comparisons
between results obtained from different training strategies, traditional Monte
Carlo sampling and numerically solved optimal bias potential function under
different temperatures. Our numerical results demonstrate the efficacy of the
DNN-based importance sampling of rare events.</div><div><a href='http://arxiv.org/abs/2401.06936v1'>2401.06936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00853v1")'>LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force
  Fields</div>
<div id='2402.00853v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:50:42Z</div><div>Authors: Joshua A. Vita, Amit Samanta, Fei Zhou, Vincenzo Lordi</div><div style='padding-top: 10px; width: 80ex'>Model ensembles are simple and effective tools for estimating the prediction
uncertainty of deep learning atomistic force fields. Despite this, widespread
adoption of ensemble-based uncertainty quantification (UQ) techniques is
limited by the high computational costs incurred by ensembles during both
training and inference. In this work we leverage the cumulative distribution
functions (CDFs) of per-sample errors obtained over the course of training to
efficiently represent the model ensemble, and couple them with a distance-based
similarity search in the model latent space. Using these tools, we develop a
simple UQ metric (which we call LTAU) that leverages the strengths of
ensemble-based techniques without requiring the evaluation of multiple models
during either training or inference. As an initial test, we apply our method
towards estimating the epistemic uncertainty in atomistic force fields
(LTAU-FF) and demonstrate that it can be easily calibrated to accurately
predict test errors on multiple datasets from the literature. We then
illustrate the utility of LTAU-FF in two practical applications: 1) tuning the
training-validation gap for an example dataset, and 2) predicting errors in
relaxation trajectories on the OC20 IS2RS task. Though in this work we focus on
the use of LTAU with deep learning atomistic force fields, we emphasize that it
can be readily applied to any regression task, or any ensemble-generation
technique, to provide a reliable and easy-to-implement UQ metric.</div><div><a href='http://arxiv.org/abs/2402.00853v1'>2402.00853v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03753v1")'>Enhanced sampling of robust molecular datasets with uncertainty-based
  collective variables</div>
<div id='2402.03753v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:42:51Z</div><div>Authors: Aik Rui Tan, Johannes C. B. Dietschreit, Rafael Gomez-Bombarelli</div><div style='padding-top: 10px; width: 80ex'>Generating a data set that is representative of the accessible configuration
space of a molecular system is crucial for the robustness of machine learned
interatomic potentials (MLIP). However, the complexity of molecular systems,
characterized by intricate potential energy surfaces (PESs) with numerous local
minima and energy barriers, presents a significant challenge. Traditional
methods of data generation, such as random sampling or exhaustive exploration,
are either intractable or may not capture rare, but highly informative
configurations. In this study, we propose a method that leverages uncertainty
as the collective variable (CV) to guide the acquisition of chemically-relevant
data points, focusing on regions of the configuration space where ML model
predictions are most uncertain. This approach employs a Gaussian Mixture
Model-based uncertainty metric from a single model as the CV for biased
molecular dynamics simulations. The effectiveness of our approach in overcoming
energy barriers and exploring unseen energy minima, thereby enhancing the data
set in an active learning framework, is demonstrated on the alanine dipeptide
benchmark system.</div><div><a href='http://arxiv.org/abs/2402.03753v1'>2402.03753v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13402v1")'>Towards accelerating physical discovery via non-interactive and
  interactive multi-fidelity Bayesian Optimization: Current challenges and
  future opportunities</div>
<div id='2402.13402v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T22:12:33Z</div><div>Authors: Arpan Biswas, Sai Mani Prudhvi Valleti, Rama Vasudevan, Maxim Ziatdinov, Sergei V. Kalinin</div><div style='padding-top: 10px; width: 80ex'>Both computational and experimental material discovery bring forth the
challenge of exploring multidimensional and often non-differentiable parameter
spaces, such as phase diagrams of Hamiltonians with multiple interactions,
composition spaces of combinatorial libraries, processing spaces, and molecular
embedding spaces. Often these systems are expensive or time-consuming to
evaluate a single instance, and hence classical approaches based on exhaustive
grid or random search are too data intensive. This resulted in strong interest
towards active learning methods such as Bayesian optimization (BO) where the
adaptive exploration occurs based on human learning (discovery) objective.
However, classical BO is based on a predefined optimization target, and
policies balancing exploration and exploitation are purely data driven. In
practical settings, the domain expert can pose prior knowledge on the system in
form of partially known physics laws and often varies exploration policies
during the experiment. Here, we explore interactive workflows building on
multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then
structured (physics-driven) sMFBO, and extending it to allow human in the loop
interactive iMFBO workflows for adaptive and domain expert aligned exploration.
These approaches are demonstrated over highly non-smooth multi-fidelity
simulation data generated from an Ising model, considering spin-spin
interaction as parameter space, lattice sizes as fidelity spaces, and the
objective as maximizing heat capacity. Detailed analysis and comparison show
the impact of physics knowledge injection and on-the-fly human decisions for
improved exploration, current challenges, and potential opportunities for
algorithm development with combining data, physics and real time human
decisions.</div><div><a href='http://arxiv.org/abs/2402.13402v1'>2402.13402v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09811v1")'>Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials</div>
<div id='2403.09811v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T18:59:54Z</div><div>Authors: Christian M. Clausen, Jan Rossmeisl, Zachary W. Ulissi</div><div style='padding-top: 10px; width: 80ex'>Computational high-throughput studies, especially in research on high-entropy
materials and catalysts, are hampered by high-dimensional composition spaces
and myriad structural microstates. They present bottlenecks to the conventional
use of density functional theory calculations, and consequently, the use of
machine-learned potentials is becoming increasingly prevalent in atomic
structure simulations. In this communication, we show the results of adjusting
and fine-tuning the pretrained EquiformerV2 model from the Open Catalyst
Project to infer adsorption energies of *OH and *O on the out-of-domain
high-entropy alloy Ag-Ir-Pd-Pt-Ru. By applying an energy filter based on the
local environment of the binding site the zero-shot inference is markedly
improved and through few-shot fine-tuning the model yields state-of-the-art
accuracy. It is also found that EquiformerV2, assuming the role of general
machine learning potential, is able to inform a smaller, more focused direct
inference model. This knowledge distillation setup boosts performance on
complex binding sites. Collectively, this shows that foundational knowledge
learned from ordered intermetallic structures, can be extrapolated to the
highly disordered structures of solid-solutions. With the vastly accelerated
computational throughput of these models, hitherto infeasible research in the
high-entropy material space is now readily accessible.</div><div><a href='http://arxiv.org/abs/2403.09811v1'>2403.09811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00259v1")'>Deciphering diffuse scattering with machine learning and the equivariant
  foundation model: The case of molten FeO</div>
<div id='2403.00259v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T03:50:03Z</div><div>Authors: Ganesh Sivaraman, Chris J. Benmore</div><div style='padding-top: 10px; width: 80ex'>Bridging the gap between diffuse x-ray or neutron scattering measurements and
predicted structures derived from atom-atom pair potentials in disordered
materials, has been a longstanding challenge in condensed matter physics. This
perspective gives a brief overview of the traditional approaches employed over
the past several decades. Namely, the use of approximate interatomic pair
potentials that relate 3-dimensional structural models to the measured
structure factor and its associated pair distribution function. The use of
machine learned interatomic potentials has grown in the past few years, and has
been particularly successful in the cases of ionic and oxide systems. Recent
advances in large scale sampling, along with a direct integration of scattering
measurements into the model development, has provided improved agreement
between experiments and large-scale models calculated with quantum mechanical
accuracy. However, details of local polyhedral bonding and connectivity in
meta-stable disordered systems still require improvement. Here we leverage
MACE-MP-0; a newly introduced equivariant foundation model and validate the
results against high-quality experimental scattering data for the case of
molten iron(II) oxide (FeO). These preliminary results suggest that the
emerging foundation model has the potential to surpass the traditional
limitations of classical interatomic potentials.</div><div><a href='http://arxiv.org/abs/2403.00259v1'>2403.00259v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07472v2")'>Cartesian atomic cluster expansion for machine learning interatomic
  potentials</div>
<div id='2402.07472v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T08:17:23Z</div><div>Authors: Bingqing Cheng</div><div style='padding-top: 10px; width: 80ex'>Machine learning interatomic potentials are revolutionizing large-scale,
accurate atomistic modelling in material science and chemistry. Many potentials
use atomic cluster expansion or equivariant message passing frameworks. Such
frameworks typically use spherical harmonics as angular basis functions, and
then use Clebsch-Gordan contraction to maintain rotational symmetry, which may
introduce redundancies in representations and computational overhead. We
propose an alternative: a Cartesian-coordinates-based atomic density expansion.
This approach provides a complete set of polynormially indepedent features of
atomic environments while maintaining interaction body orders. Additionally, we
integrate low-dimensional embeddings of various chemical elements and
inter-atomic message passing. The resulting potential, named Cartesian Atomic
Cluster Expansion (CACE), exhibits good accuracy, stability, and
generalizability. We validate its performance in diverse systems, including
bulk water, small molecules, and 25-element high-entropy alloys.</div><div><a href='http://arxiv.org/abs/2402.07472v2'>2402.07472v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05119v1")'>Estimation of Electronic Band Gap Energy From Material Properties Using
  Machine Learning</div>
<div id='2403.05119v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T07:32:28Z</div><div>Authors: Sagar Prakash Barad, Sajag Kumar, Subhankar Mishra</div><div style='padding-top: 10px; width: 80ex'>Machine learning techniques are utilized to estimate the electronic band gap
energy and forecast the band gap category of materials based on experimentally
quantifiable properties. The determination of band gap energy is critical for
discerning various material properties, such as its metallic nature, and
potential applications in electronic and optoelectronic devices. While
numerical methods exist for computing band gap energy, they often entail high
computational costs and have limitations in accuracy and scalability. A machine
learning-driven model capable of swiftly predicting material band gap energy
using easily obtainable experimental properties would offer a superior
alternative to conventional density functional theory (DFT) methods. Our model
does not require any preliminary DFT-based calculation or knowledge of the
structure of the material. We present a scheme for improving the performance of
simple regression and classification models by partitioning the dataset into
multiple clusters. A new evaluation scheme for comparing the performance of
ML-based models in material sciences involving both regression and
classification tasks is introduced based on traditional evaluation metrics. It
is shown that on this new evaluation metric, our method of clustering the
dataset results in better performance.</div><div><a href='http://arxiv.org/abs/2403.05119v1'>2403.05119v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13243v1")'>A Comparative Study of Machine Learning Models Predicting Energetics of
  Interacting Defects</div>
<div id='2403.13243v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T02:15:48Z</div><div>Authors: Hao Yu</div><div style='padding-top: 10px; width: 80ex'>Interacting defect systems are ubiquitous in materials under realistic
scenarios, yet gaining an atomic-level understanding of these systems from a
computational perspective is challenging - it often demands substantial
resources due to the necessity of employing supercell calculations. While
machine learning techniques have shown potential in accelerating materials
simulations, their application to systems involving interacting defects remains
relatively rare. In this work, we present a comparative study of three
different methods to predict the free energy change of systems with interacting
defects. We leveraging a limited dataset from Density Functional Theory(DFT)
calculations to assess the performance models using materials descriptors,
graph neural networks and cluster expansion. Our findings indicate that the
cluster expansion model can achieve precise energetics predictions even with
this limited dataset. Furthermore, with synthetic data generate from cluster
expansion model at near-DFT levels, we obtained enlarged dataset to assess the
demands on data for training accurate prediction models using graph neural
networks for systems featuring interacting defects. A brief discussion of the
computational cost for each method is provided at the end. This research
provide a preliminary evaluation of applying machine learning techniques in
imperfect surface systems.</div><div><a href='http://arxiv.org/abs/2403.13243v1'>2403.13243v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16611v1")'>Accelerating superconductor discovery through tempered deep learning of
  the electron-phonon spectral function</div>
<div id='2401.16611v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T22:44:28Z</div><div>Authors: Jason B. Gibson, Ajinkya C. Hire, Philip M. Dee, Oscar Barrera, Benjamin Geisler, Peter J. Hirschfeld, Richard G. Hennig</div><div style='padding-top: 10px; width: 80ex'>Integrating deep learning with the search for new electron-phonon
superconductors represents a burgeoning field of research, where the primary
challenge lies in the computational intensity of calculating the
electron-phonon spectral function, $\alpha^2F(\omega)$, the essential
ingredient of Midgal-Eliashberg theory of superconductivity. To overcome this
challenge, we adopt a two-step approach. First, we compute $\alpha^2F(\omega)$
for 818 dynamically stable materials. We then train a deep-learning model to
predict $\alpha^2F(\omega)$, using an unconventional training strategy to
temper the model's overfitting, enhancing predictions. Specifically, we train a
Bootstrapped Ensemble of Tempered Equivariant graph neural NETworks (BETE-NET),
obtaining an MAE of 0.21, 45 K, and 43 K for the Eliashberg moments derived
from $\alpha^2F(\omega)$: $\lambda$, $\omega_{\log}$, and $\omega_{2}$,
respectively, yielding an MAE of 2.5 K for the critical temperature, $T_c$.
Further, we incorporate domain knowledge of the site-projected phonon density
of states to impose inductive bias into the model's node attributes and enhance
predictions. This methodological innovation decreases the MAE to 0.18, 29 K,
and 28 K, respectively, yielding an MAE of 2.1 K for $T_c$. We illustrate the
practical application of our model in high-throughput screening for high-$T_c$
materials. The model demonstrates an average precision nearly five times higher
than random screening, highlighting the potential of ML in accelerating
superconductor discovery. BETE-NET accelerates the search for high-$T_c$
superconductors while setting a precedent for applying ML in materials
discovery, particularly when data is limited.</div><div><a href='http://arxiv.org/abs/2401.16611v1'>2401.16611v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10287v1")'>Open-Source Fermionic Neural Networks with Ionic Charge Initialization</div>
<div id='2401.10287v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T08:51:58Z</div><div>Authors: Shai Pranesh, Shang Zhu, Venkat Viswanathan, Bharath Ramsundar</div><div style='padding-top: 10px; width: 80ex'>Finding accurate solutions to the electronic Schr\"odinger equation plays an
important role in discovering important molecular and material energies and
characteristics. Consequently, solving systems with large numbers of electrons
has become increasingly important. Variational Monte Carlo (VMC) methods,
especially those approximated through deep neural networks, are promising in
this regard. In this paper, we aim to integrate one such model called the
FermiNet, a post-Hartree-Fock (HF) Deep Neural Network (DNN) model, into a
standard and widely used open source library, DeepChem. We also propose novel
initialization techniques to overcome the difficulties associated with the
assignment of excess or lack of electrons for ions.</div><div><a href='http://arxiv.org/abs/2401.10287v1'>2401.10287v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01874v1")'>Graph Neural Networks for Surfactant Multi-Property Prediction</div>
<div id='2401.01874v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T18:32:25Z</div><div>Authors: Christoforos Brozos, Jan G. Rittig, Sandip Bhattacharya, Elie Akanny, Christina Kohlmann, Alexander Mitsos</div><div style='padding-top: 10px; width: 80ex'>Surfactants are of high importance in different industrial sectors such as
cosmetics, detergents, oil recovery and drug delivery systems. Therefore, many
quantitative structure-property relationship (QSPR) models have been developed
for surfactants. Each predictive model typically focuses on one surfactant
class, mostly nonionics. Graph Neural Networks (GNNs) have exhibited a great
predictive performance for property prediction of ionic liquids, polymers and
drugs in general. Specifically for surfactants, GNNs can successfully predict
critical micelle concentration (CMC), a key surfactant property associated with
micellization. A key factor in the predictive ability of QSPR and GNN models is
the data available for training. Based on extensive literature search, we
create the largest available CMC database with 429 molecules and the first
large data collection for surface excess concentration ($\Gamma$$_{m}$),
another surfactant property associated with foaming, with 164 molecules. Then,
we develop GNN models to predict the CMC and $\Gamma$$_{m}$ and we explore
different learning approaches, i.e., single- and multi-task learning, as well
as different training strategies, namely ensemble and transfer learning. We
find that a multi-task GNN with ensemble learning trained on all $\Gamma$$_{m}$
and CMC data performs best. Finally, we test the ability of our CMC model to
generalize on industrial grade pure component surfactants. The GNN yields
highly accurate predictions for CMC, showing great potential for future
industrial applications.</div><div><a href='http://arxiv.org/abs/2401.01874v1'>2401.01874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03767v1")'>Predicting the Temperature Dependence of Surfactant CMCs Using Graph
  Neural Networks</div>
<div id='2403.03767v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:03:04Z</div><div>Authors: Christoforos Brozos, Jan G. Rittig, Sandip Bhattacharya, Elie Akanny, Christina Kohlmann, Alexander Mitsos</div><div style='padding-top: 10px; width: 80ex'>The critical micelle concentration (CMC) of surfactant molecules is an
essential property for surfactant applications in industry. Recently, classical
QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been
successfully applied to predict the CMC of surfactants at room temperature.
However, these models have not yet considered the temperature dependency of the
CMC, which is highly relevant for practical applications. We herein develop a
GNN model for temperature-dependent CMC prediction of surfactants. We collect
about 1400 data points from public sources for all surfactant classes, i.e.,
ionic, nonionic, and zwitterionic, at multiple temperatures. We test the
predictive quality of the model for following scenarios: i) when CMC data for
surfactants are present in the training of the model in at least one different
temperature, and ii) CMC data for surfactants are not present in the training,
i.e., generalizing to unseen surfactants. In both test scenarios, our model
exhibits a high predictive performance of R$^2 \geq $ 0.94 on test data. We
also find that the model performance varies by surfactant class. Finally, we
evaluate the model for sugar-based surfactants with complex molecular
structures, as these represent a more sustainable alternative to synthetic
surfactants and are therefore of great interest for future applications in the
personal and home care industries.</div><div><a href='http://arxiv.org/abs/2403.03767v1'>2403.03767v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12659v1")'>Zeolite Adsorption Property Prediction using Deep Learning</div>
<div id='2403.12659v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T11:49:08Z</div><div>Authors: Marko Petković, José Manuel Vicent-Luna, Vlado Menkovski, Sofía Calero</div><div style='padding-top: 10px; width: 80ex'>The ability to efficiently predict adsorption properties of zeolites can be
of large benefit in accelerating the design process of novel materials. The
existing configuration space for these materials is wide, while existing
molecular simulation methods are computationally expensive. In this work, we
propose a model which is 4 to 5 orders of magnitude faster at adsorption
properties compared to molecular simulations. To validate the model, we
generated datasets containing various aluminium configurations for the MOR,
MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry
coefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions
obtained from the Machine Learning model are in agreement with the values
obtained from the Monte Carlo simulations, confirming that the model can be
used for property prediction. Furthermore, we show that the model can be used
for identifying adsorption sites. Finally, we evaluate the capability of our
model for generating novel zeolite configurations by using it in combination
with a genetic algorithm.</div><div><a href='http://arxiv.org/abs/2403.12659v1'>2403.12659v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12982v1")'>Knowledge-Reuse Transfer Learning Methods in Molecular and Material
  Science</div>
<div id='2403.12982v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T12:41:25Z</div><div>Authors: An Chen, Zhilong Wang, Karl Luigi Loza Vidaurre, Yanqiang Han, Simin Ye, Kehao Tao, Shiwei Wang, Jing Gao, Jinjin Li</div><div style='padding-top: 10px; width: 80ex'>Molecules and materials are the foundation for the development of modern
advanced industries such as energy storage systems and semiconductor devices.
However, traditional trial-and-error methods or theoretical calculations are
highly resource-intensive, and extremely long R&amp;D (Research and Development)
periods cannot meet the urgent need for molecules/materials in industrial
development. Machine learning (ML) methods based on big data are expected to
break this dilemma. However, the difficulty in constructing large-scale
datasets of new molecules/materials due to the high cost of data acquisition
and annotation limits the development of machine learning. The application of
transfer learning lowers the data requirements for model training, which makes
transfer learning stand out in researches addressing data quality issues. In
this review, we summarize recent advances in transfer learning related to
molecular and materials science. We focus on the application of transfer
learning methods for the discovery of advanced molecules/materials,
particularly, the construction of transfer learning frameworks for different
systems, and how transfer learning can enhance the performance of models. In
addition, the challenges of transfer learning are also discussed.</div><div><a href='http://arxiv.org/abs/2403.12982v1'>2403.12982v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08715v1")'>Selecting Subsets of Source Data for Transfer Learning with Applications
  in Metal Additive Manufacturing</div>
<div id='2401.08715v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T00:14:37Z</div><div>Authors: Yifan Tang, M. Rahmani Dehaghani, Pouyan Sajadi, G. Gary Wang</div><div style='padding-top: 10px; width: 80ex'>Considering data insufficiency in metal additive manufacturing (AM), transfer
learning (TL) has been adopted to extract knowledge from source domains (e.g.,
completed printings) to improve the modeling performance in target domains
(e.g., new printings). Current applications use all accessible source data
directly in TL with no regard to the similarity between source and target data.
This paper proposes a systematic method to find appropriate subsets of source
data based on similarities between the source and target datasets for a given
set of limited target domain data. Such similarity is characterized by the
spatial and model distance metrics. A Pareto frontier-based source data
selection method is developed, where the source data located on the Pareto
frontier defined by two similarity distance metrics are selected iteratively.
The method is integrated into an instance-based TL method (decision tree
regression model) and a model-based TL method (fine-tuned artificial neural
network). Both models are then tested on several regression tasks in metal AM.
Comparison results demonstrate that 1) the source data selection method is
general and supports integration with various TL methods and distance metrics,
2) compared with using all source data, the proposed method can find a small
subset of source data from the same domain with better TL performance in metal
AM regression tasks involving different processes and machines, and 3) when
multiple source domains exist, the source data selection method could find the
subset from one source domain to obtain comparable or better TL performance
than the model constructed using data from all source domains.</div><div><a href='http://arxiv.org/abs/2401.08715v1'>2401.08715v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05568v1")'>Phase discovery with active learning: Application to structural phase
  transitions in equiatomic NiTi</div>
<div id='2401.05568v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:22:47Z</div><div>Authors: Jonathan Vandermause, Anders Johansson, Yucong Miao, Joost J. Vlassak, Boris Kozinsky</div><div style='padding-top: 10px; width: 80ex'>Nickel titanium (NiTi) is a protypical shape-memory alloy used in a range of
biomedical and engineering devices, but direct molecular dynamics simulations
of the martensitic B19' -&gt; B2 phase transition driving its shape-memory
behavior are rare and have relied on classical force fields with limited
accuracy. Here, we train four machine-learned force fields for equiatomic NiTi
based on the LDA, PBE, PBEsol, and SCAN DFT functionals. The models are trained
on the fly during NPT molecular dynamics, with DFT calculations and model
updates performed automatically whenever the uncertainty of a local energy
prediction exceeds a chosen threshold. The models achieve accuracies of 1-2
meV/atom during training and are shown to closely track DFT predictions of B2
and B19' elastic constants and phonon frequencies. Surprisingly, in large-scale
molecular dynamics simulations, only the SCAN model predicts a reversible B19'
-&gt; B2 phase transition, with the LDA, PBE, and PBEsol models predicting a
reversible transition to a previously uncharacterized low-volume phase, which
we hypothesize to be a new stable high-pressure phase. We examine the structure
of the new phase and estimate its stability on the temperature-pressure phase
diagram. This work establishes an automated active learning protocol for
studying displacive transformations, reveals important differences between DFT
functionals that can only be detected in large-scale simulations, provides an
accurate force field for NiTi, and identifies a new phase.</div><div><a href='http://arxiv.org/abs/2401.05568v1'>2401.05568v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13627v1")'>Efficient exploration of high-Tc superconductors by a gradient-based
  composition design</div>
<div id='2403.13627v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T14:23:17Z</div><div>Authors: Akihiro Fujii, Koji Shimizu, Satoshi Watanabe</div><div style='padding-top: 10px; width: 80ex'>We propose a material design method via gradient-based optimization on
compositions, overcoming the limitations of traditional methods: exhaustive
database searches and conditional generation models. It optimizes inputs via
backpropagation, aligning the model's output closely with the target property
and facilitating the discovery of unlisted materials and precise property
determination. Our method is also capable of adaptive optimization under new
conditions without retraining. Applying to exploring high-Tc superconductors,
we identified potential compositions beyond existing databases and discovered
new hydrogen superconductors via conditional optimization. This method is
versatile and significantly advances material design by enabling efficient,
extensive searches and adaptability to new constraints.</div><div><a href='http://arxiv.org/abs/2403.13627v1'>2403.13627v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05848v1")'>Pushing the Pareto front of band gap and permittivity: ML-guided search
  for dielectric materials</div>
<div id='2401.05848v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T11:38:20Z</div><div>Authors: Janosh Riebesell, T. Wesley Surta, Rhys Goodall, Michael Gaultois, Alpha A Lee</div><div style='padding-top: 10px; width: 80ex'>Materials with high-dielectric constant easily polarize under external
electric fields, allowing them to perform essential functions in many modern
electronic devices. Their practical utility is determined by two conflicting
properties: high dielectric constants tend to occur in materials with narrow
band gaps, limiting the operating voltage before dielectric breakdown. We
present a high-throughput workflow that combines element substitution, ML
pre-screening, ab initio simulation and human expert intuition to efficiently
explore the vast space of unknown materials for potential dielectrics, leading
to the synthesis and characterization of two novel dielectric materials,
CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective
optimization setting with concave Pareto front. While usually considered more
challenging than single-objective optimization, we argue and show preliminary
evidence that the $1/x$-correlation between band gap and permittivity in fact
makes the task more amenable to ML methods by allowing separate models for band
gap and permittivity to each operate in regions of good training support while
still predicting materials of exceptional merit. To our knowledge, this is the
first instance of successful ML-guided multi-objective materials optimization
achieving experimental synthesis and characterization. CsTaTeO6 is a structure
generated via element substitution not present in our reference data sources,
thus exemplifying successful de-novo materials design. Meanwhile, we report the
first high-purity synthesis and dielectric characterization of Bi2Zr2O7 with a
band gap of 2.27 eV and a permittivity of 20.5, meeting all target metrics of
our multi-objective search.</div><div><a href='http://arxiv.org/abs/2401.05848v1'>2401.05848v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06955v1")'>Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic
  Perovskites</div>
<div id='2403.06955v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:39:08Z</div><div>Authors: Nima Karimitari, William J. Baldwin, Evan W. Muller, Zachary J. L. Bare, W. Joshua Kennedy, Gábor Csányi, Christopher Sutton</div><div style='padding-top: 10px; width: 80ex'>Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a
promising class of electronically active materials for both light absorption
and emission. The design space of HOIPs is extremely large, since a diverse
space of organic cations can be combined with different inorganic frameworks.
This immense design space allows for tunable electronic and mechanical
properties, but also necessitates the development of new tools for in silico
high throughput analysis of candidate structures. In this work, we present an
accurate, efficient, transferable and widely applicable machine learning
interatomic potential (MLIP) for predicting the structure of new 2D HOIPs.
Using the MACE architecture, an MLIP is trained on 86 diverse experimentally
reported HOIP structures. The model is tested on 73 unseen perovskite
compositions, and achieves chemical accuracy with respect to the reference
electronic structure method. Our model is then combined with a simple random
structure search algorithm to predict the structure of hypothetical HOIPs given
only the proposed composition. Success is demonstrated by correctly and
reliably recovering the crystal structure of a set of experimentally known 2D
perovskites. Such a random structure search is impossible with ab initio
methods due to the associated computational cost, but is relatively inexpensive
with the MACE potential. Finally, the procedure is used to predict the
structure formed by a new organic cation with no previously known corresponding
perovskite. Laboratory synthesis of the new hybrid perovskite confirms the
accuracy of our prediction. This capability, applied at scale, enables
efficient screening of thousands of combinations of organic cations and
inorganic layers.</div><div><a href='http://arxiv.org/abs/2403.06955v1'>2403.06955v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11101v3")'>Physics-based material parameters extraction from perovskite experiments
  via Gaussian process</div>
<div id='2402.11101v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T22:14:21Z</div><div>Authors: Hualin Zhan, Viqar Ahmad, Azul Mayon, Grace Tabi, Anh Dinh Bui, Zhuofeng Li, Daniel Walter, Hieu Nguyen, Klaus Weber, Thomas White, Kylie Catchpole</div><div style='padding-top: 10px; width: 80ex'>The ability to extract material parameters of perovskite from quantitative
experimental analysis is essential for rational design of photovoltaic and
optoelectronic applications. However, the difficulty of this analysis increases
significantly with the complexity of the theoretical model and the number of
material parameters for perovskite. Here we use Gaussian process to develop an
analysis platform that can extract up to 8 fundamental material parameters of
an organometallic perovskite semiconductor from a transient photoluminescence
experiment, based on a complex full physics model that includes drift-diffusion
of carriers and dynamic defect occupation. An example study of thermal
degradation reveals that changes in doping concentration and carrier mobility
dominate, while the defect energy level remains nearly unchanged. This platform
can be conveniently applied to other experiments or to combinations of
experiments, accelerating materials discovery and optimization of semiconductor
materials for photovoltaics and other applications.</div><div><a href='http://arxiv.org/abs/2402.11101v3'>2402.11101v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10874v1")'>Design of 2D Skyrmionic Metamaterial Through Controlled Assembly</div>
<div id='2402.10874v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:20:33Z</div><div>Authors: Qichen Xu, Zhuanglin Shen, Alexander Edström, I. P. Miranda, Zhiwei Lu, Anders Bergman, Danny Thonig, Wanjian Yin, Olle Eriksson, Anna Delin</div><div style='padding-top: 10px; width: 80ex'>Despite extensive research on magnetic skyrmions and antiskyrmions, a
significant challenge remains in crafting nontrivial high-order skyrmionic
textures with varying, or even tailor-made, topologies. We address this
challenge, by focusing on a construction pathway of skyrmionics metamaterial
within a monolayer thin film and suggest several promising lattice-like,
flakes-like, and cell-like skyrmionic metamaterials that are surprisingly
stable. Central to our approach is the concept of 'simulated controlled
assembly', in short, a protocol inspired by 'click chemistry' that allows for
positioning topological magnetic structures where one likes, and then allowing
for energy minimization to elucidate the stability. Utilizing high-throughput
atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven
tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions
(Q=-1), and skyrmionium (Q=0). These entities serve as foundational 'skyrmionic
building blocks' to forming reported intricate textures. In this work, two key
contributions are introduced to the field of skyrmionic systems. First, we
present a novel method for integrating control assembly protocols for the
stabilization and investigation of topological magnets, which marks a
significant advancement in the ability to explore new skyrmionic textures.
Second, we report on the discovery of skyrmionic metamaterials, which shows a
plethora of complex topologies that are possible to investigate theoretically
and experimentally.</div><div><a href='http://arxiv.org/abs/2402.10874v1'>2402.10874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07526v1")'>Physics-Transfer Learning for Material Strength Screening</div>
<div id='2403.07526v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:05:05Z</div><div>Authors: Yingjie Zhao, Zian Zhang, Zhiping Xu</div><div style='padding-top: 10px; width: 80ex'>The strength of materials, like many problems in the natural sciences, spans
multiple length and time scales, and the solution has to balance accuracy and
performance. Peierls stress is one of the central concepts in crystal
plasticity that measures the strength through the resistance of a dislocation
to plastic flow. The determination of Peierls stress involves a multiscale
nature depending on both elastic lattice responses and the energy landscape of
crystal slips. Material screening by strength via the Peierls stress from
first-principles calculations is computationally intractable for the nonlocal
characteristics of dislocations, and not included in the state-of-the-art
computational material databases. In this work, we propose a physics-transfer
framework to learn the physics of crystal plasticity from empirical atomistic
simulations and then predict the Peierls stress from chemically accurate
density functional theory-based calculations of material parameters. Notably,
the strengths of single-crystalline metals can be predicted from a few
single-point calculations for the deformed lattice and on the {\gamma} surface,
allowing efficient, high-throughput screening for material discovery.
Uncertainty quantification is carried out to assess the accuracy of models and
sources of errors, showing reduced physical and system uncertainties in the
predictions by elevating the fidelity of training models. This physics-transfer
framework can be generalized to other problems facing the accuracy-performance
dilemma, by harnessing the hierarchy of physics in the multiscale models of
materials science.</div><div><a href='http://arxiv.org/abs/2403.07526v1'>2403.07526v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17686v1")'>Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces</div>
<div id='2402.17686v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:01:21Z</div><div>Authors: Luis Itza Vazquez-Salazar, Silvan Käser, Markus Meuwly</div><div style='padding-top: 10px; width: 80ex'>Uncertainty quantification (UQ) to detect samples with large expected errors
(outliers) is applied to reactive molecular potential energy surfaces (PESs).
Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian
Mixture Models (GMM) - were applied to the H-transfer reaction between ${\it
syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble
models provide the best results for detecting outliers, followed by GMM. For
example, from a pool of 1000 structures with the largest uncertainty, the
detection quality for outliers is $\sim 90$ \% and $\sim 50$ \%, respectively,
if 25 or 1000 structures with large errors are sought. On the contrary, the
limitations of the statistical assumptions of DER greatly impacted its
prediction capabilities. Finally, a structure-based indicator was found to be
correlated with large average error, which may help to rapidly classify new
structures into those that provide an advantage for refining the neural
network.</div><div><a href='http://arxiv.org/abs/2402.17686v1'>2402.17686v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04580v1")'>Beyond Major Product Prediction: Reproducing Reaction Mechanisms with
  Machine Learning Models Trained on a Large-Scale Mechanistic Dataset</div>
<div id='2403.04580v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T15:26:23Z</div><div>Authors: Joonyoung F. Joung, Mun Hong Fong, Jihye Roh, Zhengkai Tu, John Bradshaw, Connor W. Coley</div><div style='padding-top: 10px; width: 80ex'>Mechanistic understanding of organic reactions can facilitate reaction
development, impurity prediction, and in principle, reaction discovery. While
several machine learning models have sought to address the task of predicting
reaction products, their extension to predicting reaction mechanisms has been
impeded by the lack of a corresponding mechanistic dataset. In this study, we
construct such a dataset by imputing intermediates between experimentally
reported reactants and products using expert reaction templates and train
several machine learning models on the resulting dataset of 5,184,184
elementary steps. We explore the performance and capabilities of these models,
focusing on their ability to predict reaction pathways and recapitulate the
roles of catalysts and reagents. Additionally, we demonstrate the potential of
mechanistic models in predicting impurities, often overlooked by conventional
models. We conclude by evaluating the generalizability of mechanistic models to
new reaction types, revealing challenges related to dataset diversity,
consecutive predictions, and violations of atom conservation.</div><div><a href='http://arxiv.org/abs/2403.04580v1'>2403.04580v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09560v1")'>Self-Consistency Training for Hamiltonian Prediction</div>
<div id='2403.09560v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T16:52:57Z</div><div>Authors: He Zhang, Chang Liu, Zun Wang, Xinran Wei, Siyuan Liu, Nanning Zheng, Bin Shao, Tie-Yan Liu</div><div style='padding-top: 10px; width: 80ex'>Hamiltonian prediction is a versatile formulation to leverage machine
learning for solving molecular science problems. Yet, its applicability is
limited by insufficient labeled data for training. In this work, we highlight
that Hamiltonian prediction possesses a self-consistency principle, based on
which we propose an exact training method that does not require labeled data.
This merit addresses the data scarcity difficulty, and distinguishes the task
from other property prediction formulations with unique benefits: (1)
self-consistency training enables the model to be trained on a large amount of
unlabeled data, hence substantially enhances generalization; (2)
self-consistency training is more efficient than labeling data with DFT for
supervised training, since it is an amortization of DFT calculation over a set
of molecular structures. We empirically demonstrate the better generalization
in data-scarce and out-of-distribution scenarios, and the better efficiency
from the amortization. These benefits push forward the applicability of
Hamiltonian prediction to an ever larger scale.</div><div><a href='http://arxiv.org/abs/2403.09560v1'>2403.09560v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01975v1")'>Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks</div>
<div id='2402.01975v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T00:58:41Z</div><div>Authors: Duy M. H. Nguyen, Nina Lukashina, Tai Nguyen, An T. Le, TrungTin Nguyen, Nhat Ho, Jan Peters, Daniel Sonntag, Viktor Zaverkin, Mathias Niepert</div><div style='padding-top: 10px; width: 80ex'>A molecule's 2D representation consists of its atoms, their attributes, and
the molecule's covalent bonds. A 3D (geometric) representation of a molecule is
called a conformer and consists of its atom types and Cartesian coordinates.
Every conformer has a potential energy, and the lower this energy, the more
likely it occurs in nature. Most existing machine learning methods for
molecular property prediction consider either 2D molecular graphs or 3D
conformer structure representations in isolation. Inspired by recent work on
using ensembles of conformers in conjunction with 2D graph representations, we
propose E(3)-invariant molecular conformer aggregation networks. The method
integrates a molecule's 2D representation with that of multiple of its
conformers. Contrary to prior work, we propose a novel 2D--3D aggregation
mechanism based on a differentiable solver for the \emph{Fused
Gromov-Wasserstein Barycenter} problem and the use of an efficient online
conformer generation method based on distance geometry. We show that the
proposed aggregation mechanism is E(3) invariant and provides an efficient GPU
implementation. Moreover, we demonstrate that the aggregation mechanism helps
to outperform state-of-the-art property prediction methods on established
datasets significantly.</div><div><a href='http://arxiv.org/abs/2402.01975v1'>2402.01975v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01398v1")'>Accelerating Black-Box Molecular Property Optimization by Adaptively
  Learning Sparse Subspaces</div>
<div id='2401.01398v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T18:34:29Z</div><div>Authors: Farshud Sorourifar, Thomas Banker, Joel A. Paulson</div><div style='padding-top: 10px; width: 80ex'>Molecular property optimization (MPO) problems are inherently challenging
since they are formulated over discrete, unstructured spaces and the labeling
process involves expensive simulations or experiments, which fundamentally
limits the amount of available data. Bayesian optimization (BO) is a powerful
and popular framework for efficient optimization of noisy, black-box objective
functions (e.g., measured property values), thus is a potentially attractive
framework for MPO. To apply BO to MPO problems, one must select a structured
molecular representation that enables construction of a probabilistic surrogate
model. Many molecular representations have been developed, however, they are
all high-dimensional, which introduces important challenges in the BO process
-- mainly because the curse of dimensionality makes it difficult to define and
perform inference over a suitable class of surrogate models. This challenge has
been recently addressed by learning a lower-dimensional encoding of a SMILE or
graph representation of a molecule in an unsupervised manner and then
performing BO in the encoded space. In this work, we show that such methods
have a tendency to "get stuck," which we hypothesize occurs since the mapping
from the encoded space to property values is not necessarily well-modeled by a
Gaussian process. We argue for an alternative approach that combines numerical
molecular descriptors with a sparse axis-aligned Gaussian process model, which
is capable of rapidly identifying sparse subspaces that are most relevant to
modeling the unknown property function. We demonstrate that our proposed method
substantially outperforms existing MPO methods on a variety of benchmark and
real-world problems. Specifically, we show that our method can routinely find
near-optimal molecules out of a set of more than $&gt;100$k alternatives within
100 or fewer expensive queries.</div><div><a href='http://arxiv.org/abs/2401.01398v1'>2401.01398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18396v1")'>Deep Confident Steps to New Pockets: Strategies for Docking
  Generalization</div>
<div id='2402.18396v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:15:23Z</div><div>Authors: Gabriele Corso, Arthur Deng, Benjamin Fry, Nicholas Polizzi, Regina Barzilay, Tommi Jaakkola</div><div style='padding-top: 10px; width: 80ex'>Accurate blind docking has the potential to lead to new biological
breakthroughs, but for this promise to be realized, docking methods must
generalize well across the proteome. Existing benchmarks, however, fail to
rigorously assess generalizability. Therefore, we develop DockGen, a new
benchmark based on the ligand-binding domains of proteins, and we show that
existing machine learning-based docking models have very weak generalization
abilities. We carefully analyze the scaling laws of ML-based docking and show
that, by scaling data and model size, as well as integrating synthetic data
strategies, we are able to significantly increase the generalization capacity
and set new state-of-the-art performance across benchmarks. Further, we propose
Confidence Bootstrapping, a new training paradigm that solely relies on the
interaction between diffusion and confidence models and exploits the
multi-resolution generation process of diffusion models. We demonstrate that
Confidence Bootstrapping significantly improves the ability of ML-based docking
methods to dock to unseen protein classes, edging closer to accurate and
generalizable blind docking methods.</div><div><a href='http://arxiv.org/abs/2402.18396v1'>2402.18396v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08986v1")'>Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid
  Interface Prediction</div>
<div id='2401.08986v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T05:39:03Z</div><div>Authors: Ziyang Yu, Wenbing Huang, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>The study of rigid protein-protein docking plays an essential role in a
variety of tasks such as drug design and protein engineering. Recently, several
learning-based methods have been proposed for the task, exhibiting much faster
docking speed than those computational methods. In this paper, we propose a
novel learning-based method called ElliDock, which predicts an elliptic
paraboloid to represent the protein-protein docking interface. To be specific,
our model estimates elliptic paraboloid interfaces for the two input proteins
respectively, and obtains the roto-translation transformation for docking by
making two interfaces coincide. By its design, ElliDock is independently
equivariant with respect to arbitrary rotations/translations of the proteins,
which is an indispensable property to ensure the generalization of the docking
process. Experimental evaluations show that ElliDock achieves the fastest
inference time among all compared methods and is strongly competitive with
current state-of-the-art learning-based models such as DiffDock-PP and Multimer
particularly for antibody-antigen docking.</div><div><a href='http://arxiv.org/abs/2401.08986v1'>2401.08986v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01542v1")'>Learning Collective Variables for Protein Folding with Labeled Data
  Augmentation through Geodesic Interpolation</div>
<div id='2402.01542v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T16:35:02Z</div><div>Authors: Soojung Yang, Juno Nam, Johannes C. B. Dietschreit, Rafael Gómez-Bombarelli</div><div style='padding-top: 10px; width: 80ex'>In molecular dynamics (MD) simulations, rare events, such as protein folding,
are typically studied by means of enhanced sampling techniques, most of which
rely on the definition of a collective variable (CV) along which the
acceleration occurs. Obtaining an expressive CV is crucial, but often hindered
by the lack of information about the particular event, e.g., the transition
from unfolded to folded conformation. We propose a simulation-free data
augmentation strategy using physics-inspired metrics to generate geodesic
interpolations resembling protein folding transitions, thereby improving
sampling efficiency without true transition state samples. Leveraging
interpolation progress parameters, we introduce a regression-based learning
scheme for CV models, which outperforms classifier-based methods when
transition state data is limited and noisy</div><div><a href='http://arxiv.org/abs/2402.01542v1'>2402.01542v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11591v1")'>A physics-informed neural network method for the approximation of slow
  invariant manifolds for the general class of stiff systems of ODEs</div>
<div id='2403.11591v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T09:10:39Z</div><div>Authors: Dimitrios G. Patsatzis, Lucia Russo, Constantinos Siettos</div><div style='padding-top: 10px; width: 80ex'>We present a physics-informed neural network (PINN) approach for the
discovery of slow invariant manifolds (SIMs), for the most general class of
fast/slow dynamical systems of ODEs. In contrast to other machine learning (ML)
approaches that construct reduced order black box surrogate models using simple
regression, and/or require a priori knowledge of the fast and slow variables,
our approach, simultaneously decomposes the vector field into fast and slow
components and provides a functional of the underlying SIM in a closed form.
The decomposition is achieved by finding a transformation of the state
variables to the fast and slow ones, which enables the derivation of an
explicit, in terms of fast variables, SIM functional. The latter is obtained by
solving a PDE corresponding to the invariance equation within the Geometric
Singular Perturbation Theory (GSPT) using a single-layer feedforward neural
network with symbolic differentiation. The performance of the proposed
physics-informed ML framework is assessed via three benchmark problems: the
Michaelis-Menten, the target mediated drug disposition (TMDD) reaction model
and a fully competitive substrate-inhibitor(fCSI) mechanism. We also provide a
comparison with other GPST methods, namely the quasi steady state approximation
(QSSA), the partial equilibrium approximation (PEA) and CSP with one and two
iterations. We show that the proposed PINN scheme provides SIM approximations,
of equivalent or even higher accuracy, than those provided by QSSA, PEA and
CSP, especially close to the boundaries of the underlying SIMs.</div><div><a href='http://arxiv.org/abs/2403.11591v1'>2403.11591v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13952v1")'>Considerations in the use of ML interaction potentials for free energy
  calculations</div>
<div id='2403.13952v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T19:49:21Z</div><div>Authors: Orlando A. Mendible, Jonathan K. Whitmer, Yamil J. Colón</div><div style='padding-top: 10px; width: 80ex'>Machine learning potentials (MLPs) offer the potential to accurately model
the energy and free energy landscapes of molecules with the precision of
quantum mechanics and an efficiency similar to classical simulations. This
research focuses on using equivariant graph neural networks MLPs due to their
proven effectiveness in modeling equilibrium molecular trajectories. A key
issue addressed is the capability of MLPs to accurately predict free energies
and transition states by considering both the energy and the diversity of
molecular configurations. We examined how the distribution of collective
variables (CVs) in the training data affects MLP accuracy in determining the
free energy surface (FES) of systems, using Metadynamics simulations for butane
and alanine dipeptide (ADP). The study involved training forty-three MLPs, half
based on classical molecular dynamics data and the rest on ab initio computed
energies. The MLPs were trained using different distributions that aim to
replicate hypothetical scenarios of sampled CVs obtained if the underlying FES
of the system was unknown. Findings for butane revealed that training data
coverage of key FES regions ensures model accuracy regardless of CV
distribution. However, missing significant FES regions led to correct potential
energy predictions but failed free energy reconstruction. For ADP, models
trained on classical dynamics data were notably less accurate, while ab
initio-based MLPs predicted potential energy well but faltered on free energy
predictions. These results emphasize the challenge of assembling an
all-encompassing training set for accurate FES prediction and highlight the
importance of understanding the FES in preparing training data. The study
points out the limitations of MLPs in free energy calculations, stressing the
need for comprehensive data that encompasses the system's full FES for
effective model training.</div><div><a href='http://arxiv.org/abs/2403.13952v1'>2403.13952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09834v1")'>All in One and One for All: A Simple yet Effective Method towards
  Cross-domain Graph Pretraining</div>
<div id='2402.09834v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T09:55:39Z</div><div>Authors: Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have revolutionized the fields of computer
vision (CV) and natural language processing (NLP). One of the most notable
advancements of LLMs is that a single model is trained on vast and diverse
datasets spanning multiple domains -- a paradigm we term `All in One'. This
methodology empowers LLMs with super generalization capabilities, facilitating
an encompassing comprehension of varied data distributions. Leveraging these
capabilities, a single LLM demonstrates remarkable versatility across a variety
of domains -- a paradigm we term `One for All'. However, applying this idea to
the graph field remains a formidable challenge, with cross-domain pretraining
often resulting in negative transfer. This issue is particularly important in
few-shot learning scenarios, where the paucity of training data necessitates
the incorporation of external knowledge sources. In response to this challenge,
we propose a novel approach called Graph COordinators for PrEtraining (GCOPE),
that harnesses the underlying commonalities across diverse graph datasets to
enhance few-shot learning. Our novel methodology involves a unification
framework that amalgamates disparate graph datasets during the pretraining
phase to distill and transfer meaningful knowledge to target tasks. Extensive
experiments across multiple graph datasets demonstrate the superior efficacy of
our approach. By successfully leveraging the synergistic potential of multiple
graph datasets for pretraining, our work stands as a pioneering contribution to
the realm of graph foundational model.</div><div><a href='http://arxiv.org/abs/2402.09834v1'>2402.09834v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16024v1")'>HiGPT: Heterogeneous Graph Language Model</div>
<div id='2402.16024v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T08:07:22Z</div><div>Authors: Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang</div><div style='padding-top: 10px; width: 80ex'>Heterogeneous graph learning aims to capture complex relationships and
diverse relational semantics among entities in a heterogeneous graph to obtain
meaningful representations for nodes and edges. Recent advancements in
heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art
performance by considering relation heterogeneity and using specialized message
functions and aggregation rules. However, existing frameworks for heterogeneous
graph learning have limitations in generalizing across diverse heterogeneous
graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune"
paradigm on the same dataset, which restricts their capacity to adapt to new
and unseen data. This raises the question: "Can we generalize heterogeneous
graph models to be well-adapted to diverse downstream learning tasks with
distribution shifts in both node token sets and relation type heterogeneity?''
To tackle those challenges, we propose HiGPT, a general large graph model with
Heterogeneous graph instruction-tuning paradigm. Our framework enables learning
from arbitrary heterogeneous graphs without the need for any fine-tuning
process from downstream datasets. To handle distribution shifts in
heterogeneity, we introduce an in-context heterogeneous graph tokenizer that
captures semantic relationships in different heterogeneous graphs, facilitating
model adaptation. We incorporate a large corpus of heterogeneity-aware graph
instructions into our HiGPT, enabling the model to effectively comprehend
complex relation heterogeneity and distinguish between various types of graph
tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction
augmentation paradigm to mitigate data scarcity by generating diverse and
informative instructions. Through comprehensive evaluations, our proposed
framework demonstrates exceptional performance in terms of generalization
performance.</div><div><a href='http://arxiv.org/abs/2402.16024v1'>2402.16024v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00891v1")'>A Regularization-based Transfer Learning Method for Information
  Extraction via Instructed Graph Decoder</div>
<div id='2403.00891v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T13:04:12Z</div><div>Authors: Kedi Chen, Jie Zhou, Qin Chen, Shunyu Liu, Liang He</div><div style='padding-top: 10px; width: 80ex'>Information extraction (IE) aims to extract complex structured information
from the text. Numerous datasets have been constructed for various IE tasks,
leading to time-consuming and labor-intensive data annotations. Nevertheless,
most prevailing methods focus on training task-specific models, while the
common knowledge among different IE tasks is not explicitly modeled. Moreover,
the same phrase may have inconsistent labels in different tasks, which poses a
big challenge for knowledge transfer using a unified model. In this study, we
propose a regularization-based transfer learning method for IE (TIE) via an
instructed graph decoder. Specifically, we first construct an instruction pool
for datasets from all well-known IE tasks, and then present an instructed graph
decoder, which decodes various complex structures into a graph uniformly based
on corresponding instructions. In this way, the common knowledge shared with
existing datasets can be learned and transferred to a new dataset with new
labels. Furthermore, to alleviate the label inconsistency problem among various
IE tasks, we introduce a task-specific regularization strategy, which does not
update the gradients of two tasks with 'opposite direction'. We conduct
extensive experiments on 12 datasets spanning four IE tasks, and the results
demonstrate the great advantages of our proposed method</div><div><a href='http://arxiv.org/abs/2403.00891v1'>2403.00891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12022v1")'>Distilling Large Language Models for Text-Attributed Graph Learning</div>
<div id='2402.12022v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T10:31:53Z</div><div>Authors: Bo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao</div><div style='padding-top: 10px; width: 80ex'>Text-Attributed Graphs (TAGs) are graphs of connected textual documents.
Graph models can efficiently learn TAGs, but their training heavily relies on
human-annotated labels, which are scarce or even unavailable in many
applications. Large language models (LLMs) have recently demonstrated
remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer
from scalability, cost, and privacy issues. Therefore, in this work, we focus
on synergizing LLMs and graph models with their complementary strengths by
distilling the power of LLMs to a local graph model on TAG learning. To address
the inherent gaps between LLMs (generative models for texts) and graph models
(discriminative models for graphs), we propose first to let LLMs teach an
interpreter with rich textual rationale and then let a student model mimic the
interpreter's reasoning without LLMs' textual rationale. Extensive experiments
validate the efficacy of our proposed framework.</div><div><a href='http://arxiv.org/abs/2402.12022v1'>2402.12022v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06802v1")'>Hierarchical Knowledge Distillation on Text Graph for Data-limited
  Attribute Inference</div>
<div id='2401.06802v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T05:50:34Z</div><div>Authors: Quan Li, Shixiong Jing, Lingwei Chen</div><div style='padding-top: 10px; width: 80ex'>The popularization of social media increases user engagements and generates a
large amount of user-oriented data. Among them, text data (e.g., tweets, blogs)
significantly attracts researchers and speculators to infer user attributes
(e.g., age, gender, location) for fulfilling their intents. Generally, this
line of work casts attribute inference as a text classification problem, and
starts to leverage graph neural networks (GNNs) to utilize higher-level
representations of source texts. However, these text graphs are constructed
over words, suffering from high memory consumption and ineffectiveness on few
labeled texts. To address this challenge, we design a text-graph-based few-shot
learning model for attribute inferences on social media text data. Our model
first constructs and refines a text graph using manifold learning and message
passing, which offers a better trade-off between expressiveness and complexity.
Afterwards, to further use cross-domain texts and unlabeled texts to improve
few-shot performance, a hierarchical knowledge distillation is devised over
text graph to optimize the problem, which derives better text representations,
and advances model generalization ability. Experiments on social media datasets
demonstrate the state-of-the-art performance of our model on attribute
inferences with considerably fewer labeled texts.</div><div><a href='http://arxiv.org/abs/2401.06802v1'>2401.06802v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13556v1")'>Inductive Graph Alignment Prompt: Bridging the Gap between Graph
  Pre-training and Inductive Fine-tuning From Spectral Perspective</div>
<div id='2402.13556v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T06:25:54Z</div><div>Authors: Yuchen Yan, Peiyan Zhang, Zheng Fang, Qingqing Long</div><div style='padding-top: 10px; width: 80ex'>The "Graph pre-training and fine-tuning" paradigm has significantly improved
Graph Neural Networks(GNNs) by capturing general knowledge without manual
annotations for downstream tasks. However, due to the immense gap of data and
tasks between the pre-training and fine-tuning stages, the model performance is
still limited. Inspired by prompt fine-tuning in Natural Language
Processing(NLP), many endeavors have been made to bridge the gap in graph
domain. But existing methods simply reformulate the form of fine-tuning tasks
to the pre-training ones. With the premise that the pre-training graphs are
compatible with the fine-tuning ones, these methods typically operate in
transductive setting. In order to generalize graph pre-training to inductive
scenario where the fine-tuning graphs might significantly differ from
pre-training ones, we propose a novel graph prompt based method called
Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph
pre-training frameworks and analyze the essence of graph pre-training from
graph spectral theory. Then we identify the two sources of the data gap in
inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on
the insight of graph pre-training, we propose to bridge the graph signal gap
and the graph structure gap with learnable prompts in the spectral space. A
theoretical analysis ensures the effectiveness of our method. At last, we
conduct extensive experiments among nodes classification and graph
classification tasks under the transductive, semi-inductive and inductive
settings. The results demonstrate that our proposed method can successfully
bridge the data gap under different settings.</div><div><a href='http://arxiv.org/abs/2402.13556v1'>2402.13556v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07105v2")'>Graph Language Models</div>
<div id='2401.07105v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T16:09:49Z</div><div>Authors: Moritz Plenz, Anette Frank</div><div style='padding-top: 10px; width: 80ex'>While Language Models (LMs) are the workhorses of NLP, their interplay with
structured knowledge graphs (KGs) is still actively researched. Current methods
for encoding such graphs typically either (i) linearize them for embedding with
LMs -- which underutilize structural information, or (ii) use Graph Neural
Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent
text features as well as pretrained LMs. In our work we introduce a novel LM
type, the Graph Language Model (GLM), that integrates the strengths of both
approaches and mitigates their weaknesses. The GLM parameters are initialized
from a pretrained LM to enhance understanding of individual graph concepts and
triplets. Simultaneously, we design the GLM's architecture to incorporate graph
biases, thereby promoting effective knowledge distribution within the graph.
This enables GLMs to process graphs, texts, and interleaved inputs of both.
Empirical evaluations on relation classification tasks show that GLM embeddings
surpass both LM- and GNN-based baselines in supervised and zero-shot setting,
demonstrating their versatility.</div><div><a href='http://arxiv.org/abs/2401.07105v2'>2401.07105v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07311v4")'>Knowledge Graph Large Language Model (KG-LLM) for Link Prediction</div>
<div id='2403.07311v4' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T04:47:29Z</div><div>Authors: Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Chong Zhang, Mengnan Du, Yongfeng Zhang</div><div style='padding-top: 10px; width: 80ex'>The task of predicting multiple links within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, a challenge increasingly
resolvable due to advancements in natural language processing (NLP) and KG
embedding techniques. This paper introduces a novel methodology, the Knowledge
Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP
paradigms, including chain-of-thought (CoT) prompting and in-context learning
(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a
CoT prompt, our framework is designed to discern and learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)
within this framework, employing both non-ICL and ICL tasks for a comprehensive
evaluation. Further, we explore the framework's potential to provide LLMs with
zero-shot capabilities for handling previously unseen prompts. Our experimental
findings discover that integrating ICL and CoT not only augments the
performance of our approach but also significantly boosts the models'
generalization capacity, thereby ensuring more precise predictions in
unfamiliar scenarios.</div><div><a href='http://arxiv.org/abs/2403.07311v4'>2403.07311v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15444v1")'>Unleashing the Power of Imbalanced Modality Information for Multi-modal
  Knowledge Graph Completion</div>
<div id='2402.15444v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T05:48:03Z</div><div>Authors: Yichi Zhang, Zhuo Chen, Lei Liang, Huajun Chen, Wen Zhang</div><div style='padding-top: 10px; width: 80ex'>Multi-modal knowledge graph completion (MMKGC) aims to predict the missing
triples in the multi-modal knowledge graphs by incorporating structural,
visual, and textual information of entities into the discriminant models. The
information from different modalities will work together to measure the triple
plausibility. Existing MMKGC methods overlook the imbalance problem of modality
information among entities, resulting in inadequate modal fusion and
inefficient utilization of the raw modality information. To address the
mentioned problems, we propose Adaptive Multi-modal Fusion and Modality
Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality
information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive
modality weights and further generates adversarial samples by
modality-adversarial training to enhance the imbalanced modality information.
Our approach is a co-design of the MMKGC model and training strategy which can
outperform 19 recent MMKGC methods and achieve new state-of-the-art results on
three public MMKGC benchmarks. Our code and data have been released at
https://github.com/zjukg/AdaMF-MAT.</div><div><a href='http://arxiv.org/abs/2402.15444v1'>2402.15444v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01203v1")'>Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment</div>
<div id='2403.01203v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T12:44:59Z</div><div>Authors: Luyao Wang, Pengnian Qi, Xigang Bao, Chunlai Zhou, Biao Qin</div><div style='padding-top: 10px; width: 80ex'>Multi-modal entity alignment (MMEA) aims to identify equivalent entities
between two multi-modal knowledge graphs for integration. Unfortunately, prior
arts have attempted to improve the interaction and fusion of multi-modal
information, which have overlooked the influence of modal-specific noise and
the usage of labeled and unlabeled data in semi-supervised settings. In this
work, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment
(PCMEA) in a semi-supervised way. Specifically, in order to generate holistic
entity representations, we first devise various embedding modules and attention
mechanisms to extract visual, structural, relational, and attribute features.
Different from the prior direct fusion methods, we next propose to exploit
mutual information maximization to filter the modal-specific noise and to
augment modal-invariant commonality. Then, we combine pseudo-label calibration
with momentum-based contrastive learning to make full use of the labeled and
unlabeled data, which improves the quality of pseudo-label and pulls aligned
entities closer. Finally, extensive experiments on two MMEA datasets
demonstrate the effectiveness of our PCMEA, which yields state-of-the-art
performance.</div><div><a href='http://arxiv.org/abs/2403.01203v1'>2403.01203v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03025v1")'>Understanding and Guiding Weakly Supervised Entity Alignment with
  Potential Isomorphism Propagation</div>
<div id='2402.03025v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:06:15Z</div><div>Authors: Yuanyi Wang, Wei Tang, Haifeng Sun, Zirui Zhuang, Xiaoyuan Fu, Jingyu Wang, Qi Qi, Jianxin Liao</div><div style='padding-top: 10px; width: 80ex'>Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent
entities across diverse knowledge graphs (KGs) using only a limited number of
seed alignments. Despite substantial advances in aggregation-based weakly
supervised EA, the underlying mechanisms in this setting remain unexplored. In
this paper, we present a propagation perspective to analyze weakly supervised
EA and explain the existing aggregation-based EA models. Our theoretical
analysis reveals that these models essentially seek propagation operators for
pairwise entity similarities. We further prove that, despite the structural
heterogeneity of different KGs, the potentially aligned entities within
aggregation-based EA models have isomorphic subgraphs, which is the core
premise of EA but has not been investigated. Leveraging this insight, we
introduce a potential isomorphism propagation operator to enhance the
propagation of neighborhood information across KGs. We develop a general EA
framework, PipEA, incorporating this operator to improve the accuracy of every
type of aggregation-based model without altering the learning process.
Extensive experiments substantiate our theoretical findings and demonstrate
PipEA's significant performance gains over state-of-the-art weakly supervised
EA methods. Our work not only advances the field but also enhances our
comprehension of aggregation-based weakly supervised EA.</div><div><a href='http://arxiv.org/abs/2402.03025v1'>2402.03025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07630v2")'>G-Retriever: Retrieval-Augmented Generation for Textual Graph
  Understanding and Question Answering</div>
<div id='2402.07630v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T13:13:04Z</div><div>Authors: Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi</div><div style='padding-top: 10px; width: 80ex'>Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop
our Graph Question Answering (GraphQA) benchmark with data collected from
different tasks. Then, we propose our G-Retriever approach, which integrates
the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can
be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and resists
hallucination. (Our codes and datasets are available at:
https://github.com/XiaoxinHe/G-Retriever.)</div><div><a href='http://arxiv.org/abs/2402.07630v2'>2402.07630v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07040v1")'>All in One: Multi-Task Prompting for Graph Neural Networks (Extended
  Abstract)</div>
<div id='2403.07040v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:04:58Z</div><div>Authors: Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan</div><div style='padding-top: 10px; width: 80ex'>This paper is an extended abstract of our original work published in KDD23,
where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li,
Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural
networks. KDD 23) The paper introduces a novel approach to bridging the gap
between pre-trained graph models and the diverse tasks they're applied to,
inspired by the success of prompt learning in NLP. Recognizing the challenge of
aligning pre-trained models with varied graph tasks (node level, edge level,
and graph level), which can lead to negative transfer and poor performance, we
propose a multi-task prompting method for graphs. This method involves unifying
graph and language prompt formats, enabling NLP's prompting strategies to be
adapted for graph tasks. By analyzing the task space of graph applications, we
reformulate problems to fit graph-level tasks and apply meta-learning to
improve prompt initialization for multiple tasks. Experiments show our method's
effectiveness in enhancing model performance across different graph tasks.
  Beyond the original work, in this extended abstract, we further discuss the
graph prompt from a bigger picture and provide some of the latest work toward
this area.</div><div><a href='http://arxiv.org/abs/2403.07040v1'>2403.07040v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00284v1")'>PAP-REC: Personalized Automatic Prompt for Recommendation Language Model</div>
<div id='2402.00284v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T02:29:16Z</div><div>Authors: Zelong Li, Jianchao Ji, Yingqiang Ge, Wenyue Hua, Yongfeng Zhang</div><div style='padding-top: 10px; width: 80ex'>Recently emerged prompt-based Recommendation Language Models (RLM) can solve
multiple recommendation tasks uniformly. The RLMs make full use of the
inherited knowledge learned from the abundant pre-training data to solve the
downstream recommendation tasks by prompts, without introducing additional
parameters or network training. However, handcrafted prompts require
significant expertise and human effort since slightly rewriting prompts may
cause massive performance changes. In this paper, we propose PAP-REC, a
framework to generate the Personalized Automatic Prompt for RECommendation
language models to mitigate the inefficiency and ineffectiveness problems
derived from manually designed prompts. Specifically, personalized automatic
prompts allow different users to have different prompt tokens for the same
task, automatically generated using a gradient-based method. One challenge for
personalized automatic prompt generation for recommendation language models is
the extremely large search space, leading to a long convergence time. To
effectively and efficiently address the problem, we develop surrogate metrics
and leverage an alternative updating schedule for prompting recommendation
language models. Experimental results show that our PAP-REC framework manages
to generate personalized prompts, and the automatically generated prompts
outperform manually constructed prompts and also outperform various baseline
recommendation models. The source code of the work is available at
https://github.com/rutgerswiselab/PAP-REC.</div><div><a href='http://arxiv.org/abs/2402.00284v1'>2402.00284v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00781v2")'>ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender
  Chatbots through an LLM-Augmented Framework</div>
<div id='2403.00781v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T06:07:17Z</div><div>Authors: Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman Azimi, Ramesh Jain, Amir M. Rahmani</div><div style='padding-top: 10px; width: 80ex'>The profound impact of food on health necessitates advanced
nutrition-oriented food recommendation services. Conventional methods often
lack the crucial elements of personalization, explainability, and
interactivity. While Large Language Models (LLMs) bring interpretability and
explainability, their standalone use falls short of achieving true
personalization. In this paper, we introduce ChatDiet, a novel LLM-powered
framework designed specifically for personalized nutrition-oriented food
recommendation chatbots. ChatDiet integrates personal and population models,
complemented by an orchestrator, to seamlessly retrieve and process pertinent
information. The personal model leverages causal discovery and inference
techniques to assess personalized nutritional effects for a specific user,
whereas the population model provides generalized information on food
nutritional content. The orchestrator retrieves, synergizes and delivers the
output of both models to the LLM, providing tailored food recommendations
designed to support targeted health outcomes. The result is a dynamic delivery
of personalized and explainable food recommendations, tailored to individual
user preferences. Our evaluation of ChatDiet includes a compelling case study,
where we establish a causal personal model to estimate individual nutrition
effects. Our assessments, including a food recommendation test showcasing a
92\% effectiveness rate, coupled with illustrative dialogue examples,
underscore ChatDiet's strengths in explainability, personalization, and
interactivity.</div><div><a href='http://arxiv.org/abs/2403.00781v2'>2403.00781v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05873v1")'>LEGION: Harnessing Pre-trained Language Models for GitHub Topic
  Recommendations with Distribution-Balance Loss</div>
<div id='2403.05873v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T10:49:31Z</div><div>Authors: Yen-Trang Dang, Thanh-Le Cong, Phuc-Thanh Nguyen, Anh M. T. Bui, Phuong T. Nguyen, Bach Le, Quyet-Thang Huynh</div><div style='padding-top: 10px; width: 80ex'>Open-source development has revolutionized the software industry by promoting
collaboration, transparency, and community-driven innovation. Today, a vast
amount of various kinds of open-source software, which form networks of
repositories, is often hosted on GitHub - a popular software development
platform. To enhance the discoverability of the repository networks, i.e.,
groups of similar repositories, GitHub introduced repository topics in 2017
that enable users to more easily explore relevant projects by type, technology,
and more. It is thus crucial to accurately assign topics for each GitHub
repository. Current methods for automatic topic recommendation rely heavily on
TF-IDF for encoding textual data, presenting challenges in understanding
semantic nuances. This paper addresses the limitations of existing techniques
by proposing Legion, a novel approach that leverages Pre-trained Language
Models (PTMs) for recommending topics for GitHub repositories. The key novelty
of Legion is three-fold. First, Legion leverages the extensive capabilities of
PTMs in language understanding to capture contextual information and semantic
meaning in GitHub repositories. Second, Legion overcomes the challenge of
long-tailed distribution, which results in a bias toward popular topics in
PTMs, by proposing a Distribution-Balanced Loss (DB Loss) to better train the
PTMs. Third, Legion employs a filter to eliminate vague recommendations,
thereby improving the precision of PTMs. Our empirical evaluation on a
benchmark dataset of real-world GitHub repositories shows that Legion can
improve vanilla PTMs by up to 26% on recommending GitHubs topics. Legion also
can suggest GitHub topics more precisely and effectively than the
state-of-the-art baseline with an average improvement of 20% and 5% in terms of
Precision and F1-score, respectively.</div><div><a href='http://arxiv.org/abs/2403.05873v1'>2403.05873v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03942v1")'>The Heuristic Core: Understanding Subnetwork Generalization in
  Pretrained Language Models</div>
<div id='2403.03942v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T18:50:14Z</div><div>Authors: Adithya Bhaskar, Dan Friedman, Danqi Chen</div><div style='padding-top: 10px; width: 80ex'>Prior work has found that pretrained language models (LMs) fine-tuned with
different random seeds can achieve similar in-domain performance but generalize
differently on tests of syntactic generalization. In this work, we show that,
even within a single model, we can find multiple subnetworks that perform
similarly in-domain, but generalize vastly differently. To better understand
these phenomena, we investigate if they can be understood in terms of
"competing subnetworks": the model initially represents a variety of distinct
algorithms, corresponding to different subnetworks, and generalization occurs
when it ultimately converges to one. This explanation has been used to account
for generalization in simple algorithmic tasks. Instead of finding competing
subnetworks, we find that all subnetworks -- whether they generalize or not --
share a set of attention heads, which we refer to as the heuristic core.
Further analysis suggests that these attention heads emerge early in training
and compute shallow, non-generalizing features. The model learns to generalize
by incorporating additional attention heads, which depend on the outputs of the
"heuristic" heads to compute higher-level features. Overall, our results offer
a more detailed picture of the mechanisms for syntactic generalization in
pretrained LMs.</div><div><a href='http://arxiv.org/abs/2403.03942v1'>2403.03942v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07899v1")'>A systematic investigation of learnability from single child linguistic
  input</div>
<div id='2402.07899v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:58:58Z</div><div>Authors: Yulu Qin, Wentao Wang, Brenden M. Lake</div><div style='padding-top: 10px; width: 80ex'>Language models (LMs) have demonstrated remarkable proficiency in generating
linguistically coherent text, sparking discussions about their relevance to
understanding human language learnability. However, a significant gap exists
between the training data for these models and the linguistic input a child
receives. LMs are typically trained on data that is orders of magnitude larger
and fundamentally different from child-directed speech (Warstadt and Bowman,
2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our
research focuses on training LMs on subsets of a single child's linguistic
input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in
this setting can form syntactic and semantic word clusters and develop
sensitivity to certain linguistic phenomena, but they only considered LSTMs and
simpler neural networks trained from just one single-child dataset. Here, to
examine the robustness of learnability from single-child input, we
systematically train six different model architectures on five datasets (3
single-child and 2 baselines). We find that the models trained on single-child
datasets showed consistent results that matched with previous work,
underscoring the robustness of forming meaningful syntactic and semantic
representations from a subset of a child's linguistic input.</div><div><a href='http://arxiv.org/abs/2402.07899v1'>2402.07899v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16998v1")'>What Do Language Models Hear? Probing for Auditory Representations in
  Language Models</div>
<div id='2402.16998v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T20:13:58Z</div><div>Authors: Jerry Ngo, Yoon Kim</div><div style='padding-top: 10px; width: 80ex'>This work explores whether language models encode meaningfully grounded
representations of sounds of objects. We learn a linear probe that retrieves
the correct text representation of an object given a snippet of audio related
to that object, where the sound representation is given by a pretrained audio
model. This probe is trained via a contrastive loss that pushes the language
representations and sound representations of an object to be close to one
another. After training, the probe is tested on its ability to generalize to
objects that were not seen during training. Across different language models
and audio models, we find that the probe generalization is above chance in many
cases, indicating that despite being trained only on raw text, language models
encode grounded knowledge of sounds for some objects.</div><div><a href='http://arxiv.org/abs/2402.16998v1'>2402.16998v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13086v1")'>Listenable Maps for Audio Classifiers</div>
<div id='2403.13086v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T18:32:48Z</div><div>Authors: Francesco Paissan, Mirco Ravanelli, Cem Subakan</div><div style='padding-top: 10px; width: 80ex'>Despite the impressive performance of deep learning models across diverse
tasks, their complexity poses challenges for interpretation. This challenge is
particularly evident for audio signals, where conveying interpretations becomes
inherently difficult. To address this issue, we introduce Listenable Maps for
Audio Classifiers (L-MAC), a posthoc interpretation method that generates
faithful and listenable interpretations. L-MAC utilizes a decoder on top of a
pretrained classifier to generate binary masks that highlight relevant portions
of the input audio. We train the decoder with a special loss that maximizes the
confidence of the classifier decision on the masked-in portion of the audio
while minimizing the probability of model output for the masked-out portion.
Quantitative evaluations on both in-domain and out-of-domain data demonstrate
that L-MAC consistently produces more faithful interpretations than several
gradient and masking-based methodologies. Furthermore, a user study confirms
that, on average, users prefer the interpretations generated by the proposed
technique.</div><div><a href='http://arxiv.org/abs/2403.13086v1'>2403.13086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15390v1")'>Explorations of Self-Repair in Language Models</div>
<div id='2402.15390v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:42:12Z</div><div>Authors: Cody Rushing, Neel Nanda</div><div style='padding-top: 10px; width: 80ex'>Prior interpretability research studying narrow distributions has
preliminarily identified self-repair, a phenomena where if components in large
language models are ablated, later components will change their behavior to
compensate. Our work builds off this past literature, demonstrating that
self-repair exists on a variety of models families and sizes when ablating
individual attention heads on the full training distribution. We further show
that on the full training distribution self-repair is imperfect, as the
original direct effect of the head is not fully restored, and noisy, since the
degree of self-repair varies significantly across different prompts (sometimes
overcorrecting beyond the original effect). We highlight two different
mechanisms that contribute to self-repair, including changes in the final
LayerNorm scaling factor (which can repair up to 30% of the direct effect) and
sparse sets of neurons implementing Anti-Erasure. We additionally discuss the
implications of these results for interpretability practitioners and close with
a more speculative discussion on the mystery of why self-repair occurs in these
models at all, highlighting evidence for the Iterative Inference hypothesis in
language models, a framework that predicts self-repair.</div><div><a href='http://arxiv.org/abs/2402.15390v1'>2402.15390v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17700v1")'>RAVEL: Evaluating Interpretability Methods on Disentangling Language
  Model Representations</div>
<div id='2402.17700v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:25:37Z</div><div>Authors: Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger</div><div style='padding-top: 10px; width: 80ex'>Individual neurons participate in the representation of multiple high-level
concepts. To what extent can different interpretability methods successfully
disentangle these roles? To help address this question, we introduce RAVEL
(Resolving Attribute-Value Entanglements in Language Models), a dataset that
enables tightly controlled, quantitative comparisons between a variety of
existing interpretability methods. We use the resulting conceptual framework to
define the new method of Multi-task Distributed Alignment Search (MDAS), which
allows us to find distributed representations satisfying multiple causal
criteria. With Llama2-7B as the target language model, MDAS achieves
state-of-the-art results on RAVEL, demonstrating the importance of going beyond
neuron-level analyses to identify features distributed across activations. We
release our benchmark at https://github.com/explanare/ravel.</div><div><a href='http://arxiv.org/abs/2402.17700v1'>2402.17700v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12181v1")'>Universal Neurons in GPT2 Language Models</div>
<div id='2401.12181v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:11:01Z</div><div>Authors: Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, Dimitris Bertsimas</div><div style='padding-top: 10px; width: 80ex'>A basic question within the emerging field of mechanistic interpretability is
the degree to which neural networks learn the same underlying mechanisms. In
other words, are neural mechanisms universal across different models? In this
work, we study the universality of individual neurons across GPT2 models
trained from different initial random seeds, motivated by the hypothesis that
universal neurons are likely to be interpretable. In particular, we compute
pairwise correlations of neuron activations over 100 million tokens for every
neuron pair across five different seeds and find that 1-5\% of neurons are
universal, that is, pairs of neurons which consistently activate on the same
inputs. We then study these universal neurons in detail, finding that they
usually have clear interpretations and taxonomize them into a small number of
neuron families. We conclude by studying patterns in neuron weights to
establish several universal functional roles of neurons in simple circuits:
deactivating attention heads, changing the entropy of the next token
distribution, and predicting the next token to (not) be within a particular
set.</div><div><a href='http://arxiv.org/abs/2401.12181v1'>2401.12181v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15055v1")'>Interpreting Context Look-ups in Transformers: Investigating
  Attention-MLP Interactions</div>
<div id='2402.15055v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T02:15:47Z</div><div>Authors: Clement Neo, Shay B. Cohen, Fazl Barez</div><div style='padding-top: 10px; width: 80ex'>In this paper, we investigate the interplay between attention heads and
specialized "next-token" neurons in the Multilayer Perceptron that predict
specific tokens. By prompting an LLM like GPT-4 to explain these model
internals, we can elucidate attention mechanisms that activate certain
next-token neurons. Our analysis identifies attention heads that recognize
contexts relevant to predicting a particular token, activating the associated
neuron through the residual connection. We focus specifically on heads in
earlier layers consistently activating the same next-token neuron across
similar prompts. Exploring these differential activation patterns reveals that
heads that specialize for distinct linguistic contexts are tied to generating
certain tokens. Overall, our method combines neural explanations and probing
isolated components to illuminate how attention enables context-dependent,
specialized processing in LLMs.</div><div><a href='http://arxiv.org/abs/2402.15055v1'>2402.15055v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02883v1")'>Approximate Attributions for Off-the-Shelf Siamese Transformers</div>
<div id='2402.02883v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:49:05Z</div><div>Authors: Lucas Möller, Dmitry Nikolaev, Sebastian Padó</div><div style='padding-top: 10px; width: 80ex'>Siamese encoders such as sentence transformers are among the least understood
deep models. Established attribution methods cannot tackle this model class
since it compares two inputs rather than processing a single one. To address
this gap, we have recently proposed an attribution method specifically for
Siamese encoders (M\"oller et al., 2023). However, it requires models to be
adjusted and fine-tuned and therefore cannot be directly applied to
off-the-shelf models. In this work, we reassess these restrictions and propose
(i) a model with exact attribution ability that retains the original model's
predictive performance and (ii) a way to compute approximate attributions for
off-the-shelf models. We extensively compare approximate and exact attributions
and use them to analyze the models' attendance to different linguistic aspects.
We gain insights into which syntactic roles Siamese transformers attend to,
confirm that they mostly ignore negation, explore how they judge semantically
opposite adjectives, and find that they exhibit lexical bias.</div><div><a href='http://arxiv.org/abs/2402.02883v1'>2402.02883v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13512v1")'>From Self-Attention to Markov Models: Unveiling the Dynamics of
  Generative Transformers</div>
<div id='2402.13512v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T03:51:34Z</div><div>Authors: M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, Samet Oymak</div><div style='padding-top: 10px; width: 80ex'>Modern language models rely on the transformer architecture and attention
mechanism to perform language understanding and text generation. In this work,
we study learning a 1-layer self-attention model from a set of prompts and
associated output data sampled from the model. We first establish a precise
mapping between the self-attention mechanism and Markov models: Inputting a
prompt to the model samples the output token according to a context-conditioned
Markov chain (CCMC) which weights the transition matrix of a base Markov chain.
Additionally, incorporating positional encoding results in position-dependent
scaling of the transition probabilities. Building on this formalism, we develop
identifiability/coverage conditions for the prompt distribution that guarantee
consistent estimation and establish sample complexity guarantees under IID
samples. Finally, we study the problem of learning from a single output
trajectory generated from an initial prompt. We characterize an intriguing
winner-takes-all phenomenon where the generative process implemented by
self-attention collapses into sampling a limited subset of tokens due to its
non-mixing nature. This provides a mathematical explanation to the tendency of
modern LLMs to generate repetitive text. In summary, the equivalence to CCMC
provides a simple but powerful framework to study self-attention and its
properties.</div><div><a href='http://arxiv.org/abs/2402.13512v1'>2402.13512v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15415v1")'>The Impact of LoRA on the Emergence of Clusters in Transformers</div>
<div id='2402.15415v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:26:01Z</div><div>Authors: Hugo Koubbi, Matthieu Boussard, Louis Hernandez</div><div style='padding-top: 10px; width: 80ex'>In this paper, we employ the mathematical framework on Transformers developed
by
\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}
to explore how variations in attention parameters and initial token values
impact the structural dynamics of token clusters. Our analysis demonstrates
that while the clusters within a modified attention matrix dynamics can exhibit
significant divergence from the original over extended periods, they maintain
close similarities over shorter intervals, depending on the parameter
differences. This work contributes to the fine-tuning field through practical
applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our
understanding of the behavior of LoRA-enhanced Transformer models.</div><div><a href='http://arxiv.org/abs/2402.15415v1'>2402.15415v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13006v1")'>Investigating the Impact of Model Instability on Explanations and
  Uncertainty</div>
<div id='2402.13006v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T13:41:21Z</div><div>Authors: Sara Vera Marjanović, Isabelle Augenstein, Christina Lioma</div><div style='padding-top: 10px; width: 80ex'>Explainable AI methods facilitate the understanding of model behaviour, yet,
small, imperceptible perturbations to inputs can vastly distort explanations.
As these explanations are typically evaluated holistically, before model
deployment, it is difficult to assess when a particular explanation is
trustworthy. Some studies have tried to create confidence estimators for
explanations, but none have investigated an existing link between uncertainty
and explanation quality. We artificially simulate epistemic uncertainty in text
input by introducing noise at inference time. In this large-scale empirical
study, we insert different levels of noise perturbations and measure the effect
on the output of pre-trained language models and different uncertainty metrics.
Realistic perturbations have minimal effect on performance and explanations,
yet masking has a drastic effect. We find that high uncertainty doesn't
necessarily imply low explanation plausibility; the correlation between the two
metrics can be moderately positive when noise is exposed during the training
process. This suggests that noise-augmented models may be better at identifying
salient tokens when uncertain. Furthermore, when predictive and epistemic
uncertainty measures are over-confident, the robustness of a saliency map to
perturbation can indicate model stability issues. Integrated Gradients shows
the overall greatest robustness to perturbation, while still showing
model-specific patterns in performance; however, this phenomenon is limited to
smaller Transformer-based language models.</div><div><a href='http://arxiv.org/abs/2402.13006v1'>2402.13006v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01865v1")'>What Will My Model Forget? Forecasting Forgotten Examples in Language
  Model Refinement</div>
<div id='2402.01865v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:43:15Z</div><div>Authors: Xisen Jin, Xiang Ren</div><div style='padding-top: 10px; width: 80ex'>Language models deployed in the wild make errors. However, simply updating
the model with the corrected error instances causes catastrophic forgetting --
the updated model makes errors on instances learned during the instruction
tuning or upstream training phase. Randomly replaying upstream data yields
unsatisfactory performance and often comes with high variance and poor
controllability. To this end, we try to forecast upstream examples that will be
forgotten due to a model update for improved controllability of the replay
process and interpretability. We train forecasting models given a collection of
online learned examples and corresponding forgotten upstream pre-training
examples. We propose a partially interpretable forecasting model based on the
observation that changes in pre-softmax logit scores of pretraining examples
resemble that of online learned examples, which performs decently on BART but
fails on T5 models. We further show a black-box classifier based on inner
products of example representations achieves better forecasting performance
over a series of setups. Finally, we show that we reduce forgetting of upstream
pretraining examples by replaying examples that are forecasted to be forgotten,
demonstrating the practical utility of forecasting example forgetting.</div><div><a href='http://arxiv.org/abs/2402.01865v1'>2402.01865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13130v1")'>Self-generated Replay Memories for Continual Neural Machine Translation</div>
<div id='2403.13130v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:59:54Z</div><div>Authors: Michele Resta, Davide Bacciu</div><div style='padding-top: 10px; width: 80ex'>Modern Neural Machine Translation systems exhibit strong performance in
several different languages and are constantly improving. Their ability to
learn continuously is, however, still severely limited by the catastrophic
forgetting issue. In this work, we leverage a key property of encoder-decoder
Transformers, i.e. their generative ability, to propose a novel approach to
continually learning Neural Machine Translation systems. We show how this can
effectively learn on a stream of experiences comprising different languages, by
leveraging a replay memory populated by using the model itself as a generator
of parallel sentences. We empirically demonstrate that our approach can
counteract catastrophic forgetting without requiring explicit memorization of
training data. Code will be publicly available upon publication. Code:
https://github.com/m-resta/sg-rep</div><div><a href='http://arxiv.org/abs/2403.13130v1'>2403.13130v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11834v1")'>Towards Understanding the Relationship between In-context Learning and
  Compositional Generalization</div>
<div id='2403.11834v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:45:52Z</div><div>Authors: Sungjun Han, Sebastian Padó</div><div style='padding-top: 10px; width: 80ex'>According to the principle of compositional generalization, the meaning of a
complex expression can be understood as a function of the meaning of its parts
and of how they are combined. This principle is crucial for human language
processing and also, arguably, for NLP models in the face of
out-of-distribution data. However, many neural network models, including
Transformers, have been shown to struggle with compositional generalization. In
this paper, we hypothesize that forcing models to in-context learn can provide
an inductive bias to promote compositional generalization. To test this
hypothesis, we train a causal Transformer in a setting that renders ordinary
learning very difficult: we present it with different orderings of the training
instance and shuffle instance labels. This corresponds to training the model on
all possible few-shot learning problems attainable from the dataset. The model
can solve the task, however, by utilizing earlier examples to generalize to
later ones (i.e. in-context learning). In evaluations on the datasets, SCAN,
COGS, and GeoQuery, models trained in this manner indeed show improved
compositional generalization. This indicates the usefulness of in-context
learning problems as an inductive bias for generalization.</div><div><a href='http://arxiv.org/abs/2403.11834v1'>2403.11834v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01629v1")'>Position Paper: Generalized grammar rules and structure-based
  generalization beyond classical equivariance for lexical tasks and
  transduction</div>
<div id='2402.01629v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:44:37Z</div><div>Authors: Mircea Petrache, Shubhendu Trivedi</div><div style='padding-top: 10px; width: 80ex'>Compositional generalization is one of the main properties which
differentiates lexical learning in humans from state-of-art neural networks. We
propose a general framework for building models that can generalize
compositionally using the concept of Generalized Grammar Rules (GGRs), a class
of symmetry-based compositional constraints for transduction tasks, which we
view as a transduction analogue of equivariance constraints in physics-inspired
tasks. Besides formalizing generalized notions of symmetry for language
transduction, our framework is general enough to contain many existing works as
special cases. We present ideas on how GGRs might be implemented, and in the
process draw connections to reinforcement learning and other areas of research.</div><div><a href='http://arxiv.org/abs/2402.01629v1'>2402.01629v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15030v2")'>On the generalization capacity of neural networks during generic
  multimodal reasoning</div>
<div id='2401.15030v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T17:42:59Z</div><div>Authors: Takuya Ito, Soham Dan, Mattia Rigotti, James Kozloski, Murray Campbell</div><div style='padding-top: 10px; width: 80ex'>The advent of the Transformer has led to the development of large language
models (LLM), which appear to demonstrate human-like capabilities. To assess
the generality of this class of models and a variety of other base neural
network architectures to multimodal domains, we evaluated and compared their
capacity for multimodal generalization. We introduce a multimodal
question-answer benchmark to evaluate three specific types of
out-of-distribution (OOD) generalization performance: distractor generalization
(generalization in the presence of distractors), systematic compositional
generalization (generalization to new task permutations), and productive
compositional generalization (generalization to more complex tasks structures).
We found that across model architectures (e.g., RNNs, Transformers, Perceivers,
etc.), models with multiple attention layers, or models that leveraged
cross-attention mechanisms between input domains, fared better. Our positive
results demonstrate that for multimodal distractor and systematic
generalization, either cross-modal attention or models with deeper attention
layers are key architectural features required to integrate multimodal inputs.
On the other hand, neither of these architectural features led to productive
generalization, suggesting fundamental limitations of existing architectures
for specific types of multimodal generalization. These results demonstrate the
strengths and limitations of specific architectural components underlying
modern neural models for multimodal reasoning. Finally, we provide Generic COG
(gCOG), a configurable benchmark with several multimodal generalization splits,
for future studies to explore.</div><div><a href='http://arxiv.org/abs/2401.15030v2'>2401.15030v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06121v1")'>TOFU: A Task of Fictitious Unlearning for LLMs</div>
<div id='2401.06121v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:57:12Z</div><div>Authors: Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, J. Zico Kolter</div><div style='padding-top: 10px; width: 80ex'>Large language models trained on massive corpora of data from the web can
memorize and reproduce sensitive or private data raising both legal and ethical
concerns. Unlearning, or tuning models to forget information present in their
training data, provides us with a way to protect private data after training.
Although several methods exist for such unlearning, it is unclear to what
extent they result in models equivalent to those where the data to be forgotten
was never learned in the first place. To address this challenge, we present
TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen
our understanding of unlearning. We offer a dataset of 200 diverse synthetic
author profiles, each consisting of 20 question-answer pairs, and a subset of
these profiles called the forget set that serves as the target for unlearning.
We compile a suite of metrics that work together to provide a holistic picture
of unlearning efficacy. Finally, we provide a set of baseline results from
existing unlearning algorithms. Importantly, none of the baselines we consider
show effective unlearning motivating continued efforts to develop approaches
for unlearning that effectively tune models so that they truly behave as if
they were never trained on the forget data at all.</div><div><a href='http://arxiv.org/abs/2401.06121v1'>2401.06121v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11265v1")'>Forging the Forger: An Attempt to Improve Authorship Verification via
  Data Augmentation</div>
<div id='2403.11265v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T16:36:26Z</div><div>Authors: Silvia Corbara, Alejandro Moreo</div><div style='padding-top: 10px; width: 80ex'>Authorship Verification (AV) is a text classification task concerned with
inferring whether a candidate text has been written by one specific author or
by someone else. It has been shown that many AV systems are vulnerable to
adversarial attacks, where a malicious author actively tries to fool the
classifier by either concealing their writing style, or by imitating the style
of another author. In this paper, we investigate the potential benefits of
augmenting the classifier training set with (negative) synthetic examples.
These synthetic examples are generated to imitate the style of the author of
interest. We analyze the improvements in classifier prediction that this
augmentation brings to bear in the task of AV in an adversarial setting. In
particular, we experiment with three different generator architectures (one
based on Recurrent Neural Networks, another based on small-scale transformers,
and another based on the popular GPT model) and with two training strategies
(one inspired by standard Language Models, and another inspired by Wasserstein
Generative Adversarial Networks). We evaluate our hypothesis on five datasets
(three of which have been specifically collected to represent an adversarial
setting) and using two learning algorithms for the AV classifier (Support
Vector Machines and Convolutional Neural Networks). This experimentation has
yielded negative results, revealing that, although our methodology proves
effective in many adversarial settings, its benefits are too sporadic for a
pragmatical application.</div><div><a href='http://arxiv.org/abs/2403.11265v1'>2403.11265v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07347v1")'>Accuracy of TextFooler black box adversarial attacks on 01 loss sign
  activation neural network ensemble</div>
<div id='2402.07347v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T00:36:34Z</div><div>Authors: Yunzhe Xue, Usman Roshan</div><div style='padding-top: 10px; width: 80ex'>Recent work has shown the defense of 01 loss sign activation neural networks
against image classification adversarial attacks. A public challenge to attack
the models on CIFAR10 dataset remains undefeated. We ask the following question
in this study: are 01 loss sign activation neural networks hard to deceive with
a popular black box text adversarial attack program called TextFooler? We study
this question on four popular text classification datasets: IMDB reviews, Yelp
reviews, MR sentiment classification, and AG news classification. We find that
our 01 loss sign activation network is much harder to attack with TextFooler
compared to sigmoid activation cross entropy and binary neural networks. We
also study a 01 loss sign activation convolutional neural network with a novel
global pooling step specific to sign activation networks. With this new
variation we see a significant gain in adversarial accuracy rendering
TextFooler practically useless against it. We make our code freely available at
\url{https://github.com/zero-one-loss/wordcnn01} and
\url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that
01 loss sign activation networks could be further developed to create fool
proof models against text adversarial attacks.</div><div><a href='http://arxiv.org/abs/2402.07347v1'>2402.07347v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06787v1")'>Deep Learning Based Cyberbullying Detection in Bangla Language</div>
<div id='2401.06787v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T04:58:59Z</div><div>Authors: Sristy Shidul Nath, Razuan Karim, Mahdi H. Miraz</div><div style='padding-top: 10px; width: 80ex'>The Internet is currently the largest platform for global communication
including expressions of opinions, reviews, contents, images, videos and so
forth. Moreover, social media has now become a very broad and highly engaging
platform due to its immense popularity and swift adoption trend. Increased
social networking, however, also has detrimental impacts on the society leading
to a range of unwanted phenomena, such as online assault, intimidation, digital
bullying, criminality and trolling. Hence, cyberbullying has become a pervasive
and worrying problem that poses considerable psychological and emotional harm
to the people, particularly amongst the teens and the young adults. In order to
lessen its negative effects and provide victims with prompt support, a great
deal of research to identify cyberbullying instances at various online
platforms is emerging. In comparison to other languages, Bangla (also known as
Bengali) has fewer research studies in this domain. This study demonstrates a
deep learning strategy for identifying cyberbullying in Bengali, using a
dataset of 12282 versatile comments from multiple social media sites. In this
study, a two-layer bidirectional long short-term memory (Bi-LSTM) model has
been built to identify cyberbullying, using a variety of optimisers as well as
5-fold cross validation. To evaluate the functionality and efficacy of the
proposed system, rigorous assessment and validation procedures have been
employed throughout the project. The results of this study reveals that the
proposed model's accuracy, using momentum-based stochastic gradient descent
(SGD) optimiser, is 94.46%. It also reflects a higher accuracy of 95.08% and a
F1 score of 95.23% using Adam optimiser as well as a better accuracy of 94.31%
in 5-fold cross validation.</div><div><a href='http://arxiv.org/abs/2401.06787v1'>2401.06787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14135v1")'>Convolutional Neural Networks can achieve binary bail judgement
  classification</div>
<div id='2401.14135v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T12:31:41Z</div><div>Authors: Amit Barman, Devangan Roy, Debapriya Paul, Indranil Dutta, Shouvik Kumar Guha, Samir Karmakar, Sudip Kumar Naskar</div><div style='padding-top: 10px; width: 80ex'>There is an evident lack of implementation of Machine Learning (ML) in the
legal domain in India, and any research that does take place in this domain is
usually based on data from the higher courts of law and works with English
data. The lower courts and data from the different regional languages of India
are often overlooked. In this paper, we deploy a Convolutional Neural Network
(CNN) architecture on a corpus of Hindi legal documents. We perform a bail
Prediction task with the help of a CNN model and achieve an overall accuracy of
93\% which is an improvement on the benchmark accuracy, set by Kapoor et al.
(2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.</div><div><a href='http://arxiv.org/abs/2401.14135v1'>2401.14135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16458v1")'>Credit Risk Meets Large Language Models: Building a Risk Indicator from
  Loan Descriptions in P2P Lending</div>
<div id='2401.16458v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T10:11:05Z</div><div>Authors: Mario Sanz-Guerrero, Javier Arroyo</div><div style='padding-top: 10px; width: 80ex'>Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,
linking borrowers with lenders through online platforms. However, P2P lending
faces the challenge of information asymmetry, as lenders often lack sufficient
data to assess the creditworthiness of borrowers. This paper proposes a novel
approach to address this issue by leveraging the textual descriptions provided
by borrowers during the loan application process. Our methodology involves
processing these textual descriptions using a Large Language Model (LLM), a
powerful tool capable of discerning patterns and semantics within the text.
Transfer learning is applied to adapt the LLM to the specific task at hand.
  Our results derived from the analysis of the Lending Club dataset show that
the risk score generated by BERT, a widely used LLM, significantly improves the
performance of credit risk classifiers. However, the inherent opacity of
LLM-based systems, coupled with uncertainties about potential biases,
underscores critical considerations for regulatory frameworks and engenders
trust-related concerns among end-users, opening new avenues for future research
in the dynamic landscape of P2P lending and artificial intelligence.</div><div><a href='http://arxiv.org/abs/2401.16458v1'>2401.16458v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06644v1")'>Elephants Never Forget: Testing Language Models for Memorization of
  Tabular Data</div>
<div id='2403.06644v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T12:07:13Z</div><div>Authors: Sebastian Bordt, Harsha Nori, Rich Caruana</div><div style='padding-top: 10px; width: 80ex'>While many have shown how Large Language Models (LLMs) can be applied to a
diverse set of tasks, the critical issues of data contamination and
memorization are often glossed over. In this work, we address this concern for
tabular data. Starting with simple qualitative tests for whether an LLM knows
the names and values of features, we introduce a variety of different
techniques to assess the degrees of contamination, including statistical tests
for conditional distribution modeling and four tests that identify
memorization. Our investigation reveals that LLMs are pre-trained on many
popular tabular datasets. This exposure can lead to invalid performance
evaluation on downstream tasks because the LLMs have, in effect, been fit to
the test set. Interestingly, we also identify a regime where the language model
reproduces important statistics of the data, but fails to reproduce the dataset
verbatim. On these datasets, although seen during training, good performance on
downstream tasks might not be due to overfitting. Our findings underscore the
need for ensuring data integrity in machine learning tasks with LLMs. To
facilitate future research, we release an open-source tool that can perform
various tests for memorization
\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.</div><div><a href='http://arxiv.org/abs/2403.06644v1'>2403.06644v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05681v1")'>DP-TabICL: In-Context Learning with Differentially Private Tabular Data</div>
<div id='2403.05681v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T21:19:01Z</div><div>Authors: Alycia N. Carey, Karuna Bhaila, Kennedy Edemacu, Xintao Wu</div><div style='padding-top: 10px; width: 80ex'>In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks by conditioning on demonstrations of question-answer pairs and it has
been shown to have comparable performance to costly model retraining and
fine-tuning. Recently, ICL has been extended to allow tabular data to be used
as demonstration examples by serializing individual records into natural
language formats. However, it has been shown that LLMs can leak information
contained in prompts, and since tabular data often contain sensitive
information, understanding how to protect the underlying tabular data used in
ICL is a critical area of research. This work serves as an initial
investigation into how to use differential privacy (DP) -- the long-established
gold standard for data privacy and anonymization -- to protect tabular data
used in ICL. Specifically, we investigate the application of DP mechanisms for
private tabular ICL via data privatization prior to serialization and
prompting. We formulate two private ICL frameworks with provable privacy
guarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios
via injecting noise into individual records or group statistics, respectively.
We evaluate our DP-based frameworks on eight real-world tabular datasets and
across multiple ICL and DP settings. Our evaluations show that DP-based ICL can
protect the privacy of the underlying tabular data while achieving comparable
performance to non-LLM baselines, especially under high privacy regimes.</div><div><a href='http://arxiv.org/abs/2403.05681v1'>2403.05681v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11137v2")'>TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks</div>
<div id='2402.11137v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:02:23Z</div><div>Authors: Benjamin Feuer, Robin Tibor Schirrmeister, Valeriia Cherepanova, Chinmay Hegde, Frank Hutter, Micah Goldblum, Niv Cohen, Colin White</div><div style='padding-top: 10px; width: 80ex'>While tabular classification has traditionally relied on from-scratch
training, a recent breakthrough called prior-data fitted networks (PFNs)
challenges this approach. Similar to large language models, PFNs make use of
pretraining and in-context learning to achieve strong performance on new tasks
in a single forward pass. However, current PFNs have limitations that prohibit
their widespread adoption. Notably, TabPFN achieves very strong performance on
small tabular datasets but is not designed to make predictions for datasets of
size larger than 1000. In this work, we overcome these limitations and
substantially improve the performance of PFNs by developing context
optimization techniques for PFNs. Specifically, we propose TuneTables, a novel
prompt-tuning strategy that compresses large datasets into a smaller learned
context. TuneTables scales TabPFN to be competitive with state-of-the-art
tabular classification methods on larger datasets, while having a substantially
lower inference time than TabPFN. Furthermore, we show that TuneTables can be
used as an interpretability tool and can even be used to mitigate biases by
optimizing a fairness objective.</div><div><a href='http://arxiv.org/abs/2402.11137v2'>2402.11137v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12406v1")'>Enhancing In-context Learning via Linear Probe Calibration</div>
<div id='2401.12406v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T23:35:09Z</div><div>Authors: Momin Abbas, Yi Zhou, Parikshit Ram, Nathalie Baracaldo, Horst Samulowitz, Theodoros Salonidis, Tianyi Chen</div><div style='padding-top: 10px; width: 80ex'>In-context learning (ICL) is a new paradigm for natural language processing
that utilizes Generative Pre-trained Transformer (GPT)-like models. This
approach uses prompts that include in-context demonstrations to generate the
corresponding output for a new query input. However, applying ICL in real cases
does not scale with the number of samples, and lacks robustness to different
prompt templates and demonstration permutations. In this paper, we first show
that GPT-like models using ICL result in unreliable predictions based on a new
metric based on Shannon entropy. Then, to solve this problem, we propose a new
technique called the Linear Probe Calibration (LinC), a method that calibrates
the model's output probabilities, resulting in reliable predictions and
improved performance, while requiring only minimal additional samples (as few
as five labeled data samples). LinC significantly enhances the ICL test
performance of GPT models on various benchmark datasets, with an average
improvement of up to 21%, and up to a 50% improvement in some cases, and
significantly boosts the performance of PEFT methods, especially in the low
resource regime. Moreover, LinC achieves lower expected calibration error, and
is highly robust to varying label proportions, prompt templates, and
demonstration permutations. Our code is available at
\url{https://github.com/mominabbass/LinC}.</div><div><a href='http://arxiv.org/abs/2401.12406v1'>2401.12406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15833v1")'>Prompt Perturbation Consistency Learning for Robust Language Models</div>
<div id='2402.15833v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T15:00:58Z</div><div>Authors: Yao Qiang, Subhrangshu Nandi, Ninareh Mehrabi, Greg Ver Steeg, Anoop Kumar, Anna Rumshisky, Aram Galstyan</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have demonstrated impressive performance on a
number of natural language processing tasks, such as question answering and
text summarization. However, their performance on sequence labeling tasks such
as intent classification and slot filling (IC-SF), which is a central component
in personal assistant systems, lags significantly behind discriminative models.
Furthermore, there is a lack of substantive research on the robustness of LLMs
to various perturbations in the input prompts. The contributions of this paper
are three-fold. First, we show that fine-tuning sufficiently large LLMs can
produce IC-SF performance comparable to discriminative models. Next, we
systematically analyze the performance deterioration of those fine-tuned models
due to three distinct yet relevant types of input perturbations - oronyms,
synonyms, and paraphrasing. Finally, we propose an efficient mitigation
approach, Prompt Perturbation Consistency Learning (PPCL), which works by
regularizing the divergence between losses from clean and perturbed samples.
Our experiments demonstrate that PPCL can recover on average 59% and 69% of the
performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the
data augmentation approach while using ten times fewer augmented data samples.</div><div><a href='http://arxiv.org/abs/2402.15833v1'>2402.15833v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12817v1")'>On Sensitivity of Learning with Limited Labelled Data to the Effects of
  Randomness: Impact of Interactions and Systematic Choices</div>
<div id='2402.12817v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T08:38:19Z</div><div>Authors: Branislav Pecher, Ivan Srba, Maria Bielikova</div><div style='padding-top: 10px; width: 80ex'>While learning with limited labelled data can improve performance when the
labels are lacking, it is also sensitive to the effects of uncontrolled
randomness introduced by so-called randomness factors (e.g., varying order of
data). We propose a method to systematically investigate the effects of
randomness factors while taking the interactions between them into
consideration. To measure the true effects of an individual randomness factor,
our method mitigates the effects of other factors and observes how the
performance varies across multiple runs. Applying our method to multiple
randomness factors across in-context learning and fine-tuning approaches on 7
representative text classification tasks and meta-learning on 3 tasks, we show
that: 1) disregarding interactions between randomness factors in existing works
caused inconsistent findings due to incorrect attribution of the effects of
randomness factors, such as disproving the consistent sensitivity of in-context
learning to sample order even with random sample selection; and 2) besides
mutual interactions, the effects of randomness factors, especially sample
order, are also dependent on more systematic choices unexplored in existing
works, such as number of classes, samples per class or choice of prompt format.</div><div><a href='http://arxiv.org/abs/2402.12817v1'>2402.12817v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12117v1")'>Transfer Learning for T-Cell Response Prediction</div>
<div id='2403.12117v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:32:19Z</div><div>Authors: Josua Stadelmaier, Brandon Malone, Ralf Eggeling</div><div style='padding-top: 10px; width: 80ex'>We study the prediction of T-cell response for specific given peptides, which
could, among other applications, be a crucial step towards the development of
personalized cancer vaccines. It is a challenging task due to limited,
heterogeneous training data featuring a multi-domain structure; such data
entail the danger of shortcut learning, where models learn general
characteristics of peptide sources, such as the source organism, rather than
specific peptide characteristics associated with T-cell response.
  Using a transformer model for T-cell response prediction, we show that the
danger of inflated predictive performance is not merely theoretical but occurs
in practice. Consequently, we propose a domain-aware evaluation scheme. We then
study different transfer learning techniques to deal with the multi-domain
structure and shortcut learning. We demonstrate a per-source fine tuning
approach to be effective across a wide range of peptide sources and further
show that our final model outperforms existing state-of-the-art approaches for
predicting T-cell responses for human peptides.</div><div><a href='http://arxiv.org/abs/2403.12117v1'>2403.12117v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.00676v1")'>Digger: Detecting Copyright Content Mis-usage in Large Language Model
  Training</div>
<div id='2401.00676v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T06:04:52Z</div><div>Authors: Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang</div><div style='padding-top: 10px; width: 80ex'>Pre-training, which utilizes extensive and varied datasets, is a critical
factor in the success of Large Language Models (LLMs) across numerous
applications. However, the detailed makeup of these datasets is often not
disclosed, leading to concerns about data security and potential misuse. This
is particularly relevant when copyrighted material, still under legal
protection, is used inappropriately, either intentionally or unintentionally,
infringing on the rights of the authors.
  In this paper, we introduce a detailed framework designed to detect and
assess the presence of content from potentially copyrighted books within the
training datasets of LLMs. This framework also provides a confidence estimation
for the likelihood of each content sample's inclusion. To validate our
approach, we conduct a series of simulated experiments, the results of which
affirm the framework's effectiveness in identifying and addressing instances of
content misuse in LLM training processes. Furthermore, we investigate the
presence of recognizable quotes from famous literary works within these
datasets. The outcomes of our study have significant implications for ensuring
the ethical use of copyrighted materials in the development of LLMs,
highlighting the need for more transparent and responsible data management
practices in this field.</div><div><a href='http://arxiv.org/abs/2401.00676v1'>2401.00676v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09910v1")'>DE-COP: Detecting Copyrighted Content in Language Models Training Data</div>
<div id='2402.09910v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T12:17:15Z</div><div>Authors: André V. Duarte, Xuandong Zhao, Arlindo L. Oliveira, Lei Li</div><div style='padding-top: 10px; width: 80ex'>How can we detect if copyrighted content was used in the training process of
a language model, considering that the training data is typically undisclosed?
We are motivated by the premise that a language model is likely to identify
verbatim excerpts from its training text. We propose DE-COP, a method to
determine whether a piece of copyrighted content was included in training.
DE-COP's core approach is to probe an LLM with multiple-choice questions, whose
options include both verbatim text and their paraphrases. We construct
BookTection, a benchmark with excerpts from 165 books published prior and
subsequent to a model's training cutoff, along with their paraphrases. Our
experiments show that DE-COP surpasses the prior best method by 9.6% in
detection performance (AUC) on models with logits available. Moreover, DE-COP
also achieves an average accuracy of 72% for detecting suspect books on fully
black-box models where prior methods give $\approx$ 4% accuracy. Our code and
datasets are available at https://github.com/avduarte333/DE-COP_Method</div><div><a href='http://arxiv.org/abs/2402.09910v1'>2402.09910v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06712v1")'>Few-Shot Detection of Machine-Generated Text using Style Representations</div>
<div id='2401.06712v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T17:26:51Z</div><div>Authors: Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews</div><div style='padding-top: 10px; width: 80ex'>The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. For example, such models could be
used for plagiarism, disinformation, spam, or phishing. However, such abuse may
be counteracted with the ability to detect whether a piece of text was composed
by a language model rather than a human. Some previous approaches to this
problem have relied on supervised methods trained on corpora of confirmed human
and machine-written documents. Unfortunately, model under-specification poses
an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of further language
models producing still more fluent text than the models used to train the
detectors. Other previous approaches require access to the models that may have
generated a document in question at inference or detection time, which is often
impractical. In light of these challenges, we pursue a fundamentally different
approach not relying on samples from language models of concern at training
time. Instead, we propose to leverage representations of writing style
estimated from human-authored text. Indeed, we find that features effective at
distinguishing among human authors are also effective at distinguishing human
from machine authors, including state of the art large language models like
Llama 2, ChatGPT, and GPT-4. Furthermore, given a handful of examples composed
by each of several specific language models of interest, our approach affords
the ability to predict which model generated a given document.</div><div><a href='http://arxiv.org/abs/2401.06712v1'>2401.06712v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08787v2")'>Rethinking Machine Unlearning for Large Language Models</div>
<div id='2402.08787v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T20:51:58Z</div><div>Authors: Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>We explore machine unlearning (MU) in the domain of large language models
(LLMs), referred to as LLM unlearning. This initiative aims to eliminate
undesirable data influence (e.g., sensitive or illegal information) and the
associated model capabilities, while maintaining the integrity of essential
knowledge generation and not affecting causally unrelated information. We
envision LLM unlearning becoming a pivotal element in the life-cycle management
of LLMs, potentially standing as an essential foundation for developing
generative AI that is not only safe, secure, and trustworthy, but also
resource-efficient without the need of full retraining. We navigate the
unlearning landscape in LLMs from conceptual formulation, methodologies,
metrics, and applications. In particular, we highlight the often-overlooked
aspects of existing LLM unlearning research, e.g., unlearning scope, data-model
interaction, and multifaceted efficacy assessment. We also draw connections
between LLM unlearning and related areas such as model editing, influence
functions, model explanation, adversarial training, and reinforcement learning.
Furthermore, we outline an effective assessment framework for LLM unlearning
and explore its applications in copyright and privacy safeguards and
sociotechnical harm reduction.</div><div><a href='http://arxiv.org/abs/2402.08787v2'>2402.08787v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06009v1")'>Detectors for Safe and Reliable LLMs: Implementations, Uses, and
  Limitations</div>
<div id='2403.06009v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:07:16Z</div><div>Authors: Swapnaja Achintalwar, Adriana Alvarado Garcia, Ateret Anaby-Tavor, Ioana Baldini, Sara E. Berger, Bishwaranjan Bhattacharjee, Djallel Bouneffouf, Subhajit Chaudhury, Pin-Yu Chen, Lamogha Chiazor, Elizabeth M. Daly, Rogério Abreu de Paula, Pierre Dognin, Eitan Farchi, Soumya Ghosh, Michael Hind, Raya Horesh, George Kour, Ja Young Lee, Erik Miehling, Keerthiram Murugesan, Manish Nagireddy, Inkit Padhi, David Piorkowski, Ambrish Rawat, Orna Raz, Prasanna Sattigeri, Hendrik Strobelt, Sarathkrishna Swaminathan, Christoph Tillmann, Aashka Trivedi, Kush R. Varshney, Dennis Wei, Shalisha Witherspooon, Marcel Zalmanovici</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are susceptible to a variety of risks, from
non-faithful output to biased and toxic generations. Due to several limiting
factors surrounding LLMs (training cost, API access, data availability, etc.),
it may not always be feasible to impose direct safety constraints on a deployed
model. Therefore, an efficient and reliable alternative is required. To this
end, we present our ongoing efforts to create and deploy a library of
detectors: compact and easy-to-build classification models that provide labels
for various harms. In addition to the detectors themselves, we discuss a wide
range of uses for these detector models - from acting as guardrails to enabling
effective AI governance. We also deep dive into inherent challenges in their
development and discuss future work aimed at making the detectors more reliable
and broadening their scope.</div><div><a href='http://arxiv.org/abs/2403.06009v1'>2403.06009v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00826v1")'>LLMGuard: Guarding Against Unsafe LLM Behavior</div>
<div id='2403.00826v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T10:22:45Z</div><div>Authors: Shubh Goyal, Medha Hira, Shubham Mishra, Sukriti Goyal, Arnav Goel, Niharika Dadu, Kirushikesh DB, Sameep Mehta, Nishtha Madaan</div><div style='padding-top: 10px; width: 80ex'>Although the rise of Large Language Models (LLMs) in enterprise settings
brings new opportunities and capabilities, it also brings challenges, such as
the risk of generating inappropriate, biased, or misleading content that
violates regulations and can have legal concerns. To alleviate this, we present
"LLMGuard", a tool that monitors user interactions with an LLM application and
flags content against specific behaviours or conversation topics. To do this
robustly, LLMGuard employs an ensemble of detectors.</div><div><a href='http://arxiv.org/abs/2403.00826v1'>2403.00826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04123v1")'>Exploring LLM-based Agents for Root Cause Analysis</div>
<div id='2403.04123v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T00:44:01Z</div><div>Authors: Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, Saravan Rajmohan</div><div style='padding-top: 10px; width: 80ex'>The growing complexity of cloud based software systems has resulted in
incident management becoming an integral part of the software development
lifecycle. Root cause analysis (RCA), a critical part of the incident
management process, is a demanding task for on-call engineers, requiring deep
domain knowledge and extensive experience with a team's specific services.
Automation of RCA can result in significant savings of time, and ease the
burden of incident management on on-call engineers. Recently, researchers have
utilized Large Language Models (LLMs) to perform RCA, and have demonstrated
promising results. However, these approaches are not able to dynamically
collect additional diagnostic information such as incident related logs,
metrics or databases, severely restricting their ability to diagnose root
causes. In this work, we explore the use of LLM based agents for RCA to address
this limitation. We present a thorough empirical evaluation of a ReAct agent
equipped with retrieval tools, on an out-of-distribution dataset of production
incidents collected at Microsoft. Results show that ReAct performs
competitively with strong retrieval and reasoning baselines, but with highly
increased factual accuracy. We then extend this evaluation by incorporating
discussions associated with incident reports as additional inputs for the
models, which surprisingly does not yield significant performance improvements.
Lastly, we conduct a case study with a team at Microsoft to equip the ReAct
agent with tools that give it access to external diagnostic services that are
used by the team for manual RCA. Our results show how agents can overcome the
limitations of prior work, and practical considerations for implementing such a
system in practice.</div><div><a href='http://arxiv.org/abs/2403.04123v1'>2403.04123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14020v1")'>Coercing LLMs to do and reveal (almost) anything</div>
<div id='2402.14020v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:59:13Z</div><div>Authors: Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, Tom Goldstein</div><div style='padding-top: 10px; width: 80ex'>It has recently been shown that adversarial attacks on large language models
(LLMs) can "jailbreak" the model into making harmful statements. In this work,
we argue that the spectrum of adversarial attacks on LLMs is much larger than
merely jailbreaking. We provide a broad overview of possible attack surfaces
and attack goals. Based on a series of concrete examples, we discuss,
categorize and systematize attacks that coerce varied unintended behaviors,
such as misdirection, model control, denial-of-service, or data extraction.
  We analyze these attacks in controlled experiments, and find that many of
them stem from the practice of pre-training LLMs with coding capabilities, as
well as the continued existence of strange "glitch" tokens in common LLM
vocabularies that should be removed for security reasons.</div><div><a href='http://arxiv.org/abs/2402.14020v1'>2402.14020v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09132v2")'>Exploring the Adversarial Capabilities of Large Language Models</div>
<div id='2402.09132v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T12:28:38Z</div><div>Authors: Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting</div><div style='padding-top: 10px; width: 80ex'>The proliferation of large language models (LLMs) has sparked widespread and
general interest due to their strong language generation capabilities, offering
great potential for both industry and research. While previous research delved
into the security and privacy issues of LLMs, the extent to which these models
can exhibit adversarial behavior remains largely unexplored. Addressing this
gap, we investigate whether common publicly available LLMs have inherent
capabilities to perturb text samples to fool safety measures, so-called
adversarial examples resp.~attacks. More specifically, we investigate whether
LLMs are inherently able to craft adversarial examples out of benign samples to
fool existing safe rails. Our experiments, which focus on hate speech
detection, reveal that LLMs succeed in finding adversarial perturbations,
effectively undermining hate speech detection systems. Our findings carry
significant implications for (semi-)autonomous systems relying on LLMs,
highlighting potential challenges in their interaction with existing systems
and safety measures.</div><div><a href='http://arxiv.org/abs/2402.09132v2'>2402.09132v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13031v1")'>RigorLLM: Resilient Guardrails for Large Language Models against
  Undesired Content</div>
<div id='2403.13031v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T07:25:02Z</div><div>Authors: Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, Bo Li</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in Large Language Models (LLMs) have showcased remarkable
capabilities across various tasks in different domains. However, the emergence
of biases and the potential for generating harmful content in LLMs,
particularly under malicious inputs, pose significant challenges. Current
mitigation strategies, while effective, are not resilient under adversarial
attacks. This paper introduces Resilient Guardrails for Large Language Models
(RigorLLM), a novel framework designed to efficiently and effectively moderate
harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted
approach that includes energy-based training data augmentation through Langevin
dynamics, optimizing a safe suffix for inputs via minimax optimization, and
integrating a fusion-based model combining robust KNN with LLMs based on our
data augmentation, RigorLLM offers a robust solution to harmful content
moderation. Our experimental evaluations demonstrate that RigorLLM not only
outperforms existing baselines like OpenAI API and Perspective API in detecting
harmful content but also exhibits unparalleled resilience to jailbreaking
attacks. The innovative use of constrained optimization and a fusion-based
guardrail approach represents a significant step forward in developing more
secure and reliable LLMs, setting a new standard for content moderation
frameworks in the face of evolving digital threats.</div><div><a href='http://arxiv.org/abs/2403.13031v1'>2403.13031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09674v1")'>PAL: Proxy-Guided Black-Box Attack on Large Language Models</div>
<div id='2402.09674v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T02:54:49Z</div><div>Authors: Chawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have surged in popularity in recent months, but
they have demonstrated concerning capabilities to generate harmful content when
manipulated. While techniques like safety fine-tuning aim to minimize harmful
use, recent works have shown that LLMs remain vulnerable to attacks that elicit
toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs
(PAL), the first optimization-based attack on LLMs in a black-box query-only
setting. In particular, it relies on a surrogate model to guide the
optimization and a sophisticated loss designed for real-world LLM APIs. Our
attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on
Llama-2-7B, compared to 4% for the current state of the art. We also propose
GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box
Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple
baseline for query-based attacks. We believe the techniques proposed in this
work will enable more comprehensive safety testing of LLMs and, in the long
term, the development of better security guardrails. The code can be found at
https://github.com/chawins/pal.</div><div><a href='http://arxiv.org/abs/2402.09674v1'>2402.09674v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12329v1")'>Query-Based Adversarial Prompt Generation</div>
<div id='2402.12329v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:01:36Z</div><div>Authors: Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramèr, Milad Nasr</div><div style='padding-top: 10px; width: 80ex'>Recent work has shown it is possible to construct adversarial examples that
cause an aligned language model to emit harmful strings or perform harmful
behavior. Existing attacks work either in the white-box setting (with full
access to the model weights), or through transferability: the phenomenon that
adversarial examples crafted on one model often remain effective on other
models. We improve on prior work with a query-based attack that leverages API
access to a remote language model to construct adversarial examples that cause
the model to emit harmful strings with (much) higher probability than with
transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety
classifier; we can cause GPT-3.5 to emit harmful strings that current transfer
attacks fail at, and we can evade the safety classifier with nearly 100%
probability.</div><div><a href='http://arxiv.org/abs/2402.12329v1'>2402.12329v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12242v1")'>BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models</div>
<div id='2401.12242v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T04:53:35Z</div><div>Authors: Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are shown to benefit from chain-of-thought (COT)
prompting, particularly when tackling tasks that require systematic reasoning
processes. On the other hand, COT prompting also poses new vulnerabilities in
the form of backdoor attacks, wherein the model will output unintended
malicious content under specific backdoor-triggered conditions during
inference. Traditional methods for launching backdoor attacks involve either
contaminating the training dataset with backdoored instances or directly
manipulating the model parameters during deployment. However, these approaches
are not practical for commercial LLMs that typically operate via API access. In
this paper, we propose BadChain, the first backdoor attack against LLMs
employing COT prompting, which does not require access to the training dataset
or model parameters and imposes low computational overhead. BadChain leverages
the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning
step into the sequence of reasoning steps of the model output, thereby altering
the final response when a backdoor trigger exists in the query prompt.
Empirically, we show the effectiveness of BadChain for two COT strategies
across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark
tasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover,
we show that LLMs endowed with stronger reasoning capabilities exhibit higher
susceptibility to BadChain, exemplified by a high average attack success rate
of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two
defenses based on shuffling and demonstrate their overall ineffectiveness
against BadChain. Therefore, BadChain remains a severe threat to LLMs,
underscoring the urgency for the development of robust and effective future
defenses.</div><div><a href='http://arxiv.org/abs/2401.12242v1'>2401.12242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05266v1")'>ERBench: An Entity-Relationship based Automatically Verifiable
  Hallucination Benchmark for Large Language Models</div>
<div id='2403.05266v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T12:42:36Z</div><div>Authors: Jio Oh, Soyeon Kim, Junseok Seo, Jindong Wang, Ruochen Xu, Xing Xie, Steven Euijong Whang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have achieved unprecedented performance in
various applications, yet their evaluation remains a critical issue. Existing
hallucination benchmarks are either static or lack adjustable complexity for
thorough analysis. We contend that utilizing existing relational databases is a
promising approach for constructing benchmarks due to their accurate knowledge
description via functional dependencies. We propose ERBench to automatically
convert any relational database into a benchmark based on the
entity-relationship (ER) model. Our key idea is to construct questions using
the database schema, records, and functional dependencies such that they can be
automatically verified. In addition, we use foreign key constraints to join
relations and construct multihop questions, which can be arbitrarily complex
and used to debug the intermediate answers of LLMs. Finally, ERBench supports
continuous evaluation, multimodal questions, and various prompt engineering
techniques. In our experiments, we construct an LLM benchmark using databases
of multiple domains and make an extensive comparison of contemporary LLMs. We
observe that better LLMs like GPT-4 can handle a larger variety of question
types, but are by no means perfect. Also, correct answers do not necessarily
imply correct rationales, which is an important evaluation that ERBench does
better than other benchmarks for various question types. Code is available at
https: //github.com/DILAB-KAIST/ERBench.</div><div><a href='http://arxiv.org/abs/2403.05266v1'>2403.05266v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12242v1")'>Reference-based Metrics Disprove Themselves in Question Generation</div>
<div id='2403.12242v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T20:47:10Z</div><div>Authors: Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang</div><div style='padding-top: 10px; width: 80ex'>Reference-based metrics such as BLEU and BERTScore are widely used to
evaluate question generation (QG). In this study, on QG benchmarks such as
SQuAD and HotpotQA, we find that using human-written references cannot
guarantee the effectiveness of the reference-based metrics. Most QG benchmarks
have only one reference; we replicated the annotation process and collect
another reference. A good metric was expected to grade a human-validated
question no worse than generated questions. However, the results of
reference-based metrics on our newly collected reference disproved the metrics
themselves. We propose a reference-free metric consisted of multi-dimensional
criteria such as naturalness, answerability, and complexity, utilizing large
language models. These criteria are not constrained to the syntactic or
semantic of a single reference question, and the metric does not require a
diverse set of references. Experiments reveal that our metric accurately
distinguishes between high-quality questions and flawed ones, and achieves
state-of-the-art alignment with human judgment.</div><div><a href='http://arxiv.org/abs/2403.12242v1'>2403.12242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12821v1")'>Identifying Factual Inconsistency in Summaries: Towards Effective
  Utilization of Large Language Model</div>
<div id='2402.12821v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T08:41:23Z</div><div>Authors: Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, Fei Liu</div><div style='padding-top: 10px; width: 80ex'>Factual inconsistency poses a significant hurdle for the commercial
deployment of abstractive summarizers. Under this Large Language Model (LLM)
era, this work focuses around two important questions: what is the best way to
leverage LLM for factual inconsistency detection, and how could we distill a
smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms
are firstly proposed and evaluated across five diverse datasets: direct
inference on the entire summary or each summary window; entity verification
through question generation and answering. Experiments suggest that LLM itself
is capable to resolve this task train-free under the proper paradigm design,
surpassing strong trained baselines by 2.8% on average. To further promote
practical utility, we then propose training strategies aimed at distilling
smaller open-source LLM that learns to score the entire summary at once with
high accuracy, which outperforms the zero-shot approaches by much larger LLM,
serving as an effective and efficient ready-to-use scorer.</div><div><a href='http://arxiv.org/abs/2402.12821v1'>2402.12821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15162v1")'>Entity-level Factual Adaptiveness of Fine-tuning based Abstractive
  Summarization Models</div>
<div id='2402.15162v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T07:53:39Z</div><div>Authors: Jongyoon Song, Nohil Park, Bongkyu Hwang, Jaewoong Yun, Seongho Joe, Youngjune L. Gwon, Sungroh Yoon</div><div style='padding-top: 10px; width: 80ex'>Abstractive summarization models often generate factually inconsistent
content particularly when the parametric knowledge of the model conflicts with
the knowledge in the input document. In this paper, we analyze the robustness
of fine-tuning based summarization models to the knowledge conflict, which we
call factual adaptiveness. We utilize pre-trained language models to construct
evaluation sets and find that factual adaptiveness is not strongly correlated
with factual consistency on original datasets. Furthermore, we introduce a
controllable counterfactual data augmentation method where the degree of
knowledge conflict within the augmented data can be adjustable. Our
experimental results on two pre-trained language models (PEGASUS and BART) and
two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method
enhances factual adaptiveness while achieving factual consistency on original
datasets on par with the contrastive learning baseline.</div><div><a href='http://arxiv.org/abs/2402.15162v1'>2402.15162v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17809v2")'>SWEA: Changing Factual Knowledge in Large Language Models via Subject
  Word Embedding Altering</div>
<div id='2401.17809v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T13:08:45Z</div><div>Authors: Xiaopeng Li, Shasha Li, Shezheng Song, Huijun Liu, Bin Ji, Xi Wang, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Weimin Zhang</div><div style='padding-top: 10px; width: 80ex'>Model editing has recently gained widespread attention. Current model editing
methods primarily involve modifying model parameters or adding additional
modules to the existing model. However, the former causes irreversible damage
to Large Language Models (LLMs), while the latter incurs additional inference
overhead and fuzzy vector matching is not always reliable. To address these
issues, we propose an expandable Subject Word Embedding Altering (SWEA)
framework, which finds the fused embeddings through character-level key-value
matching and adds them to the subject word embeddings in Transformer input. To
get these fused embeddings, we propose optimizing then suppressing fusion
method, which first optimizes learnable embedding vectors for the editing
target and then suppresses the Knowledge Embedding Dimensions (KEDs) to obtain
final fused embeddings. We thus propose SWEA$\oplus$OS method for editing
factual knowledge in LLMs. We demonstrate the overall state-of-the-art (SOTA)
performance of SWEA$\oplus$OS on the COUNTERFACT and zsRE datasets. To further
validate the reasoning ability of SWEA$\oplus$OS in editing knowledge, we
evaluate it on the more complex RippleEdits benchmark. The results demonstrate
that SWEA$\oplus$OS possesses SOTA reasoning ability.</div><div><a href='http://arxiv.org/abs/2401.17809v2'>2401.17809v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03509v1")'>Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains</div>
<div id='2402.03509v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T20:51:11Z</div><div>Authors: Sanjana Ramprasad, Kundan Krishna, Zachary C Lipton, Byron C Wallace</div><div style='padding-top: 10px; width: 80ex'>Recent work has shown that large language models (LLMs) are capable of
generating summaries zero-shot (i.e., without explicit supervision) that, under
human assessment, are often comparable or even preferred to manually composed
reference summaries. However, this prior work has focussed almost exclusively
on evaluating news article summarization. How do zero-shot summarizers perform
in other (potentially more specialized) domains? In this work we evaluate
zero-shot generated summaries across specialized domains including biomedical
articles, and legal bills (in addition to standard news benchmarks for
reference). We focus especially on the factuality of outputs. We acquire
annotations from domain experts to identify inconsistencies in summaries and
systematically categorize these errors. We analyze whether the prevalence of a
given domain in the pretraining corpus affects extractiveness and faithfulness
of generated summaries of articles in this domain. We release all collected
annotations to facilitate additional research toward measuring and realizing
factually accurate summarization, beyond news articles. The dataset can be
downloaded from https://github.com/sanjanaramprasad/zero_shot_faceval_domains</div><div><a href='http://arxiv.org/abs/2402.03509v1'>2402.03509v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07557v1")'>SIFiD: Reassess Summary Factual Inconsistency Detection with LLM</div>
<div id='2403.07557v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:41:51Z</div><div>Authors: Jiuding Yang, Hui Liu, Weidong Guo, Zhuwei Rao, Yu Xu, Di Niu</div><div style='padding-top: 10px; width: 80ex'>Ensuring factual consistency between the summary and the original document is
paramount in summarization tasks. Consequently, considerable effort has been
dedicated to detecting inconsistencies. With the advent of Large Language
Models (LLMs), recent studies have begun to leverage their advanced language
understanding capabilities for inconsistency detection. However, early attempts
have shown that LLMs underperform traditional models due to their limited
ability to follow instructions and the absence of an effective detection
methodology. In this study, we reassess summary inconsistency detection with
LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in
LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency
Detection with Filtered Document) that identify key sentences within documents
by either employing natural language inference or measuring semantic similarity
between summaries and documents.</div><div><a href='http://arxiv.org/abs/2403.07557v1'>2403.07557v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06973v1")'>Event-Keyed Summarization</div>
<div id='2402.06973v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T15:32:53Z</div><div>Authors: William Gantt, Alexander Martin, Pavlo Kuchmiichuk, Aaron Steven White</div><div style='padding-top: 10px; width: 80ex'>We introduce event-keyed summarization (EKS), a novel task that marries
traditional summarization and document-level event extraction, with the goal of
generating a contextualized summary for a specific event, given a document and
an extracted event structure. We introduce a dataset for this task, MUCSUM,
consisting of summaries of all events in the classic MUC-4 dataset, along with
a set of baselines that comprises both pretrained LM standards in the
summarization literature, as well as larger frontier models. We show that
ablations that reduce EKS to traditional summarization or structure-to-text
yield inferior summaries of target events and that MUCSUM is a robust benchmark
for this task. Lastly, we conduct a human evaluation of both reference and
model summaries, and provide some detailed analysis of the results.</div><div><a href='http://arxiv.org/abs/2402.06973v1'>2402.06973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02966v1")'>Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot
  Question Answering</div>
<div id='2403.02966v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:43:58Z</div><div>Authors: Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee</div><div style='padding-top: 10px; width: 80ex'>Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance
Quesetion Answering (QA) performance of Large Language Models (LLMs), yet
structured KG verbalization remains challengin. Existing methods, such as
triple-form or free-form textual conversion of triple-form facts, encounter
several issues. These include reduced evidence density due to duplicated
entities or relationships, and reduced evidence clarity due to an inability to
emphasize crucial evidence. To address these issues, we propose EFSum, an
Evidence-focused Fact Summarization framework for enhanced QA with
knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer
through distillation and preference alignment. Our extensive experiments show
that EFSum improves LLM's zero-shot QA performance, and it is possible to
ensure both the helpfulness and faithfulness of the summary.</div><div><a href='http://arxiv.org/abs/2403.02966v1'>2403.02966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03304v1")'>Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event
  Argument Data</div>
<div id='2403.03304v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T20:07:42Z</div><div>Authors: Joseph Gatto, Parker Seegmiller, Omar Sharif, Sarah M. Preum</div><div style='padding-top: 10px; width: 80ex'>Document-Level Event Argument Extraction (DocEAE) is an extremely difficult
information extraction problem -- with significant limitations in low-resource
cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA),
a novel generative DocEAE data augmentation framework. Our approach leverages
the intuition that Mad Libs, which are categorically masked documents used as a
part of a popular game, can be generated and solved by LLMs to produce data for
DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1
score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in
zero and few-shot event roles compared to augmentation-free baselines across
all experiments.
  To better facilitate analysis of cross-domain DocEAE, we additionally
introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to
identify roles in the target domain which are semantic outliers with respect to
roles observed in the source domain. Our experiments show that MLA augmentation
can boost RDF1 performance by an average of 5.85 points compared to
non-augmented datasets.</div><div><a href='http://arxiv.org/abs/2403.03304v1'>2403.03304v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16282v1")'>MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim
  Verification</div>
<div id='2401.16282v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T16:39:39Z</div><div>Authors: Xia Zeng, Arkaitz Zubiaga</div><div style='padding-top: 10px; width: 80ex'>Claim verification is an essential step in the automated fact-checking
pipeline which assesses the veracity of a claim against a piece of evidence. In
this work, we explore the potential of few-shot claim verification, where only
very limited data is available for supervision. We propose MAPLE (Micro
Analysis of Pairwise Language Evolution), a pioneering approach that explores
the alignment between a claim and its evidence with a small seq2seq model and a
novel semantic measure. Its innovative utilization of micro language evolution
path leverages unlabelled pairwise data to facilitate claim verification while
imposing low demand on data annotations and computing resources. MAPLE
demonstrates significant performance improvements over SOTA baselines SEED, PET
and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and
SciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE</div><div><a href='http://arxiv.org/abs/2401.16282v1'>2401.16282v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01472v2")'>A First Look at Information Highlighting in Stack Overflow Answers</div>
<div id='2401.01472v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T00:13:52Z</div><div>Authors: Shahla Shaan Ahmed, Shaowei Wang, Yuan Tian, Tse-Hsun, Chen, Haoxiang Zhang</div><div style='padding-top: 10px; width: 80ex'>Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.
To make the posts vivid to users, SO allows users to write and edit posts with
Markdown or HTML so that users can leverage various formatting styles (e.g.,
bold, italic, and code) to highlight the important information. Nonetheless,
there have been limited studies on the highlighted information. Objective: We
carried out the first large-scale exploratory study on the information
highlighted in SO answers in our recent study. To extend our previous study, we
develop approaches to automatically recommend highlighted content with
formatting styles using neural network architectures initially designed for the
Named Entity Recognition task. Method: In this paper, we studied 31,169,429
answers of Stack Overflow. For training recommendation models, we choose CNN
and BERT models for each type of formatting (i.e., Bold, Italic, Code, and
Heading) using the information highlighting dataset we collected from SO
answers. Results: Our models based on CNN architecture achieve precision
ranging from 0.71 to 0.82. The trained model for automatic code content
highlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming
the trained models for other formatting styles. The BERT models have even lower
recalls and F1 scores than the CNN models. Our analysis of failure cases
indicates that the majority of the failure cases are missing identification
(i.e., the model misses the content that is supposed to be highlighted) due to
the models tend to learn the frequently highlighted words while struggling to
learn less frequent words. Conclusion: Our findings suggest that it is possible
to develop recommendation models for highlighting information for answers with
different formatting styles on Stack Overflow.</div><div><a href='http://arxiv.org/abs/2401.01472v2'>2401.01472v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16822v1")'>Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</div>
<div id='2402.16822v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:47:27Z</div><div>Authors: Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, Roberta Raileanu</div><div style='padding-top: 10px; width: 80ex'>As large language models (LLMs) become increasingly prevalent across many
real-world applications, understanding and enhancing their robustness to user
inputs is of paramount importance. Existing methods for identifying adversarial
prompts tend to focus on specific domains, lack diversity, or require extensive
human annotations. To address these limitations, we present Rainbow Teaming, a
novel approach for producing a diverse collection of adversarial prompts.
Rainbow Teaming casts adversarial prompt generation as a quality-diversity
problem, and uses open-ended search to generate prompts that are both effective
and diverse. It can uncover a model's vulnerabilities across a broad range of
domains including, in this paper, safety, question answering, and
cybersecurity. We also demonstrate that fine-tuning on synthetic data generated
by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting
their general capabilities and helpfulness, paving the path to open-ended
self-improvement.</div><div><a href='http://arxiv.org/abs/2402.16822v1'>2402.16822v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01642v1")'>Detection of Machine-Generated Text: Literature Survey</div>
<div id='2402.01642v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T01:44:15Z</div><div>Authors: Dmytro Valiaiev</div><div style='padding-top: 10px; width: 80ex'>Since language models produce fake text quickly and easily, there is an
oversupply of such content in the public domain. The degree of sophistication
and writing style has reached a point where differentiating between human
authored and machine-generated content is nearly impossible. As a result, works
generated by language models rather than human authors have gained significant
media attention and stirred controversy.Concerns regarding the possible
influence of advanced language models on society have also arisen, needing a
fuller knowledge of these processes. Natural language generation (NLG) and
generative pre-trained transformer (GPT) models have revolutionized a variety
of sectors: the scope not only permeated throughout journalism and customer
service but also reached academia. To mitigate the hazardous implications that
may arise from the use of these models, preventative measures must be
implemented, such as providing human agents with the capacity to distinguish
between artificially made and human composed texts utilizing automated systems
and possibly reverse-engineered language models. Furthermore, to ensure a
balanced and responsible approach, it is critical to have a full grasp of the
socio-technological ramifications of these breakthroughs. This literature
survey aims to compile and synthesize accomplishments and developments in the
aforementioned work, while also identifying future prospects. It also gives an
overview of machine-generated text trends and explores the larger societal
implications. Ultimately, this survey intends to contribute to the development
of robust and effective approaches for resolving the issues connected with the
usage and detection of machine-generated text by exploring the interplay
between the capabilities of language models and their possible implications.</div><div><a href='http://arxiv.org/abs/2402.01642v1'>2402.01642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05750v1")'>Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated
  Text</div>
<div id='2403.05750v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T01:13:54Z</div><div>Authors: Sara Abdali, Richard Anarfi, CJ Barberan, Jia He</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have revolutionized the field of Natural
Language Generation (NLG) by demonstrating an impressive ability to generate
human-like text. However, their widespread usage introduces challenges that
necessitate thoughtful examination, ethical scrutiny, and responsible
practices. In this study, we delve into these challenges, explore existing
strategies for mitigating them, with a particular emphasis on identifying
AI-generated text as the ultimate solution. Additionally, we assess the
feasibility of detection from a theoretical perspective and propose novel
research directions to address the current limitations in this domain.</div><div><a href='http://arxiv.org/abs/2403.05750v1'>2403.05750v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14838v1")'>RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic
  Features for Distinguishing AI-Generated and Human-Written Texts</div>
<div id='2402.14838v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T00:40:17Z</div><div>Authors: Mohammad Heydari Rad, Farhan Farsi, Shayan Bali, Romina Etezadi, Mehrnoush Shamsfard</div><div style='padding-top: 10px; width: 80ex'>Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs
have been used to generate texts in different languages and for different
tasks. Additionally, due to the participation of remarkable companies such as
Google and OpenAI, LLMs are now more accessible, and people can easily use
them. However, an important issue is how we can detect AI-generated texts from
human-written ones. In this article, we have investigated the problem of
AI-generated text detection from two different aspects: semantics and syntax.
Finally, we presented an AI model that can distinguish AI-generated texts from
human-written ones with high accuracy on both multilingual and monolingual
tasks using the M4 dataset. According to our results, using a semantic approach
would be more helpful for detection. However, there is a lot of room for
improvement in the syntactic approach, and it would be a good approach for
future work.</div><div><a href='http://arxiv.org/abs/2402.14838v1'>2402.14838v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11815v1")'>HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to
  Detect Machine-Generated Text?</div>
<div id='2402.11815v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T04:11:34Z</div><div>Authors: Shubhashis Roy Dipta, Sadat Shahriar</div><div style='padding-top: 10px; width: 80ex'>This paper describes our system developed for SemEval-2024 Task 8,
"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text
Detection." Machine-generated texts have been one of the main concerns due to
the use of large language models (LLM) in fake text generation, phishing,
cheating in exams, or even plagiarizing copyright materials. A lot of systems
have been developed to detect machine-generated text. Nonetheless, the majority
of these systems rely on the text-generating model, a limitation that is
impractical in real-world scenarios, as it's often impossible to know which
specific model the user has used for text generation. In this work, we propose
a single model based on contrastive learning, which uses ~40% of the baseline's
parameters (149M vs. 355M) but shows a comparable performance on the test
dataset (21st out of 137 participants). Our key finding is that even without an
ensemble of multiple models, a single base model can have comparable
performance with the help of data augmentation and contrastive learning.</div><div><a href='http://arxiv.org/abs/2402.11815v1'>2402.11815v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13335v1")'>Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text
  Detection</div>
<div id='2403.13335v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T06:38:13Z</div><div>Authors: Zhixin Lai, Xuesheng Zhang, Suiyao Chen</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have reached human-like proficiency in
generating diverse textual content, underscoring the necessity for effective
fake text detection to avoid potential risks such as fake news in social media.
Previous research has mostly tested single models on in-distribution datasets,
limiting our understanding of how these models perform on different types of
data for LLM-generated text detection task. We researched this by testing five
specialized transformer-based models on both in-distribution and
out-of-distribution datasets to better assess their performance and
generalizability. Our results revealed that single transformer-based
classifiers achieved decent performance on in-distribution dataset but limited
generalization ability on out-of-distribution dataset. To improve it, we
combined the individual classifiers models using adaptive ensemble algorithms,
which improved the average accuracy significantly from 91.8% to 99.2% on an
in-distribution test set and from 62.9% to 72.5% on an out-of-distribution test
set. The results indicate the effectiveness, good generalization ability, and
great potential of adaptive ensemble algorithms in LLM-generated text
detection.</div><div><a href='http://arxiv.org/abs/2403.13335v1'>2403.13335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08309v2")'>Prompted Contextual Vectors for Spear-Phishing Detection</div>
<div id='2402.08309v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T09:12:55Z</div><div>Authors: Daniel Nahmias, Gal Engelberg, Dan Klein, Asaf Shabtai</div><div style='padding-top: 10px; width: 80ex'>Spear-phishing attacks present a significant security challenge, with large
language models (LLMs) escalating the threat by generating convincing emails
and facilitating target reconnaissance. To address this, we propose a detection
approach based on a novel document vectorization method that utilizes an
ensemble of LLMs to create representation vectors. By prompting LLMs to reason
and respond to human-crafted questions, we quantify the presence of common
persuasion principles in the email's content, producing prompted contextual
document vectors for a downstream supervised machine learning model. We
evaluate our method using a unique dataset generated by a proprietary system
that automates target reconnaissance and spear-phishing email creation. Our
method achieves a 91% F1 score in identifying LLM-generated spear-phishing
emails, with the training set comprising only traditional phishing and benign
emails. Key contributions include an innovative document vectorization method
utilizing LLM reasoning, a publicly available dataset of high-quality
spear-phishing emails, and the demonstrated effectiveness of our method in
detecting such emails. This methodology can be utilized for various document
classification tasks, particularly in adversarial problem domains.</div><div><a href='http://arxiv.org/abs/2402.08309v2'>2402.08309v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13871v1")'>An Explainable Transformer-based Model for Phishing Email Detection: A
  Large Language Model Approach</div>
<div id='2402.13871v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T15:23:21Z</div><div>Authors: Mohammad Amaz Uddin, Iqbal H. Sarker</div><div style='padding-top: 10px; width: 80ex'>Phishing email is a serious cyber threat that tries to deceive users by
sending false emails with the intention of stealing confidential information or
causing financial harm. Attackers, often posing as trustworthy entities,
exploit technological advancements and sophistication to make detection and
prevention of phishing more challenging. Despite extensive academic research,
phishing detection remains an ongoing and formidable challenge in the
cybersecurity landscape. Large Language Models (LLMs) and Masked Language
Models (MLMs) possess immense potential to offer innovative solutions to
address long-standing challenges. In this research paper, we present an
optimized, fine-tuned transformer-based DistilBERT model designed for the
detection of phishing emails. In the detection process, we work with a phishing
email dataset and utilize the preprocessing techniques to clean and solve the
imbalance class issues. Through our experiments, we found that our model
effectively achieves high accuracy, demonstrating its capability to perform
well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)
techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and
Transformer Interpret to explain how our model makes predictions in the context
of text classification for phishing emails.</div><div><a href='http://arxiv.org/abs/2402.13871v1'>2402.13871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12403v1")'>Towards Interpretable Hate Speech Detection using Large Language
  Model-extracted Rationales</div>
<div id='2403.12403v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T03:22:35Z</div><div>Authors: Ayushi Nirmal, Amrita Bhattacharjee, Paras Sheth, Huan Liu</div><div style='padding-top: 10px; width: 80ex'>Although social media platforms are a prominent arena for users to engage in
interpersonal discussions and express opinions, the facade and anonymity
offered by social media may allow users to spew hate speech and offensive
content. Given the massive scale of such platforms, there arises a need to
automatically identify and flag instances of hate speech. Although several hate
speech detection methods exist, most of these black-box methods are not
interpretable or explainable by design. To address the lack of
interpretability, in this paper, we propose to use state-of-the-art Large
Language Models (LLMs) to extract features in the form of rationales from the
input text, to train a base hate speech classifier, thereby enabling faithful
interpretability by design. Our framework effectively combines the textual
understanding capabilities of LLMs and the discriminative power of
state-of-the-art hate speech classifiers to make these classifiers faithfully
interpretable. Our comprehensive evaluation on a variety of social media hate
speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales,
and (2) the surprising retention of detector performance even after training to
ensure interpretability.</div><div><a href='http://arxiv.org/abs/2403.12403v1'>2403.12403v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03346v1")'>An Investigation of Large Language Models for Real-World Hate Speech
  Detection</div>
<div id='2401.03346v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T00:39:33Z</div><div>Authors: Keyan Guo, Alexander Hu, Jaden Mu, Ziheng Shi, Ziming Zhao, Nishant Vishwamitra, Hongxin Hu</div><div style='padding-top: 10px; width: 80ex'>Hate speech has emerged as a major problem plaguing our social spaces today.
While there have been significant efforts to address this problem, existing
methods are still significantly limited in effectively detecting hate speech
online. A major limitation of existing methods is that hate speech detection is
a highly contextual problem, and these methods cannot fully capture the context
of hate speech to make accurate predictions. Recently, large language models
(LLMs) have demonstrated state-of-the-art performance in several natural
language tasks. LLMs have undergone extensive training using vast amounts of
natural language data, enabling them to grasp intricate contextual details.
Hence, they could be used as knowledge bases for context-aware hate speech
detection. However, a fundamental problem with using LLMs to detect hate speech
is that there are no studies on effectively prompting LLMs for context-aware
hate speech detection. In this study, we conduct a large-scale study of hate
speech detection, employing five established hate speech datasets. We discover
that LLMs not only match but often surpass the performance of current benchmark
machine learning models in identifying hate speech. By proposing four diverse
prompting strategies that optimize the use of LLMs in detecting hate speech.
Our study reveals that a meticulously crafted reasoning prompt can effectively
capture the context of hate speech by fully utilizing the knowledge base in
LLMs, significantly outperforming existing techniques. Furthermore, although
LLMs can provide a rich knowledge base for the contextual detection of hate
speech, suitable prompting strategies play a crucial role in effectively
leveraging this knowledge base for efficient detection.</div><div><a href='http://arxiv.org/abs/2401.03346v1'>2401.03346v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10653v1")'>Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech
  Detection</div>
<div id='2401.10653v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T11:59:13Z</div><div>Authors: Atanu Mandal, Gargi Roy, Amit Barman, Indranil Dutta, Sudip Kumar Naskar</div><div style='padding-top: 10px; width: 80ex'>With the recent surge and exponential growth of social media usage,
scrutinizing social media content for the presence of any hateful content is of
utmost importance. Researchers have been diligently working since the past
decade on distinguishing between content that promotes hatred and content that
does not. Traditionally, the main focus has been on analyzing textual content.
However, recent research attempts have also commenced into the identification
of audio-based content. Nevertheless, studies have shown that relying solely on
audio or text-based content may be ineffective, as recent upsurge indicates
that individuals often employ sarcasm in their speech and writing. To overcome
these challenges, we present an approach to identify whether a speech promotes
hate or not utilizing both audio and textual representations. Our methodology
is based on the Transformer framework that incorporates both audio and text
sampling, accompanied by our very own layer called "Attentive Fusion". The
results of our study surpassed previous state-of-the-art techniques, achieving
an impressive macro F1 score of 0.927 on the Test Set.</div><div><a href='http://arxiv.org/abs/2401.10653v1'>2401.10653v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01752v1")'>Identifying False Content and Hate Speech in Sinhala YouTube Videos by
  Analyzing the Audio</div>
<div id='2402.01752v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T08:08:34Z</div><div>Authors: W. A. K. M. Wickramaarachchi, Sameeri Sathsara Subasinghe, K. K. Rashani Tharushika Wijerathna, A. Sahashra Udani Athukorala, Lakmini Abeywardhana, A. Karunasena</div><div style='padding-top: 10px; width: 80ex'>YouTube faces a global crisis with the dissemination of false information and
hate speech. To counter these issues, YouTube has implemented strict rules
against uploading content that includes false information or promotes hate
speech. While numerous studies have been conducted to reduce offensive
English-language content, there's a significant lack of research on Sinhala
content. This study aims to address the aforementioned gap by proposing a
solution to minimize the spread of violence and misinformation in Sinhala
YouTube videos. The approach involves developing a rating system that assesses
whether a video contains false information by comparing the title and
description with the audio content and evaluating whether the video includes
hate speech. The methodology encompasses several steps, including audio
extraction using the Pytube library, audio transcription via the fine-tuned
Whisper model, hate speech detection employing the distilroberta-base model and
a text classification LSTM model, and text summarization through the fine-tuned
BART-Large- XSUM model. Notably, the Whisper model achieved a 48.99\% word
error rate, while the distilroberta-base model demonstrated an F1 score of
0.856 and a recall value of 0.861 in comparison to the LSTM model, which
exhibited signs of overfitting.</div><div><a href='http://arxiv.org/abs/2402.01752v1'>2402.01752v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12198v1")'>Zero shot VLMs for hate meme detection: Are we there yet?</div>
<div id='2402.12198v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:03:04Z</div><div>Authors: Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee</div><div style='padding-top: 10px; width: 80ex'>Multimedia content on social media is rapidly evolving, with memes gaining
prominence as a distinctive form. Unfortunately, some malicious users exploit
memes to target individuals or vulnerable communities, making it imperative to
identify and address such instances of hateful memes. Extensive research has
been conducted to address this issue by developing hate meme detection models.
However, a notable limitation of traditional machine/deep learning models is
the requirement for labeled datasets for accurate classification. Recently, the
research community has witnessed the emergence of several visual language
models that have exhibited outstanding performance across various tasks. In
this study, we aim to investigate the efficacy of these visual language models
in handling intricate tasks such as hate meme detection. We use various prompt
settings to focus on zero-shot classification of hateful/harmful memes. Through
our analysis, we observe that large VLMs are still vulnerable for zero-shot
hate meme detection.</div><div><a href='http://arxiv.org/abs/2402.12198v1'>2402.12198v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01759v1")'>Systematic Literature Review: Computational Approaches for Humour Style
  Classification</div>
<div id='2402.01759v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T16:21:47Z</div><div>Authors: Mary Ogbuka Kenneth, Foaad Khosmood, Abbas Edalat</div><div style='padding-top: 10px; width: 80ex'>Understanding various humour styles is essential for comprehending the
multifaceted nature of humour and its impact on fields such as psychology and
artificial intelligence. This understanding has revealed that humour, depending
on the style employed, can either have therapeutic or detrimental effects on an
individual's health and relationships. Although studies dedicated exclusively
to computational-based humour style analysis remain somewhat rare, an expansive
body of research thrives within related task, particularly binary humour and
sarcasm recognition. In this systematic literature review (SLR), we survey the
landscape of computational techniques applied to these related tasks and also
uncover their fundamental relevance to humour style analysis. Through this
study, we unveil common approaches, illuminate various datasets and evaluation
metrics, and effectively navigate the complex terrain of humour research. Our
efforts determine potential research gaps and outlined promising directions.
Furthermore, the SLR identifies a range of features and computational models
that can seamlessly transition from related tasks like binary humour and
sarcasm detection to invigorate humour style classification. These features
encompass incongruity, sentiment and polarity analysis, ambiguity detection,
acoustic nuances, visual cues, contextual insights, and more. The computational
models that emerge contain traditional machine learning paradigms, neural
network architectures, transformer-based models, and specialised models attuned
to the nuances of humour. Finally, the SLR provides access to existing datasets
related to humour and sarcasm, facilitating the work of future researchers.</div><div><a href='http://arxiv.org/abs/2402.01759v1'>2402.01759v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12469v1")'>When Do "More Contexts" Help with Sarcasm Recognition?</div>
<div id='2403.12469v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T06:01:02Z</div><div>Authors: Ojas Nimase, Sanghyun Hong</div><div style='padding-top: 10px; width: 80ex'>Sarcasm recognition is challenging because it needs an understanding of the
true intention, which is opposite to or different from the literal meaning of
the words. Prior work has addressed this challenge by developing a series of
methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to
models. While shown to be effective individually, no study has systematically
evaluated their collective effectiveness. As a result, it remains unclear to
what extent additional contexts can improve sarcasm recognition. In this work,
we explore the improvements that existing methods bring by incorporating more
contexts into a model. To this end, we develop a framework where we can
integrate multiple contextual cues and test different approaches. In evaluation
with four approaches on three sarcasm recognition benchmarks, we achieve
existing state-of-the-art performances and also demonstrate the benefits of
sequentially adding more contexts. We also identify inherent drawbacks of using
more contexts, highlighting that in the pursuit of even better results, the
model may need to adopt societal biases.</div><div><a href='http://arxiv.org/abs/2403.12469v1'>2403.12469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05963v1")'>Robust Emotion Recognition in Context Debiasing</div>
<div id='2403.05963v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T17:05:43Z</div><div>Authors: Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, Lihua Zhang</div><div style='padding-top: 10px; width: 80ex'>Context-aware emotion recognition (CAER) has recently boosted the practical
applications of affective computing techniques in unconstrained environments.
Mainstream CAER methods invariably extract ensemble representations from
diverse contexts and subject-centred characteristics to perceive the target
person's emotional state. Despite advancements, the biggest challenge remains
due to context bias interference. The harmful bias forces the models to rely on
spurious correlations between background contexts and emotion labels in
likelihood estimation, causing severe performance bottlenecks and confounding
valuable context priors. In this paper, we propose a counterfactual emotion
inference (CLEF) framework to address the above issue. Specifically, we first
formulate a generalized causal graph to decouple the causal relationships among
the variables in CAER. Following the causal graph, CLEF introduces a
non-invasive context branch to capture the adverse direct effect caused by the
context bias. During the inference, we eliminate the direct context effect from
the total causal effect by comparing factual and counterfactual outcomes,
resulting in bias mitigation and robust prediction. As a model-agnostic
framework, CLEF can be readily integrated into existing methods, bringing
consistent performance gains.</div><div><a href='http://arxiv.org/abs/2403.05963v1'>2403.05963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10841v2")'>Using LLMs to discover emerging coded antisemitic hate-speech in
  extremist social media</div>
<div id='2401.10841v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T17:40:50Z</div><div>Authors: Dhanush Kikkisetti, Raza Ul Mustafa, Wendy Melillo, Roberto Corizzo, Zois Boukouvalas, Jeff Gill, Nathalie Japkowicz</div><div style='padding-top: 10px; width: 80ex'>Online hate speech proliferation has created a difficult problem for social
media platforms. A particular challenge relates to the use of coded language by
groups interested in both creating a sense of belonging for its users and
evading detection. Coded language evolves quickly and its use varies over time.
This paper proposes a methodology for detecting emerging coded hate-laden
terminology. The methodology is tested in the context of online antisemitic
discourse. The approach considers posts scraped from social media platforms,
often used by extremist users. The posts are scraped using seed expressions
related to previously known discourse of hatred towards Jews. The method begins
by identifying the expressions most representative of each post and calculating
their frequency in the whole corpus. It filters out grammatically incoherent
expressions as well as previously encountered ones so as to focus on emergent
well-formed terminology. This is followed by an assessment of semantic
similarity to known antisemitic terminology using a fine-tuned large language
model, and subsequent filtering out of the expressions that are too distant
from known expressions of hatred. Emergent antisemitic expressions containing
terms clearly relating to Jewish topics are then removed to return only coded
expressions of hatred.</div><div><a href='http://arxiv.org/abs/2401.10841v2'>2401.10841v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16678v1")'>The Detection and Understanding of Fictional Discourse</div>
<div id='2401.16678v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T01:57:17Z</div><div>Authors: Andrew Piper, Haiqi Zhou</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present a variety of classification experiments related to
the task of fictional discourse detection. We utilize a diverse array of
datasets, including contemporary professionally published fiction, historical
fiction from the Hathi Trust, fanfiction, stories from Reddit, folk tales,
GPT-generated stories, and anglophone world literature. Additionally, we
introduce a new feature set of word "supersenses" that facilitate the goal of
semantic generalization. The detection of fictional discourse can help enrich
our knowledge of large cultural heritage archives and assist with the process
of understanding the distinctive qualities of fictional storytelling more
broadly.</div><div><a href='http://arxiv.org/abs/2401.16678v1'>2401.16678v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16441v1")'>FaKnow: A Unified Library for Fake News Detection</div>
<div id='2401.16441v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T13:29:17Z</div><div>Authors: Yiyuan Zhu, Yongjun Li, Jialiang Wang, Ming Gao, Jiali Wei</div><div style='padding-top: 10px; width: 80ex'>Over the past years, a large number of fake news detection algorithms based
on deep learning have emerged. However, they are often developed under
different frameworks, each mandating distinct utilization methodologies,
consequently hindering reproducibility. Additionally, a substantial amount of
redundancy characterizes the code development of such fake news detection
models. To address these concerns, we propose FaKnow, a unified and
comprehensive fake news detection algorithm library. It encompasses a variety
of widely used fake news detection models, categorized as content-based and
social context-based approaches. This library covers the full spectrum of the
model training and evaluation process, effectively organizing the data, models,
and training procedures within a unified framework. Furthermore, it furnishes a
series of auxiliary functionalities and tools, including visualization, and
logging. Our work contributes to the standardization and unification of fake
news detection research, concurrently facilitating the endeavors of researchers
in this field. The open-source code and documentation can be accessed at
https://github.com/NPURG/FaKnow and https://faknow.readthedocs.io,
respectively.</div><div><a href='http://arxiv.org/abs/2401.16441v1'>2401.16441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05569v1")'>SENet: Visual Detection of Online Social Engineering Attack Campaigns</div>
<div id='2401.05569v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:25:44Z</div><div>Authors: Irfan Ozen, Karthika Subramani, Phani Vadrevu, Roberto Perdisci</div><div style='padding-top: 10px; width: 80ex'>Social engineering (SE) aims at deceiving users into performing actions that
may compromise their security and privacy. These threats exploit weaknesses in
human's decision making processes by using tactics such as pretext, baiting,
impersonation, etc. On the web, SE attacks include attack classes such as
scareware, tech support scams, survey scams, sweepstakes, etc., which can
result in sensitive data leaks, malware infections, and monetary loss. For
instance, US consumers lose billions of dollars annually due to various SE
attacks. Unfortunately, generic social engineering attacks remain understudied,
compared to other important threats, such as software vulnerabilities and
exploitation, network intrusions, malicious software, and phishing. The few
existing technical studies that focus on social engineering are limited in
scope and mostly focus on measurements rather than developing a generic
defense. To fill this gap, we present SEShield, a framework for in-browser
detection of social engineering attacks. SEShield consists of three main
components: (i) a custom security crawler, called SECrawler, that is dedicated
to scouting the web to collect examples of in-the-wild SE attacks; (ii) SENet,
a deep learning-based image classifier trained on data collected by SECrawler
that aims to detect the often glaring visual traits of SE attack pages; and
(iii) SEGuard, a proof-of-concept extension that embeds SENet into the web
browser and enables real-time SE attack detection. We perform an extensive
evaluation of our system and show that SENet is able to detect new instances of
SE attacks with a detection rate of up to 99.6% at 1% false positive, thus
providing an effective first defense against SE attacks on the web.</div><div><a href='http://arxiv.org/abs/2401.05569v1'>2401.05569v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03740v1")'>BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning</div>
<div id='2402.03740v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:13:13Z</div><div>Authors: Mohammad Majid Akhtar, Navid Shadman Bhuiyan, Rahat Masood, Muhammad Ikram, Salil S. Kanhere</div><div style='padding-top: 10px; width: 80ex'>The detection of automated accounts, also known as "social bots", has been an
increasingly important concern for online social networks (OSNs). While several
methods have been proposed for detecting social bots, significant research gaps
remain. First, current models exhibit limitations in detecting sophisticated
bots that aim to mimic genuine OSN users. Second, these methods often rely on
simplistic profile features, which are susceptible to manipulation. In addition
to their vulnerability to adversarial manipulations, these models lack
generalizability, resulting in subpar performance when trained on one dataset
and tested on another.
  To address these challenges, we propose a novel framework for social Bot
detection with Self-Supervised Contrastive Learning (BotSSCL). Our framework
leverages contrastive learning to distinguish between social bots and humans in
the embedding space to improve linear separability. The high-level
representations derived by BotSSCL enhance its resilience to variations in data
distribution and ensure generalizability. We evaluate BotSSCL's robustness
against adversarial attempts to manipulate bot accounts to evade detection.
Experiments on two datasets featuring sophisticated bots demonstrate that
BotSSCL outperforms other supervised, unsupervised, and self-supervised
baseline methods. We achieve approx. 6% and approx. 8% higher (F1) performance
than SOTA on both datasets. In addition, BotSSCL also achieves 67% F1 when
trained on one dataset and tested with another, demonstrating its
generalizability. Lastly, BotSSCL increases adversarial complexity and only
allows 4% success to the adversary in evading detection.</div><div><a href='http://arxiv.org/abs/2402.03740v1'>2402.03740v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15537v1")'>Evaluating the Performance of ChatGPT for Spam Email Detection</div>
<div id='2402.15537v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T04:52:08Z</div><div>Authors: Yuwei Wu, Shijing Si, Yugui Zhang, Jiawen Gu, Jedrek Wosik</div><div style='padding-top: 10px; width: 80ex'>Email continues to be a pivotal and extensively utilized communication medium
within professional and commercial domains. Nonetheless, the prevalence of spam
emails poses a significant challenge for users, disrupting their daily routines
and diminishing productivity. Consequently, accurately identifying and
filtering spam based on content has become crucial for cybersecurity. Recent
advancements in natural language processing, particularly with large language
models like ChatGPT, have shown remarkable performance in tasks such as
question answering and text generation. However, its potential in spam
identification remains underexplored. To fill in the gap, this study attempts
to evaluate ChatGPT's capabilities for spam identification in both English and
Chinese email datasets. We employ ChatGPT for spam email detection using
in-context learning, which requires a prompt instruction and a few
demonstrations. We also investigate how the training example size affects the
performance of ChatGPT. For comparison, we also implement five popular
benchmark methods, including naive Bayes, support vector machines (SVM),
logistic regression (LR), feedforward dense neural networks (DNN), and BERT
classifiers. Though extensive experiments, the performance of ChatGPT is
significantly worse than deep supervised learning methods in the large English
dataset, while it presents superior performance on the low-resourced Chinese
dataset, even outperforming BERT in this case.</div><div><a href='http://arxiv.org/abs/2402.15537v1'>2402.15537v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01720v1")'>Deep Learning Based Amharic Chatbot for FAQs in Universities</div>
<div id='2402.01720v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T18:37:21Z</div><div>Authors: Goitom Ybrah Hailu, Shishay Welay</div><div style='padding-top: 10px; width: 80ex'>University students often spend a considerable amount of time seeking answers
to common questions from administrators or teachers. This can become tedious
for both parties, leading to a need for a solution. In response, this paper
proposes a chatbot model that utilizes natural language processing and deep
learning techniques to answer frequently asked questions (FAQs) in the Amharic
language. Chatbots are computer programs that simulate human conversation
through the use of artificial intelligence (AI), acting as a virtual assistant
to handle questions and other tasks. The proposed chatbot program employs
tokenization, normalization, stop word removal, and stemming to analyze and
categorize Amharic input sentences. Three machine learning model algorithms
were used to classify tokens and retrieve appropriate responses: Support Vector
Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented
through TensorFlow, Keras, and NLTK. The deep learning model achieved the best
results with 91.55% accuracy and a validation loss of 0.3548 using an Adam
optimizer and SoftMax activation function. The chatbot model was integrated
with Facebook Messenger and deployed on a Heroku server for 24-hour
accessibility. The experimental results demonstrate that the chatbot framework
achieved its objectives and effectively addressed challenges such as Amharic
Fidel variation, morphological variation, and lexical gaps. Future research
could explore the integration of Amharic WordNet to narrow the lexical gap and
support more complex questions.</div><div><a href='http://arxiv.org/abs/2402.01720v1'>2402.01720v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01023v1")'>CautionSuicide: A Deep Learning Based Approach for Detecting Suicidal
  Ideation in Real Time Chatbot Conversation</div>
<div id='2401.01023v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T04:14:16Z</div><div>Authors: Nelly Elsayed, Zag ElSayed, Murat Ozer</div><div style='padding-top: 10px; width: 80ex'>Suicide is recognized as one of the most serious concerns in the modern
society. Suicide causes tragedy that affects countries, communities, and
families. There are many factors that lead to suicidal ideations. Early
detection of suicidal ideations can help to prevent suicide occurrence by
providing the victim with the required professional support, especially when
the victim does not recognize the danger of having suicidal ideations. As
technology usage has increased, people share and express their ideations
digitally via social media, chatbots, and other digital platforms. In this
paper, we proposed a novel, simple deep learning-based model to detect suicidal
ideations in digital content, mainly focusing on chatbots as the primary data
source. In addition, we provide a framework that employs the proposed suicide
detection integration with a chatbot-based support system.</div><div><a href='http://arxiv.org/abs/2401.01023v1'>2401.01023v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00828v1")'>Deep Learning Detection Method for Large Language Models-Generated
  Scientific Content</div>
<div id='2403.00828v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T19:16:39Z</div><div>Authors: Bushra Alhijawi, Rawan Jarrar, Aseel AbuAlRub, Arwa Bader</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual
content is written and communicated. These models have the potential to
generate scientific content that is indistinguishable from that written by
humans. Hence, LLMs carry severe consequences for the scientific community,
which relies on the integrity and reliability of publications. This research
paper presents a novel ChatGPT-generated scientific text detection method,
AI-Catcher. AI-Catcher integrates two deep learning models, multilayer
perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the
feature representations of the linguistic and statistical features. The CNN
extracts high-level representations of the sequential patterns from the textual
content. AI-Catcher is a multimodal model that fuses hidden patterns derived
from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset
is collected to enhance AI-generated text detection tools, AIGTxt. AIGTxt
contains 3000 records collected from published academic articles across ten
domains and divided into three classes: Human-written, ChatGPT-generated, and
Mixed text. Several experiments are conducted to evaluate the performance of
AI-Catcher. The comparative results demonstrate the capability of AI-Catcher to
distinguish between human-written and ChatGPT-generated scientific text more
accurately than alternative methods. On average, AI-Catcher improved accuracy
by 37.4%.</div><div><a href='http://arxiv.org/abs/2403.00828v1'>2403.00828v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01712v1")'>Socially Aware Synthetic Data Generation for Suicidal Ideation Detection
  Using Large Language Models</div>
<div id='2402.01712v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:25:05Z</div><div>Authors: Hamideh Ghanadian, Isar Nejadgholi, Hussein Al Osman</div><div style='padding-top: 10px; width: 80ex'>Suicidal ideation detection is a vital research area that holds great
potential for improving mental health support systems. However, the sensitivity
surrounding suicide-related data poses challenges in accessing large-scale,
annotated datasets necessary for training effective machine learning models. To
address this limitation, we introduce an innovative strategy that leverages the
capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to
create synthetic data for suicidal ideation detection. Our data generation
approach is grounded in social factors extracted from psychology literature and
aims to ensure coverage of essential information related to suicidal ideation.
In our study, we benchmarked against state-of-the-art NLP classification
models, specifically, those centered around the BERT family structures. When
trained on the real-world dataset, UMD, these conventional models tend to yield
F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed
by social factors, offers consistent F1-scores of 0.82 for both models,
suggesting that the richness of topics in synthetic data can bridge the
performance gap across different model complexities. Most impressively, when we
combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a
substantial increase in performance, achieving an F1-score of 0.88 on the UMD
test set. Such results underscore the cost-effectiveness and potential of our
approach in confronting major challenges in the field, such as data scarcity
and the quest for diversity in data representation.</div><div><a href='http://arxiv.org/abs/2402.01712v1'>2402.01712v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17477v1")'>Detecting mental disorder on social media: a ChatGPT-augmented
  explainable approach</div>
<div id='2401.17477v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T22:22:55Z</div><div>Authors: Loris Belcastro, Riccardo Cantini, Fabrizio Marozzo, Domenico Talia, Paolo Trunfio</div><div style='padding-top: 10px; width: 80ex'>In the digital era, the prevalence of depressive symptoms expressed on social
media has raised serious concerns, necessitating advanced methodologies for
timely detection. This paper addresses the challenge of interpretable
depression detection by proposing a novel methodology that effectively combines
Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and
conversational agents like ChatGPT. In our methodology, explanations are
achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a
novel self-explanatory model, namely BERT-XDD, capable of providing both
classification and explanations via masked attention. The interpretability is
further enhanced using ChatGPT to transform technical explanations into
human-readable commentaries. By introducing an effective and modular approach
for interpretable depression detection, our methodology can contribute to the
development of socially responsible digital platforms, fostering early
intervention and support for mental health challenges under the guidance of
qualified healthcare professionals.</div><div><a href='http://arxiv.org/abs/2401.17477v1'>2401.17477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15951v1")'>GreenLLaMA: A Framework for Detoxification with Explanations</div>
<div id='2402.15951v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T01:56:47Z</div><div>Authors: Md Tawkat Islam Khondaker, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan</div><div style='padding-top: 10px; width: 80ex'>Prior works on detoxification are scattered in the sense that they do not
cover all aspects of detoxification needed in a real-world scenario. Notably,
prior works restrict the task of developing detoxification models to only a
seen subset of platforms, leaving the question of how the models would perform
on unseen platforms unexplored. Additionally, these works do not address
non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified
without altering the meaning. We propose GreenLLaMA, the first comprehensive
end-to-end detoxification framework, which attempts to alleviate the
aforementioned limitations. We first introduce a cross-platform pseudo-parallel
corpus applying multi-step data processing and generation strategies leveraging
ChatGPT. We then train a suite of detoxification models with our cross-platform
corpus. We show that our detoxification models outperform the SoTA model
trained with human-annotated parallel corpus. We further introduce explanation
to promote transparency and trustworthiness. GreenLLaMA additionally offers a
unique paraphrase detector especially dedicated for the detoxification task to
tackle the non-detoxifiable cases. Through experimental analysis, we
demonstrate the effectiveness of our cross-platform corpus and the robustness
of GreenLLaMA against adversarial toxicity.</div><div><a href='http://arxiv.org/abs/2402.15951v1'>2402.15951v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08225v2")'>Improving Black-box Robustness with In-Context Rewriting</div>
<div id='2402.08225v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T05:33:35Z</div><div>Authors: Kyle O'Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, Thomas Hartvigsen</div><div style='padding-top: 10px; width: 80ex'>Machine learning models often excel on in-distribution (ID) data but struggle
with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD
robustness are not applicable to settings where the model is effectively a
black box, such as when the weights are frozen, retraining is costly, or the
model is leveraged via an API. Test-time augmentation (TTA) is a simple
post-hoc technique for improving robustness that sidesteps black-box
constraints by aggregating predictions across multiple augmentations of the
test input. TTA has seen limited use in NLP due to the challenge of generating
effective natural language augmentations. In this work, we propose LLM-TTA,
which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA
outperforms conventional augmentation functions across sentiment, toxicity, and
news classification tasks for BERT and T5 models, with BERT's OOD robustness
improving by an average of 4.30 percentage points without regressing average ID
performance. We explore selectively augmenting inputs based on prediction
entropy to reduce the rate of expensive LLM augmentations, allowing us to
maintain performance gains while reducing the average number of generated
augmentations by 57.76%. LLM-TTA is agnostic to the task model architecture,
does not require OOD labels, and is effective across low and high-resource
settings. We share our data, models, and code for reproducibility.</div><div><a href='http://arxiv.org/abs/2402.08225v2'>2402.08225v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09407v2")'>Deciphering Textual Authenticity: A Generalized Strategy through the
  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated
  Text</div>
<div id='2401.09407v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T18:45:13Z</div><div>Authors: Mazal Bethany, Brandon Wherry, Emet Bethany, Nishant Vishwamitra, Anthony Rios, Peyman Najafirad</div><div style='padding-top: 10px; width: 80ex'>With the recent proliferation of Large Language Models (LLMs), there has been
an increasing demand for tools to detect machine-generated text. The effective
detection of machine-generated text face two pertinent problems: First, they
are severely limited in generalizing against real-world scenarios, where
machine-generated text is produced by a variety of generators, including but
not limited to GPT-4 and Dolly, and spans diverse domains, ranging from
academic manuscripts to social media posts. Second, existing detection
methodologies treat texts produced by LLMs through a restrictive binary
classification lens, neglecting the nuanced diversity of artifacts generated by
different LLMs. In this work, we undertake a systematic study on the detection
of machine-generated text in real-world scenarios. We first study the
effectiveness of state-of-the-art approaches and find that they are severely
limited against text produced by diverse generators and domains in the real
world. Furthermore, t-SNE visualizations of the embeddings from a pretrained
LLM's encoder show that they cannot reliably distinguish between human and
machine-generated text. Based on our findings, we introduce a novel system,
T5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder
combined with LLM embedding sub-clustering to address the text produced by
diverse generators and domains in the real world. We evaluate our approach
across 9 machine-generated text systems and 9 domains and find that our
approach provides state-of-the-art generalization ability, with an average
increase in F1 score on machine-generated text of 19.6\% on unseen generators
and domains compared to the top performing existing approaches and correctly
attributes the generator of text with an accuracy of 93.6\%.</div><div><a href='http://arxiv.org/abs/2401.09407v2'>2401.09407v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11399v1")'>k-SemStamp: A Clustering-Based Semantic Watermark for Detection of
  Machine-Generated Text</div>
<div id='2402.11399v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T22:50:38Z</div><div>Authors: Abe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, Tianxing He</div><div style='padding-top: 10px; width: 80ex'>Recent watermarked generation algorithms inject detectable signatures during
language generation to facilitate post-hoc detection. While token-level
watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023)
applies watermark on the semantic representation of sentences and demonstrates
promising robustness. SemStamp employs locality-sensitive hashing (LSH) to
partition the semantic space with arbitrary hyperplanes, which results in a
suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a
simple yet effective enhancement of SemStamp, utilizing k-means clustering as
an alternative of LSH to partition the embedding space with awareness of
inherent semantic structure. Experimental results indicate that k-SemStamp
saliently improves its robustness and sampling efficiency while preserving the
generation quality, advancing a more effective tool for machine-generated text
detection.</div><div><a href='http://arxiv.org/abs/2402.11399v1'>2402.11399v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13000v1")'>Duwak: Dual Watermarks in Large Language Models</div>
<div id='2403.13000v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:25:38Z</div><div>Authors: Chaoyi Zhu, Jeroen Galjaard, Pin-Yu Chen, Lydia Y. Chen</div><div style='padding-top: 10px; width: 80ex'>As large language models (LLM) are increasingly used for text generation
tasks, it is critical to audit their usages, govern their applications, and
mitigate their potential harms. Existing watermark techniques are shown
effective in embedding single human-imperceptible and machine-detectable
patterns without significantly affecting generated text quality and semantics.
However, the efficiency in detecting watermarks, i.e., the minimum number of
tokens required to assert detection with significance and robustness against
post-editing, is still debatable. In this paper, we propose, Duwak, to
fundamentally enhance the efficiency and quality of watermarking by embedding
dual secret patterns in both token probability distribution and sampling
schemes. To mitigate expression degradation caused by biasing toward certain
tokens, we design a contrastive search to watermark the sampling scheme, which
minimizes the token repetition and enhances the diversity. We theoretically
explain the interdependency of the two watermarks within Duwak. We evaluate
Duwak extensively on Llama2 under various post-editing attacks, against four
state-of-the-art watermarking techniques and combinations of them. Our results
show that Duwak marked text achieves the highest watermarked text quality at
the lowest required token count for detection, up to 70% tokens less than
existing approaches, especially under post paraphrasing.</div><div><a href='http://arxiv.org/abs/2403.13000v1'>2403.13000v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18059v2")'>Token-Specific Watermarking with Enhanced Detectability and Semantic
  Coherence for Large Language Models</div>
<div id='2402.18059v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T05:43:22Z</div><div>Authors: Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz Koushanfar, Pengtao Xie</div><div style='padding-top: 10px; width: 80ex'>Large language models generate high-quality responses with potential
misinformation, underscoring the need for regulation by distinguishing
AI-generated and human-written texts. Watermarking is pivotal in this context,
which involves embedding hidden markers in texts during the LLM inference
phase, which is imperceptible to humans. Current watermarking algorithms,
however, face the challenge of achieving both the detectability of inserted
watermarks and the semantic integrity of generated texts, where enhancing one
aspect often undermines the other. To overcome this, we introduce a novel
multi-objective optimization (MOO) approach for watermarking that utilizes
lightweight networks to generate token-specific watermarking logits and
splitting ratios. By leveraging MOO to optimize for both detection and semantic
objective functions, our method simultaneously achieves detectability and
semantic integrity. Experimental results show that our method outperforms
current watermarking techniques in enhancing the detectability of texts
generated by LLMs while maintaining their semantic coherence. Our code is
available at https://github.com/mignonjia/TS_watermark.</div><div><a href='http://arxiv.org/abs/2402.18059v2'>2402.18059v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13027v1")'>Towards Better Statistical Understanding of Watermarking LLMs</div>
<div id='2403.13027v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T01:57:09Z</div><div>Authors: Zhongze Cai, Shang Liu, Hanzhao Wang, Huaiyang Zhong, Xiaocheng Li</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the problem of watermarking large language models
(LLMs). We consider the trade-off between model distortion and detection
ability and formulate it as a constrained optimization problem based on the
green-red algorithm of Kirchenbauer et al. (2023a). We show that the optimal
solution to the optimization problem enjoys a nice analytical property which
provides a better understanding and inspires the algorithm design for the
watermarking process. We develop an online dual gradient ascent watermarking
algorithm in light of this optimization formulation and prove its asymptotic
Pareto optimality between model distortion and detection ability. Such a result
guarantees an averaged increased green list probability and henceforth
detection ability explicitly (in contrast to previous results). Moreover, we
provide a systematic discussion on the choice of the model distortion metrics
for the watermarking problem. We justify our choice of KL divergence and
present issues with the existing criteria of ``distortion-free'' and
perplexity. Finally, we empirically evaluate our algorithms on extensive
datasets against benchmark algorithms.</div><div><a href='http://arxiv.org/abs/2403.13027v1'>2403.13027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16578v1")'>Multi-Bit Distortion-Free Watermarking for Large Language Models</div>
<div id='2402.16578v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T14:01:34Z</div><div>Authors: Massieh Kordi Boroujeny, Ya Jiang, Kai Zeng, Brian Mark</div><div style='padding-top: 10px; width: 80ex'>Methods for watermarking large language models have been proposed that
distinguish AI-generated text from human-generated text by slightly altering
the model output distribution, but they also distort the quality of the text,
exposing the watermark to adversarial detection. More recently, distortion-free
watermarking methods were proposed that require a secret key to detect the
watermark. The prior methods generally embed zero-bit watermarks that do not
provide additional information beyond tagging a text as being AI-generated. We
extend an existing zero-bit distortion-free watermarking method by embedding
multiple bits of meta-information as part of the watermark. We also develop a
computationally efficient decoder that extracts the embedded information from
the watermark with low bit error rate.</div><div><a href='http://arxiv.org/abs/2402.16578v1'>2402.16578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19361v1")'>Watermark Stealing in Large Language Models</div>
<div id='2402.19361v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T17:12:39Z</div><div>Authors: Nikola Jovanović, Robin Staab, Martin Vechev</div><div style='padding-top: 10px; width: 80ex'>LLM watermarking has attracted attention as a promising way to detect
AI-generated content, with some works suggesting that current schemes may
already be fit for deployment. In this work we dispute this claim, identifying
watermark stealing (WS) as a fundamental vulnerability of these schemes. We
show that querying the API of the watermarked LLM to approximately
reverse-engineer a watermark enables practical spoofing attacks, as suggested
in prior work, but also greatly boosts scrubbing attacks, which was previously
unnoticed. We are the first to propose an automated WS algorithm and use it in
the first comprehensive study of spoofing and scrubbing in realistic settings.
We show that for under $50 an attacker can both spoof and scrub
state-of-the-art schemes previously considered safe, with average success rate
of over 80%. Our findings challenge common beliefs about LLM watermarking,
stressing the need for more robust schemes. We make all our code and additional
examples available at https://watermark-stealing.org.</div><div><a href='http://arxiv.org/abs/2402.19361v1'>2402.19361v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04808v1")'>WaterMax: breaking the LLM watermark detectability-robustness-quality
  trade-off</div>
<div id='2403.04808v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T10:55:30Z</div><div>Authors: Eva Giboulot, Furon Teddy</div><div style='padding-top: 10px; width: 80ex'>Watermarking is a technical means to dissuade malfeasant usage of Large
Language Models. This paper proposes a novel watermarking scheme, so-called
WaterMax, that enjoys high detectability while sustaining the quality of the
generated text of the original LLM. Its new design leaves the LLM untouched (no
modification of the weights, logits, temperature, or sampling technique).
WaterMax balances robustness and complexity contrary to the watermarking
techniques of the literature inherently provoking a trade-off between quality
and robustness. Its performance is both theoretically proven and experimentally
validated. It outperforms all the SotA techniques under the most complete
benchmark suite.</div><div><a href='http://arxiv.org/abs/2403.04808v1'>2403.04808v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16187v1")'>Attacking LLM Watermarks by Exploiting Their Strengths</div>
<div id='2402.16187v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T20:24:07Z</div><div>Authors: Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith</div><div style='padding-top: 10px; width: 80ex'>Advances in generative models have made it possible for AI-generated text,
code, and images to mirror human-generated content in many applications.
Watermarking, a technique that aims to embed information in the output of a
model to verify its source, is useful for mitigating misuse of such
AI-generated content. However, existing watermarking schemes remain
surprisingly susceptible to attack. In particular, we show that desirable
properties shared by existing LLM watermarking systems such as quality
preservation, robustness, and public detection APIs can in turn make these
systems vulnerable to various attacks. We rigorously study potential attacks in
terms of common watermark design choices, and propose best practices and
defenses for mitigation -- establishing a set of practical guidelines for
embedding and detection of LLM watermarks.</div><div><a href='http://arxiv.org/abs/2402.16187v1'>2402.16187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15365v1")'>A Transfer Attack to Image Watermarks</div>
<div id='2403.15365v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:33:11Z</div><div>Authors: Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong</div><div style='padding-top: 10px; width: 80ex'>Watermark has been widely deployed by industry to detect AI-generated images.
The robustness of such watermark-based detector against evasion attacks in the
white-box and black-box settings is well understood in the literature. However,
the robustness in the no-box setting is much less understood. In particular,
multiple studies claimed that image watermark is robust in such setting. In
this work, we propose a new transfer evasion attack to image watermark in the
no-box setting. Our transfer attack adds a perturbation to a watermarked image
to evade multiple surrogate watermarking models trained by the attacker itself,
and the perturbed watermarked image also evades the target watermarking model.
Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks
even if the attacker does not have access to the watermarking model nor the
detection API.</div><div><a href='http://arxiv.org/abs/2403.15365v1'>2403.15365v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16889v1")'>Generative Models are Self-Watermarked: Declaring Model Authentication
  through Re-Generation</div>
<div id='2402.16889v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T10:48:21Z</div><div>Authors: Aditya Desu, Xuanli He, Qiongkai Xu, Wei Lu</div><div style='padding-top: 10px; width: 80ex'>As machine- and AI-generated content proliferates, protecting the
intellectual property of generative models has become imperative, yet verifying
data ownership poses formidable challenges, particularly in cases of
unauthorized reuse of generated data. The challenge of verifying data ownership
is further amplified by using Machine Learning as a Service (MLaaS), which
often functions as a black-box system.
  Our work is dedicated to detecting data reuse from even an individual sample.
Traditionally, watermarking has been leveraged to detect AI-generated content.
However, unlike watermarking techniques that embed additional information as
triggers into models or generated content, potentially compromising output
quality, our approach identifies latent fingerprints inherently present within
the outputs through re-generation. We propose an explainable verification
procedure that attributes data ownership through re-generation, and further
amplifies these fingerprints in the generative models through iterative data
re-generation. This methodology is theoretically grounded and demonstrates
viability and robustness using recent advanced text and image generative
models. Our methodology is significant as it goes beyond protecting the
intellectual property of APIs and addresses important issues such as the spread
of misinformation and academic misconduct. It provides a useful tool to ensure
the integrity of sources and authorship, expanding its application in different
scenarios where authenticity and ownership verification are essential.</div><div><a href='http://arxiv.org/abs/2402.16889v1'>2402.16889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11162v1")'>CGI-DM: Digital Copyright Authentication for Diffusion Models via
  Contrasting Gradient Inversion</div>
<div id='2403.11162v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T10:06:38Z</div><div>Authors: Xiaoyu Wu, Yang Hua, Chumeng Liang, Jiaru Zhang, Hao Wang, Tao Song, Haibing Guan</div><div style='padding-top: 10px; width: 80ex'>Diffusion Models (DMs) have evolved into advanced image generation tools,
especially for few-shot generation where a pretrained model is fine-tuned on a
small set of images to capture a specific style or object. Despite their
success, concerns exist about potential copyright violations stemming from the
use of unauthorized data in this process. In response, we present Contrasting
Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring
vivid visual representations for digital copyright authentication. Our approach
involves removing partial information of an image and recovering missing
details by exploiting conceptual differences between the pretrained and
fine-tuned models. We formulate the differences as KL divergence between latent
variables of the two models when given the same input image, which can be
maximized through Monte Carlo sampling and Projected Gradient Descent (PGD).
The similarity between original and recovered images serves as a strong
indicator of potential infringements. Extensive experiments on the WikiArt and
Dreambooth datasets demonstrate the high accuracy of CGI-DM in digital
copyright authentication, surpassing alternative validation techniques. Code
implementation is available at https://github.com/Nicholas0228/Revelio.</div><div><a href='http://arxiv.org/abs/2403.11162v1'>2403.11162v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07087v1")'>Exploring Adversarial Attacks against Latent Diffusion Model from the
  Perspective of Adversarial Transferability</div>
<div id='2401.07087v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T14:34:18Z</div><div>Authors: Junxi Chen, Junhao Dong, Xiaohua Xie</div><div style='padding-top: 10px; width: 80ex'>Recently, many studies utilized adversarial examples (AEs) to raise the cost
of malicious image editing and copyright violation powered by latent diffusion
models (LDMs). Despite their successes, a few have studied the surrogate model
they used to generate AEs. In this paper, from the perspective of adversarial
transferability, we investigate how the surrogate model's property influences
the performance of AEs for LDMs. Specifically, we view the time-step sampling
in the Monte-Carlo-based (MC-based) adversarial attack as selecting surrogate
models. We find that the smoothness of surrogate models at different time steps
differs, and we substantially improve the performance of the MC-based AEs by
selecting smoother surrogate models. In the light of the theoretical framework
on adversarial transferability in image classification, we also conduct a
theoretical analysis to explain why smooth surrogate models can also boost AEs
for LDMs.</div><div><a href='http://arxiv.org/abs/2401.07087v1'>2401.07087v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13006v1")'>CIMGEN: Controlled Image Manipulation by Finetuning Pretrained
  Generative Models on Limited Data</div>
<div id='2401.13006v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T06:30:47Z</div><div>Authors: Chandrakanth Gudavalli, Erik Rosten, Lakshmanan Nataraj, Shivkumar Chandrasekaran, B. S. Manjunath</div><div style='padding-top: 10px; width: 80ex'>Content creation and image editing can benefit from flexible user controls. A
common intermediate representation for conditional image generation is a
semantic map, that has information of objects present in the image. When
compared to raw RGB pixels, the modification of semantic map is much easier.
One can take a semantic map and easily modify the map to selectively insert,
remove, or replace objects in the map. The method proposed in this paper takes
in the modified semantic map and alter the original image in accordance to the
modified map. The method leverages traditional pre-trained image-to-image
translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a
limited dataset of reference images associated with the semantic maps. We
discuss the qualitative and quantitative performance of our technique to
illustrate its capacity and possible applications in the fields of image
forgery and image editing. We also demonstrate the effectiveness of the
proposed image forgery technique in thwarting the numerous deep learning-based
image forensic techniques, highlighting the urgent need to develop robust and
generalizable image forensic tools in the fight against the spread of fake
media.</div><div><a href='http://arxiv.org/abs/2401.13006v1'>2401.13006v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03736v1")'>Unifying Generation and Compression: Ultra-low bitrate Image Coding Via
  Multi-stage Transformer</div>
<div id='2403.03736v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T14:27:02Z</div><div>Authors: Naifu Xue, Qi Mao, Zijian Wang, Yuan Zhang, Siwei Ma</div><div style='padding-top: 10px; width: 80ex'>Recent progress in generative compression technology has significantly
improved the perceptual quality of compressed data. However, these advancements
primarily focus on producing high-frequency details, often overlooking the
ability of generative models to capture the prior distribution of image
content, thus impeding further bitrate reduction in extreme compression
scenarios (&lt;0.05 bpp). Motivated by the capabilities of predictive language
models for lossless compression, this paper introduces a novel Unified Image
Generation-Compression (UIGC) paradigm, merging the processes of generation and
compression. A key feature of the UIGC framework is the adoption of
vector-quantized (VQ) image models for tokenization, alongside a multi-stage
transformer designed to exploit spatial contextual information for modeling the
prior distribution. As such, the dual-purpose framework effectively utilizes
the learned prior for entropy estimation and assists in the regeneration of
lost tokens. Extensive experiments demonstrate the superiority of the proposed
UIGC framework over existing codecs in perceptual quality and human perception,
particularly in ultra-low bitrate scenarios (&lt;=0.03 bpp), pioneering a new
direction in generative compression.</div><div><a href='http://arxiv.org/abs/2403.03736v1'>2403.03736v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10892v1")'>Proving membership in LLM pretraining data via data watermarks</div>
<div id='2402.10892v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:49:27Z</div><div>Authors: Johnny Tian-Zheng Wei, Ryan Yixiang Wang, Robin Jia</div><div style='padding-top: 10px; width: 80ex'>Detecting whether copyright holders' works were used in LLM pretraining is
poised to be an important problem. This work proposes using data watermarks to
enable principled detection with only black-box model access, provided that the
rightholder contributed multiple training documents and watermarked them before
public release. By applying a randomly sampled data watermark, detection can be
framed as hypothesis testing, which provides guarantees on the false detection
rate. We study two watermarks: one that inserts random sequences, and another
that randomly substitutes characters with Unicode lookalikes. We first show how
three aspects of watermark design -- watermark length, number of duplications,
and interference -- affect the power of the hypothesis test. Next, we study how
a watermark's detection strength changes under model and dataset scaling: while
increasing the dataset size decreases the strength of the watermark, watermarks
remain strong if the model size also increases. Finally, we view SHA hashes as
natural watermarks and show that we can robustly detect hashes from
BLOOM-176B's training data, as long as they occurred at least 90 times.
Together, our results point towards a promising future for data watermarks in
real world use.</div><div><a href='http://arxiv.org/abs/2402.10892v1'>2402.10892v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11227v1")'>On the Role of Similarity in Detecting Masquerading Files</div>
<div id='2402.11227v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T09:10:05Z</div><div>Authors: Jonathan Oliver, Jue Mo, Susmit Yenkar, Raghav Batta, Sekhar Josyoula</div><div style='padding-top: 10px; width: 80ex'>Similarity has been applied to a wide range of security applications,
typically used in machine learning models. We examine the problem posed by
masquerading samples; that is samples crafted by bad actors to be similar or
near identical to legitimate samples. We find that these samples potentially
create significant problems for machine learning solutions. The primary problem
being that bad actors can circumvent machine learning solutions by using
masquerading samples.
  We then examine the interplay between digital signatures and machine learning
solutions. In particular, we focus on executable files and code signing. We
offer a taxonomy for masquerading files. We use a combination of similarity and
clustering to find masquerading files. We use the insights gathered in this
process to offer improvements to similarity based and machine learning security
solutions.</div><div><a href='http://arxiv.org/abs/2402.11227v1'>2402.11227v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02209v2")'>On the Exploitation of DCT-Traces in the Generative-AI Domain</div>
<div id='2402.02209v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T16:45:31Z</div><div>Authors: Orazio Pontorno, Luca Guarnera, Sebastiano Battiato</div><div style='padding-top: 10px; width: 80ex'>Deepfakes represent one of the toughest challenges in the world of
Cybersecurity and Digital Forensics, especially considering the high-quality
results obtained with recent generative AI-based solutions. Almost all
generative models leave unique traces in synthetic data that, if analyzed and
identified in detail, can be exploited to improve the generalization
limitations of existing deepfake detectors. In this paper we analyzed deepfake
images in the frequency domain generated by both GAN and Diffusion Model
engines, examining in detail the underlying statistical distribution of
Discrete Cosine Transform (DCT) coefficients. Recognizing that not all
coefficients contribute equally to image detection, we hypothesize the
existence of a unique "discriminative fingerprint", embedded in specific
combinations of coefficients. To identify them, Machine Learning classifiers
were trained on various combinations of coefficients. In addition, the
Explainable AI (XAI) LIME algorithm was used to search for intrinsic
discriminative combinations of coefficients. Finally, we performed a robustness
test to analyze the persistence of traces by applying JPEG compression. The
experimental results reveal the existence of traces left by the generative
models that are more discriminative and persistent at JPEG attacks.</div><div><a href='http://arxiv.org/abs/2402.02209v2'>2402.02209v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09370v1")'>Pseudorandom Error-Correcting Codes</div>
<div id='2402.09370v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:17:45Z</div><div>Authors: Miranda Christ, Sam Gunn</div><div style='padding-top: 10px; width: 80ex'>We construct pseudorandom error-correcting codes (or simply pseudorandom
codes), which are error-correcting codes with the property that any polynomial
number of codewords are pseudorandom to any computationally-bounded adversary.
Efficient decoding of corrupted codewords is possible with the help of a
decoding key.
  We build pseudorandom codes that are robust to substitution and deletion
errors, where pseudorandomness rests on standard cryptographic assumptions.
Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of
LPN, or polynomial hardness of LPN and the planted XOR problem at low density.
  As our primary application of pseudorandom codes, we present an undetectable
watermarking scheme for outputs of language models that is robust to cropping
and a constant rate of random substitutions and deletions. The watermark is
undetectable in the sense that any number of samples of watermarked text are
computationally indistinguishable from text output by the original model. This
is the first undetectable watermarking scheme that can tolerate a constant rate
of errors.
  Our second application is to steganography, where a secret message is hidden
in innocent-looking content. We present a constant-rate stateless steganography
scheme with robustness to a constant rate of substitutions. Ours is the first
stateless steganography scheme with provable steganographic security and any
robustness to errors.</div><div><a href='http://arxiv.org/abs/2402.09370v1'>2402.09370v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08573v2")'>Benchmarking the Robustness of Image Watermarks</div>
<div id='2401.08573v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:58:36Z</div><div>Authors: Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu, Chenghao Deng, Sicheng Zhu, Abdirisak Mohamed, Yuxin Wen, Tom Goldstein, Furong Huang</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the weaknesses of image watermarking techniques. We
present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel
benchmark for assessing watermark robustness, overcoming the limitations of
current evaluation methods.WAVES integrates detection and identification tasks,
and establishes a standardized evaluation protocol comprised of a diverse range
of stress tests. The attacks in WAVES range from traditional image distortions
to advanced and novel variations of diffusive, and adversarial attacks. Our
evaluation examines two pivotal dimensions: the degree of image quality
degradation and the efficacy of watermark detection after attacks. We develop a
series of Performance vs. Quality 2D plots, varying over several prominent
image similarity metrics, which are then aggregated in a heuristically novel
manner to paint an overall picture of watermark robustness and attack potency.
Our comprehensive evaluation reveals previously undetected vulnerabilities of
several modern watermarking algorithms. We envision WAVES as a toolkit for the
future development of robust watermarking systems. The project is available at
https://wavesbench.github.io/</div><div><a href='http://arxiv.org/abs/2401.08573v2'>2401.08573v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14904v1")'>Watermarking Makes Language Models Radioactive</div>
<div id='2402.14904v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:55:22Z</div><div>Authors: Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy Furon</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the radioactivity of LLM-generated texts, i.e.
whether it is possible to detect that such input was used as training data.
Conventional methods like membership inference can carry out this detection
with some level of accuracy. We show that watermarked training data leaves
traces easier to detect and much more reliable than membership inference. We
link the contamination level to the watermark robustness, its proportion in the
training set, and the fine-tuning process. We notably demonstrate that training
on watermarked synthetic instructions can be detected with high confidence
(p-value &lt; 1e-5) even when as little as 5% of training text is watermarked.
Thus, LLM watermarking, originally designed for detecting machine-generated
text, gives the ability to easily identify if the outputs of a watermarked LLM
were used to fine-tune another LLM.</div><div><a href='http://arxiv.org/abs/2402.14904v1'>2402.14904v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00835v1")'>ALISON: Fast and Effective Stylometric Authorship Obfuscation</div>
<div id='2402.00835v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:22:32Z</div><div>Authors: Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee</div><div style='padding-top: 10px; width: 80ex'>Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing
tasks of increasing importance in privacy research. Modern AA leverages an
author's consistent writing style to match a text to its author using an AA
classifier. AO is the corresponding adversarial task, aiming to modify a text
in such a way that its semantics are preserved, yet an AA model cannot
correctly infer its authorship. To address privacy concerns raised by
state-of-the-art (SOTA) AA methods, new AO methods have been proposed but
remain largely impractical to use due to their prohibitively slow training and
obfuscation speed, often taking hours. To this challenge, we propose a
practical AO method, ALISON, that (1) dramatically reduces training/obfuscation
time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2)
achieves better obfuscation success through attacking three transformer-based
AA methods on two benchmark datasets, typically performing 15% better than
competing methods, (3) does not require direct signals from a target AA
classifier during obfuscation, and (4) utilizes unique stylometric features,
allowing sound model interpretation for explainable obfuscation. We also
demonstrate that ALISON can effectively prevent four SOTA AA methods from
accurately determining the authorship of ChatGPT-generated texts, all while
minimally changing the original text semantics. To ensure the reproducibility
of our findings, our code and data are available at:
https://github.com/EricX003/ALISON.</div><div><a href='http://arxiv.org/abs/2402.00835v1'>2402.00835v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14719v1")'>Bypassing LLM Watermarks with Color-Aware Substitutions</div>
<div id='2403.14719v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:54:39Z</div><div>Authors: Qilong Wu, Varun Chandrasekaran</div><div style='padding-top: 10px; width: 80ex'>Watermarking approaches are proposed to identify if text being circulated is
human or large language model (LLM) generated. The state-of-the-art
watermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate
specific (``green'') tokens. However, determining the robustness of this
watermarking method is an open problem. Existing attack methods fail to evade
detection for longer text segments. We overcome this limitation, and propose
{\em Self Color Testing-based Substitution (SCTS)}, the first ``color-aware''
attack. SCTS obtains color information by strategically prompting the
watermarked LLM and comparing output tokens frequencies. It uses this
information to determine token colors, and substitutes green tokens with
non-green ones. In our experiments, SCTS successfully evades watermark
detection using fewer number of edits than related work. Additionally, we show
both theoretically and empirically that SCTS can remove the watermark for
arbitrarily long watermarked text.</div><div><a href='http://arxiv.org/abs/2403.14719v1'>2403.14719v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11297v1")'>A Modified Word Saliency-Based Adversarial Attack on Text Classification
  Models</div>
<div id='2403.11297v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T18:39:14Z</div><div>Authors: Hetvi Waghela, Sneha Rakshit, Jaydip Sen</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel adversarial attack method targeting text
classification models, termed the Modified Word Saliency-based Adversarial
At-tack (MWSAA). The technique builds upon the concept of word saliency to
strategically perturb input texts, aiming to mislead classification models
while preserving semantic coherence. By refining the traditional adversarial
attack approach, MWSAA significantly enhances its efficacy in evading detection
by classification systems. The methodology involves first identifying salient
words in the input text through a saliency estimation process, which
prioritizes words most influential to the model's decision-making process.
Subsequently, these salient words are subjected to carefully crafted
modifications, guided by semantic similarity metrics to ensure that the altered
text remains coherent and retains its original meaning. Empirical evaluations
conducted on diverse text classification datasets demonstrate the effectiveness
of the proposed method in generating adversarial examples capable of
successfully deceiving state-of-the-art classification models. Comparative
analyses with existing adversarial attack techniques further indicate the
superiority of the proposed approach in terms of both attack success rate and
preservation of text coherence.</div><div><a href='http://arxiv.org/abs/2403.11297v1'>2403.11297v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11833v1")'>SSCAE -- Semantic, Syntactic, and Context-aware natural language
  Adversarial Examples generator</div>
<div id='2403.11833v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:45:20Z</div><div>Authors: Javad Rafiei Asl, Mohammad H. Rafiei, Manar Alohaly, Daniel Takabi</div><div style='padding-top: 10px; width: 80ex'>Machine learning models are vulnerable to maliciously crafted Adversarial
Examples (AEs). Training a machine learning model with AEs improves its
robustness and stability against adversarial attacks. It is essential to
develop models that produce high-quality AEs. Developing such models has been
much slower in natural language processing (NLP) than in areas such as computer
vision. This paper introduces a practical and efficient adversarial attack
model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and
\textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE
identifies important words and uses a masked language model to generate an
early set of substitutions. Next, two well-known language models are employed
to evaluate the initial set in terms of semantic and syntactic characteristics.
We introduce (1) a dynamic threshold to capture more efficient perturbations
and (2) a local greedy search to generate high-quality AEs. As a black-box
method, SSCAE generates humanly imperceptible and context-aware AEs that
preserve semantic consistency and the source language's syntactical and
grammatical requirements. The effectiveness and superiority of the proposed
SSCAE model are illustrated with fifteen comparative experiments and extensive
sensitivity analysis for parameter optimization. SSCAE outperforms the existing
models in all experiments while maintaining a higher semantic consistency with
a lower query number and a comparable perturbation rate.</div><div><a href='http://arxiv.org/abs/2403.11833v1'>2403.11833v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06783v1")'>MultiSiam: A Multiple Input Siamese Network For Social Media Text
  Classification And Duplicate Text Detection</div>
<div id='2401.06783v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T09:13:34Z</div><div>Authors: Sudhanshu Bhoi, Swapnil Markhedkar, Shruti Phadke, Prashant Agrawal</div><div style='padding-top: 10px; width: 80ex'>Social media accounts post increasingly similar content, creating a chaotic
experience across platforms, which makes accessing desired information
difficult. These posts can be organized by categorizing and grouping duplicates
across social handles and accounts. There can be more than one duplicate of a
post, however, a conventional Siamese neural network only considers a pair of
inputs for duplicate text detection. In this paper, we first propose a
multiple-input Siamese network, MultiSiam. This condensed network is then used
to propose another model, SMCD (Social Media Classification and Duplication
Model) to perform both duplicate text grouping and categorization. The
MultiSiam network, just like the Siamese, can be used in multiple applications
by changing the sub-network appropriately.</div><div><a href='http://arxiv.org/abs/2401.06783v1'>2401.06783v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09199v1")'>Ten Words Only Still Help: Improving Black-Box AI-Generated Text
  Detection via Proxy-Guided Efficient Re-Sampling</div>
<div id='2402.09199v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T14:32:16Z</div><div>Authors: Yuhui Shi, Qiang Sheng, Juan Cao, Hao Mi, Beizhe Hu, Danding Wang</div><div style='padding-top: 10px; width: 80ex'>With the rapidly increasing application of large language models (LLMs),
their abuse has caused many undesirable societal problems such as fake news,
academic dishonesty, and information pollution. This makes AI-generated text
(AIGT) detection of great importance. Among existing methods, white-box methods
are generally superior to black-box methods in terms of performance and
generalizability, but they require access to LLMs' internal states and are not
applicable to black-box settings. In this paper, we propose to estimate word
generation probabilities as pseudo white-box features via multiple re-sampling
to help improve AIGT detection under the black-box setting. Specifically, we
design POGER, a proxy-guided efficient re-sampling method, which selects a
small subset of representative words (e.g., 10 words) for performing multiple
re-sampling in black-box AIGT detection. Experiments on datasets containing
texts from humans and seven LLMs show that POGER outperforms all baselines in
macro F1 under black-box, partial white-box, and out-of-distribution settings
and maintains lower re-sampling costs than its existing counterparts.</div><div><a href='http://arxiv.org/abs/2402.09199v1'>2402.09199v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16041v2")'>Detecting Machine-Generated Texts by Multi-Population Aware Optimization
  for Maximum Mean Discrepancy</div>
<div id='2402.16041v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T09:44:56Z</div><div>Authors: Shuhai Zhang, Yiliao Song, Jiahao Yang, Yuanqing Li, Bo Han, Mingkui Tan</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) such as ChatGPT have exhibited remarkable
performance in generating human-like texts. However, machine-generated texts
(MGTs) may carry critical risks, such as plagiarism issues, misleading
information, or hallucination issues. Therefore, it is very urgent and
important to detect MGTs in many situations. Unfortunately, it is challenging
to distinguish MGTs and human-written texts because the distributional
discrepancy between them is often very subtle due to the remarkable performance
of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy}
(MMD) to address this issue in the sense that MMD can well identify
distributional discrepancies. However, directly training a detector with MMD
using diverse MGTs will incur a significantly increased variance of MMD since
MGTs may contain \textit{multiple text populations} due to various LLMs. This
will severely impair MMD's ability to measure the difference between two
samples. To tackle this, we propose a novel \textit{multi-population} aware
optimization method for MMD called MMD-MP, which can \textit{avoid variance
increases} and thus improve the stability to measure the distributional
discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and
sentence-based detection, respectively. Extensive experiments on various LLMs,
\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The
source code is available at \url{https://github.com/ZSHsh98/MMD-MP}.</div><div><a href='http://arxiv.org/abs/2402.16041v2'>2402.16041v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14731v1")'>Reversible Jump Attack to Textual Classifiers with Modification
  Reduction</div>
<div id='2403.14731v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T04:54:31Z</div><div>Authors: Mingze Ni, Zhensu Sun, Wei Liu</div><div style='padding-top: 10px; width: 80ex'>Recent studies on adversarial examples expose vulnerabilities of natural
language processing (NLP) models. Existing techniques for generating
adversarial examples are typically driven by deterministic hierarchical rules
that are agnostic to the optimal adversarial examples, a strategy that often
results in adversarial samples with a suboptimal balance between magnitudes of
changes and attack successes. To this end, in this research we propose two
algorithms, Reversible Jump Attack (RJA) and Metropolis-Hasting Modification
Reduction (MMR), to generate highly effective adversarial examples and to
improve the imperceptibility of the examples, respectively. RJA utilizes a
novel randomization mechanism to enlarge the search space and efficiently
adapts to a number of perturbed words for adversarial examples. With these
generated adversarial examples, MMR applies the Metropolis-Hasting sampler to
enhance the imperceptibility of adversarial examples. Extensive experiments
demonstrate that RJA-MMR outperforms current state-of-the-art methods in attack
performance, imperceptibility, fluency and grammar correctness.</div><div><a href='http://arxiv.org/abs/2403.14731v1'>2403.14731v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18792v1")'>MPAT: Building Robust Deep Neural Networks against Textual Adversarial
  Attacks</div>
<div id='2402.18792v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T01:49:18Z</div><div>Authors: Fangyuan Zhang, Huichi Zhou, Shuangjiao Li, Hongtao Wang</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks have been proven to be vulnerable to adversarial
examples and various methods have been proposed to defend against adversarial
attacks for natural language processing tasks. However, previous defense
methods have limitations in maintaining effective defense while ensuring the
performance of the original task. In this paper, we propose a malicious
perturbation based adversarial training method (MPAT) for building robust deep
neural networks against textual adversarial attacks. Specifically, we construct
a multi-level malicious example generation strategy to generate adversarial
examples with malicious perturbations, which are used instead of original
inputs for model training. Additionally, we employ a novel training objective
function to ensure achieving the defense goal without compromising the
performance on the original task. We conduct comprehensive experiments to
evaluate our defense method by attacking five victim models on three benchmark
datasets. The result demonstrates that our method is more effective against
malicious adversarial attacks compared with previous defense methods while
maintaining or further improving the performance on the original task.</div><div><a href='http://arxiv.org/abs/2402.18792v1'>2402.18792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11469v1")'>A Curious Case of Searching for the Correlation between Training Data
  and Adversarial Robustness of Transformer Textual Models</div>
<div id='2402.11469v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T05:58:25Z</div><div>Authors: Cuong Dang, Dung D. Le, Thai Le</div><div style='padding-top: 10px; width: 80ex'>Existing works have shown that fine-tuned textual transformer models achieve
state-of-the-art prediction performances but are also vulnerable to adversarial
text perturbations. Traditional adversarial evaluation is often done
\textit{only after} fine-tuning the models and ignoring the training data. In
this paper, we want to prove that there is also a strong correlation between
training data and model robustness. To this end, we extract 13 different
features representing a wide range of input fine-tuning corpora properties and
use them to predict the adversarial robustness of the fine-tuned models.
Focusing mostly on encoder-only transformer models BERT and RoBERTa with
additional results for BART, ELECTRA and GPT2, we provide diverse evidence to
support our argument. First, empirical analyses show that (a) extracted
features can be used with a lightweight classifier such as Random Forest to
effectively predict the attack success rate and (b) features with the most
influence on the model robustness have a clear correlation with the robustness.
Second, our framework can be used as a fast and effective additional tool for
robustness evaluation since it (a) saves 30x-193x runtime compared to the
traditional technique, (b) is transferable across models, (c) can be used under
adversarial training, and (d) robust to statistical randomness. Our code will
be publicly available.</div><div><a href='http://arxiv.org/abs/2402.11469v1'>2402.11469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00794v1")'>Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large
  Language Models</div>
<div id='2403.00794v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T02:58:12Z</div><div>Authors: Zachary Horvitz, Jingru Chen, Rahul Aditya, Harshvardhan Srivastava, Robert West, Zhou Yu, Kathleen McKeown</div><div style='padding-top: 10px; width: 80ex'>Humor is a fundamental facet of human cognition and interaction. Yet, despite
recent advances in natural language processing, humor detection remains a
challenging task that is complicated by the scarcity of datasets that pair
humorous texts with similar non-humorous counterparts. In our work, we
investigate whether large language models (LLMs), can generate synthetic data
for humor detection via editing texts. We benchmark LLMs on an existing human
dataset and show that current LLMs display an impressive ability to `unfun'
jokes, as judged by humans and as measured on the downstream task of humor
detection. We extend our approach to a code-mixed English-Hindi humor dataset,
where we find that GPT-4's synthetic data is highly rated by bilingual
annotators and provides challenging adversarial examples for humor classifiers.</div><div><a href='http://arxiv.org/abs/2403.00794v1'>2403.00794v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03299v3")'>GUARD: Role-playing to Generate Natural-language Jailbreakings to Test
  Guideline Adherence of Large Language Models</div>
<div id='2402.03299v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:54:43Z</div><div>Authors: Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang</div><div style='padding-top: 10px; width: 80ex'>The discovery of "jailbreaks" to bypass safety filters of Large Language
Models (LLMs) and harmful responses have encouraged the community to implement
safety measures. One major safety measure is to proactively test the LLMs with
jailbreaks prior to the release. Therefore, such testing will require a method
that can generate jailbreaks massively and efficiently. In this paper, we
follow a novel yet intuitive strategy to generate jailbreaks in the style of
the human generation. We propose a role-playing system that assigns four
different roles to the user LLMs to collaborate on new jailbreaks. Furthermore,
we collect existing jailbreaks and split them into different independent
characteristics using clustering frequency and semantic patterns sentence by
sentence. We organize these characteristics into a knowledge graph, making them
more accessible and easier to retrieve. Our system of different roles will
leverage this knowledge graph to generate new jailbreaks, which have proved
effective in inducing LLMs to generate unethical or guideline-violating
responses. In addition, we also pioneer a setting in our system that will
automatically follow the government-issued guidelines to generate jailbreaks to
test whether LLMs follow the guidelines accordingly. We refer to our system as
GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have
empirically validated the effectiveness of GUARD on three cutting-edge
open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a
widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the
realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing
GUARD's versatility and contributing valuable insights for the development of
safer, more reliable LLM-based applications across diverse modalities.</div><div><a href='http://arxiv.org/abs/2402.03299v3'>2402.03299v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09154v1")'>Attacking Large Language Models with Projected Gradient Descent</div>
<div id='2402.09154v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:13:26Z</div><div>Authors: Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Johannes Gasteiger, Stephan Günnemann</div><div style='padding-top: 10px; width: 80ex'>Current LLM alignment methods are readily broken through specifically crafted
adversarial prompts. While crafting adversarial prompts using discrete
optimization is highly effective, such attacks typically use more than 100,000
LLM calls. This high computational cost makes them unsuitable for, e.g.,
quantitative analyses and adversarial training. To remedy this, we revisit
Projected Gradient Descent (PGD) on the continuously relaxed input prompt.
Although previous attempts with ordinary gradient-based attacks largely failed,
we show that carefully controlling the error introduced by the continuous
relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one
order of magnitude faster than state-of-the-art discrete optimization to
achieve the same devastating attack results.</div><div><a href='http://arxiv.org/abs/2402.09154v1'>2402.09154v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09179v2")'>Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
  Customization</div>
<div id='2402.09179v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:47:35Z</div><div>Authors: Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang</div><div style='padding-top: 10px; width: 80ex'>The increasing demand for customized Large Language Models (LLMs) has led to
the development of solutions like GPTs. These solutions facilitate tailored LLM
creation via natural language prompts without coding. However, the
trustworthiness of third-party custom versions of LLMs remains an essential
concern. In this paper, we propose the first instruction backdoor attacks
against applications integrated with untrusted customized LLMs (e.g., GPTs).
Specifically, these attacks embed the backdoor into the custom version of LLMs
by designing prompts with backdoor instructions, outputting the attacker's
desired result when inputs contain the pre-defined triggers. Our attack
includes 3 levels of attacks: word-level, syntax-level, and semantic-level,
which adopt different types of triggers with progressive stealthiness. We
stress that our attacks do not require fine-tuning or any modification to the
backend LLMs, adhering strictly to GPTs development guidelines. We conduct
extensive experiments on 4 prominent LLMs and 5 benchmark text classification
datasets. The results show that our instruction backdoor attacks achieve the
desired attack performance without compromising utility. Additionally, we
propose an instruction-ignoring defense mechanism and demonstrate its partial
effectiveness in mitigating such attacks. Our findings highlight the
vulnerability and the potential risks of LLM customization such as GPTs.</div><div><a href='http://arxiv.org/abs/2402.09179v2'>2402.09179v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02309v1")'>Jailbreaking Attack against Multimodal Large Language Model</div>
<div id='2402.02309v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T01:29:24Z</div><div>Authors: Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, Rong Jin</div><div style='padding-top: 10px; width: 80ex'>This paper focuses on jailbreaking attacks against multi-modal large language
models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to
harmful user queries. A maximum likelihood-based algorithm is proposed to find
an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs
across multiple unseen prompts and images (i.e., data-universal property). Our
approach exhibits strong model-transferability, as the generated imgJP can be
transferred to jailbreak various models, including MiniGPT-v2, LLaVA,
InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a
connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we
introduce a construction-based method to harness our approach for
LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art
methods. The code is available here. \textbf{Warning: some content generated by
language models may be offensive to some readers.}</div><div><a href='http://arxiv.org/abs/2402.02309v1'>2402.02309v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08577v1")'>Test-Time Backdoor Attacks on Multimodal Large Language Models</div>
<div id='2402.08577v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:28:28Z</div><div>Authors: Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin</div><div style='padding-top: 10px; width: 80ex'>Backdoor attacks are commonly executed by contaminating training data, such
that a trigger can activate predetermined harmful effects during the test
phase. In this work, we present AnyDoor, a test-time backdoor attack against
multimodal large language models (MLLMs), which involves injecting the backdoor
into the textual modality using adversarial test images (sharing the same
universal perturbation), without requiring access to or modification of the
training data. AnyDoor employs similar techniques used in universal adversarial
attacks, but distinguishes itself by its ability to decouple the timing of
setup and activation of harmful effects. In our experiments, we validate the
effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4,
InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies.
Notably, because the backdoor is injected by a universal perturbation, AnyDoor
can dynamically change its backdoor trigger prompts/harmful effects, exposing a
new challenge for defending against backdoor attacks. Our project page is
available at https://sail-sg.github.io/AnyDoor/.</div><div><a href='http://arxiv.org/abs/2402.08577v1'>2402.08577v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09611v1")'>Towards Privacy-Aware Sign Language Translation at Scale</div>
<div id='2402.09611v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T22:57:03Z</div><div>Authors: Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, Jean Maillard</div><div style='padding-top: 10px; width: 80ex'>A major impediment to the advancement of sign language translation (SLT) is
data scarcity. Much of the sign language data currently available on the web
cannot be used for training supervised models due to the lack of aligned
captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears
privacy risks due to the presence of biometric information, which the
responsible development of SLT technologies should account for. In this work,
we propose a two-stage framework for privacy-aware SLT at scale that addresses
both of these issues. We introduce SSVP-SLT, which leverages self-supervised
video pretraining on anonymized and unannotated videos, followed by supervised
SLT finetuning on a curated parallel dataset. SSVP-SLT achieves
state-of-the-art finetuned and zero-shot gloss-free SLT performance on the
How2Sign dataset, outperforming the strongest respective baselines by over 3
BLEU-4. Based on controlled experiments, we further discuss the advantages and
limitations of self-supervised pretraining and anonymization via facial
obfuscation for SLT.</div><div><a href='http://arxiv.org/abs/2402.09611v1'>2402.09611v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01109v3")'>Vaccine: Perturbation-aware Alignment for Large Language Model</div>
<div id='2402.01109v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:56:50Z</div><div>Authors: Tiansheng Huang, Sihao Hu, Ling Liu</div><div style='padding-top: 10px; width: 80ex'>The new paradigm of finetuning-as-a-service introduces a new attack surface
for Large Language Models (LLMs): a few harmful data uploaded by users can
easily trick the finetuning to produce an alignment-broken model. We conduct an
empirical analysis and uncover a \textit{harmful embedding drift} phenomenon,
showing a probable cause of the alignment-broken effect. Inspired by our
findings, we propose Vaccine, a perturbation-aware alignment technique to
mitigate the security risk of users finetuning. The core idea of Vaccine is to
produce invariant hidden embeddings by progressively adding crafted
perturbation to them in the alignment phase. This enables the embeddings to
withstand harmful perturbation from un-sanitized user data in the finetuning
phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)
demonstrate that Vaccine can boost the robustness of alignment against harmful
prompts induced embedding drift while reserving reasoning ability towards
benign prompts. Our code is available at
\url{https://github.com/git-disl/Vaccine}.</div><div><a href='http://arxiv.org/abs/2402.01109v3'>2402.01109v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09283v2")'>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</div>
<div id='2402.09283v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T16:14:03Z</div><div>Authors: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.</div><div><a href='http://arxiv.org/abs/2402.09283v2'>2402.09283v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00871v1")'>Teach LLMs to Phish: Stealing Private Information from Language Models</div>
<div id='2403.00871v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T06:15:07Z</div><div>Authors: Ashwinee Panda, Christopher A. Choquette-Choo, Zhengming Zhang, Yaoqing Yang, Prateek Mittal</div><div style='padding-top: 10px; width: 80ex'>When large language models are trained on private data, it can be a
significant privacy risk for them to memorize and regurgitate sensitive
information. In this work, we propose a new practical data extraction attack
that we call "neural phishing". This attack enables an adversary to target and
extract sensitive or personally identifiable information (PII), e.g., credit
card numbers, from a model trained on user data with upwards of 10% attack
success rates, at times, as high as 50%. Our attack assumes only that an
adversary can insert as few as 10s of benign-appearing sentences into the
training dataset using only vague priors on the structure of the user data.</div><div><a href='http://arxiv.org/abs/2403.00871v1'>2403.00871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00793v2")'>SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for
  Large Language Models</div>
<div id='2401.00793v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T15:40:35Z</div><div>Authors: Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu</div><div style='padding-top: 10px; width: 80ex'>With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and difficult to circumvent or optimize effectively. To
address this concern, we introduce an advanced optimization framework called
SecFormer, to achieve fast and accurate PPI for Transformer models. By
implementing model design optimization, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials, Fourier series and Goldschmidt's
method to handle other complex nonlinear functions within PPI, such as GeLU,
LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer
outperforms MPCFormer in performance, showing improvements of $5.6\%$ and
$24.2\%$ for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In
terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, demonstrating its effectiveness
and speed.</div><div><a href='http://arxiv.org/abs/2401.00793v2'>2401.00793v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04960v1")'>SecGPT: An Execution Isolation Architecture for LLM-Based Systems</div>
<div id='2403.04960v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T00:02:30Z</div><div>Authors: Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) extended as systems, such as ChatGPT, have begun
supporting third-party applications. These LLM apps leverage the de facto
natural language-based automated execution paradigm of LLMs: that is, apps and
their interactions are defined in natural language, provided access to user
data, and allowed to freely interact with each other and the system. These LLM
app ecosystems resemble the settings of earlier computing platforms, where
there was insufficient isolation between apps and the system. Because
third-party apps may not be trustworthy, and exacerbated by the imprecision of
the natural language interfaces, the current designs pose security and privacy
risks for users. In this paper, we propose SecGPT, an architecture for
LLM-based systems that aims to mitigate the security and privacy issues that
arise with the execution of third-party apps. SecGPT's key idea is to isolate
the execution of apps and more precisely mediate their interactions outside of
their isolated environments. We evaluate SecGPT against a number of case study
attacks and demonstrate that it protects against many security, privacy, and
safety issues that exist in non-isolated LLM-based systems. The performance
overhead incurred by SecGPT to improve security is under 0.3x for
three-quarters of the tested queries. To foster follow-up research, we release
SecGPT's source code at https://github.com/llm-platform-security/SecGPT.</div><div><a href='http://arxiv.org/abs/2403.04960v1'>2403.04960v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10067v1")'>LLM-based policy generation for intent-based management of applications</div>
<div id='2402.10067v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T15:37:04Z</div><div>Authors: Kristina Dzeparoska, Jieyu Lin, Ali Tizghadam, Alberto Leon-Garcia</div><div style='padding-top: 10px; width: 80ex'>Automated management requires decomposing high-level user requests, such as
intents, to an abstraction that the system can understand and execute. This is
challenging because even a simple intent requires performing a number of
ordered steps. And the task of identifying and adapting these steps (as
conditions change) requires a decomposition approach that cannot be exactly
pre-defined beforehand. To tackle these challenges and support automated intent
decomposition and execution, we explore the few-shot capability of Large
Language Models (LLMs). We propose a pipeline that progressively decomposes
intents by generating the required actions using a policy-based abstraction.
This allows us to automate the policy execution by creating a closed control
loop for the intent deployment. To do so, we generate and map the policies to
APIs and form application management loops that perform the necessary
monitoring, analysis, planning and execution. We evaluate our proposal with a
use-case to fulfill and assure an application service chain of virtual network
functions. Using our approach, we can generalize and generate the necessary
steps to realize intents, thereby enabling intent automation for application
management.</div><div><a href='http://arxiv.org/abs/2402.10067v1'>2402.10067v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12503v1")'>Securing Large Language Models: Threats, Vulnerabilities and Responsible
  Practices</div>
<div id='2403.12503v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T07:10:58Z</div><div>Authors: Sara Abdali, Richard Anarfi, CJ Barberan, Jia He</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have significantly transformed the landscape of
Natural Language Processing (NLP). Their impact extends across a diverse
spectrum of tasks, revolutionizing how we approach language understanding and
generations. Nevertheless, alongside their remarkable utility, LLMs introduce
critical security and risk considerations. These challenges warrant careful
examination to ensure responsible deployment and safeguard against potential
vulnerabilities. This research paper thoroughly investigates security and
privacy concerns related to LLMs from five thematic perspectives: security and
privacy concerns, vulnerabilities against adversarial attacks, potential harms
caused by misuses of LLMs, mitigation strategies to address these challenges
while identifying limitations of current strategies. Lastly, the paper
recommends promising avenues for future research to enhance the security and
risk management of LLMs.</div><div><a href='http://arxiv.org/abs/2403.12503v1'>2403.12503v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14845v1")'>Purifying Large Language Models by Ensembling a Small Language Model</div>
<div id='2402.14845v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:00:39Z</div><div>Authors: Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min Lin</div><div style='padding-top: 10px; width: 80ex'>The emerging success of large language models (LLMs) heavily relies on
collecting abundant training data from external (untrusted) sources. Despite
substantial efforts devoted to data cleaning and curation, well-constructed
LLMs have been reported to suffer from copyright infringement, data poisoning,
and/or privacy violations, which would impede practical deployment of LLMs. In
this study, we propose a simple and easily implementable method for purifying
LLMs from the negative effects caused by uncurated data, namely, through
ensembling LLMs with benign and small language models (SLMs). Aside from
theoretical guarantees, we perform comprehensive experiments to empirically
confirm the efficacy of ensembling LLMs with SLMs, which can effectively
preserve the performance of LLMs while mitigating issues such as copyright
infringement, data poisoning, and privacy violations.</div><div><a href='http://arxiv.org/abs/2402.14845v1'>2402.14845v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13086v1")'>Towards Trustable Language Models: Investigating Information Quality of
  Large Language Models</div>
<div id='2401.13086v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T20:55:49Z</div><div>Authors: Rick Rejeleene, Xiaowei Xu, John Talburt</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLM) are generating information at a rapid pace,
requiring users to increasingly rely and trust the data. Despite remarkable
advances of LLM, Information generated by LLM is not completely trustworthy,
due to challenges in information quality. Specifically, integrity of
Information quality decreases due to unreliable, biased, tokenization during
pre-training of LLM. Moreover, due to decreased information quality issues, has
led towards hallucination, fabricated information. Unreliable information can
lead towards flawed decisions in businesses, which impacts economic activity.
In this work, we introduce novel mathematical information quality evaluation of
LLM, we furthermore analyze and highlight information quality challenges,
scaling laws to systematically scale language models.</div><div><a href='http://arxiv.org/abs/2401.13086v1'>2401.13086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15929v1")'>QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs</div>
<div id='2402.15929v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T23:16:57Z</div><div>Authors: Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have demonstrated impressive performance on
several benchmarks. However, traditional studies do not provide formal
guarantees on the performance of LLMs. In this work, we propose a novel
certification framework for LLM, QuaCer-C, wherein we formally certify the
knowledge-comprehension capabilities of popular LLMs. Our certificates are
quantitative - they consist of high-confidence, tight bounds on the probability
that the target LLM gives the correct answer on any relevant knowledge
comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs
indicate that the knowledge comprehension capability improves with an increase
in the number of parameters and that the Mistral model is less performant than
the rest in this evaluation.</div><div><a href='http://arxiv.org/abs/2402.15929v1'>2402.15929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06513v1")'>ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A
  Case Study</div>
<div id='2401.06513v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T11:27:15Z</div><div>Authors: Hala Abdelkader, Mohamed Abdelrazek, Scott Barnett, Jean-Guy Schneider, Priya Rani, Rajesh Vasa</div><div style='padding-top: 10px; width: 80ex'>Machine learning (ML), especially with the emergence of large language models
(LLMs), has significantly transformed various industries. However, the
transition from ML model prototyping to production use within software systems
presents several challenges. These challenges primarily revolve around ensuring
safety, security, and transparency, subsequently influencing the overall
robustness and trustworthiness of ML models. In this paper, we introduce
ML-On-Rails, a protocol designed to safeguard ML models, establish a
well-defined endpoint interface for different ML tasks, and clear communication
between ML providers and ML consumers (software engineers). ML-On-Rails
enhances the robustness of ML models via incorporating detection capabilities
to identify unique challenges specific to production ML. We evaluated the
ML-On-Rails protocol through a real-world case study of the MoveReminder
application. Through this evaluation, we emphasize the importance of
safeguarding ML models in production.</div><div><a href='http://arxiv.org/abs/2401.06513v1'>2401.06513v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03218v2")'>The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning</div>
<div id='2403.03218v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:59:35Z</div><div>Authors: Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, Dan Hendrycks</div><div style='padding-top: 10px; width: 80ex'>The White House Executive Order on Artificial Intelligence highlights the
risks of large language models (LLMs) empowering malicious actors in developing
biological, cyber, and chemical weapons. To measure these risks of malicious
use, government institutions and major AI labs are developing evaluations for
hazardous capabilities in LLMs. However, current evaluations are private,
preventing further research into mitigating risk. Furthermore, they focus on
only a few, highly specific pathways for malicious use. To fill these gaps, we
publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a
dataset of 4,157 multiple-choice questions that serve as a proxy measurement of
hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP
was developed by a consortium of academics and technical consultants, and was
stringently filtered to eliminate sensitive information prior to public
release. WMDP serves two roles: first, as an evaluation for hazardous knowledge
in LLMs, and second, as a benchmark for unlearning methods to remove such
hazardous knowledge. To guide progress on unlearning, we develop CUT, a
state-of-the-art unlearning method based on controlling model representations.
CUT reduces model performance on WMDP while maintaining general capabilities in
areas such as biology and computer science, suggesting that unlearning may be a
concrete path towards reducing malicious use from LLMs. We release our
benchmark and code publicly at https://wmdp.ai</div><div><a href='http://arxiv.org/abs/2403.03218v2'>2403.03218v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00891v1")'>Large Language Models in Cybersecurity: State-of-the-Art</div>
<div id='2402.00891v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T16:55:25Z</div><div>Authors: Farzad Nourmohammadzadeh Motlagh, Mehrdad Hajizadeh, Mehryar Majd, Pejman Najafi, Feng Cheng, Christoph Meinel</div><div style='padding-top: 10px; width: 80ex'>The rise of Large Language Models (LLMs) has revolutionized our comprehension
of intelligence bringing us closer to Artificial Intelligence. Since their
introduction, researchers have actively explored the applications of LLMs
across diverse fields, significantly elevating capabilities. Cybersecurity,
traditionally resistant to data-driven solutions and slow to embrace machine
learning, stands out as a domain. This study examines the existing literature,
providing a thorough characterization of both defensive and adversarial
applications of LLMs within the realm of cybersecurity. Our review not only
surveys and categorizes the current landscape but also identifies critical
research gaps. By evaluating both offensive and defensive applications, we aim
to provide a holistic understanding of the potential risks and opportunities
associated with LLM-driven cybersecurity.</div><div><a href='http://arxiv.org/abs/2402.00891v1'>2402.00891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15335v1")'>L-AutoDA: Leveraging Large Language Models for Automated Decision-based
  Adversarial Attacks</div>
<div id='2401.15335v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T07:57:20Z</div><div>Authors: Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang</div><div style='padding-top: 10px; width: 80ex'>In the rapidly evolving field of machine learning, adversarial attacks
present a significant challenge to model robustness and security.
Decision-based attacks, which only require feedback on the decision of a model
rather than detailed probabilities or scores, are particularly insidious and
difficult to defend against. This work introduces L-AutoDA (Large Language
Model-based Automated Decision-based Adversarial Attacks), a novel approach
leveraging the generative capabilities of Large Language Models (LLMs) to
automate the design of these attacks. By iteratively interacting with LLMs in
an evolutionary framework, L-AutoDA automatically designs competitive attack
algorithms efficiently without much human effort. We demonstrate the efficacy
of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline
methods in both success rate and computational efficiency. Our findings
underscore the potential of language models as tools for adversarial attack
generation and highlight new avenues for the development of robust AI systems.</div><div><a href='http://arxiv.org/abs/2401.15335v1'>2401.15335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10576v1")'>Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for
  Pretraining on the Cybersecurity Domain</div>
<div id='2403.10576v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T05:35:02Z</div><div>Authors: Eugene Jang, Jian Cui, Dayeon Yim, Youngjin Jin, Jin-Woo Chung, Seungwon Shin, Yongjae Lee</div><div style='padding-top: 10px; width: 80ex'>Cybersecurity information is often technically complex and relayed through
unstructured text, making automation of cyber threat intelligence highly
challenging. For such text domains that involve high levels of expertise,
pretraining on in-domain corpora has been a popular method for language models
to obtain domain expertise. However, cybersecurity texts often contain
non-linguistic elements (such as URLs and hash values) that could be unsuitable
with the established pretraining methodologies. Previous work in other domains
have removed or filtered such text as noise, but the effectiveness of these
methods have not been investigated, especially in the cybersecurity domain. We
propose different pretraining methodologies and evaluate their effectiveness
through downstream tasks and probing tasks. Our proposed strategy (selective
MLM and jointly training NLE token classification) outperforms the commonly
taken approach of replacing non-linguistic elements (NLEs). We use our
domain-customized methodology to train CyBERTuned, a cybersecurity domain
language model that outperforms other cybersecurity PLMs on most tasks.</div><div><a href='http://arxiv.org/abs/2403.10576v1'>2403.10576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09063v1")'>Soft Prompt Threats: Attacking Safety Alignment and Unlearning in
  Open-Source LLMs through the Embedding Space</div>
<div id='2402.09063v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:20:03Z</div><div>Authors: Leo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, Stephan Gunnemann</div><div style='padding-top: 10px; width: 80ex'>Current research in adversarial robustness of LLMs focuses on discrete input
manipulations in the natural language space, which can be directly transferred
to closed-source models. However, this approach neglects the steady progression
of open-source models. As open-source models advance in capability, ensuring
their safety also becomes increasingly imperative. Yet, attacks tailored to
open-source LLMs that exploit full model access remain largely unexplored. We
address this research gap and propose the embedding space attack, which
directly attacks the continuous embedding representation of input tokens. We
find that embedding space attacks circumvent model alignments and trigger
harmful behaviors more efficiently than discrete attacks or model fine-tuning.
Furthermore, we present a novel threat model in the context of unlearning and
show that embedding space attacks can extract supposedly deleted information
from unlearned LLMs across multiple datasets and models. Our findings highlight
embedding space attacks as an important threat model in open-source LLMs.
Trigger Warning: the appendix contains LLM-generated text with violence and
harassment.</div><div><a href='http://arxiv.org/abs/2402.09063v1'>2402.09063v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15127v2")'>Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness</div>
<div id='2401.15127v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T13:15:24Z</div><div>Authors: Samaneh Shafee, Alysson Bessani, Pedro M. Ferreira</div><div style='padding-top: 10px; width: 80ex'>Knowledge sharing about emerging threats is crucial in the rapidly advancing
field of cybersecurity and forms the foundation of Cyber Threat Intelligence
(CTI). In this context, Large Language Models are becoming increasingly
significant in the field of cybersecurity, presenting a wide range of
opportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly,
Stanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary
classification and Named Entity Recognition (NER) tasks performed using Open
Source INTelligence (OSINT). We utilize well-established data collected in
previous research from Twitter to assess the competitiveness of these chatbots
when compared to specialized models trained for those tasks. In binary
classification experiments, Chatbot GPT-4 as a commercial model achieved an
acceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1
score of 0.90. However, concerning cybersecurity entity recognition, all
evaluated chatbots have limitations and are less effective. This study
demonstrates the capability of chatbots for OSINT binary classification and
shows that they require further improvement in NER to effectively replace
specially trained models. Our results shed light on the limitations of the LLM
chatbots when compared to specialized models, and can help researchers improve
chatbots technology with the objective to reduce the required effort to
integrate machine learning in OSINT-based CTI tools.</div><div><a href='http://arxiv.org/abs/2401.15127v2'>2401.15127v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08038v1")'>Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with
  Crowdsourcing and Active Learning</div>
<div id='2401.08038v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T01:27:26Z</div><div>Authors: Wenjun Qiu, David Lie, Lisa Austin</div><div style='padding-top: 10px; width: 80ex'>A significant challenge to training accurate deep learning models on privacy
policies is the cost and difficulty of obtaining a large and comprehensive set
of training data. To address these challenges, we present Calpric , which
combines automatic text selection and segmentation, active learning and the use
of crowdsourced annotators to generate a large, balanced training set for
privacy policies at low cost. Automated text selection and segmentation
simplifies the labeling task, enabling untrained annotators from crowdsourcing
platforms, like Amazon's Mechanical Turk, to be competitive with trained
annotators, such as law students, and also reduces inter-annotator agreement,
which decreases labeling cost. Having reliable labels for training enables the
use of active learning, which uses fewer training samples to efficiently cover
the input space, further reducing cost and improving class and data category
balance in the data set. The combination of these techniques allows Calpric to
produce models that are accurate over a wider range of data categories, and
provide more detailed, fine-grain labels than previous work. Our crowdsourcing
process enables Calpric to attain reliable labeled data at a cost of roughly
$0.92-$1.71 per labeled text segment. Calpric 's training process also
generates a labeled data set of 16K privacy policy text segments across 9 Data
categories with balanced positive and negative samples.</div><div><a href='http://arxiv.org/abs/2401.08038v1'>2401.08038v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09810v1")'>LabelAId: Just-in-time AI Interventions for Improving Human Labeling
  Quality and Domain Knowledge in Crowdsourcing Systems</div>
<div id='2403.09810v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T18:59:10Z</div><div>Authors: Chu Li, Zhihan Zhang, Michael Saugstad, Esteban Safranchik, Minchu Kulkarni, Xiaoyu Huang, Shwetak Patel, Vikram Iyer, Tim Althoff, Jon E. Froehlich</div><div style='padding-top: 10px; width: 80ex'>Crowdsourcing platforms have transformed distributed problem-solving, yet
quality control remains a persistent challenge. Traditional quality control
measures, such as prescreening workers and refining instructions, often focus
solely on optimizing economic output. This paper explores just-in-time AI
interventions to enhance both labeling quality and domain-specific knowledge
among crowdworkers. We introduce LabelAId, an advanced inference model
combining Programmatic Weak Supervision (PWS) with FT-Transformers to infer
label correctness based on user behavior and domain knowledge. Our technical
evaluation shows that our LabelAId pipeline consistently outperforms
state-of-the-art ML baselines, improving mistake inference accuracy by 36.7%
with 50 downstream samples. We then implemented LabelAId into Project Sidewalk,
an open-source crowdsourcing platform for urban accessibility. A
between-subjects study with 34 participants demonstrates that LabelAId
significantly enhances label precision without compromising efficiency while
also increasing labeler confidence. We discuss LabelAId's success factors,
limitations, and its generalizability to other crowdsourced science domains.</div><div><a href='http://arxiv.org/abs/2403.09810v1'>2403.09810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08826v1")'>A Dataset for the Validation of Truth Inference Algorithms Suitable for
  Online Deployment</div>
<div id='2403.08826v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T16:00:41Z</div><div>Authors: Fei Wang, Haoyu Liu, Haoyang Bi, Xiangzhuang Shen, Renyu Zhu, Runze Wu, Minmin Lin, Tangjie Lv, Changjie Fan, Qi Liu, Zhenya Huang, Enhong Chen</div><div style='padding-top: 10px; width: 80ex'>For the purpose of efficient and cost-effective large-scale data labeling,
crowdsourcing is increasingly being utilized. To guarantee the quality of data
labeling, multiple annotations need to be collected for each data sample, and
truth inference algorithms have been developed to accurately infer the true
labels. Despite previous studies having released public datasets to evaluate
the efficacy of truth inference algorithms, these have typically focused on a
single type of crowdsourcing task and neglected the temporal information
associated with workers' annotation activities. These limitations significantly
restrict the practical applicability of these algorithms, particularly in the
context of long-term and online truth inference. In this paper, we introduce a
substantial crowdsourcing annotation dataset collected from a real-world
crowdsourcing platform. This dataset comprises approximately two thousand
workers, one million tasks, and six million annotations. The data was gathered
over a period of approximately six months from various types of tasks, and the
timestamps of each annotation were preserved. We analyze the characteristics of
the dataset from multiple perspectives and evaluate the effectiveness of
several representative truth inference algorithms on this dataset. We
anticipate that this dataset will stimulate future research on tracking
workers' abilities over time in relation to different types of tasks, as well
as enhancing online truth inference.</div><div><a href='http://arxiv.org/abs/2403.08826v1'>2403.08826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15116v1")'>Efficient Online Crowdsourcing with Complex Annotations</div>
<div id='2401.15116v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T22:38:03Z</div><div>Authors: Reshef Meir, Viet-An Nguyen, Xu Chen, Jagdish Ramakrishnan, Udi Weinsberg</div><div style='padding-top: 10px; width: 80ex'>Crowdsourcing platforms use various truth discovery algorithms to aggregate
annotations from multiple labelers. In an online setting, however, the main
challenge is to decide whether to ask for more annotations for each item to
efficiently trade off cost (i.e., the number of annotations) for quality of the
aggregated annotations. In this paper, we propose a novel approach for general
complex annotation (such as bounding boxes and taxonomy paths), that works in
an online crowdsourcing setting. We prove that the expected average similarity
of a labeler is linear in their accuracy \emph{conditional on the reported
label}. This enables us to infer reported label accuracy in a broad range of
scenarios. We conduct extensive evaluations on real-world crowdsourcing data
from Meta and show the effectiveness of our proposed online algorithms in
improving the cost-quality trade-off.</div><div><a href='http://arxiv.org/abs/2401.15116v1'>2401.15116v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16795v1")'>If in a Crowdsourced Data Annotation Pipeline, a GPT-4</div>
<div id='2402.16795v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:08:52Z</div><div>Authors: Zeyu He, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Shaurya Rohatgi, Ting-Hao 'Kenneth' Huang</div><div style='padding-top: 10px; width: 80ex'>Recent studies indicated GPT-4 outperforms online crowd workers in data
labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).
However, these studies were criticized for deviating from standard
crowdsourcing practices and emphasizing individual workers' performances over
the whole data-annotation process. This paper compared GPT-4 and an ethical and
well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments
from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces
yielded 127,080 labels, which were then used to infer the final labels through
eight label-aggregation algorithms. Our evaluation showed that despite best
practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved
83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected
via an advanced worker interface for aggregation, 2 out of the 8 algorithms
achieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested
that, when the crowd's and GPT-4's labeling strengths are complementary,
aggregating them could increase labeling accuracy.</div><div><a href='http://arxiv.org/abs/2402.16795v1'>2402.16795v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13239v2")'>Adaptive Crowdsourcing Via Self-Supervised Learning</div>
<div id='2401.13239v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T05:57:36Z</div><div>Authors: Anmol Kagrecha, Henrik Marklund, Benjamin Van Roy, Hong Jun Jeon, Richard Zeckhauser</div><div style='padding-top: 10px; width: 80ex'>Common crowdsourcing systems average estimates of a latent quantity of
interest provided by many crowdworkers to produce a group estimate. We develop
a new approach -- predict-each-worker -- that leverages self-supervised
learning and a novel aggregation scheme. This approach adapts weights assigned
to crowdworkers based on estimates they provided for previous quantities. When
skills vary across crowdworkers or their estimates correlate, the weighted sum
offers a more accurate group estimate than the average. Existing algorithms
such as expectation maximization can, at least in principle, produce similarly
accurate group estimates. However, their computational requirements become
onerous when complex models, such as neural networks, are required to express
relationships among crowdworkers. Predict-each-worker accommodates such
complexity as well as many other practical challenges. We analyze the efficacy
of predict-each-worker through theoretical and computational studies. Among
other things, we establish asymptotic optimality as the number of engagements
per crowdworker grows.</div><div><a href='http://arxiv.org/abs/2401.13239v2'>2401.13239v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.11006v1")'>Automated Detection and Analysis of Data Practices Using A Real-World
  Corpus</div>
<div id='2402.11006v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:51:40Z</div><div>Authors: Mukund Srinath, Pranav Venkit, Maria Badillo, Florian Schaub, C. Lee Giles, Shomir Wilson</div><div style='padding-top: 10px; width: 80ex'>Privacy policies are crucial for informing users about data practices, yet
their length and complexity often deter users from reading them. In this paper,
we propose an automated approach to identify and visualize data practices
within privacy policies at different levels of detail. Leveraging crowd-sourced
annotations from the ToS;DR platform, we experiment with various methods to
match policy excerpts with predefined data practice descriptions. We further
conduct a case study to evaluate our approach on a real-world policy,
demonstrating its effectiveness in simplifying complex policies. Experiments
show that our approach accurately matches data practice descriptions with
policy excerpts, facilitating the presentation of simplified privacy
information to users.</div><div><a href='http://arxiv.org/abs/2402.11006v1'>2402.11006v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12108v1")'>On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using
  Streaming Data</div>
<div id='2401.12108v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:45:15Z</div><div>Authors: Jeremias Dötterl, Ralf Bruns, Jürgen Dunkel, Sascha Ossowski</div><div style='padding-top: 10px; width: 80ex'>In parcel delivery, the "last mile" from the parcel hub to the customer is
costly, especially for time-sensitive delivery tasks that have to be completed
within hours after arrival. Recently, crowdshipping has attracted increased
attention as a new alternative to traditional delivery modes. In crowdshipping,
private citizens ("the crowd") perform short detours in their daily lives to
contribute to parcel delivery in exchange for small incentives. However,
achieving desirable crowd behavior is challenging as the crowd is highly
dynamic and consists of autonomous, self-interested individuals. Leveraging
crowdshipping for time-sensitive deliveries remains an open challenge. In this
paper, we present an agent-based approach to on-time parcel delivery with
crowds. Our system performs data stream processing on the couriers' smartphone
sensor data to predict delivery delays. Whenever a delay is predicted, the
system attempts to forge an agreement for transferring the parcel from the
current deliverer to a more promising courier nearby. Our experiments show that
through accurate delay predictions and purposeful task transfers many delays
can be prevented that would occur without our approach.</div><div><a href='http://arxiv.org/abs/2401.12108v1'>2401.12108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12866v1")'>Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported
  Coordination of Mobile Crowdsourcing</div>
<div id='2401.12866v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T16:00:45Z</div><div>Authors: Ralf Bruns, Jeremias Dötterl, Jürgen Dunkel, Sascha Ossowski</div><div style='padding-top: 10px; width: 80ex'>Mobile crowdsourcing refers to systems where the completion of tasks
necessarily requires physical movement of crowdworkers in an on-demand
workforce. Evidence suggests that in such systems, tasks often get assigned to
crowdworkers who struggle to complete those tasks successfully, resulting in
high failure rates and low service quality. A promising solution to ensure
higher quality of service is to continuously adapt the assignment and respond
to failure-causing events by transferring tasks to better-suited workers who
use different routes or vehicles. However, implementing task transfers in
mobile crowdsourcing is difficult because workers are autonomous and may reject
transfer requests. Moreover, task outcomes are uncertain and need to be
predicted. In this paper, we propose different mechanisms to achieve outcome
prediction and task coordination in mobile crowdsourcing. First, we analyze
different data stream learning approaches for the prediction of task outcomes.
Second, based on the suggested prediction model, we propose and evaluate two
different approaches for task coordination with different degrees of autonomy:
an opportunistic approach for crowdshipping with collaborative, but
non-autonomous workers, and a market-based model with autonomous workers for
crowdsensing.</div><div><a href='http://arxiv.org/abs/2401.12866v1'>2401.12866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13414v1")'>Harnessing Large Language Models as Post-hoc Correctors</div>
<div id='2402.13414v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T22:50:41Z</div><div>Authors: Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin</div><div style='padding-top: 10px; width: 80ex'>As Machine Learning (ML) models grow in size and demand higher-quality
training data, the expenses associated with re-training and fine-tuning these
models are escalating rapidly. Inspired by recent impressive achievements of
Large Language Models (LLMs) in different fields, this paper delves into the
question: can LLMs efficiently improve an ML's performance at a minimal cost?
We show that, through our proposed training-free framework LlmCorr, an LLM can
work as a post-hoc corrector to propose corrections for the predictions of an
arbitrary ML model. In particular, we form a contextual knowledge database by
incorporating the dataset's label information and the ML model's predictions on
the validation dataset. Leveraging the in-context learning capability of LLMs,
we ask the LLM to summarise the instances in which the ML model makes mistakes
and the correlation between primary predictions and true labels. Following
this, the LLM can transfer its acquired knowledge to suggest corrections for
the ML model's predictions. Our experimental results on the challenging
molecular predictions show that LlmCorr improves the performance of a number of
models by up to 39%.</div><div><a href='http://arxiv.org/abs/2402.13414v1'>2402.13414v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12189v1")'>Amplifying Training Data Exposure through Fine-Tuning with
  Pseudo-Labeled Memberships</div>
<div id='2402.12189v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:52:50Z</div><div>Authors: Myung Gyo Oh, Hong Eun Ahn, Leo Hyun Park, Taekyoung Kwon</div><div style='padding-top: 10px; width: 80ex'>Neural language models (LMs) are vulnerable to training data extraction
attacks due to data memorization. This paper introduces a novel attack scenario
wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the
exposure of the original training data. This strategy differs from prior
studies by aiming to intensify the LM's retention of its pre-training dataset.
To achieve this, the attacker needs to collect generated texts that are closely
aligned with the pre-training data. However, without knowledge of the actual
dataset, quantifying the amount of pre-training data within generated texts is
challenging. To address this, we propose the use of pseudo-labels for these
generated texts, leveraging membership approximations indicated by
machine-generated probabilities from the target LM. We subsequently fine-tune
the LM to favor generations with higher likelihoods of originating from the
pre-training data, based on their membership probabilities. Our empirical
findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a
four to eight-fold increase in training data exposure. We discuss potential
mitigations and suggest future research directions.</div><div><a href='http://arxiv.org/abs/2402.12189v1'>2402.12189v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01093v1")'>Specialized Language Models with Cheap Inference from Limited Domain
  Data</div>
<div id='2402.01093v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T01:45:18Z</div><div>Authors: David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun</div><div style='padding-top: 10px; width: 80ex'>Large language models have emerged as a versatile tool but are challenging to
apply to tasks lacking large inference budgets and large in-domain training
sets. This work formalizes these constraints and distinguishes four important
variables: the pretraining budget (for training before the target domain is
known), the specialization budget (for training after the target domain is
known), the inference budget, and the in-domain training set size. Across these
settings, we compare different approaches from the machine learning literature.
Limited by inference cost, we find better alternatives to the standard practice
of training very large vanilla transformer models. In particular, we show that
hyper-networks and mixture of experts have better perplexity for large
pretraining budgets, while small models trained on importance sampled datasets
are attractive for large specialization budgets.</div><div><a href='http://arxiv.org/abs/2402.01093v1'>2402.01093v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06751v1")'>The Unreasonable Effectiveness of Easy Training Data for Hard Tasks</div>
<div id='2401.06751v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:36:29Z</div><div>Authors: Peter Hase, Mohit Bansal, Peter Clark, Sarah Wiegreffe</div><div style='padding-top: 10px; width: 80ex'>How can we train models to perform well on hard test data when hard training
data is by definition difficult to label correctly? This question has been
termed the scalable oversight problem and has drawn increasing attention as
language models have continually improved. In this paper, we present the
surprising conclusion that current language models often generalize relatively
well from easy to hard data, even performing as well as "oracle" models trained
on hard data. We demonstrate this kind of easy-to-hard generalization using
simple training methods like in-context learning, linear classifier heads, and
QLoRA for seven different measures of datapoint hardness, including six
empirically diverse human hardness measures (like grade level) and one
model-based measure (loss-based). Furthermore, we show that even if one cares
most about model performance on hard data, it can be better to collect and
train on easy data rather than hard data, since hard data is generally noisier
and costlier to collect. Our experiments use open models up to 70b in size and
four publicly available question-answering datasets with questions ranging in
difficulty from 3rd grade science questions to college level STEM questions and
general-knowledge trivia. We conclude that easy-to-hard generalization in LMs
is surprisingly strong for the tasks studied, suggesting the scalable oversight
problem may be easier than previously thought. Our code is available at
https://github.com/allenai/easy-to-hard-generalization</div><div><a href='http://arxiv.org/abs/2401.06751v1'>2401.06751v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07028v1")'>Semi-Supervised Learning for Bilingual Lexicon Induction</div>
<div id='2402.07028v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T19:27:22Z</div><div>Authors: Paul Garnier, Gauthier Guinet</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of aligning two sets of continuous word
representations, corresponding to languages, to a common space in order to
infer a bilingual lexicon. It was recently shown that it is possible to infer
such lexicon, without using any parallel data, by aligning word embeddings
trained on monolingual data. Such line of work is called unsupervised bilingual
induction. By wondering whether it was possible to gain experience in the
progressive learning of several languages, we asked ourselves to what extent we
could integrate the knowledge of a given set of languages when learning a new
one, without having parallel data for the latter. In other words, while keeping
the core problem of unsupervised learning in the latest step, we allowed the
access to other corpora of idioms, hence the name semi-supervised. This led us
to propose a novel formulation, considering the lexicon induction as a ranking
problem for which we used recent tools of this machine learning field. Our
experiments on standard benchmarks, inferring dictionary from English to more
than 20 languages, show that our approach consistently outperforms existing
state of the art benchmark. In addition, we deduce from this new scenario
several relevant conclusions allowing a better understanding of the alignment
phenomenon.</div><div><a href='http://arxiv.org/abs/2402.07028v1'>2402.07028v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02926v1")'>Automated Cognate Detection as a Supervised Link Prediction Task with
  Cognate Transformer</div>
<div id='2402.02926v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:47:36Z</div><div>Authors: V. S. D. S. Mahesh Akavarapu, Arnab Bhattacharya</div><div style='padding-top: 10px; width: 80ex'>Identification of cognates across related languages is one of the primary
problems in historical linguistics. Automated cognate identification is helpful
for several downstream tasks including identifying sound correspondences,
proto-language reconstruction, phylogenetic classification, etc. Previous
state-of-the-art methods for cognate identification are mostly based on
distributions of phonemes computed across multilingual wordlists and make
little use of the cognacy labels that define links among cognate clusters. In
this paper, we present a transformer-based architecture inspired by
computational biology for the task of automated cognate detection. Beyond a
certain amount of supervision, this method performs better than the existing
methods, and shows steady improvement with further increase in supervision,
thereby proving the efficacy of utilizing the labeled information. We also
demonstrate that accepting multiple sequence alignments as input and having an
end-to-end architecture with link prediction head saves much computation time
while simultaneously yielding superior performance.</div><div><a href='http://arxiv.org/abs/2402.02926v1'>2402.02926v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12513v1")'>Induced Model Matching: How Restricted Models Can Help Larger Ones</div>
<div id='2402.12513v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:21:09Z</div><div>Authors: Usama Muneeb, Mesrob I. Ohannessian</div><div style='padding-top: 10px; width: 80ex'>We consider scenarios where a very accurate predictive model using restricted
features is available at the time of training of a larger, full-featured,
model. This restricted model may be thought of as "side-information", derived
either from an auxiliary exhaustive dataset or on the same dataset, by forcing
the restriction. How can the restricted model be useful to the full model? We
propose an approach for transferring the knowledge of the restricted model to
the full model, by aligning the full model's context-restricted performance
with that of the restricted model's. We call this methodology Induced Model
Matching (IMM) and first illustrate its general applicability by using logistic
regression as a toy example. We then explore IMM's use in language modeling,
the application that initially inspired it, and where it offers an explicit
foundation in contrast to the implicit use of restricted models in techniques
such as noising. We demonstrate the methodology on both LSTM and transformer
full models, using $N$-grams as restricted models. To further illustrate the
potential of the principle whenever it is much cheaper to collect restricted
rather than full information, we conclude with a simple RL example where POMDP
policies can improve learned MDP policies via IMM.</div><div><a href='http://arxiv.org/abs/2402.12513v1'>2402.12513v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13229v1")'>From Random to Informed Data Selection: A Diversity-Based Approach to
  Optimize Human Annotation and Few-Shot Learning</div>
<div id='2401.13229v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T04:57:32Z</div><div>Authors: Alexandre Alcoforado, Thomas Palmeira Ferraz, Lucas Hideki Okamura, Israel Campos Fama, Arnold Moya Lavado, Bárbara Dias Bueno, Bruno Veloso, Anna Helena Reali Costa</div><div style='padding-top: 10px; width: 80ex'>A major challenge in Natural Language Processing is obtaining annotated data
for supervised learning. An option is the use of crowdsourcing platforms for
data annotation. However, crowdsourcing introduces issues related to the
annotator's experience, consistency, and biases. An alternative is to use
zero-shot methods, which in turn have limitations compared to their few-shot or
fully supervised counterparts. Recent advancements driven by large language
models show potential, but struggle to adapt to specialized domains with
severely limited data. The most common approaches therefore involve the human
itself randomly annotating a set of datapoints to build initial datasets. But
randomly sampling data to be annotated is often inefficient as it ignores the
characteristics of the data and the specific needs of the model. The situation
worsens when working with imbalanced datasets, as random sampling tends to
heavily bias towards the majority classes, leading to excessive annotated data.
To address these issues, this paper contributes an automatic and informed data
selection architecture to build a small dataset for few-shot learning. Our
proposal minimizes the quantity and maximizes diversity of data selected for
human annotation, while improving model performance.</div><div><a href='http://arxiv.org/abs/2401.13229v1'>2401.13229v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13468v1")'>STENCIL: Submodular Mutual Information Based Weak Supervision for
  Cold-Start Active Learning</div>
<div id='2402.13468v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T01:54:58Z</div><div>Authors: Nathan Beck, Adithya Iyer, Rishabh Iyer</div><div style='padding-top: 10px; width: 80ex'>As supervised fine-tuning of pre-trained models within NLP applications
increases in popularity, larger corpora of annotated data are required,
especially with increasing parameter counts in large language models. Active
learning, which attempts to mine and annotate unlabeled instances to improve
model performance maximally fast, is a common choice for reducing the
annotation cost; however, most methods typically ignore class imbalance and
either assume access to initial annotated data or require multiple rounds of
active learning selection before improving rare classes. We present STENCIL,
which utilizes a set of text exemplars and the recently proposed submodular
mutual information to select a set of weakly labeled rare-class instances that
are then strongly labeled by an annotator. We show that STENCIL improves
overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on
multiple text classification datasets over common active learning methods
within the class-imbalanced cold-start setting.</div><div><a href='http://arxiv.org/abs/2402.13468v1'>2402.13468v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07639v1")'>Compute-Efficient Active Learning</div>
<div id='2401.07639v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T12:32:07Z</div><div>Authors: Gábor Németh, Tamás Matuszka</div><div style='padding-top: 10px; width: 80ex'>Active learning, a powerful paradigm in machine learning, aims at reducing
labeling costs by selecting the most informative samples from an unlabeled
dataset. However, the traditional active learning process often demands
extensive computational resources, hindering scalability and efficiency. In
this paper, we address this critical issue by presenting a novel method
designed to alleviate the computational burden associated with active learning
on massive datasets. To achieve this goal, we introduce a simple, yet effective
method-agnostic framework that outlines how to strategically choose and
annotate data points, optimizing the process for efficiency while maintaining
model performance. Through case studies, we demonstrate the effectiveness of
our proposed method in reducing computational costs while maintaining or, in
some cases, even surpassing baseline model outcomes. Code is available at
https://github.com/aimotive/Compute-Efficient-Active-Learning.</div><div><a href='http://arxiv.org/abs/2401.07639v1'>2401.07639v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01101v1")'>Feature Alignment: Rethinking Efficient Active Learning via Proxy in the
  Context of Pre-trained Models</div>
<div id='2403.01101v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T06:01:34Z</div><div>Authors: Ziting Wen, Oscar Pizarro, Stefan Williams</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning the pre-trained model with active learning holds promise for
reducing annotation costs. However, this combination introduces significant
computational costs, particularly with the growing scale of pre-trained models.
Recent research has proposed proxy-based active learning, which pre-computes
features to reduce computational costs. Yet, this approach often incurs a
significant loss in active learning performance, which may even outweigh the
computational cost savings. In this paper, we argue the performance drop stems
not only from pre-computed features' inability to distinguish between
categories of labeled samples, resulting in the selection of redundant samples
but also from the tendency to compromise valuable pre-trained information when
fine-tuning with samples selected through the proxy model. To address this
issue, we propose a novel method called aligned selection via proxy to update
pre-computed features while selecting a proper training method to inherit
valuable pre-training information. Extensive experiments validate that our
method significantly improves the total cost of efficient active learning while
maintaining computational efficiency.</div><div><a href='http://arxiv.org/abs/2403.01101v1'>2403.01101v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03728v1")'>Bridging Diversity and Uncertainty in Active learning with
  Self-Supervised Pre-Training</div>
<div id='2403.03728v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T14:18:24Z</div><div>Authors: Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer</div><div style='padding-top: 10px; width: 80ex'>This study addresses the integration of diversity-based and uncertainty-based
sampling strategies in active learning, particularly within the context of
self-supervised pre-trained models. We introduce a straightforward heuristic
called TCM that mitigates the cold start problem while maintaining strong
performance across various data levels. By initially applying TypiClust for
diversity sampling and subsequently transitioning to uncertainty sampling with
Margin, our approach effectively combines the strengths of both strategies. Our
experiments demonstrate that TCM consistently outperforms existing methods
across various datasets in both low and high data regimes.</div><div><a href='http://arxiv.org/abs/2403.03728v1'>2403.03728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09066v1")'>Hyperparameters in Continual Learning: a Reality Check</div>
<div id='2403.09066v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T03:13:01Z</div><div>Authors: Sungmin Cha, Kyunghyun Cho</div><div style='padding-top: 10px; width: 80ex'>Various algorithms for continual learning (CL) have been designed with the
goal of effectively alleviating the trade-off between stability and plasticity
during the CL process. To achieve this goal, tuning appropriate hyperparameters
for each algorithm is essential. As an evaluation protocol, it has been common
practice to train a CL algorithm using diverse hyperparameter values on a CL
scenario constructed with a benchmark dataset. Subsequently, the best
performance attained with the optimal hyperparameter value serves as the
criterion for evaluating the CL algorithm. In this paper, we contend that this
evaluation protocol is not only impractical but also incapable of effectively
assessing the CL capability of a CL algorithm. Returning to the fundamental
principles of model evaluation in machine learning, we propose an evaluation
protocol that involves Hyperparameter Tuning and Evaluation phases. Those
phases consist of different datasets but share the same CL scenario. In the
Hyperparameter Tuning phase, each algorithm is iteratively trained with
different hyperparameter values to find the optimal hyperparameter values.
Subsequently, in the Evaluation phase, the optimal hyperparameter values is
directly applied for training each algorithm, and their performance in the
Evaluation phase serves as the criterion for evaluating them. Through
experiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in
class-incremental learning, we not only observed that the existing evaluation
method fail to properly assess the CL capability of each algorithm but also
observe that some recently proposed state-of-the-art algorithms, which reported
superior performance, actually exhibit inferior performance compared to the
previous algorithm.</div><div><a href='http://arxiv.org/abs/2403.09066v1'>2403.09066v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05532v1")'>Tune without Validation: Searching for Learning Rate and Weight Decay on
  Training Sets</div>
<div id='2403.05532v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T18:57:00Z</div><div>Authors: Lorenzo Brigato, Stavroula Mougiakakou</div><div style='padding-top: 10px; width: 80ex'>We introduce Tune without Validation (Twin), a pipeline for tuning learning
rate and weight decay without validation sets. We leverage a recent theoretical
framework concerning learning phases in hypothesis space to devise a heuristic
that predicts what hyper-parameter (HP) combinations yield better
generalization. Twin performs a grid search of trials according to an
early-/non-early-stopping scheduler and then segments the region that provides
the best results in terms of training loss. Among these trials, the weight norm
strongly correlates with predicting generalization. To assess the effectiveness
of Twin, we run extensive experiments on 20 image classification datasets and
train several families of deep networks, including convolutional, transformer,
and feed-forward models. We demonstrate proper HP selection when training from
scratch and fine-tuning, emphasizing small-sample scenarios.</div><div><a href='http://arxiv.org/abs/2403.05532v1'>2403.05532v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00946v1")'>Fine-tuning with Very Large Dropout</div>
<div id='2403.00946v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T19:50:22Z</div><div>Authors: Jianyu Zhang, Léon Bottou</div><div style='padding-top: 10px; width: 80ex'>It is impossible today to pretend that the practice of machine learning is
compatible with the idea that training and testing data follow the same
distribution. Several authors have recently used ensemble techniques to show
how scenarios involving multiple data distributions are best served by
representations that are both richer than those obtained by regularizing for
the best in-distribution performance, and richer than those obtained under the
influence of the implicit sparsity bias of common stochastic gradient
procedures.
  This contribution investigates the use of very high dropout rates instead of
ensembles to obtain such rich representations. Although training a deep network
from scratch using such dropout rates is virtually impossible, fine-tuning a
large pre-trained model under such conditions is not only possible but also
achieves out-of-distribution performances that exceed those of both ensembles
and weight averaging methods such as model soups. This result has practical
significance because the importance of the fine-tuning scenario has
considerably grown in recent years. This result also provides interesting
insights on the nature of rich representations and on the intrinsically linear
nature of fine-tuning a large network using a comparatively small dataset.</div><div><a href='http://arxiv.org/abs/2403.00946v1'>2403.00946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13204v1")'>Diversity-Aware Agnostic Ensemble of Sharpness Minimizers</div>
<div id='2403.13204v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T23:50:11Z</div><div>Authors: Anh Bui, Vy Vo, Tung Pham, Dinh Phung, Trung Le</div><div style='padding-top: 10px; width: 80ex'>There has long been plenty of theoretical and empirical evidence supporting
the success of ensemble learning. Deep ensembles in particular take advantage
of training randomness and expressivity of individual neural networks to gain
prediction diversity, ultimately leading to better generalization, robustness
and uncertainty estimation. In respect of generalization, it is found that
pursuing wider local minima result in models being more robust to shifts
between training and testing sets. A natural research question arises out of
these two approaches as to whether a boost in generalization ability can be
achieved if ensemble learning and loss sharpness minimization are integrated.
Our work investigates this connection and proposes DASH - a learning algorithm
that promotes diversity and flatness within deep ensembles. More concretely,
DASH encourages base learners to move divergently towards low-loss regions of
minimal sharpness. We provide a theoretical backbone for our method along with
extensive empirical evidence demonstrating an improvement in ensemble
generalizability.</div><div><a href='http://arxiv.org/abs/2403.13204v1'>2403.13204v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00910v2")'>Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning</div>
<div id='2402.00910v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T09:24:36Z</div><div>Authors: Ahmed Radwan, Layan Zaafarani, Jetana Abudawood, Faisal AlZahrani, Fares Fourati</div><div style='padding-top: 10px; width: 80ex'>Addressing biases in AI models is crucial for ensuring fair and accurate
predictions. However, obtaining large, unbiased datasets for training can be
challenging. This paper proposes a comprehensive approach using multiple
methods to remove bias in AI models, with only a small dataset and a
potentially biased pretrained model. We train multiple models with the
counter-bias of the pre-trained model through data splitting, local training,
and regularized fine-tuning, gaining potentially counter-biased models. Then,
we employ ensemble learning for all models to reach unbiased predictions. To
further accelerate the inference time of our ensemble model, we conclude our
solution with knowledge distillation that results in a single unbiased neural
network. We demonstrate the effectiveness of our approach through experiments
on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work
contributes to the ongoing effort to create more unbiased and reliable AI
models, even with limited data availability.</div><div><a href='http://arxiv.org/abs/2402.00910v2'>2402.00910v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10097v1")'>Adaptive Random Feature Regularization on Fine-tuning Deep Neural
  Networks</div>
<div id='2403.10097v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T08:26:59Z</div><div>Authors: Shin'ya Yamaguchi, Sekitoshi Kanai, Kazuki Adachi, Daiki Chijiwa</div><div style='padding-top: 10px; width: 80ex'>While fine-tuning is a de facto standard method for training deep neural
networks, it still suffers from overfitting when using small target datasets.
Previous methods improve fine-tuning performance by maintaining knowledge of
the source datasets or introducing regularization terms such as contrastive
loss. However, these methods require auxiliary source information (e.g., source
labels or datasets) or heavy additional computations. In this paper, we propose
a simple method called adaptive random feature regularization (AdaRand).
AdaRand helps the feature extractors of training models to adaptively change
the distribution of feature vectors for downstream classification tasks without
auxiliary source information and with reasonable computation costs. To this
end, AdaRand minimizes the gap between feature vectors and random reference
vectors that are sampled from class conditional Gaussian distributions.
Furthermore, AdaRand dynamically updates the conditional distribution to follow
the currently updated feature extractors and balance the distance between
classes in feature spaces. Our experiments show that AdaRand outperforms the
other fine-tuning regularization, which requires auxiliary source information
and heavy computation costs.</div><div><a href='http://arxiv.org/abs/2403.10097v1'>2403.10097v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03187v1")'>How Good is a Single Basin?</div>
<div id='2402.03187v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:51:59Z</div><div>Authors: Kai Lion, Lorenzo Noci, Thomas Hofmann, Gregor Bachmann</div><div style='padding-top: 10px; width: 80ex'>The multi-modal nature of neural loss landscapes is often considered to be
the main driver behind the empirical success of deep ensembles. In this work,
we probe this belief by constructing various "connected" ensembles which are
restricted to lie in the same basin. Through our experiments, we demonstrate
that increased connectivity indeed negatively impacts performance. However,
when incorporating the knowledge from other basins implicitly through
distillation, we show that the gap in performance can be mitigated by
re-discovering (multi-basin) deep ensembles within a single basin. Thus, we
conjecture that while the extra-basin knowledge is at least partially present
in any given basin, it cannot be easily harnessed without learning it from
other basins.</div><div><a href='http://arxiv.org/abs/2402.03187v1'>2402.03187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07052v1")'>Understanding the Training Speedup from Sampling with Approximate Losses</div>
<div id='2402.07052v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T21:51:59Z</div><div>Authors: Rudrajit Das, Xi Chen, Bertram Ieong, Parikshit Bansal, Sujay Sanghavi</div><div style='padding-top: 10px; width: 80ex'>It is well known that selecting samples with large losses/gradients can
significantly reduce the number of training steps. However, the selection
overhead is often too high to yield any meaningful gains in terms of overall
training time. In this work, we focus on the greedy approach of selecting
samples with large \textit{approximate losses} instead of exact losses in order
to reduce the selection overhead. For smooth convex losses, we show that such a
greedy strategy can converge to a constant factor of the minimum value of the
average loss in fewer iterations than the standard approach of random
selection. We also theoretically quantify the effect of the approximation
level. We then develop SIFT which uses early exiting to obtain approximate
losses with an intermediate layer's representations for sample selection. We
evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model
and show significant gains (in terms of training hours and number of
backpropagation steps) without any optimized implementation over vanilla
training. For e.g., to reach 64% validation accuracy, SIFT with exit at the
first layer takes ~43 hours compared to ~57 hours of vanilla training.</div><div><a href='http://arxiv.org/abs/2402.07052v1'>2402.07052v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08384v1")'>Selective Learning: Towards Robust Calibration with Dynamic
  Regularization</div>
<div id='2402.08384v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T11:25:20Z</div><div>Authors: Zongbo Han, Yifeng Yang, Changqing Zhang, Linjun Zhang, Joey Tianyi Zhou, Qinghua Hu, Huaxiu Yao</div><div style='padding-top: 10px; width: 80ex'>Miscalibration in deep learning refers to there is a discrepancy between the
predicted confidence and performance. This problem usually arises due to the
overfitting problem, which is characterized by learning everything presented in
the training set, resulting in overconfident predictions during testing.
Existing methods typically address overfitting and mitigate the miscalibration
by adding a maximum-entropy regularizer to the objective function. The
objective can be understood as seeking a model that fits the ground-truth
labels by increasing the confidence while also maximizing the entropy of
predicted probabilities by decreasing the confidence. However, previous methods
lack clear guidance on confidence adjustment, leading to conflicting objectives
(increasing but also decreasing confidence). Therefore, we introduce a method
called Dynamic Regularization (DReg), which aims to learn what should be
learned during training thereby circumventing the confidence adjusting
trade-off. At a high level, DReg aims to obtain a more reliable model capable
of acknowledging what it knows and does not know. Specifically, DReg
effectively fits the labels for in-distribution samples (samples that should be
learned) while applying regularization dynamically to samples beyond model
capabilities (e.g., outliers), thereby obtaining a robust calibrated model
especially on the samples beyond model capabilities. Both theoretical and
empirical analyses sufficiently demonstrate the superiority of DReg compared
with previous methods.</div><div><a href='http://arxiv.org/abs/2402.08384v1'>2402.08384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13765v1")'>Accuracy-Preserving Calibration via Statistical Modeling on Probability
  Simplex</div>
<div id='2402.13765v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:39:20Z</div><div>Authors: Yasushi Esaki, Akihiro Nakamura, Keisuke Kawano, Ryoko Tokuhisa, Takuro Kutsuna</div><div style='padding-top: 10px; width: 80ex'>Classification models based on deep neural networks (DNNs) must be calibrated
to measure the reliability of predictions. Some recent calibration methods have
employed a probabilistic model on the probability simplex. However, these
calibration methods cannot preserve the accuracy of pre-trained models, even
those with a high classification accuracy. We propose an accuracy-preserving
calibration method using the Concrete distribution as the probabilistic model
on the probability simplex. We theoretically prove that a DNN model trained on
cross-entropy loss has optimality as the parameter of the Concrete
distribution. We also propose an efficient method that synthetically generates
samples for training probabilistic models on the probability simplex. We
demonstrate that the proposed method can outperform previous methods in
accuracy-preserving calibration tasks using benchmarks.</div><div><a href='http://arxiv.org/abs/2402.13765v1'>2402.13765v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08703v2")'>Decoupled Prototype Learning for Reliable Test-Time Adaptation</div>
<div id='2401.08703v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T03:33:39Z</div><div>Authors: Guowei Wang, Changxing Ding, Wentao Tan, Mingkui Tan</div><div style='padding-top: 10px; width: 80ex'>Test-time adaptation (TTA) is a task that continually adapts a pre-trained
source model to the target domain during inference. One popular approach
involves fine-tuning model with cross-entropy loss according to estimated
pseudo-labels. However, its performance is significantly affected by noisy
pseudo-labels. This study reveals that minimizing the classification error of
each sample causes the cross-entropy loss's vulnerability to label noise. To
address this issue, we propose a novel Decoupled Prototype Learning (DPL)
method that features prototype-centric loss computation. First, we decouple the
optimization of class prototypes. For each class prototype, we reduce its
distance with positive samples and enlarge its distance with negative samples
in a contrastive manner. This strategy prevents the model from overfitting to
noisy pseudo-labels. Second, we propose a memory-based strategy to enhance
DPL's robustness for the small batch sizes often encountered in TTA. We update
each class's pseudo-feature from a memory in a momentum manner and insert an
additional DPL loss. Finally, we introduce a consistency regularization-based
approach to leverage samples with unconfident pseudo-labels. This approach
transfers feature styles of samples with unconfident pseudo-labels to those
with confident pseudo-labels. Thus, more reliable samples for TTA are created.
The experimental results demonstrate that our methods achieve state-of-the-art
performance on domain generalization benchmarks, and reliably improve the
performance of self-training-based methods on image corruption benchmarks. The
code will be released.</div><div><a href='http://arxiv.org/abs/2401.08703v2'>2401.08703v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07366v1")'>Entropy is not Enough for Test-Time Adaptation: From the Perspective of
  Disentangled Factors</div>
<div id='2403.07366v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T07:01:57Z</div><div>Authors: Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin, Uiwon Hwang, Sungroh Yoon</div><div style='padding-top: 10px; width: 80ex'>Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for
unseen test data. The primary challenge of TTA is limited access to the entire
test dataset during online updates, causing error accumulation. To mitigate it,
TTA methods have utilized the model output's entropy as a confidence metric
that aims to determine which samples have a lower likelihood of causing error.
Through experimental studies, however, we observed the unreliability of entropy
as a confidence metric for TTA under biased scenarios and theoretically
revealed that it stems from the neglect of the influence of latent disentangled
factors of data on predictions. Building upon these findings, we introduce a
novel TTA method named Destroy Your Object (DeYO), which leverages a newly
proposed confidence metric named Pseudo-Label Probability Difference (PLPD).
PLPD quantifies the influence of the shape of an object on prediction by
measuring the difference between predictions before and after applying an
object-destructive transformation. DeYO consists of sample selection and sample
weighting, which employ entropy and PLPD concurrently. For robust adaptation,
DeYO prioritizes samples that dominantly incorporate shape information when
making predictions. Our extensive experiments demonstrate the consistent
superiority of DeYO over baseline methods across various scenarios, including
biased and wild. Project page is publicly available at
https://whitesnowdrop.github.io/DeYO/.</div><div><a href='http://arxiv.org/abs/2403.07366v1'>2403.07366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11491v1")'>Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting</div>
<div id='2403.11491v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T05:49:45Z</div><div>Authors: Mingkui Tan, Guohao Chen, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Peilin Zhao, Shuaicheng Niu</div><div style='padding-top: 10px; width: 80ex'>Test-time adaptation (TTA) seeks to tackle potential distribution shifts
between training and test data by adapting a given model w.r.t. any test
sample. Although recent TTA has shown promising performance, we still face two
key challenges: 1) prior methods perform backpropagation for each test sample,
resulting in unbearable optimization costs to many applications; 2) while
existing TTA can significantly improve the test performance on
out-of-distribution data, they often suffer from severe performance degradation
on in-distribution data after TTA (known as forgetting). To this end, we have
proposed an Efficient Anti-Forgetting Test-Time Adaptation (EATA) method which
develops an active sample selection criterion to identify reliable and
non-redundant samples for test-time entropy minimization. To alleviate
forgetting, EATA introduces a Fisher regularizer estimated from test samples to
constrain important model parameters from drastic changes. However, in EATA,
the adopted entropy loss consistently assigns higher confidence to predictions
even for samples that are underlying uncertain, leading to overconfident
predictions. To tackle this, we further propose EATA with Calibration (EATA-C)
to separately exploit the reducible model uncertainty and the inherent data
uncertainty for calibrated TTA. Specifically, we measure the model uncertainty
by the divergence between predictions from the full network and its
sub-networks, on which we propose a divergence loss to encourage consistent
predictions instead of overconfident ones. To further recalibrate prediction
confidence, we utilize the disagreement among predicted labels as an indicator
of the data uncertainty, and then devise a min-max entropy regularizer to
selectively increase and decrease prediction confidence for different samples.
Experiments on image classification and semantic segmentation verify the
effectiveness of our methods.</div><div><a href='http://arxiv.org/abs/2403.11491v1'>2403.11491v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01344v1")'>Mitigating the Bias in the Model for Continual Test-Time Adaptation</div>
<div id='2403.01344v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T23:37:16Z</div><div>Authors: Inseop Chung, Kyomin Hwang, Jayeon Yoo, Nojun Kwak</div><div style='padding-top: 10px; width: 80ex'>Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt
a source pre-trained model to continually changing target domains. In the CTA
setting, a model does not know when the target domain changes, thus facing a
drastic change in the distribution of streaming inputs during the test-time.
The key challenge is to keep adapting the model to the continually changing
target domains in an online manner. We find that a model shows highly biased
predictions as it constantly adapts to the chaining distribution of the target
data. It predicts certain classes more often than other classes, making
inaccurate over-confident predictions. This paper mitigates this issue to
improve performance in the CTA scenario. To alleviate the bias issue, we make
class-wise exponential moving average target prototypes with reliable target
samples and exploit them to cluster the target features class-wisely. Moreover,
we aim to align the target distributions to the source distribution by
anchoring the target feature to its corresponding source prototype. With
extensive experiments, our proposed method achieves noteworthy performance gain
when applied on top of existing CTA methods without substantial adaptation time
overhead.</div><div><a href='http://arxiv.org/abs/2403.01344v1'>2403.01344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02561v1")'>MeTA: Multi-source Test Time Adaptation</div>
<div id='2401.02561v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T22:23:56Z</div><div>Authors: Sk Miraj Ahmed, Fahim Faisal Niloy, Dripta S. Raychaudhuri, Samet Oymak, Amit K. Roy-Chowdhury</div><div style='padding-top: 10px; width: 80ex'>Test time adaptation is the process of adapting, in an unsupervised manner, a
pre-trained source model to each incoming batch of the test data (i.e., without
requiring a substantial portion of the test data to be available, as in
traditional domain adaptation) and without access to the source data. Since it
works with each batch of test data, it is well-suited for dynamic environments
where decisions need to be made as the data is streaming in. Current test time
adaptation methods are primarily focused on a single source model. We propose
the first completely unsupervised Multi-source Test Time Adaptation (MeTA)
framework that handles multiple source models and optimally combines them to
adapt to the test data. MeTA has two distinguishing features. First, it
efficiently obtains the optimal combination weights to combine the source
models to adapt to the test data distribution. Second, it identifies which of
the source model parameters to update so that only the model which is most
correlated to the target data is adapted, leaving the less correlated ones
untouched; this mitigates the issue of "forgetting" the source model parameters
by focusing only on the source model that exhibits the strongest correlation
with the test batch distribution. Experiments on diverse datasets demonstrate
that the combination of multiple source models does at least as well as the
best source (with hindsight knowledge), and performance does not degrade as the
test data distribution changes over time (robust to forgetting).</div><div><a href='http://arxiv.org/abs/2401.02561v1'>2401.02561v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14619v1")'>Resilient Practical Test-Time Adaptation: Soft Batch Normalization
  Alignment and Entropy-driven Memory Bank</div>
<div id='2401.14619v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T03:24:55Z</div><div>Authors: Xingzhi Zhou, Zhiliang Tian, Ka Chun Cheung, Simon See, Nevin L. Zhang</div><div style='padding-top: 10px; width: 80ex'>Test-time domain adaptation effectively adjusts the source domain model to
accommodate unseen domain shifts in a target domain during inference. However,
the model performance can be significantly impaired by continuous distribution
changes in the target domain and non-independent and identically distributed
(non-i.i.d.) test samples often encountered in practical scenarios. While
existing memory bank methodologies use memory to store samples and mitigate
non-i.i.d. effects, they do not inherently prevent potential model degradation.
To address this issue, we propose a resilient practical test-time adaptation
(ResiTTA) method focused on parameter resilience and data quality.
Specifically, we develop a resilient batch normalization with estimation on
normalization statistics and soft alignments to mitigate overfitting and model
degradation. We use an entropy-driven memory bank that accounts for timeliness,
the persistence of over-confident samples, and sample uncertainty for
high-quality data in adaptation. Our framework periodically adapts the source
domain model using a teacher-student model through a self-training loss on the
memory samples, incorporating soft alignment losses on batch normalization. We
empirically validate ResiTTA across various benchmark datasets, demonstrating
state-of-the-art performance.</div><div><a href='http://arxiv.org/abs/2401.14619v1'>2401.14619v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08712v2")'>BECoTTA: Input-dependent Online Blending of Experts for Continual
  Test-time Adaptation</div>
<div id='2402.08712v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:37:53Z</div><div>Authors: Daeun Lee, Jaehong Yoon, Sung Ju Hwang</div><div style='padding-top: 10px; width: 80ex'>Continual Test Time Adaptation (CTTA) is required to adapt efficiently to
continuous unseen domains while retaining previously learned knowledge.
However, despite the progress of CTTA, forgetting-adaptation trade-offs and
efficiency are still unexplored. Moreover, current CTTA scenarios assume only
the disjoint situation, even though real-world domains are seamlessly changed.
To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet
efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts
(MoDE) that contains two core components: (i) Domain-Adaptive Routing, which
aids in selectively capturing the domain-adaptive knowledge with multiple
domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency
between each domain and expert. We validate our method outperforms multiple
CTTA scenarios including disjoint and gradual domain shits, while only
requiring ~98% fewer trainable parameters. We also provide analyses of our
method, including the construction of experts, the effect of domain-adaptive
experts, and visualizations.</div><div><a href='http://arxiv.org/abs/2402.08712v2'>2402.08712v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08182v1")'>Variational Continual Test-Time Adaptation</div>
<div id='2402.08182v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T02:41:56Z</div><div>Authors: Fan Lyu, Kaile Du, Yuyang Li, Hanyu Zhao, Zhang Zhang, Guangcan Liu, Liang Wang</div><div style='padding-top: 10px; width: 80ex'>The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods
that only use unlabeled test data, as it can cause significant error
propagation. In this paper, we introduce VCoTTA, a variational Bayesian
approach to measure uncertainties in CTTA. At the source stage, we transform a
pre-trained deterministic model into a Bayesian Neural Network (BNN) via a
variational warm-up strategy, injecting uncertainties into the model. During
the testing time, we employ a mean-teacher update strategy using variational
inference for the student model and exponential moving average for the teacher
model. Our novel approach updates the student model by combining priors from
both the source and teacher models. The evidence lower bound is formulated as
the cross-entropy between the student and teacher models, along with the
Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on
three datasets demonstrate the method's effectiveness in mitigating prior drift
within the CTTA framework.</div><div><a href='http://arxiv.org/abs/2402.08182v1'>2402.08182v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09004v1")'>Gradient Alignment with Prototype Feature for Fully Test-time Adaptation</div>
<div id='2402.09004v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T08:17:21Z</div><div>Authors: Juhyeon Shin, Jonghyun Lee, Saehyung Lee, Minjun Park, Dongjun Lee, Uiwon Hwang, Sungroh Yoon</div><div style='padding-top: 10px; width: 80ex'>In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed
Gradient Alignment with Prototype feature (GAP), which alleviates the
inappropriate guidance from entropy minimization loss from misclassified pseudo
label. We developed a gradient alignment loss to precisely manage the
adaptation process, ensuring that changes made for some data don't negatively
impact the model's performance on other data. We introduce a prototype feature
of a class as a proxy measure of the negative impact. To make GAP regularizer
feasible under the TTA constraints, where model can only access test data
without labels, we tailored its formula in two ways: approximating prototype
features with weight vectors of the classifier, calculating gradient without
back-propagation. We demonstrate GAP significantly improves TTA methods across
various datasets, which proves its versatility and effectiveness.</div><div><a href='http://arxiv.org/abs/2402.09004v1'>2402.09004v1</a></div>
</div></div>
    <div><a href="arxiv_23.html">Prev (23)</a></div>
    <div><a href="arxiv_25.html">Next (25)</a></div>
    