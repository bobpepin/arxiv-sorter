
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05572v1")'>Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent
  Systems</div>
<div id='2401.05572v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:51:10Z</div><div>Authors: Qin Yang</div><div style='padding-top: 10px; width: 80ex'>Innate values describe agents' intrinsic motivations, which reflect their
inherent interests and preferences to pursue goals and drive them to develop
diverse skills satisfying their various needs. The essence of reinforcement
learning (RL) is learning from interaction based on reward-driven (such as
utilities) behaviors, much like natural agents. It is an excellent model to
describe the innate-values-driven (IV) behaviors of AI agents. Especially in
multi-agent systems (MAS), building the awareness of AI agents to balance the
group utilities and system costs and satisfy group members' needs in their
cooperation is a crucial problem for individuals learning to support their
community and integrate human society in the long term. This paper proposes a
hierarchical compound intrinsic value reinforcement learning model --
innate-values-driven reinforcement learning termed IVRL to describe the complex
behaviors of multi-agent interaction in their cooperation. We implement the
IVRL architecture in the StarCraft Multi-Agent Challenge (SMAC) environment and
compare the cooperative performance within three characteristics of innate
value agents (Coward, Neutral, and Reckless) through three benchmark
multi-agent RL algorithms: QMIX, IQL, and QTRAN. The results demonstrate that
by organizing individual various needs rationally, the group can achieve better
performance with lower costs effectively.</div><div><a href='http://arxiv.org/abs/2401.05572v1'>2401.05572v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03950v1")'>Stop Regressing: Training Value Functions via Classification for
  Scalable Deep RL</div>
<div id='2403.03950v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T18:55:47Z</div><div>Authors: Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, Rishabh Agarwal</div><div style='padding-top: 10px; width: 80ex'>Value functions are a central component of deep reinforcement learning (RL).
These functions, parameterized by neural networks, are trained using a mean
squared error regression objective to match bootstrapped target values.
However, scaling value-based RL methods that use regression to large networks,
such as high-capacity Transformers, has proven challenging. This difficulty is
in stark contrast to supervised learning: by leveraging a cross-entropy
classification loss, supervised methods have scaled reliably to massive
networks. Observing this discrepancy, in this paper, we investigate whether the
scalability of deep RL can also be improved simply by using classification in
place of regression for training value functions. We demonstrate that value
functions trained with categorical cross-entropy significantly improves
performance and scalability in a variety of domains. These include: single-task
RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale
ResNets, robotic manipulation with Q-transformers, playing Chess without
search, and a language-agent Wordle task with high-capacity Transformers,
achieving state-of-the-art results on these domains. Through careful analysis,
we show that the benefits of categorical cross-entropy primarily stem from its
ability to mitigate issues inherent to value-based RL, such as noisy targets
and non-stationarity. Overall, we argue that a simple shift to training value
functions with categorical cross-entropy can yield substantial improvements in
the scalability of deep RL at little-to-no cost.</div><div><a href='http://arxiv.org/abs/2403.03950v1'>2403.03950v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05821v2")'>Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents</div>
<div id='2401.05821v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T10:38:22Z</div><div>Authors: Quentin Delfosse, Sebastian Sztwiertnia, Mark Rothermel, Wolfgang Stammer, Kristian Kersting</div><div style='padding-top: 10px; width: 80ex'>Goal misalignment, reward sparsity and difficult credit assignment are only a
few of the many issues that make it difficult for deep reinforcement learning
(RL) agents to learn optimal policies. Unfortunately, the black-box nature of
deep neural networks impedes the inclusion of domain experts for inspecting the
model and revising suboptimal policies. To this end, we introduce *Successive
Concept Bottleneck Agents* (SCoBots), that integrate consecutive concept
bottleneck (CB) layers. In contrast to current CB models, SCoBots do not just
represent concepts as properties of individual objects, but also as relations
between objects which is crucial for many RL tasks. Our experimental results
provide evidence of SCoBots' competitive performances, but also of their
potential for domain experts to understand and regularize their behavior. Among
other things, SCoBots enabled us to identify a previously unknown misalignment
problem in the iconic video game, Pong, and resolve it. Overall, SCoBots thus
result in more human-aligned RL agents. Our code is available at
https://github.com/k4ntz/SCoBots .</div><div><a href='http://arxiv.org/abs/2401.05821v2'>2401.05821v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02017v1")'>Value-Aided Conditional Supervised Learning for Offline RL</div>
<div id='2402.02017v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T04:17:09Z</div><div>Authors: Jeonghye Kim, Suyoung Lee, Woojun Kim, Youngchul Sung</div><div style='padding-top: 10px; width: 80ex'>Offline reinforcement learning (RL) has seen notable advancements through
return-conditioned supervised learning (RCSL) and value-based methods, yet each
approach comes with its own set of practical challenges. Addressing these, we
propose Value-Aided Conditional Supervised Learning (VCS), a method that
effectively synergizes the stability of RCSL with the stitching ability of
value-based methods. Based on the Neural Tangent Kernel analysis to discern
instances where value function may not lead to stable stitching, VCS injects
the value aid into the RCSL's loss function dynamically according to the
trajectory return. Our empirical studies reveal that VCS not only significantly
outperforms both RCSL and value-based methods but also consistently achieves,
or often surpasses, the highest trajectory returns across diverse offline RL
benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the
limits of what can be achieved and fostering further innovations.</div><div><a href='http://arxiv.org/abs/2402.02017v1'>2402.02017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05385v3")'>Switching the Loss Reduces the Cost in Batch Reinforcement Learning</div>
<div id='2403.05385v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T15:30:58Z</div><div>Authors: Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James McInerney, Dawen Liang, Nathan Kallus, Csaba Szepesvári</div><div style='padding-top: 10px; width: 80ex'>We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch
reinforcement learning (RL). We show that the number of samples needed to learn
a near-optimal policy with FQI-LOG scales with the accumulated cost of the
optimal policy, which is zero in problems where acting optimally achieves the
goal and incurs no cost. In doing so, we provide a general framework for
proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal
achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses
fewer samples than FQI trained with squared loss on problems where the optimal
policy reliably achieves the goal.</div><div><a href='http://arxiv.org/abs/2403.05385v3'>2403.05385v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14534v1")'>Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML
  Approach for the Model-free LQR</div>
<div id='2401.14534v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T21:59:52Z</div><div>Authors: Leonardo F. Toso, Donglin Zhan, James Anderson, Han Wang</div><div style='padding-top: 10px; width: 80ex'>We investigate the problem of learning Linear Quadratic Regulators (LQR) in a
multi-task, heterogeneous, and model-free setting. We characterize the
stability and personalization guarantees of a Policy Gradient-based (PG)
Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR
problem under different task-heterogeneity settings. We show that the MAML-LQR
approach produces a stabilizing controller close to each task-specific optimal
controller up to a task-heterogeneity bias for both model-based and model-free
settings. Moreover, in the model-based setting, we show that this controller is
achieved with a linear convergence rate, which improves upon sub-linear rates
presented in existing MAML-LQR work. In contrast to existing MAML-LQR results,
our theoretical guarantees demonstrate that the learned controller can
efficiently adapt to unseen LQR tasks.</div><div><a href='http://arxiv.org/abs/2401.14534v1'>2401.14534v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07886v1")'>Learned Best-Effort LLM Serving</div>
<div id='2401.07886v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T18:28:17Z</div><div>Authors: Siddharth Jha, Coleman Hooper, Xiaoxuan Liu, Sehoon Kim, Kurt Keutzer</div><div style='padding-top: 10px; width: 80ex'>Many applications must provide low-latency LLM service to users or risk
unacceptable user experience. However, over-provisioning resources to serve
fluctuating request patterns is often prohibitively expensive. In this work, we
present a best-effort serving system that employs deep reinforcement learning
to adjust service quality based on the task distribution and system load. Our
best-effort system can maintain availability with over 10x higher client
request rates, serves above 96% of peak performance 4.1x more often, and serves
above 98% of peak performance 2.3x more often than static serving on
unpredictable workloads. Our learned router is robust to shifts in both the
arrival and task distribution. Compared to static serving, learned best-effort
serving allows for cost-efficient serving through increased hardware utility.
Additionally, we argue that learned best-effort LLM serving is applicable in
wide variety of settings and provides application developers great flexibility
to meet their specific needs.</div><div><a href='http://arxiv.org/abs/2401.07886v1'>2401.07886v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07588v2")'>Rethinking Scaling Laws for Learning in Strategic Environments</div>
<div id='2402.07588v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:41:42Z</div><div>Authors: Tinashe Handina, Eric Mazumdar</div><div style='padding-top: 10px; width: 80ex'>The deployment of ever-larger machine learning models reflects a growing
consensus that the more expressive the model$\unicode{x2013}$and the more data
one has access to$\unicode{x2013}$the more one can improve performance. As
models get deployed in a variety of real world scenarios, they inevitably face
strategic environments. In this work, we consider the natural question of how
the interplay of models and strategic interactions affects scaling laws. We
find that strategic interactions can break the conventional view of scaling
laws$\unicode{x2013}$meaning that performance does not necessarily
monotonically improve as models get larger and/ or more expressive (even with
infinite data). We show the implications of this phenomenon in several contexts
including strategic regression, strategic classification, and multi-agent
reinforcement learning through examples of strategic environments in
which$\unicode{x2013}$by simply restricting the expressivity of one's model or
policy class$\unicode{x2013}$one can achieve strictly better equilibrium
outcomes. Motivated by these examples, we then propose a new paradigm for
model-selection in games wherein an agent seeks to choose amongst different
model classes to use as their action set in a game.</div><div><a href='http://arxiv.org/abs/2402.07588v2'>2402.07588v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00787v1")'>Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour
  with Multi-Agent Reinforcement Learning</div>
<div id='2402.00787v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:21:45Z</div><div>Authors: Benjamin Patrick Evans, Sumitra Ganesh</div><div style='padding-top: 10px; width: 80ex'>Agent-based models (ABMs) have shown promise for modelling various real world
phenomena incompatible with traditional equilibrium analysis. However, a
critical concern is the manual definition of behavioural rules in ABMs. Recent
developments in multi-agent reinforcement learning (MARL) offer a way to
address this issue from an optimisation perspective, where agents strive to
maximise their utility, eliminating the need for manual rule specification.
This learning-focused approach aligns with established economic and financial
models through the use of rational utility-maximising agents. However, this
representation departs from the fundamental motivation for ABMs: that realistic
dynamics emerging from bounded rationality and agent heterogeneity can be
modelled. To resolve this apparent disparity between the two approaches, we
propose a novel technique for representing heterogeneous processing-constrained
agents within a MARL framework. The proposed approach treats agents as
constrained optimisers with varying degrees of strategic skills, permitting
departure from strict utility maximisation. Behaviour is learnt through
repeated simulations with policy gradients to adjust action likelihoods. To
allow efficient computation, we use parameterised shared policy learning with
distributions of agent skill levels. Shared policy learning avoids the need for
agents to learn individual policies yet still enables a spectrum of bounded
rational behaviours. We validate our model's effectiveness using real-world
data on a range of canonical $n$-agent settings, demonstrating significantly
improved predictive capability.</div><div><a href='http://arxiv.org/abs/2402.00787v1'>2402.00787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15856v1")'>Look Around! Unexpected gains from training on environments in the
  vicinity of the target</div>
<div id='2401.15856v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T03:07:04Z</div><div>Authors: Serena Bono, Spandan Madan, Ishaan Grover, Mao Yasueda, Cynthia Breazeal, Hanspeter Pfister, Gabriel Kreiman</div><div style='padding-top: 10px; width: 80ex'>Solutions to Markov Decision Processes (MDP) are often very sensitive to
state transition probabilities. As the estimation of these probabilities is
often inaccurate in practice, it is important to understand when and how
Reinforcement Learning (RL) agents generalize when transition probabilities
change. Here we present a new methodology to evaluate such generalization of RL
agents under small shifts in the transition probabilities. Specifically, we
evaluate agents in new environments (MDPs) in the vicinity of the training MDP
created by adding quantifiable, parametric noise into the transition function
of the training MDP. We refer to this process as Noise Injection, and the
resulting environments as $\delta$-environments. This process allows us to
create controlled variations of the same environment with the level of the
noise serving as a metric of distance between environments. Conventional wisdom
suggests that training and testing on the same MDP should yield the best
results. However, we report several cases of the opposite -- when targeting a
specific environment, training the agent in an alternative noise setting can
yield superior outcomes. We showcase this phenomenon across $60$ different
variations of ATARI games, including PacMan, Pong, and Breakout.</div><div><a href='http://arxiv.org/abs/2401.15856v1'>2401.15856v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12284v1")'>Refining Minimax Regret for Unsupervised Environment Design</div>
<div id='2402.12284v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T16:51:29Z</div><div>Authors: Michael Beukman, Samuel Coward, Michael Matthews, Mattie Fellows, Minqi Jiang, Michael Dennis, Jakob Foerster</div><div style='padding-top: 10px; width: 80ex'>In unsupervised environment design, reinforcement learning agents are trained
on environment configurations (levels) generated by an adversary that maximises
some objective. Regret is a commonly used objective that theoretically results
in a minimax regret (MMR) policy with desirable robustness guarantees; in
particular, the agent's maximum regret is bounded. However, once the agent
reaches this regret bound on all levels, the adversary will only sample levels
where regret cannot be further reduced. Although there are possible performance
improvements to be made outside of these regret-maximising levels, learning
stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a
refinement of the minimax regret objective that overcomes this limitation. We
formally show that solving for this objective results in a subset of MMR
policies, and that BLP policies act consistently with a Perfect Bayesian policy
over all levels. We further introduce an algorithm, ReMiDi, that results in a
BLP policy at convergence. We empirically demonstrate that training on levels
from a minimax regret adversary causes learning to prematurely stagnate, but
that ReMiDi continues learning.</div><div><a href='http://arxiv.org/abs/2402.12284v1'>2402.12284v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10855v1")'>Reinforcement Learning with Options</div>
<div id='2403.10855v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T08:30:55Z</div><div>Authors: Ayoub Ghriss, Masashi Sugiyama, Alessandro Lazaric</div><div style='padding-top: 10px; width: 80ex'>The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the "Eigenoption". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.</div><div><a href='http://arxiv.org/abs/2403.10855v1'>2403.10855v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08936v1")'>DeLF: Designing Learning Environments with Foundation Models</div>
<div id='2401.08936v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T03:14:28Z</div><div>Authors: Aida Afshar, Wenchao Li</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) offers a capable and intuitive structure for the
fundamental sequential decision-making problem. Despite impressive
breakthroughs, it can still be difficult to employ RL in practice in many
simple applications. In this paper, we try to address this issue by introducing
a method for designing the components of the RL environment for a given,
user-intended application. We provide an initial formalization for the problem
of RL component design, that concentrates on designing a good representation
for observation and action space. We propose a method named DeLF: Designing
Learning Environments with Foundation Models, that employs large language
models to design and codify the user's intended learning scenario. By testing
our method on four different learning environments, we demonstrate that DeLF
can obtain executable environment codes for the corresponding RL problems.</div><div><a href='http://arxiv.org/abs/2401.08936v1'>2401.08936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00993v1")'>On the Role of Information Structure in Reinforcement Learning for
  Partially-Observable Sequential Teams and Games</div>
<div id='2403.00993v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T21:28:19Z</div><div>Authors: Awni Altabaa, Zhuoran Yang</div><div style='padding-top: 10px; width: 80ex'>In a sequential decision-making problem, the information structure is the
description of how events in the system occurring at different points in time
affect each other. Classical models of reinforcement learning (e.g., MDPs,
POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular
information structure, while more general models like predictive state
representations do not explicitly model the information structure. By contrast,
real-world sequential decision-making problems typically involve a complex and
time-varying interdependence of system variables, requiring a rich and flexible
representation of information structure.
  In this paper, we argue for the perspective that explicit representation of
information structures is an important component of analyzing and solving
reinforcement learning problems. We propose novel reinforcement learning models
with an explicit representation of information structure, capturing classical
models as special cases. We show that this leads to a richer analysis of
sequential decision-making problems and enables more tailored algorithm design.
In particular, we characterize the "complexity" of the observable dynamics of
any sequential decision-making problem through a graph-theoretic analysis of
the DAG representation of its information structure. The central quantity in
this analysis is the minimal set of variables that $d$-separates the past
observations from future observations. Furthermore, through constructing a
generalization of predictive state representations, we propose tailored
reinforcement learning algorithms and prove that the sample complexity is in
part determined by the information structure. This recovers known tractability
results and gives a novel perspective on reinforcement learning in general
sequential decision-making problems, providing a systematic way of identifying
new tractable classes of problems.</div><div><a href='http://arxiv.org/abs/2403.00993v1'>2403.00993v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12874v1")'>Skill or Luck? Return Decomposition via Advantage Functions</div>
<div id='2402.12874v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T10:09:00Z</div><div>Authors: Hsiao-Ru Pan, Bernhard Schölkopf</div><div style='padding-top: 10px; width: 80ex'>Learning from off-policy data is essential for sample-efficient reinforcement
learning. In the present work, we build on the insight that the advantage
function can be understood as the causal effect of an action on the return, and
show that this allows us to decompose the return of a trajectory into parts
caused by the agent's actions (skill) and parts outside of the agent's control
(luck). Furthermore, this decomposition enables us to naturally extend Direct
Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The
resulting method can learn from off-policy trajectories without relying on
importance sampling techniques or truncating off-policy actions. We draw
connections between Off-policy DAE and previous methods to demonstrate how it
can speed up learning and when the proposed off-policy corrections are
important. Finally, we use the MinAtar environments to illustrate how ignoring
off-policy corrections can lead to suboptimal policy optimization performance.</div><div><a href='http://arxiv.org/abs/2402.12874v1'>2402.12874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13178v1")'>Fast Value Tracking for Deep Reinforcement Learning</div>
<div id='2403.13178v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T22:18:19Z</div><div>Authors: Frank Shih, Faming Liang</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) tackles sequential decision-making problems by
creating agents that interacts with their environment. However, existing
algorithms often view these problem as static, focusing on point estimates for
model parameters to maximize expected rewards, neglecting the stochastic
dynamics of agent-environment interactions and the critical role of uncertainty
quantification. Our research leverages the Kalman filtering paradigm to
introduce a novel and scalable sampling algorithm called Langevinized Kalman
Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm,
grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently
draws samples from the posterior distribution of deep neural network
parameters. Under mild conditions, we prove that the posterior samples
generated by the LKTD algorithm converge to a stationary distribution. This
convergence not only enables us to quantify uncertainties associated with the
value function and model parameters but also allows us to monitor these
uncertainties during policy updates throughout the training phase. The LKTD
algorithm paves the way for more robust and adaptable reinforcement learning
approaches.</div><div><a href='http://arxiv.org/abs/2403.13178v1'>2403.13178v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08898v2")'>Bridging State and History Representations: Understanding
  Self-Predictive RL</div>
<div id='2401.08898v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T00:47:43Z</div><div>Authors: Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement Gehring, Aditya Mahajan, Pierre-Luc Bacon</div><div style='padding-top: 10px; width: 80ex'>Representations are at the core of all deep reinforcement learning (RL)
methods for both Markov decision processes (MDPs) and partially observable
Markov decision processes (POMDPs). Many representation learning methods and
theoretical frameworks have been developed to understand what constitutes an
effective representation. However, the relationships between these methods and
the shared properties among them remain unclear. In this paper, we show that
many of these seemingly distinct methods and frameworks for state and history
abstractions are, in fact, based on a common idea of self-predictive
abstraction. Furthermore, we provide theoretical insights into the widely
adopted objectives and optimization, such as the stop-gradient technique, in
learning self-predictive representations. These findings together yield a
minimalist algorithm to learn self-predictive representations for states and
histories. We validate our theories by applying our algorithm to standard MDPs,
MDPs with distractors, and POMDPs with sparse rewards. These findings culminate
in a set of preliminary guidelines for RL practitioners.</div><div><a href='http://arxiv.org/abs/2401.08898v2'>2401.08898v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09113v1")'>Measuring Exploration in Reinforcement Learning via Optimal Transport in
  Policy Space</div>
<div id='2402.09113v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T11:55:50Z</div><div>Authors: Reabetswe M. Nkhumise, Debabrota Basu, Tony J. Prescott, Aditya Gilra</div><div style='padding-top: 10px; width: 80ex'>Exploration is the key ingredient of reinforcement learning (RL) that
determines the speed and success of learning. Here, we quantify and compare the
amount of exploration and learning accomplished by a Reinforcement Learning
(RL) algorithm. Specifically, we propose a novel measure, named Exploration
Index, that quantifies the relative effort of knowledge transfer
(transferability) by an RL algorithm in comparison to supervised learning (SL)
that transforms the initial data distribution of RL to the corresponding final
data distribution. The comparison is established by formulating learning in RL
as a sequence of SL tasks, and using optimal transport based metrics to compare
the total path traversed by the RL and SL algorithms in the data distribution
space. We perform extensive empirical analysis on various environments and with
multiple algorithms to demonstrate that the exploration index yields insights
about the exploration behaviour of any RL algorithm, and also allows us to
compare the exploratory behaviours of different RL algorithms.</div><div><a href='http://arxiv.org/abs/2402.09113v1'>2402.09113v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09838v1")'>Performative Reinforcement Learning in Gradually Shifting Environments</div>
<div id='2402.09838v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T10:00:13Z</div><div>Authors: Ben Rank, Stelios Triantafyllou, Debmalya Mandal, Goran Radanovic</div><div style='padding-top: 10px; width: 80ex'>When Reinforcement Learning (RL) agents are deployed in practice, they might
impact their environment and change its dynamics. Ongoing research attempts to
formally model this phenomenon and to analyze learning algorithms in these
models. To this end, we propose a framework where the current environment
depends on the deployed policy as well as its previous dynamics. This is a
generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our
framework allows to model scenarios where the environment gradually adjusts to
a deployed policy. We adapt two algorithms from the performative prediction
literature to our setting and propose a novel algorithm called Mixed Delayed
Repeated Retraining (MDRR). We provide conditions under which these algorithms
converge and compare them using three metrics: number of retrainings,
approximation guarantee, and number of samples per deployment. Unlike previous
approaches, MDRR combines samples from multiple deployments in its training.
This makes MDRR particularly suitable for scenarios where the environment's
response strongly depends on its previous dynamics, which are common in
practice. We experimentally compare the algorithms using a simulation-based
testbed and our results show that MDRR converges significantly faster than
previous approaches.</div><div><a href='http://arxiv.org/abs/2402.09838v1'>2402.09838v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08819v2")'>Learning from Sparse Offline Datasets via Conservative Density
  Estimation</div>
<div id='2401.08819v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T20:42:15Z</div><div>Authors: Zhepeng Cen, Zuxin Liu, Zitong Wang, Yihang Yao, Henry Lam, Ding Zhao</div><div style='padding-top: 10px; width: 80ex'>Offline reinforcement learning (RL) offers a promising direction for learning
policies from pre-collected datasets without requiring further interactions
with the environment. However, existing methods struggle to handle
out-of-distribution (OOD) extrapolation errors, especially in sparse reward or
scarce data settings. In this paper, we propose a novel training algorithm
called Conservative Density Estimation (CDE), which addresses this challenge by
explicitly imposing constraints on the state-action occupancy stationary
distribution. CDE overcomes the limitations of existing approaches, such as the
stationary distribution correction method, by addressing the support mismatch
issue in marginal importance sampling. Our method achieves state-of-the-art
performance on the D4RL benchmark. Notably, CDE consistently outperforms
baselines in challenging tasks with sparse rewards or insufficient data,
demonstrating the advantages of our approach in addressing the extrapolation
error problem in offline RL.</div><div><a href='http://arxiv.org/abs/2401.08819v2'>2401.08819v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03643v2")'>A Survey on Applications of Reinforcement Learning in Spatial Resource
  Allocation</div>
<div id='2403.03643v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:05:56Z</div><div>Authors: Di Zhang, Moyang Wang, Joseph Mango, Xiang Li, Xianrui Xu</div><div style='padding-top: 10px; width: 80ex'>The challenge of spatial resource allocation is pervasive across various
domains such as transportation, industry, and daily life. As the scale of
real-world issues continues to expand and demands for real-time solutions
increase, traditional algorithms face significant computational pressures,
struggling to achieve optimal efficiency and real-time capabilities. In recent
years, with the escalating computational power of computers, the remarkable
achievements of reinforcement learning in domains like Go and robotics have
demonstrated its robust learning and sequential decision-making capabilities.
Given these advancements, there has been a surge in novel methods employing
reinforcement learning to tackle spatial resource allocation problems. These
methods exhibit advantages such as rapid solution convergence and strong model
generalization abilities, offering a new perspective on resolving spatial
resource allocation problems. Therefore, this paper aims to summarize and
review recent theoretical methods and applied research utilizing reinforcement
learning to address spatial resource allocation problems. It provides a summary
and comprehensive overview of its fundamental principles, related
methodologies, and applied research. Additionally, it highlights several
unresolved issues that urgently require attention in this direction for the
future.</div><div><a href='http://arxiv.org/abs/2403.03643v2'>2403.03643v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01849v1")'>Capturing waste collection planning expert knowledge in a fitness
  function through preference learning</div>
<div id='2402.01849v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:04:53Z</div><div>Authors: Laura Fernández Díaz, Miriam Fernández Díaz, José Ramón Quevedo, Elena Montañés</div><div style='padding-top: 10px; width: 80ex'>This paper copes with the COGERSA waste collection process. Up to now,
experts have been manually designed the process using a trial and error
mechanism. This process is not globally optimized, since it has been
progressively and locally built as council demands appear. Planning
optimization algorithms usually solve it, but they need a fitness function to
evaluate a route planning quality. The drawback is that even experts are not
able to propose one in a straightforward way due to the complexity of the
process. Hence, the goal of this paper is to build a fitness function though a
preference framework, taking advantage of the available expert knowledge and
expertise. Several key performance indicators together with preference
judgments are carefully established according to the experts for learning a
promising fitness function. Particularly, the additivity property of them makes
the task be much more affordable, since it allows to work with routes rather
than with route plannings. Besides, a feature selection analysis is performed
over such indicators, since the experts suspect of a potential existing (but
unknown) redundancy among them. The experiment results confirm this hypothesis,
since the best $C-$index ($98\%$ against around $94\%$) is reached when 6 or 8
out of 21 indicators are taken. Particularly, truck load seems to be a highly
promising key performance indicator, together to the travelled distance along
non-main roads. A comparison with other existing approaches shows that the
proposed method clearly outperforms them, since the $C-$index goes from $72\%$
or $90\%$ to $98\%$.</div><div><a href='http://arxiv.org/abs/2402.01849v1'>2402.01849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02957v1")'>Multi-Agent Reinforcement Learning for Offloading Cellular
  Communications with Cooperating UAVs</div>
<div id='2402.02957v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:36:08Z</div><div>Authors: Abhishek Mondal, Deepak Mishra, Ganesh Prasad, George C. Alexandropoulos, Azzam Alnahari, Riku Jantti</div><div style='padding-top: 10px; width: 80ex'>Effective solutions for intelligent data collection in terrestrial cellular
networks are crucial, especially in the context of Internet of Things
applications. The limited spectrum and coverage area of terrestrial base
stations pose challenges in meeting the escalating data rate demands of network
users. Unmanned aerial vehicles, known for their high agility, mobility, and
flexibility, present an alternative means to offload data traffic from
terrestrial BSs, serving as additional access points. This paper introduces a
novel approach to efficiently maximize the utilization of multiple UAVs for
data traffic offloading from terrestrial BSs. Specifically, the focus is on
maximizing user association with UAVs by jointly optimizing UAV trajectories
and users association indicators under quality of service constraints. Since,
the formulated UAVs control problem is nonconvex and combinatorial, this study
leverages the multi agent reinforcement learning framework. In this framework,
each UAV acts as an independent agent, aiming to maintain inter UAV cooperative
behavior. The proposed approach utilizes the finite state Markov decision
process to account for UAVs velocity constraints and the relationship between
their trajectories and state space. A low complexity distributed state action
reward state action algorithm is presented to determine UAVs optimal sequential
decision making policies over training episodes. The extensive simulation
results validate the proposed analysis and offer valuable insights into the
optimal UAV trajectories. The derived trajectories demonstrate superior average
UAV association performance compared to benchmark techniques such as Q learning
and particle swarm optimization.</div><div><a href='http://arxiv.org/abs/2402.02957v1'>2402.02957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11118v1")'>Meta Reinforcement Learning for Strategic IoT Deployments Coverage in
  Disaster-Response UAV Swarms</div>
<div id='2401.11118v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T05:05:39Z</div><div>Authors: Marwan Dhuheir, Aiman Erbad, Ala Al-Fuqaha</div><div style='padding-top: 10px; width: 80ex'>In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the
attention of researchers in academia and industry for their potential use in
critical emergency applications, such as providing wireless services to ground
users and collecting data from areas affected by disasters, due to their
advantages in terms of maneuverability and movement flexibility. The UAVs'
limited resources, energy budget, and strict mission completion time have posed
challenges in adopting UAVs for these applications. Our system model considers
a UAV swarm that navigates an area collecting data from ground IoT devices
focusing on providing better service for strategic locations and allowing UAVs
to join and leave the swarm (e.g., for recharging) in a dynamic way. In this
work, we introduce an optimization model with the aim of minimizing the total
energy consumption and provide the optimal path planning of UAVs under the
constraints of minimum completion time and transmit power. The formulated
optimization is NP-hard making it not applicable for real-time decision making.
Therefore, we introduce a light-weight meta-reinforcement learning solution
that can also cope with sudden changes in the environment through fast
convergence. We conduct extensive simulations and compare our approach to three
state-of-the-art learning models. Our simulation results prove that our
introduced approach is better than the three state-of-the-art algorithms in
providing coverage to strategic locations with fast convergence.</div><div><a href='http://arxiv.org/abs/2401.11118v1'>2401.11118v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18487v1")'>Human-Centric Aware UAV Trajectory Planning in Search and Rescue
  Missions Employing Multi-Objective Reinforcement Learning with AHP and
  Similarity-Based Experience Replay</div>
<div id='2402.18487v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:10:22Z</div><div>Authors: Mahya Ramezani, Jose Luis Sanchez-Lopez</div><div style='padding-top: 10px; width: 80ex'>The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue
(SAR) missions presents a promising avenue for enhancing operational efficiency
and effectiveness. However, the success of these missions is not solely
dependent on the technical capabilities of the drones but also on their
acceptance and interaction with humans on the ground. This paper explores the
effect of human-centric factor in UAV trajectory planning for SAR missions. We
introduce a novel approach based on the reinforcement learning augmented with
Analytic Hierarchy Process and novel similarity-based experience replay to
optimize UAV trajectories, balancing operational objectives with human comfort
and safety considerations. Additionally, through a comprehensive survey, we
investigate the impact of gender cues and anthropomorphism in UAV design on
public acceptance and trust, revealing significant implications for drone
interaction strategies in SAR. Our contributions include (1) a reinforcement
learning framework for UAV trajectory planning that dynamically integrates
multi-objective considerations, (2) an analysis of human perceptions towards
gendered and anthropomorphized drones in SAR contexts, and (3) the application
of similarity-based experience replay for enhanced learning efficiency in
complex SAR scenarios. The findings offer valuable insights into designing UAV
systems that are not only technically proficient but also aligned with
human-centric values.</div><div><a href='http://arxiv.org/abs/2402.18487v1'>2402.18487v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12079v1")'>Collaborative Reinforcement Learning Based Unmanned Aerial Vehicle (UAV)
  Trajectory Design for 3D UAV Tracking</div>
<div id='2401.12079v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:21:19Z</div><div>Authors: Yujiao Zhu, Mingzhe Chen, Sihua Wang, Ye Hu, Yuchen Liu, Changchuan Yin</div><div style='padding-top: 10px; width: 80ex'>In this paper, the problem of using one active unmanned aerial vehicle (UAV)
and four passive UAVs to localize a 3D target UAV in real time is investigated.
In the considered model, each passive UAV receives reflection signals from the
target UAV, which are initially transmitted by the active UAV. The received
reflection signals allow each passive UAV to estimate the signal transmission
distance which will be transmitted to a base station (BS) for the estimation of
the position of the target UAV. Due to the movement of the target UAV, each
active/passive UAV must optimize its trajectory to continuously localize the
target UAV. Meanwhile, since the accuracy of the distance estimation depends on
the signal-to-noise ratio of the transmission signals, the active UAV must
optimize its transmit power. This problem is formulated as an optimization
problem whose goal is to jointly optimize the transmit power of the active UAV
and trajectories of both active and passive UAVs so as to maximize the target
UAV positioning accuracy. To solve this problem, a Z function decomposition
based reinforcement learning (ZD-RL) method is proposed. Compared to value
function decomposition based RL (VD-RL), the proposed method can find the
probability distribution of the sum of future rewards to accurately estimate
the expected value of the sum of future rewards thus finding better transmit
power of the active UAV and trajectories for both active and passive UAVs and
improving target UAV positioning accuracy. Simulation results show that the
proposed ZD-RL method can reduce the positioning errors by up to 39.4% and
64.6%, compared to VD-RL and independent deep RL methods, respectively.</div><div><a href='http://arxiv.org/abs/2401.12079v1'>2401.12079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03204v1")'>Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell
  Massive MIMO Systems</div>
<div id='2402.03204v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:15:00Z</div><div>Authors: Tianzhang Cai, Qichen Wang, Shuai Zhang, Özlem Tuğfe Demir, Cicek Cavdar</div><div style='padding-top: 10px; width: 80ex'>We develop a multi-agent reinforcement learning (MARL) algorithm to minimize
the total energy consumption of multiple massive MIMO (multiple-input
multiple-output) base stations (BSs) in a multi-cell network while preserving
the overall quality-of-service (QoS) by making decisions on the multi-level
advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is
modeled as a decentralized partially observable Markov decision process
(DEC-POMDP) to enable collaboration between individual BSs, which is necessary
to tackle inter-cell interference. A multi-agent proximal policy optimization
(MAPPO) algorithm is designed to learn a collaborative BS control policy. To
enhance its scalability, a modified version called MAPPO-neighbor policy is
further proposed. Simulation results demonstrate that the trained MAPPO agent
achieves better performance compared to baseline policies. Specifically,
compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the
MAPPO-neighbor policy reduces power consumption by approximately 8.7% during
low-traffic hours and improves energy efficiency by approximately 19% during
high-traffic hours, respectively.</div><div><a href='http://arxiv.org/abs/2402.03204v1'>2402.03204v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08022v1")'>Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale
  Wireless Networks</div>
<div id='2402.08022v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:39:07Z</div><div>Authors: Talha Bozkus, Urbashi Mitra</div><div style='padding-top: 10px; width: 80ex'>Optimizing large-scale wireless networks, including optimal resource
management, power allocation, and throughput maximization, is inherently
challenging due to their non-observable system dynamics and heterogeneous and
complex nature. Herein, a novel ensemble Q-learning algorithm that addresses
the performance and complexity challenges of the traditional Q-learning
algorithm for optimizing wireless networks is presented. Ensemble learning with
synthetic Markov Decision Processes is tailored to wireless networks via new
models for approximating large state-space observable wireless networks. In
particular, digital cousins are proposed as an extension of the traditional
digital twin concept wherein multiple Q-learning algorithms on multiple
synthetic Markovian environments are run in parallel and their outputs are
fused into a single Q-function. Convergence analyses of key statistics and
Q-functions and derivations of upper bounds on the estimation bias and variance
are provided. Numerical results across a variety of real-world wireless
networks show that the proposed algorithm can achieve up to 50% less average
policy error with up to 40% less runtime complexity than the state-of-the-art
reinforcement learning algorithms. It is also shown that theoretical results
properly predict trends in the experimental results.</div><div><a href='http://arxiv.org/abs/2402.08022v1'>2402.08022v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08301v1")'>Sum Throughput Maximization in Multi-BD Symbiotic Radio NOMA Network
  Assisted by Active-STAR-RIS</div>
<div id='2401.08301v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T11:54:32Z</div><div>Authors: Rahman Saadat Yeganeh, Mohammad Javad Omidi, Farshad Zeinali, Mohammad Robat Mili, Mohammad Ghavami</div><div style='padding-top: 10px; width: 80ex'>In this paper, we employ active simultaneously transmitting and reflecting
reconfigurable intelligent surface (ASRIS) to aid in establishing and enhancing
communication within a commensal symbiotic radio (CSR) network. Unlike
traditional RIS, ASRIS not only ensures coverage in an omni directional manner
but also amplifies received signals, consequently elevating overall network
performance. in the first phase, base station (BS) with active massive MIMO
antennas, send ambient signal to SBDs. In the first phase, the BS transmits
ambient signals to the symbiotic backscatter devices (SBDs), and after
harvesting the energy and modulating their information onto the signal carrier,
the SBDs send Backscatter signals back to the BS. In this scheme, we employ the
Backscatter Relay system to facilitate the transmission of information from the
SBDs to the symbiotic User Equipments (SUEs) with the assistance of the BS. In
the second phase, the BS transmits information signals to the SUEs after
eliminating interference using the Successive Interference Cancellation (SIC)
method. ASRIS is employed to establish communication among SUEs lacking a line
of sight (LoS) and to amplify power signals for SUEs with a LoS connection to
the BS. It is worth noting that we use NOMA for multiple access in all network.
  The main goal of this paper is to maximize the sum throughput between all
users. To achieve this, we formulate an optimization problem with variables
including active beamforming coefficients at the BS and ASRIS, as well as the
phase adjustments of ASRIS and scheduling parameters between the first and
second phases. To model this optimization problem, we employ three deep
reinforcement learning (DRL) methods, namely PPO, TD3, and A3C. Finally, the
mentioned methods are simulated and compared with each other.</div><div><a href='http://arxiv.org/abs/2401.08301v1'>2401.08301v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02254v1")'>Teacher-Student Learning based Low Complexity Relay Selection in
  Wireless Powered Communications</div>
<div id='2402.02254v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T20:22:41Z</div><div>Authors: Aysun Gurur Onalan, Berkay Kopru, Sinem Coleri</div><div style='padding-top: 10px; width: 80ex'>Radio Frequency Energy Harvesting (RF-EH) networks are key enablers of
massive Internet-of-things by providing controllable and long-distance energy
transfer to energy-limited devices. Relays, helping either energy or
information transfer, have been demonstrated to significantly improve the
performance of these networks. This paper studies the joint relay selection,
scheduling, and power control problem in multiple-source-multiple-relay RF-EH
networks under nonlinear EH conditions. We first obtain the optimal solution to
the scheduling and power control problem for the given relay selection. Then,
the relay selection problem is formulated as a classification problem, for
which two convolutional neural network (CNN) based architectures are proposed.
While the first architecture employs conventional 2D convolution blocks and
benefits from skip connections between layers; the second architecture replaces
them with inception blocks, to decrease trainable parameter size without
sacrificing accuracy for memory-constrained applications. To decrease the
runtime complexity further, teacher-student learning is employed such that the
teacher network is larger, and the student is a smaller size CNN-based
architecture distilling the teacher's knowledge. A novel dichotomous
search-based algorithm is employed to determine the best architecture for the
student network. Our simulation results demonstrate that the proposed solutions
provide lower complexity than the state-of-art iterative approaches without
compromising optimality.</div><div><a href='http://arxiv.org/abs/2402.02254v1'>2402.02254v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17880v1")'>Graph Attention-based Reinforcement Learning for Trajectory Design and
  Resource Assignment in Multi-UAV Assisted Communication</div>
<div id='2401.17880v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T14:37:06Z</div><div>Authors: Zikai Feng, Di Wu, Mengxing Huang, Chau Yuen</div><div style='padding-top: 10px; width: 80ex'>In the multiple unmanned aerial vehicle (UAV)- assisted downlink
communication, it is challenging for UAV base stations (UAV BSs) to realize
trajectory design and resource assignment in unknown environments. The
cooperation and competition between UAV BSs in the communication network leads
to a Markov game problem. Multi-agent reinforcement learning is a significant
solution for the above decision-making. However, there are still many common
issues, such as the instability of the system and low utilization of historical
data, that limit its application. In this paper, a novel graph-attention
multi-agent trust region (GA-MATR) reinforcement learning framework is proposed
to solve the multi-UAV assisted communication problem. Graph recurrent network
is introduced to process and analyze complex topology of the communication
network, so as to extract useful information and patterns from observational
information. The attention mechanism provides additional weighting for conveyed
information, so that the critic network can accurately evaluate the value of
behavior for UAV BSs. This provides more reliable feedback signals and helps
the actor network update the strategy more effectively. Ablation simulations
indicate that the proposed approach attains improved convergence over the
baselines. UAV BSs learn the optimal communication strategies to achieve their
maximum cumulative rewards. Additionally, multi-agent trust region method with
monotonic convergence provides an estimated Nash equilibrium for the multi-UAV
assisted communication Markov game.</div><div><a href='http://arxiv.org/abs/2401.17880v1'>2401.17880v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08421v1")'>Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning
  for Digital Twins</div>
<div id='2402.08421v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T12:49:22Z</div><div>Authors: Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, Hirley Alves</div><div style='padding-top: 10px; width: 80ex'>Digital twin (DT) platforms are increasingly regarded as a promising
technology for controlling, optimizing, and monitoring complex engineering
systems such as next-generation wireless networks. An important challenge in
adopting DT solutions is their reliance on data collected offline, lacking
direct access to the physical environment. This limitation is particularly
severe in multi-agent systems, for which conventional multi-agent reinforcement
(MARL) requires online interactions with the environment. A direct application
of online MARL schemes to an offline setting would generally fail due to the
epistemic uncertainty entailed by the limited availability of data. In this
work, we propose an offline MARL scheme for DT-based wireless networks that
integrates distributional RL and conservative Q-learning to address the
environment's inherent aleatoric uncertainty and the epistemic uncertainty
arising from limited data. To further exploit the offline data, we adapt the
proposed scheme to the centralized training decentralized execution framework,
allowing joint training of the agents' policies. The proposed MARL scheme,
referred to as multi-agent conservative quantile regression (MA-CQR) addresses
general risk-sensitive design criteria and is applied to the trajectory
planning problem in drone networks, showcasing its advantages.</div><div><a href='http://arxiv.org/abs/2402.08421v1'>2402.08421v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09498v2")'>Technical Report: On the Convergence of Gossip Learning in the Presence
  of Node Inaccessibility</div>
<div id='2401.09498v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T06:11:19Z</div><div>Authors: Tian Liu, Yue Cui, Xueyang Hu, Yecheng Xu, Bo Liu</div><div style='padding-top: 10px; width: 80ex'>Gossip learning (GL), as a decentralized alternative to federated learning
(FL), is more suitable for resource-constrained wireless networks, such as
Flying Ad-Hoc Networks (FANETs) that are formed by unmanned aerial vehicles
(UAVs). GL can significantly enhance the efficiency and extend the battery life
of UAV networks. Despite the advantages, the performance of GL is strongly
affected by data distribution, communication speed, and network connectivity.
However, how these factors influence the GL convergence is still unclear.
Existing work studied the convergence of GL based on a virtual quantity for the
sake of convenience, which failed to reflect the real state of the network when
some nodes are inaccessible. In this paper, we formulate and investigate the
impact of inaccessible nodes to GL under a dynamic network topology. We first
decompose the weight divergence by whether the node is accessible or not. Then,
we investigate the GL convergence under the dynamic of node accessibility and
theoretically provide how the number of inaccessible nodes, data
non-i.i.d.-ness, and duration of inaccessibility affect the convergence.
Extensive experiments are carried out in practical settings to comprehensively
verify the correctness of our theoretical findings.</div><div><a href='http://arxiv.org/abs/2401.09498v2'>2401.09498v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05133v1")'>RIS-empowered Topology Control for Distributed Learning in Urban Air
  Mobility</div>
<div id='2403.05133v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T08:05:50Z</div><div>Authors: Kai Xiong, Rui Wang, Supeng Leng, Wenyang Che, Chongwen Huang, Chau Yuen</div><div style='padding-top: 10px; width: 80ex'>Urban Air Mobility (UAM) expands vehicles from the ground to the near-ground
space, envisioned as a revolution for transportation systems. Comprehensive
scene perception is the foundation for autonomous aerial driving. However, UAM
encounters the intelligent perception challenge: high perception learning
requirements conflict with the limited sensors and computing chips of flying
cars. To overcome the challenge, federated learning (FL) and other
collaborative learning have been proposed to enable resource-limited devices to
conduct onboard deep learning (DL) collaboratively. But traditional
collaborative learning like FL relies on a central integrator for DL model
aggregation, which is difficult to deploy in dynamic environments. The fully
decentralized learning schemes may be the intuitive solution while the
convergence of distributed learning cannot be guaranteed. Accordingly, this
paper explores reconfigurable intelligent surfaces (RIS) empowered distributed
learning, taking account of topological attributes to facilitate the learning
performance with convergence guarantee. We propose several FL topological
criteria for optimizing the transmission delay and convergence rate by
exploiting the Laplacian matrix eigenvalues of the communication network.
Subsequently, we innovatively leverage the RIS link modification ability to
remold the current network according to the proposed topological criteria. This
paper rethinks the functions of RIS from the perspective of the network layer.
Furthermore, a deep deterministic policy gradient-based RIS phase shift control
algorithm is developed to construct or deconstruct the network links
simultaneously to reshape the communication network. Simulation experiments are
conducted over MobileNet-based multi-view learning to verify the efficiency of
the distributed FL framework.</div><div><a href='http://arxiv.org/abs/2403.05133v1'>2403.05133v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13827v1")'>Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink
  in Markovian IoT Models</div>
<div id='2401.13827v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T21:57:55Z</div><div>Authors: Eslam Eldeeb, Mohammad Shehab, Hirley Alves</div><div style='padding-top: 10px; width: 80ex'>The age of information (AoI) is used to measure the freshness of the data. In
IoT networks, the traditional resource management schemes rely on a message
exchange between the devices and the base station (BS) before communication
which causes high AoI, high energy consumption, and low reliability. Unmanned
aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the
AoI, energy-saving, and throughput improvement. In this paper, we present a
novel learning-based framework that estimates the traffic arrival of IoT
devices based on Markovian events. The learning proceeds to optimize the
trajectory of multiple UAVs and their scheduling policy. First, the BS predicts
the future traffic of the devices. We compare two traffic predictors: the
forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we
propose a deep reinforcement learning (DRL) approach to optimize the optimal
policy of each UAV. Finally, we manipulate the optimum reward function for the
proposed DRL approach. Simulation results show that the proposed algorithm
outperforms the random-walk (RW) baseline model regarding the AoI, scheduling
accuracy, and transmission power.</div><div><a href='http://arxiv.org/abs/2401.13827v1'>2401.13827v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10927v1")'>Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground
  Cooperative MEC</div>
<div id='2403.10927v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T13:50:31Z</div><div>Authors: Yang Huang, Miaomiao Dong, Yijie Mao, Wenqiang Liu, Zhen Gao</div><div style='padding-top: 10px; width: 80ex'>Utilizing unmanned aerial vehicles (UAVs) with edge server to assist
terrestrial mobile edge computing (MEC) has attracted tremendous attention.
Nevertheless, state-of-the-art schemes based on deterministic optimizations or
single-objective reinforcement learning (RL) cannot reduce the backlog of task
bits and simultaneously improve energy efficiency in highly dynamic network
environments, where the design problem amounts to a sequential decision-making
problem. In order to address the aforementioned problems, as well as the curses
of dimensionality introduced by the growing number of terrestrial terrestrial
users, this paper proposes a distributed multi-objective (MO) dynamic
trajectory planning and offloading scheduling scheme, integrated with MORL and
the kernel method. The design of n-step return is also applied to average
fluctuations in the backlog. Numerical results reveal that the n-step return
can benefit the proposed kernel-based approach, achieving significant
improvement in the long-term average backlog performance, compared to the
conventional 1-step return design. Due to such design and the kernel-based
neural network, to which decision-making features can be continuously added,
the kernel-based approach can outperform the approach based on fully-connected
deep neural network, yielding improvement in energy consumption and the backlog
performance, as well as a significant reduction in decision-making and online
learning time.</div><div><a href='http://arxiv.org/abs/2403.10927v1'>2403.10927v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15972v1")'>Structural Knowledge-Driven Meta-Learning for Task Offloading in
  Vehicular Networks with Integrated Communications, Sensing and Computing</div>
<div id='2402.15972v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T03:31:59Z</div><div>Authors: Ruijin Sun, Yao Wen, Nan Cheng, Wei Wan, Rong Chai, Yilong Hui</div><div style='padding-top: 10px; width: 80ex'>Task offloading is a potential solution to satisfy the strict requirements of
computation-intensive and latency-sensitive vehicular applications due to the
limited onboard computing resources. However, the overwhelming upload traffic
may lead to unacceptable uploading time. To tackle this issue, for tasks taking
environmental data as input, the data perceived by roadside units (RSU)
equipped with several sensors can be directly exploited for computation,
resulting in a novel task offloading paradigm with integrated communications,
sensing and computing (I-CSC). With this paradigm, vehicles can select to
upload their sensed data to RSUs or transmit computing instructions to RSUs
during the offloading. By optimizing the computation mode and network
resources, in this paper, we investigate an I-CSC-based task offloading problem
to reduce the cost caused by resource consumption while guaranteeing the
latency of each task. Although this non-convex problem can be handled by the
alternating minimization (AM) algorithm that alternatively minimizes the
divided four sub-problems, it leads to high computational complexity and local
optimal solution. To tackle this challenge, we propose a creative structural
knowledge-driven meta-learning (SKDML) method, involving both the model-based
AM algorithm and neural networks. Specifically, borrowing the iterative
structure of the AM algorithm, also referred to as structural knowledge, the
proposed SKDML adopts long short-term memory (LSTM) network-based meta-learning
to learn an adaptive optimizer for updating variables in each sub-problem,
instead of the handcrafted counterpart in the AM algorithm.</div><div><a href='http://arxiv.org/abs/2402.15972v1'>2402.15972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09426v1")'>Graph Koopman Autoencoder for Predictive Covert Communication Against
  UAV Surveillance</div>
<div id='2402.09426v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T23:42:55Z</div><div>Authors: Sivaram Krishnan, Jihong Park, Gregory Sherman, Benjamin Campbell, Jinho Choi</div><div style='padding-top: 10px; width: 80ex'>Low Probability of Detection (LPD) communication aims to obscure the very
presence of radio frequency (RF) signals, going beyond just hiding the content
of the communication. However, the use of Unmanned Aerial Vehicles (UAVs)
introduces a challenge, as UAVs can detect RF signals from the ground by
hovering over specific areas of interest. With the growing utilization of UAVs
in modern surveillance, there is a crucial need for a thorough understanding of
their unknown nonlinear dynamic trajectories to effectively implement LPD
communication. Unfortunately, this critical information is often not readily
available, posing a significant hurdle in LPD communication. To address this
issue, we consider a case-study for enabling terrestrial LPD communication in
the presence of multiple UAVs that are engaged in surveillance. We introduce a
novel framework that combines graph neural networks (GNN) with Koopman theory
to predict the trajectories of multiple fixed-wing UAVs over an extended
prediction horizon. Using the predicted UAV locations, we enable LPD
communication in a terrestrial ad-hoc network by controlling nodes' transmit
powers to keep the received power at UAVs' predicted locations minimized. Our
extensive simulations validate the efficacy of the proposed framework in
accurately predicting the trajectories of multiple UAVs, thereby effectively
establishing LPD communication.</div><div><a href='http://arxiv.org/abs/2402.09426v1'>2402.09426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.06514v1")'>Personalized Reinforcement Learning with a Budget of Policies</div>
<div id='2401.06514v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T11:27:55Z</div><div>Authors: Dmitry Ivanov, Omer Ben-Porat</div><div style='padding-top: 10px; width: 80ex'>Personalization in machine learning (ML) tailors models' decisions to the
individual characteristics of users. While this approach has seen success in
areas like recommender systems, its expansion into high-stakes fields such as
healthcare and autonomous driving is hindered by the extensive regulatory
approval processes involved. To address this challenge, we propose a novel
framework termed represented Markov Decision Processes (r-MDPs) that is
designed to balance the need for personalization with the regulatory
constraints. In an r-MDP, we cater to a diverse user population, each with
unique preferences, through interaction with a small set of representative
policies. Our objective is twofold: efficiently match each user to an
appropriate representative policy and simultaneously optimize these policies to
maximize overall social welfare. We develop two deep reinforcement learning
algorithms that efficiently solve r-MDPs. These algorithms draw inspiration
from the principles of classic K-means clustering and are underpinned by robust
theoretical foundations. Our empirical investigations, conducted across a
variety of simulated environments, showcase the algorithms' ability to
facilitate meaningful personalization even under constrained policy budgets.
Furthermore, they demonstrate scalability, efficiently adapting to larger
policy budgets.</div><div><a href='http://arxiv.org/abs/2401.06514v1'>2401.06514v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02858v1")'>Deep autoregressive density nets vs neural ensembles for model-based
  offline reinforcement learning</div>
<div id='2402.02858v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:18:15Z</div><div>Authors: Abdelhakim Benechehab, Albert Thomas, Balázs Kégl</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of offline reinforcement learning where only a set of
system transitions is made available for policy optimization. Following recent
advances in the field, we consider a model-based reinforcement learning
algorithm that infers the system dynamics from the available data and performs
policy optimization on imaginary model rollouts. This approach is vulnerable to
exploiting model errors which can lead to catastrophic failures on the real
system. The standard solution is to rely on ensembles for uncertainty
heuristics and to avoid exploiting the model where it is too uncertain. We
challenge the popular belief that we must resort to ensembles by showing that
better performance can be obtained with a single well-calibrated autoregressive
model on the D4RL benchmark. We also analyze static metrics of model-learning
and conclude on the important model properties for the final performance of the
agent.</div><div><a href='http://arxiv.org/abs/2402.02858v1'>2402.02858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00916v1")'>Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning</div>
<div id='2401.00916v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T06:53:36Z</div><div>Authors: Mohamad Abed El Rahman Hammoud, Naila Raboudi, Edriss S. Titi, Omar Knio, Ibrahim Hoteit</div><div style='padding-top: 10px; width: 80ex'>Data assimilation (DA) plays a pivotal role in diverse applications, ranging
from climate predictions and weather forecasts to trajectory planning for
autonomous vehicles. A prime example is the widely used ensemble Kalman filter
(EnKF), which relies on linear updates to minimize variance among the ensemble
of forecast states. Recent advancements have seen the emergence of deep
learning approaches in this domain, primarily within a supervised learning
framework. However, the adaptability of such models to untrained scenarios
remains a challenge. In this study, we introduce a novel DA strategy that
utilizes reinforcement learning (RL) to apply state corrections using full or
partial observations of the state variables. Our investigation focuses on
demonstrating this approach to the chaotic Lorenz '63 system, where the agent's
objective is to minimize the root-mean-squared error between the observations
and corresponding forecast states. Consequently, the agent develops a
correction strategy, enhancing model forecasts based on available system state
observations. Our strategy employs a stochastic action policy, enabling a Monte
Carlo-based DA framework that relies on randomly sampling the policy to
generate an ensemble of assimilated realizations. Results demonstrate that the
developed RL algorithm performs favorably when compared to the EnKF.
Additionally, we illustrate the agent's capability to assimilate non-Gaussian
data, addressing a significant limitation of the EnKF.</div><div><a href='http://arxiv.org/abs/2401.00916v1'>2401.00916v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12687v1")'>DVL Calibration using Data-driven Methods</div>
<div id='2401.12687v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T11:52:25Z</div><div>Authors: Zeev Yampolsky, Itzik Klein</div><div style='padding-top: 10px; width: 80ex'>Autonomous underwater vehicles (AUVs) are used in a wide range of underwater
applications, ranging from seafloor mapping to industrial operations. While
underwater, the AUV navigation solution commonly relies on the fusion between
inertial sensors and Doppler velocity logs (DVL). To achieve accurate DVL
measurements a calibration procedure should be conducted before the mission
begins. Model-based calibration approaches include filtering approaches
utilizing global navigation satellite system signals. In this paper, we propose
an end-to-end deep-learning framework for the calibration procedure. Using
stimulative data, we show that our proposed approach outperforms model-based
approaches by 35% in accuracy and 80% in the required calibration time.</div><div><a href='http://arxiv.org/abs/2401.12687v1'>2401.12687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06854v1")'>Quantifying the Sensitivity of Inverse Reinforcement Learning to
  Misspecification</div>
<div id='2403.06854v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:09:39Z</div><div>Authors: Joar Skalse, Alessandro Abate</div><div style='padding-top: 10px; width: 80ex'>Inverse reinforcement learning (IRL) aims to infer an agent's preferences
(represented as a reward function $R$) from their behaviour (represented as a
policy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to
$R$. In the current literature, the most common behavioural models are
optimality, Boltzmann-rationality, and causal entropy maximisation. However,
the true relationship between a human's preferences and their behaviour is much
more complex than any of these behavioural models. This means that the
behavioural models are misspecified, which raises the concern that they may
lead to systematic errors if applied to real data. In this paper, we analyse
how sensitive the IRL problem is to misspecification of the behavioural model.
Specifically, we provide necessary and sufficient conditions that completely
characterise how the observed data may differ from the assumed behavioural
model without incurring an error above a given threshold. In addition to this,
we also characterise the conditions under which a behavioural model is robust
to small perturbations of the observed policy, and we analyse how robust many
behavioural models are to misspecification of their parameter values (such as
e.g.\ the discount rate). Our analysis suggests that the IRL problem is highly
sensitive to misspecification, in the sense that very mild misspecification can
lead to very large errors in the inferred reward function.</div><div><a href='http://arxiv.org/abs/2403.06854v1'>2403.06854v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04221v1")'>Why Online Reinforcement Learning is Causal</div>
<div id='2403.04221v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T04:49:48Z</div><div>Authors: Oliver Schulte, Pascal Poupart</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) and causal modelling naturally complement each
other. The goal of causal modelling is to predict the effects of interventions
in an environment, while the goal of reinforcement learning is to select
interventions that maximize the rewards the agent receives from the
environment. Reinforcement learning includes the two most powerful sources of
information for estimating causal relationships: temporal ordering and the
ability to act on an environment. This paper examines which reinforcement
learning settings we can expect to benefit from causal modelling, and how. In
online learning, the agent has the ability to interact directly with their
environment, and learn from exploring it. Our main argument is that in online
learning, conditional probabilities are causal, and therefore offline RL is the
setting where causal learning has the most potential to make a difference.
Essentially, the reason is that when an agent learns from their {\em own}
experience, there are no unobserved confounders that influence both the agent's
own exploratory actions and the rewards they receive. Our paper formalizes this
argument. For offline RL, where an agent may and typically does learn from the
experience of {\em others}, we describe previous and new methods for leveraging
a causal model, including support for counterfactual queries.</div><div><a href='http://arxiv.org/abs/2403.04221v1'>2403.04221v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13929v1")'>Reinforcement Learning with Hidden Markov Models for Discovering
  Decision-Making Dynamics</div>
<div id='2401.13929v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T04:03:32Z</div><div>Authors: Xingche Guo, Donglin Zeng, Yuanjia Wang</div><div style='padding-top: 10px; width: 80ex'>Major depressive disorder (MDD) presents challenges in diagnosis and
treatment due to its complex and heterogeneous nature. Emerging evidence
indicates that reward processing abnormalities may serve as a behavioral marker
for MDD. To measure reward processing, patients perform computer-based
behavioral tasks that involve making choices or responding to stimulants that
are associated with different outcomes. Reinforcement learning (RL) models are
fitted to extract parameters that measure various aspects of reward processing
to characterize how patients make decisions in behavioral tasks. Recent
findings suggest the inadequacy of characterizing reward learning solely based
on a single RL model; instead, there may be a switching of decision-making
processes between multiple strategies. An important scientific question is how
the dynamics of learning strategies in decision-making affect the reward
learning ability of individuals with MDD. Motivated by the probabilistic reward
task (PRT) within the EMBARC study, we propose a novel RL-HMM framework for
analyzing reward-based decision-making. Our model accommodates learning
strategy switching between two distinct approaches under a hidden Markov model
(HMM): subjects making decisions based on the RL model or opting for random
choices. We account for continuous RL state space and allow time-varying
transition probabilities in the HMM. We introduce a computationally efficient
EM algorithm for parameter estimation and employ a nonparametric bootstrap for
inference. We apply our approach to the EMBARC study to show that MDD patients
are less engaged in RL compared to the healthy controls, and engagement is
associated with brain activities in the negative affect circuitry during an
emotional conflict task.</div><div><a href='http://arxiv.org/abs/2401.13929v1'>2401.13929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10877v4")'>Robust agents learn causal world models</div>
<div id='2402.10877v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:29:19Z</div><div>Authors: Jonathan Richens, Tom Everitt</div><div style='padding-top: 10px; width: 80ex'>It has long been hypothesised that causal reasoning plays a fundamental role
in robust and general intelligence. However, it is not known if agents must
learn causal models in order to generalise to new domains, or if other
inductive biases are sufficient. We answer this question, showing that any
agent capable of satisfying a regret bound under a large set of distributional
shifts must have learned an approximate causal model of the data generating
process, which converges to the true causal model for optimal agents. We
discuss the implications of this result for several research areas including
transfer learning and causal inference.</div><div><a href='http://arxiv.org/abs/2402.10877v4'>2402.10877v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08530v1")'>A Distributional Analogue to the Successor Representation</div>
<div id='2402.08530v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T15:35:24Z</div><div>Authors: Harley Wiltzer, Jesse Farebrother, Arthur Gretton, Yunhao Tang, André Barreto, Will Dabney, Marc G. Bellemare, Mark Rowland</div><div style='padding-top: 10px; width: 80ex'>This paper contributes a new approach for distributional reinforcement
learning which elucidates a clean separation of transition structure and reward
in the learning process. Analogous to how the successor representation (SR)
describes the expected consequences of behaving according to a given policy,
our distributional successor measure (SM) describes the distributional
consequences of this behaviour. We formulate the distributional SM as a
distribution over distributions and provide theory connecting it with
distributional and model-based reinforcement learning. Moreover, we propose an
algorithm that learns the distributional SM from data by minimizing a two-level
maximum mean discrepancy. Key to our method are a number of algorithmic
techniques that are independently valuable for learning generative models of
state. As an illustration of the usefulness of the distributional SM, we show
that it enables zero-shot risk-sensitive policy evaluation in a way that was
not previously possible.</div><div><a href='http://arxiv.org/abs/2402.08530v1'>2402.08530v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07598v1")'>Near-Minimax-Optimal Distributional Reinforcement Learning with a
  Generative Model</div>
<div id='2402.07598v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:58:18Z</div><div>Authors: Mark Rowland, Li Kevin Wenliang, Rémi Munos, Clare Lyle, Yunhao Tang, Will Dabney</div><div style='padding-top: 10px; width: 80ex'>We propose a new algorithm for model-based distributional reinforcement
learning (RL), and prove that it is minimax-optimal for approximating return
distributions with a generative model (up to logarithmic factors), resolving an
open question of Zhang et al. (2023). Our analysis provides new theoretical
results on categorical approaches to distributional RL, and also introduces a
new distributional Bellman equation, the stochastic categorical CDF Bellman
equation, which we expect to be of independent interest. We also provide an
experimental study comparing several model-based distributional RL algorithms,
with several takeaways for practitioners.</div><div><a href='http://arxiv.org/abs/2402.07598v1'>2402.07598v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07198v1")'>More Benefits of Being Distributional: Second-Order Bounds for
  Reinforcement Learning</div>
<div id='2402.07198v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T13:25:53Z</div><div>Authors: Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, Wen Sun</div><div style='padding-top: 10px; width: 80ex'>In this paper, we prove that Distributional Reinforcement Learning (DistRL),
which learns the return distribution, can obtain second-order bounds in both
online and offline RL in general settings with function approximation.
Second-order bounds are instance-dependent bounds that scale with the variance
of return, which we prove are tighter than the previously known small-loss
bounds of distributional RL. To the best of our knowledge, our results are the
first second-order bounds for low-rank MDPs and for offline RL. When
specializing to contextual bandits (one-step RL problem), we show that a
distributional learning based optimism algorithm achieves a second-order
worst-case regret bound, and a second-order gap dependent bound,
simultaneously. We also empirically demonstrate the benefit of DistRL in
contextual bandits on real-world datasets. We highlight that our analysis with
DistRL is relatively simple, follows the general framework of optimism in the
face of uncertainty and does not require weighted regression. Our results
suggest that DistRL is a promising framework for obtaining second-order bounds
in general RL settings, thus further reinforcing the benefits of DistRL.</div><div><a href='http://arxiv.org/abs/2402.07198v1'>2402.07198v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01879v1")'>Theoretical guarantees on the best-of-n alignment policy</div>
<div id='2401.01879v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T18:39:13Z</div><div>Authors: Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh</div><div style='padding-top: 10px; width: 80ex'>A simple and effective method for the alignment of generative models is the
best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked
based on a reward function, and the highest ranking one is selected. A commonly
used analytical expression in the literature claims that the KL divergence
between the best-of-$n$ policy and the base policy is equal to $\log (n) -
(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper
bound on the actual KL divergence. We also explore the tightness of this upper
bound in different regimes. Finally, we propose a new estimator for the KL
divergence and empirically show that it provides a tight approximation through
a few examples.</div><div><a href='http://arxiv.org/abs/2401.01879v1'>2401.01879v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03163v1")'>An Empirical Investigation of Value-Based Multi-objective Reinforcement
  Learning for Stochastic Environments</div>
<div id='2401.03163v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:43:08Z</div><div>Authors: Kewen Ding, Peter Vamplew, Cameron Foale, Richard Dazeley</div><div style='padding-top: 10px; width: 80ex'>One common approach to solve multi-objective reinforcement learning (MORL)
problems is to extend conventional Q-learning by using vector Q-values in
combination with a utility function. However issues can arise with this
approach in the context of stochastic environments, particularly when
optimising for the Scalarised Expected Reward (SER) criterion. This paper
extends prior research, providing a detailed examination of the factors
influencing the frequency with which value-based MORL Q-learning algorithms
learn the SER-optimal policy for an environment with stochastic state
transitions. We empirically examine several variations of the core
multi-objective Q-learning algorithm as well as reward engineering approaches,
and demonstrate the limitations of these methods. In particular, we highlight
the critical impact of the noisy Q-value estimates issue on the stability and
convergence of these algorithms.</div><div><a href='http://arxiv.org/abs/2401.03163v1'>2401.03163v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14860v1")'>Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive
  Control</div>
<div id='2403.14860v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T22:15:09Z</div><div>Authors: Minjun Sung, Sambhu H. Karumanchi, Aditya Gahlawat, Naira Hovakimyan</div><div style='padding-top: 10px; width: 80ex'>We introduce $\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme
for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free
approaches, MBRL algorithms learn a model of the transition function using data
and use it to design a control input. Our approach generates a series of
approximate control-affine models of the learned transition function according
to the proposed switching law. Using the approximate model, control input
produced by the underlying MBRL is perturbed by the $\mathcal{L}_1$ adaptive
control, which is designed to enhance the robustness of the system against
uncertainties. Importantly, this approach is agnostic to the choice of MBRL
algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL
algorithms with $\mathcal{L}_1$ augmentation exhibit enhanced performance and
sample efficiency across multiple MuJoCo environments, outperforming the
original MBRL algorithms, both with and without system noise.</div><div><a href='http://arxiv.org/abs/2403.14860v1'>2403.14860v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03146v1")'>A Multi-step Loss Function for Robust Learning of the Dynamics in
  Model-based Reinforcement Learning</div>
<div id='2402.03146v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:13:00Z</div><div>Authors: Abdelhakim Benechehab, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, Balázs Kégl</div><div style='padding-top: 10px; width: 80ex'>In model-based reinforcement learning, most algorithms rely on simulating
trajectories from one-step models of the dynamics learned on data. A critical
challenge of this approach is the compounding of one-step prediction errors as
the length of the trajectory grows. In this paper we tackle this issue by using
a multi-step objective to train one-step models. Our objective is a weighted
sum of the mean squared error (MSE) loss at various future horizons. We find
that this new loss is particularly useful when the data is noisy (additive
Gaussian noise in the observations), which is often the case in real-life
environments. To support the multi-step loss, first we study its properties in
two tractable cases: i) uni-dimensional linear system, and ii) two-parameter
non-linear system. Second, we show in a variety of tasks (environments or
datasets) that the models learned with this loss achieve a significant
improvement in terms of the averaged R2-score on future prediction horizons.
Finally, in the pure batch reinforcement learning setting, we demonstrate that
one-step models serve as strong baselines when dynamics are deterministic,
while multi-step models would be more advantageous in the presence of noise,
highlighting the potential of our approach in real-world applications.</div><div><a href='http://arxiv.org/abs/2402.03146v1'>2402.03146v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11940v1")'>Multistep Inverse Is Not All You Need</div>
<div id='2403.11940v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:36:01Z</div><div>Authors: Alexander Levine, Peter Stone, Amy Zhang</div><div style='padding-top: 10px; width: 80ex'>In real-world control settings, the observation space is often unnecessarily
high-dimensional and subject to time-correlated noise. However, the
controllable dynamics of the system are often far simpler than the dynamics of
the raw observations. It is therefore desirable to learn an encoder to map the
observation space to a simpler space of control-relevant variables. In this
work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022),
which formalizes control problems where observations can be factorized into an
action-dependent latent state which evolves deterministically, and
action-independent time-correlated noise. Lamb et al. (2022) proposes the
"AC-State" method for learning an encoder to extract a complete
action-dependent latent state representation from the observations in such
problems. AC-State is a multistep-inverse method, in that it uses the encoding
of the the first and last state in a path to predict the first action in the
path. However, we identify cases where AC-State will fail to learn a correct
latent representation of the agent-controllable factor of the state. We
therefore propose a new algorithm, ACDF, which combines multistep-inverse
prediction with a latent forward model. ACDF is guaranteed to correctly infer
an action-dependent latent state encoder for a large class of Ex-BMDP models.
We demonstrate the effectiveness of ACDF on tabular Ex-BMDPs through numerical
simulations; as well as high-dimensional environments using
neural-network-based encoders. Code is available at
https://github.com/midi-lab/acdf.</div><div><a href='http://arxiv.org/abs/2403.11940v1'>2403.11940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15290v1")'>Linear Dynamics-embedded Neural Network for Long-Sequence Modeling</div>
<div id='2402.15290v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:36:31Z</div><div>Authors: Tongyi Liang, Han-Xiong Li</div><div style='padding-top: 10px; width: 80ex'>The trade-off between performance and computational efficiency in
long-sequence modeling becomes a bottleneck for existing models. Inspired by
the continuous state space models (SSMs) with multi-input and multi-output in
control theory, we propose a new neural network called Linear Dynamics-embedded
Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties
enable LDNN to have few parameters, flexible inference, and efficient training
in long-sequence tasks. Two efficient strategies, diagonalization and
$'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to
reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to
$O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional
noncausal and multi-head settings to accommodate a broader range of
applications. Extensive experiments on the Long Range Arena (LRA) demonstrate
the effectiveness and state-of-the-art performance of LDNN.</div><div><a href='http://arxiv.org/abs/2402.15290v1'>2402.15290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10123v1")'>Regularization-Based Efficient Continual Learning in Deep State-Space
  Models</div>
<div id='2403.10123v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T09:14:18Z</div><div>Authors: Yuanhang Zhang, Zhidi Lin, Yiyong Sun, Feng Yin, Carsten Fritsche</div><div style='padding-top: 10px; width: 80ex'>Deep state-space models (DSSMs) have gained popularity in recent years due to
their potent modeling capacity for dynamic systems. However, existing DSSM
works are limited to single-task modeling, which requires retraining with
historical task data upon revisiting a forepassed task. To address this
limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of
adapting to evolving tasks without catastrophic forgetting. Our proposed
CLDSSMs integrate mainstream regularization-based continual learning (CL)
methods, ensuring efficient updates with constant computational and memory
costs for modeling multiple dynamic systems. We also conduct a comprehensive
cost analysis of each CL method applied to the respective CLDSSMs, and
demonstrate the efficacy of CLDSSMs through experiments on real-world datasets.
The results corroborate that while various competing CL methods exhibit
different merits, the proposed CLDSSMs consistently outperform traditional
DSSMs in terms of effectively addressing catastrophic forgetting, enabling
swift and accurate parameter transfer to new tasks.</div><div><a href='http://arxiv.org/abs/2403.10123v1'>2403.10123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02902v1")'>State Derivative Normalization for Continuous-Time Deep Neural Networks</div>
<div id='2401.02902v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T17:04:33Z</div><div>Authors: Jonas Weigand, Gerben I. Beintema, Jonas Ulmen, Daniel Görges, Roland Tóth, Maarten Schoukens, Martin Ruskowski</div><div style='padding-top: 10px; width: 80ex'>The importance of proper data normalization for deep neural networks is well
known. However, in continuous-time state-space model estimation, it has been
observed that improper normalization of either the hidden state or hidden state
derivative of the model estimate, or even of the time interval can lead to
numerical and optimization challenges with deep learning based methods. This
results in a reduced model quality. In this contribution, we show that these
three normalization tasks are inherently coupled. Due to the existence of this
coupling, we propose a solution to all three normalization challenges by
introducing a normalization constant at the state derivative level. We show
that the appropriate choice of the normalization constant is related to the
dynamics of the to-be-identified system and we derive multiple methods of
obtaining an effective normalization constant. We compare and discuss all the
normalization strategies on a benchmark problem based on experimental data from
a cascaded tanks system and compare our results with other methods of the
identification literature.</div><div><a href='http://arxiv.org/abs/2401.02902v1'>2401.02902v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04508v1")'>Data-driven Nonlinear Model Reduction using Koopman Theory: Integrated
  Control Form and NMPC Case Study</div>
<div id='2401.04508v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T11:54:54Z</div><div>Authors: Jan C. Schulze, Alexander Mitsos</div><div style='padding-top: 10px; width: 80ex'>We use Koopman theory for data-driven model reduction of nonlinear dynamical
systems with controls. We propose generic model structures combining
delay-coordinate encoding of measurements and full-state decoding to integrate
reduced Koopman modeling and state estimation. We present a deep-learning
approach to train the proposed models. A case study demonstrates that our
approach provides accurate control models and enables real-time capable
nonlinear model predictive control of a high-purity cryogenic distillation
column.</div><div><a href='http://arxiv.org/abs/2401.04508v1'>2401.04508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15993v2")'>Learning Method for S4 with Diagonal State Space Layers using Balanced
  Truncation</div>
<div id='2402.15993v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T05:22:45Z</div><div>Authors: Haruka Ezoe, Kazuhiro Sato</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel learning method for Structured State Space Sequence (S4)
models incorporating Diagonal State Space (DSS) layers, tailored for processing
long-sequence data in edge intelligence applications, including sensor data
analysis and real-time analytics. This method utilizes the balanced truncation,
a prevalent model reduction technique in control theory, applied specifically
to DSS layers to reduce computational costs during inference. By leveraging
parameters from the reduced model, we refine the initialization process of S4
models, outperforming the widely used Skew-HiPPO initialization in terms of
performance. Numerical experiments demonstrate that our trained S4 models with
DSS layers surpass conventionally trained models in accuracy and efficiency
metrics. Furthermore, our observations reveal a positive correlation: higher
accuracy in the original model consistently leads to increased accuracy in
models trained using our method, suggesting that our approach effectively
leverages the strengths of the original model.</div><div><a href='http://arxiv.org/abs/2402.15993v2'>2402.15993v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17698v1")'>Learning reduced-order Quadratic-Linear models in Process Engineering
  using Operator Inference</div>
<div id='2402.17698v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:21:10Z</div><div>Authors: Ion Victor Gosea, Luisa Peterson, Pawan Goyal, Jens Bremer, Kai Sundmacher, Peter Benner</div><div style='padding-top: 10px; width: 80ex'>In this work, we address the challenge of efficiently modeling dynamical
systems in process engineering. We use reduced-order model learning,
specifically operator inference. This is a non-intrusive, data-driven method
for learning dynamical systems from time-domain data. The application in our
study is carbon dioxide methanation, an important reaction within the
Power-to-X framework, to demonstrate its potential. The numerical results show
the ability of the reduced-order models constructed with operator inference to
provide a reduced yet accurate surrogate solution. This represents an important
milestone towards the implementation of fast and reliable digital twin
architectures.</div><div><a href='http://arxiv.org/abs/2402.17698v1'>2402.17698v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13836v1")'>Machine learning for industrial sensing and control: A survey and
  practical perspective</div>
<div id='2401.13836v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T22:27:04Z</div><div>Authors: Nathan P. Lawrence, Seshu Kumar Damarla, Jong Woo Kim, Aditya Tulsyan, Faraz Amjad, Kai Wang, Benoit Chachuat, Jong Min Lee, Biao Huang, R. Bhushan Gopaluni</div><div style='padding-top: 10px; width: 80ex'>With the rise of deep learning, there has been renewed interest within the
process industries to utilize data on large-scale nonlinear sensing and control
problems. We identify key statistical and machine learning techniques that have
seen practical success in the process industries. To do so, we start with
hybrid modeling to provide a methodological framework underlying core
application areas: soft sensing, process optimization, and control. Soft
sensing contains a wealth of industrial applications of statistical and machine
learning methods. We quantitatively identify research trends, allowing insight
into the most successful techniques in practice.
  We consider two distinct flavors for data-driven optimization and control:
hybrid modeling in conjunction with mathematical programming techniques and
reinforcement learning. Throughout these application areas, we discuss their
respective industrial requirements and challenges.
  A common challenge is the interpretability and efficiency of purely
data-driven methods. This suggests a need to carefully balance deep learning
techniques with domain knowledge. As a result, we highlight ways prior
knowledge may be integrated into industrial machine learning applications. The
treatment of methods, problems, and applications presented here is poised to
inform and inspire practitioners and researchers to develop impactful
data-driven sensing, optimization, and control solutions in the process
industries.</div><div><a href='http://arxiv.org/abs/2401.13836v1'>2401.13836v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13219v1")'>Analyzing Operator States and the Impact of AI-Enhanced Decision Support
  in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning
  Framework for Intervention Strategies</div>
<div id='2402.13219v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:31:27Z</div><div>Authors: Ammar N. Abbas, Chidera W. Amazu, Joseph Mietkiewicz, Houda Briwa, Andres Alonzo Perez, Gabriele Baldissone, Micaela Demichela, Georgios G. Chasparis, John D. Kelleher, Maria Chiara Leva</div><div style='padding-top: 10px; width: 80ex'>In complex industrial and chemical process control rooms, effective
decision-making is crucial for safety and efficiency. The experiments in this
paper evaluate the impact and applications of an AI-based decision support
system integrated into an improved human-machine interface, using dynamic
influence diagrams, a hidden Markov model, and deep reinforcement learning. The
enhanced support system aims to reduce operator workload, improve situational
awareness, and provide different intervention strategies to the operator
adapted to the current state of both the system and human performance. Such a
system can be particularly useful in cases of information overload when many
alarms and inputs are presented all within the same time window, or for junior
operators during training. A comprehensive cross-data analysis was conducted,
involving 47 participants and a diverse range of data sources such as
smartwatch metrics, eye-tracking data, process logs, and responses from
questionnaires. The results indicate interesting insights regarding the
effectiveness of the approach in aiding decision-making, decreasing perceived
workload, and increasing situational awareness for the scenarios considered.
Additionally, the results provide valuable insights to compare differences
between styles of information gathering when using the system by individual
participants. These findings are particularly relevant when predicting the
overall performance of the individual participant and their capacity to
successfully handle a plant upset and the alarms connected to it using process
and human-machine interaction logs in real-time. These predictions enable the
development of more effective intervention strategies.</div><div><a href='http://arxiv.org/abs/2402.13219v1'>2402.13219v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10794v1")'>Deep Reinforcement Learning Empowered Activity-Aware Dynamic Health
  Monitoring Systems</div>
<div id='2401.10794v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:26:35Z</div><div>Authors: Ziqiaing Ye, Yulan Gao, Yue Xiao, Zehui Xiong, Dusit Niyato</div><div style='padding-top: 10px; width: 80ex'>In smart healthcare, health monitoring utilizes diverse tools and
technologies to analyze patients' real-time biosignal data, enabling immediate
actions and interventions. Existing monitoring approaches were designed on the
premise that medical devices track several health metrics concurrently,
tailored to their designated functional scope. This means that they report all
relevant health values within that scope, which can result in excess resource
use and the gathering of extraneous data due to monitoring irrelevant health
metrics. In this context, we propose Dynamic Activity-Aware Health Monitoring
strategy (DActAHM) for striking a balance between optimal monitoring
performance and cost efficiency, a novel framework based on Deep Reinforcement
Learning (DRL) and SlowFast Model to ensure precise monitoring based on users'
activities. Specifically, with the SlowFast Model, DActAHM efficiently
identifies individual activities and captures these results for enhanced
processing. Subsequently, DActAHM refines health metric monitoring in response
to the identified activity by incorporating a DRL framework. Extensive
experiments comparing DActAHM against three state-of-the-art approaches
demonstrate it achieves 27.3% higher gain than the best-performing baseline
that fixes monitoring actions over timeline.</div><div><a href='http://arxiv.org/abs/2401.10794v1'>2401.10794v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12939v1")'>Discovering Behavioral Modes in Deep Reinforcement Learning Policies
  Using Trajectory Clustering in Latent Space</div>
<div id='2402.12939v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T11:50:50Z</div><div>Authors: Sindre Benjamin Remman, Anastasios M. Lekkas</div><div style='padding-top: 10px; width: 80ex'>Understanding the behavior of deep reinforcement learning (DRL) agents is
crucial for improving their performance and reliability. However, the
complexity of their policies often makes them challenging to understand. In
this paper, we introduce a new approach for investigating the behavior modes of
DRL policies, which involves utilizing dimensionality reduction and trajectory
clustering in the latent space of neural networks. Specifically, we use
Pairwise Controlled Manifold Approximation Projection (PaCMAP) for
dimensionality reduction and TRACLUS for trajectory clustering to analyze the
latent space of a DRL policy trained on the Mountain Car control task. Our
methodology helps identify diverse behavior patterns and suboptimal choices by
the policy, thus allowing for targeted improvements. We demonstrate how our
approach, combined with domain knowledge, can enhance a policy's performance in
specific regions of the state space.</div><div><a href='http://arxiv.org/abs/2402.12939v1'>2402.12939v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16236v1")'>Effective Communication with Dynamic Feature Compression</div>
<div id='2401.16236v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T15:35:05Z</div><div>Authors: Pietro Talli, Francesco Pase, Federico Chiariotti, Andrea Zanella, Michele Zorzi</div><div style='padding-top: 10px; width: 80ex'>The remote wireless control of industrial systems is one of the major use
cases for 5G and beyond systems: in these cases, the massive amounts of sensory
information that need to be shared over the wireless medium may overload even
high-capacity connections. Consequently, solving the effective communication
problem by optimizing the transmission strategy to discard irrelevant
information can provide a significant advantage, but is often a very complex
task. In this work, we consider a prototypal system in which an observer must
communicate its sensory data to a robot controlling a task (e.g., a mobile
robot in a factory). We then model it as a remote Partially Observable Markov
Decision Process (POMDP), considering the effect of adopting semantic and
effective communication-oriented solutions on the overall system performance.
We split the communication problem by considering an ensemble Vector Quantized
Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement
Learning (DRL) agent to dynamically adapt the quantization level, considering
both the current state of the environment and the memory of past messages. We
tested the proposed approach on the well-known CartPole reference control
problem, obtaining a significant performance increase over traditional
approaches.</div><div><a href='http://arxiv.org/abs/2401.16236v1'>2401.16236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02508v1")'>Towards an Adaptable and Generalizable Optimization Engine in Decision
  and Control: A Meta Reinforcement Learning Approach</div>
<div id='2401.02508v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T19:41:33Z</div><div>Authors: Sungwook Yang, Chaoying Pei, Ran Dai, Chuangchuang Sun</div><div style='padding-top: 10px; width: 80ex'>Sampling-based model predictive control (MPC) has found significant success
in optimal control problems with non-smooth system dynamics and cost function.
Many machine learning-based works proposed to improve MPC by a) learning or
fine-tuning the dynamics/ cost function, or b) learning to optimize for the
update of the MPC controllers. For the latter, imitation learning-based
optimizers are trained to update the MPC controller by mimicking the expert
demonstrations, which, however, are expensive or even unavailable. More
significantly, many sequential decision-making problems are in non-stationary
environments, requiring that an optimizer should be adaptable and generalizable
to update the MPC controller for solving different tasks. To address those
issues, we propose to learn an optimizer based on meta-reinforcement learning
(RL) to update the controllers. This optimizer does not need expert
demonstration and can enable fast adaptation (e.g., few-shots) when it is
deployed in unseen control tasks. Experimental results validate the
effectiveness of the learned optimizer regarding fast adaptation.</div><div><a href='http://arxiv.org/abs/2401.02508v1'>2401.02508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14425v1")'>Task-optimal data-driven surrogate models for eNMPC via differentiable
  simulation and optimization</div>
<div id='2403.14425v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:28:43Z</div><div>Authors: Daniel Mayfrank, Na Young Ahn, Alexander Mitsos, Manuel Dahmen</div><div style='padding-top: 10px; width: 80ex'>We present a method for end-to-end learning of Koopman surrogate models for
optimal performance in control. In contrast to previous contributions that
employ standard reinforcement learning (RL) algorithms, we use a training
algorithm that exploits the potential differentiability of environments based
on mechanistic simulation models. We evaluate the performance of our method by
comparing it to that of other controller type and training algorithm
combinations on a literature known eNMPC case study. Our method exhibits
superior performance on this problem, thereby constituting a promising avenue
towards more capable controllers that employ dynamic surrogate models.</div><div><a href='http://arxiv.org/abs/2403.14425v1'>2403.14425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08848v1")'>Hybrid Inverse Reinforcement Learning</div>
<div id='2402.08848v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T23:29:09Z</div><div>Authors: Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J. Andrew Bagnell, Sanjiban Choudhury</div><div style='padding-top: 10px; width: 80ex'>The inverse reinforcement learning approach to imitation learning is a
double-edged sword. On the one hand, it can enable learning from a smaller
number of expert demonstrations with more robustness to error compounding than
behavioral cloning approaches. On the other hand, it requires that the learner
repeatedly solve a computationally expensive reinforcement learning (RL)
problem. Often, much of this computation is wasted searching over policies very
dissimilar to the expert's. In this work, we propose using hybrid RL --
training on a mixture of online and expert data -- to curtail unnecessary
exploration. Intuitively, the expert data focuses the learner on good states
during training, which reduces the amount of exploration required to compute a
strong policy. Notably, such an approach doesn't need the ability to reset the
learner to arbitrary states in the environment, a requirement of prior work in
efficient inverse RL. More formally, we derive a reduction from inverse RL to
expert-competitive RL (rather than globally optimal RL) that allows us to
dramatically reduce interaction during the inner policy search loop while
maintaining the benefits of the IRL approach. This allows us to derive both
model-free and model-based hybrid inverse RL algorithms with strong policy
performance guarantees. Empirically, we find that our approaches are
significantly more sample efficient than standard inverse RL and several other
baselines on a suite of continuous control tasks.</div><div><a href='http://arxiv.org/abs/2402.08848v1'>2402.08848v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10765v1")'>Policy Learning for Off-Dynamics RL with Deficient Support</div>
<div id='2402.10765v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T15:39:51Z</div><div>Authors: Linh Le Pham Van, Hung The Tran, Sunil Gupta</div><div style='padding-top: 10px; width: 80ex'>Reinforcement Learning (RL) can effectively learn complex policies. However,
learning these policies often demands extensive trial-and-error interactions
with the environment. In many real-world scenarios, this approach is not
practical due to the high costs of data collection and safety concerns. As a
result, a common strategy is to transfer a policy trained in a low-cost, rapid
source simulator to a real-world target environment. However, this process
poses challenges. Simulators, no matter how advanced, cannot perfectly
replicate the intricacies of the real world, leading to dynamics discrepancies
between the source and target environments. Past research posited that the
source domain must encompass all possible target transitions, a condition we
term full support. However, expecting full support is often unrealistic,
especially in scenarios where significant dynamics discrepancies arise. In this
paper, our emphasis shifts to addressing large dynamics mismatch adaptation. We
move away from the stringent full support condition of earlier research,
focusing instead on crafting an effective policy for the target domain. Our
proposed approach is simple but effective. It is anchored in the central
concepts of the skewing and extension of source support towards target support
to mitigate support deficiencies. Through comprehensive testing on a varied set
of benchmarks, our method's efficacy stands out, showcasing notable
improvements over previous techniques.</div><div><a href='http://arxiv.org/abs/2402.10765v1'>2402.10765v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16349v1")'>C-GAIL: Stabilizing Generative Adversarial Imitation Learning with
  Control Theory</div>
<div id='2402.16349v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:07:00Z</div><div>Authors: Tianjiao Luo, Tim Pearce, Huayu Chen, Jianfei Chen, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>Generative Adversarial Imitation Learning (GAIL) trains a generative policy
to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to
optimize a reward signal derived from a GAN-like discriminator. A major
drawback of GAIL is its training instability - it inherits the complex training
dynamics of GANs, and the distribution shift introduced by RL. This can cause
oscillations during training, harming its sample efficiency and final policy
performance. Recent work has shown that control theory can help with the
convergence of a GAN's training. This paper extends this line of work,
conducting a control-theoretic analysis of GAIL and deriving a novel controller
that not only pushes GAIL to the desired equilibrium but also achieves
asymptotic stability in a 'one-step' setting. Based on this, we propose a
practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled
variant is able to speed up the rate of convergence, reduce the range of
oscillation and match the expert's distribution more closely both for vanilla
GAIL and GAIL-DAC.</div><div><a href='http://arxiv.org/abs/2402.16349v1'>2402.16349v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07369v1")'>CoVO-MPC: Theoretical Analysis of Sampling-based MPC and Optimal
  Covariance Design</div>
<div id='2401.07369v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T21:10:59Z</div><div>Authors: Zeji Yi, Chaoyi Pan, Guanqi He, Guannan Qu, Guanya Shi</div><div style='padding-top: 10px; width: 80ex'>Sampling-based Model Predictive Control (MPC) has been a practical and
effective approach in many domains, notably model-based reinforcement learning,
thanks to its flexibility and parallelizability. Despite its appealing
empirical performance, the theoretical understanding, particularly in terms of
convergence analysis and hyperparameter tuning, remains absent. In this paper,
we characterize the convergence property of a widely used sampling-based MPC
method, Model Predictive Path Integral Control (MPPI). We show that MPPI enjoys
at least linear convergence rates when the optimization is quadratic, which
covers time-varying LQR systems. We then extend to more general nonlinear
systems. Our theoretical analysis directly leads to a novel sampling-based MPC
algorithm, CoVariance-Optimal MPC (CoVo-MPC) that optimally schedules the
sampling covariance to optimize the convergence rate. Empirically, CoVo-MPC
significantly outperforms standard MPPI by 43-54% in both simulations and
real-world quadrotor agile control tasks. Videos and Appendices are available
at \url{https://lecar-lab.github.io/CoVO-MPC/}.</div><div><a href='http://arxiv.org/abs/2401.07369v1'>2401.07369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15508v1")'>Proto-MPC: An Encoder-Prototype-Decoder Approach for Quadrotor Control
  in Challenging Winds</div>
<div id='2401.15508v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T21:32:04Z</div><div>Authors: Yuliang Gu, Sheng Cheng, Naira Hovakimyan</div><div style='padding-top: 10px; width: 80ex'>Quadrotors are increasingly used in the evolving field of aerial robotics for
their agility and mechanical simplicity. However, inherent uncertainties, such
as aerodynamic effects coupled with quadrotors' operation in dynamically
changing environments, pose significant challenges for traditional, nominal
model-based control designs. We propose a multi-task meta-learning method
called Encoder-Prototype-Decoder (EPD), which has the advantage of effectively
balancing shared and distinctive representations across diverse training tasks.
Subsequently, we integrate the EPD model into a model predictive control
problem (Proto-MPC) to enhance the quadrotor's ability to adapt and operate
across a spectrum of dynamically changing tasks with an efficient online
implementation. We validate the proposed method in simulations, which
demonstrates Proto-MPC's robust performance in trajectory tracking of a
quadrotor being subject to static and spatially varying side winds.</div><div><a href='http://arxiv.org/abs/2401.15508v1'>2401.15508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10036v1")'>Predictive Linear Online Tracking for Unknown Targets</div>
<div id='2402.10036v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T15:59:59Z</div><div>Authors: Anastasios Tsiamis, Aren Karapetyan, Yueshan Li, Efe C. Balta, John Lygeros</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the problem of online tracking in linear control
systems, where the objective is to follow a moving target. Unlike classical
tracking control, the target is unknown, non-stationary, and its state is
revealed sequentially, thus, fitting the framework of online non-stochastic
control. We consider the case of quadratic costs and propose a new algorithm,
called predictive linear online tracking (PLOT). The algorithm uses recursive
least squares with exponential forgetting to learn a time-varying dynamic model
of the target. The learned model is used in the optimal policy under the
framework of receding horizon control. We show the dynamic regret of PLOT
scales with $\mathcal{O}(\sqrt{TV_T})$, where $V_T$ is the total variation of
the target dynamics and $T$ is the time horizon. Unlike prior work, our
theoretical results hold for non-stationary targets. We implement PLOT on a
real quadrotor and provide open-source software, thus, showcasing one of the
first successful applications of online control methods on real hardware.</div><div><a href='http://arxiv.org/abs/2402.10036v1'>2402.10036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08999v1")'>Continuous Time Continuous Space Homeostatic Reinforcement Learning
  (CTCS-HRRL) : Towards Biological Self-Autonomous Agent</div>
<div id='2401.08999v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T06:29:34Z</div><div>Authors: Hugo Laurencon, Yesoda Bhargava, Riddhi Zantye, Charbel-Raphaël Ségerie, Johann Lussange, Veeky Baths, Boris Gutkin</div><div style='padding-top: 10px; width: 80ex'>Homeostasis is a biological process by which living beings maintain their
internal balance. Previous research suggests that homeostasis is a learned
behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning
(HRRL) framework attempts to explain this learned homeostatic behavior by
linking Drive Reduction Theory and Reinforcement Learning. This linkage has
been proven in the discrete time-space, but not in the continuous time-space.
In this work, we advance the HRRL framework to a continuous time-space
environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL)
framework. We achieve this by designing a model that mimics the homeostatic
mechanisms in a real-world biological agent. This model uses the
Hamilton-Jacobian Bellman Equation, and function approximation based on neural
networks and Reinforcement Learning. Through a simulation-based experiment we
demonstrate the efficacy of this model and uncover the evidence linked to the
agent's ability to dynamically choose policies that favor homeostasis in a
continuously changing internal-state milieu. Results of our experiments
demonstrate that agent learns homeostatic behaviour in a CTCS environment,
making CTCS-HRRL a promising framework for modellng animal dynamics and
decision-making.</div><div><a href='http://arxiv.org/abs/2401.08999v1'>2401.08999v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00675v1")'>Reusing Historical Trajectories in Natural Policy Gradient via
  Importance Sampling: Convergence and Convergence Rate</div>
<div id='2403.00675v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T17:08:30Z</div><div>Authors: Yifan Lin, Yuhao Wang, Enlu Zhou</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning provides a mathematical framework for learning-based
control, whose success largely depends on the amount of data it can utilize.
The efficient utilization of historical trajectories obtained from previous
policies is essential for expediting policy optimization. Empirical evidence
has shown that policy gradient methods based on importance sampling work well.
However, existing literature often neglect the interdependence between
trajectories from different iterations, and the good empirical performance
lacks a rigorous theoretical justification. In this paper, we study a variant
of the natural policy gradient method with reusing historical trajectories via
importance sampling. We show that the bias of the proposed estimator of the
gradient is asymptotically negligible, the resultant algorithm is convergent,
and reusing past trajectories helps improve the convergence rate. We further
apply the proposed estimator to popular policy optimization algorithms such as
trust region policy optimization. Our theoretical results are verified on
classical benchmarks.</div><div><a href='http://arxiv.org/abs/2403.00675v1'>2403.00675v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11654v1")'>Model-Free $μ$-Synthesis: A Nonsmooth Optimization Perspective</div>
<div id='2402.11654v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T17:17:17Z</div><div>Authors: Darioush Keivan, Xingang Guo, Peter Seiler, Geir Dullerud, Bin Hu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we revisit model-free policy search on an important robust
control benchmark, namely $\mu$-synthesis. In the general output-feedback
setting, there do not exist convex formulations for this problem, and hence
global optimality guarantees are not expected. Apkarian (2011) presented a
nonconvex nonsmooth policy optimization approach for this problem, and achieved
state-of-the-art design results via using subgradient-based policy search
algorithms which generate update directions in a model-based manner. Despite
the lack of convexity and global optimality guarantees, these subgradient-based
policy search methods have led to impressive numerical results in practice.
Built upon such a policy optimization persepctive, our paper extends these
subgradient-based search methods to a model-free setting. Specifically, we
examine the effectiveness of two model-free policy optimization strategies: the
model-free non-derivative sampling method and the zeroth-order policy search
with uniform smoothing. We performed an extensive numerical study to
demonstrate that both methods consistently replicate the design outcomes
achieved by their model-based counterparts. Additionally, we provide some
theoretical justifications showing that convergence guarantees to stationary
points can be established for our model-free $\mu$-synthesis under some
assumptions related to the coerciveness of the cost function. Overall, our
results demonstrate that derivative-free policy optimization offers a
competitive and viable approach for solving general output-feedback
$\mu$-synthesis problems in the model-free setting.</div><div><a href='http://arxiv.org/abs/2402.11654v1'>2402.11654v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10810v1")'>Double Duality: Variational Primal-Dual Policy Optimization for
  Constrained Reinforcement Learning</div>
<div id='2402.10810v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:35:18Z</div><div>Authors: Zihao Li, Boyi Liu, Zhuoran Yang, Zhaoran Wang, Mengdi Wang</div><div style='padding-top: 10px; width: 80ex'>We study the Constrained Convex Markov Decision Process (MDP), where the goal
is to minimize a convex functional of the visitation measure, subject to a
convex constraint. Designing algorithms for a constrained convex MDP faces
several challenges, including (1) handling the large state space, (2) managing
the exploration/exploitation tradeoff, and (3) solving the constrained
optimization where the objective and the constraint are both nonlinear
functions of the visitation measure. In this work, we present a model-based
algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which
Lagrangian and Fenchel duality are implemented to reformulate the original
constrained problem into an unconstrained primal-dual optimization. Moreover,
the primal variables are updated by model-based value iteration following the
principle of Optimism in the Face of Uncertainty (OFU), while the dual
variables are updated by gradient ascent. Moreover, by embedding the visitation
measure into a finite-dimensional space, we can handle large state spaces by
incorporating function approximation. Two notable examples are (1) Kernelized
Nonlinear Regulators and (2) Low-rank MDPs. We prove that with an optimistic
planning oracle, our algorithm achieves sublinear regret and constraint
violation in both cases and can attain the globally optimal policy of the
original constrained problem.</div><div><a href='http://arxiv.org/abs/2402.10810v1'>2402.10810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15776v2")'>Truly No-Regret Learning in Constrained MDPs</div>
<div id='2402.15776v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T09:47:46Z</div><div>Authors: Adrian Müller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao He</div><div style='padding-top: 10px; width: 80ex'>Constrained Markov decision processes (CMDPs) are a common way to model
safety constraints in reinforcement learning. State-of-the-art methods for
efficiently solving CMDPs are based on primal-dual algorithms. For these
algorithms, all currently known regret bounds allow for error cancellations --
one can compensate for a constraint violation in one round with a strict
constraint satisfaction in another. This makes the online learning process
unsafe since it only guarantees safety for the final (mixture) policy but not
during learning. As Efroni et al. (2020) pointed out, it is an open question
whether primal-dual algorithms can provably achieve sublinear regret if we do
not allow error cancellations. In this paper, we give the first affirmative
answer. We first generalize a result on last-iterate convergence of regularized
primal-dual schemes to CMDPs with multiple constraints. Building upon this
insight, we propose a model-based primal-dual algorithm to learn in an unknown
CMDP. We prove that our algorithm achieves sublinear regret without error
cancellations.</div><div><a href='http://arxiv.org/abs/2402.15776v2'>2402.15776v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17780v2")'>A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with
  Uniform PAC Guarantees</div>
<div id='2401.17780v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T12:23:24Z</div><div>Authors: Toshinori Kitamura, Tadashi Kozuno, Masahiro Kato, Yuki Ichihara, Soichiro Nishimori, Akiyoshi Sannai, Sho Sonoda, Wataru Kumagai, Yutaka Matsuo</div><div style='padding-top: 10px; width: 80ex'>We study a primal-dual reinforcement learning (RL) algorithm for the online
constrained Markov decision processes (CMDP) problem, wherein the agent
explores an optimal policy that maximizes return while satisfying constraints.
Despite its widespread practical use, the existing theoretical literature on
primal-dual RL algorithms for this problem only provides sublinear regret
guarantees and fails to ensure convergence to optimal policies. In this paper,
we introduce a novel policy gradient primal-dual algorithm with uniform
probably approximate correctness (Uniform-PAC) guarantees, simultaneously
ensuring convergence to optimal policies, sublinear regret, and polynomial
sample complexity for any target accuracy. Notably, this represents the first
Uniform-PAC algorithm for the online CMDP problem. In addition to the
theoretical guarantees, we empirically demonstrate in a simple CMDP that our
algorithm converges to optimal policies, while an existing algorithm exhibits
oscillatory performance and constraint violation.</div><div><a href='http://arxiv.org/abs/2401.17780v2'>2401.17780v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02042v2")'>Learning General Parameterized Policies for Infinite Horizon Average
  Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm</div>
<div id='2402.02042v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:35:58Z</div><div>Authors: Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal</div><div style='padding-top: 10px; width: 80ex'>This paper explores the realm of infinite horizon average reward Constrained
Markov Decision Processes (CMDP). To the best of our knowledge, this work is
the first to delve into the regret and constraint violation analysis of average
reward CMDPs with a general policy parametrization. To address this challenge,
we propose a primal dual based policy gradient algorithm that adeptly manages
the constraints while ensuring a low regret guarantee toward achieving a global
optimal policy. In particular, we demonstrate that our proposed algorithm
achieves $\tilde{\mathcal{O}}({T}^{4/5})$ objective regret and
$\tilde{\mathcal{O}}({T}^{4/5})$ constraint violation bounds.</div><div><a href='http://arxiv.org/abs/2402.02042v2'>2402.02042v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06806v1")'>On the Global Convergence of Policy Gradient in Average Reward Markov
  Decision Processes</div>
<div id='2403.06806v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:25:03Z</div><div>Authors: Navdeep Kumar, Yashaswini Murthy, Itai Shufaro, Kfir Y. Levy, R. Srikant, Shie Mannor</div><div style='padding-top: 10px; width: 80ex'>We present the first finite time global convergence analysis of policy
gradient in the context of infinite horizon average reward Markov decision
processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite
state and action spaces. Our analysis shows that the policy gradient iterates
converge to the optimal policy at a sublinear rate of
$O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$
regret, where $T$ represents the number of iterations. Prior work on
performance bounds for discounted reward MDPs cannot be extended to average
reward MDPs because the bounds grow proportional to the fifth power of the
effective horizon. Thus, our primary contribution is in proving that the policy
gradient algorithm converges for average-reward MDPs and in obtaining
finite-time performance guarantees. In contrast to the existing discounted
reward performance bounds, our performance bounds have an explicit dependence
on constants that capture the complexity of the underlying MDP. Motivated by
this observation, we reexamine and improve the existing performance bounds for
discounted reward MDPs. We also present simulations to empirically evaluate the
performance of average reward policy gradient algorithm.</div><div><a href='http://arxiv.org/abs/2403.06806v1'>2403.06806v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05738v1")'>Provable Policy Gradient Methods for Average-Reward Markov Potential
  Games</div>
<div id='2403.05738v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T00:20:33Z</div><div>Authors: Min Cheng, Ruida Zhou, P. R. Kumar, Chao Tian</div><div style='padding-top: 10px; width: 80ex'>We study Markov potential games under the infinite horizon average reward
criterion. Most previous studies have been for discounted rewards. We prove
that both algorithms based on independent policy gradient and independent
natural policy gradient converge globally to a Nash equilibrium for the average
reward criterion. To set the stage for gradient-based methods, we first
establish that the average reward is a smooth function of policies and provide
sensitivity bounds for the differential value functions, under certain
conditions on ergodicity and the second largest eigenvalue of the underlying
Markov decision process (MDP). We prove that three algorithms, policy gradient,
proximal-Q, and natural policy gradient (NPG), converge to an $\epsilon$-Nash
equilibrium with time complexity $O(\frac{1}{\epsilon^2})$, given a
gradient/differential Q function oracle. When policy gradients have to be
estimated, we propose an algorithm with
$\tilde{O}(\frac{1}{\min_{s,a}\pi(a|s)\delta})$ sample complexity to achieve
$\delta$ approximation error w.r.t~the $\ell_2$ norm. Equipped with the
estimator, we derive the first sample complexity analysis for a policy gradient
ascent algorithm, featuring a sample complexity of $\tilde{O}(1/\epsilon^5)$.
Simulation studies are presented.</div><div><a href='http://arxiv.org/abs/2403.05738v1'>2403.05738v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08813v1")'>Model approximation in MDPs with unbounded per-step cost</div>
<div id='2402.08813v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T21:36:30Z</div><div>Authors: Berk Bozkurt, Aditya Mahajan, Ashutosh Nayyar, Yi Ouyang</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of designing a control policy for an infinite-horizon
discounted cost Markov decision process $\mathcal{M}$ when we only have access
to an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy
$\hat{\pi}^{\star}$ of the approximate model perform when used in the original
model $\mathcal{M}$? We answer this question by bounding a weighted norm of the
difference between the value function of $\hat{\pi}^\star $ when used in
$\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extend
our results and obtain potentially tighter upper bounds by considering affine
transformations of the per-step cost. We further provide upper bounds that
explicitly depend on the weighted distance between cost functions and weighted
distance between transition kernels of the original and approximate models. We
present examples to illustrate our results.</div><div><a href='http://arxiv.org/abs/2402.08813v1'>2402.08813v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01431v2")'>Approximate Control for Continuous-Time POMDPs</div>
<div id='2402.01431v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:20:04Z</div><div>Authors: Yannick Eich, Bastian Alt, Heinz Koeppl</div><div style='padding-top: 10px; width: 80ex'>This work proposes a decision-making framework for partially observable
systems in continuous time with discrete state and action spaces. As optimal
decision-making becomes intractable for large state spaces we employ
approximation methods for the filtering and the control problem that scale well
with an increasing number of states. Specifically, we approximate the
high-dimensional filtering distribution by projecting it onto a parametric
family of distributions, and integrate it into a control heuristic based on the
fully observable system to obtain a scalable policy. We demonstrate the
effectiveness of our approach on several partially observed systems, including
queueing systems and chemical reaction networks.</div><div><a href='http://arxiv.org/abs/2402.01431v2'>2402.01431v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16324v1")'>Achieving $\tilde{O}(1/ε)$ Sample Complexity for Constrained
  Markov Decision Process</div>
<div id='2402.16324v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T06:08:25Z</div><div>Authors: Jiashuo Jiang, Yinyu Ye</div><div style='padding-top: 10px; width: 80ex'>We consider the reinforcement learning problem for the constrained Markov
decision process (CMDP), which plays a central role in satisfying safety or
resource constraints in sequential learning and decision-making. In this
problem, we are given finite resources and a MDP with unknown transition
probabilities. At each stage, we take an action, collecting a reward and
consuming some resources, all assumed to be unknown and need to be learned over
time. In this work, we take the first step towards deriving optimal
problem-dependent guarantees for the CMDP problems. We derive a logarithmic
regret bound, which translates into a
$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound,
with $\kappa$ being a problem-dependent parameter, yet independent of
$\epsilon$. Our sample complexity bound improves upon the state-of-art
$O(1/\epsilon^2)$ sample complexity for CMDP problems established in the
previous literature, in terms of the dependency on $\epsilon$. To achieve this
advance, we develop a new framework for analyzing CMDP problems. To be
specific, our algorithm operates in the primal space and we resolve the primal
LP for the CMDP problem at each period in an online manner, with
\textit{adaptive} remaining resource capacities. The key elements of our
algorithm are: i). an eliminating procedure that characterizes one optimal
basis of the primal LP, and; ii) a resolving procedure that is adaptive to the
remaining resources and sticks to the characterized optimal basis.</div><div><a href='http://arxiv.org/abs/2402.16324v1'>2402.16324v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08991v2")'>Towards Robust Model-Based Reinforcement Learning Against Adversarial
  Corruption</div>
<div id='2402.08991v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T07:27:30Z</div><div>Authors: Chenlu Ye, Jiafan He, Quanquan Gu, Tong Zhang</div><div style='padding-top: 10px; width: 80ex'>This study tackles the challenges of adversarial corruption in model-based
reinforcement learning (RL), where the transition dynamics can be corrupted by
an adversary. Existing studies on corruption-robust RL mostly focus on the
setting of model-free RL, where robust least-square regression is often
employed for value function estimation. However, these techniques cannot be
directly applied to model-based RL. In this paper, we focus on model-based RL
and take the maximum likelihood estimation (MLE) approach to learn transition
model. Our work encompasses both online and offline settings. In the online
setting, we introduce an algorithm called corruption-robust optimistic MLE
(CR-OMLE), which leverages total-variation (TV)-based information ratios as
uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of
$\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative
corruption level after $T$ episodes. We also prove a lower bound to show that
the additive dependence on $C$ is optimal. We extend our weighting technique to
the offline setting, and propose an algorithm named corruption-robust
pessimistic MLE (CR-PMLE). Under a uniform coverage condition, CR-PMLE exhibits
suboptimality worsened by $\mathcal{O}(C/n)$, nearly matching the lower bound.
To the best of our knowledge, this is the first work on corruption-robust
model-based RL algorithms with provable guarantees.</div><div><a href='http://arxiv.org/abs/2402.08991v2'>2402.08991v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04050v1")'>Belief-Enriched Pessimistic Q-Learning against Adversarial State
  Perturbations</div>
<div id='2403.04050v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T20:52:49Z</div><div>Authors: Xiaolin Sun, Zizhan Zheng</div><div style='padding-top: 10px; width: 80ex'>Reinforcement learning (RL) has achieved phenomenal success in various
domains. However, its data-driven nature also introduces new vulnerabilities
that can be exploited by malicious opponents. Recent work shows that a
well-trained RL agent can be easily manipulated by strategically perturbing its
state observations at the test stage. Existing solutions either introduce a
regularization term to improve the smoothness of the trained policy against
perturbations or alternatively train the agent's policy and the attacker's
policy. However, the former does not provide sufficient protection against
strong attacks, while the latter is computationally prohibitive for large
environments. In this work, we propose a new robust RL algorithm for deriving a
pessimistic policy to safeguard against an agent's uncertainty about true
states. This approach is further enhanced with belief state inference and
diffusion-based state purification to reduce uncertainty. Empirical results
show that our approach obtains superb performance under strong attacks and has
a comparable training overhead with regularization-based methods. Our code is
available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning.</div><div><a href='http://arxiv.org/abs/2403.04050v1'>2403.04050v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02165v1")'>Towards Optimal Adversarial Robust Q-learning with Bellman
  Infinity-error</div>
<div id='2402.02165v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T14:25:33Z</div><div>Authors: Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Yudong Hu, Tiande Guo, Shichen Liao</div><div style='padding-top: 10px; width: 80ex'>Establishing robust policies is essential to counter attacks or disturbances
affecting deep reinforcement learning (DRL) agents. Recent studies explore
state-adversarial robustness and suggest the potential lack of an optimal
robust policy (ORP), posing challenges in setting strict robustness
constraints. This work further investigates ORP: At first, we introduce a
consistency assumption of policy (CAP) stating that optimal actions in the
Markov decision process remain consistent with minor perturbations, supported
by empirical and theoretical evidence. Building upon CAP, we crucially prove
the existence of a deterministic and stationary ORP that aligns with the
Bellman optimal policy. Furthermore, we illustrate the necessity of
$L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding
clarifies the vulnerability of prior DRL algorithms that target the Bellman
optimal policy with $L^{1}$-norm and motivates us to train a Consistent
Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of
Bellman Infinity-error. The top-tier performance of CAR-DQN across various
benchmarks validates its practical effectiveness and reinforces the soundness
of our theoretical analysis.</div><div><a href='http://arxiv.org/abs/2402.02165v1'>2402.02165v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03357v1")'>Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers
  via Self-Imitation Learning</div>
<div id='2402.03357v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T06:05:01Z</div><div>Authors: Xiaofei Xu, Ke Deng, Michael Dann, Xiuzhen Zhang</div><div style='padding-top: 10px; width: 80ex'>This study aims to minimize the influence of fake news on social networks by
deploying debunkers to propagate true news. This is framed as a reinforcement
learning problem, where, at each stage, one user is selected to propagate true
news. A challenging issue is episodic reward where the "net" effect of
selecting individual debunkers cannot be discerned from the interleaving
information propagation on social networks, and only the collective effect from
mitigation efforts can be observed. Existing Self-Imitation Learning (SIL)
methods have shown promise in learning from episodic rewards, but are
ill-suited to the real-world application of fake news mitigation because of
their poor sample efficiency. To learn a more effective debunker selection
policy for fake news mitigation, this study proposes NAGASIL - Negative
sampling and state Augmented Generative Adversarial Self-Imitation Learning,
which consists of two improvements geared towards fake news mitigation:
learning from negative samples, and an augmented state representation to
capture the "real" environment state by integrating the current observed state
with the previous state-action pairs from the same campaign. Experiments on two
social networks show that NAGASIL yields superior performance to standard GASIL
and state-of-the-art fake news mitigation models.</div><div><a href='http://arxiv.org/abs/2402.03357v1'>2402.03357v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15102v1")'>Trajectory-wise Iterative Reinforcement Learning Framework for
  Auto-bidding</div>
<div id='2402.15102v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T05:20:23Z</div><div>Authors: Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, Fan Wu</div><div style='padding-top: 10px; width: 80ex'>In online advertising, advertisers participate in ad auctions to acquire ad
opportunities, often by utilizing auto-bidding tools provided by demand-side
platforms (DSPs). The current auto-bidding algorithms typically employ
reinforcement learning (RL). However, due to safety concerns, most RL-based
auto-bidding policies are trained in simulation, leading to a performance
degradation when deployed in online environments. To narrow this gap, we can
deploy multiple auto-bidding agents in parallel to collect a large interaction
dataset. Offline RL algorithms can then be utilized to train a new policy. The
trained policy can subsequently be deployed for further data collection,
resulting in an iterative training framework, which we refer to as iterative
offline RL. In this work, we identify the performance bottleneck of this
iterative offline RL framework, which originates from the ineffective
exploration and exploitation caused by the inherent conservatism of offline RL
algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration
and Exploitation (TEE), which introduces a novel data collecting and data
utilization method for iterative offline RL from a trajectory perspective.
Furthermore, to ensure the safety of online exploration while preserving the
dataset quality for TEE, we propose Safe Exploration by Adaptive Action
Selection (SEAS). Both offline experiments and real-world experiments on
Alibaba display advertising platform demonstrate the effectiveness of our
proposed method.</div><div><a href='http://arxiv.org/abs/2402.15102v1'>2402.15102v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11544v2")'>RL in Markov Games with Independent Function Approximation: Improved
  Sample Complexity Bound under the Local Access Model</div>
<div id='2403.11544v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T07:54:11Z</div><div>Authors: Junyi Fan, Yuxuan Han, Jialin Zeng, Jian-Feng Cai, Yang Wang, Yang Xiang, Jiheng Zhang</div><div style='padding-top: 10px; width: 80ex'>Efficiently learning equilibria with large state and action spaces in
general-sum Markov games while overcoming the curse of multi-agency is a
challenging problem. Recent works have attempted to solve this problem by
employing independent linear function classes to approximate the marginal
$Q$-value for each agent. However, existing sample complexity bounds under such
a framework have a suboptimal dependency on the desired accuracy $\varepsilon$
or the action space. In this work, we introduce a new algorithm,
Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local
access to the simulator, i.e., one can interact with the underlying environment
on the visited states. Up to a logarithmic dependence on the size of the state
space, Lin-Confident-FTRL learns $\epsilon$-CCE with a provable optimal
accuracy bound $O(\epsilon^{-2})$ and gets rids of the linear dependency on the
action space, while scaling polynomially with relevant problem parameters (such
as the number of agents and time horizon). Moreover, our analysis of
Linear-Confident-FTRL generalizes the virtual policy iteration technique in the
single-agent local planning literature, which yields a new computationally
efficient algorithm with a tighter sample complexity bound when assuming random
access to the simulator.</div><div><a href='http://arxiv.org/abs/2403.11544v2'>2403.11544v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17885v1")'>Independent Learning in Constrained Markov Potential Games</div>
<div id='2402.17885v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T20:57:35Z</div><div>Authors: Philip Jordan, Anas Barakat, Niao He</div><div style='padding-top: 10px; width: 80ex'>Constrained Markov games offer a formal mathematical framework for modeling
multi-agent reinforcement learning problems where the behavior of the agents is
subject to constraints. In this work, we focus on the recently introduced class
of constrained Markov Potential Games. While centralized algorithms have been
proposed for solving such constrained games, the design of converging
independent learning algorithms tailored for the constrained setting remains an
open question. We propose an independent policy gradient algorithm for learning
approximate constrained Nash equilibria: Each agent observes their own actions
and rewards, along with a shared state. Inspired by the optimization
literature, our algorithm performs proximal-point-like updates augmented with a
regularized constraint set. Each proximal step is solved inexactly using a
stochastic switching gradient algorithm. Notably, our algorithm can be
implemented independently without a centralized coordination mechanism
requiring turn-based agent updates. Under some technical constraint
qualification conditions, we establish convergence guarantees towards
constrained approximate Nash equilibria. We perform simulations to illustrate
our results.</div><div><a href='http://arxiv.org/abs/2402.17885v1'>2402.17885v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06566v1")'>Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field
  Games</div>
<div id='2401.06566v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T13:22:03Z</div><div>Authors: Berkay Anahtarci, Can Deha Kariksiz, Naci Saldi</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce the maximum casual entropy Inverse Reinforcement
Learning (IRL) problem for discrete-time mean-field games (MFGs) under an
infinite-horizon discounted-reward optimality criterion. The state space of a
typical agent is finite. Our approach begins with a comprehensive review of the
maximum entropy IRL problem concerning deterministic and stochastic Markov
decision processes (MDPs) in both finite and infinite-horizon scenarios.
Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a
non-convex optimization problem with respect to policies. Leveraging the linear
programming formulation of MDPs, we restructure this IRL problem into a convex
optimization problem and establish a gradient descent algorithm to compute the
optimal solution with a rate of convergence. Finally, we present a new
algorithm by formulating the MFG problem as a generalized Nash equilibrium
problem (GNEP), which is capable of computing the mean-field equilibrium (MFE)
for the forward RL problem. This method is employed to produce data for a
numerical example. We note that this novel algorithm is also applicable to
general MFE computations.</div><div><a href='http://arxiv.org/abs/2401.06566v1'>2401.06566v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11345v1")'>Independent RL for Cooperative-Competitive Agents: A Mean-Field
  Perspective</div>
<div id='2403.11345v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T21:11:55Z</div><div>Authors: Muhammad Aneeq uz Zaman, Alec Koppel, Mathieu Laurière, Tamer Başar</div><div style='padding-top: 10px; width: 80ex'>We address in this paper Reinforcement Learning (RL) among agents that are
grouped into teams such that there is cooperation within each team but
general-sum (non-zero sum) competition across different teams. To develop an RL
method that provably achieves a Nash equilibrium, we focus on a
linear-quadratic structure. Moreover, to tackle the non-stationarity induced by
multi-agent interactions in the finite population setting, we consider the case
where the number of agents within each team is infinite, i.e., the mean-field
setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We
characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard
invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE
for the finite population game where $M$ is a lower bound on the number of
agents in each team. These structural results motivate an algorithm called
Multi-player Receding-horizon Natural Policy Gradient (MRPG), where each team
minimizes its cumulative cost independently in a receding-horizon manner.
Despite the non-convexity of the problem, we establish that the resulting
algorithm converges to a global NE through a novel problem decomposition into
sub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs
(HJI) equations, in which independent natural policy gradient is shown to
exhibit linear convergence under time-independent diagonal dominance.
Experiments illuminate the merits of this approach in practice.</div><div><a href='http://arxiv.org/abs/2403.11345v1'>2403.11345v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07437v1")'>Learning Optimal Tax Design in Nonatomic Congestion Games</div>
<div id='2402.07437v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T06:32:53Z</div><div>Authors: Qiwen Cui, Maryam Fazel, Simon S. Du</div><div style='padding-top: 10px; width: 80ex'>We study how to learn the optimal tax design to maximize the efficiency in
nonatomic congestion games. It is known that self-interested behavior among the
players can damage the system's efficiency. Tax mechanisms is a common method
to alleviate this issue and induce socially optimal behavior. In this work, we
take the initial step for learning the optimal tax that can minimize the social
cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe
the equilibrium state under the enforced tax. Existing algorithms are not
applicable due to the exponentially large tax function space, nonexistence of
the gradient, and nonconvexity of the objective. To tackle these challenges,
our algorithm leverages several novel components: (1) piece-wise linear tax to
approximate the optimal tax; (2) an extra linear term to guarantee a strongly
convex potential function; (3) efficient subroutine to find the ``boundary''
tax. The algorithm can find an $\epsilon$-optimal tax with $O(\beta
F^2/\epsilon)$ sample complexity, where $\beta$ is the smoothness of the cost
function and $F$ is the number of facilities.</div><div><a href='http://arxiv.org/abs/2402.07437v1'>2402.07437v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07890v1")'>$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in
  Full-Information General-Sum Markov Games</div>
<div id='2403.07890v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:40:27Z</div><div>Authors: Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Tamer Başar</div><div style='padding-top: 10px; width: 80ex'>No-regret learning has a long history of being closely connected to game
theory. Recent works have devised uncoupled no-regret learning dynamics that,
when adopted by all the players in normal-form games, converge to various
equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a
significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret
learners. However, analogous convergence results are scarce in Markov games, a
more generic setting that lays the foundation for multi-agent reinforcement
learning. In this work, we close this gap by showing that the
optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with
appropriate value update procedures, can find
$\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in
full-information general-sum Markov games within $T$ iterations. Numerical
results are also included to corroborate our theoretical findings.</div><div><a href='http://arxiv.org/abs/2403.07890v1'>2403.07890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15240v1")'>Near-Optimal Policy Optimization for Correlated Equilibrium in
  General-Sum Markov Games</div>
<div id='2401.15240v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T23:13:47Z</div><div>Authors: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng</div><div style='padding-top: 10px; width: 80ex'>We study policy optimization algorithms for computing correlated equilibria
in multi-player general-sum Markov Games. Previous results achieve
$O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated
$O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated
equilibrium. In this paper, we improve both results significantly by providing
an uncoupled policy optimization algorithm that attains a near-optimal
$\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium.
Our algorithm is constructed by combining two main elements (i) smooth value
updates and (ii) the optimistic-follow-the-regularized-leader algorithm with
the log barrier regularizer.</div><div><a href='http://arxiv.org/abs/2401.15240v1'>2401.15240v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02954v2")'>Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form
  Game Approach</div>
<div id='2402.02954v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:33:05Z</div><div>Authors: Johan Peralez, Aurélien Delage, Olivier Buffet, Jilles S. Dibangoye</div><div style='padding-top: 10px; width: 80ex'>A recent theory shows that a multi-player decentralized partially observable
Markov decision process can be transformed into an equivalent single-player
game, enabling the application of \citeauthor{bellman}'s principle of
optimality to solve the single-player game by breaking it down into
single-stage subgames. However, this approach entangles the decision variables
of all players at each single-stage subgame, resulting in backups with a
double-exponential complexity. This paper demonstrates how to disentangle these
decision variables while maintaining optimality under hierarchical information
sharing, a prominent management style in our society. To achieve this, we apply
the principle of optimality to solve any single-stage subgame by breaking it
down further into smaller subgames, enabling us to make single-player decisions
at a time. Our approach reveals that extensive-form games always exist with
solutions to a single-stage subgame, significantly reducing time complexity.
Our experimental results show that the algorithms leveraging these findings can
scale up to much larger multi-player games without compromising optimality.</div><div><a href='http://arxiv.org/abs/2402.02954v2'>2402.02954v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00188v1")'>Impact of Decentralized Learning on Player Utilities in Stackelberg
  Games</div>
<div id='2403.00188v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T23:38:28Z</div><div>Authors: Kate Donahue, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier, Aleksandrs Slivkins</div><div style='padding-top: 10px; width: 80ex'>When deployed in the world, a learning agent such as a recommender system or
a chatbot often repeatedly interacts with another learning agent (such as a
user) over time. In many such two-agent systems, each agent learns separately
and the rewards of the two agents are not perfectly aligned. To better
understand such cases, we examine the learning dynamics of the two-agent system
and the implications for each agent's objective. We model these systems as
Stackelberg games with decentralized learning and show that standard regret
benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case
linear regret for at least one player. To better capture these systems, we
construct a relaxed regret benchmark that is tolerant to small learning errors
by agents. We show that standard learning algorithms fail to provide sublinear
regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret
for both players with respect to these benchmarks. We further design relaxed
environments under which faster learning ($O(\sqrt{T})$) is possible.
Altogether, our results take a step towards assessing how two-agent
interactions in sequential and decentralized learning environments affect the
utility of both agents.</div><div><a href='http://arxiv.org/abs/2403.00188v1'>2403.00188v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07143v1")'>New Perspectives in Online Contract Design: Heterogeneous, Homogeneous,
  Non-myopic Agents and Team Production</div>
<div id='2403.07143v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:28:23Z</div><div>Authors: Shiliang Zuo</div><div style='padding-top: 10px; width: 80ex'>This work studies the repeated principal-agent problem from an online
learning perspective. The principal's goal is to learn the optimal contract
that maximizes her utility through repeated interactions, without prior
knowledge of the agent's type (i.e., the agent's cost and production
functions).
  I study three different settings when the principal contracts with a
$\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the
agents are homogenous; 3. the principal interacts with the same agent and the
agent is non-myopic. I present different approaches and techniques for
designing learning algorithms in each setting. For heterogeneous agent types, I
identify a condition that allows the problem to be reduced to Lipschitz bandits
directly. For identical agents, I give a polynomial sample complexity scheme to
learn the optimal contract based on inverse game theory. For strategic
non-myopic agents, I design a low strategic-regret mechanism. Also, I identify
a connection between linear contracts and posted-price auctions, showing the
two can be reduced to one another, and give a regret lower bound on learning
the optimal linear contract based on this observation.
  I also study a $\textit{team production}$ model. I identify a condition under
which the principal's learning problem can be reformulated as solving a family
of convex programs, thereby showing the optimal contract can be found
efficiently.</div><div><a href='http://arxiv.org/abs/2403.07143v1'>2403.07143v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16198v1")'>Contracting with a Learning Agent</div>
<div id='2401.16198v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T14:53:22Z</div><div>Authors: Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Joshua R. Wang, S. Matthew Weinberg</div><div style='padding-top: 10px; width: 80ex'>Many real-life contractual relations differ completely from the clean, static
model at the heart of principal-agent theory. Typically, they involve repeated
strategic interactions of the principal and agent, taking place under
uncertainty and over time. While appealing in theory, players seldom use
complex dynamic strategies in practice, often preferring to circumvent
complexity and approach uncertainty through learning. We initiate the study of
repeated contracts with a learning agent, focusing on agents who achieve
no-regret outcomes.
  Optimizing against a no-regret agent is a known open problem in general
games; we achieve an optimal solution to this problem for a canonical contract
setting, in which the agent's choice among multiple actions leads to
success/failure. The solution has a surprisingly simple structure: for some
$\alpha &gt; 0$, initially offer the agent a linear contract with scalar $\alpha$,
then switch to offering a linear contract with scalar $0$. This switch causes
the agent to ``free-fall'' through their action space and during this time
provides the principal with non-zero reward at zero cost. Despite apparent
exploitation of the agent, this dynamic contract can leave \emph{both} players
better off compared to the best static contract. Our results generalize beyond
success/failure, to arbitrary non-linear contracts which the principal rescales
dynamically.
  Finally, we quantify the dependence of our results on knowledge of the time
horizon, and are the first to address this consideration in the study of
strategizing against learning agents.</div><div><a href='http://arxiv.org/abs/2401.16198v1'>2401.16198v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17108v1")'>Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and
  Limited Liability</div>
<div id='2402.17108v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T01:01:59Z</div><div>Authors: Natalie Collina, Varun Gupta, Aaron Roth</div><div style='padding-top: 10px; width: 80ex'>We study a repeated contracting setting in which a Principal adaptively
chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic,
and so a mechanism for the Principal induces a $T$-round extensive form game
amongst the Agents. We give several results aimed at understanding an
under-explored aspect of contract theory -- the game induced when choosing an
Agent to contract with. First, we show that this game admits a pure-strategy
\emph{non-responsive} equilibrium amongst the Agents -- informally an
equilibrium in which the Agent's actions depend on the history of realized
states of nature, but not on the history of each other's actions, and so avoids
the complexities of collusion and threats. Next, we show that if the Principal
selects Agents using a \emph{monotone} bandit algorithm, then for any concave
contract, in any such equilibrium, the Principal obtains no regret to
contracting with the best Agent in hindsight -- not just given their realized
actions, but also to the counterfactual world in which they had offered a
guaranteed $T$-round contract to the best Agent in hindsight, which would have
induced a different sequence of actions. Finally, we show that if the Principal
selects Agents using a monotone bandit algorithm which guarantees no
swap-regret, then the Principal can additionally offer only limited liability
contracts (in which the Agent never needs to pay the Principal) while getting
no-regret to the counterfactual world in which she offered a linear contract to
the best Agent in hindsight -- despite the fact that linear contracts are not
limited liability. We instantiate this theorem by demonstrating the existence
of a monotone no swap-regret bandit algorithm, which to our knowledge has not
previously appeared in the literature.</div><div><a href='http://arxiv.org/abs/2402.17108v1'>2402.17108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03811v1")'>Incentivized Learning in Principal-Agent Bandit Games</div>
<div id='2403.03811v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:00:46Z</div><div>Authors: Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine, El Mahdi El Mhamdi, Eric Moulines, Michael I. Jordan, Alain Durmus</div><div style='padding-top: 10px; width: 80ex'>This work considers a repeated principal-agent bandit game, where the
principal can only interact with her environment through the agent. The
principal and the agent have misaligned objectives and the choice of action is
only left to the agent. However, the principal can influence the agent's
decisions by offering incentives which add up to his rewards. The principal
aims to iteratively learn an incentive policy to maximize her own total
utility. This framework extends usual bandit problems and is motivated by
several practical applications, such as healthcare or ecological taxation,
where traditionally used mechanism design theories often overlook the learning
aspect of the problem. We present nearly optimal (with respect to a horizon
$T$) learning algorithms for the principal's regret in both multi-armed and
linear contextual settings. Finally, we support our theoretical guarantees
through numerical experiments.</div><div><a href='http://arxiv.org/abs/2403.03811v1'>2403.03811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09721v2")'>Persuading a Learning Agent</div>
<div id='2402.09721v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T05:30:47Z</div><div>Authors: Tao Lin, Yiling Chen</div><div style='padding-top: 10px; width: 80ex'>We study a repeated Bayesian persuasion problem (and more generally, any
generalized principal-agent problem with complete information) where the
principal does not have commitment power and the agent uses algorithms to learn
to respond to the principal's signals. We reduce this problem to a one-shot
generalized principal-agent problem with an approximately-best-responding
agent. This reduction allows us to show that: if the agent uses contextual
no-regret learning algorithms, then the principal can guarantee a utility that
is arbitrarily close to the principal's optimal utility in the classic
non-learning model with commitment; if the agent uses contextual no-swap-regret
learning algorithms, then the principal cannot obtain any utility significantly
more than the optimal utility in the non-learning model with commitment. The
difference between the principal's obtainable utility in the learning model and
the non-learning model is bounded by the agent's regret (swap-regret). If the
agent uses mean-based learning algorithms (which can be no-regret but not
no-swap-regret), then the principal can do significantly better than the
non-learning model. These conclusions hold not only for Bayesian persuasion,
but also for any generalized principal-agent problem with complete information,
including Stackelberg games and contract design.</div><div><a href='http://arxiv.org/abs/2402.09721v2'>2402.09721v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03077v2")'>Markov Persuasion Processes: Learning to Persuade from Scratch</div>
<div id='2402.03077v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:09:41Z</div><div>Authors: Francesco Bacchiocchi, Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti</div><div style='padding-top: 10px; width: 80ex'>In Bayesian persuasion, an informed sender strategically discloses
information to a receiver so as to persuade them to undertake desirable
actions. Recently, a growing attention has been devoted to settings in which
sender and receivers interact sequentially. Recently, Markov persuasion
processes (MPPs) have been introduced to capture sequential scenarios where a
sender faces a stream of myopic receivers in a Markovian environment. The MPPs
studied so far in the literature suffer from issues that prevent them from
being fully operational in practice, e.g., they assume that the sender knows
receivers' rewards. We fix such issues by addressing MPPs where the sender has
no knowledge about the environment. We design a learning algorithm for the
sender, working with partial feedback. We prove that its regret with respect to
an optimal information-disclosure policy grows sublinearly in the number of
episodes, as it is the case for the loss in persuasiveness cumulated while
learning. Moreover, we provide a lower bound for our setting matching the
guarantees of our algorithm.</div><div><a href='http://arxiv.org/abs/2402.03077v2'>2402.03077v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13338v1")'>Incentivized Exploration via Filtered Posterior Sampling</div>
<div id='2402.13338v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T19:30:55Z</div><div>Authors: Anand Kalvit, Aleksandrs Slivkins, Yonatan Gur</div><div style='padding-top: 10px; width: 80ex'>We study "incentivized exploration" (IE) in social learning problems where
the principal (a recommendation algorithm) can leverage information asymmetry
to incentivize sequentially-arriving agents to take exploratory actions. We
identify posterior sampling, an algorithmic approach that is well known in the
multi-armed bandits literature, as a general-purpose solution for IE. In
particular, we expand the existing scope of IE in several practically-relevant
dimensions, from private agent types to informative recommendations to
correlated Bayesian priors. We obtain a general analysis of posterior sampling
in IE which allows us to subsume these extended settings as corollaries, while
also recovering existing results as special cases.</div><div><a href='http://arxiv.org/abs/2402.13338v1'>2402.13338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00036v1")'>Influencing Bandits: Arm Selection for Preference Shaping</div>
<div id='2403.00036v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T05:59:27Z</div><div>Authors: Viraj Nadkarni, D. Manjunath, Sharayu Moharir</div><div style='padding-top: 10px; width: 80ex'>We consider a non stationary multi-armed bandit in which the population
preferences are positively and negatively reinforced by the observed rewards.
The objective of the algorithm is to shape the population preferences to
maximize the fraction of the population favouring a predetermined arm. For the
case of binary opinions, two types of opinion dynamics are considered --
decreasing elasticity (modeled as a Polya urn with increasing number of balls)
and constant elasticity (using the voter model). For the first case, we
describe an Explore-then-commit policy and a Thompson sampling policy and
analyse the regret for each of these policies. We then show that these
algorithms and their analyses carry over to the constant elasticity case. We
also describe a Thompson sampling based algorithm for the case when more than
two types of opinions are present. Finally, we discuss the case where presence
of multiple recommendation systems gives rise to a trade-off between their
popularity and opinion shaping objectives.</div><div><a href='http://arxiv.org/abs/2403.00036v1'>2403.00036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08753v1")'>Forecasting for Swap Regret for All Downstream Agents</div>
<div id='2402.08753v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:39:11Z</div><div>Authors: Aaron Roth, Mirah Shi</div><div style='padding-top: 10px; width: 80ex'>We study the problem of making predictions so that downstream agents who best
respond to them will be guaranteed diminishing swap regret, no matter what
their utility functions are. It has been known since Foster and Vohra (1997)
that agents who best-respond to calibrated forecasts have no swap regret.
Unfortunately, the best known algorithms for guaranteeing calibrated forecasts
in sequential adversarial environments do so at rates that degrade
exponentially with the dimension of the prediction space. In this work, we show
that by making predictions that are not calibrated, but are unbiased subject to
a carefully selected collection of events, we can guarantee arbitrary
downstream agents diminishing swap regret at rates that substantially improve
over the rates that result from calibrated forecasts -- while maintaining the
appealing property that our forecasts give guarantees for any downstream agent,
without our forecasting algorithm needing to know their utility function.
  We give separate results in the ``low'' (1 or 2) dimensional setting and the
``high'' ($&gt; 2$) dimensional setting. In the low dimensional setting, we show
how to make predictions such that all agents who best respond to our
predictions have diminishing swap regret -- in 1 dimension, at the optimal
$O(\sqrt{T})$ rate. In the high dimensional setting we show how to make
forecasts that guarantee regret scaling at a rate of $O(T^{2/3})$ (crucially, a
dimension independent exponent), under the assumption that downstream agents
smoothly best respond. Our results stand in contrast to rates that derive from
agents who best respond to calibrated forecasts, which have an exponential
dependence on the dimension of the prediction space.</div><div><a href='http://arxiv.org/abs/2402.08753v1'>2402.08753v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14483v1")'>Four Facets of Forecast Felicity: Calibration, Predictiveness,
  Randomness and Regret</div>
<div id='2401.14483v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T19:36:11Z</div><div>Authors: Rabanus Derr, Robert C. Williamson</div><div style='padding-top: 10px; width: 80ex'>Machine learning is about forecasting. Forecasts, however, obtain their
usefulness only through their evaluation. Machine learning has traditionally
focused on types of losses and their corresponding regret. Currently, the
machine learning community regained interest in calibration. In this work, we
show the conceptual equivalence of calibration and regret in evaluating
forecasts. We frame the evaluation problem as a game between a forecaster, a
gambler and nature. Putting intuitive restrictions on gambler and forecaster,
calibration and regret naturally fall out of the framework. In addition, this
game links evaluation of forecasts to randomness of outcomes. Random outcomes
with respect to forecasts are equivalent to good forecasts with respect to
outcomes. We call those dual aspects, calibration and regret, predictiveness
and randomness, the four facets of forecast felicity.</div><div><a href='http://arxiv.org/abs/2401.14483v1'>2401.14483v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17743v1")'>Algorithmic Robust Forecast Aggregation</div>
<div id='2401.17743v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T11:02:45Z</div><div>Authors: Yongkang Guo, Jason D. Hartline, Zhihuan Huang, Yuqing Kong, Anant Shah, Fang-Yi Yu</div><div style='padding-top: 10px; width: 80ex'>Forecast aggregation combines the predictions of multiple forecasters to
improve accuracy. However, the lack of knowledge about forecasters' information
structure hinders optimal aggregation. Given a family of information
structures, robust forecast aggregation aims to find the aggregator with
minimal worst-case regret compared to the omniscient aggregator. Previous
approaches for robust forecast aggregation rely on heuristic observations and
parameter tuning. We propose an algorithmic framework for robust forecast
aggregation. Our framework provides efficient approximation schemes for general
information aggregation with a finite family of possible information
structures. In the setting considered by Arieli et al. (2018) where two agents
receive independent signals conditioned on a binary state, our framework also
provides efficient approximation schemes by imposing Lipschitz conditions on
the aggregator or discrete conditions on agents' reports. Numerical experiments
demonstrate the effectiveness of our method by providing a nearly optimal
aggregator in the setting considered by Arieli et al. (2018).</div><div><a href='http://arxiv.org/abs/2401.17743v1'>2401.17743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14486v1")'>Are Bounded Contracts Learnable and Approximately Optimal?</div>
<div id='2402.14486v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T12:19:19Z</div><div>Authors: Yurong Chen, Zhaohua Chen, Xiaotie Deng, Zhiyi Huang</div><div style='padding-top: 10px; width: 80ex'>This paper considers the hidden-action model of the principal-agent problem,
in which a principal incentivizes an agent to work on a project using a
contract. We investigate whether contracts with bounded payments are learnable
and approximately optimal. Our main results are two learning algorithms that
can find a nearly optimal bounded contract using a polynomial number of
queries, under two standard assumptions in the literature: a costlier action
for the agent leads to a better outcome distribution for the principal, and the
agent's cost/effort has diminishing returns. Our polynomial query complexity
upper bound shows that standard assumptions are sufficient for achieving an
exponential improvement upon the known lower bound for general instances.
Unlike the existing algorithms, which relied on discretizing the contract
space, our algorithms directly learn the underlying outcome distributions. As
for the approximate optimality of bounded contracts, we find that they could be
far from optimal in terms of multiplicative or additive approximation, but
satisfy a notion of mixed approximation.</div><div><a href='http://arxiv.org/abs/2402.14486v1'>2402.14486v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01930v2")'>Reducing Optimism Bias in Incomplete Cooperative Games</div>
<div id='2402.01930v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T21:58:26Z</div><div>Authors: Filip Úradník, David Sychrovský, Jakub Černý, Martin Černý</div><div style='padding-top: 10px; width: 80ex'>Cooperative game theory has diverse applications in contemporary artificial
intelligence, including domains like interpretable machine learning, resource
allocation, and collaborative decision-making. However, specifying a
cooperative game entails assigning values to exponentially many coalitions, and
obtaining even a single value can be resource-intensive in practice. Yet simply
leaving certain coalition values undisclosed introduces ambiguity regarding
individual contributions to the collective grand coalition. This ambiguity
often leads to players holding overly optimistic expectations, stemming from
either inherent biases or strategic considerations, frequently resulting in
collective claims exceeding the actual grand coalition value. In this paper, we
present a framework aimed at optimizing the sequence for revealing coalition
values, with the overarching goal of efficiently closing the gap between
players' expectations and achievable outcomes in cooperative games. Our
contributions are threefold: (i) we study the individual players' optimistic
completions of games with missing coalition values along with the arising gap,
and investigate its analytical characteristics that facilitate more efficient
optimization; (ii) we develop methods to minimize this gap over classes of
games with a known prior by disclosing values of additional coalitions in both
offline and online fashion; and (iii) we empirically demonstrate the
algorithms' performance in practical scenarios, together with an investigation
into the typical order of revealing coalition values.</div><div><a href='http://arxiv.org/abs/2402.01930v2'>2402.01930v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07067v1")'>Learning the Expected Core of Strictly Convex Stochastic Cooperative
  Games</div>
<div id='2402.07067v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T23:49:49Z</div><div>Authors: Nam Phuong Tran, The Anh Ta, Shuqing Shi, Debmalya Mandal, Yali Du, Long Tran-Thanh</div><div style='padding-top: 10px; width: 80ex'>Reward allocation, also known as the credit assignment problem, has been an
important topic in economics, engineering, and machine learning. An important
concept in credit assignment is the core, which is the set of stable
allocations where no agent has the motivation to deviate from the grand
coalition. In this paper, we consider the stable allocation learning problem of
stochastic cooperative games, where the reward function is characterised as a
random variable with an unknown distribution. Given an oracle that returns a
stochastic reward for an enquired coalition each round, our goal is to learn
the expected core, that is, the set of allocations that are stable in
expectation. Within the class of strictly convex games, we present an algorithm
named \texttt{Common-Points-Picking} that returns a stable allocation given a
polynomial number of samples, with high probability. The analysis of our
algorithm involves the development of several new results in convex geometry,
including an extension of the separation hyperplane theorem for multiple convex
sets, and may be of independent interest.</div><div><a href='http://arxiv.org/abs/2402.07067v1'>2402.07067v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10243v1")'>Understanding team collapse via probabilistic graphical models</div>
<div id='2402.10243v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T22:23:26Z</div><div>Authors: Iasonas Nikolaou, Konstantinos Pelechrinis, Evimaria Terzi</div><div style='padding-top: 10px; width: 80ex'>In this work, we develop a graphical model to capture team dynamics. We
analyze the model and show how to learn its parameters from data. Using our
model we study the phenomenon of team collapse from a computational
perspective. We use simulations and real-world experiments to find the main
causes of team collapse. We also provide the principles of building resilient
teams, i.e., teams that avoid collapsing. Finally, we use our model to analyze
the structure of NBA teams and dive deeper into games of interest.</div><div><a href='http://arxiv.org/abs/2402.10243v1'>2402.10243v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15273v1")'>Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement
  Learning</div>
<div id='2401.15273v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T02:43:45Z</div><div>Authors: Chenyu Zhang, Han Wang, Aritra Mitra, James Anderson</div><div style='padding-top: 10px; width: 80ex'>Federated reinforcement learning (FRL) has emerged as a promising paradigm
for reducing the sample complexity of reinforcement learning tasks by
exploiting information from different agents. However, when each agent
interacts with a potentially different environment, little to nothing is known
theoretically about the non-asymptotic performance of FRL algorithms. The lack
of such results can be attributed to various technical challenges and their
intricate interplay: Markovian sampling, linear function approximation,
multiple local updates to save communication, heterogeneity in the reward
functions and transition kernels of the agents' MDPs, and continuous
state-action spaces. Moreover, in the on-policy setting, the behavior policies
vary with time, further complicating the analysis. In response, we introduce
FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped
with linear function approximation, to address these challenges and provide a
comprehensive finite-time error analysis. Notably, we establish that FedSARSA
converges to a policy that is near-optimal for all agents, with the extent of
near-optimality proportional to the level of heterogeneity. Furthermore, we
prove that FedSARSA leverages agent collaboration to enable linear speedups as
the number of agents increases, which holds for both fixed and adaptive
step-size configurations.</div><div><a href='http://arxiv.org/abs/2401.15273v1'>2401.15273v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09940v1")'>Global Convergence Guarantees for Federated Policy Gradient Methods with
  Adversaries</div>
<div id='2403.09940v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T00:45:36Z</div><div>Authors: Swetha Ganesh, Jiayu Chen, Gugan Thoppe, Vaneet Aggarwal</div><div style='padding-top: 10px; width: 80ex'>Federated Reinforcement Learning (FRL) allows multiple agents to
collaboratively build a decision making policy without sharing raw
trajectories. However, if a small fraction of these agents are adversarial, it
can lead to catastrophic results. We propose a policy gradient based approach
that is robust to adversarial agents which can send arbitrary values to the
server. Under this setting, our results form the first global convergence
guarantees with general parametrization. These results demonstrate resilience
with adversaries, while achieving sample complexity of order
$\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} +
\frac{f^2}{(N-f)^2}\right)\right)$, where $N$ is the total number of agents and
$f$ is the number of adversarial agents.</div><div><a href='http://arxiv.org/abs/2403.09940v1'>2403.09940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03489v1")'>Decentralized Federated Policy Gradient with Byzantine Fault-Tolerance
  and Provably Fast Convergence</div>
<div id='2401.03489v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T14:06:06Z</div><div>Authors: Philip Jordan, Florian Grötschla, Flint Xiaofeng Fan, Roger Wattenhofer</div><div style='padding-top: 10px; width: 80ex'>In Federated Reinforcement Learning (FRL), agents aim to collaboratively
learn a common task, while each agent is acting in its local environment
without exchanging raw trajectories. Existing approaches for FRL either (a) do
not provide any fault-tolerance guarantees (against misbehaving agents), or (b)
rely on a trusted central agent (a single point of failure) for aggregating
updates. We provide the first decentralized Byzantine fault-tolerant FRL
method. Towards this end, we first propose a new centralized Byzantine
fault-tolerant policy gradient (PG) algorithm that improves over existing
methods by relying only on assumptions standard for non-fault-tolerant PG.
Then, as our main contribution, we show how a combination of robust aggregation
and Byzantine-resilient agreement methods can be leveraged in order to
eliminate the need for a trusted central entity. Since our results represent
the first sample complexity analysis for Byzantine fault-tolerant decentralized
federated non-convex optimization, our technical contributions may be of
independent interest. Finally, we corroborate our theoretical results
experimentally for common RL environments, demonstrating the speed-up of
decentralized federations w.r.t. the number of participating agents and
resilience against various Byzantine attacks.</div><div><a href='http://arxiv.org/abs/2401.03489v1'>2401.03489v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00222v1")'>Efficient Reinforcement Learning for Global Decision Making in the
  Presence of Local Agents at Scale</div>
<div id='2403.00222v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T01:49:57Z</div><div>Authors: Emile Anand, Guannan Qu</div><div style='padding-top: 10px; width: 80ex'>We study reinforcement learning for global decision-making in the presence of
many local agents, where the global decision-maker makes decisions affecting
all local agents, and the objective is to learn a policy that maximizes the
rewards of both the global and the local agents. Such problems find many
applications, e.g. demand response, EV charging, queueing, etc. In this
setting, scalability has been a long-standing challenge due to the size of the
state/action space which can be exponential in the number of agents. This work
proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$
local agents to compute an optimal policy in time that is only exponential in
$k$, providing an exponential speedup from standard methods that are
exponential in $n$. We show that the learned policy converges to the optimal
policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of
sub-sampled agents $k$ increases, where $\epsilon_{k,m}$ is the Bellman noise.
We also conduct numerical simulations in a demand-response setting and a
queueing setting.</div><div><a href='http://arxiv.org/abs/2403.00222v1'>2403.00222v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15546v1")'>HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent
  Pathfinding</div>
<div id='2402.15546v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:01:13Z</div><div>Authors: Huijie Tang, Federico Berto, Zihan Ma, Chuanbo Hua, Kyuree Ahn, Jinkyoo Park</div><div style='padding-top: 10px; width: 80ex'>Large-scale multi-agent pathfinding (MAPF) presents significant challenges in
several areas. As systems grow in complexity with a multitude of autonomous
agents operating simultaneously, efficient and collision-free coordination
becomes paramount. Traditional algorithms often fall short in scalability,
especially in intricate scenarios. Reinforcement Learning (RL) has shown
potential to address the intricacies of MAPF; however, it has also been shown
to struggle with scalability, demanding intricate implementation, lengthy
training, and often exhibiting unstable convergence, limiting its practical
application. In this paper, we introduce Heuristics-Informed Multi-Agent
Pathfinding (HiMAP), a novel scalable approach that employs imitation learning
with heuristic guidance in a decentralized manner. We train on small-scale
instances using a heuristic policy as a teacher that maps each single agent
observation information to an action probability distribution. During
pathfinding, we adopt several inference techniques to improve performance. With
a simple training scheme and implementation, HiMAP demonstrates competitive
results in terms of success rate and scalability in the field of
imitation-learning-only MAPF, showing the potential of imitation-learning-only
MAPF equipped with inference techniques.</div><div><a href='http://arxiv.org/abs/2402.15546v1'>2402.15546v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07559v1")'>Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding</div>
<div id='2403.07559v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:47:12Z</div><div>Authors: Huijie Tang, Federico Berto, Jinkyoo Park</div><div style='padding-top: 10px; width: 80ex'>Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding
(MAPF) has recently gained attention due to its efficiency and scalability.
Several MARL-MAPF methods choose to use communication to enrich the information
one agent can perceive. However, existing works still struggle in structured
environments with high obstacle density and a high number of agents. To further
improve the performance of the communication-based MARL-MAPF solvers, we
propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first
propose a selective communication block to gather richer information for better
agent coordination within multi-agent environments and train the model with a
Q-learning-based algorithm. We further introduce three advanced inference
strategies aimed at bolstering performance during the execution phase. First,
we hybridize the neural policy with single-agent expert guidance for navigating
conflict-free zones. Secondly, we propose Q value-based methods for prioritized
resolution of conflicts as well as deadlock situations. Finally, we introduce a
robust ensemble method that can efficiently collect the best out of multiple
possible solutions. We empirically evaluate EPH in complex multi-agent
environments and demonstrate competitive performance against state-of-the-art
neural methods for MAPF.</div><div><a href='http://arxiv.org/abs/2403.07559v1'>2403.07559v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14583v1")'>Co-Optimization of Environment and Policies for Decentralized
  Multi-Agent Navigation</div>
<div id='2403.14583v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:37:43Z</div><div>Authors: Zhan Gao, Guang Yang, Amanda Prorok</div><div style='padding-top: 10px; width: 80ex'>This work views the multi-agent system and its surrounding environment as a
co-evolving system, where the behavior of one affects the other. The goal is to
take both agent actions and environment configurations as decision variables,
and optimize these two components in a coordinated manner to improve some
measure of interest. Towards this end, we consider the problem of decentralized
multi-agent navigation in cluttered environments. By introducing two
sub-objectives of multi-agent navigation and environment optimization, we
propose an $\textit{agent-environment co-optimization}$ problem and develop a
$\textit{coordinated algorithm}$ that alternates between these sub-objectives
to search for an optimal synthesis of agent actions and obstacle configurations
in the environment; ultimately, improving the navigation performance. Due to
the challenge of explicitly modeling the relation between agents, environment
and performance, we leverage policy gradient to formulate a model-free learning
mechanism within the coordinated framework. A formal convergence analysis shows
that our coordinated algorithm tracks the local minimum trajectory of an
associated time-varying non-convex optimization problem. Extensive numerical
results corroborate theoretical findings and show the benefits of
co-optimization over baselines. Interestingly, the results also indicate that
optimized environment configurations are able to offer structural guidance that
is key to de-conflicting agents in motion.</div><div><a href='http://arxiv.org/abs/2403.14583v1'>2403.14583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03154v2")'>Decentralized Multi-Agent Active Search and Tracking when Targets
  Outnumber Agents</div>
<div id='2401.03154v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:10:58Z</div><div>Authors: Arundhati Banerjee, Jeff Schneider</div><div style='padding-top: 10px; width: 80ex'>Multi-agent multi-target tracking has a wide range of applications, including
wildlife patrolling, security surveillance or environment monitoring. Such
algorithms often make restrictive assumptions: the number of targets and/or
their initial locations may be assumed known, or agents may be pre-assigned to
monitor disjoint partitions of the environment, reducing the burden of
exploration. This also limits applicability when there are fewer agents than
targets, since agents are unable to continuously follow the targets in their
fields of view. Multi-agent tracking algorithms additionally assume inter-agent
synchronization of observations, or the presence of a central controller to
coordinate joint actions. Instead, we focus on the setting of decentralized
multi-agent, multi-target, simultaneous active search-and-tracking with
asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a
sequential monte carlo implementation of the probability hypothesis density
filter for posterior inference combined with Thompson sampling for
decentralized multi-agent decision making. We compare different action
selection policies, focusing on scenarios where targets outnumber agents. In
simulation, we demonstrate that DecSTER is robust to unreliable inter-agent
communication and outperforms information-greedy baselines in terms of the
Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets
and varying teamsizes.</div><div><a href='http://arxiv.org/abs/2401.03154v2'>2401.03154v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03048v1")'>Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems
  Tracking Control under Switching Topologies</div>
<div id='2402.03048v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:33:52Z</div><div>Authors: Zewen Yang, Songbo Dong, Armin Lederer, Xiaobing Dai, Siyu Chen, Stefan Sosnowski, Georges Hattab, Sandra Hirche</div><div style='padding-top: 10px; width: 80ex'>This work presents an innovative learning-based approach to tackle the
tracking control problem of Euler-Lagrange multi-agent systems with partially
unknown dynamics operating under switching communication topologies. The
approach leverages a correlation-aware cooperative algorithm framework built
upon Gaussian process regression, which adeptly captures inter-agent
correlations for uncertainty predictions. A standout feature is its exceptional
efficiency in deriving the aggregation weights achieved by circumventing the
computationally intensive posterior variance calculations. Through Lyapunov
stability analysis, the distributed control law ensures bounded tracking errors
with high probability. Simulation experiments validate the protocol's efficacy
in effectively managing complex scenarios, establishing it as a promising
solution for robust tracking control in multi-agent systems characterized by
uncertain dynamics and dynamic communication structures.</div><div><a href='http://arxiv.org/abs/2402.03048v1'>2402.03048v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03174v1")'>Decentralized Event-Triggered Online Learning for Safe Consensus of
  Multi-Agent Systems with Gaussian Process Regression</div>
<div id='2402.03174v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:41:17Z</div><div>Authors: Xiaobing Dai, Zewen Yang, Mengtian Xu, Fangzhou Liu, Georges Hattab, Sandra Hirche</div><div style='padding-top: 10px; width: 80ex'>Consensus control in multi-agent systems has received significant attention
and practical implementation across various domains. However, managing
consensus control under unknown dynamics remains a significant challenge for
control design due to system uncertainties and environmental disturbances. This
paper presents a novel learning-based distributed control law, augmented by an
auxiliary dynamics. Gaussian processes are harnessed to compensate for the
unknown components of the multi-agent system. For continuous enhancement in
predictive performance of Gaussian process model, a data-efficient online
learning strategy with a decentralized event-triggered mechanism is proposed.
Furthermore, the control performance of the proposed approach is ensured via
the Lyapunov theory, based on a probabilistic guarantee for prediction error
bounds. To demonstrate the efficacy of the proposed learning-based controller,
a comparative analysis is conducted, contrasting it with both conventional
distributed control laws and offline learning methodologies.</div><div><a href='http://arxiv.org/abs/2402.03174v1'>2402.03174v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03014v1")'>Whom to Trust? Elective Learning for Distributed Gaussian Process
  Regression</div>
<div id='2402.03014v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:52:56Z</div><div>Authors: Zewen Yang, Xiaobing Dai, Akshat Dubey, Sandra Hirche, Georges Hattab</div><div style='padding-top: 10px; width: 80ex'>This paper introduces an innovative approach to enhance distributed
cooperative learning using Gaussian process (GP) regression in multi-agent
systems (MASs). The key contribution of this work is the development of an
elective learning algorithm, namely prior-aware elective distributed GP
(Pri-GP), which empowers agents with the capability to selectively request
predictions from neighboring agents based on their trustworthiness. The
proposed Pri-GP effectively improves individual prediction accuracy, especially
in cases where the prior knowledge of an agent is incorrect. Moreover, it
eliminates the need for computationally intensive variance calculations for
determining aggregation weights in distributed GP. Furthermore, we establish a
prediction error bound within the Pri-GP framework, ensuring the reliability of
predictions, which is regarded as a crucial property in safety-critical MAS
applications.</div><div><a href='http://arxiv.org/abs/2402.03014v1'>2402.03014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17987v2")'>Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A
  Bayesian Fusion Approach</div>
<div id='2402.17987v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T02:11:47Z</div><div>Authors: Michael Potter, Murat Akcakaya, Marius Necsoiu, Gunar Schirner, Deniz Erdogmus, Tales Imbiriba</div><div style='padding-top: 10px; width: 80ex'>Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs)
involves transmitting Electromagnetic Waves (EMWs) and performing target type
recognition on the received radar echo, crucial for defense and aerospace
applications. Previous studies highlighted the advantages of multistatic radar
configurations over monostatic ones in RATR. However, fusion methods in
multistatic radar configurations often suboptimally combine classification
vectors from individual radars probabilistically. To address this, we propose a
fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to
aggregate classification probability vectors from multiple radars. OBF, based
on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC)
posterior distribution for target UAV type, conditioned on historical
observations across multiple time steps. We evaluate the approach using
simulated random walk trajectories for seven drones, correlating target aspect
angles to Radar Cross Section (RCS) measurements in an anechoic chamber.
Comparing against single radar Automated Target Recognition (ATR) systems and
suboptimal fusion methods, our empirical results demonstrate that the OBF
method integrated with RBC significantly enhances classification accuracy
compared to other fusion methods and single radar configurations.</div><div><a href='http://arxiv.org/abs/2402.17987v2'>2402.17987v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13827v1")'>Self-Supervised Path Planning in UAV-aided Wireless Networks based on
  Active Inference</div>
<div id='2403.13827v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T09:58:35Z</div><div>Authors: Ali Krayani, Khalid Khan, Lucio Marcenaro, Mario Marchese, Carlo Regazzoni</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel self-supervised path-planning method for
UAV-aided networks. First, we employed an optimizer to solve training examples
offline and then used the resulting solutions as demonstrations from which the
UAV can learn the world model to understand the environment and implicitly
discover the optimizer's policy. UAV equipped with the world model can make
real-time autonomous decisions and engage in online planning using active
inference. During planning, UAV can score different policies based on the
expected surprise, allowing it to choose among alternative futures.
Additionally, UAV can anticipate the outcomes of its actions using the world
model and assess the expected surprise in a self-supervised manner. Our method
enables quicker adaptation to new situations and better performance than
traditional RL, leading to broader generalizability.</div><div><a href='http://arxiv.org/abs/2403.13827v1'>2403.13827v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01147v1")'>Efficient Reinforcement Learning for Routing Jobs in Heterogeneous
  Queueing Systems</div>
<div id='2402.01147v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T05:22:41Z</div><div>Authors: Neharika Jali, Guannan Qu, Weina Wang, Gauri Joshi</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of efficiently routing jobs that arrive into a
central queue to a system of heterogeneous servers. Unlike homogeneous systems,
a threshold policy, that routes jobs to the slow server(s) when the queue
length exceeds a certain threshold, is known to be optimal for the
one-fast-one-slow two-server system. But an optimal policy for the multi-server
system is unknown and non-trivial to find. While Reinforcement Learning (RL)
has been recognized to have great potential for learning policies in such
cases, our problem has an exponentially large state space size, rendering
standard RL inefficient. In this work, we propose ACHQ, an efficient policy
gradient based algorithm with a low dimensional soft threshold policy
parameterization that leverages the underlying queueing structure. We provide
stationary-point convergence guarantees for the general case and despite the
low-dimensional parameterization prove that ACHQ converges to an approximate
global optimum for the special case of two servers. Simulations demonstrate an
improvement in expected response time of up to ~30% over the greedy policy that
routes to the fastest available server.</div><div><a href='http://arxiv.org/abs/2402.01147v1'>2402.01147v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11093v1")'>Learning-Based Pricing and Matching for Two-Sided Queues</div>
<div id='2403.11093v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T05:07:04Z</div><div>Authors: Zixian Yang, Lei Ying</div><div style='padding-top: 10px; width: 80ex'>We consider a dynamic system with multiple types of customers and servers.
Each type of waiting customer or server joins a separate queue, forming a
bipartite graph with customer-side queues and server-side queues. The platform
can match the servers and customers if their types are compatible. The matched
pairs then leave the system. The platform will charge a customer a price
according to their type when they arrive and will pay a server a price
according to their type. The arrival rate of each queue is determined by the
price according to some unknown demand or supply functions. Our goal is to
design pricing and matching algorithms to maximize the profit of the platform
with unknown demand and supply functions, while keeping queue lengths of both
customers and servers below a predetermined threshold. This system can be used
to model two-sided markets such as ride-sharing markets with passengers and
drivers. The difficulties of the problem include simultaneous learning and
decision making, and the tradeoff between maximizing profit and minimizing
queue length. We use a longest-queue-first matching algorithm and propose a
learning-based pricing algorithm, which combines gradient-free stochastic
projected gradient ascent with bisection search. We prove that our proposed
algorithm yields a sublinear regret $\tilde{O}(T^{5/6})$ and queue-length bound
$\tilde{O}(T^{2/3})$, where $T$ is the time horizon. We further establish a
tradeoff between the regret bound and the queue-length bound:
$\tilde{O}(T^{1-\gamma/4})$ versus $\tilde{O}(T^{\gamma})$ for $\gamma \in (0,
2/3].$</div><div><a href='http://arxiv.org/abs/2403.11093v1'>2403.11093v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16025v1")'>Simple Policy Optimization</div>
<div id='2401.16025v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T10:17:54Z</div><div>Authors: Zhengpeng Xie</div><div style='padding-top: 10px; width: 80ex'>PPO (Proximal Policy Optimization) algorithm has demonstrated excellent
performance in many fields, and it is considered as a simple version of TRPO
(Trust Region Policy Optimization) algorithm. However, the ratio clipping
operation in PPO may not always effectively enforce the trust region
constraints, this can be a potential factor affecting the stability of the
algorithm. In this paper, we propose SPO (Simple Policy Optimization)
algorithm, which introduces a novel clipping method for KL divergence between
the old and current policies. SPO can effectively enforce the trust region
constraints in almost all environments, while still maintaining the simplicity
of a first-order algorithm. Comparative experiments in Atari 2600 environments
show that SPO sometimes provides stronger performance than PPO. Code is
available at https://github.com/MyRepositories-hub/Simple-Policy-Optimization.</div><div><a href='http://arxiv.org/abs/2401.16025v1'>2401.16025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12686v2")'>Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach</div>
<div id='2401.12686v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T11:52:00Z</div><div>Authors: Christian Fabian, Kai Cui, Heinz Koeppl</div><div style='padding-top: 10px; width: 80ex'>Learning the behavior of large agent populations is an important task for
numerous research areas. Although the field of multi-agent reinforcement
learning (MARL) has made significant progress towards solving these systems,
solutions for many agents often remain computationally infeasible and lack
theoretical guarantees. Mean Field Games (MFGs) address both of these issues
and can be extended to Graphon MFGs (GMFGs) to include network structures
between agents. Despite their merits, the real world applicability of GMFGs is
limited by the fact that graphons only capture dense graphs. Since most
empirically observed networks show some degree of sparsity, such as power law
graphs, the GMFG framework is insufficient for capturing these network
topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which
builds on the graph theoretical concept of graphexes. Graphexes are the
limiting objects to sparse graph sequences that also have other desirable
features such as the small world property. Learning equilibria in these games
is challenging due to the rich and sparse structure of the underlying graphs.
To tackle these challenges, we design a new learning algorithm tailored to the
GXMFG setup. This hybrid graphex learning approach leverages that the system
mainly consists of a highly connected core and a sparse periphery. After
defining the system and providing a theoretical analysis, we state our learning
approach and demonstrate its learning capabilities on both synthetic graphs and
real-world networks. This comparison shows that our GXMFG learning algorithm
successfully extends MFGs to a highly relevant class of hard, realistic
learning problems that are not accurately addressed by current MARL and MFG
methods.</div><div><a href='http://arxiv.org/abs/2401.12686v2'>2401.12686v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07182v1")'>Divide and Conquer: Provably Unveiling the Pareto Front with
  Multi-Objective Reinforcement Learning</div>
<div id='2402.07182v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T12:35:13Z</div><div>Authors: Willem Röpke, Mathieu Reymond, Patrick Mannion, Diederik M. Roijers, Ann Nowé, Roxana Rădulescu</div><div style='padding-top: 10px; width: 80ex'>A significant challenge in multi-objective reinforcement learning is
obtaining a Pareto front of policies that attain optimal performance under
different preferences. We introduce Iterated Pareto Referent Optimisation
(IPRO), a principled algorithm that decomposes the task of finding the Pareto
front into a sequence of single-objective problems for which various solution
methods exist. This enables us to establish convergence guarantees while
providing an upper bound on the distance to undiscovered Pareto optimal
solutions at each step. Empirical evaluations demonstrate that IPRO matches or
outperforms methods that require additional domain knowledge. By leveraging
problem-specific single-objective solvers, our approach also holds promise for
applications beyond multi-objective reinforcement learning, such as in
pathfinding and optimisation.</div><div><a href='http://arxiv.org/abs/2402.07182v1'>2402.07182v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11477v1")'>Span-Based Optimal Sample Complexity for Weakly Communicating and
  General Average Reward MDPs</div>
<div id='2403.11477v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T04:52:11Z</div><div>Authors: Matthew Zurek, Yudong Chen</div><div style='padding-top: 10px; width: 80ex'>We study the sample complexity of learning an $\epsilon$-optimal policy in an
average-reward Markov decision process (MDP) under a generative model. For
weakly communicating MDPs, we establish the complexity bound
$\tilde{O}(SA\frac{H}{\epsilon^2})$, where $H$ is the span of the bias function
of the optimal policy and $SA$ is the cardinality of the state-action space.
Our result is the first that is minimax optimal (up to log factors) in all
parameters $S,A,H$ and $\epsilon$, improving on existing work that either
assumes uniformly bounded mixing times for all policies or has suboptimal
dependence on the parameters. We further investigate sample complexity in
general (non-weakly-communicating) average-reward MDPs. We argue a new
transient time parameter $B$ is necessary, establish an
$\tilde{O}(SA\frac{B+H}{\epsilon^2})$ complexity bound, and prove a matching
(up to log factors) minimax lower bound. Both results are based on reducing the
average-reward MDP to a discounted MDP, which requires new ideas in the general
setting. To establish the optimality of this reduction, we develop improved
bounds for $\gamma$-discounted MDPs, showing that
$\tilde{\Omega}\left(SA\frac{H}{(1-\gamma)^2\epsilon^2}\right)$ samples suffice
to learn an $\epsilon$-optimal policy in weakly communicating MDPs under the
regime that $\gamma\geq 1-1/H$, and
$\tilde{\Omega}\left(SA\frac{B+H}{(1-\gamma)^2\epsilon^2}\right)$ samples
suffice in general MDPs when $\gamma\geq 1-\frac{1}{B+H}$. Both these results
circumvent the well-known lower bound of
$\tilde{\Omega}\left(SA\frac{1}{(1-\gamma)^3\epsilon^2}\right)$ for arbitrary
$\gamma$-discounted MDPs. Our analysis develops upper bounds on certain
instance-dependent variance parameters in terms of the span and transient time
parameters. The weakly communicating bounds are tighter than those based on the
mixing time or diameter of the MDP and may be of broader use.</div><div><a href='http://arxiv.org/abs/2403.11477v1'>2403.11477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11637v1")'>The Value of Reward Lookahead in Reinforcement Learning</div>
<div id='2403.11637v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T10:19:52Z</div><div>Authors: Nadav Merlis, Dorian Baudry, Vianney Perchet</div><div style='padding-top: 10px; width: 80ex'>In reinforcement learning (RL), agents sequentially interact with changing
environments while aiming to maximize the obtained rewards. Usually, rewards
are observed only after acting, and so the goal is to maximize the expected
cumulative reward. Yet, in many practical settings, reward information is
observed in advance -- prices are observed before performing transactions;
nearby traffic information is partially known; and goals are oftentimes given
to agents prior to the interaction. In this work, we aim to quantifiably
analyze the value of such future reward information through the lens of
competitive analysis. In particular, we measure the ratio between the value of
standard RL agents and that of agents with partial future-reward lookahead. We
characterize the worst-case reward distribution and derive exact ratios for the
worst-case reward expectations. Surprisingly, the resulting ratios relate to
known quantities in offline RL and reward-free exploration. We further provide
tight bounds for the ratio given the worst-case dynamics. Our results cover the
full spectrum between observing the immediate rewards before acting to
observing all the rewards before the interaction starts.</div><div><a href='http://arxiv.org/abs/2403.11637v1'>2403.11637v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09992v1")'>Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning
  under Distribution Shifts</div>
<div id='2402.09992v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:55:38Z</div><div>Authors: Tobias Enders, James Harrison, Maximilian Schiffer</div><div style='padding-top: 10px; width: 80ex'>We study the robustness of deep reinforcement learning algorithms against
distribution shifts within contextual multi-stage stochastic combinatorial
optimization problems from the operations research domain. In this context,
risk-sensitive algorithms promise to learn robust policies. While this field is
of general interest to the reinforcement learning community, most studies
up-to-date focus on theoretical results rather than real-world performance.
With this work, we aim to bridge this gap by formally deriving a novel
risk-sensitive deep reinforcement learning algorithm while providing numerical
evidence for its efficacy. Specifically, we introduce discrete Soft
Actor-Critic for the entropic risk measure by deriving a version of the Bellman
equation for the respective Q-values. We establish a corresponding policy
improvement result and infer a practical algorithm. We introduce an environment
that represents typical contextual multi-stage stochastic combinatorial
optimization problems and perform numerical experiments to empirically validate
our algorithm's robustness against realistic distribution shifts, without
compromising performance on the training distribution. We show that our
algorithm is superior to risk-neutral Soft Actor-Critic as well as to two
benchmark approaches for robust deep reinforcement learning. Thereby, we
provide the first structured analysis on the robustness of reinforcement
learning under distribution shifts in the realm of contextual multi-stage
stochastic combinatorial optimization problems.</div><div><a href='http://arxiv.org/abs/2402.09992v1'>2402.09992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06011v1")'>Reinforcement Learning Paycheck Optimization for Multivariate Financial
  Goals</div>
<div id='2403.06011v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:10:10Z</div><div>Authors: Melda Alaluf, Giulia Crippa, Sinong Geng, Zijian Jing, Nikhil Krishnan, Sanjeev Kulkarni, Wyatt Navarro, Ronnie Sircar, Jonathan Tang</div><div style='padding-top: 10px; width: 80ex'>We study paycheck optimization, which examines how to allocate income in
order to achieve several competing financial goals. For paycheck optimization,
a quantitative methodology is missing, due to a lack of a suitable problem
formulation. To deal with this issue, we formulate the problem as a utility
maximization problem. The proposed formulation is able to (i) unify different
financial goals; (ii) incorporate user preferences regarding the goals; (iii)
handle stochastic interest rates. The proposed formulation also facilitates an
end-to-end reinforcement learning solution, which is implemented on a variety
of problem settings.</div><div><a href='http://arxiv.org/abs/2403.06011v1'>2403.06011v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11835v1")'>Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret
  Minimization</div>
<div id='2402.11835v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T04:58:39Z</div><div>Authors: Luca D'Amico-Wong, Hugh Zhang, Marc Lanctot, David C. Parkes</div><div style='padding-top: 10px; width: 80ex'>We propose ABCs (Adaptive Branching through Child stationarity), a
best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic
reinforcement learning algorithm for single-agent domains, and counterfactual
regret minimization (CFR), a central algorithm for learning in multi-agent
domains. ABCs adaptively chooses what fraction of the environment to explore
each iteration by measuring the stationarity of the environment's reward and
transition dynamics. In Markov decision processes, ABCs converges to the
optimal policy with at most an O(A) factor slowdown compared to BQL, where A is
the number of actions in the environment. In two-player zero-sum games, ABCs is
guaranteed to converge to a Nash equilibrium (assuming access to a perfect
oracle for detecting stationarity), while BQL has no such guarantees.
Empirically, ABCs demonstrates strong performance when benchmarked across
environments drawn from the OpenSpiel game library and OpenAI Gym and exceeds
all prior methods in environments which are neither fully stationary nor fully
nonstationary.</div><div><a href='http://arxiv.org/abs/2402.11835v1'>2402.11835v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04344v1")'>RL-CFR: Improving Action Abstraction for Imperfect Information
  Extensive-Form Games with Reinforcement Learning</div>
<div id='2403.04344v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T09:12:23Z</div><div>Authors: Boning Li, Zhixuan Fang, Longbo Huang</div><div style='padding-top: 10px; width: 80ex'>Effective action abstraction is crucial in tackling challenges associated
with large action spaces in Imperfect Information Extensive-Form Games
(IIEFGs). However, due to the vast state space and computational complexity in
IIEFGs, existing methods often rely on fixed abstractions, resulting in
sub-optimal performance. In response, we introduce RL-CFR, a novel
reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR
builds upon our innovative Markov Decision Process (MDP) formulation, with
states corresponding to public information and actions represented as feature
vectors indicating specific action abstractions. The reward is defined as the
expected payoff difference between the selected and default action
abstractions. RL-CFR constructs a game tree with RL-guided action abstractions
and utilizes counterfactual regret minimization (CFR) for strategy derivation.
Impressively, it can be trained from scratch, achieving higher expected payoff
without increased CFR solving time. In experiments on Heads-up No-limit Texas
Hold'em, RL-CFR outperforms ReBeL's replication and Slumbot, demonstrating
significant win-rate margins of $64\pm 11$ and $84\pm 17$ mbb/hand,
respectively.</div><div><a href='http://arxiv.org/abs/2403.04344v1'>2403.04344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18617v1")'>ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games</div>
<div id='2402.18617v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:44:02Z</div><div>Authors: Shiqi Lei, Kanghoon Lee, Linjing Li, Jinkyoo Park, Jiachen Li</div><div style='padding-top: 10px; width: 80ex'>Offline learning has become widely used due to its ability to derive
effective policies from offline datasets gathered by expert demonstrators
without interacting with the environment directly. Recent research has explored
various ways to enhance offline learning efficiency by considering the
characteristics (e.g., expertise level or multiple demonstrators) of the
dataset. However, a different approach is necessary in the context of zero-sum
games, where outcomes vary significantly based on the strategy of the opponent.
In this study, we introduce a novel approach that uses unsupervised learning
techniques to estimate the exploited level of each trajectory from the offline
dataset of zero-sum games made by diverse demonstrators. Subsequently, we
incorporate the estimated exploited level into the offline learning to maximize
the influence of the dominant strategy. Our method enables interpretable
exploited level estimation in multiple zero-sum games and effectively
identifies dominant strategy data. Also, our exploited level augmented offline
learning significantly enhances the original offline learning algorithms
including imitation learning and offline reinforcement learning for zero-sum
games.</div><div><a href='http://arxiv.org/abs/2402.18617v1'>2402.18617v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00841v1")'>Offline Fictitious Self-Play for Competitive Games</div>
<div id='2403.00841v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T11:36:48Z</div><div>Authors: Jingxiao Chen, Weiji Xie, Weinan Zhang, Yong yu, Ying Wen</div><div style='padding-top: 10px; width: 80ex'>Offline Reinforcement Learning (RL) has received significant interest due to
its ability to improve policies in previously collected datasets without online
interactions. Despite its success in the single-agent setting, offline
multi-agent RL remains a challenge, especially in competitive games. Firstly,
unaware of the game structure, it is impossible to interact with the opponents
and conduct a major learning paradigm, self-play, for competitive games.
Secondly, real-world datasets cannot cover all the state and action space in
the game, resulting in barriers to identifying Nash equilibrium (NE). To
address these issues, this paper introduces Off-FSP, the first practical
model-free offline RL algorithm for competitive games. We start by simulating
interactions with various opponents by adjusting the weights of the fixed
dataset with importance sampling. This technique allows us to learn best
responses to different opponents and employ the Offline Self-Play learning
framework. In this framework, we further implement Fictitious Self-Play (FSP)
to approximate NE. In partially covered real-world datasets, our methods show
the potential to approach NE by incorporating any single-agent offline RL
method. Experimental results in Leduc Hold'em Poker show that our method
significantly improves performances compared with state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2403.00841v1'>2403.00841v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03552v1")'>Population-aware Online Mirror Descent for Mean-Field Games by Deep
  Reinforcement Learning</div>
<div id='2403.03552v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:55:34Z</div><div>Authors: Zida Wu, Mathieu Lauriere, Samuel Jia Cong Chua, Matthieu Geist, Olivier Pietquin, Ankur Mehta</div><div style='padding-top: 10px; width: 80ex'>Mean Field Games (MFGs) have the ability to handle large-scale multi-agent
systems, but learning Nash equilibria in MFGs remains a challenging task. In
this paper, we propose a deep reinforcement learning (DRL) algorithm that
achieves population-dependent Nash equilibrium without the need for averaging
or sampling from history, inspired by Munchausen RL and Online Mirror Descent.
Through the design of an additional inner-loop replay buffer, the agents can
effectively learn to achieve Nash equilibrium from any distribution, mitigating
catastrophic forgetting. The resulting policy can be applied to various initial
distributions. Numerical experiments on four canonical examples demonstrate our
algorithm has better convergence properties than SOTA algorithms, in particular
a DRL version of Fictitious Play for population-dependent policies.</div><div><a href='http://arxiv.org/abs/2403.03552v1'>2403.03552v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12406v1")'>Offline Imitation of Badminton Player Behavior via Experiential Contexts
  and Brownian Motion</div>
<div id='2403.12406v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T03:34:23Z</div><div>Authors: Kuang-Da Wang, Wei-Yao Wang, Ping-Chun Hsieh, Wen-Chih Peng</div><div style='padding-top: 10px; width: 80ex'>In the dynamic and rapid tactic involvements of turn-based sports, badminton
stands out as an intrinsic paradigm that requires alter-dependent
decision-making of players. While the advancement of learning from offline
expert data in sequential decision-making has been witnessed in various
domains, how to rally-wise imitate the behaviors of human players from offline
badminton matches has remained underexplored. Replicating opponents' behavior
benefits players by allowing them to undergo strategic development with
direction before matches. However, directly applying existing methods suffers
from the inherent hierarchy of the match and the compounding effect due to the
turn-based nature of players alternatively taking actions. In this paper, we
propose RallyNet, a novel hierarchical offline imitation learning model for
badminton player behaviors: (i) RallyNet captures players' decision
dependencies by modeling decision-making processes as a contextual Markov
decision process. (ii) RallyNet leverages the experience to generate context as
the agent's intent in the rally. (iii) To generate more realistic behavior,
RallyNet leverages Geometric Brownian Motion (GBM) to model the interactions
between players by introducing a valuable inductive bias for learning player
behaviors. In this manner, RallyNet links player intents with interaction
models with GBM, providing an understanding of interactions for sports
analytics. We extensively validate RallyNet with the largest available
real-world badminton dataset consisting of men's and women's singles,
demonstrating its ability to imitate player behaviors. Results reveal
RallyNet's superiority over offline imitation learning methods and
state-of-the-art turn-based approaches, outperforming them by at least 16% in
mean rule-based agent normalization score. Furthermore, we discuss various
practical use cases to highlight RallyNet's applicability.</div><div><a href='http://arxiv.org/abs/2403.12406v1'>2403.12406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14194v1")'>BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human
  Racing Gameplay</div>
<div id='2402.14194v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T00:38:43Z</div><div>Authors: Catherine Weaver, Chen Tang, Ce Hao, Kenta Kawamoto, Masayoshi Tomizuka, Wei Zhan</div><div style='padding-top: 10px; width: 80ex'>Imitation learning learns a policy from demonstrations without requiring
hand-designed reward functions. In many robotic tasks, such as autonomous
racing, imitated policies must model complex environment dynamics and human
decision-making. Sequence modeling is highly effective in capturing intricate
patterns of motion sequences but struggles to adapt to new environments or
distribution shifts that are common in real-world robotics tasks. In contrast,
Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles
with sample inefficiency and handling complex motion patterns. Thus, we propose
BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a
Behavior Transformer (BeT) policy from human demonstrations with online AIL.
BeTAIL adds an AIL residual policy to the BeT policy to model the sequential
decision-making process of human experts and correct for out-of-distribution
states or shifts in environment dynamics. We test BeTAIL on three challenges
with expert-level demonstrations of real human gameplay in Gran Turismo Sport.
Our proposed residual BeTAIL reduces environment interactions and improves
racing performance and stability, even when the BeT is pretrained on different
tracks than downstream learning. Videos and code available at:
https://sites.google.com/berkeley.edu/BeTAIL/home.</div><div><a href='http://arxiv.org/abs/2402.14194v1'>2402.14194v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03406v1")'>Improving Dribbling, Passing, and Marking Actions in Soccer Simulation
  2D Games Using Machine Learning</div>
<div id='2401.03406v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T07:54:26Z</div><div>Authors: Nader Zare, Omid Amini, Aref Sayareh, Mahtab Sarvmaili, Arad Firouzkouhi, Stan Matwin, Amilcar Soares</div><div style='padding-top: 10px; width: 80ex'>The RoboCup competition was started in 1997, and is known as the oldest
RoboCup league. The RoboCup 2D Soccer Simulation League is a stochastic,
partially observable soccer environment in which 24 autonomous agents play on
two opposing teams. In this paper, we detail the main strategies and
functionalities of CYRUS, the RoboCup 2021 2D Soccer Simulation League
champions. The new functionalities presented and discussed in this work are (i)
Multi Action Dribble, (ii) Pass Prediction and (iii) Marking Decision. The
Multi Action Dribbling strategy enabled CYRUS to succeed more often and to be
safer when dribbling actions were performed during a game. The Pass Prediction
enhanced our gameplay by predicting our teammate's passing behavior,
anticipating and making our agents collaborate better towards scoring goals.
Finally, the Marking Decision addressed the multi-agent matching problem to
improve CYRUS defensive strategy by finding an optimal solution to mark
opponents' players.</div><div><a href='http://arxiv.org/abs/2401.03406v1'>2401.03406v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03410v1")'>Engineering Features to Improve Pass Prediction in Soccer Simulation 2D
  Games</div>
<div id='2401.03410v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T08:01:25Z</div><div>Authors: Nader Zare, Mahtab Sarvmaili, Aref Sayareh, Omid Amini, Stan Matwin Amilcar Soares</div><div style='padding-top: 10px; width: 80ex'>Soccer Simulation 2D (SS2D) is a simulation of a real soccer game in two
dimensions. In soccer, passing behavior is an essential action for keeping the
ball in possession of our team and creating goal opportunities. Similarly, for
SS2D, predicting the passing behaviors of both opponents and our teammates
helps manage resources and score more goals. Therefore, in this research, we
have tried to address the modeling of passing behavior of soccer 2D players
using Deep Neural Networks (DNN) and Random Forest (RF). We propose an embedded
data extraction module that can record the decision-making of agents in an
online format. Afterward, we apply four data sorting techniques for training
data preparation. After, we evaluate the trained models' performance playing
against 6 top teams of RoboCup 2019 that have distinctive playing strategies.
Finally, we examine the importance of different feature groups on the
prediction of a passing strategy. All results in each step of this work prove
our suggested methodology's effectiveness and improve the performance of the
pass prediction in Soccer Simulation 2D games ranging from 5\% (e.g., playing
against the same team) to 10\% (e.g., playing against Robocup top teams).</div><div><a href='http://arxiv.org/abs/2401.03410v1'>2401.03410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07669v1")'>Machine Learning for Soccer Match Result Prediction</div>
<div id='2403.07669v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T14:00:50Z</div><div>Authors: Rory Bunker, Calvin Yeung, Keisuke Fujii</div><div style='padding-top: 10px; width: 80ex'>Machine learning has become a common approach to predicting the outcomes of
soccer matches, and the body of literature in this domain has grown
substantially in the past decade and a half. This chapter discusses available
datasets, the types of models and features, and ways of evaluating model
performance in this application domain. The aim of this chapter is to give a
broad overview of the current state and potential future developments in
machine learning for soccer match results prediction, as a resource for those
interested in conducting future studies in the area. Our main findings are that
while gradient-boosted tree models such as CatBoost, applied to soccer-specific
ratings such as pi-ratings, are currently the best-performing models on
datasets containing only goals as the match features, there needs to be a more
thorough comparison of the performance of deep learning models and Random
Forest on a range of datasets with different types of features. Furthermore,
new rating systems using both player- and team-level information and
incorporating additional information from, e.g., spatiotemporal tracking and
event data, could be investigated further. Finally, the interpretability of
match result prediction models needs to be enhanced for them to be more useful
for team management.</div><div><a href='http://arxiv.org/abs/2403.07669v1'>2403.07669v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09650v1")'>Foul prediction with estimated poses from soccer broadcast video</div>
<div id='2402.09650v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T01:25:19Z</div><div>Authors: Jiale Fang, Calvin Yeung, Keisuke Fujii</div><div style='padding-top: 10px; width: 80ex'>Recent advances in computer vision have made significant progress in tracking
and pose estimation of sports players. However, there have been fewer studies
on behavior prediction with pose estimation in sports, in particular, the
prediction of soccer fouls is challenging because of the smaller image size of
each player and of difficulty in the usage of e.g., the ball and pose
information. In our research, we introduce an innovative deep learning approach
for anticipating soccer fouls. This method integrates video data, bounding box
positions, image details, and pose information by curating a novel soccer foul
dataset. Our model utilizes a combination of convolutional and recurrent neural
networks (CNNs and RNNs) to effectively merge information from these four
modalities. The experimental results show that our full model outperformed the
ablated models, and all of the RNN modules, bounding box position and image,
and estimated pose were useful for the foul prediction. Our findings have
important implications for a deeper understanding of foul play in soccer and
provide a valuable reference for future research and practice in this area.</div><div><a href='http://arxiv.org/abs/2402.09650v1'>2402.09650v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00031v1")'>An Integrated Framework for Team Formation and Winner Prediction in the
  FIRST Robotics Competition: Model, Algorithm, and Analysis</div>
<div id='2402.00031v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T23:11:50Z</div><div>Authors: Federico Galbiati, Ranier X. Gran, Brendan D. Jacques, Sullivan J. Mulhern, Chun-Kit Ngan</div><div style='padding-top: 10px; width: 80ex'>This research work aims to develop an analytical approach for optimizing team
formation and predicting team performance in a competitive environment based on
data on the competitors' skills prior to the team formation. There are several
approaches in scientific literature to optimize and predict a team's
performance. However, most studies employ fine-grained skill statistics of the
individual members or constraints such as teams with a set group of members.
Currently, no research tackles the highly constrained domain of the FIRST
Robotics Competition. This research effort aims to fill this gap by providing
an analytical method for optimizing and predicting team performance in a
competitive environment while allowing these constraints and only using metrics
on previous team performance, not on each individual member's performance. We
apply our method to the drafting process of the FIRST Robotics competition, a
domain in which the skills change year-over-year, team members change
throughout the season, each match only has a superficial set of statistics, and
alliance formation is key to competitive success. First, we develop a method
that could extrapolate individual members' performance based on overall team
performance. An alliance optimization algorithm is developed to optimize team
formation and a deep neural network model is trained to predict the winning
team, both using highly post-processed real-world data. Our method is able to
successfully extract individual members' metrics from overall team statistics,
form competitive teams, and predict the winning team with 84.08% accuracy.</div><div><a href='http://arxiv.org/abs/2402.00031v1'>2402.00031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16235v2")'>Player Pressure Map -- A Novel Representation of Pressure in Soccer for
  Evaluating Player Performance in Different Game Contexts</div>
<div id='2401.16235v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T15:34:49Z</div><div>Authors: Chaoyi Gu, Jiaming Na, Yisheng Pei, Varuna De Silva</div><div style='padding-top: 10px; width: 80ex'>In soccer, contextual player performance metrics are invaluable to coaches.
For example, the ability to perform under pressure during matches distinguishes
the elite from the average. Appropriate pressure metric enables teams to assess
players' performance accurately under pressure and design targeted training
scenarios to address their weaknesses. The primary objective of this paper is
to leverage both tracking and event data and game footage to capture the
pressure experienced by the possession team in a soccer game scene. We propose
a player pressure map to represent a given game scene, which lowers the
dimension of raw data and still contains rich contextual information. Not only
does it serve as an effective tool for visualizing and evaluating the pressure
on the team and each individual, but it can also be utilized as a backbone for
accessing players' performance. Overall, our model provides coaches and
analysts with a deeper understanding of players' performance under pressure so
that they make data-oriented tactical decisions.</div><div><a href='http://arxiv.org/abs/2401.16235v2'>2401.16235v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12400v1")'>Estimating the age-conditioned average treatment effects curves: An
  application for assessing load-management strategies in the NBA</div>
<div id='2402.12400v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T20:16:41Z</div><div>Authors: Shinpei Nakamura-Sakai, Laura Forastiere, Brian Macdonald</div><div style='padding-top: 10px; width: 80ex'>In the realm of competitive sports, understanding the performance dynamics of
athletes, represented by the age curve (showing progression, peak, and
decline), is vital. Our research introduces a novel framework for quantifying
age-specific treatment effects, enhancing the granularity of performance
trajectory analysis. Firstly, we propose a methodology for estimating the age
curve using game-level data, diverging from traditional season-level data
approaches, and tackling its inherent complexities with a meta-learner
framework that leverages advanced machine learning models. This approach
uncovers intricate non-linear patterns missed by existing methods. Secondly,
our framework enables the identification of causal effects, allowing for a
detailed examination of age curves under various conditions. By defining the
Age-Conditioned Treatment Effect (ACTE), we facilitate the exploration of
causal relationships regarding treatment impacts at specific ages. Finally,
applying this methodology to study the effects of rest days on performance
metrics, particularly across different ages, offers valuable insights into load
management strategies' effectiveness. Our findings underscore the importance of
tailored rest periods, highlighting their positive impact on athlete
performance and suggesting a reevaluation of current management practices for
optimizing athlete performance.</div><div><a href='http://arxiv.org/abs/2402.12400v1'>2402.12400v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12149v1")'>MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore
  the Momentum in Competitive Sports</div>
<div id='2402.12149v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:02:13Z</div><div>Authors: Ruixin Peng, Ziqing Li</div><div style='padding-top: 10px; width: 80ex'>Tennis is so popular that coaches and players are curious about factors other
than skill, such as momentum. This article will try to define and quantify
momentum, providing a basis for real-time analysis of tennis matches. Based on
the tennis Grand Slam men's singles match data in recent years, we built two
models, one is to build a model based on data-driven, and the other is to build
a model based on empirical formulas. For the data-driven model, we first found
a large amount of public data including public data on tennis matches in the
past five years and personal information data of players. Then the data is
preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest
algorithm and XGBoost was established. For the mechanism analysis model,
important features were selected based on the suggestions of many tennis
players and enthusiasts, the sliding window algorithm was used to calculate the
weight, and different methods were used to visualize the momentum. For further
analysis of the momentum fluctuation, it is based on the popular CUMSUM
algorithm in the industry as well as the RUN Test, and the result shows the
momentum is not random and the trend might be random. At last, the robustness
of the fusion model is analyzed by Monte Carlo simulation.</div><div><a href='http://arxiv.org/abs/2402.12149v1'>2402.12149v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06086v1")'>XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an
  Agent-Based Model of a Sports Betting Exchange</div>
<div id='2401.06086v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:03:17Z</div><div>Authors: Chawin Terawong, Dave Cliff</div><div style='padding-top: 10px; width: 80ex'>We present first results from the use of XGBoost, a highly effective machine
learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source
agent-based model (ABM) designed to simulate a contemporary sports-betting
exchange with in-play betting during track-racing events such as horse races.
We use the BBE ABM and its array of minimally-simple bettor-agents as a
synthetic data generator which feeds into our XGBoost ML system, with the
intention that XGBoost discovers profitable dynamic betting strategies by
learning from the more profitable bets made by the BBE bettor-agents. After
this XGBoost training, which results in one or more decision trees, a
bettor-agent with a betting strategy determined by the XGBoost-learned decision
tree(s) is added to the BBE ABM and made to bet on a sequence of races under
various conditions and betting-market scenarios, with profitability serving as
the primary metric of comparison and evaluation. Our initial findings presented
here show that XGBoost trained in this way can indeed learn profitable betting
strategies, and can generalise to learn strategies that outperform each of the
set of strategies used for creation of the training data. To foster further
research and enhancements, the complete version of our extended BBE, including
the XGBoost integration, has been made freely available as an open-source
release on GitHub.</div><div><a href='http://arxiv.org/abs/2401.06086v1'>2401.06086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08718v1")'>Investigating Fouling Efficiency in Football Using Expected Booking (xB)
  Model</div>
<div id='2401.08718v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T05:21:51Z</div><div>Authors: Adnan Azmat, Su Su Yi</div><div style='padding-top: 10px; width: 80ex'>This paper introduces the Expected Booking (xB) model, a novel metric
designed to estimate the likelihood of a foul resulting in a yellow card in
football. Through three iterative experiments, employing ensemble methods, the
model demonstrates improved performance with additional features and an
expanded dataset. Analysis of FIFA World Cup 2022 data validates the model's
efficacy in providing insights into team and player fouling tactics, aligning
with actual defensive performance. The xB model addresses a gap in fouling
efficiency examination, emphasizing defensive strategies which often
overlooked. Further enhancements are suggested through the incorporation of
comprehensive data and spatial features.</div><div><a href='http://arxiv.org/abs/2401.08718v1'>2401.08718v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16795v1")'>Performance Insights-based AI-driven Football Transfer Fee Prediction</div>
<div id='2401.16795v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T07:16:09Z</div><div>Authors: Daniil Sulimov</div><div style='padding-top: 10px; width: 80ex'>We developed an artificial intelligence approach to predict the transfer fee
of a football player. This model can help clubs make better decisions about
which players to buy and sell, which can lead to improved performance and
increased club budgets. Having collected data on player performance, transfer
fees, and other factors that might affect a player's value, we then used this
data to train a machine learning model that can accurately predict a player's
impact on the game. We further passed the obtained results as one of the
features to the predictor of transfer fees. The model can help clubs identify
players who are undervalued and who could be sold for a profit. It can also
help clubs avoid overpaying for players. We believe that our model can be a
valuable tool for football clubs. It can help them make better decisions about
player recruitment and transfers.</div><div><a href='http://arxiv.org/abs/2401.16795v1'>2401.16795v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12203v1")'>Bootstrapping Reinforcement Learning with Imitation for Vision-Based
  Agile Flight</div>
<div id='2403.12203v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T19:25:57Z</div><div>Authors: Jiaxu Xing, Angel Romero, Leonard Bauersfeld, Davide Scaramuzza</div><div style='padding-top: 10px; width: 80ex'>We combine the effectiveness of Reinforcement Learning (RL) and the
efficiency of Imitation Learning (IL) in the context of vision-based,
autonomous drone racing. We focus on directly processing visual input without
explicit state estimation. While RL offers a general framework for learning
complex controllers through trial and error, it faces challenges regarding
sample efficiency and computational demands due to the high dimensionality of
visual inputs. Conversely, IL demonstrates efficiency in learning from visual
demonstrations but is limited by the quality of those demonstrations and faces
issues like covariate shift. To overcome these limitations, we propose a novel
training framework combining RL and IL's advantages. Our framework involves
three stages: initial training of a teacher policy using privileged state
information, distilling this policy into a student policy using IL, and
performance-constrained adaptive RL fine-tuning. Our experiments in both
simulated and real-world environments demonstrate that our approach achieves
superior performance and robustness than IL or RL alone in navigating a
quadrotor through a racing course using only visual information without
explicit state estimation.</div><div><a href='http://arxiv.org/abs/2403.12203v1'>2403.12203v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02468v1")'>Fast Peer Adaptation with Context-aware Exploration</div>
<div id='2402.02468v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T13:02:27Z</div><div>Authors: Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, Yizhou Wang</div><div style='padding-top: 10px; width: 80ex'>Fast adapting to unknown peers (partners or opponents) with different
strategies is a key challenge in multi-agent games. To do so, it is crucial for
the agent to efficiently probe and identify the peer's strategy, as this is the
prerequisite for carrying out the best response in adaptation. However, it is
difficult to explore the strategies of unknown peers, especially when the games
are partially observable and have a long horizon. In this paper, we propose a
peer identification reward, which rewards the learning agent based on how well
it can identify the behavior pattern of the peer over the historical context,
such as the observation over multiple episodes. This reward motivates the agent
to learn a context-aware policy for effective exploration and fast adaptation,
i.e., to actively seek and collect informative feedback from peers when
uncertain about their policies and to exploit the context to perform the best
response when confident. We evaluate our method on diverse testbeds that
involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed
(Predator-Prey-W) games with peer agents. We demonstrate that our method
induces more active exploration behavior, achieving faster adaptation and
better outcomes than existing methods.</div><div><a href='http://arxiv.org/abs/2402.02468v1'>2402.02468v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01502v1")'>Pontryagin Neural Operator for Solving Parametric General-Sum
  Differential Games</div>
<div id='2401.01502v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T02:15:32Z</div><div>Authors: Lei Zhang, Mukesh Ghimire, Zhe Xu, Wenlong Zhang, Yi Ren</div><div style='padding-top: 10px; width: 80ex'>The values of two-player general-sum differential games are viscosity
solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy
approximations for such games suffer from the curse of dimensionality (CoD).
Alleviating CoD through physics-informed neural networks (PINN) encounters
convergence issues when value discontinuity is present due to state
constraints. On top of these challenges, it is often necessary to learn
generalizable values and policies across a parametric space of games, e.g., for
game parameter inference when information is incomplete. To address these
challenges, we propose in this paper a Pontryagin-mode neural operator that
outperforms existing state-of-the-art (SOTA) on safety performance across games
with parametric state constraints. Our key contribution is the introduction of
a costate loss defined on the discrepancy between forward and backward costate
rollouts, which are computationally cheap. We show that the discontinuity of
costate dynamics (in the presence of state constraints) effectively enables the
learning of discontinuous values, without requiring manually supervised data as
suggested by the current SOTA. More importantly, we show that the close
relationship between costates and policies makes the former critical in
learning feedback control policies with generalizable safety performance.</div><div><a href='http://arxiv.org/abs/2401.01502v1'>2401.01502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07107v2")'>Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential
  Reinforcement Learning</div>
<div id='2402.07107v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T05:17:56Z</div><div>Authors: Alex Christopher Stutts, Danilo Erricolo, Theja Tulabandhula, Amit Ranjan Trivedi</div><div style='padding-top: 10px; width: 80ex'>We present a novel statistical approach to incorporating uncertainty
awareness in model-free distributional reinforcement learning involving
quantile regression-based deep Q networks. The proposed algorithm,
$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks
(CEQR-DQN)}$, aims to address key challenges associated with separately
estimating aleatoric and epistemic uncertainty in stochastic environments. It
combines deep evidential learning with quantile calibration based on principles
of conformal inference to provide explicit, sample-free computations of
$\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on
simple variance, overcoming limitations of traditional methods in computational
and statistical efficiency and handling of out-of-distribution (OOD)
observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar),
CEQR-DQN is shown to surpass similar existing frameworks in scores and learning
speed. Its ability to rigorously evaluate uncertainty improves exploration
strategies and can serve as a blueprint for other algorithms requiring
uncertainty awareness.</div><div><a href='http://arxiv.org/abs/2402.07107v2'>2402.07107v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13530v1")'>Best of Many in Both Worlds: Online Resource Allocation with Predictions
  under Unknown Arrival Model</div>
<div id='2402.13530v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T04:57:32Z</div><div>Authors: Lin An, Andrew A. Li, Benjamin Moseley, Gabriel Visotsky</div><div style='padding-top: 10px; width: 80ex'>Online decision-makers today can often obtain predictions on future
variables, such as arrivals, demands, inventories, and so on. These predictions
can be generated from simple forecasting algorithms for univariate time-series,
all the way to state-of-the-art machine learning models that leverage multiple
time-series and additional feature information. However, the prediction quality
is often unknown to decisions-makers a priori, hence blindly following the
predictions can be harmful. In this paper, we address this problem by giving
algorithms that take predictions as inputs and perform robustly against the
unknown prediction quality.
  We consider the online resource allocation problem, one of the most generic
models in revenue management and online decision-making. In this problem, a
decision maker has a limited amount of resources, and requests arrive
sequentially. For each request, the decision-maker needs to decide on an
action, which generates a certain amount of rewards and consumes a certain
amount of resources, without knowing the future requests. The decision-maker's
objective is to maximize the total rewards subject to resource constraints. We
take the shadow price of each resource as prediction, which can be obtained by
predictions on future requests. Prediction quality is naturally defined to be
the $\ell_1$ distance between the prediction and the actual shadow price. Our
main contribution is an algorithm which takes the prediction of unknown quality
as an input, and achieves asymptotically optimal performance under both
requests arrival models (stochastic and adversarial) without knowing the
prediction quality and the requests arrival model beforehand. We show our
algorithm's performance matches the best achievable performance of any
algorithm had the arrival models and the accuracy of the predictions been
known. We empirically validate our algorithm with experiments.</div><div><a href='http://arxiv.org/abs/2402.13530v1'>2402.13530v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16945v1")'>Online Resource Allocation with Non-Stationary Customers</div>
<div id='2401.16945v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T12:19:09Z</div><div>Authors: Xiaoyue Zhang, Hanzhang Qin, Mabel C. Chou</div><div style='padding-top: 10px; width: 80ex'>We propose a novel algorithm for online resource allocation with
non-stationary customer arrivals and unknown click-through rates. We assume
multiple types of customers arrive in a nonstationary stochastic fashion, with
unknown arrival rates in each period, and that customers' click-through rates
are unknown and can only be learned online. By leveraging results from the
stochastic contextual bandit with knapsack and online matching with adversarial
arrivals, we develop an online scheme to allocate the resources to
nonstationary customers. We prove that under mild conditions, our scheme
achieves a ``best-of-both-world'' result: the scheme has a sublinear regret
when the customer arrivals are near-stationary, and enjoys an optimal
competitive ratio under general (non-stationary) customer arrival
distributions. Finally, we conduct extensive numerical experiments to show our
approach generates near-optimal revenues for all different customer scenarios.</div><div><a href='http://arxiv.org/abs/2401.16945v1'>2401.16945v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12562v1")'>Dynamic Pricing and Learning with Long-term Reference Effects</div>
<div id='2402.12562v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T21:36:54Z</div><div>Authors: Shipra Agrawal, Wei Tang</div><div style='padding-top: 10px; width: 80ex'>We consider a dynamic pricing problem where customer response to the current
price is impacted by the customer price expectation, aka reference price. We
study a simple and novel reference price mechanism where reference price is the
average of the past prices offered by the seller. As opposed to the more
commonly studied exponential smoothing mechanism, in our reference price
mechanism the prices offered by seller have a longer term effect on the future
customer expectations.
  We show that under this mechanism, a markdown policy is near-optimal
irrespective of the parameters of the model. This matches the common intuition
that a seller may be better off by starting with a higher price and then
decreasing it, as the customers feel like they are getting bargains on items
that are ordinarily more expensive. For linear demand models, we also provide a
detailed characterization of the near-optimal markdown policy along with an
efficient way of computing it.
  We then consider a more challenging dynamic pricing and learning problem,
where the demand model parameters are apriori unknown, and the seller needs to
learn them online from the customers' responses to the offered prices while
simultaneously optimizing revenue. The objective is to minimize regret, i.e.,
the $T$-round revenue loss compared to a clairvoyant optimal policy. This task
essentially amounts to learning a non-stationary optimal policy in a
time-variant Markov Decision Process (MDP). For linear demand models, we
provide an efficient learning algorithm with an optimal $\tilde{O}(\sqrt{T})$
regret upper bound.</div><div><a href='http://arxiv.org/abs/2402.12562v1'>2402.12562v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14844v1")'>The New Era of Dynamic Pricing: Synergizing Supervised Learning and
  Quadratic Programming</div>
<div id='2402.14844v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T12:48:02Z</div><div>Authors: Gustavo Bramao, Ilia Tarygin</div><div style='padding-top: 10px; width: 80ex'>In this paper, we explore a novel combination of supervised learning and
quadratic programming to refine dynamic pricing models in the car rental
industry. We utilize dynamic modeling of price elasticity, informed by ordinary
least squares (OLS) metrics such as p-values, homoscedasticity, error
normality. These metrics, when their underlying assumptions hold, are integral
in guiding a quadratic programming agent. The program is tasked with optimizing
margin for a given finite set target.</div><div><a href='http://arxiv.org/abs/2402.14844v1'>2402.14844v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03812v1")'>ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing</div>
<div id='2403.03812v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:00:50Z</div><div>Authors: Kiran Madhusudhanan, Gunnar Behrens, Maximilian Stubbemann, Lars Schmidt-Thieme</div><div style='padding-top: 10px; width: 80ex'>Used car pricing is a critical aspect of the automotive industry, influenced
by many economic factors and market dynamics. With the recent surge in online
marketplaces and increased demand for used cars, accurate pricing would benefit
both buyers and sellers by ensuring fair transactions. However, the transition
towards automated pricing algorithms using machine learning necessitates the
comprehension of model uncertainties, specifically the ability to flag
predictions that the model is unsure about. Although recent literature proposes
the use of boosting algorithms or nearest neighbor-based approaches for swift
and precise price predictions, encapsulating model uncertainties with such
algorithms presents a complex challenge. We introduce ProbSAINT, a model that
offers a principled approach for uncertainty quantification of its price
predictions, along with accurate point predictions that are comparable to
state-of-the-art boosting techniques. Furthermore, acknowledging that the
business prefers pricing used cars based on the number of days the vehicle was
listed for sale, we show how ProbSAINT can be used as a dynamic forecasting
model for predicting price probabilities for different expected offer duration.
Our experiments further indicate that ProbSAINT is especially accurate on
instances where it is highly certain. This proves the applicability of its
probabilistic predictions in real-world scenarios where trustworthiness is
crucial.</div><div><a href='http://arxiv.org/abs/2403.03812v1'>2403.03812v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03564v1")'>SkipPredict: When to Invest in Predictions for Scheduling</div>
<div id='2402.03564v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:24:19Z</div><div>Authors: Rana Shahout, Michael Mitzenmacher</div><div style='padding-top: 10px; width: 80ex'>In light of recent work on scheduling with predicted job sizes, we consider
the effect of the cost of predictions in queueing systems, removing the
assumption in prior research that predictions are external to the system's
resources and/or cost-free. In particular, we introduce a novel approach to
utilizing predictions, SkipPredict, designed to address their inherent cost.
Rather than uniformly applying predictions to all jobs, we propose a tailored
approach that categorizes jobs based on their prediction requirements. To
achieve this, we employ one-bit "cheap predictions" to classify jobs as either
short or long. SkipPredict prioritizes predicted short jobs over long jobs, and
for the latter, SkipPredict applies a second round of more detailed "expensive
predictions" to approximate Shortest Remaining Processing Time for these jobs.
Our analysis takes into account the cost of prediction. We examine the effect
of this cost for two distinct models. In the external cost model, predictions
are generated by some external method without impacting job service times but
incur a cost. In the server time cost model, predictions themselves require
server processing time, and are scheduled on the same server as the jobs.</div><div><a href='http://arxiv.org/abs/2402.03564v1'>2402.03564v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02801v1")'>Credence: Augmenting Datacenter Switch Buffer Sharing with ML
  Predictions</div>
<div id='2401.02801v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T13:29:59Z</div><div>Authors: Vamsi Addanki, Maciej Pacut, Stefan Schmid</div><div style='padding-top: 10px; width: 80ex'>Packet buffers in datacenter switches are shared across all the switch ports
in order to improve the overall throughput. The trend of shrinking buffer sizes
in datacenter switches makes buffer sharing extremely challenging and a
critical performance issue. Literature suggests that push-out buffer sharing
algorithms have significantly better performance guarantees compared to
drop-tail algorithms. Unfortunately, switches are unable to benefit from these
algorithms due to lack of support for push-out operations in hardware. Our key
observation is that drop-tail buffers can emulate push-out buffers if the
future packet arrivals are known ahead of time. This suggests that augmenting
drop-tail algorithms with predictions about the future arrivals has the
potential to significantly improve performance.
  This paper is the first research attempt in this direction. We propose
Credence, a drop-tail buffer sharing algorithm augmented with machine-learned
predictions. Credence can unlock the performance only attainable by push-out
algorithms so far. Its performance hinges on the accuracy of predictions.
Specifically, Credence achieves near-optimal performance of the best known
push-out algorithm LQD (Longest Queue Drop) with perfect predictions, but
gracefully degrades to the performance of the simplest drop-tail algorithm
Complete Sharing when the prediction error gets arbitrarily worse. Our
evaluations show that Credence improves throughput by $1.5$x compared to
traditional approaches. In terms of flow completion times, we show that
Credence improves upon the state-of-the-art approaches by up to $95\%$ using
off-the-shelf machine learning techniques that are also practical in today's
hardware. We believe this work opens several interesting future work
opportunities both in systems and theory that we discuss at the end of this
paper.</div><div><a href='http://arxiv.org/abs/2401.02801v1'>2401.02801v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05786v1")'>Optimistic Safety for Linearly-Constrained Online Convex Optimization</div>
<div id='2403.05786v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T04:01:39Z</div><div>Authors: Spencer Hutchinson, Tianyi Chen, Mahnoosh Alizadeh</div><div style='padding-top: 10px; width: 80ex'>The setting of online convex optimization (OCO) under unknown constraints has
garnered significant attention in recent years. In this work, we consider a
version of this problem with static linear constraints that the player receives
noisy feedback of and must always satisfy. By leveraging our novel design
paradigm of optimistic safety, we give an algorithm for this problem that
enjoys $\tilde{\mathcal{O}}(\sqrt{T})$ regret. This improves on the previous
best regret bound of $\tilde{\mathcal{O}}(T^{2/3})$ while using only slightly
stronger assumptions of independent noise and an oblivious adversary. Then, by
recasting this problem as OCO under time-varying stochastic linear constraints,
we show that our algorithm enjoys the same regret guarantees in such a setting
and never violates the constraints in expectation. This contributes to the
literature on OCO under time-varying stochastic constraints, where the
state-of-the-art algorithms enjoy $\tilde{\mathcal{O}}(\sqrt{T})$ regret and
$\tilde{\mathcal{O}}(\sqrt{T})$ violation when the constraints are convex and
the player receives full feedback. Additionally, we provide a version of our
algorithm that is more computationally efficient and give numerical experiments
comparing it with benchmark algorithms.</div><div><a href='http://arxiv.org/abs/2403.05786v1'>2403.05786v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08621v1")'>A Generalized Approach to Online Convex Optimization</div>
<div id='2402.08621v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T17:42:27Z</div><div>Authors: Mohammad Pedramfar, Vaneet Aggarwal</div><div style='padding-top: 10px; width: 80ex'>In this paper, we analyze the problem of online convex optimization in
different settings. We show that any algorithm for online linear optimization
with fully adaptive adversaries is an algorithm for online convex optimization.
We also show that any such algorithm that requires full-information feedback
may be transformed to an algorithm with semi-bandit feedback with comparable
regret bound. We further show that algorithms that are designed for fully
adaptive adversaries using deterministic semi-bandit feedback can obtain
similar bounds using only stochastic semi-bandit feedback when facing oblivious
adversaries. We use this to describe general meta-algorithms to convert first
order algorithms to zeroth order algorithms with comparable regret bounds. Our
framework allows us to analyze online optimization in various settings, such
full-information feedback, bandit feedback, stochastic regret, adversarial
regret and various forms of non-stationary regret. Using our analysis, we
provide the first efficient projection-free online convex optimization
algorithm using linear optimization oracles.</div><div><a href='http://arxiv.org/abs/2402.08621v1'>2402.08621v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12868v1")'>Fast Rates in Online Convex Optimization by Exploiting the Curvature of
  Feasible Sets</div>
<div id='2402.12868v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T09:59:33Z</div><div>Authors: Taira Tsuchiya, Shinji Ito</div><div style='padding-top: 10px; width: 80ex'>In this paper, we explore online convex optimization (OCO) and introduce a
new analysis that provides fast rates by exploiting the curvature of feasible
sets. In online linear optimization, it is known that if the average gradient
of loss functions is larger than a certain value, the curvature of feasible
sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a
logarithmic regret. This paper reveals that algorithms adaptive to the
curvature of loss functions can also leverage the curvature of feasible sets.
We first prove that if an optimal decision is on the boundary of a feasible set
and the gradient of an underlying loss function is non-zero, then the algorithm
achieves a regret upper bound of $O(\rho \log T)$ in stochastic environments.
Here, $\rho &gt; 0$ is the radius of the smallest sphere that includes the optimal
decision and encloses the feasible set. Our approach, unlike existing ones, can
work directly with convex loss functions, exploiting the curvature of loss
functions simultaneously, and can achieve the logarithmic regret only with a
local property of feasible sets. Additionally, it achieves an $O(\sqrt{T})$
regret even in adversarial environments where FTL suffers an $\Omega(T)$
regret, and attains an $O(\rho \log T + \sqrt{C \rho \log T})$ regret bound in
corrupted stochastic environments with corruption level $C$. Furthermore, by
extending our analysis, we establish a regret upper bound of
$O\Big(T^{\frac{q-2}{2(q-1)}} (\log T)^{\frac{q}{2(q-1)}}\Big)$ for
$q$-uniformly convex feasible sets, where uniformly convex sets include
strongly convex sets and $\ell_p$-balls for $p \in [1,\infty)$. This bound
bridges the gap between the $O(\log T)$ regret bound for strongly convex sets
($q=2$) and the $O(\sqrt{T})$ regret bound for non-curved sets ($q\to\infty$).</div><div><a href='http://arxiv.org/abs/2402.12868v1'>2402.12868v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09173v1")'>Nearly Optimal Regret for Decentralized Online Convex Optimization</div>
<div id='2402.09173v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:44:16Z</div><div>Authors: Yuanyu Wan, Tong Wei, Mingli Song, Lijun Zhang</div><div style='padding-top: 10px; width: 80ex'>We investigate decentralized online convex optimization (D-OCO), in which a
set of local learners are required to minimize a sequence of global loss
functions using only local computations and communications. Previous studies
have established $O(n^{5/4}\rho^{-1/2}\sqrt{T})$ and ${O}(n^{3/2}\rho^{-1}\log
T)$ regret bounds for convex and strongly convex functions respectively, where
$n$ is the number of local learners, $\rho&lt;1$ is the spectral gap of the
communication matrix, and $T$ is the time horizon. However, there exist large
gaps from the existing lower bounds, i.e., $\Omega(n\sqrt{T})$ for convex
functions and $\Omega(n)$ for strongly convex functions. To fill these gaps, in
this paper, we first develop novel D-OCO algorithms that can respectively
reduce the regret bounds for convex and strongly convex functions to
$\tilde{O}(n\rho^{-1/4}\sqrt{T})$ and $\tilde{O}(n\rho^{-1/2}\log T)$. The
primary technique is to design an online accelerated gossip strategy that
enjoys a faster average consensus among local learners. Furthermore, by
carefully exploiting the spectral properties of a specific network topology, we
enhance the lower bounds for convex and strongly convex functions to
$\Omega(n\rho^{-1/4}\sqrt{T})$ and $\Omega(n\rho^{-1/2})$, respectively. These
lower bounds suggest that our algorithms are nearly optimal in terms of $T$,
$n$, and $\rho$.</div><div><a href='http://arxiv.org/abs/2402.09173v1'>2402.09173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02011v1")'>Decentralized Multi-Task Online Convex Optimization Under Random Link
  Failures</div>
<div id='2401.02011v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T00:57:33Z</div><div>Authors: Wenjing Yan, Xuanyu Cao</div><div style='padding-top: 10px; width: 80ex'>Decentralized optimization methods often entail information exchange between
neighbors. Transmission failures can happen due to network congestion,
hardware/software issues, communication outage, and other factors. In this
paper, we investigate the random link failure problem in decentralized
multi-task online convex optimization, where agents have individual decisions
that are coupled with each other via pairwise constraints. Although widely used
in constrained optimization, conventional saddle-point algorithms are not
directly applicable here because of random packet dropping. To address this
issue, we develop a robust decentralized saddle-point algorithm against random
link failures with heterogeneous probabilities by replacing the missing
decisions of neighbors with their latest received values. Then, by judiciously
bounding the accumulated deviation stemming from this replacement, we first
establish that our algorithm achieves $\mathcal{O}(\sqrt{T})$ regret and
$\mathcal{O}(T^\frac{3}{4})$ constraint violations for the full information
scenario, where the complete information on the local cost function is revealed
to each agent at the end of each time slot. These two bounds match, in order
sense, the performance bounds of algorithms with perfect communications.
Further, we extend our algorithm and analysis to the two-point bandit feedback
scenario, where only the values of the local cost function at two random points
are disclosed to each agent sequentially. Performance bounds of the same orders
as the full information case are derived. Finally, we corroborate the efficacy
of the proposed algorithms and the analytical results through numerical
simulations.</div><div><a href='http://arxiv.org/abs/2401.02011v1'>2401.02011v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03055v1")'>Distributed Policy Gradient for Linear Quadratic Networked Control with
  Limited Communication Range</div>
<div id='2403.03055v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T15:38:54Z</div><div>Authors: Yuzi Yan, Yuan Shen</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a scalable distributed policy gradient method and proves
its convergence to near-optimal solution in multi-agent linear quadratic
networked systems. The agents engage within a specified network under local
communication constraints, implying that each agent can only exchange
information with a limited number of neighboring agents. On the underlying
graph of the network, each agent implements its control input depending on its
nearby neighbors' states in the linear quadratic control setting. We show that
it is possible to approximate the exact gradient only using local information.
Compared with the centralized optimal controller, the performance gap decreases
to zero exponentially as the communication and control ranges increase. We also
demonstrate how increasing the communication range enhances system stability in
the gradient descent process, thereby elucidating a critical trade-off. The
simulation results verify our theoretical findings.</div><div><a href='http://arxiv.org/abs/2403.03055v1'>2403.03055v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01258v1")'>Towards Model-Free LQR Control over Rate-Limited Channels</div>
<div id='2401.01258v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T15:59:00Z</div><div>Authors: Aritra Mitra, Lintao Ye, Vijay Gupta</div><div style='padding-top: 10px; width: 80ex'>Given the success of model-free methods for control design in many problem
settings, it is natural to ask how things will change if realistic
communication channels are utilized for the transmission of gradients or
policies. While the resulting problem has analogies with the formulations
studied under the rubric of networked control systems, the rich literature in
that area has typically assumed that the model of the system is known. As a
step towards bridging the fields of model-free control design and networked
control systems, we ask: \textit{Is it possible to solve basic control problems
- such as the linear quadratic regulator (LQR) problem - in a model-free manner
over a rate-limited channel?} Toward answering this question, we study a
setting where a worker agent transmits quantized policy gradients (of the LQR
cost) to a server over a noiseless channel with a finite bit-rate. We propose a
new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and
prove that above a certain finite threshold bit-rate, \texttt{AQGD} guarantees
exponentially fast convergence to the globally optimal policy, with \textit{no
deterioration of the exponent relative to the unquantized setting}. More
generally, our approach reveals the benefits of adaptive quantization in
preserving fast linear convergence rates, and, as such, may be of independent
interest to the literature on compressed optimization.</div><div><a href='http://arxiv.org/abs/2401.01258v1'>2401.01258v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10063v1")'>Unified Projection-Free Algorithms for Adversarial DR-Submodular
  Optimization</div>
<div id='2403.10063v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T07:05:44Z</div><div>Authors: Mohammad Pedramfar, Yididiya Y. Nadew, Christopher J. Quinn, Vaneet Aggarwal</div><div style='padding-top: 10px; width: 80ex'>This paper introduces unified projection-free Frank-Wolfe type algorithms for
adversarial continuous DR-submodular optimization, spanning scenarios such as
full information and (semi-)bandit feedback, monotone and non-monotone
functions, different constraints, and types of stochastic queries. For every
problem considered in the non-monotone setting, the proposed algorithms are
either the first with proven sub-linear $\alpha$-regret bounds or have better
$\alpha$-regret bounds than the state of the art, where $\alpha$ is a
corresponding approximation bound in the offline setting. In the monotone
setting, the proposed approach gives state-of-the-art sub-linear
$\alpha$-regret bounds among projection-free algorithms in 7 of the 8
considered cases while matching the result of the remaining case. Additionally,
this paper addresses semi-bandit and bandit feedback for adversarial
DR-submodular optimization, advancing the understanding of this optimization
area.</div><div><a href='http://arxiv.org/abs/2403.10063v1'>2403.10063v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00138v1")'>Decomposable Submodular Maximization in Federated Setting</div>
<div id='2402.00138v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T19:32:33Z</div><div>Authors: Akbar Rafiey</div><div style='padding-top: 10px; width: 80ex'>Submodular functions, as well as the sub-class of decomposable submodular
functions, and their optimization appear in a wide range of applications in
machine learning, recommendation systems, and welfare maximization. However,
optimization of decomposable submodular functions with millions of component
functions is computationally prohibitive. Furthermore, the component functions
may be private (they might represent user preference function, for example) and
cannot be widely shared. To address these issues, we propose a {\em federated
optimization} setting for decomposable submodular optimization. In this
setting, clients have their own preference functions, and a weighted sum of
these preferences needs to be maximized. We implement the popular {\em
continuous greedy} algorithm in this setting where clients take parallel small
local steps towards the local solution and then the local changes are
aggregated at a central server. To address the large number of clients, the
aggregation is performed only on a subsampled set. Further, the aggregation is
performed only intermittently between stretches of parallel local steps, which
reduces communication cost significantly. We show that our federated algorithm
is guaranteed to provide a good approximate solution, even in the presence of
above cost-cutting measures. Finally, we show how the federated setting can be
incorporated in solving fundamental discrete submodular optimization problems
such as Maximum Coverage and Facility Location.</div><div><a href='http://arxiv.org/abs/2402.00138v1'>2402.00138v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09251v1")'>Bridging the Gap Between General and Down-Closed Convex Sets in
  Submodular Maximization</div>
<div id='2401.09251v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T14:56:42Z</div><div>Authors: Loay Mualem, Murad Tukan, Moran Fledman</div><div style='padding-top: 10px; width: 80ex'>Optimization of DR-submodular functions has experienced a notable surge in
significance in recent times, marking a pivotal development within the domain
of non-convex optimization. Motivated by real-world scenarios, some recent
works have delved into the maximization of non-monotone DR-submodular functions
over general (not necessarily down-closed) convex set constraints. Up to this
point, these works have all used the minimum $\ell_\infty$ norm of any feasible
solution as a parameter. Unfortunately, a recent hardness result due to Mualem
\&amp; Feldman~\cite{mualem2023resolving} shows that this approach cannot yield a
smooth interpolation between down-closed and non-down-closed constraints. In
this work, we suggest novel offline and online algorithms that provably provide
such an interpolation based on a natural decomposition of the convex body
constraint into two distinct convex bodies: a down-closed convex body and a
general convex body. We also empirically demonstrate the superiority of our
proposed algorithms across three offline and two online applications.</div><div><a href='http://arxiv.org/abs/2401.09251v1'>2401.09251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08330v1")'>Boosting Gradient Ascent for Continuous DR-submodular Maximization</div>
<div id='2401.08330v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T12:49:10Z</div><div>Authors: Qixin Zhang, Zongqi Wan, Zengde Deng, Zaiyi Chen, Xiaoming Sun, Jialin Zhang, Yu Yang</div><div style='padding-top: 10px; width: 80ex'>Projected Gradient Ascent (PGA) is the most commonly used optimization scheme
in machine learning and operations research areas. Nevertheless, numerous
studies and examples have shown that the PGA methods may fail to achieve the
tight approximation ratio for continuous DR-submodular maximization problems.
To address this challenge, we present a boosting technique in this paper, which
can efficiently improve the approximation guarantee of the standard PGA to
\emph{optimal} with only small modifications on the objective function. The
fundamental idea of our boosting technique is to exploit non-oblivious search
to derive a novel auxiliary function $F$, whose stationary points are excellent
approximations to the global maximum of the original DR-submodular objective
$f$. Specifically, when $f$ is monotone and $\gamma$-weakly DR-submodular, we
propose an auxiliary function $F$ whose stationary points can provide a better
$(1-e^{-\gamma})$-approximation than the
$(\gamma^2/(1+\gamma^2))$-approximation guaranteed by the stationary points of
$f$ itself. Similarly, for the non-monotone case, we devise another auxiliary
function $F$ whose stationary points can achieve an optimal
$\frac{1-\min_{\boldsymbol{x}\in\mathcal{C}}\|\boldsymbol{x}\|_{\infty}}{4}$-approximation
guarantee where $\mathcal{C}$ is a convex constraint set. In contrast, the
stationary points of the original non-monotone DR-submodular function can be
arbitrarily bad~\citep{chen2023continuous}. Furthermore, we demonstrate the
scalability of our boosting technique on four problems. In all of these four
problems, our resulting variants of boosting PGA algorithm beat the previous
standard PGA in several aspects such as approximation ratio and efficiency.
Finally, we corroborate our theoretical findings with numerical experiments,
which demonstrate the effectiveness of our boosting PGA methods.</div><div><a href='http://arxiv.org/abs/2401.08330v1'>2401.08330v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07108v1")'>Decoupling Learning and Decision-Making: Breaking the
  $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with
  First-Order Methods</div>
<div id='2402.07108v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T05:35:50Z</div><div>Authors: Wenzhi Gao, Chunlin Sun, Chenyu Xue, Dongdong Ge, Yinyu Ye</div><div style='padding-top: 10px; width: 80ex'>Online linear programming plays an important role in both revenue management
and resource allocation, and recent research has focused on developing
efficient first-order online learning algorithms. Despite the empirical success
of first-order methods, they typically achieve a regret no better than
$\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log
T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based
online algorithms. This paper establishes several important facts about online
linear programming, which unveils the challenge for first-order-method-based
online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address
the challenge, we introduce a new algorithmic framework that decouples learning
from decision-making. More importantly, for the first time, we show that
first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new
framework. Lastly, we conduct numerical experiments to validate our theoretical
findings.</div><div><a href='http://arxiv.org/abs/2402.07108v1'>2402.07108v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15045v1")'>DP-Dueling: Learning from Preference Feedback without Compromising User
  Privacy</div>
<div id='2403.15045v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T09:02:12Z</div><div>Authors: Aadirupa Saha, Hilal Asi</div><div style='padding-top: 10px; width: 80ex'>We consider the well-studied dueling bandit problem, where a learner aims to
identify near-optimal actions using pairwise comparisons, under the constraint
of differential privacy. We consider a general class of utility-based
preference matrices for large (potentially unbounded) decision spaces and give
the first differentially private dueling bandit algorithm for active learning
with user preferences. Our proposed algorithms are computationally efficient
with near-optimal performance, both in terms of the private and non-private
regret bound. More precisely, we show that when the decision space is of finite
size $K$, our proposed algorithm yields order optimal $O\Big(\sum_{i =
2}^K\log\frac{KT}{\Delta_i} + \frac{K}{\epsilon}\Big)$ regret bound for pure
$\epsilon$-DP, where $\Delta_i$ denotes the suboptimality gap of the $i$-th
arm. We also present a matching lower bound analysis which proves the
optimality of our algorithms. Finally, we extend our results to any general
decision space in $d$-dimensions with potentially infinite arms and design an
$\epsilon$-DP algorithm with regret $\tilde{O} \left( \frac{d^6}{\kappa
\epsilon } + \frac{ d\sqrt{T }}{\kappa} \right)$, providing privacy for free
when $T \gg d$.</div><div><a href='http://arxiv.org/abs/2403.15045v1'>2403.15045v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02114v1")'>Handling Delayed Feedback in Distributed Online Optimization : A
  Projection-Free Approach</div>
<div id='2402.02114v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T10:43:22Z</div><div>Authors: Tuan-Anh Nguyen, Nguyen Kim Thang, Denis Trystram</div><div style='padding-top: 10px; width: 80ex'>Learning at the edges has become increasingly important as large quantities
of data are continually generated locally. Among others, this paradigm requires
algorithms that are simple (so that they can be executed by local devices),
robust (again uncertainty as data are continually generated), and reliable in a
distributed manner under network issues, especially delays. In this study, we
investigate the problem of online convex optimization under adversarial delayed
feedback. We propose two projection-free algorithms for centralised and
distributed settings in which they are carefully designed to achieve a regret
bound of O(\sqrt{B}) where B is the sum of delay, which is optimal for the OCO
problem in the delay setting while still being projection-free. We provide an
extensive theoretical study and experimentally validate the performance of our
algorithms by comparing them with existing ones on real-world problems.</div><div><a href='http://arxiv.org/abs/2402.02114v1'>2402.02114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08799v1")'>Projection-Free Online Convex Optimization with Time-Varying Constraints</div>
<div id='2402.08799v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T21:13:29Z</div><div>Authors: Dan Garber, Ben Kretzu</div><div style='padding-top: 10px; width: 80ex'>We consider the setting of online convex optimization with adversarial
time-varying constraints in which actions must be feasible w.r.t. a fixed
constraint set, and are also required on average to approximately satisfy
additional time-varying constraints. Motivated by scenarios in which the fixed
feasible set (hard constraint) is difficult to project on, we consider
projection-free algorithms that access this set only through a linear
optimization oracle (LOO). We present an algorithm that, on a sequence of
length $T$ and using overall $T$ calls to the LOO, guarantees
$\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints
violation (ignoring all quantities except for $T$) . In particular, these
bounds hold w.r.t. any interval of the sequence. We also present a more
efficient algorithm that requires only first-order oracle access to the soft
constraints and achieves similar bounds w.r.t. the entire sequence. We extend
the latter to the setting of bandit feedback and obtain similar bounds (as a
function of $T$) in expectation.</div><div><a href='http://arxiv.org/abs/2402.08799v1'>2402.08799v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02552v1")'>Long-term Fairness For Real-time Decision Making: A Constrained Online
  Optimization Approach</div>
<div id='2401.02552v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T21:55:50Z</div><div>Authors: Ruijie Du, Deepan Muthirayan, Pramod P. Khargonekar, Yanning Shen</div><div style='padding-top: 10px; width: 80ex'>Machine learning (ML) has demonstrated remarkable capabilities across many
real-world systems, from predictive modeling to intelligent automation.
However, the widespread integration of machine learning also makes it necessary
to ensure machine learning-driven decision-making systems do not violate
ethical principles and values of society in which they operate. As ML-driven
decisions proliferate, particularly in cases involving sensitive attributes
such as gender, race, and age, to name a few, the need for equity and
impartiality has emerged as a fundamental concern. In situations demanding
real-time decision-making, fairness objectives become more nuanced and complex:
instantaneous fairness to ensure equity in every time slot, and long-term
fairness to ensure fairness over a period of time. There is a growing awareness
that real-world systems that operate over long periods and require fairness
over different timelines. However, existing approaches mainly address dynamic
costs with time-invariant fairness constraints, often disregarding the
challenges posed by time-varying fairness constraints. To bridge this gap, this
work introduces a framework for ensuring long-term fairness within dynamic
decision-making systems characterized by time-varying fairness constraints. We
formulate the decision problem with fairness constraints over a period as a
constrained online optimization problem. A novel online algorithm, named
LoTFair, is presented that solves the problem 'on the fly'. We prove that
LoTFair can make overall fairness violations negligible while maintaining the
performance over the long run.</div><div><a href='http://arxiv.org/abs/2401.02552v1'>2401.02552v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06318v1")'>Striking a Balance in Fairness for Dynamic Systems Through Reinforcement
  Learning</div>
<div id='2401.06318v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T01:29:26Z</div><div>Authors: Yaowei Hu, Jacob Lear, Lu Zhang</div><div style='padding-top: 10px; width: 80ex'>While significant advancements have been made in the field of fair machine
learning, the majority of studies focus on scenarios where the decision model
operates on a static population. In this paper, we study fairness in dynamic
systems where sequential decisions are made. Each decision may shift the
underlying distribution of features or user behavior. We model the dynamic
system through a Markov Decision Process (MDP). By acknowledging that
traditional fairness notions and long-term fairness are distinct requirements
that may not necessarily align with one another, we propose an algorithmic
framework to integrate various fairness considerations with reinforcement
learning using both pre-processing and in-processing approaches. Three case
studies show that our method can strike a balance between traditional fairness
notions, long-term fairness, and utility.</div><div><a href='http://arxiv.org/abs/2401.06318v1'>2401.06318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12319v1")'>Dynamic Environment Responsive Online Meta-Learning with Fairness
  Awareness</div>
<div id='2402.12319v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T17:44:35Z</div><div>Authors: Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen</div><div style='padding-top: 10px; width: 80ex'>The fairness-aware online learning framework has emerged as a potent tool
within the context of continuous lifelong learning. In this scenario, the
learner's objective is to progressively acquire new tasks as they arrive over
time, while also guaranteeing statistical parity among various protected
sub-populations, such as race and gender, when it comes to the newly introduced
tasks. A significant limitation of current approaches lies in their heavy
reliance on the i.i.d (independent and identically distributed) assumption
concerning data, leading to a static regret analysis of the framework.
Nevertheless, it's crucial to note that achieving low static regret does not
necessarily translate to strong performance in dynamic environments
characterized by tasks sampled from diverse distributions. In this paper, to
tackle the fairness-aware online learning challenge in evolving settings, we
introduce a unique regret measure, FairSAR, by incorporating long-term fairness
constraints into a strongly adapted loss regret framework. Moreover, to
determine an optimal model parameter at each time step, we introduce an
innovative adaptive fairness-aware online meta-learning algorithm, referred to
as FairSAOML. This algorithm possesses the ability to adjust to dynamic
environments by effectively managing bias control and model accuracy. The
problem is framed as a bi-level convex-concave optimization, considering both
the model's primal and dual parameters, which pertain to its accuracy and
fairness attributes, respectively. Theoretical analysis yields sub-linear upper
bounds for both loss regret and the cumulative violation of fairness
constraints. Our experimental evaluation on various real-world datasets in
dynamic environments demonstrates that our proposed FairSAOML algorithm
consistently outperforms alternative approaches rooted in the most advanced
prior online learning methods.</div><div><a href='http://arxiv.org/abs/2402.12319v1'>2402.12319v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07051v1")'>COIN: Chance-Constrained Imitation Learning for Uncertainty-aware
  Adaptive Resource Oversubscription Policy</div>
<div id='2401.07051v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T11:43:25Z</div><div>Authors: Lu Wang, Mayukh Das, Fangkai Yang, Chao Duo, Bo Qiao, Hang Dong, Si Qin, Chetan Bansal, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</div><div style='padding-top: 10px; width: 80ex'>We address the challenge of learning safe and robust decision policies in
presence of uncertainty in context of the real scientific problem of adaptive
resource oversubscription to enhance resource efficiency while ensuring safety
against resource congestion risk.
  Traditional supervised prediction or forecasting models are ineffective in
learning adaptive policies whereas standard online optimization or
reinforcement learning is difficult to deploy on real systems. Offline methods
such as imitation learning (IL) are ideal since we can directly leverage
historical resource usage telemetry. But, the underlying aleatoric uncertainty
in such telemetry is a critical bottleneck.
  We solve this with our proposed novel chance-constrained imitation learning
framework, which ensures implicit safety against uncertainty in a principled
manner via a combination of stochastic (chance) constraints on resource
congestion risk and ensemble value functions. This leads to substantial
($\approx 3-4\times$) improvement in resource efficiency and safety in many
oversubscription scenarios, including resource management in cloud services.</div><div><a href='http://arxiv.org/abs/2401.07051v1'>2401.07051v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04033v1")'>Online Learning with Unknown Constraints</div>
<div id='2403.04033v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T20:23:59Z</div><div>Authors: Karthik Sridharan, Seung Won Wilson Yoo</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of online learning where the sequence of actions
played by the learner must adhere to an unknown safety constraint at every
round. The goal is to minimize regret with respect to the best safe action in
hindsight while simultaneously satisfying the safety constraint with high
probability on each round. We provide a general meta-algorithm that leverages
an online regression oracle to estimate the unknown safety constraint, and
converts the predictions of an online learning oracle to predictions that
adhere to the unknown safety constraint. On the theoretical side, our
algorithm's regret can be bounded by the regret of the online regression and
online learning oracles, the eluder dimension of the model class containing the
unknown safety constraint, and a novel complexity measure that captures the
difficulty of safe learning. We complement our result with an asymptotic lower
bound that shows that the aforementioned complexity measure is necessary. When
the constraints are linear, we instantiate our result to provide a concrete
algorithm with $\sqrt{T}$ regret using a scaling transformation that balances
optimistic exploration with pessimistic constraint satisfaction.</div><div><a href='http://arxiv.org/abs/2403.04033v1'>2403.04033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11425v2")'>Online Local False Discovery Rate Control: A Resource Allocation
  Approach</div>
<div id='2402.11425v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T02:11:54Z</div><div>Authors: Ruicheng Ao, Hongyu Chen, David Simchi-Levi, Feng Zhu</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of online local false discovery rate (FDR) control
where multiple tests are conducted sequentially, with the goal of maximizing
the total expected number of discoveries. We formulate the problem as an online
resource allocation problem with accept/reject decisions, which from a high
level can be viewed as an online knapsack problem, with the additional
uncertainty of exogenous random budget replenishment. We start with general
arrival distributions and propose a simple policy that achieves a $O(\sqrt{T})$
regret. We complement the result by showing that such regret rate is in general
not improvable. We then shift our focus to discrete arrival distributions. We
find that many existing re-solving heuristics in the online resource allocation
literature, albeit achieve bounded loss in canonical settings, may incur a
$\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that
canonical policies tend to be too optimistic and over accept arrivals, we
propose a novel policy that incorporates budget buffers. We show that small
additional logarithmic buffers suffice to reduce the regret from
$\Omega(\sqrt{T})$ or even $\Omega(T)$ to $O(\ln^2 T)$. Numerical experiments
are conducted to validate our theoretical findings. Our formulation may have
wider applications beyond the problem considered in this paper, and our results
emphasize how effective policies should be designed to reach a balance between
circumventing wrong accept and reducing wrong reject in online resource
allocation problems with exogenous budget replenishment.</div><div><a href='http://arxiv.org/abs/2402.11425v2'>2402.11425v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12711v1")'>Achieving Near-Optimal Regret for Bandit Algorithms with Uniform
  Last-Iterate Guarantee</div>
<div id='2402.12711v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:21:13Z</div><div>Authors: Junyan Liu, Yunfan Li, Lin Yang</div><div style='padding-top: 10px; width: 80ex'>Existing performance measures for bandit algorithms such as regret, PAC
bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative
performance, while allowing the play of an arbitrarily bad arm at any finite
time t. Such a behavior can be highly detrimental in high-stakes applications.
This paper introduces a stronger performance measure, the uniform last-iterate
(ULI) guarantee, capturing both cumulative and instantaneous performance of
bandit algorithms. Specifically, ULI characterizes the instantaneous
performance since it ensures that the per-round regret of the played arm is
bounded by a function, monotonically decreasing w.r.t. (large) round t,
preventing revisits to bad arms when sufficient samples are available. We
demonstrate that a near-optimal ULI guarantee directly implies near-optimal
cumulative performance across aforementioned performance measures. To examine
the achievability of ULI in the finite arm setting, we first provide two
positive results that some elimination-based algorithms and high-probability
adversarial algorithms with stronger analysis or additional designs, can attain
near-optimal ULI guarantees. Then, we also provide a negative result,
indicating that optimistic algorithms cannot achieve a near-optimal ULI
guarantee. Finally, we propose an efficient algorithm for linear bandits with
infinitely many arms, which achieves the ULI guarantee, given access to an
optimization oracle.</div><div><a href='http://arxiv.org/abs/2402.12711v1'>2402.12711v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09073v1")'>Fixed-Budget Differentially Private Best Arm Identification</div>
<div id='2401.09073v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T09:23:25Z</div><div>Authors: Zhirui Chen, P. N. Karthik, Yeow Meng Chee, Vincent Y. F. Tan</div><div style='padding-top: 10px; width: 80ex'>We study best arm identification (BAI) in linear bandits in the fixed-budget
regime under differential privacy constraints, when the arm rewards are
supported on the unit interval. Given a finite budget $T$ and a privacy
parameter $\varepsilon&gt;0$, the goal is to minimise the error probability in
finding the arm with the largest mean after $T$ sampling rounds, subject to the
constraint that the policy of the decision maker satisfies a certain {\em
$\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct
a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}) by
proposing the principle of {\em maximum absolute determinants}, and derive an
upper bound on its error probability. Furthermore, we derive a minimax lower
bound on the error probability, and demonstrate that the lower and the upper
bounds decay exponentially in $T$, with exponents in the two bounds matching
order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and
(c) the problem complexity that is expressible as the sum of two terms, one
characterising the complexity of standard fixed-budget BAI (without privacy
constraints), and the other accounting for the $\varepsilon$-DP constraint.
Additionally, we present some auxiliary results that contribute to the
derivation of the lower bound on the error probability. These results, we
posit, may be of independent interest and could prove instrumental in proving
lower bounds on error probabilities in several other bandit problems. Whereas
prior works provide results for BAI in the fixed-budget regime without privacy
constraints or in the fixed-confidence regime with privacy constraints, our
work fills the gap in the literature by providing the results for BAI in the
fixed-budget regime under the $\varepsilon$-DP constraint.</div><div><a href='http://arxiv.org/abs/2401.09073v1'>2401.09073v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05134v1")'>Follow-the-Perturbed-Leader with Fréchet-type Tail Distributions:
  Optimality in Adversarial Bandits and Best-of-Both-Worlds</div>
<div id='2403.05134v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T08:07:26Z</div><div>Authors: Jongyeong Lee, Junya Honda, Shinji Ito, Min-hwan Oh</div><div style='padding-top: 10px; width: 80ex'>This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL)
policy in both adversarial and stochastic $K$-armed bandits. Despite the
widespread use of the Follow-the-Regularized-Leader (FTRL) framework with
various choices of regularization, the FTPL framework, which relies on random
perturbations, has not received much attention, despite its inherent
simplicity. In adversarial bandits, there has been conjecture that FTPL could
potentially achieve $\mathcal{O}(\sqrt{KT})$ regrets if perturbations follow a
distribution with a Fr\'{e}chet-type tail. Recent work by Honda et al. (2023)
showed that FTPL with Fr\'{e}chet distribution with shape $\alpha=2$ indeed
attains this bound and, notably logarithmic regret in stochastic bandits,
meaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result
only partly resolves the above conjecture because their analysis heavily relies
on the specific form of the Fr\'{e}chet distribution with this shape. In this
paper, we establish a sufficient condition for perturbations to achieve
$\mathcal{O}(\sqrt{KT})$ regrets in the adversarial setting, which covers,
e.g., Fr\'{e}chet, Pareto, and Student-$t$ distributions. We also demonstrate
the BOBW achievability of FTPL with certain Fr\'{e}chet-type tail
distributions. Our results contribute not only to resolving existing
conjectures through the lens of extreme value theory but also potentially offer
insights into the effect of the regularization functions in FTRL through the
mapping from FTPL to FTRL.</div><div><a href='http://arxiv.org/abs/2403.05134v1'>2403.05134v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10732v1")'>Variance-Dependent Regret Bounds for Non-stationary Linear Bandits</div>
<div id='2403.10732v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T23:36:55Z</div><div>Authors: Zhiyong Wang, Jize Xie, Yi Chen, John C. S. Lui, Dongruo Zhou</div><div style='padding-top: 10px; width: 80ex'>We investigate the non-stationary stochastic linear bandit problem where the
reward distribution evolves each round. Existing algorithms characterize the
non-stationarity by the total variation budget $B_K$, which is the summation of
the change of the consecutive feature vectors of the linear bandits over $K$
rounds. However, such a quantity only measures the non-stationarity with
respect to the expectation of the reward distribution, which makes existing
algorithms sub-optimal under the general non-stationary distribution setting.
In this work, we propose algorithms that utilize the variance of the reward
distribution as well as the $B_K$, and show that they can achieve tighter
regret upper bounds. Specifically, we introduce two novel algorithms: Restarted
Weighted$\text{OFUL}^+$ and Restarted $\text{SAVE}^+$. These algorithms address
cases where the variance information of the rewards is known and unknown,
respectively. Notably, when the total variance $V_K$ is much smaller than $K$,
our algorithms outperform previous state-of-the-art results on non-stationary
stochastic linear bandits under different settings. Experimental evaluations
further validate the superior performance of our proposed algorithms over
existing works.</div><div><a href='http://arxiv.org/abs/2403.10732v1'>2403.10732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15127v1")'>Multi-Armed Bandits with Abstention</div>
<div id='2402.15127v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:27:12Z</div><div>Authors: Junwen Yang, Tianyuan Jin, Vincent Y. F. Tan</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel extension of the canonical multi-armed bandit problem
that incorporates an additional strategic element: abstention. In this enhanced
framework, the agent is not only tasked with selecting an arm at each time
step, but also has the option to abstain from accepting the stochastic
instantaneous reward before observing it. When opting for abstention, the agent
either suffers a fixed regret or gains a guaranteed reward. Given this added
layer of complexity, we ask whether we can develop efficient algorithms that
are both asymptotically and minimax optimal. We answer this question
affirmatively by designing and analyzing algorithms whose regrets meet their
corresponding information-theoretic lower bounds. Our results offer valuable
quantitative insights into the benefits of the abstention option, laying the
groundwork for further exploration in other online decision-making problems
with such an option. Numerical results further corroborate our theoretical
findings.</div><div><a href='http://arxiv.org/abs/2402.15127v1'>2402.15127v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10819v1")'>Incentivized Exploration of Non-Stationary Stochastic Bandits</div>
<div id='2403.10819v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T06:06:44Z</div><div>Authors: Sourav Chakraborty, Lijun Chen</div><div style='padding-top: 10px; width: 80ex'>We study incentivized exploration for the multi-armed bandit (MAB) problem
with non-stationary reward distributions, where players receive compensation
for exploring arms other than the greedy choice and may provide biased feedback
on the reward. We consider two different non-stationary environments:
abruptly-changing and continuously-changing, and propose respective
incentivized exploration algorithms. We show that the proposed algorithms
achieve sublinear regret and compensation over time, thus effectively
incentivizing exploration despite the nonstationarity and the biased or drifted
feedback.</div><div><a href='http://arxiv.org/abs/2403.10819v1'>2403.10819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12950v1")'>Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized
  Borda Criterion</div>
<div id='2403.12950v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:50:55Z</div><div>Authors: Joe Suk, Arpit Agarwal</div><div style='padding-top: 10px; width: 80ex'>In dueling bandits, the learner receives preference feedback between arms,
and the regret of an arm is defined in terms of its suboptimality to a winner
arm. The more challenging and practically motivated non-stationary variant of
dueling bandits, where preferences change over time, has been the focus of
several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and
Agarwal, 2023). The goal is to design algorithms without foreknowledge of the
amount of change.
  The bulk of known results here studies the Condorcet winner setting, where an
arm preferred over any other exists at all times. Yet, such a winner may not
exist and, to contrast, the Borda version of this problem (which is always
well-defined) has received little attention. In this work, we establish the
first optimal and adaptive Borda dynamic regret upper bound, which highlights
fundamental differences in the learnability of severe non-stationarity between
Condorcet vs. Borda regret objectives in dueling bandits.
  Surprisingly, our techniques for non-stationary Borda dueling bandits also
yield improved rates within the Condorcet winner setting, and reveal new
preference models where tighter notions of non-stationarity are adaptively
learnable. This is accomplished through a novel generalized Borda score
framework which unites the Borda and Condorcet problems, thus allowing
reduction of Condorcet regret to a Borda-like task. Such a generalization was
not previously known and is likely to be of independent interest.</div><div><a href='http://arxiv.org/abs/2403.12950v1'>2403.12950v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00715v2")'>Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive
  Analysis and Best-of-Both-Worlds</div>
<div id='2403.00715v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T18:03:49Z</div><div>Authors: Shinji Ito, Taira Tsuchiya, Junya Honda</div><div style='padding-top: 10px; width: 80ex'>Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile
approach in online learning, where appropriate choice of the learning rate is
crucial for smaller regret. To this end, we formulate the problem of adjusting
FTRL's learning rate as a sequential decision-making problem and introduce the
framework of competitive analysis. We establish a lower bound for the
competitive ratio and propose update rules for learning rate that achieves an
upper bound within a constant factor of this lower bound. Specifically, we
illustrate that the optimal competitive ratio is characterized by the
(approximate) monotonicity of components of the penalty term, showing that a
constant competitive ratio is achievable if the components of the penalty term
form a monotonically non-increasing sequence, and derive a tight competitive
ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our
proposed update rule, referred to as \textit{stability-penalty matching}, also
facilitates constructing the Best-Of-Both-Worlds (BOBW) algorithms for
stochastic and adversarial environments. In these environments our result
contributes to achieve tighter regret bound and broaden the applicability of
algorithms for various settings such as multi-armed bandits, graph bandits,
linear bandits, and contextual bandits.</div><div><a href='http://arxiv.org/abs/2403.00715v2'>2403.00715v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17720v1")'>The SMART approach to instance-optimal online learning</div>
<div id='2402.17720v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:55:33Z</div><div>Authors: Siddhartha Banerjee, Alankrita Bhatt, Christina Lee Yu</div><div style='padding-top: 10px; width: 80ex'>We devise an online learning algorithm -- titled Switching via Monotone
Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret
that is instance optimal, i.e., simultaneously competitive on every input
sequence compared to the performance of the follow-the-leader (FTL) policy and
the worst case guarantee of any other input policy. We show that the regret of
the SMART policy on any input sequence is within a multiplicative factor
$e/(e-1) \approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the
sequence, and 2) the upper bound on regret guaranteed by the given worst-case
policy. This implies a strictly stronger guarantee than typical
`best-of-both-worlds' bounds as the guarantee holds for every input sequence
regardless of how it is generated. SMART is simple to implement as it begins by
playing FTL and switches at most once during the time horizon to the worst-case
algorithm. Our approach and results follow from an operational reduction of
instance optimal online learning to competitive analysis for the ski-rental
problem. We complement our competitive ratio upper bounds with a fundamental
lower bound showing that over all input sequences, no algorithm can get better
than a $1.43$-fraction of the minimum regret achieved by FTL and the
minimax-optimal policy. We also present a modification of SMART that combines
FTL with a ``small-loss" algorithm to achieve instance optimality between the
regret of FTL and the small loss regret bound.</div><div><a href='http://arxiv.org/abs/2402.17720v1'>2402.17720v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01528v1")'>Improved Bandits in Many-to-one Matching Markets with Incentive
  Compatibility</div>
<div id='2401.01528v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T03:45:35Z</div><div>Authors: Fang Kong, Shuai Li</div><div style='padding-top: 10px; width: 80ex'>Two-sided matching markets have been widely studied in the literature due to
their rich applications. Since participants are usually uncertain about their
preferences, online algorithms have recently been adopted to learn them through
iterative interactions. \citet{wang2022bandit} initiate the study of this
problem in a many-to-one setting with \textit{responsiveness}. However, their
results are far from optimal and lack guarantees of incentive compatibility. An
extension of \citet{kong2023player} to this more general setting achieves a
near-optimal bound for player-optimal regret. Nevertheless, due to the
substantial requirement for collaboration, a single player's deviation could
lead to a huge increase in its own cumulative rewards and an $O(T)$ regret for
others. In this paper, we aim to enhance the regret bound in many-to-one
markets while ensuring incentive compatibility. We first propose the adaptively
explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting
and derive an $O(N\min\left\{N,K\right\}C\log T/\Delta^2)$ upper bound for
player-optimal stable regret while demonstrating its guarantee of incentive
compatibility, where $N$ represents the number of players, $K$ is the number of
arms, $T$ denotes the time horizon, $C$ is arms' total capacities and $\Delta$
signifies the minimum preference gap among players. This result is a
significant improvement over \citet{wang2022bandit}. And to the best of our
knowledge, it constitutes the first player-optimal guarantee in matching
markets that offers such robust assurances. We also consider broader
\textit{substitutable} preferences, one of the most general conditions to
ensure the existence of a stable matching and cover responsiveness. We devise
an online DA (ODA) algorithm and establish an $O(NK\log T/\Delta^2)$
player-pessimal stable regret bound for this setting.</div><div><a href='http://arxiv.org/abs/2401.01528v1'>2401.01528v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01315v1")'>Near-optimal Per-Action Regret Bounds for Sleeping Bandits</div>
<div id='2403.01315v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T21:22:46Z</div><div>Authors: Quan Nguyen, Nishant A. Mehta</div><div style='padding-top: 10px; width: 80ex'>We derive near-optimal per-action regret bounds for sleeping bandits, in
which both the sets of available arms and their losses in every round are
chosen by an adversary. In a setting with $K$ total arms and at most $A$
available arms in each round over $T$ rounds, the best known upper bound is
$O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping
regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper
bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap
by directly minimizing the per-action regret using generalized versions of
EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal
bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our
results to the setting of bandits with advice from sleeping experts,
generalizing EXP4 along the way. This leads to new proofs for a number of
existing adaptive and tracking regret bounds for standard non-sleeping bandits.
Extending our results to the bandit version of experts that report their
confidences leads to new bounds for the confidence regret that depends
primarily on the sum of experts' confidences. We prove a lower bound, showing
that for any minimax optimal algorithms, there exists an action whose regret is
sublinear in $T$ but linear in the number of its active rounds.</div><div><a href='http://arxiv.org/abs/2403.01315v1'>2403.01315v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13487v1")'>Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits</div>
<div id='2402.13487v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T02:54:00Z</div><div>Authors: Zhiwei Wang, Huazheng Wang, Hongning Wang</div><div style='padding-top: 10px; width: 80ex'>Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms
have been extensively studied in the literature. In this work, we focus on
reward poisoning attacks and find most existing attacks can be easily detected
by our proposed detection method based on the test of homogeneity, due to their
aggressive nature in reward manipulations. This motivates us to study the
notion of stealthy attack against stochastic MABs and investigate the resulting
attackability. Our analysis shows that against two popularly employed MAB
algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack
depends on the environmental conditions and the realized reward of the arm
pulled in the first round. We also analyze the situation for general MAB
algorithms equipped with our attack detection method and find that it is
possible to have a stealthy attack that almost always succeeds. This brings new
insights into the security risks of MAB algorithms.</div><div><a href='http://arxiv.org/abs/2402.13487v1'>2402.13487v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12428v1")'>Transfer in Sequential Multi-armed Bandits via Reward Samples</div>
<div id='2403.12428v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T04:35:59Z</div><div>Authors: Rahul N R, Vaibhav Katewa</div><div style='padding-top: 10px; width: 80ex'>We consider a sequential stochastic multi-armed bandit problem where the
agent interacts with bandit over multiple episodes. The reward distribution of
the arms remain constant throughout an episode but can change over different
episodes. We propose an algorithm based on UCB to transfer the reward samples
from the previous episodes and improve the cumulative regret performance over
all the episodes. We provide regret analysis and empirical results for our
algorithm, which show significant improvement over the standard UCB algorithm
without transfer.</div><div><a href='http://arxiv.org/abs/2403.12428v1'>2403.12428v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11520v1")'>State-Separated SARSA: A Practical Sequential Decision-Making Algorithm
  with Recovering Rewards</div>
<div id='2403.11520v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T07:14:21Z</div><div>Authors: Yuto Tanimoto, Kenji Fukumizu</div><div style='padding-top: 10px; width: 80ex'>While many multi-armed bandit algorithms assume that rewards for all arms are
constant across rounds, this assumption does not hold in many real-world
scenarios. This paper considers the setting of recovering bandits (Pike-Burke &amp;
Grunewalder, 2019), where the reward depends on the number of rounds elapsed
since the last time an arm was pulled. We propose a new reinforcement learning
(RL) algorithm tailored to this setting, named the State-Separate SARSA
(SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm
achieves efficient learning by reducing the number of state combinations
required for Q-learning/SARSA, which often suffers from combinatorial issues
for large-scale RL problems. Additionally, it makes minimal assumptions about
the reward structure and offers lower computational complexity. Furthermore, we
prove asymptotic convergence to an optimal policy under mild assumptions.
Simulation studies demonstrate the superior performance of our algorithm across
various settings.</div><div><a href='http://arxiv.org/abs/2403.11520v1'>2403.11520v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03110v1")'>Non-Stationary Latent Auto-Regressive Bandits</div>
<div id='2402.03110v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:38:01Z</div><div>Authors: Anna L. Trella, Walter Dempsey, Finale Doshi-Velez, Susan A. Murphy</div><div style='padding-top: 10px; width: 80ex'>We consider the stochastic multi-armed bandit problem with non-stationary
rewards. We present a novel formulation of non-stationarity in the environment
where changes in the mean reward of the arms over time are due to some unknown,
latent, auto-regressive (AR) state of order $k$. We call this new environment
the latent AR bandit. Different forms of the latent AR bandit appear in many
real-world settings, especially in emerging scientific fields such as
behavioral health or education where there are few mechanistic models of the
environment. If the AR order $k$ is known, we propose an algorithm that
achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our
algorithm outperforms standard UCB across multiple non-stationary environments,
even if $k$ is mis-specified.</div><div><a href='http://arxiv.org/abs/2402.03110v1'>2402.03110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09467v1")'>Optimal Thresholding Linear Bandit</div>
<div id='2402.09467v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T21:25:14Z</div><div>Authors: Eduardo Ochoa Rivera, Ambuj Tewari</div><div style='padding-top: 10px; width: 80ex'>We study a novel pure exploration problem: the $\epsilon$-Thresholding Bandit
Problem (TBP) with fixed confidence in stochastic linear bandits. We prove a
lower bound for the sample complexity and extend an algorithm designed for Best
Arm Identification in the linear case to TBP that is asymptotically optimal.</div><div><a href='http://arxiv.org/abs/2402.09467v1'>2402.09467v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09123v1")'>Optimal Top-Two Method for Best Arm Identification and Fluid Analysis</div>
<div id='2403.09123v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T06:14:07Z</div><div>Authors: Agniv Bandyopadhyay, Sandeep Juneja, Shubhada Agrawal</div><div style='padding-top: 10px; width: 80ex'>Top-$2$ methods have become popular in solving the best arm identification
(BAI) problem. The best arm, or the arm with the largest mean amongst finitely
many, is identified through an algorithm that at any sequential step
independently pulls the empirical best arm, with a fixed probability $\beta$,
and pulls the best challenger arm otherwise. The probability of incorrect
selection is guaranteed to lie below a specified $\delta &gt;0$. Information
theoretic lower bounds on sample complexity are well known for BAI problem and
are matched asymptotically as $\delta \rightarrow 0$ by computationally
demanding plug-in methods. The above top 2 algorithm for any $\beta \in (0,1)$
has sample complexity within a constant of the lower bound. However,
determining the optimal $\beta$ that matches the lower bound has proven
difficult. In this paper, we address this and propose an optimal top-2 type
algorithm. We consider a function of allocations anchored at a threshold. If it
exceeds the threshold then the algorithm samples the empirical best arm.
Otherwise, it samples the challenger arm. We show that the proposed algorithm
is optimal as $\delta \rightarrow 0$. Our analysis relies on identifying a
limiting fluid dynamics of allocations that satisfy a series of ordinary
differential equations pasted together and that describe the asymptotic path
followed by our algorithm. We rely on the implicit function theorem to show
existence and uniqueness of these fluid ode's and to show that the proposed
algorithm remains close to the ode solution.</div><div><a href='http://arxiv.org/abs/2403.09123v1'>2403.09123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16710v1")'>Cost Aware Best Arm Identification</div>
<div id='2402.16710v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T16:27:08Z</div><div>Authors: Kellen Kanarios, Qining Zhang, Lei Ying</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study a best arm identification problem with dual objects.
In addition to the classic reward, each arm is associated with a cost
distribution and the goal is to identify the largest reward arm using the
minimum expected cost. We call it \emph{Cost Aware Best Arm Identification}
(CABAI), which captures the separation of testing and implementation phases in
product development pipelines and models the objective shift between phases,
i.e., cost for testing and reward for implementation. We first derive an
theoretic lower bound for CABAI and propose an algorithm called $\mathsf{CTAS}$
to match it asymptotically. To reduce the computation of $\mathsf{CTAS}$, we
further propose a low-complexity algorithm called CO, based on a square-root
rule, which proves optimal in simplified two-armed models and generalizes
surprisingly well in numerical experiments. Our results show (i) ignoring the
heterogeneous action cost results in sub-optimality in practice, and (ii)
low-complexity algorithms deliver near-optimal performance over a wide range of
problems.</div><div><a href='http://arxiv.org/abs/2402.16710v1'>2402.16710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15879v3")'>lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold
  Gap</div>
<div id='2401.15879v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T04:21:47Z</div><div>Authors: Tzu-Hsien Tsai, Yun-Da Tsai, Shou-De Lin</div><div style='padding-top: 10px; width: 80ex'>Good arm identification (GAI) is a pure-exploration bandit problem in which a
single learner outputs an arm as soon as it is identified as a good arm. A good
arm is defined as an arm with an expected reward greater than or equal to a
given threshold. This paper focuses on the GAI problem under a small threshold
gap, which refers to the distance between the expected rewards of arms and the
given threshold. We propose a new algorithm called lil'HDoC to significantly
improve the total sample complexity of the HDoC algorithm. We demonstrate that
the sample complexity of the first $\lambda$ output arm in lil'HDoC is bounded
by the original HDoC algorithm, except for one negligible term, when the
distance between the expected reward and threshold is small. Extensive
experiments confirm that our algorithm outperforms the state-of-the-art
algorithms in both synthetic and real-world datasets.</div><div><a href='http://arxiv.org/abs/2401.15879v3'>2401.15879v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19090v1")'>Best Arm Identification with Resource Constraints</div>
<div id='2402.19090v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T12:17:54Z</div><div>Authors: Zitian Li, Wang Chi Cheung</div><div style='padding-top: 10px; width: 80ex'>Motivated by the cost heterogeneity in experimentation across different
alternatives, we study the Best Arm Identification with Resource Constraints
(BAIwRC) problem. The agent aims to identify the best arm under resource
constraints, where resources are consumed for each arm pull. We make two novel
contributions. We design and analyze the Successive Halving with Resource
Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic
rate of convergence in terms of the probability of successively identifying an
optimal arm. Interestingly, we identify a difference in convergence rates
between the cases of deterministic and stochastic resource consumption.</div><div><a href='http://arxiv.org/abs/2402.19090v1'>2402.19090v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01845v1")'>Multi-Armed Bandits with Interference</div>
<div id='2402.01845v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:02:47Z</div><div>Authors: Su Jia, Peter Frazier, Nathan Kallus</div><div style='padding-top: 10px; width: 80ex'>Experimentation with interference poses a significant challenge in
contemporary online platforms. Prior research on experimentation with
interference has concentrated on the final output of a policy. The cumulative
performance, while equally crucial, is less well understood. To address this
gap, we introduce the problem of {\em Multi-armed Bandits with Interference}
(MABI), where the learner assigns an arm to each of $N$ experimental units over
a time horizon of $T$ rounds. The reward of each unit in each round depends on
the treatments of {\em all} units, where the influence of a unit decays in the
spatial distance between units. Furthermore, we employ a general setup wherein
the reward functions are chosen by an adversary and may vary arbitrarily across
rounds and units. We first show that switchback policies achieve an optimal
{\em expected} regret $\tilde O(\sqrt T)$ against the best fixed-arm policy.
Nonetheless, the regret (as a random variable) for any switchback policy
suffers a high variance, as it does not account for $N$. We propose a cluster
randomization policy whose regret (i) is optimal in {\em expectation} and (ii)
admits a high probability bound that vanishes in $N$.</div><div><a href='http://arxiv.org/abs/2402.01845v1'>2402.01845v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09043v1")'>Under manipulations, are some AI models harder to audit?</div>
<div id='2402.09043v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T09:38:09Z</div><div>Authors: Augustin Godinot, Gilles Tredan, Erwan Le Merrer, Camilla Penzo, Francois Taïani</div><div style='padding-top: 10px; width: 80ex'>Auditors need robust methods to assess the compliance of web platforms with
the law. However, since they hardly ever have access to the algorithm,
implementation, or training data used by a platform, the problem is harder than
a simple metric estimation. Within the recent framework of manipulation-proof
auditing, we study in this paper the feasibility of robust audits in realistic
settings, in which models exhibit large capacities. We first prove a
constraining result: if a web platform uses models that may fit any data, no
audit strategy -- whether active or not -- can outperform random sampling when
estimating properties such as demographic parity. To better understand the
conditions under which state-of-the-art auditing techniques may remain
competitive, we then relate the manipulability of audits to the capacity of the
targeted models, using the Rademacher complexity. We empirically validate these
results on popular models of increasing capacities, thus confirming
experimentally that large-capacity models, which are commonly used in practice,
are particularly hard to audit robustly. These results refine the limits of the
auditing problem, and open up enticing questions on the connection between
model capacity and the ability of platforms to manipulate audit attempts.</div><div><a href='http://arxiv.org/abs/2402.09043v1'>2402.09043v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07918v1")'>On the Societal Impact of Open Foundation Models</div>
<div id='2403.07918v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T16:49:53Z</div><div>Authors: Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, Rumman Chowdhury, Alex Engler, Peter Henderson, Yacine Jernite, Seth Lazar, Stefano Maffulli, Alondra Nelson, Joelle Pineau, Aviya Skowron, Dawn Song, Victor Storchan, Daniel Zhang, Daniel E. Ho, Percy Liang, Arvind Narayanan</div><div style='padding-top: 10px; width: 80ex'>Foundation models are powerful technologies: how they are released publicly
directly shapes their societal impact. In this position paper, we focus on open
foundation models, defined here as those with broadly available model weights
(e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties
(e.g. greater customizability, poor monitoring) of open foundation models that
lead to both their benefits and risks. Open foundation models present
significant benefits, with some caveats, that span innovation, competition, the
distribution of decision-making power, and transparency. To understand their
risks of misuse, we design a risk assessment framework for analyzing their
marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons),
we find that current research is insufficient to effectively characterize the
marginal risk of open foundation models relative to pre-existing technologies.
The framework helps explain why the marginal risk is low in some cases,
clarifies disagreements about misuse risks by revealing that past work has
focused on different subsets of the framework with different assumptions, and
articulates a way forward for more constructive debate. Overall, our work helps
support a more grounded assessment of the societal impact of open foundation
models by outlining what research is needed to empirically validate their
theoretical benefits and risks.</div><div><a href='http://arxiv.org/abs/2403.07918v1'>2403.07918v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16268v1")'>Foundation Model Transparency Reports</div>
<div id='2402.16268v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T03:09:06Z</div><div>Authors: Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor Maslej, Arvind Narayanan, Percy Liang</div><div style='padding-top: 10px; width: 80ex'>Foundation models are critical digital technologies with sweeping societal
impact that necessitates transparency. To codify how foundation model
developers should provide transparency about the development and deployment of
their models, we propose Foundation Model Transparency Reports, drawing upon
the transparency reporting practices in social media. While external
documentation of societal harms prompted social media transparency reports, our
objective is to institutionalize transparency reporting for foundation models
while the industry is still nascent. To design our reports, we identify 6
design principles given the successes and shortcomings of social media
transparency reporting. To further schematize our reports, we draw upon the 100
transparency indicators from the Foundation Model Transparency Index. Given
these indicators, we measure the extent to which they overlap with the
transparency requirements included in six prominent government policies (e.g.,
the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI).
Well-designed transparency reports could reduce compliance costs, in part due
to overlapping regulatory requirements across different jurisdictions. We
encourage foundation model developers to regularly publish transparency
reports, building upon recommendations from the G7 and the White House.</div><div><a href='http://arxiv.org/abs/2402.16268v1'>2402.16268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07904v1")'>Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem
  Beyond the AIA by Including Civil Society</div>
<div id='2403.07904v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T11:32:42Z</div><div>Authors: David Hartmann, José Renato Laranjeira de Pereira, Chiara Streitbörger, Bettina Berendt</div><div style='padding-top: 10px; width: 80ex'>The European legislature has proposed the Digital Services Act (DSA) and
Artificial Intelligence Act (AIA) to regulate platforms and Artificial
Intelligence (AI) products. We review to what extent third-party audits are
part of both laws and to what extent access to models and data is provided. By
considering the value of third-party audits and third-party data access in an
audit ecosystem, we identify a regulatory gap in that the Artificial
Intelligence Act does not provide access to data for researchers and civil
society. Our contributions to the literature include: (1) Defining an AI audit
ecosystem that incorporates compliance and oversight. (2) Highlighting a
regulatory gap within the DSA and AIA regulatory framework, preventing the
establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits
by research and civil society must be part of that ecosystem and demand that
the AIA include data and model access for certain AI products. We call for the
DSA to provide NGOs and investigative journalists with data access to platforms
by delegated acts and for adaptions and amendments of the AIA to provide
third-party audits and data and model access at least for high-risk systems to
close the regulatory gap. Regulations modeled after European Union AI
regulations should enable data access and third-party audits, fostering an AI
audit ecosystem that promotes compliance and oversight mechanisms.</div><div><a href='http://arxiv.org/abs/2403.07904v1'>2403.07904v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11046v1")'>Regulating Chatbot Output via Inter-Informational Competition</div>
<div id='2403.11046v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T00:11:15Z</div><div>Authors: Jiawei Zhang</div><div style='padding-top: 10px; width: 80ex'>The advent of ChatGPT has sparked over a year of regulatory frenzy. However,
few existing studies have rigorously questioned the assumption that, if left
unregulated, AI chatbot's output would inflict tangible, severe real harm on
human affairs. Most researchers have overlooked the critical possibility that
the information market itself can effectively mitigate these risks and, as a
result, they tend to use regulatory tools to address the issue directly. This
Article develops a yardstick for reevaluating both AI-related content risks and
corresponding regulatory proposals by focusing on inter-informational
competition among various outlets. The decades-long history of regulating
information and communications technologies indicates that regulators tend to
err too much on the side of caution and to put forward excessive regulatory
measures when encountering the uncertainties brought about by new technologies.
In fact, a trove of empirical evidence has demonstrated that market competition
among information outlets can effectively mitigate most risks and that
overreliance on regulation is not only unnecessary but detrimental, as well.
This Article argues that sufficient competition among chatbots and other
information outlets in the information marketplace can sufficiently mitigate
and even resolve most content risks posed by generative AI technologies. This
renders certain loudly advocated regulatory strategies, like mandatory
prohibitions, licensure, curation of datasets, and notice-and-response regimes,
truly unnecessary and even toxic to desirable competition and innovation
throughout the AI industry. Ultimately, the ideas that I advance in this
Article should pour some much-needed cold water on the regulatory frenzy over
generative AI and steer the issue back to a rational track.</div><div><a href='http://arxiv.org/abs/2403.11046v1'>2403.11046v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.08016v1")'>Contextual Bandits with Stage-wise Constraints</div>
<div id='2401.08016v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T23:58:21Z</div><div>Authors: Aldo Pacchiano, Mohammad Ghavamzadeh, Peter Bartlett</div><div style='padding-top: 10px; width: 80ex'>We study contextual bandits in the presence of a stage-wise constraint (a
constraint at each round), when the constraint must be satisfied both with high
probability and in expectation. Obviously the setting where the constraint is
in expectation is a relaxation of the one with high probability. We start with
the linear case where both the contextual bandit problem (reward function) and
the stage-wise constraint (cost function) are linear. In each of the high
probability and in expectation settings, we propose an upper-confidence bound
algorithm for the problem and prove a $T$-round regret bound for it. Our
algorithms balance exploration and constraint satisfaction using a novel idea
that scales the radii of the reward and cost confidence sets with different
scaling factors. We also prove a lower-bound for this constrained problem, show
how our algorithms and analyses can be extended to multiple constraints, and
provide simulations to validate our theoretical results. In the high
probability setting, we describe the minimum requirements for the action set in
order for our algorithm to be tractable. In the setting that the constraint is
in expectation, we further specialize our results to multi-armed bandits and
propose a computationally efficient algorithm for this setting with regret
analysis. Finally, we extend our results to the case where the reward and cost
functions are both non-linear. We propose an algorithm for this case and prove
a regret bound for it that characterize the function class complexity by the
eluder dimension.</div><div><a href='http://arxiv.org/abs/2401.08016v1'>2401.08016v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01857v1")'>Optimal cross-learning for contextual bandits with unknown context
  distributions</div>
<div id='2401.01857v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T18:02:13Z</div><div>Authors: Jon Schneider, Julian Zimmert</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of designing contextual bandit algorithms in the
``cross-learning'' setting of Balseiro et al., where the learner observes the
loss for the action they play in all possible contexts, not just the context of
the current round. We specifically consider the setting where losses are chosen
adversarially and contexts are sampled i.i.d. from an unknown distribution. In
this setting, we resolve an open problem of Balseiro et al. by providing an
efficient algorithm with a nearly tight (up to logarithmic factors) regret
bound of $\widetilde{O}(\sqrt{TK})$, independent of the number of contexts. As
a consequence, we obtain the first nearly tight regret bounds for the problems
of learning to bid in first-price auctions (under unknown value distributions)
and sleeping bandits with a stochastic action set.
  At the core of our algorithm is a novel technique for coordinating the
execution of a learning algorithm over multiple epochs in such a way to remove
correlations between estimation of the unknown distribution and the actions
played by the algorithm. This technique may be of independent interest for
other learning problems involving estimation of an unknown context
distribution.</div><div><a href='http://arxiv.org/abs/2401.01857v1'>2401.01857v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03219v1")'>LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits</div>
<div id='2403.03219v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:59:47Z</div><div>Authors: Masahiro Kato, Shinji Ito</div><div style='padding-top: 10px; width: 80ex'>This study considers the linear contextual bandit problem with independent
and identically distributed (i.i.d.) contexts. In this problem, existing
studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets
satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with
a suboptimality gap lower-bounded by a positive constant, while satisfying
$O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room
for improvement, and the suboptimality-gap assumption can be relaxed. For this
issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in
the setting when the suboptimality gap is lower-bounded. Furthermore, we
introduce a margin condition, a milder assumption on the suboptimality gap.
That condition characterizes the problem difficulty linked to the suboptimality
gap using a parameter $\beta \in (0, \infty]$. We then show that the
algorithm's regret satisfies
$O\left(\left\{\log(T)\right\}^{\frac{1+\beta}{2+\beta}}T^{\frac{1}{2+\beta}}\right)$.
Here, $\beta= \infty$ corresponds to the case in the existing studies where a
lower bound exists in the suboptimality gap, and our regret satisfies
$O(\log(T))$ in that case. Our proposed algorithm is based on the
Follow-The-Regularized-Leader with the Tsallis entropy and referred to as the
$\alpha$-Linear-Contextual (LC)-Tsallis-INF.</div><div><a href='http://arxiv.org/abs/2403.03219v1'>2403.03219v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18591v1")'>Stochastic contextual bandits with graph feedback: from independence
  number to MAS number</div>
<div id='2402.18591v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T06:56:13Z</div><div>Authors: Yuxiao Wen, Yanjun Han, Zhengyuan Zhou</div><div style='padding-top: 10px; width: 80ex'>We consider contextual bandits with graph feedback, a class of interactive
learning problems with richer structures than vanilla contextual bandits, where
taking an action reveals the rewards for all neighboring actions in the
feedback graph under all contexts. Unlike the multi-armed bandits setting where
a growing literature has painted a near-complete understanding of graph
feedback, much remains unexplored in the contextual bandits counterpart. In
this paper, we make inroads into this inquiry by establishing a regret lower
bound $\Omega(\sqrt{\beta_M(G) T})$, where $M$ is the number of contexts, $G$
is the feedback graph, and $\beta_M(G)$ is our proposed graph-theoretical
quantity that characterizes the fundamental learning limit for this class of
problems. Interestingly, $\beta_M(G)$ interpolates between $\alpha(G)$ (the
independence number of the graph) and $\mathsf{m}(G)$ (the maximum acyclic
subgraph (MAS) number of the graph) as the number of contexts $M$ varies. We
also provide algorithms that achieve near-optimal regrets for important classes
of context sequences and/or feedback graphs, such as transitively closed graphs
that find applications in auctions and inventory control. In particular, with
many contexts, our results show that the MAS number completely characterizes
the statistical complexity for contextual bandits, as opposed to the
independence number in multi-armed bandits.</div><div><a href='http://arxiv.org/abs/2402.18591v1'>2402.18591v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08127v1")'>Efficient Contextual Bandits with Uninformed Feedback Graphs</div>
<div id='2402.08127v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T23:50:47Z</div><div>Authors: Mengxiao Zhang, Yuheng Zhang, Haipeng Luo, Paul Mineiro</div><div style='padding-top: 10px; width: 80ex'>Bandits with feedback graphs are powerful online learning models that
interpolate between the full information and classic bandit problems, capturing
many real-life applications. A recent work by Zhang et al. (2023) studies the
contextual version of this problem and proposes an efficient and optimal
algorithm via a reduction to online regression. However, their algorithm
crucially relies on seeing the feedback graph before making each decision,
while in many applications, the feedback graph is uninformed, meaning that it
is either only revealed after the learner makes her decision or even never
fully revealed at all. This work develops the first contextual algorithm for
such uninformed settings, via an efficient reduction to online regression over
both the losses and the graphs. Importantly, we show that it is critical to
learn the graphs using log loss instead of squared loss to obtain favorable
regret guarantees. We also demonstrate the empirical effectiveness of our
algorithm on a bidding application using both synthetic and real-world data.</div><div><a href='http://arxiv.org/abs/2402.08127v1'>2402.08127v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08126v2")'>Contextual Multinomial Logit Bandits with General Value Functions</div>
<div id='2402.08126v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T23:50:44Z</div><div>Authors: Mengxiao Zhang, Haipeng Luo</div><div style='padding-top: 10px; width: 80ex'>Contextual multinomial logit (MNL) bandits capture many real-world assortment
recommendation problems such as online retailing/advertising. However, prior
work has only considered (generalized) linear value functions, which greatly
limits its applicability. Motivated by this fact, in this work, we consider
contextual MNL bandits with a general value function class that contains the
ground truth, borrowing ideas from a recent trend of studies on contextual
bandits. Specifically, we consider both the stochastic and the adversarial
settings, and propose a suite of algorithms, each with different
computation-regret trade-off. When applied to the linear case, our results not
only are the first ones with no dependence on a certain problem-dependent
constant that can be exponentially large, but also enjoy other advantages such
as computational efficiency, dimension-free regret bounds, or the ability to
handle completely adversarial contexts and rewards.</div><div><a href='http://arxiv.org/abs/2402.08126v2'>2402.08126v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03379v1")'>Entire Chain Uplift Modeling with Context-Enhanced Learning for
  Intelligent Marketing</div>
<div id='2402.03379v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T03:30:25Z</div><div>Authors: Yinqiu Huang, Shuli Wang, Min Gao, Xue Wei, Changhao Li, Chuan Luo, Yinhua Zhu, Xiong Xiao, Yi Luo</div><div style='padding-top: 10px; width: 80ex'>Uplift modeling, vital in online marketing, seeks to accurately measure the
impact of various strategies, such as coupons or discounts, on different users
by predicting the Individual Treatment Effect (ITE). In an e-commerce setting,
user behavior follows a defined sequential chain, including impression, click,
and conversion. Marketing strategies exert varied uplift effects at each stage
within this chain, impacting metrics like click-through and conversion rate.
Despite its utility, existing research has neglected to consider the inter-task
across all stages impacts within a specific treatment and has insufficiently
utilized the treatment information, potentially introducing substantial bias
into subsequent marketing decisions. We identify these two issues as the
chain-bias problem and the treatment-unadaptive problem. This paper introduces
the Entire Chain UPlift method with context-enhanced learning (ECUP), devised
to tackle these issues. ECUP consists of two primary components: 1) the Entire
Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE
throughout the entire chain space, models the various impacts of treatments on
each task, and integrates task prior information to enhance context awareness
across all stages, capturing the impact of treatment on different tasks, and 2)
the Treatment-Enhanced Network, which facilitates fine-grained treatment
modeling through bit-level feature interactions, thereby enabling adaptive
feature adjustment. Extensive experiments on public and industrial datasets
validate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan
food delivery platform, serving millions of daily active users, with the
related dataset released for future research.</div><div><a href='http://arxiv.org/abs/2402.03379v1'>2402.03379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17655v1")'>Confidence-Aware Multi-Field Model Calibration</div>
<div id='2402.17655v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T16:24:28Z</div><div>Authors: Yuang Zhao, Chuhan Wu, Qinglin Jia, Hong Zhu, Jia Yan, Libin Zong, Linxuan Zhang, Zhenhua Dong, Muyu Zhang</div><div style='padding-top: 10px; width: 80ex'>Accurately predicting the probabilities of user feedback, such as clicks and
conversions, is critical for ad ranking and bidding. However, there often exist
unwanted mismatches between predicted probabilities and true likelihoods due to
the shift of data distributions and intrinsic model biases. Calibration aims to
address this issue by post-processing model predictions, and field-aware
calibration can adjust model output on different feature field values to
satisfy fine-grained advertising demands. Unfortunately, the observed samples
corresponding to certain field values can be too limited to make confident
calibrations, which may yield bias amplification and online disturbance. In
this paper, we propose a confidence-aware multi-field calibration method, which
adaptively adjusts the calibration intensity based on the confidence levels
derived from sample statistics. It also utilizes multiple feature fields for
joint model calibration with awareness of their importance to mitigate the data
sparsity effect of a single field. Extensive offline and online experiments
show the superiority of our method in boosting advertising performance and
reducing prediction deviations.</div><div><a href='http://arxiv.org/abs/2402.17655v1'>2402.17655v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09507v1")'>Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in
  Online Advertising</div>
<div id='2401.09507v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T11:41:11Z</div><div>Authors: Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, Yifan Zeng</div><div style='padding-top: 10px; width: 80ex'>In the e-commerce advertising scenario, estimating the true probabilities
(known as a calibrated estimate) on CTR and CVR is critical and can directly
affect the benefits of the buyer, seller and platform. Previous research has
introduced numerous solutions for addressing the calibration problem. These
methods typically involve the training of calibrators using a validation set
and subsequently applying these calibrators to correct the original estimated
values during online inference. However, what sets e-commerce advertising
scenarios is the challenge of multi-field calibration. Multi-field calibration
can be subdivided into two distinct sub-problems: value calibration and shape
calibration. Value calibration is defined as no over- or under-estimation for
each value under concerned fields. Shape calibration is defined as no over- or
under-estimation for each subset of the pCTR within the specified range under
condition of concerned fields. In order to achieve shape calibration and value
calibration, it is necessary to have a strong data utilization ability.Because
the quantity of pCTR specified range for single field-value sample is relative
small, which makes the calibrator more difficult to train. However the existing
methods cannot simultaneously fulfill both value calibration and shape
calibration. To solve these problems, we propose a new method named Deep
Ensemble Shape Calibration (DESC). We introduce innovative basis calibration
functions, which enhance both function expression capabilities and data
utilization by combining these basis calibration functions. A significant
advancement lies in the development of an allocator capable of allocating the
most suitable shape calibrators to different estimation error distributions
within diverse fields and values.</div><div><a href='http://arxiv.org/abs/2401.09507v1'>2401.09507v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16432v1")'>Improving conversion rate prediction via self-supervised pre-training in
  online advertising</div>
<div id='2401.16432v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T08:44:22Z</div><div>Authors: Alex Shtoff, Yohay Kaplan, Ariel Raviv</div><div style='padding-top: 10px; width: 80ex'>The task of predicting conversion rates (CVR) lies at the heart of online
advertising systems aiming to optimize bids to meet advertiser performance
requirements. Even with the recent rise of deep neural networks, these
predictions are often made by factorization machines (FM), especially in
commercial settings where inference latency is key. These models are trained
using the logistic regression framework on labeled tabular data formed from
past user activity that is relevant to the task at hand.
  Many advertisers only care about click-attributed conversions. A major
challenge in training models that predict conversions-given-clicks comes from
data sparsity - clicks are rare, conversions attributed to clicks are even
rarer. However, mitigating sparsity by adding conversions that are not
click-attributed to the training set impairs model calibration. Since
calibration is critical to achieving advertiser goals, this is infeasible.
  In this work we use the well-known idea of self-supervised pre-training, and
use an auxiliary auto-encoder model trained on all conversion events, both
click-attributed and not, as a feature extractor to enrich the main CVR
prediction model. Since the main model does not train on non click-attributed
conversions, this does not impair calibration. We adapt the basic
self-supervised pre-training idea to our online advertising setup by using a
loss function designed for tabular data, facilitating continual learning by
ensuring auto-encoder stability, and incorporating a neural network into a
large-scale real-time ad auction that ranks tens of thousands of ads, under
strict latency constraints, and without incurring a major engineering cost. We
show improvements both offline, during training, and in an online A/B test.
Following its success in A/B tests, our solution is now fully deployed to the
Yahoo native advertising system.</div><div><a href='http://arxiv.org/abs/2401.16432v1'>2401.16432v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16692v1")'>Calibration-then-Calculation: A Variance Reduced Metric Framework in
  Deep Click-Through Rate Prediction Models</div>
<div id='2401.16692v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T02:38:23Z</div><div>Authors: Yewen Fan, Nian Si, Xiangchen Song, Kun Zhang</div><div style='padding-top: 10px; width: 80ex'>Deep learning has been widely adopted across various fields, but there has
been little focus on evaluating the performance of deep learning pipelines.
With the increased use of large datasets and complex models, it has become
common to run the training process only once and compare the result to previous
benchmarks. However, this procedure can lead to imprecise comparisons due to
the variance in neural network evaluation metrics. The metric variance comes
from the randomness inherent in the training process of deep learning
pipelines. Traditional solutions such as running the training process multiple
times are usually not feasible in deep learning due to computational
limitations. In this paper, we propose a new metric framework, Calibrated Loss
Metric, that addresses this issue by reducing the variance in its vanilla
counterpart. As a result, the new metric has a higher accuracy to detect
effective modeling improvement. Our approach is supported by theoretical
justifications and extensive experimental validations in the context of Deep
Click-Through Rate Prediction Models.</div><div><a href='http://arxiv.org/abs/2401.16692v1'>2401.16692v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08875v2")'>DCRMTA: Unbiased Causal Representation for Multi-touch Attribution</div>
<div id='2401.08875v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T23:16:18Z</div><div>Authors: Jiaming Tang</div><div style='padding-top: 10px; width: 80ex'>Multi-touch attribution (MTA) currently plays a pivotal role in achieving a
fair estimation of the contributions of each advertising touchpoint to-wards
conversion behavior, deeply influencing budget allocation and advertising
recommenda-tion. Previous works attempted to eliminate the bias caused by user
preferences to achieve the unbiased assumption of the conversion model. The
multi-model collaboration method is not ef-ficient, and the complete
elimination of user in-fluence also eliminates the causal effect of user
features on conversion, resulting in limited per-formance of the conversion
model. This paper re-defines the causal effect of user features on con-versions
and proposes a novel end-to-end ap-proach, Deep Causal Representation for MTA
(DCRMTA). Our model focuses on extracting causa features between conversions
and users while eliminating confounding variables. Fur-thermore, extensive
experiments demonstrate DCRMTA's superior performance in converting prediction
across varying data distributions, while also effectively attributing value
across dif-ferent advertising channels.</div><div><a href='http://arxiv.org/abs/2401.08875v2'>2401.08875v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00798v1")'>Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian
  Eigenvalue Regularization</div>
<div id='2403.00798v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:00:46Z</div><div>Authors: Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, Yang You</div><div style='padding-top: 10px; width: 80ex'>Click-Through Rate (CTR) prediction holds paramount significance in online
advertising and recommendation scenarios. Despite the proliferation of recent
CTR prediction models, the improvements in performance have remained limited,
as evidenced by open-source benchmark assessments. Current researchers tend to
focus on developing new models for various datasets and settings, often
neglecting a crucial question: What is the key challenge that truly makes CTR
prediction so demanding?
  In this paper, we approach the problem of CTR prediction from an optimization
perspective. We explore the typical data characteristics and optimization
statistics of CTR prediction, revealing a strong positive correlation between
the top hessian eigenvalue and feature frequency. This correlation implies that
frequently occurring features tend to converge towards sharp local minima,
ultimately leading to suboptimal performance. Motivated by the recent
advancements in sharpness-aware minimization (SAM), which considers the
geometric aspects of the loss landscape during optimization, we present a
dedicated optimizer crafted for CTR prediction, named Helen. Helen incorporates
frequency-wise Hessian eigenvalue regularization, achieved through adaptive
perturbations based on normalized feature frequencies.
  Empirical results under the open-source benchmark framework underscore
Helen's effectiveness. It successfully constrains the top eigenvalue of the
Hessian matrix and demonstrates a clear advantage over widely used optimization
algorithms when applied to seven popular models across three public benchmark
datasets on BARS. Our code locates at github.com/NUS-HPC-AI-Lab/Helen.</div><div><a href='http://arxiv.org/abs/2403.00798v1'>2403.00798v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07745v1")'>Predictive Churn with the Set of Good Models</div>
<div id='2402.07745v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:15:25Z</div><div>Authors: Jamelle Watson-Daniels, Flavio du Pin Calmon, Alexander D'Amour, Carol Long, David C. Parkes, Berk Ustun</div><div style='padding-top: 10px; width: 80ex'>Machine learning models in modern mass-market applications are often updated
over time. One of the foremost challenges faced is that, despite increasing
overall performance, these updates may flip specific model predictions in
unpredictable ways. In practice, researchers quantify the number of unstable
predictions between models pre and post update -- i.e., predictive churn. In
this paper, we study this effect through the lens of predictive multiplicity --
i.e., the prevalence of conflicting predictions over the set of near-optimal
models (the Rashomon set). We show how traditional measures of predictive
multiplicity can be used to examine expected churn over this set of prospective
models -- i.e., the set of models that may be used to replace a baseline model
in deployment. We present theoretical results on the expected churn between
models within the Rashomon set from different perspectives. And we characterize
expected churn over model updates via the Rashomon set, pairing our analysis
with empirical results on real-world datasets -- showing how our approach can
be used to better anticipate, reduce, and avoid churn in consumer-facing
applications. Further, we show that our approach is useful even for models
enhanced with uncertainty awareness.</div><div><a href='http://arxiv.org/abs/2402.07745v1'>2402.07745v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06710v1")'>Model-Free Approximate Bayesian Learning for Large-Scale Conversion
  Funnel Optimization</div>
<div id='2401.06710v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T17:19:44Z</div><div>Authors: Garud Iyengar, Raghav Singal</div><div style='padding-top: 10px; width: 80ex'>The flexibility of choosing the ad action as a function of the consumer state
is critical for modern-day marketing campaigns. We study the problem of
identifying the optimal sequential personalized interventions that maximize the
adoption probability for a new product. We model consumer behavior by a
conversion funnel that captures the state of each consumer (e.g., interaction
history with the firm) and allows the consumer behavior to vary as a function
of both her state and firm's sequential interventions. We show our model
captures consumer behavior with very high accuracy (out-of-sample AUC of over
0.95) in a real-world email marketing dataset. However, it results in a very
large-scale learning problem, where the firm must learn the state-specific
effects of various interventions from consumer interactions. We propose a novel
attribution-based decision-making algorithm for this problem that we call
model-free approximate Bayesian learning. Our algorithm inherits the
interpretability and scalability of Thompson sampling for bandits and maintains
an approximate belief over the value of each state-specific intervention. The
belief is updated as the algorithm interacts with the consumers. Despite being
an approximation to the Bayes update, we prove the asymptotic optimality of our
algorithm and analyze its convergence rate. We show that our algorithm
significantly outperforms traditional approaches on extensive simulations
calibrated to a real-world email marketing dataset.</div><div><a href='http://arxiv.org/abs/2401.06710v1'>2401.06710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03231v1")'>Improved prediction of future user activity in online A/B testing</div>
<div id='2402.03231v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:44:21Z</div><div>Authors: Lorenzo Masoero, Mario Beraha, Thomas Richardson, Stefano Favaro</div><div style='padding-top: 10px; width: 80ex'>In online randomized experiments or A/B tests, accurate predictions of
participant inclusion rates are of paramount importance. These predictions not
only guide experimenters in optimizing the experiment's duration but also
enhance the precision of treatment effect estimates. In this paper we present a
novel, straightforward, and scalable Bayesian nonparametric approach for
predicting the rate at which individuals will be exposed to interventions
within the realm of online A/B testing. Our approach stands out by offering
dual prediction capabilities: it forecasts both the quantity of new customers
expected in future time windows and, unlike available alternative methods, the
number of times they will be observed. We derive closed-form expressions for
the posterior distributions of the quantities needed to form predictions about
future user activity, thereby bypassing the need for numerical algorithms such
as Markov chain Monte Carlo. After a comprehensive exposition of our model, we
test its performance on experiments on real and simulated data, where we show
its superior performance with respect to existing alternatives in the
literature.</div><div><a href='http://arxiv.org/abs/2402.03231v1'>2402.03231v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14722v1")'>A Nonparametric Bayes Approach to Online Activity Prediction</div>
<div id='2401.14722v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T09:11:42Z</div><div>Authors: Mario Beraha, Lorenzo Masoero, Stefano Favaro, Thomas S. Richardson</div><div style='padding-top: 10px; width: 80ex'>Accurately predicting the onset of specific activities within defined
timeframes holds significant importance in several applied contexts. In
particular, accurate prediction of the number of future users that will be
exposed to an intervention is an important piece of information for
experimenters running online experiments (A/B tests). In this work, we propose
a novel approach to predict the number of users that will be active in a given
time period, as well as the temporal trajectory needed to attain a desired user
participation threshold. We model user activity using a Bayesian nonparametric
approach which allows us to capture the underlying heterogeneity in user
engagement. We derive closed-form expressions for the number of new users
expected in a given period, and a simple Monte Carlo algorithm targeting the
posterior distribution of the number of days needed to attain a desired number
of users; the latter is important for experimental planning. We illustrate the
performance of our approach via several experiments on synthetic and real world
data, in which we show that our novel method outperforms existing competitors.</div><div><a href='http://arxiv.org/abs/2401.14722v1'>2401.14722v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10870v3")'>Best of Three Worlds: Adaptive Experimentation for Digital Marketing in
  Practice</div>
<div id='2402.10870v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:13:35Z</div><div>Authors: Tanner Fiez, Houssam Nassif, Yu-Cheng Chen, Sergio Gamez, Lalit Jain</div><div style='padding-top: 10px; width: 80ex'>Adaptive experimental design (AED) methods are increasingly being used in
industry as a tool to boost testing throughput or reduce experimentation cost
relative to traditional A/B/N testing methods. However, the behavior and
guarantees of such methods are not well-understood beyond idealized stationary
settings. This paper shares lessons learned regarding the challenges of naively
using AED systems in industrial settings where non-stationarity is prevalent,
while also providing perspectives on the proper objectives and system
specifications in such settings. We developed an AED framework for
counterfactual inference based on these experiences, and tested it in a
commercial environment.</div><div><a href='http://arxiv.org/abs/2402.10870v3'>2402.10870v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05653v3")'>Quantifying Marketing Performance at Channel-Partner Level by Using
  Marketing Mix Modeling (MMM) and Shapley Value Regression</div>
<div id='2401.05653v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T04:24:19Z</div><div>Authors: Sean Tang, Sriya Musunuru, Baoshi Zong, Brooks Thornton</div><div style='padding-top: 10px; width: 80ex'>This paper explores the application of Shapley Value Regression in dissecting
marketing performance at channel-partner level, complementing channel-level
Marketing Mix Modeling (MMM). Utilizing real-world data from the financial
services industry, we demonstrate the practicality of Shapley Value Regression
in evaluating individual partner contributions. Although structured in-field
testing along with cooperative game theory is most accurate, it can often be
highly complex and expensive to conduct. Shapley Value Regression is thus a
more feasible approach to disentangle the influence of each marketing partner
within a marketing channel. We also propose a simple method to derive adjusted
coefficients of Shapley Value Regression and compare it with alternative
approaches.</div><div><a href='http://arxiv.org/abs/2401.05653v3'>2401.05653v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.01301v1")'>Supplier Recommendation in Online Procurement</div>
<div id='2403.01301v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T19:55:38Z</div><div>Authors: Victor Coscrato, Derek Bridge</div><div style='padding-top: 10px; width: 80ex'>Supply chain optimization is key to a healthy and profitable business. Many
companies use online procurement systems to agree contracts with suppliers. It
is vital that the most competitive suppliers are invited to bid for such
contracts. In this work, we propose a recommender system to assist with
supplier discovery in road freight online procurement. Our system is able to
provide personalized supplier recommendations, taking into account customer
needs and preferences. This is a novel application of recommender systems,
calling for design choices that fit the unique requirements of online
procurement. Our preliminary results, using real-world data, are promising.</div><div><a href='http://arxiv.org/abs/2403.01301v1'>2403.01301v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02171v2")'>Off-Policy Evaluation of Slate Bandit Policies via Optimizing
  Abstraction</div>
<div id='2402.02171v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T14:38:09Z</div><div>Authors: Haruka Kiyohara, Masahiro Nomura, Yuta Saito</div><div style='padding-top: 10px; width: 80ex'>We study off-policy evaluation (OPE) in the problem of slate contextual
bandits where a policy selects multi-dimensional actions known as slates. This
problem is widespread in recommender systems, search engines, marketing, to
medical applications, however, the typical Inverse Propensity Scoring (IPS)
estimator suffers from substantial variance due to large action spaces, making
effective OPE a significant challenge. The PseudoInverse (PI) estimator has
been introduced to mitigate the variance issue by assuming linearity in the
reward function, but this can result in significant bias as this assumption is
hard-to-verify from observed data and is often substantially violated. To
address the limitations of previous estimators, we develop a novel estimator
for OPE of slate bandits, called Latent IPS (LIPS), which defines importance
weights in a low-dimensional slate abstraction space where we optimize slate
abstractions to minimize the bias and variance of LIPS in a data-driven way. By
doing so, LIPS can substantially reduce the variance of IPS without imposing
restrictive assumptions on the reward function structure like linearity.
Through empirical evaluation, we demonstrate that LIPS substantially
outperforms existing estimators, particularly in scenarios with non-linear
rewards and large slate spaces.</div><div><a href='http://arxiv.org/abs/2402.02171v2'>2402.02171v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07363v1")'>Strategically-Robust Learning Algorithms for Bidding in First-Price
  Auctions</div>
<div id='2402.07363v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T01:33:33Z</div><div>Authors: Rachitesh Kumar, Jon Schneider, Balasubramanian Sivan</div><div style='padding-top: 10px; width: 80ex'>Learning to bid in repeated first-price auctions is a fundamental problem at
the interface of game theory and machine learning, which has seen a recent
surge in interest due to the transition of display advertising to first-price
auctions. In this work, we propose a novel concave formulation for
pure-strategy bidding in first-price auctions, and use it to analyze natural
Gradient-Ascent-based algorithms for this problem. Importantly, our analysis
goes beyond regret, which was the typical focus of past work, and also accounts
for the strategic backdrop of online-advertising markets where bidding
algorithms are deployed -- we prove that our algorithms cannot be exploited by
a strategic seller and that they incentivize truth-telling for the buyer.
  Concretely, we show that our algorithms achieve $O(\sqrt{T})$ regret when the
highest competing bids are generated adversarially, and show that no online
algorithm can do better. We further prove that the regret improves to $O(\log
T)$ when the competition is stationary and stochastic. Moving beyond regret, we
show that a strategic seller cannot exploit our algorithms to extract more
revenue on average than is possible under the optimal mechanism, i.e., the
seller cannot do much better than posting the monopoly reserve price in each
auction. Finally, we prove that our algorithm is also incentive compatible --
it is a (nearly) dominant strategy for the buyer to report her values
truthfully to the algorithm as a whole.</div><div><a href='http://arxiv.org/abs/2402.07363v1'>2402.07363v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08637v1")'>Strategizing against No-Regret Learners in First-Price Auctions</div>
<div id='2402.08637v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:03:56Z</div><div>Authors: Aviad Rubinstein, Junyao Zhao</div><div style='padding-top: 10px; width: 80ex'>We study repeated first-price auctions and general repeated Bayesian games
between two players, where one player, the learner, employs a no-regret
learning algorithm, and the other player, the optimizer, knowing the learner's
algorithm, strategizes to maximize its own utility. For a commonly used class
of no-regret learning algorithms called mean-based algorithms, we show that (i)
in standard (i.e., full-information) first-price auctions, the optimizer cannot
get more than the Stackelberg utility -- a standard benchmark in the
literature, but (ii) in Bayesian first-price auctions, there are instances
where the optimizer can achieve much higher than the Stackelberg utility.
  On the other hand, Mansour et al. (2022) showed that a more sophisticated
class of algorithms called no-polytope-swap-regret algorithms are sufficient to
cap the optimizer's utility at the Stackelberg utility in any repeated Bayesian
game (including Bayesian first-price auctions), and they pose the open question
whether no-polytope-swap-regret algorithms are necessary to cap the optimizer's
utility. For general Bayesian games, under a reasonable and necessary
condition, we prove that no-polytope-swap-regret algorithms are indeed
necessary to cap the optimizer's utility and thus answer their open question.
For Bayesian first-price auctions, we give a simple improvement of the standard
algorithm for minimizing the polytope swap regret by exploiting the structure
of Bayesian first-price auctions.</div><div><a href='http://arxiv.org/abs/2402.08637v1'>2402.08637v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15892v1")'>Statistical Games</div>
<div id='2402.15892v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T19:59:15Z</div><div>Authors: Jozsef Konczer</div><div style='padding-top: 10px; width: 80ex'>This work contains the mathematical exploration of a few prototypical games
in which central concepts from statistics and probability theory naturally
emerge. The first two kinds of games are termed Fisher and Bayesian games,
which are connected to Frequentist and Bayesian statistics, respectively.
Later, a more general type of game is introduced, termed Statistical game, in
which a further parameter, the players' relative risk aversion, can be set. In
this work, we show that Fisher and Bayesian games can be viewed as limiting
cases of Statistical games. Therefore, Statistical games can be viewed as a
unified framework, incorporating both Frequentist and Bayesian statistics.
Furthermore, a philosophical framework is (re-)presented -- often referred to
as minimax regret criterion -- as a general approach to decision making.
  The main motivation for this work was to embed Bayesian statistics into a
broader decision-making framework, where, based on collected data, actions with
consequences have to be made, which can be translated to utilities (or
rewards/losses) of the decision-maker. The work starts with the simplest
possible toy model, related to hypothesis testing and statistical inference.
This choice has two main benefits: i.) it allows us to determine (conjecture)
the behaviour of the equilibrium strategies in various limiting cases ii.) this
way, we can introduce Statistical games without requiring additional stochastic
parameters. The work contains game theoretical methods related to two-player,
non-cooperative games to determine and prove equilibrium strategies of Fisher,
Bayesian and Statistical games. It also relies on analytical tools for
derivations concerning various limiting cases.</div><div><a href='http://arxiv.org/abs/2402.15892v1'>2402.15892v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02741v1")'>State-Constrained Zero-Sum Differential Games with One-Sided Information</div>
<div id='2403.02741v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T07:51:38Z</div><div>Authors: Mukesh Ghimire, Lei Zhang, Zhe Xu, Yi Ren</div><div style='padding-top: 10px; width: 80ex'>We study zero-sum differential games with state constraints and one-sided
information, where the informed player (Player 1) has a categorical payoff type
unknown to the uninformed player (Player 2). The goal of Player 1 is to
minimize his payoff without violating the constraints, while that of Player 2
is to either violate the state constraints, or otherwise, to maximize the
payoff. One example of the game is a man-to-man matchup in football. Without
state constraints, Cardaliaguet (2007) showed that the value of such a game
exists and is convex to the common belief of players. Our theoretical
contribution is an extension of this result to differential games with state
constraints and the derivation of the primal and dual subdynamic principles
necessary for computing the behavioral strategies. Compared with existing works
on imperfect-information dynamic games that focus on scalability and
generalization, our focus is instead on revealing the mechanism of belief
manipulation behaviors resulted from information asymmetry and state
constraints. We use a simplified football game to demonstrate the utility of
this work, where we reveal player positions and belief states in which the
attacker should (or should not) play specific random fake moves to take
advantage of information asymmetry, and compute how the defender should
respond.</div><div><a href='http://arxiv.org/abs/2403.02741v1'>2403.02741v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00845v1")'>Improved Online Learning Algorithms for CTR Prediction in Ad Auctions</div>
<div id='2403.00845v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T14:10:26Z</div><div>Authors: Zhe Feng, Christopher Liaw, Zixin Zhou</div><div style='padding-top: 10px; width: 80ex'>In this work, we investigate the online learning problem of revenue
maximization in ad auctions, where the seller needs to learn the click-through
rates (CTRs) of each ad candidate and charge the price of the winner through a
pay-per-click manner. We focus on two models of the advertisers' strategic
behaviors. First, we assume that the advertiser is completely myopic; i.e.~in
each round, they aim to maximize their utility only for the current round. In
this setting, we develop an online mechanism based on upper-confidence bounds
that achieves a tight $O(\sqrt{T})$ regret in the worst-case and negative
regret when the values are static across all the auctions and there is a gap
between the highest expected value (i.e.~value multiplied by their CTR) and
second highest expected value ad. Next, we assume that the advertiser is
non-myopic and cares about their long term utility. This setting is much more
complex since an advertiser is incentivized to influence the mechanism by
bidding strategically in earlier rounds. In this setting, we provide an
algorithm to achieve negative regret for the static valuation setting (with a
positive gap), which is in sharp contrast with the prior work that shows
$O(T^{2/3})$ regret when the valuation is generated by adversary.</div><div><a href='http://arxiv.org/abs/2403.00845v1'>2403.00845v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08701v1")'>Primal-Dual Algorithms with Predictions for Online Bounded Allocation
  and Ad-Auctions Problems</div>
<div id='2402.08701v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T13:02:11Z</div><div>Authors: Eniko Kevi, Nguyen Kim Thang</div><div style='padding-top: 10px; width: 80ex'>Matching problems have been widely studied in the research community,
especially Ad-Auctions with many applications ranging from network design to
advertising. Following the various advancements in machine learning, one
natural question is whether classical algorithms can benefit from machine
learning and obtain better-quality solutions. Even a small percentage of
performance improvement in matching problems could result in significant gains
for the studied use cases. For example, the network throughput or the revenue
of Ad-Auctions can increase remarkably. This paper presents algorithms with
machine learning predictions for the Online Bounded Allocation and the Online
Ad-Auctions problems. We constructed primal-dual algorithms that achieve
competitive performance depending on the quality of the predictions. When the
predictions are accurate, the algorithms' performance surpasses previous
performance bounds, while when the predictions are misleading, the algorithms
maintain standard worst-case performance guarantees. We provide supporting
experiments on generated data for our theoretical findings.</div><div><a href='http://arxiv.org/abs/2402.08701v1'>2402.08701v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11904v1")'>Scalable Virtual Valuations Combinatorial Auction Design by Combining
  Zeroth-Order and First-Order Optimization Method</div>
<div id='2402.11904v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T07:45:04Z</div><div>Authors: Zhijian Duan, Haoran Sun, Yichong Xia, Siqiang Wang, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng, Xiaotie Deng</div><div style='padding-top: 10px; width: 80ex'>Automated auction design seeks to discover empirically high-revenue and
incentive-compatible mechanisms using machine learning. Ensuring dominant
strategy incentive compatibility (DSIC) is crucial, and the most effective
approach is to confine the mechanism to Affine Maximizer Auctions (AMAs).
Nevertheless, existing AMA-based approaches encounter challenges such as
scalability issues (arising from combinatorial candidate allocations) and the
non-differentiability of revenue. In this paper, to achieve a scalable
AMA-based method, we further restrict the auction mechanism to Virtual
Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly
fewer parameters. Initially, we employ a parallelizable dynamic programming
algorithm to compute the winning allocation of a VVCA. Subsequently, we propose
a novel optimization method that combines both zeroth-order and first-order
techniques to optimize the VVCA parameters. Extensive experiments demonstrate
the efficacy and scalability of our proposed approach, termed Zeroth-order and
First-order Optimization of VVCAs (ZFO-VVCA), particularly when applied to
large-scale auctions.</div><div><a href='http://arxiv.org/abs/2402.11904v1'>2402.11904v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05193v1")'>Experiment Planning with Function Approximation</div>
<div id='2401.05193v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T14:40:23Z</div><div>Authors: Aldo Pacchiano, Jonathan N. Lee, Emma Brunskill</div><div style='padding-top: 10px; width: 80ex'>We study the problem of experiment planning with function approximation in
contextual bandit problems. In settings where there is a significant overhead
to deploying adaptive algorithms -- for example, when the execution of the data
collection policies is required to be distributed, or a human in the loop is
needed to implement these policies -- producing in advance a set of policies
for data collection is paramount. We study the setting where a large dataset of
contexts but not rewards is available and may be used by the learner to design
an effective data collection strategy. Although when rewards are linear this
problem has been well studied, results are still missing for more complex
reward models. In this work we propose two experiment planning strategies
compatible with function approximation. The first is an eluder planning and
sampling procedure that can recover optimality guarantees depending on the
eluder dimension of the reward function class. For the second, we show that a
uniform sampler achieves competitive optimality rates in the setting where the
number of actions is small. We finalize our results introducing a statistical
gap fleshing out the fundamental differences between planning and adaptive
learning and provide results for planning with model selection.</div><div><a href='http://arxiv.org/abs/2401.05193v1'>2401.05193v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17732v1")'>Batched Nonparametric Contextual Bandits</div>
<div id='2402.17732v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:06:20Z</div><div>Authors: Rong Jiang, Cong Ma</div><div style='padding-top: 10px; width: 80ex'>We study nonparametric contextual bandits under batch constraints, where the
expected reward for each action is modeled as a smooth function of covariates,
and the policy updates are made at the end of each batch of observations. We
establish a minimax regret lower bound for this setting and propose Batched
Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal
regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the
covariate space into smaller bins, carefully aligning their widths with the
batch size. We also show the suboptimality of static binning under batch
constraints, highlighting the necessity of dynamic binning. Additionally, our
results suggest that a nearly constant number of policy updates can attain
optimal regret in the fully online setting.</div><div><a href='http://arxiv.org/abs/2402.17732v1'>2402.17732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07341v1")'>Noise-Adaptive Confidence Sets for Linear Bandits and Application to
  Bayesian Optimization</div>
<div id='2402.07341v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T00:19:09Z</div><div>Authors: Kwang-Sung Jun, Jungtaek Kim</div><div style='padding-top: 10px; width: 80ex'>Adapting to a priori unknown noise level is a very important but challenging
problem in sequential decision-making as efficient exploration typically
requires knowledge of the noise level, which is often loosely specified. We
report significant progress in addressing this issue in linear bandits in two
respects. First, we propose a novel confidence set that is `semi-adaptive' to
the unknown sub-Gaussian parameter $\sigma_*^2$ in the sense that the
(normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$
where $d$ is the dimension and $\sigma_0^2$ is the specified sub-Gaussian
parameter (known) that can be much larger than $\sigma_*^2$. This is a
significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence
set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that
this leads to an improved regret bound in linear bandits. Second, for bounded
rewards, we propose a novel variance-adaptive confidence set that has a much
improved numerical performance upon prior art. We then apply this confidence
set to develop, as we claim, the first practical variance-adaptive linear
bandit algorithm via an optimistic approach, which is enabled by our novel
regret analysis technique. Both of our confidence sets rely critically on
`regret equality' from online learning. Our empirical evaluation in Bayesian
optimization tasks shows that our algorithms demonstrate better or comparable
performance compared to existing methods.</div><div><a href='http://arxiv.org/abs/2402.07341v1'>2402.07341v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15347v1")'>Information-Theoretic Safe Bayesian Optimization</div>
<div id='2402.15347v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:31:10Z</div><div>Authors: Alessandro G. Bottero, Carlos E. Luis, Julia Vinogradska, Felix Berkenkamp, Jan Peters</div><div style='padding-top: 10px; width: 80ex'>We consider a sequential decision making task, where the goal is to optimize
an unknown function without evaluating parameters that violate an a~priori
unknown (safety) constraint. A common approach is to place a Gaussian process
prior on the unknown functions and allow evaluations only in regions that are
safe with high probability. Most current methods rely on a discretization of
the domain and cannot be directly extended to the continuous case. Moreover,
the way in which they exploit regularity assumptions about the constraint
introduces an additional critical hyperparameter. In this paper, we propose an
information-theoretic safe exploration criterion that directly exploits the GP
posterior to identify the most informative safe parameters to evaluate. The
combination of this exploration criterion with a well known Bayesian
optimization acquisition function yields a novel safe Bayesian optimization
selection criterion. Our approach is naturally applicable to continuous domains
and does not require additional explicit hyperparameters. We theoretically
analyze the method and show that we do not violate the safety constraint with
high probability and that we learn about the value of the safe optimum up to
arbitrary precision. Empirical evaluations demonstrate improved data-efficiency
and scalability.</div><div><a href='http://arxiv.org/abs/2402.15347v1'>2402.15347v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14402v1")'>Global Safe Sequential Learning via Efficient Knowledge Transfer</div>
<div id='2402.14402v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:43:25Z</div><div>Authors: Cen-You Li, Olaf Duennbier, Marc Toussaint, Barbara Rakitsch, Christoph Zimmer</div><div style='padding-top: 10px; width: 80ex'>Sequential learning methods such as active learning and Bayesian optimization
select the most informative data to learn about a task. In many medical or
engineering applications, the data selection is constrained by a priori unknown
safety conditions. A promissing line of safe learning methods utilize Gaussian
processes (GPs) to model the safety probability and perform data selection in
areas with high safety confidence. However, accurate safety modeling requires
prior knowledge or consumes data. In addition, the safety confidence centers
around the given observations which leads to local exploration. As transferable
source knowledge is often available in safety critical experiments, we propose
to consider transfer safe sequential learning to accelerate the learning of
safety. We further consider a pre-computation of source components to reduce
the additional computational load that is introduced by incorporating source
data. In this paper, we theoretically analyze the maximum explorable safe
regions of conventional safe learning methods. Furthermore, we empirically
demonstrate that our approach 1) learns a task with lower data consumption, 2)
globally explores multiple disjoint safe regions under guidance of the source
knowledge, and 3) operates with computation comparable to conventional safe
learning methods.</div><div><a href='http://arxiv.org/abs/2402.14402v1'>2402.14402v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09201v1")'>Better-than-KL PAC-Bayes Bounds</div>
<div id='2402.09201v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T14:33:39Z</div><div>Authors: Ilja Kuzborskij, Kwang-Sung Jun, Yulian Wu, Kyoungseok Jang, Francesco Orabona</div><div style='padding-top: 10px; width: 80ex'>Let $f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$ be a sequence of random
elements, where $f$ is a fixed scalar function, $X_1, \dots, X_n$ are
independent random variables (data), and $\theta$ is a random parameter
distributed according to some data-dependent posterior distribution $P_n$. In
this paper, we consider the problem of proving concentration inequalities to
estimate the mean of the sequence. An example of such a problem is the
estimation of the generalization error of some predictor trained by a
stochastic algorithm, such as a neural network where $f$ is a loss function.
Classically, this problem is approached through a PAC-Bayes analysis where, in
addition to the posterior, we choose a prior distribution which captures our
belief about the inductive bias of the learning problem. Then, the key quantity
in PAC-Bayes concentration bounds is a divergence that captures the complexity
of the learning problem where the de facto standard choice is the KL
divergence. However, the tightness of this choice has rarely been questioned.
  In this paper, we challenge the tightness of the KL-divergence-based bounds
by showing that it is possible to achieve a strictly tighter bound. In
particular, we demonstrate new high-probability PAC-Bayes bounds with a novel
and better-than-KL divergence that is inspired by Zhang et al. (2022). Our
proof is inspired by recent advances in regret analysis of gambling algorithms,
and its use to derive concentration inequalities. Our result is
first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are
not known to be strictly better than KL. Thus, we believe our work marks the
first step towards identifying optimal rates of PAC-Bayes bounds.</div><div><a href='http://arxiv.org/abs/2402.09201v1'>2402.09201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08961v3")'>Cascading Reinforcement Learning</div>
<div id='2401.08961v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T04:20:26Z</div><div>Authors: Yihan Du, R. Srikant, Wei Chen</div><div style='padding-top: 10px; width: 80ex'>Cascading bandits have gained popularity in recent years due to their
applicability to recommendation systems and online advertising. In the
cascading bandit model, at each timestep, an agent recommends an ordered subset
of items (called an item list) from a pool of items, each associated with an
unknown attraction probability. Then, the user examines the list, and clicks
the first attractive item (if any), and after that, the agent receives a
reward. The goal of the agent is to maximize the expected cumulative reward.
However, the prior literature on cascading bandits ignores the influences of
user states (e.g., historical behaviors) on recommendations and the change of
states as the session proceeds. Motivated by this fact, we propose a
generalized cascading RL framework, which considers the impact of user states
and state transition into decisions. In cascading RL, we need to select items
not only with large attraction probabilities but also leading to good successor
states. This imposes a huge computational challenge due to the combinatorial
action space. To tackle this challenge, we delve into the properties of value
functions, and design an oracle BestPerm to efficiently find the optimal item
list. Equipped with BestPerm, we develop two algorithms CascadingVI and
CascadingBPI, which are both computationally-efficient and sample-efficient,
and provide near-optimal regret and sample complexity guarantees. Furthermore,
we present experiments to show the improved computational and sample
efficiencies of our algorithms compared to straightforward adaptations of
existing RL algorithms in practice.</div><div><a href='http://arxiv.org/abs/2401.08961v3'>2401.08961v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14013v1")'>Misalignment, Learning, and Ranking: Harnessing Users Limited Attention</div>
<div id='2402.14013v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:52:20Z</div><div>Authors: Arpit Agarwal, Rad Niazadeh, Prathamesh Patil</div><div style='padding-top: 10px; width: 80ex'>In digital health and EdTech, recommendation systems face a significant
challenge: users often choose impulsively, in ways that conflict with the
platform's long-term payoffs. This misalignment makes it difficult to
effectively learn to rank items, as it may hinder exploration of items with
greater long-term payoffs. Our paper tackles this issue by utilizing users'
limited attention spans. We propose a model where a platform presents items
with unknown payoffs to the platform in a ranked list to $T$ users over time.
Each user selects an item by first considering a prefix window of these ranked
items and then picking the highest preferred item in that window (and the
platform observes its payoff for this item). We study the design of online
bandit algorithms that obtain vanishing regret against hindsight optimal
benchmarks.
  We first consider adversarial window sizes and stochastic iid payoffs. We
design an active-elimination-based algorithm that achieves an optimal
instance-dependent regret bound of $O(\log(T))$, by showing matching regret
upper and lower bounds. The key idea is using the combinatorial structure of
the problem to either obtain a large payoff from each item or to explore by
getting a sample from that item. This method systematically narrows down the
item choices to enhance learning efficiency and payoff.
  Second, we consider adversarial payoffs and stochastic iid window sizes. We
start from the full-information problem of finding the permutation that
maximizes the expected payoff. By a novel combinatorial argument, we
characterize the polytope of admissible item selection probabilities by a
permutation and show it has a polynomial-size representation. Using this
representation, we show how standard algorithms for adversarial online linear
optimization in the space of admissible probabilities can be used to obtain a
polynomial-time algorithm with $O(\sqrt{T})$ regret.</div><div><a href='http://arxiv.org/abs/2402.14013v1'>2402.14013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05304v2")'>Can Probabilistic Feedback Drive User Impacts in Online Platforms?</div>
<div id='2401.05304v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T18:12:31Z</div><div>Authors: Jessica Dai, Bailey Flanigan, Nika Haghtalab, Meena Jagadeesan, Chara Podimata</div><div style='padding-top: 10px; width: 80ex'>A common explanation for negative user impacts of content recommender systems
is misalignment between the platform's objective and user welfare. In this
work, we show that misalignment in the platform's objective is not the only
potential cause of unintended impacts on users: even when the platform's
objective is fully aligned with user welfare, the platform's learning algorithm
can induce negative downstream impacts on users. The source of these user
impacts is that different pieces of content may generate observable user
reactions (feedback information) at different rates; these feedback rates may
correlate with content properties, such as controversiality or demographic
similarity of the creator, that affect the user experience. Since differences
in feedback rates can impact how often the learning algorithm engages with
different content, the learning algorithm may inadvertently promote content
with certain such properties. Using the multi-armed bandit framework with
probabilistic feedback, we examine the relationship between feedback rates and
a learning algorithm's engagement with individual arms for different no-regret
algorithms. We prove that no-regret algorithms can exhibit a wide range of
dependencies: if the feedback rate of an arm increases, some no-regret
algorithms engage with the arm more, some no-regret algorithms engage with the
arm less, and other no-regret algorithms engage with the arm approximately the
same number of times. From a platform design perspective, our results highlight
the importance of looking beyond regret when measuring an algorithm's
performance, and assessing the nature of a learning algorithm's engagement with
different types of content as well as their resulting downstream impacts.</div><div><a href='http://arxiv.org/abs/2401.05304v2'>2401.05304v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18917v1")'>Stop Relying on No-Choice and Do not Repeat the Moves: Optimal,
  Efficient and Practical Algorithms for Assortment Optimization</div>
<div id='2402.18917v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T07:17:04Z</div><div>Authors: Aadirupa Saha, Pierre Gaillard</div><div style='padding-top: 10px; width: 80ex'>We address the problem of active online assortment optimization problem with
preference feedback, which is a framework for modeling user choices and
subsetwise utility maximization. The framework is useful in various real-world
applications including ad placement, online retail, recommender systems,
fine-tuning language models, amongst many. The problem, although has been
studied in the past, lacks an intuitive and practical solution approach with
simultaneously efficient algorithm and optimal regret guarantee. E.g.,
popularly used assortment selection algorithms often require the presence of a
`strong reference' which is always included in the choice sets, further they
are also designed to offer the same assortments repeatedly until the reference
item gets selected -- all such requirements are quite unrealistic for practical
applications. In this paper, we designed efficient algorithms for the problem
of regret minimization in assortment selection with \emph{Plackett Luce} (PL)
based user choices. We designed a novel concentration guarantee for estimating
the score parameters of the PL model using `\emph{Pairwise Rank-Breaking}',
which builds the foundation of our proposed algorithms. Moreover, our methods
are practical, provably optimal, and devoid of the aforementioned limitations
of the existing methods. Empirical evaluations corroborate our findings and
outperform the existing baselines.</div><div><a href='http://arxiv.org/abs/2402.18917v1'>2402.18917v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16464v1")'>Towards Regret Free Slot Allocation in Billboard Advertisement</div>
<div id='2401.16464v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T16:10:05Z</div><div>Authors: Dildar Ali, Suman Banerjee, Yamuna Prasad</div><div style='padding-top: 10px; width: 80ex'>Creating and maximizing influence among the customers is one of the central
goals of an advertiser, and hence, remains an active area of research in recent
times. In this advertisement technique, the advertisers approach an influence
provider for a specific number of views of their content on a payment basis.
Now, if the influence provider can provide the required number of views or
more, he will receive the full, else a partial payment. In the context of an
influence provider, it is a loss for him if he offers more or less views. This
is formalized as 'Regret', and naturally, in the context of the influence
provider, the goal will be to minimize this quantity. In this paper, we solve
this problem in the context of billboard advertisement and pose it as a
discrete optimization problem. We propose four efficient solution approaches
for this problem and analyze them to understand their time and space
complexity. We implement all the solution methodologies with real-life datasets
and compare the obtained results with the existing solution approaches from the
literature. We observe that the proposed solutions lead to less regret while
taking less computational time.</div><div><a href='http://arxiv.org/abs/2401.16464v1'>2401.16464v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00817v1")'>Doubly Calibrated Estimator for Recommendation on Data Missing Not At
  Random</div>
<div id='2403.00817v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T05:08:52Z</div><div>Authors: Wonbin Kweon, Hwanjo Yu</div><div style='padding-top: 10px; width: 80ex'>Recommender systems often suffer from selection bias as users tend to rate
their preferred items. The datasets collected under such conditions exhibit
entries missing not at random and thus are not randomized-controlled trials
representing the target population. To address this challenge, a doubly robust
estimator and its enhanced variants have been proposed as they ensure
unbiasedness when accurate imputed errors or predicted propensities are
provided. However, we argue that existing estimators rely on miscalibrated
imputed errors and propensity scores as they depend on rudimentary models for
estimation. We provide theoretical insights into how miscalibrated imputation
and propensity models may limit the effectiveness of doubly robust estimators
and validate our theorems using real-world datasets. On this basis, we propose
a Doubly Calibrated Estimator that involves the calibration of both the
imputation and propensity models. To achieve this, we introduce calibration
experts that consider different logit distributions across users. Moreover, we
devise a tri-level joint learning framework, allowing the simultaneous
optimization of calibration experts alongside prediction and imputation models.
Through extensive experiments on real-world datasets, we demonstrate the
superiority of the Doubly Calibrated Estimator in the context of debiased
recommendation tasks.</div><div><a href='http://arxiv.org/abs/2403.00817v1'>2403.00817v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15819v1")'>Debiased Model-based Interactive Recommendation</div>
<div id='2402.15819v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T14:10:04Z</div><div>Authors: Zijian Li, Ruichu Cai, Haiqin Huang, Sili Zhang, Yuguang Yan, Zhifeng Hao, Zhenghua Dong</div><div style='padding-top: 10px; width: 80ex'>Existing model-based interactive recommendation systems are trained by
querying a world model to capture the user preference, but learning the world
model from historical logged data will easily suffer from bias issues such as
popularity bias and sampling bias. This is why some debiased methods have been
proposed recently. However, two essential drawbacks still remain: 1) ignoring
the dynamics of the time-varying popularity results in a false reweighting of
items. 2) taking the unknown samples as negative samples in negative sampling
results in the sampling bias. To overcome these two drawbacks, we develop a
model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based
\textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In
iDMIR, for the first drawback, we devise a debiased causal world model based on
the causal mechanism of the time-varying recommendation generation process with
identification guarantees; for the second drawback, we devise a debiased
contrastive policy, which coincides with the debiased contrastive learning and
avoids sampling bias. Moreover, we demonstrate that the proposed method not
only outperforms several latest interactive recommendation algorithms but also
enjoys diverse recommendation performance.</div><div><a href='http://arxiv.org/abs/2402.15819v1'>2402.15819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16792v1")'>Rate-Optimal Rank Aggregation with Private Pairwise Rankings</div>
<div id='2402.16792v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:05:55Z</div><div>Authors: Shirong Xu, Will Wei Sun, Guang Cheng</div><div style='padding-top: 10px; width: 80ex'>In various real-world scenarios like recommender systems and political
surveys, pairwise rankings are commonly collected and utilized for rank
aggregation to obtain an overall ranking of items. However, preference rankings
can reveal individuals' personal preferences, underscoring the need to protect
them before releasing for downstream analysis. In this paper, we address the
challenge of preserving privacy while ensuring the utility of rank aggregation
based on pairwise rankings generated from the Bradley-Terry-Luce (BTL) model.
Using the randomized response mechanism to perturb raw pairwise rankings is a
common privacy protection strategy used in practice, but a critical challenge
arises because the privatized rankings no longer adhere to the BTL model,
resulting in significant bias in downstream rank aggregation tasks. Motivated
from this, we propose a debiased randomized response mechanism to protect the
raw pairwise rankings, ensuring consistent estimation of true preferences and
rankings in downstream rank aggregation. Theoretically, we offer insights into
the relationship between overall privacy guarantees and estimation errors from
private ranking data, and establish minimax rates for estimation errors. This
enables the determination of optimal privacy guarantees that balance
consistency in rank aggregation with robust privacy protection. We also
investigate convergence rates of expected ranking errors for partial and full
ranking recovery, quantifying how privacy protection influences the
specification of top-$K$ item sets and complete rankings. Our findings are
validated through extensive simulations and a real application.</div><div><a href='http://arxiv.org/abs/2402.16792v1'>2402.16792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12069v1")'>Fairness Evaluation for Uplift Modeling in the Absence of Ground Truth</div>
<div id='2403.12069v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T06:13:24Z</div><div>Authors: Serdar Kadioglu, Filip Michalsky</div><div style='padding-top: 10px; width: 80ex'>The acceleration in the adoption of AI-based automated decision-making
systems poses a challenge for evaluating the fairness of algorithmic decisions,
especially in the absence of ground truth. When designing interventions, uplift
modeling is used extensively to identify candidates that are likely to benefit
from treatment. However, these models remain particularly susceptible to
fairness evaluation due to the lack of ground truth on the outcome measure
since a candidate cannot be in both treatment and control simultaneously. In
this article, we propose a framework that overcomes the missing ground truth
problem by generating surrogates to serve as a proxy for counterfactual labels
of uplift modeling campaigns. We then leverage the surrogate ground truth to
conduct a more comprehensive binary fairness evaluation. We show how to apply
the approach in a comprehensive study from a real-world marketing campaign for
promotional offers and demonstrate its enhancement for fairness evaluation.</div><div><a href='http://arxiv.org/abs/2403.12069v1'>2403.12069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01361v1")'>Bandit Profit-maximization for Targeted Marketing</div>
<div id='2403.01361v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T01:33:47Z</div><div>Authors: Joon Suk Huh, Ellen Vitercik, Kirthevasan Kandasamy</div><div style='padding-top: 10px; width: 80ex'>We study a sequential profit-maximization problem, optimizing for both price
and ancillary variables like marketing expenditures. Specifically, we aim to
maximize profit over an arbitrary sequence of multiple demand curves, each
dependent on a distinct ancillary variable, but sharing the same price. A
prototypical example is targeted marketing, where a firm (seller) wishes to
sell a product over multiple markets. The firm may invest different marketing
expenditures for different markets to optimize customer acquisition, but must
maintain the same price across all markets. Moreover, markets may have
heterogeneous demand curves, each responding to prices and marketing
expenditures differently. The firm's objective is to maximize its gross profit,
the total revenue minus marketing costs.
  Our results are near-optimal algorithms for this class of problems in an
adversarial bandit setting, where demand curves are arbitrary non-adaptive
sequences, and the firm observes only noisy evaluations of chosen points on the
demand curves. We prove a regret upper bound of
$\widetilde{\mathcal{O}}\big(nT^{3/4}\big)$ and a lower bound of
$\Omega\big((nT)^{3/4}\big)$ for monotonic demand curves, and a regret bound of
$\widetilde{\Theta}\big(nT^{2/3}\big)$ for demands curves that are monotonic in
price and concave in the ancillary variables.</div><div><a href='http://arxiv.org/abs/2403.01361v1'>2403.01361v1</a></div>
</div></div>
    <div><a href="arxiv_12.html">Prev (12)</a></div>
    <div><a href="arxiv_14.html">Next (14)</a></div>
    