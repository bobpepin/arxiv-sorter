
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00329v1")'>Learning with Logical Constraints but without Shortcut Satisfaction</div>
<div id='2403.00329v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T07:17:20Z</div><div>Authors: Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, Jian LÃ¼</div><div style='padding-top: 10px; width: 80ex'>Recent studies in neuro-symbolic learning have explored the integration of
logical knowledge into deep learning via encoding logical constraints as an
additional loss function. However, existing approaches tend to vacuously
satisfy logical constraints through shortcuts, failing to fully exploit the
knowledge. In this paper, we present a new framework for learning with logical
constraints. Specifically, we address the shortcut satisfaction issue by
introducing dual variables for logical connectives, encoding how the constraint
is satisfied. We further propose a variational framework where the encoded
logical constraint is expressed as a distributional loss that is compatible
with the model's original training loss. The theoretical analysis shows that
the proposed approach bears salient properties, and the experimental
evaluations demonstrate its superior performance in both model generalizability
and constraint satisfaction.</div><div><a href='http://arxiv.org/abs/2403.00329v1'>2403.00329v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17431v1")'>The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning
  with Kandinsky Patterns</div>
<div id='2402.17431v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T11:43:41Z</div><div>Authors: Luca Salvatore Lorello, Marco Lippi, Stefano Melacci</div><div style='padding-top: 10px; width: 80ex'>Artificial intelligence is continuously seeking novel challenges and
benchmarks to effectively measure performance and to advance the
state-of-the-art. In this paper we introduce KANDY, a benchmarking framework
that can be used to generate a variety of learning and reasoning tasks inspired
by Kandinsky patterns. By creating curricula of binary classification tasks
with increasing complexity and with sparse supervisions, KANDY can be used to
implement benchmarks for continual and semi-supervised learning, with a
specific focus on symbol compositionality. Classification rules are also
provided in the ground truth to enable analysis of interpretable solutions.
Together with the benchmark generation pipeline, we release two curricula, an
easier and a harder one, that we propose as new challenges for the research
community. With a thorough experimental evaluation, we show how both
state-of-the-art neural models and purely symbolic approaches struggle with
solving most of the tasks, thus calling for the application of advanced
neuro-symbolic methods trained over time.</div><div><a href='http://arxiv.org/abs/2402.17431v1'>2402.17431v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01889v1")'>The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning</div>
<div id='2402.01889v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:33:14Z</div><div>Authors: Daniel Cunnington, Mark Law, Jorge Lobo, Alessandra Russo</div><div style='padding-top: 10px; width: 80ex'>Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI
systems, as interpretable symbolic techniques provide formal behaviour
guarantees. The challenge is how to effectively integrate neural and symbolic
computation, to enable learning and reasoning from raw data. Existing pipelines
that train the neural and symbolic components sequentially require extensive
labelling, whereas end-to-end approaches are limited in terms of scalability,
due to the combinatorial explosion in the symbol grounding problem. In this
paper, we leverage the implicit knowledge within foundation models to enhance
the performance in NeSy tasks, whilst reducing the amount of data labelling and
manual engineering. We introduce a new architecture, called NeSyGPT, which
fine-tunes a vision-language foundation model to extract symbolic features from
raw data, before learning a highly expressive answer set program to solve a
downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has
superior accuracy over various baselines, and can scale to complex NeSy tasks.
Finally, we highlight the effective use of a large language model to generate
the programmatic interface between the neural and symbolic components,
significantly reducing the amount of manual engineering required.</div><div><a href='http://arxiv.org/abs/2402.01889v1'>2402.01889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12240v1")'>BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts</div>
<div id='2402.12240v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:54:36Z</div><div>Authors: Emanuele Marconato, Samuele Bortolotti, Emile van Krieken, Antonio Vergari, Andrea Passerini, Stefano Teso</div><div style='padding-top: 10px; width: 80ex'>Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge -
encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts
(RSs): They learn concepts consistent with the symbolic knowledge by exploiting
unintended semantics. RSs compromise reliability and generalization and, as we
show in this paper, they are linked to NeSy models being overconfident about
the predicted concepts. Unfortunately, the only trustworthy mitigation strategy
requires collecting costly dense supervision over the concepts. Rather than
attempting to avoid RSs altogether, we propose to ensure NeSy models are aware
of the semantic ambiguity of the concepts they learn, thus enabling their users
to identify and distrust low-quality concepts. Starting from three simple
desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling
technique that calibrates the model's concept-level confidence without
compromising prediction accuracy, thus encouraging NeSy architectures to be
uncertain about concepts affected by RSs. We show empirically that bears
improves RS-awareness of several state-of-the-art NeSy models, and also
facilitates acquiring informative dense annotations for mitigation purposes.</div><div><a href='http://arxiv.org/abs/2402.12240v1'>2402.12240v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03507v1")'>Neural networks for abstraction and reasoning: Towards broad
  generalization in machines</div>
<div id='2402.03507v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T20:48:57Z</div><div>Authors: Mikel Bober-Irizar, Soumya Banerjee</div><div style='padding-top: 10px; width: 80ex'>For half a century, artificial intelligence research has attempted to
reproduce the human qualities of abstraction and reasoning - creating computer
systems that can learn new concepts from a minimal set of examples, in settings
where humans find this easy. While specific neural networks are able to solve
an impressive range of problems, broad generalisation to situations outside
their training data has proved elusive.In this work, we look at several novel
approaches for solving the Abstraction &amp; Reasoning Corpus (ARC), a dataset of
abstract visual reasoning tasks introduced to test algorithms on broad
generalization. Despite three international competitions with $100,000 in
prizes, the best algorithms still fail to solve a majority of ARC tasks and
rely on complex hand-crafted rules, without using machine learning at all. We
revisit whether recent advances in neural networks allow progress on this task.
  First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.
DreamCoder automatically writes programs in a bespoke domain-specific language
to perform reasoning, using a neural network to mimic human intuition. We
present the Perceptual Abstraction and Reasoning Language (PeARL) language,
which allows DreamCoder to solve ARC tasks, and propose a new recognition model
that allows us to significantly improve on the previous best implementation.We
also propose a new encoding and augmentation scheme that allows large language
models (LLMs) to solve ARC tasks, and find that the largest models can solve
some ARC tasks. LLMs are able to solve a different group of problems to
state-of-the-art solvers, and provide an interesting way to complement other
approaches. We perform an ensemble analysis, combining models to achieve better
results than any system alone. Finally, we publish the arckit Python library to
make future research on ARC easier.</div><div><a href='http://arxiv.org/abs/2402.03507v1'>2402.03507v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16024v1")'>Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules
  in Vector-symbolic Architectures</div>
<div id='2401.16024v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T10:17:18Z</div><div>Authors: Michael Hersche, Francesco di Stefano, Thomas Hofmann, Abu Sebastian, Abbas Rahimi</div><div style='padding-top: 10px; width: 80ex'>Abstract reasoning is a cornerstone of human intelligence, and replicating it
with artificial intelligence (AI) presents an ongoing challenge. This study
focuses on efficiently solving Raven's progressive matrices (RPM), a visual
test for assessing abstract reasoning abilities, by using distributed
computation and operators provided by vector-symbolic architectures (VSA).
Instead of hard-coding the rule formulations associated with RPMs, our approach
can learn the VSA rule formulations (hence the name Learn-VRF) with just one
pass through the training data. Yet, our approach, with compact parameters,
remains transparent and interpretable. Learn-VRF yields accurate predictions on
I-RAVEN's in-distribution data, and exhibits strong out-of-distribution
capabilities concerning unseen attribute-rule pairs, significantly
outperforming pure connectionist baselines including large language models. Our
code is available at
https://github.com/IBM/learn-vector-symbolic-architectures-rule-formulations.</div><div><a href='http://arxiv.org/abs/2401.16024v1'>2401.16024v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00352v1")'>Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity
  for Abstract Visual Reasoning</div>
<div id='2403.00352v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T08:31:58Z</div><div>Authors: Ruiqian Nai, Zixin Wen, Ji Li, Yuanzhi Li, Yang Gao</div><div style='padding-top: 10px; width: 80ex'>In representation learning, a disentangled representation is highly desirable
as it encodes generative factors of data in a separable and compact pattern.
Researchers have advocated leveraging disentangled representations to complete
downstream tasks with encouraging empirical evidence. This paper further
investigates the necessity of disentangled representation in downstream
applications. Specifically, we show that dimension-wise disentangled
representations are unnecessary on a fundamental downstream task, abstract
visual reasoning. We provide extensive empirical evidence against the necessity
of disentanglement, covering multiple datasets, representation learning
methods, and downstream network architectures. Furthermore, our findings
suggest that the informativeness of representations is a better indicator of
downstream performance than disentanglement. Finally, the positive correlation
between informativeness and disentanglement explains the claimed usefulness of
disentangled representations in previous works. The source code is available at
https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git.</div><div><a href='http://arxiv.org/abs/2403.00352v1'>2403.00352v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15393v1")'>NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks</div>
<div id='2402.15393v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:51:45Z</div><div>Authors: Bernardo Esteves, Miguel Vasco, Francisco S. Melo</div><div style='padding-top: 10px; width: 80ex'>While machine learning methods excel at pattern recognition, they struggle
with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep
Thinking methods show promise in learning algorithms that extrapolate: learning
in smaller environments and executing the learned algorithm in larger
environments. However, these works are limited to symmetrical tasks, where the
input and output dimensionalities are the same. To address this gap, we propose
NeuralThink, a new recurrent architecture that can consistently extrapolate to
both symmetrical and asymmetrical tasks, where the dimensionality of the input
and output are different. We contribute with a novel benchmark of asymmetrical
tasks for extrapolation. We show that NeuralThink consistently outperforms the
prior state-of-the-art Deep Thinking architectures, in regards to stable
extrapolation to large observations from smaller training sizes.</div><div><a href='http://arxiv.org/abs/2402.15393v1'>2402.15393v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04929v1")'>On the Markov Property of Neural Algorithmic Reasoning: Analyses and
  Methods</div>
<div id='2403.04929v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T22:35:22Z</div><div>Authors: Montgomery Bohde, Meng Liu, Alexandra Saxton, Shuiwang Ji</div><div style='padding-top: 10px; width: 80ex'>Neural algorithmic reasoning is an emerging research direction that endows
neural networks with the ability to mimic algorithmic executions step-by-step.
A common paradigm in existing designs involves the use of historical embeddings
in predicting the results of future execution steps. Our observation in this
work is that such historical dependence intrinsically contradicts the Markov
nature of algorithmic reasoning tasks. Based on this motivation, we present our
ForgetNet, which does not use historical embeddings and thus is consistent with
the Markov nature of the tasks. To address challenges in training ForgetNet at
early stages, we further introduce G-ForgetNet, which uses a gating mechanism
to allow for the selective integration of historical embeddings. Such an
enhanced capability provides valuable computational pathways during the model's
early training phase. Our extensive experiments, based on the CLRS-30
algorithmic reasoning benchmark, demonstrate that both ForgetNet and
G-ForgetNet achieve better generalization capability than existing methods.
Furthermore, we investigate the behavior of the gating mechanism, highlighting
its degree of alignment with our intuitions and its effectiveness for robust
performance.</div><div><a href='http://arxiv.org/abs/2403.04929v1'>2403.04929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11628v1")'>Discrete Neural Algorithmic Reasoning</div>
<div id='2402.11628v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T16:03:04Z</div><div>Authors: Gleb Rodionov, Liudmila Prokhorenkova</div><div style='padding-top: 10px; width: 80ex'>Neural algorithmic reasoning aims to capture computations with neural
networks via learning the models to imitate the execution of classical
algorithms. While common architectures are expressive enough to contain the
correct model in the weights space, current neural reasoners are struggling to
generalize well on out-of-distribution data. On the other hand, classical
computations are not affected by distribution shifts as they can be described
as transitions between discrete computational states. In this work, we propose
to force neural reasoners to maintain the execution trajectory as a combination
of finite predefined states. Trained with supervision on the algorithm's state
transitions, such models are able to perfectly align with the original
algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark,
where we get perfect test scores for all tasks. Moreover, the proposed
architectural choice allows us to prove the correctness of the learned
algorithms for any test data.</div><div><a href='http://arxiv.org/abs/2402.11628v1'>2402.11628v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14412v1")'>Harnessing Neuron Stability to Improve DNN Verification</div>
<div id='2401.14412v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T23:48:04Z</div><div>Authors: Hai Duong, Dong Xu, ThanhVu Nguyen, Matthew B. Dwyer</div><div style='padding-top: 10px; width: 80ex'>Deep Neural Networks (DNN) have emerged as an effective approach to tackling
real-world problems. However, like human-written software, DNNs are susceptible
to bugs and attacks. This has generated significant interests in developing
effective and scalable DNN verification techniques and tools. In this paper, we
present VeriStable, a novel extension of recently proposed DPLL-based
constraint DNN verification approach. VeriStable leverages the insight that
while neuron behavior may be non-linear across the entire DNN input space, at
intermediate states computed during verification many neurons may be
constrained to have linear behavior - these neurons are stable. Efficiently
detecting stable neurons reduces combinatorial complexity without compromising
the precision of abstractions. Moreover, the structure of clauses arising in
DNN verification problems shares important characteristics with industrial SAT
benchmarks. We adapt and incorporate multi-threading and restart optimizations
targeting those characteristics to further optimize DPLL-based DNN
verification. We evaluate the effectiveness of VeriStable across a range of
challenging benchmarks including fully-connected feedforward networks (FNNs),
convolutional neural networks (CNNs) and residual networks (ResNets) applied to
the standard MNIST and CIFAR datasets. Preliminary results show that VeriStable
is competitive and outperforms state-of-the-art DNN verification tools,
including $\alpha$-$\beta$-CROWN and MN-BaB, the first and second performers of
the VNN-COMP, respectively.</div><div><a href='http://arxiv.org/abs/2401.14412v1'>2401.14412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07956v1")'>DeepCDCL: An CDCL-based Neural Network Verification Framework</div>
<div id='2403.07956v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T08:07:06Z</div><div>Authors: Zongxin Liu, Pengfei Yang, Lijun Zhang, Xiaowei Huang</div><div style='padding-top: 10px; width: 80ex'>Neural networks in safety-critical applications face increasing safety and
security concerns due to their susceptibility to little disturbance. In this
paper, we propose DeepCDCL, a novel neural network verification framework based
on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an
asynchronous clause learning and management structure, reducing redundant time
consumption compared to the direct application of the CDCL framework.
Furthermore, we also provide a detailed evaluation of the performance of our
approach on the ACAS Xu and MNIST datasets, showing that a significant speed-up
is achieved in most cases.</div><div><a href='http://arxiv.org/abs/2403.07956v1'>2403.07956v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09494v1")'>VeriBug: An Attention-based Framework for Bug-Localization in Hardware
  Designs</div>
<div id='2401.09494v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T01:33:37Z</div><div>Authors: Giuseppe Stracquadanio, Sourav Medya, Stefano Quer, Debjit Pal</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been an exponential growth in the size and
complexity of System-on-Chip designs targeting different specialized
applications. The cost of an undetected bug in these systems is much higher
than in traditional processor systems as it may imply the loss of property or
life. The problem is further exacerbated by the ever-shrinking time-to-market
and ever-increasing demand to churn out billions of devices. Despite decades of
research in simulation and formal methods for debugging and verification, it is
still one of the most time-consuming and resource intensive processes in
contemporary hardware design cycle. In this work, we propose VeriBug, which
leverages recent advances in deep learning to accelerate debugging at the
Register-Transfer Level and generates explanations of likely root causes.
First, VeriBug uses control-data flow graph of a hardware design and learns to
execute design statements by analyzing the context of operands and their
assignments. Then, it assigns an importance score to each operand in a design
statement and uses that score for generating explanations for failures.
Finally, VeriBug produces a heatmap highlighting potential buggy source code
portions. Our experiments show that VeriBug can achieve an average bug
localization coverage of 82.5% on open-source designs and different types of
injected bugs.</div><div><a href='http://arxiv.org/abs/2401.09494v1'>2401.09494v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07465v1")'>One for All and All for One: GNN-based Control-Flow Attestation for
  Embedded Devices</div>
<div id='2403.07465v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:00:06Z</div><div>Authors: Marco Chilese, Richard Mitev, Meni Orenbach, Robert Thorburn, Ahmad Atamli, Ahmad-Reza Sadeghi</div><div style='padding-top: 10px; width: 80ex'>Control-Flow Attestation (CFA) is a security service that allows an entity
(verifier) to verify the integrity of code execution on a remote computer
system (prover). Existing CFA schemes suffer from impractical assumptions, such
as requiring access to the prover's internal state (e.g., memory or code), the
complete Control-Flow Graph (CFG) of the prover's software, large sets of
measurements, or tailor-made hardware. Moreover, current CFA schemes are
inadequate for attesting embedded systems due to their high computational
overhead and resource usage.
  In this paper, we overcome the limitations of existing CFA schemes for
embedded devices by introducing RAGE, a novel, lightweight CFA approach with
minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including
control- and non-control-data attacks. It efficiently extracts features from
one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to
identify deviations from benign executions. The core intuition behind RAGE is
to exploit the correspondence between execution trace, execution graph, and
execution embeddings to eliminate the unrealistic requirement of having access
to a complete CFG.
  We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects
40 real-world attacks on embedded software; (ii) Further, we stress our scheme
with synthetic return-oriented programming (ROP) and data-oriented programming
(DOP) attacks on the real-world embedded software benchmark Embench, achieving
98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive
Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by
millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP
attack detection, with an FPR of 5.47%.</div><div><a href='http://arxiv.org/abs/2403.07465v1'>2403.07465v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12875v1")'>Chain of Thought Empowers Transformers to Solve Inherently Serial
  Problems</div>
<div id='2402.12875v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T10:11:03Z</div><div>Authors: Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma</div><div style='padding-top: 10px; width: 80ex'>Instructing the model to generate a sequence of intermediate steps, a.k.a., a
chain of thought (CoT), is a highly effective method to improve the accuracy of
large language models (LLMs) on arithmetics and symbolic reasoning tasks.
However, the mechanism behind CoT remains unclear. This work provides a
theoretical understanding of the power of CoT for decoder-only transformers
through the lens of expressiveness. Conceptually, CoT empowers the model with
the ability to perform inherently serial computation, which is otherwise
lacking in transformers, especially when depth is low. Given input length $n$,
previous works have shown that constant-depth transformers with finite
precision $\mathsf{poly}(n)$ embedding size can only solve problems in
$\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper
bound for constant-depth transformers with constant-bit precision, which can
only solve problems in $\mathsf{AC}^0$, a proper subset of $ \mathsf{TC}^0$.
However, with $T$ steps of CoT, constant-depth transformers using constant-bit
precision and $O(\log n)$ embedding size can solve any problem solvable by
boolean circuits of size $T$. Empirically, enabling CoT dramatically improves
the accuracy for tasks that are hard for parallel computation, including the
composition of permutation groups, iterated squaring, and circuit value
problems, especially for low-depth transformers.</div><div><a href='http://arxiv.org/abs/2402.12875v1'>2402.12875v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08164v2")'>On Limitations of the Transformer Architecture</div>
<div id='2402.08164v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T01:52:15Z</div><div>Authors: Binghui Peng, Srini Narayanan, Christos Papadimitriou</div><div style='padding-top: 10px; width: 80ex'>What are the root causes of hallucinations in large language models (LLMs)?
We use Communication Complexity to prove that the Transformer layer is
incapable of composing functions (e.g., identify a grandparent of a person in a
genealogy) if the domains of the functions are large enough; we show through
examples that this inability is already empirically present when the domains
are quite small. We also point out that several mathematical tasks that are at
the core of the so-called compositional tasks thought to be hard for LLMs are
unlikely to be solvable by Transformers, for large enough instances and
assuming that certain well accepted conjectures in the field of Computational
Complexity are true.</div><div><a href='http://arxiv.org/abs/2402.08164v2'>2402.08164v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08280v1")'>Pix2Code: Learning to Compose Neural Visual Concepts as Programs</div>
<div id='2402.08280v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T08:14:10Z</div><div>Authors: Antonia WÃ¼st, Wolfgang Stammer, Quentin Delfosse, Devendra Singh Dhami, Kristian Kersting</div><div style='padding-top: 10px; width: 80ex'>The challenge in learning abstract concepts from images in an unsupervised
fashion lies in the required integration of visual perception and generalizable
relational reasoning. Moreover, the unsupervised nature of this task makes it
necessary for human users to be able to understand a model's learnt concepts
and potentially revise false behaviours. To tackle both the generalizability
and interpretability constraints of visual concept learning, we propose
Pix2Code, a framework that extends program synthesis to visual relational
reasoning by utilizing the abilities of both explicit, compositional symbolic
and implicit neural representations. This is achieved by retrieving object
representations from images and synthesizing relational concepts as
lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the
challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its
ability to identify compositional visual concepts that generalize to novel data
and concept configurations. Particularly, in stark contrast to neural
approaches, we show that Pix2Code's representations remain human interpretable
and can be easily revised for improved performance.</div><div><a href='http://arxiv.org/abs/2402.08280v1'>2402.08280v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03458v1")'>Slot Abstractors: Toward Scalable Abstract Visual Reasoning</div>
<div id='2403.03458v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T04:49:02Z</div><div>Authors: Shanka Subhra Mondal, Jonathan D. Cohen, Taylor W. Webb</div><div style='padding-top: 10px; width: 80ex'>Abstract visual reasoning is a characteristically human ability, allowing the
identification of relational patterns that are abstracted away from object
features, and the systematic generalization of those patterns to unseen
problems. Recent work has demonstrated strong systematic generalization in
visual reasoning tasks involving multi-object inputs, through the integration
of slot-based methods used for extracting object-centric representations
coupled with strong inductive biases for relational abstraction. However, this
approach was limited to problems containing a single rule, and was not scalable
to visual reasoning problems containing a large number of objects. Other recent
work proposed Abstractors, an extension of Transformers that incorporates
strong relational inductive biases, thereby inheriting the Transformer's
scalability and multi-head architecture, but it has yet to be demonstrated how
this approach might be applied to multi-object visual inputs. Here we combine
the strengths of the above approaches and propose Slot Abstractors, an approach
to abstract visual reasoning that can be scaled to problems involving a large
number of objects and multiple relations among them. The approach displays
state-of-the-art performance across four abstract visual reasoning tasks.</div><div><a href='http://arxiv.org/abs/2403.03458v1'>2403.03458v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11743v1")'>PARMESAN: Parameter-Free Memory Search and Transduction for Dense
  Prediction Tasks</div>
<div id='2403.11743v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T12:55:40Z</div><div>Authors: Philip Matthias Winter, Maria Wimmer, David Major, Dimitrios Lenis, Astrid Berg, Theresa Neubauer, Gaia Romana De Paolis, Johannes Novotny, Sophia Ulonska, Katja BÃ¼hler</div><div style='padding-top: 10px; width: 80ex'>In this work we address flexibility in deep learning by means of transductive
reasoning. For adaptation to new tasks or new data, existing methods typically
involve tuning of learnable parameters or even complete re-training from
scratch, rendering such approaches unflexible in practice. We argue that the
notion of separating computation from memory by the means of transduction can
act as a stepping stone for solving these issues. We therefore propose PARMESAN
(parameter-free memory search and transduction), a scalable transduction method
which leverages a memory module for solving dense prediction tasks. At
inference, hidden representations in memory are being searched to find
corresponding examples. In contrast to other methods, PARMESAN learns without
the requirement for any continuous training or fine-tuning of learnable
parameters simply by modifying the memory content. Our method is compatible
with commonly used neural architectures and canonically transfers to 1D, 2D,
and 3D grid-based data. We demonstrate the capabilities of our approach at
complex tasks such as continual and few-shot learning. PARMESAN learns up to
370 times faster than common baselines while being on par in terms of
predictive performance, knowledge retention, and data-efficiency.</div><div><a href='http://arxiv.org/abs/2403.11743v1'>2403.11743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05461v1")'>The two-way knowledge interaction interface between humans and neural
  networks</div>
<div id='2401.05461v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T10:47:41Z</div><div>Authors: Zhanliang He, Nuoye Xiong, Hongsheng Li, Peiyi Shen, Guangming Zhu, Liang Zhang</div><div style='padding-top: 10px; width: 80ex'>Despite neural networks (NN) have been widely applied in various fields and
generally outperforms humans, they still lack interpretability to a certain
extent, and humans are unable to intuitively understand the decision logic of
NN. This also hinders the knowledge interaction between humans and NN,
preventing humans from getting involved to give direct guidance when NN's
decisions go wrong. While recent research in explainable AI has achieved
interpretability of NN from various perspectives, it has not yet provided
effective methods for knowledge exchange between humans and NN. To address this
problem, we constructed a two-way interaction interface that uses structured
representations of visual concepts and their relationships as the "language"
for knowledge exchange between humans and NN. Specifically, NN provide
intuitive reasoning explanations to humans based on the class-specific
structural concepts graph (C-SCG). On the other hand, humans can modify the
biases present in the C-SCG through their prior knowledge and reasoning
ability, and thus provide direct knowledge guidance to NN through this
interface. Through experimental validation, based on this interaction
interface, NN can provide humans with easily understandable explanations of the
reasoning process. Furthermore, human involvement and prior knowledge can
directly and effectively contribute to enhancing the performance of NN.</div><div><a href='http://arxiv.org/abs/2401.05461v1'>2401.05461v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18285v1")'>PiShield: A NeSy Framework for Learning with Requirements</div>
<div id='2402.18285v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T12:24:27Z</div><div>Authors: Mihaela CÄtÄlina Stoian, Alex Tatomir, Thomas Lukasiewicz, Eleonora Giunchiglia</div><div style='padding-top: 10px; width: 80ex'>Deep learning models have shown their strengths in various application
domains, however, they often struggle to meet safety requirements for their
outputs. In this paper, we introduce PiShield, the first framework ever
allowing for the integration of the requirements into the neural networks'
topology. PiShield guarantees compliance with these requirements, regardless of
input. Additionally, it allows for integrating requirements both at inference
and/or training time, depending on the practitioners' needs. Given the
widespread application of deep learning, there is a growing need for frameworks
allowing for the integration of the requirements across various domains. Here,
we explore three application scenarios: functional genomics, autonomous
driving, and tabular data generation.</div><div><a href='http://arxiv.org/abs/2402.18285v1'>2402.18285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18510v2")'>RNNs are not Transformers (Yet): The Key Bottleneck on In-context
  Retrieval</div>
<div id='2402.18510v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:38:06Z</div><div>Authors: Kaiyue Wen, Xingyu Dang, Kaifeng Lyu</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the gap in representation powers of Recurrent Neural
Networks (RNNs) and Transformers in the context of solving algorithmic
problems. We focus on understanding whether RNNs, known for their memory
efficiency in handling long sequences, can match the performance of
Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.
Our theoretical analysis reveals that CoT improves RNNs but is insufficient to
close the gap with Transformers. A key bottleneck lies in the inability of RNNs
to perfectly retrieve information from the context, even with CoT: for several
tasks that explicitly or implicitly require this capability, such as
associative recall and determining if a graph is a tree, we prove that RNNs are
not expressive enough to solve the tasks while Transformers can solve them with
ease. Conversely, we prove that adopting techniques to enhance the in-context
retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)
and adding a single Transformer layer, can elevate RNNs to be capable of
solving all polynomial-time solvable problems with CoT, hence closing the
representation gap with Transformers.</div><div><a href='http://arxiv.org/abs/2402.18510v2'>2402.18510v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00323v1")'>Softened Symbol Grounding for Neuro-symbolic Systems</div>
<div id='2403.00323v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T06:57:09Z</div><div>Authors: Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxing Ma, Jian LÃ¼</div><div style='padding-top: 10px; width: 80ex'>Neuro-symbolic learning generally consists of two separated worlds, i.e.,
neural network training and symbolic constraint solving, whose success hinges
on symbol grounding, a fundamental problem in AI. This paper presents a novel,
softened symbol grounding process, bridging the gap between the two worlds, and
resulting in an effective and efficient neuro-symbolic learning framework.
Technically, the framework features (1) modeling of symbol solution states as a
Boltzmann distribution, which avoids expensive state searching and facilitates
mutually beneficial interactions between network training and symbolic
reasoning;(2) a new MCMC technique leveraging projection and SMT solvers, which
efficiently samples from disconnected symbol solution spaces; (3) an annealing
mechanism that can escape from %being trapped into sub-optimal symbol
groundings. Experiments with three representative neuro symbolic learning tasks
demonstrate that, owining to its superior symbol grounding capability, our
framework successfully solves problems well beyond the frontier of the existing
proposals.</div><div><a href='http://arxiv.org/abs/2403.00323v1'>2403.00323v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15103v1")'>PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for
  Symbolic Regression</div>
<div id='2401.15103v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:53:35Z</div><div>Authors: Min Wu, Weijun Li, Lina Yu, Wenqiang Li, Jingyi Liu, Yanjie Li, Meilan Hao</div><div style='padding-top: 10px; width: 80ex'>Symbolic regression aims to derive interpretable symbolic expressions from
data in order to better understand and interpret data. %which plays an
important role in knowledge discovery and interpretable machine learning.
  In this study, a symbolic network called PruneSymNet is proposed for symbolic
regression. This is a novel neural network whose activation function consists
of common elementary functions and operators. The whole network is
differentiable and can be trained by gradient descent method. Each subnetwork
in the network corresponds to an expression, and our goal is to extract such
subnetworks to get the desired symbolic expression.
  Therefore, a greedy pruning algorithm is proposed to prune the network into a
subnetwork while ensuring the accuracy of data fitting. The proposed greedy
pruning algorithm preserves the edge with the least loss in each pruning, but
greedy algorithm often can not get the optimal solution. In order to alleviate
this problem, we combine beam search during pruning to obtain multiple
candidate expressions each time, and finally select the expression with the
smallest loss as the final result. It was tested on the public data set and
compared with the current popular algorithms. The results showed that the
proposed algorithm had better accuracy.</div><div><a href='http://arxiv.org/abs/2401.15103v1'>2401.15103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14424v3")'>Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo
  Tree Search</div>
<div id='2401.14424v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T07:47:04Z</div><div>Authors: Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng</div><div style='padding-top: 10px; width: 80ex'>Finding a concise and interpretable mathematical formula that accurately
describes the relationship between each variable and the predicted value in the
data is a crucial task in scientific research, as well as a significant
challenge in artificial intelligence. This problem is referred to as symbolic
regression, which is an NP-hard problem. In the previous year, a novel symbolic
regression methodology utilizing Monte Carlo Tree Search (MCTS) was advanced,
achieving state-of-the-art results on a diverse range of datasets. although
this algorithm has shown considerable improvement in recovering target
expressions compared to previous methods, the lack of guidance during the MCTS
process severely hampers its search efficiency. Recently, some algorithms have
added a pre-trained policy network to guide the search of MCTS, but the
pre-trained policy network generalizes poorly. To optimize the trade-off
between efficiency and versatility, we introduce SR-GPT, a novel algorithm for
symbolic regression that integrates Monte Carlo Tree Search (MCTS) with a
Generative Pre-Trained Transformer (GPT). By using GPT to guide the MCTS, the
search efficiency of MCTS is significantly improved. Next, we utilize the MCTS
results to further refine the GPT, enhancing its capabilities and providing
more accurate guidance for the MCTS. MCTS and GPT are coupled together and
optimize each other until the target expression is successfully determined. We
conducted extensive evaluations of SR-GPT using 222 expressions sourced from
over 10 different symbolic regression datasets. The experimental results
demonstrate that SR-GPT outperforms existing state-of-the-art algorithms in
accurately recovering symbolic expressions both with and without added noise.</div><div><a href='http://arxiv.org/abs/2401.14424v3'>2401.14424v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14504v1")'>Soft Learning Probabilistic Circuits</div>
<div id='2403.14504v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T15:56:15Z</div><div>Authors: Soroush Ghandi, Benjamin Quost, Cassio de Campos</div><div style='padding-top: 10px; width: 80ex'>Probabilistic Circuits (PCs) are prominent tractable probabilistic models,
allowing for a range of exact inferences. This paper focuses on the main
algorithm for training PCs, LearnSPN, a gold standard due to its efficiency,
performance, and ease of use, in particular for tabular data. We show that
LearnSPN is a greedy likelihood maximizer under mild assumptions. While
inferences in PCs may use the entire circuit structure for processing queries,
LearnSPN applies a hard method for learning them, propagating at each sum node
a data point through one and only one of the children/edges as in a hard
clustering process. We propose a new learning procedure named SoftLearn, that
induces a PC using a soft clustering process. We investigate the effect of this
learning-inference compatibility in PCs. Our experiments show that SoftLearn
outperforms LearnSPN in many situations, yielding better likelihoods and
arguably better samples. We also analyze comparable tractable models to
highlight the differences between soft/hard learning and model querying.</div><div><a href='http://arxiv.org/abs/2403.14504v1'>2403.14504v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13343v1")'>Lessons on Datasets and Paradigms in Machine Learning for Symbolic
  Computation: A Case Study on CAD</div>
<div id='2401.13343v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T10:12:43Z</div><div>Authors: Tereso del RÃ­o, Matthew England</div><div style='padding-top: 10px; width: 80ex'>Symbolic Computation algorithms and their implementation in computer algebra
systems often contain choices which do not affect the correctness of the output
but can significantly impact the resources required: such choices can benefit
from having them made separately for each problem via a machine learning model.
This study reports lessons on such use of machine learning in symbolic
computation, in particular on the importance of analysing datasets prior to
machine learning and on the different machine learning paradigms that may be
utilised. We present results for a particular case study, the selection of
variable ordering for cylindrical algebraic decomposition, but expect that the
lessons learned are applicable to other decisions in symbolic computation.
  We utilise an existing dataset of examples derived from applications which
was found to be imbalanced with respect to the variable ordering decision. We
introduce an augmentation technique for polynomial systems problems that allows
us to balance and further augment the dataset, improving the machine learning
results by 28\% and 38\% on average, respectively. We then demonstrate how the
existing machine learning methodology used for the problem $-$ classification
$-$ might be recast into the regression paradigm. While this does not have a
radical change on the performance, it does widen the scope in which the
methodology can be applied to make choices.</div><div><a href='http://arxiv.org/abs/2401.13343v1'>2401.13343v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03363v2")'>Exploring Prime Number Classification: Achieving High Recall Rate and
  Rapid Convergence with Sparse Encoding</div>
<div id='2402.03363v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T16:44:52Z</div><div>Authors: Serin Lee, S. Kim</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel approach at the intersection of machine learning
and number theory, focusing on the classification of prime and non-prime
numbers. At the core of our research is the development of a highly sparse
encoding method, integrated with conventional neural network architectures.
This combination has shown promising results, achieving a recall of over 99\%
in identifying prime numbers and 79\% for non-prime numbers from an inherently
imbalanced sequential series of integers, while exhibiting rapid model
convergence before the completion of a single training epoch. We performed
training using $10^6$ integers starting from a specified integer and tested on
a different range of $2 \times 10^6$ integers extending from $10^6$ to $3
\times 10^6$, offset by the same starting integer. While constrained by the
memory capacity of our resources, which limited our analysis to a span of
$3\times10^6$, we believe that our study contribute to the application of
machine learning in prime number analysis. This work aims to demonstrate the
potential of such applications and hopes to inspire further exploration and
possibilities in diverse fields.</div><div><a href='http://arxiv.org/abs/2402.03363v2'>2402.03363v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16318v1")'>Defining and Extracting generalizable interaction primitives from DNNs</div>
<div id='2401.16318v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:21:41Z</div><div>Authors: Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang</div><div style='padding-top: 10px; width: 80ex'>Faithfully summarizing the knowledge encoded by a deep neural network (DNN)
into a few symbolic primitive patterns without losing much information
represents a core challenge in explainable AI. To this end, Ren et al. (2023c)
have derived a series of theorems to prove that the inference score of a DNN
can be explained as a small set of interactions between input variables.
However, the lack of generalization power makes it still hard to consider such
interactions as faithful primitive patterns encoded by the DNN. Therefore,
given different DNNs trained for the same task, we develop a new method to
extract interactions that are shared by these DNNs. Experiments show that the
extracted interactions can better reflect common knowledge shared by different
DNNs.</div><div><a href='http://arxiv.org/abs/2401.16318v1'>2401.16318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12201v1")'>Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic
  Interpretability: A Case Study on Othello-GPT</div>
<div id='2402.12201v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:04:53Z</div><div>Authors: Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu</div><div style='padding-top: 10px; width: 80ex'>Sparse dictionary learning has been a rapidly growing technique in
mechanistic interpretability to attack superposition and extract more
human-understandable features from model activations. We ask a further question
based on the extracted more monosemantic features: How do we recognize circuits
connecting the enormous amount of dictionary features? We propose a circuit
discovery framework alternative to activation patching. Our framework suffers
less from out-of-distribution and proves to be more efficient in terms of
asymptotic complexity. The basic unit in our framework is dictionary features
decomposed from all modules writing to the residual stream, including
embedding, attention output and MLP output. Starting from any logit, dictionary
feature or attention score, we manage to trace down to lower-level dictionary
features of all tokens and compute their contribution to these more
interpretable and local model behaviors. We dig in a small transformer trained
on a synthetic task named Othello and find a number of human-understandable
fine-grained circuits inside of it.</div><div><a href='http://arxiv.org/abs/2402.12201v1'>2402.12201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11917v2")'>A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step
  Reasoning Task</div>
<div id='2402.11917v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:04:25Z</div><div>Authors: Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, Christian Bartelt</div><div style='padding-top: 10px; width: 80ex'>Transformers demonstrate impressive performance on a range of reasoning
benchmarks. To evaluate the degree to which these abilities are a result of
actual reasoning, existing work has focused on developing sophisticated
benchmarks for behavioral studies. However, these studies do not provide
insights into the internal mechanisms driving the observed capabilities. To
improve our understanding of the internal mechanisms of transformers, we
present a comprehensive mechanistic analysis of a transformer trained on a
synthetic reasoning task. We identify a set of interpretable mechanisms the
model uses to solve the task, and validate our findings using correlational and
causal evidence. Our results suggest that it implements a depth-bounded
recurrent mechanisms that operates in parallel and stores intermediate results
in selected token positions. We anticipate that the motifs we identified in our
synthetic setting can provide valuable insights into the broader operating
principles of transformers and thus provide a basis for understanding more
complex models.</div><div><a href='http://arxiv.org/abs/2402.11917v2'>2402.11917v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12131v2")'>NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis</div>
<div id='2401.12131v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T17:13:50Z</div><div>Authors: Matthias Cosler, Christopher Hahn, Ayham Omar, Frederik Schmitt</div><div style='padding-top: 10px; width: 80ex'>We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for
reactive synthesis. At the core of the solver lies a seamless integration of
neural and symbolic approaches to solving the reactive synthesis problem. To
ensure soundness, the neural engine is coupled with model checkers verifying
the predictions of the underlying neural models. The open-source implementation
of NeuroSynt provides an integration framework for reactive synthesis in which
new neural and state-of-the-art symbolic approaches can be seamlessly
integrated. Extensive experiments demonstrate its efficacy in handling
challenging specifications, enhancing the state-of-the-art reactive synthesis
solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP
benchmarks.</div><div><a href='http://arxiv.org/abs/2401.12131v2'>2401.12131v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08365v1")'>NeuRes: Learning Proofs of Propositional Satisfiability</div>
<div id='2402.08365v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T10:50:54Z</div><div>Authors: Mohamed Ghanem, Frederik Schmitt, Julian Siber, Bernd Finkbeiner</div><div style='padding-top: 10px; width: 80ex'>We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other
neural SAT solving methods, NeuRes is capable of proving unsatisfiability as
opposed to merely predicting it. By design, NeuRes operates in a
certificate-driven fashion by employing propositional resolution to prove
unsatisfiability and to accelerate the process of finding satisfying truth
assignments in case of unsat and sat formulas, respectively. To realize this,
we propose a novel architecture that adapts elements from Graph Neural Networks
and Pointer Networks to autoregressively select pairs of nodes from a dynamic
graph structure, which is essential to the generation of resolution proofs. Our
model is trained and evaluated on a dataset of teacher proofs and truth
assignments that we compiled with the same random formula distribution used by
NeuroSAT. In our experiments, we show that NeuRes solves more test formulas
than NeuroSAT by a rather wide margin on different distributions while being
much more data-efficient. Furthermore, we show that NeuRes is capable of
largely shortening teacher proofs by notable proportions. We use this feature
to devise a bootstrapped training procedure that manages to reduce a dataset of
proofs generated by an advanced solver by ~23% after training on it with no
extra guidance.</div><div><a href='http://arxiv.org/abs/2402.08365v1'>2402.08365v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01817v2")'>LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks</div>
<div id='2402.01817v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:43:18Z</div><div>Authors: Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy</div><div style='padding-top: 10px; width: 80ex'>There is considerable confusion about the role of Large Language Models
(LLMs) in planning and reasoning tasks. On one side are over-optimistic claims
that LLMs can indeed do these tasks with just the right prompting or
self-verification strategies. On the other side are perhaps over-pessimistic
claims that all that LLMs are good for in planning/reasoning tasks are as mere
translators of the problem specification from one syntactic format to another,
and ship the problem off to external symbolic solvers. In this position paper,
we take the view that both these extremes are misguided. We argue that
auto-regressive LLMs cannot, by themselves, do planning or self-verification
(which is after all a form of reasoning), and shed some light on the reasons
for misunderstandings in the literature. We will also argue that LLMs should be
viewed as universal approximate knowledge sources that have much more
meaningful roles to play in planning/reasoning tasks beyond simple
front-end/back-end format translators. We present a vision of {\bf LLM-Modulo
Frameworks} that combine the strengths of LLMs with external model-based
verifiers in a tighter bi-directional interaction regime. We will show how the
models driving the external verifiers themselves can be acquired with the help
of LLMs. We will also argue that rather than simply pipelining LLMs and
symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic
approach that offers tighter integration between LLMs and symbolic components,
and allows extending the scope of model-based planning/reasoning regimes
towards more flexible knowledge, problem and preference specifications.</div><div><a href='http://arxiv.org/abs/2402.01817v2'>2402.01817v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05946v1")'>Learning Cognitive Maps from Transformer Representations for Efficient
  Planning in Partially Observed Environments</div>
<div id='2401.05946v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T14:30:30Z</div><div>Authors: Antoine Dedieu, Wolfgang Lehrach, Guangyao Zhou, Dileep George, Miguel LÃ¡zaro-Gredilla</div><div style='padding-top: 10px; width: 80ex'>Despite their stellar performance on a wide range of tasks, including
in-context tasks only revealed during inference, vanilla transformers and
variants trained for next-token predictions (a) do not learn an explicit world
model of their environment which can be flexibly queried and (b) cannot be used
for planning or navigation. In this paper, we consider partially observed
environments (POEs), where an agent receives perceptually aliased observations
as it navigates, which makes path planning hard. We introduce a transformer
with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a
compressed representation of the history of observations and actions. After
training a TDB to predict the future observation(s) given the history, we
extract interpretable cognitive maps of the environment from its active
bottleneck(s) indices. These maps are then paired with an external solver to
solve (constrained) path planning problems. First, we show that a TDB trained
on POEs (a) retains the near perfect predictive performance of a vanilla
transformer or an LSTM while (b) solving shortest path problems exponentially
faster. Second, a TDB extracts interpretable representations from text
datasets, while reaching higher in-context accuracy than vanilla sequence
models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context
accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context
path planning problems.</div><div><a href='http://arxiv.org/abs/2401.05946v1'>2401.05946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10875v1")'>Probabilistic World Modeling with Asymmetric Distance Measure</div>
<div id='2403.10875v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T09:58:49Z</div><div>Authors: Meng Song</div><div style='padding-top: 10px; width: 80ex'>Representation learning is a fundamental task in machine learning, aiming at
uncovering structures from data to facilitate subsequent tasks. However, what
is a good representation for planning and reasoning in a stochastic world
remains an open problem. In this work, we posit that learning a distance
function is essential to allow planning and reasoning in the representation
space. We show that a geometric abstraction of the probabilistic world dynamics
can be embedded into the representation space through asymmetric contrastive
learning. Unlike previous approaches that focus on learning mutual similarity
or compatibility measures, we instead learn an asymmetric similarity function
that reflects the state reachability and allows multi-way probabilistic
inference. Moreover, by conditioning on a common reference state (e.g. the
observer's current state), the learned representation space allows us to
discover the geometrically salient states that only a handful of paths can lead
through. These states can naturally serve as subgoals to break down
long-horizon planning tasks. We evaluate our method in gridworld environments
with various layouts and demonstrate its effectiveness in discovering the
subgoals.</div><div><a href='http://arxiv.org/abs/2403.10875v1'>2403.10875v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03138v1")'>Just Cluster It: An Approach for Exploration in High-Dimensions using
  Clustering and Pre-Trained Representations</div>
<div id='2402.03138v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:08:58Z</div><div>Authors: Stefan Sylvius Wagner, Stefan Harmeling</div><div style='padding-top: 10px; width: 80ex'>In this paper we adopt a representation-centric perspective on exploration in
reinforcement learning, viewing exploration fundamentally as a density
estimation problem. We investigate the effectiveness of clustering
representations for exploration in 3-D environments, based on the observation
that the importance of pixel changes between transitions is less pronounced in
3-D environments compared to 2-D environments, where pixel changes between
transitions are typically distinct and significant. We propose a method that
performs episodic and global clustering on random representations and on
pre-trained DINO representations to count states, i.e, estimate pseudo-counts.
Surprisingly, even random features can be clustered effectively to count states
in 3-D environments, however when these become visually more complex,
pre-trained DINO representations are more effective thanks to the pre-trained
inductive biases in the representations. Overall, this presents a pathway for
integrating pre-trained biases into exploration. We evaluate our approach on
the VizDoom and Habitat environments, demonstrating that our method surpasses
other well-known exploration methods in these settings.</div><div><a href='http://arxiv.org/abs/2402.03138v1'>2402.03138v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18064v3")'>Automated Testing of Spatially-Dependent Environmental Hypotheses
  through Active Transfer Learning</div>
<div id='2402.18064v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T05:49:08Z</div><div>Authors: Nicholas Harrison, Nathan Wallace, Salah Sukkarieh</div><div style='padding-top: 10px; width: 80ex'>The efficient collection of samples is an important factor in outdoor
information gathering applications on account of high sampling costs such as
time, energy, and potential destruction to the environment. Utilization of
available a-priori data can be a powerful tool for increasing efficiency.
However, the relationships of this data with the quantity of interest are often
not known ahead of time, limiting the ability to leverage this knowledge for
improved planning efficiency. To this end, this work combines transfer learning
and active learning through a Multi-Task Gaussian Process and an
information-based objective function. Through this combination it can explore
the space of hypothetical inter-quantity relationships and evaluate these
hypotheses in real-time, allowing this new knowledge to be immediately
exploited for future plans. The performance of the proposed method is evaluated
against synthetic data and is shown to evaluate multiple hypotheses correctly.
Its effectiveness is also demonstrated on real datasets. The technique is able
to identify and leverage hypotheses which show a medium or strong correlation
to reduce prediction error by a factor of 1.4--3.4 within the first 7 samples,
and poor hypotheses are quickly identified and rejected eventually having no
adverse effect.</div><div><a href='http://arxiv.org/abs/2402.18064v3'>2402.18064v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10795v1")'>From Words to Routes: Applying Large Language Models to Vehicle Routing</div>
<div id='2403.10795v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T03:54:38Z</div><div>Authors: Zhehui Huang, Guangyao Shi, Gaurav S. Sukhatme</div><div style='padding-top: 10px; width: 80ex'>LLMs have shown impressive progress in robotics (e.g., manipulation and
navigation) with natural language task descriptions. The success of LLMs in
these tasks leads us to wonder: What is the ability of LLMs to solve vehicle
routing problems (VRPs) with natural language task descriptions? In this work,
we study this question in three steps. First, we construct a dataset with 21
types of single- or multi-vehicle routing problems. Second, we evaluate the
performance of LLMs across four basic prompt paradigms of text-to-code
generation, each involving different types of text input. We find that the
basic prompt paradigm, which generates code directly from natural language task
descriptions, performs the best for GPT-4, achieving 56% feasibility, 40%
optimality, and 53% efficiency. Third, based on the observation that LLMs may
not be able to provide correct solutions at the initial attempt, we propose a
framework that enables LLMs to refine solutions through self-reflection,
including self-debugging and self-verification. With GPT-4, our proposed
framework achieves a 16% increase in feasibility, a 7% increase in optimality,
and a 15% increase in efficiency. Moreover, we examine the sensitivity of GPT-4
to task descriptions, specifically focusing on how its performance changes when
certain details are omitted from the task descriptions, yet the core meaning is
preserved. Our findings reveal that such omissions lead to a notable decrease
in performance: 4% in feasibility, 4% in optimality, and 5% in efficiency.
Website: https://sites.google.com/view/words-to-routes/</div><div><a href='http://arxiv.org/abs/2403.10795v1'>2403.10795v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16364v1")'>Where Do We Go from Here? Multi-scale Allocentric Relational Inference
  from Natural Spatial Descriptions</div>
<div id='2402.16364v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:33:28Z</div><div>Authors: Tzuf Paz-Argaman, Sayali Kulkarni, John Palowitch, Jason Baldridge, Reut Tsarfaty</div><div style='padding-top: 10px; width: 80ex'>When communicating routes in natural language, the concept of {\em acquired
spatial knowledge} is crucial for geographic information retrieval (GIR) and in
spatial cognitive research. However, NLP navigation studies often overlook the
impact of such acquired knowledge on textual descriptions. Current navigation
studies concentrate on egocentric local descriptions (e.g., `it will be on your
right') that require reasoning over the agent's local perception. These
instructions are typically given as a sequence of steps, with each action-step
explicitly mentioning and being followed by a landmark that the agent can use
to verify they are on the right path (e.g., `turn right and then you will
see...'). In contrast, descriptions based on knowledge acquired through a map
provide a complete view of the environment and capture its overall structure.
These instructions (e.g., `it is south of Central Park and a block north of a
police station') are typically non-sequential, contain allocentric relations,
with multiple spatial relations and implicit actions, without any explicit
verification. This paper introduces the Rendezvous (RVS) task and dataset,
which includes 10,404 examples of English geospatial instructions for reaching
a target location using map-knowledge. Our analysis reveals that RVS exhibits a
richer use of spatial allocentric relations, and requires resolving more
spatial relations simultaneously compared to previous text-based navigation
benchmarks.</div><div><a href='http://arxiv.org/abs/2402.16364v1'>2402.16364v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07051v1")'>$L^*LM$: Learning Automata from Examples using Natural Language Oracles</div>
<div id='2402.07051v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T21:46:34Z</div><div>Authors: Marcell Vazquez-Chanlatte, Karim Elmaaroufi, Stefan J. Witwicki, Sanjit A. Seshia</div><div style='padding-top: 10px; width: 80ex'>Expert demonstrations have proven an easy way to indirectly specify complex
tasks. Recent algorithms even support extracting unambiguous formal
specifications, e.g. deterministic finite automata (DFA), from demonstrations.
Unfortunately, these techniques are generally not sample efficient. In this
work, we introduce $L^*LM$, an algorithm for learning DFAs from both
demonstrations and natural language. Due to the expressivity of natural
language, we observe a significant improvement in the data efficiency of
learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large
language models to answer membership queries about the underlying task. This is
then combined with recent techniques for transforming learning from
demonstrations into a sequence of labeled example learning problems. In our
experiments, we observe the two modalities complement each other, yielding a
powerful few-shot learner.</div><div><a href='http://arxiv.org/abs/2402.07051v1'>2402.07051v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01353v1")'>Efficient compilation of expressive problem space specifications to
  neural network solvers</div>
<div id='2402.01353v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T09:13:09Z</div><div>Authors: Matthew L. Daggitt, Wen Kokke, Robert Atkey</div><div style='padding-top: 10px; width: 80ex'>Recent work has described the presence of the embedding gap in neural network
verification. On one side of the gap is a high-level specification about the
network's behaviour, written by a domain expert in terms of the interpretable
problem space. On the other side are a logically-equivalent set of
satisfiability queries, expressed in the uninterpretable embedding space in a
form suitable for neural network solvers. In this paper we describe an
algorithm for compiling the former to the latter. We explore and overcome
complications that arise from targeting neural network solvers as opposed to
standard SMT solvers.</div><div><a href='http://arxiv.org/abs/2402.01353v1'>2402.01353v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02950v2")'>The Tactician's Web of Large-Scale Formal Knowledge</div>
<div id='2401.02950v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T18:52:35Z</div><div>Authors: Lasse Blaauwbroek</div><div style='padding-top: 10px; width: 80ex'>The Tactician's Web is a platform offering a large web of strongly
interconnected, machine-checked, formal mathematical knowledge conveniently
packaged for machine learning, analytics, and proof engineering. Built on top
of the Coq proof assistant, the platform exports a dataset containing a wide
variety of formal theories, presented as a web of definitions, theorems, proof
terms, tactics, and proof states. Theories are encoded both as a semantic graph
(rendered below) and as human-readable text, each with a unique set of
advantages and disadvantages. Proving agents may interact with Coq through the
same rich data representation and can be automatically benchmarked on a set of
theorems. Tight integration with Coq provides the unique possibility to make
agents available to proof engineers as practical tools.</div><div><a href='http://arxiv.org/abs/2401.02950v2'>2401.02950v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13310v1")'>A Semantic Search Engine for Mathlib4</div>
<div id='2403.13310v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T05:23:09Z</div><div>Authors: Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong</div><div style='padding-top: 10px; width: 80ex'>The interactive theorem prover, Lean, enables the verification of formal
mathematical proofs and is backed by an expanding community. Central to this
ecosystem is its mathematical library, mathlib4, which lays the groundwork for
the formalization of an expanding range of mathematical theories. However,
searching for theorems in mathlib4 can be challenging. To successfully search
in mathlib4, users often need to be familiar with its naming conventions or
documentation strings. Therefore, creating a semantic search engine that can be
used easily by individuals with varying familiarity with mathlib4 is very
important. In this paper, we present a semantic search engine for mathlib4 that
accepts informal queries and finds the relevant theorems. We also establish a
benchmark for assessing the performance of various search engines for mathlib4.</div><div><a href='http://arxiv.org/abs/2403.13310v1'>2403.13310v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02104v1")'>Learning Structure-Aware Representations of Dependent Types</div>
<div id='2402.02104v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T09:56:37Z</div><div>Authors: Konstantinos Kogkalidis, Orestis Melkonian, Jean-Philippe Bernardy</div><div style='padding-top: 10px; width: 80ex'>Agda is a dependently-typed programming language and a proof assistant,
pivotal in proof formalization and programming language theory. This paper
extends the Agda ecosystem into machine learning territory, and, vice versa,
makes Agda-related resources available to machine learning practitioners. We
introduce and release a novel dataset of Agda program-proofs that is elaborate
and extensive enough to support various machine learning applications -- the
first of its kind. Leveraging the dataset's ultra-high resolution, detailing
proof states at the sub-type level, we propose a novel neural architecture
targeted at faithfully representing dependently-typed programs on the basis of
structural rather than nominal principles. We instantiate and evaluate our
architecture in a premise selection setup, where it achieves strong initial
results.</div><div><a href='http://arxiv.org/abs/2402.02104v1'>2402.02104v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16383v1")'>Learning logic programs by finding minimal unsatisfiable subprograms</div>
<div id='2401.16383v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:24:16Z</div><div>Authors: Andrew Cropper, CÃ©line Hocquette</div><div style='padding-top: 10px; width: 80ex'>The goal of inductive logic programming (ILP) is to search for a logic
program that generalises training examples and background knowledge. We
introduce an ILP approach that identifies minimal unsatisfiable subprograms
(MUSPs). We show that finding MUSPs allows us to efficiently and soundly prune
the search space. Our experiments on multiple domains, including program
synthesis and game playing, show that our approach can reduce learning times by
99%.</div><div><a href='http://arxiv.org/abs/2401.16383v1'>2401.16383v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15524v1")'>Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets</div>
<div id='2402.15524v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:03:45Z</div><div>Authors: Panagiotis Lymperopoulos, Liping Liu</div><div style='padding-top: 10px; width: 80ex'>Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a
common problem in infeasibility analysis of over-constrained systems. However,
because of the exponential search space of the problem, enumerating MUSes is
extremely time-consuming in real applications. In this work, we propose to
prune formulas using a learned model to speed up MUS enumeration. We represent
formulas as graphs and then develop a graph-based learning model to predict
which part of the formula should be pruned. Importantly, our algorithm does not
require data labeling by only checking the satisfiability of pruned formulas.
It does not even require training data from the target application because it
extrapolates to data with different distributions. In our experiments we
combine our algorithm with existing MUS enumerators and validate its
effectiveness in multiple benchmarks including a set of real-world problems
outside our training distribution. The experiment results show that our method
significantly accelerates MUS enumeration on average on these benchmark
problems.</div><div><a href='http://arxiv.org/abs/2402.15524v1'>2402.15524v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17032v1")'>REFACTOR: Learning to Extract Theorems from Proofs</div>
<div id='2402.17032v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T21:21:30Z</div><div>Authors: Jin Peng Zhou, Yuhuai Wu, Qiyang Li, Roger Grosse</div><div style='padding-top: 10px; width: 80ex'>Human mathematicians are often good at recognizing modular and reusable
theorems that make complex mathematical results within reach. In this paper, we
propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for
training neural networks to mimic this ability in formal mathematical theorem
proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6%
of the theorems that humans would use to write the proofs. When applying the
model to the existing Metamath library, REFACTOR extracted 16 new theorems.
With newly extracted theorems, we show that the existing proofs in the MetaMath
database can be refactored. The new theorems are used very frequently after
refactoring, with an average usage of 733.5 times, and help shorten the proof
lengths. Lastly, we demonstrate that the prover trained on the new-theorem
refactored dataset proves more test theorems and outperforms state-of-the-art
baselines by frequently leveraging a diverse set of newly extracted theorems.
Code can be found at https://github.com/jinpz/refactor.</div><div><a href='http://arxiv.org/abs/2402.17032v1'>2402.17032v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02949v2")'>Graph2Tac: Learning Hierarchical Representations of Math Concepts in
  Theorem proving</div>
<div id='2401.02949v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T18:52:09Z</div><div>Authors: Jason Rute, Miroslav OlÅ¡Ã¡k, Lasse Blaauwbroek, Fidel Ivan Schaposnik Massolo, Jelle Piepenbrock, Vasily Pestun</div><div style='padding-top: 10px; width: 80ex'>Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users' workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.</div><div><a href='http://arxiv.org/abs/2401.02949v2'>2401.02949v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03401v1")'>BAIT: Benchmarking (Embedding) Architectures for Interactive
  Theorem-Proving</div>
<div id='2403.03401v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T01:56:17Z</div><div>Authors: Sean Lamont, Michael Norrish, Amir Dezfouli, Christian Walder, Paul Montague</div><div style='padding-top: 10px; width: 80ex'>Artificial Intelligence for Theorem Proving has given rise to a plethora of
benchmarks and methodologies, particularly in Interactive Theorem Proving
(ITP). Research in the area is fragmented, with a diverse set of approaches
being spread across several ITP systems. This presents a significant challenge
to the comparison of methods, which are often complex and difficult to
replicate. Addressing this, we present BAIT, a framework for fair and
streamlined comparison of learning approaches in ITP. We demonstrate BAIT's
capabilities with an in-depth comparison, across several ITP benchmarks, of
state-of-the-art architectures applied to the problem of formula embedding. We
find that Structure Aware Transformers perform particularly well, improving on
techniques associated with the original problem sets. BAIT also allows us to
assess the end-to-end proving performance of systems built on interactive
environments. This unified perspective reveals a novel end-to-end system that
improves on prior work. We also provide a qualitative analysis, illustrating
that improved performance is associated with more semantically-aware
embeddings. By streamlining the implementation and comparison of Machine
Learning algorithms in the ITP context, we anticipate BAIT will be a
springboard for future research.</div><div><a href='http://arxiv.org/abs/2403.03401v1'>2403.03401v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12711v1")'>When Redundancy Matters: Machine Teaching of Representations</div>
<div id='2401.12711v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T12:20:17Z</div><div>Authors: CÃ¨sar Ferri, Dario Garigliotti, Brigt Arve Toppe HÃ¥vardstun, JosÃ¨ HernÃ¡ndez-Orallo, Jan Arne Telle</div><div style='padding-top: 10px; width: 80ex'>In traditional machine teaching, a teacher wants to teach a concept to a
learner, by means of a finite set of examples, the witness set. But concepts
can have many equivalent representations. This redundancy strongly affects the
search space, to the extent that teacher and learner may not be able to easily
determine the equivalence class of each representation. In this common
situation, instead of teaching concepts, we explore the idea of teaching
representations. We work with several teaching schemas that exploit
representation and witness size (Eager, Greedy and Optimal) and analyze the
gains in teaching effectiveness for some representational languages (DNF
expressions and Turing-complete P3 programs). Our theoretical and experimental
results indicate that there are various types of redundancy, handled better by
the Greedy schema introduced here than by the Eager schema, although both can
be arbitrarily far away from the Optimal. For P3 programs we found that witness
sets are usually smaller than the programs they identify, which is an
illuminating justification of why machine teaching from examples makes sense at
all.</div><div><a href='http://arxiv.org/abs/2401.12711v1'>2401.12711v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00011v1")'>Introducing User Feedback-based Counterfactual Explanations (UFCE)</div>
<div id='2403.00011v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T20:09:44Z</div><div>Authors: Muhammad Suffian, Jose M. Alonso-Moral, Alessandro Bogliolo</div><div style='padding-top: 10px; width: 80ex'>Machine learning models are widely used in real-world applications. However,
their complexity makes it often challenging to interpret the rationale behind
their decisions. Counterfactual explanations (CEs) have emerged as a viable
solution for generating comprehensible explanations in eXplainable Artificial
Intelligence (XAI). CE provides actionable information to users on how to
achieve the desired outcome with minimal modifications to the input. However,
current CE algorithms usually operate within the entire feature space when
optimizing changes to turn over an undesired outcome, overlooking the
identification of key contributors to the outcome and disregarding the
practicality of the suggested changes. In this study, we introduce a novel
methodology, that is named as user feedback-based counterfactual explanation
(UFCE), which addresses these limitations and aims to bolster confidence in the
provided explanations. UFCE allows for the inclusion of user constraints to
determine the smallest modifications in the subset of actionable features while
considering feature dependence, and evaluates the practicality of suggested
changes using benchmark evaluation metrics. We conducted three experiments with
five datasets, demonstrating that UFCE outperforms two well-known CE methods in
terms of \textit{proximity}, \textit{sparsity}, and \textit{feasibility}.
Reported results indicate that user constraints influence the generation of
feasible CEs.</div><div><a href='http://arxiv.org/abs/2403.00011v1'>2403.00011v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01408v1")'>Climbing the Ladder of Interpretability with Counterfactual Concept
  Bottleneck Models</div>
<div id='2402.01408v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:42:12Z</div><div>Authors: Gabriele Dominici, Pietro Barbiero, Francesco Giannini, Martin Gjoreski, Giuseppe Marra, Marc Langheinrich</div><div style='padding-top: 10px; width: 80ex'>Current deep learning models are not designed to simultaneously address three
fundamental questions: predict class labels to solve a given classification
task (the "What?"), explain task predictions (the "Why?"), and imagine
alternative scenarios that could result in different predictions (the "What
if?"). The inability to answer these questions represents a crucial gap in
deploying reliable AI agents, calibrating human trust, and deepening
human-machine interaction. To bridge this gap, we introduce CounterFactual
Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently
address the above queries all at once without the need to run post-hoc
searches. Our results show that CF-CBMs produce: accurate predictions (the
"What?"), simple explanations for task predictions (the "Why?"), and
interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or
estimate the most probable counterfactual to: (i) explain the effect of concept
interventions on tasks, (ii) show users how to get a desired class label, and
(iii) propose concept interventions via "task-driven" interventions.</div><div><a href='http://arxiv.org/abs/2402.01408v1'>2402.01408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10938v1")'>Even-if Explanations: Formal Foundations, Priorities and Complexity</div>
<div id='2401.10938v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T11:38:58Z</div><div>Authors: Gianvincenzo Alfano, Sergio Greco, Domenico Mandaglio, Francesco Parisi, Reza Shahbazian, Irina Trubitsyna</div><div style='padding-top: 10px; width: 80ex'>EXplainable AI has received significant attention in recent years. Machine
learning models often operate as black boxes, lacking explainability and
transparency while supporting decision-making processes. Local post-hoc
explainability queries attempt to answer why individual inputs are classified
in a certain way by a given model. While there has been important work on
counterfactual explanations, less attention has been devoted to semifactual
ones. In this paper, we focus on local post-hoc explainability queries within
the semifactual `even-if' thinking and their computational complexity among
different classes of models, and show that both linear and tree-based models
are strictly more interpretable than neural networks. After this, we introduce
a preference-based framework that enables users to personalize explanations
based on their preferences, both in the case of semifactuals and
counterfactuals, enhancing interpretability and user-centricity. Finally, we
explore the complexity of several interpretability problems in the proposed
preference-based framework and provide algorithms for polynomial cases.</div><div><a href='http://arxiv.org/abs/2401.10938v1'>2401.10938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10888v1")'>Explainability for Machine Learning Models: From Data Adaptability to
  User Perception</div>
<div id='2402.10888v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:44:37Z</div><div>Authors: julien Delaunay</div><div style='padding-top: 10px; width: 80ex'>This thesis explores the generation of local explanations for already
deployed machine learning models, aiming to identify optimal conditions for
producing meaningful explanations considering both data and user requirements.
The primary goal is to develop methods for generating explanations for any
model while ensuring that these explanations remain faithful to the underlying
model and comprehensible to the users.
  The thesis is divided into two parts. The first enhances a widely used
rule-based explanation method. It then introduces a novel approach for
evaluating the suitability of linear explanations to approximate a model.
Additionally, it conducts a comparative experiment between two families of
counterfactual explanation methods to analyze the advantages of one over the
other. The second part focuses on user experiments to assess the impact of
three explanation methods and two distinct representations. These experiments
measure how users perceive their interaction with the model in terms of
understanding and trust, depending on the explanations and representations.
This research contributes to a better explanation generation, with potential
implications for enhancing the transparency, trustworthiness, and usability of
deployed AI systems.</div><div><a href='http://arxiv.org/abs/2402.10888v1'>2402.10888v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00347v1")'>Diverse Explanations from Data-driven and Domain-driven Perspectives for
  Machine Learning Models</div>
<div id='2402.00347v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T05:28:28Z</div><div>Authors: Sichao Li, Amanda Barnard</div><div style='padding-top: 10px; width: 80ex'>Explanations of machine learning models are important, especially in
scientific areas such as chemistry, biology, and physics, where they guide
future laboratory experiments and resource requirements. These explanations can
be derived from well-trained machine learning models (data-driven perspective)
or specific domain knowledge (domain-driven perspective). However, there exist
inconsistencies between these perspectives due to accurate yet misleading
machine learning models and various stakeholders with specific needs, wants, or
aims. This paper calls attention to these inconsistencies and suggests a way to
find an accurate model with expected explanations that reinforce physical laws
and meet stakeholders' requirements from a set of equally-good models, also
known as Rashomon sets. Our goal is to foster a comprehensive understanding of
these inconsistencies and ultimately contribute to the integration of
eXplainable Artificial Intelligence (XAI) into scientific domains.</div><div><a href='http://arxiv.org/abs/2402.00347v1'>2402.00347v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07359v2")'>Reliability and Interpretability in Science and Deep Learning</div>
<div id='2401.07359v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T20:14:07Z</div><div>Authors: Luigi Scorzato</div><div style='padding-top: 10px; width: 80ex'>In recent years, the question of the reliability of Machine Learning (ML)
methods has acquired significant importance, and the analysis of the associated
uncertainties has motivated a growing amount of research. However, most of
these studies have applied standard error analysis to ML models, and in
particular Deep Neural Network (DNN) models, which represent a rather
significant departure from standard scientific modelling. It is therefore
necessary to integrate the standard error analysis with a deeper
epistemological analysis of the possible differences between DNN models and
standard scientific modelling and the possible implications of these
differences in the assessment of reliability. This article offers several
contributions. First, it emphasises the ubiquitous role of model assumptions
(both in ML and traditional Science) against the illusion of theory-free
science. Secondly, model assumptions are analysed from the point of view of
their (epistemic) complexity, which is shown to be language-independent. It is
argued that the high epistemic complexity of DNN models hinders the estimate of
their reliability and also their prospect of long-term progress. Some potential
ways forward are suggested. Thirdly, this article identifies the close relation
between a model's epistemic complexity and its interpretability, as introduced
in the context of responsible AI. This clarifies in which sense, and to what
extent, the lack of understanding of a model (black-box problem) impacts its
interpretability in a way that is independent of individual skills. It also
clarifies how interpretability is a precondition for assessing the reliability
of any model, which cannot be based on statistical analysis alone. This article
focuses on the comparison between traditional scientific models and DNN models.
But, Random Forest and Logistic Regression models are also briefly considered.</div><div><a href='http://arxiv.org/abs/2401.07359v2'>2401.07359v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10168v1")'>Explainability through uncertainty: Trustworthy decision-making with
  neural networks</div>
<div id='2403.10168v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T10:22:48Z</div><div>Authors: Arthur Thuy, Dries F. Benoit</div><div style='padding-top: 10px; width: 80ex'>Uncertainty is a key feature of any machine learning model and is
particularly important in neural networks, which tend to be overconfident. This
overconfidence is worrying under distribution shifts, where the model
performance silently degrades as the data distribution diverges from the
training data distribution. Uncertainty estimation offers a solution to
overconfident models, communicating when the output should (not) be trusted.
Although methods for uncertainty estimation have been developed, they have not
been explicitly linked to the field of explainable artificial intelligence
(XAI). Furthermore, literature in operations research ignores the actionability
component of uncertainty estimation and does not consider distribution shifts.
This work proposes a general uncertainty framework, with contributions being
threefold: (i) uncertainty estimation in ML models is positioned as an XAI
technique, giving local and model-specific explanations; (ii) classification
with rejection is used to reduce misclassifications by bringing a human expert
in the loop for uncertain observations; (iii) the framework is applied to a
case study on neural networks in educational data mining subject to
distribution shifts. Uncertainty as XAI improves the model's trustworthiness in
downstream decision-making tasks, giving rise to more actionable and robust
machine learning systems in operations research.</div><div><a href='http://arxiv.org/abs/2403.10168v1'>2403.10168v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13914v1")'>Explain to Question not to Justify</div>
<div id='2402.13914v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:30:24Z</div><div>Authors: Przemyslaw Biecek, Wojciech Samek</div><div style='padding-top: 10px; width: 80ex'>Explainable Artificial Intelligence (XAI) is a young but very promising field
of research. Unfortunately, the progress in this field is currently slowed down
by divergent and incompatible goals. In this paper, we separate various threads
tangled within the area of XAI into two complementary cultures of
human/value-oriented explanations (BLUE XAI) and model/validation-oriented
explanations (RED XAI). We also argue that the area of RED XAI is currently
under-explored and hides great opportunities and potential for important
research necessary to ensure the safety of AI systems. We conclude this paper
by presenting promising challenges in this area.</div><div><a href='http://arxiv.org/abs/2402.13914v1'>2402.13914v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00315v1")'>Axe the X in XAI: A Plea for Understandable AI</div>
<div id='2403.00315v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T06:28:53Z</div><div>Authors: AndrÃ©s PÃ¡ez</div><div style='padding-top: 10px; width: 80ex'>In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity
of the term "explanation" in explainable AI (XAI) can be solved by adopting any
of four different extant accounts of explanation in the philosophy of science:
the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New
Mechanist models. In this chapter, I show that the authors' claim that these
accounts can be applied to deep neural networks as they would to any natural
phenomenon is mistaken. I also provide a more general argument as to why the
notion of explainability as it is currently used in the XAI literature bears
little resemblance to the traditional concept of scientific explanation. It
would be more fruitful to use the label "understandable AI" to avoid the
confusion that surrounds the goal and purposes of XAI. In the second half of
the chapter, I argue for a pragmatic conception of understanding that is better
suited to play the central role attributed to explanation in XAI. Following
Kuorikoski &amp; Ylikoski (2015), the conditions of satisfaction for understanding
an ML system are fleshed out in terms of an agent's success in using the
system, in drawing correct inferences from it.</div><div><a href='http://arxiv.org/abs/2403.00315v1'>2403.00315v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01732v1")'>Task and Explanation Network</div>
<div id='2401.01732v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T13:11:59Z</div><div>Authors: Moshe Sipper</div><div style='padding-top: 10px; width: 80ex'>Explainability in deep networks has gained increased importance in recent
years. We argue herein that an AI must be tasked not just with a task but also
with an explanation of why said task was accomplished as such. We present a
basic framework -- Task and Explanation Network (TENet) -- which fully
integrates task completion and its explanation. We believe that the field of AI
as a whole should insist -- quite emphatically -- on explainability.</div><div><a href='http://arxiv.org/abs/2401.01732v1'>2401.01732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04523v1")'>T-TAME: Trainable Attention Mechanism for Explaining Convolutional
  Networks and Vision Transformers</div>
<div id='2403.04523v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:25:03Z</div><div>Authors: Mariano V. Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris</div><div style='padding-top: 10px; width: 80ex'>The development and adoption of Vision Transformers and other deep-learning
architectures for image classification tasks has been rapid. However, the
"black box" nature of neural networks is a barrier to adoption in applications
where explainability is essential. While some techniques for generating
explanations have been proposed, primarily for Convolutional Neural Networks,
adapting such techniques to the new paradigm of Vision Transformers is
non-trivial. This paper presents T-TAME, Transformer-compatible Trainable
Attention Mechanism for Explanations, a general methodology for explaining deep
neural networks used in image classification tasks. The proposed architecture
and training technique can be easily applied to any convolutional or Vision
Transformer-like neural network, using a streamlined training approach. After
training, explanation maps can be computed in a single forward pass; these
explanation maps are comparable to or outperform the outputs of computationally
expensive perturbation-based explainability techniques, achieving SOTA
performance. We apply T-TAME to three popular deep learning classifier
architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet
dataset, and we demonstrate improvements over existing state-of-the-art
explainability methods. A detailed analysis of the results and an ablation
study provide insights into how the T-TAME design choices affect the quality of
the generated explanation maps.</div><div><a href='http://arxiv.org/abs/2403.04523v1'>2403.04523v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04647v1")'>Advancing Ante-Hoc Explainable Models through Generative Adversarial
  Networks</div>
<div id='2401.04647v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T16:16:16Z</div><div>Authors: Tanmay Garg, Deepika Vemuri, Vineeth N Balasubramanian</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel concept learning framework for enhancing model
interpretability and performance in visual classification tasks. Our approach
appends an unsupervised explanation generator to the primary classifier network
and makes use of adversarial training. During training, the explanation module
is optimized to extract visual concepts from the classifier's latent
representations, while the GAN-based module aims to discriminate images
generated from concepts, from true images. This joint training scheme enables
the model to implicitly align its internally learned concepts with
human-interpretable visual properties. Comprehensive experiments demonstrate
the robustness of our approach, while producing coherent concept activations.
We analyse the learned concepts, showing their semantic concordance with object
parts and visual attributes. We also study how perturbations in the adversarial
training protocol impact both classification and concept acquisition. In
summary, this work presents a significant step towards building inherently
interpretable deep vision models with task-aligned concept representations - a
key enabler for developing trustworthy AI for real-world perception tasks.</div><div><a href='http://arxiv.org/abs/2401.04647v1'>2401.04647v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15172v1")'>Attention-Guided Masked Autoencoders For Learning Image Representations</div>
<div id='2402.15172v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:11:25Z</div><div>Authors: Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski</div><div style='padding-top: 10px; width: 80ex'>Masked autoencoders (MAEs) have established themselves as a powerful method
for unsupervised pre-training for computer vision tasks. While vanilla MAEs put
equal emphasis on reconstructing the individual parts of the image, we propose
to inform the reconstruction process through an attention-guided loss function.
By leveraging advances in unsupervised object discovery, we obtain an attention
map of the scene which we employ in the loss function to put increased emphasis
on reconstructing relevant objects, thus effectively incentivizing the model to
learn more object-focused representations without compromising the established
masking strategy. Our evaluations show that our pre-trained models learn better
latent representations than the vanilla MAE, demonstrated by improved linear
probing and k-NN classification results on several benchmarks while at the same
time making ViTs more robust against varying backgrounds.</div><div><a href='http://arxiv.org/abs/2402.15172v1'>2402.15172v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18128v1")'>Downstream Task Guided Masking Learning in Masked Autoencoders Using
  Multi-Level Optimization</div>
<div id='2402.18128v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:37:26Z</div><div>Authors: Han Guo, Ramtin Hosseini, Ruiyi Zhang, Sai Ashish Somayajula, Ranak Roy Chowdhury, Rajesh K. Gupta, Pengtao Xie</div><div style='padding-top: 10px; width: 80ex'>Masked Autoencoder (MAE) is a notable method for self-supervised pretraining
in visual representation learning. It operates by randomly masking image
patches and reconstructing these masked patches using the unmasked ones. A key
limitation of MAE lies in its disregard for the varying informativeness of
different patches, as it uniformly selects patches to mask. To overcome this,
some approaches propose masking based on patch informativeness. However, these
methods often do not consider the specific requirements of downstream tasks,
potentially leading to suboptimal representations for these tasks. In response,
we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel
framework that leverages end-to-end feedback from downstream tasks to learn an
optimal masking strategy during pretraining. Our experimental findings
highlight MLO-MAE's significant advancements in visual representation learning.
Compared to existing methods, it demonstrates remarkable improvements across
diverse datasets and tasks, showcasing its adaptability and efficiency. Our
code is available at: https://github.com/Alexiland/MLOMAE</div><div><a href='http://arxiv.org/abs/2402.18128v1'>2402.18128v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08534v4")'>DiConStruct: Causal Concept-based Explanations through Black-Box
  Distillation</div>
<div id='2401.08534v4' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:54:02Z</div><div>Authors: Ricardo Moreira, Jacopo Bono, MÃ¡rio Cardoso, Pedro Saleiro, MÃ¡rio A. T. Figueiredo, Pedro Bizarro</div><div style='padding-top: 10px; width: 80ex'>Model interpretability plays a central role in human-AI decision-making
systems. Ideally, explanations should be expressed using human-interpretable
semantic concepts. Moreover, the causal relations between these concepts should
be captured by the explainer to allow for reasoning about the explanations.
Lastly, explanation methods should be efficient and not compromise the
performance of the predictive task. Despite the rapid advances in AI
explainability in recent years, as far as we know to date, no method fulfills
these three properties. Indeed, mainstream methods for local concept
explainability do not produce causal explanations and incur a trade-off between
explainability and prediction performance. We present DiConStruct, an
explanation method that is both concept-based and causal, with the goal of
creating more interpretable local explanations in the form of structural causal
models and concept attributions. Our explainer works as a distillation model to
any black-box machine learning model by approximating its predictions while
producing the respective explanations. Because of this, DiConStruct generates
explanations efficiently while not impacting the black-box prediction task. We
validate our method on an image dataset and a tabular dataset, showing that
DiConStruct approximates the black-box models with higher fidelity than other
concept explainability baselines, while providing explanations that include the
causal relations between the concepts.</div><div><a href='http://arxiv.org/abs/2401.08534v4'>2401.08534v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08876v5")'>Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image
  Labeling</div>
<div id='2401.08876v5' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T23:19:30Z</div><div>Authors: Dongping Zhang, Angelos Chatzimparmpas, Negar Kamali, Jessica Hullman</div><div style='padding-top: 10px; width: 80ex'>As deep neural networks are more commonly deployed in high-stakes domains,
their black-box nature makes uncertainty quantification challenging. We
investigate the effects of presenting conformal prediction sets--a
distribution-free class of methods for generating prediction sets with
specified coverage--to express uncertainty in AI-advised decision-making.
Through a large online experiment, we compare the utility of conformal
prediction sets to displays of Top-1 and Top-k predictions for AI-advised image
labeling. In a pre-registered analysis, we find that the utility of prediction
sets for accuracy varies with the difficulty of the task: while they result in
accuracy on par with or less than Top-1 and Top-k displays for easy images,
prediction sets excel at assisting humans in labeling out-of-distribution (OOD)
images, especially when the set size is small. Our results empirically pinpoint
practical challenges of conformal prediction sets and provide implications on
how to incorporate them for real-world decision-making.</div><div><a href='http://arxiv.org/abs/2401.08876v5'>2401.08876v5</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14899v2")'>Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning
  Meets Adversarial Images</div>
<div id='2402.14899v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:36:34Z</div><div>Authors: Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao, Volker Tresp, Philip Torr, Jindong Gu</div><div style='padding-top: 10px; width: 80ex'>Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand
images. However, like traditional vision models, they are still vulnerable to
adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely
explored on MLLMs, which not only improves model's performance, but also
enhances model's explainability by giving intermediate reasoning steps.
Nevertheless, there is still a lack of study regarding MLLMs' adversarial
robustness with CoT and an understanding of what the rationale looks like when
MLLMs infer wrong answers with adversarial images. Our research evaluates the
adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT
marginally improves adversarial robustness against existing attack methods.
Moreover, we introduce a novel stop-reasoning attack technique that effectively
bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the
alterations in CoT reasoning when MLLMs confront adversarial images, shedding
light on their reasoning process under adversarial attacks.</div><div><a href='http://arxiv.org/abs/2402.14899v2'>2402.14899v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06013v1")'>Are Classification Robustness and Explanation Robustness Really Strongly
  Correlated? An Analysis Through Input Loss Landscape</div>
<div id='2403.06013v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:26:10Z</div><div>Authors: Tiejin Chen, Wenwang Huang, Linsey Pang, Dongsheng Luo, Hua Wei</div><div style='padding-top: 10px; width: 80ex'>This paper delves into the critical area of deep learning robustness,
challenging the conventional belief that classification robustness and
explanation robustness in image classification systems are inherently
correlated. Through a novel evaluation approach leveraging clustering for
efficient assessment of explanation robustness, we demonstrate that enhancing
explanation robustness does not necessarily flatten the input loss landscape
with respect to explanation loss - contrary to flattened loss landscapes
indicating better classification robustness. To deeply investigate this
contradiction, a groundbreaking training method designed to adjust the loss
landscape with respect to explanation loss is proposed. Through the new
training method, we uncover that although such adjustments can impact the
robustness of explanations, they do not have an influence on the robustness of
classification. These findings not only challenge the prevailing assumption of
a strong correlation between the two forms of robustness but also pave new
pathways for understanding relationship between loss landscape and explanation
loss.</div><div><a href='http://arxiv.org/abs/2403.06013v1'>2403.06013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06910v1")'>Responsible Artificial Intelligence: A Structured Literature Review</div>
<div id='2403.06910v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:01:13Z</div><div>Authors: Sabrina Goellner, Marina Tropmann-Frick, Bostjan Brumen</div><div style='padding-top: 10px; width: 80ex'>Our research endeavors to advance the concept of responsible artificial
intelligence (AI), a topic of increasing importance within EU policy
discussions. The EU has recently issued several publications emphasizing the
necessity of trust in AI, underscoring the dual nature of AI as both a
beneficial tool and a potential weapon. This dichotomy highlights the urgent
need for international regulation. Concurrently, there is a need for frameworks
that guide companies in AI development, ensuring compliance with such
regulations. Our research aims to assist lawmakers and machine learning
practitioners in navigating the evolving landscape of AI regulation,
identifying focal areas for future attention. This paper introduces a
comprehensive and, to our knowledge, the first unified definition of
responsible AI. Through a structured literature review, we elucidate the
current understanding of responsible AI. Drawing from this analysis, we propose
an approach for developing a future framework centered around this concept. Our
findings advocate for a human-centric approach to Responsible AI. This approach
encompasses the implementation of AI methods with a strong emphasis on ethics,
model explainability, and the pillars of privacy, security, and trust.</div><div><a href='http://arxiv.org/abs/2403.06910v1'>2403.06910v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08691v1")'>Towards Responsible AI in Banking: Addressing Bias for Fair
  Decision-Making</div>
<div id='2401.08691v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T14:07:09Z</div><div>Authors: Alessandro Castelnovo</div><div style='padding-top: 10px; width: 80ex'>In an era characterized by the pervasive integration of artificial
intelligence into decision-making processes across diverse industries, the
demand for trust has never been more pronounced. This thesis embarks on a
comprehensive exploration of bias and fairness, with a particular emphasis on
their ramifications within the banking sector, where AI-driven decisions bear
substantial societal consequences. In this context, the seamless integration of
fairness, explainability, and human oversight is of utmost importance,
culminating in the establishment of what is commonly referred to as
"Responsible AI". This emphasizes the critical nature of addressing biases
within the development of a corporate culture that aligns seamlessly with both
AI regulations and universal human rights standards, particularly in the realm
of automated decision-making systems. Nowadays, embedding ethical principles
into the development, training, and deployment of AI models is crucial for
compliance with forthcoming European regulations and for promoting societal
good. This thesis is structured around three fundamental pillars: understanding
bias, mitigating bias, and accounting for bias. These contributions are
validated through their practical application in real-world scenarios, in
collaboration with Intesa Sanpaolo. This collaborative effort not only
contributes to our understanding of fairness but also provides practical tools
for the responsible implementation of AI-based decision-making systems. In line
with open-source principles, we have released Bias On Demand and FairView as
accessible Python packages, further promoting progress in the field of AI
fairness.</div><div><a href='http://arxiv.org/abs/2401.08691v1'>2401.08691v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05541v1")'>AI in ESG for Financial Institutions: An Industrial Survey</div>
<div id='2403.05541v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T02:14:47Z</div><div>Authors: Jun Xu</div><div style='padding-top: 10px; width: 80ex'>The burgeoning integration of Artificial Intelligence (AI) into
Environmental, Social, and Governance (ESG) initiatives within the financial
sector represents a paradigm shift towards more sus-tainable and equitable
financial practices. This paper surveys the industrial landscape to delineate
the necessity and impact of AI in bolstering ESG frameworks. With the advent of
stringent regulatory requirements and heightened stakeholder awareness,
financial institutions (FIs) are increasingly compelled to adopt ESG criteria.
AI emerges as a pivotal tool in navigating the complex in-terplay of financial
activities and sustainability goals. Our survey categorizes AI applications
across three main pillars of ESG, illustrating how AI enhances analytical
capabilities, risk assessment, customer engagement, reporting accuracy and
more. Further, we delve into the critical con-siderations surrounding the use
of data and the development of models, underscoring the importance of data
quality, privacy, and model robustness. The paper also addresses the imperative
of responsible and sustainable AI, emphasizing the ethical dimensions of AI
deployment in ESG-related banking processes. Conclusively, our findings suggest
that while AI offers transformative potential for ESG in banking, it also poses
significant challenges that necessitate careful consideration. The final part
of the paper synthesizes the survey's insights, proposing a forward-looking
stance on the adoption of AI in ESG practices. We conclude with recommendations
with a reference architecture for future research and development, advocating
for a balanced approach that leverages AI's strengths while mitigating its
risks within the ESG domain.</div><div><a href='http://arxiv.org/abs/2403.05541v1'>2403.05541v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15897v1")'>Red-Teaming for Generative AI: Silver Bullet or Security Theater?</div>
<div id='2401.15897v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T05:46:14Z</div><div>Authors: Michael Feffer, Anusha Sinha, Zachary C. Lipton, Hoda Heidari</div><div style='padding-top: 10px; width: 80ex'>In response to rising concerns surrounding the safety, security, and
trustworthiness of Generative AI (GenAI) models, practitioners and regulators
alike have pointed to AI red-teaming as a key component of their strategies for
identifying and mitigating these risks. However, despite AI red-teaming's
central role in policy discussions and corporate messaging, significant
questions remain about what precisely it means, what role it can play in
regulation, and how precisely it relates to conventional red-teaming practices
as originally conceived in the field of cybersecurity. In this work, we
identify recent cases of red-teaming activities in the AI industry and conduct
an extensive survey of the relevant research literature to characterize the
scope, structure, and criteria for AI red-teaming practices. Our analysis
reveals that prior methods and practices of AI red-teaming diverge along
several axes, including the purpose of the activity (which is often vague), the
artifact under evaluation, the setting in which the activity is conducted
(e.g., actors, resources, and methods), and the resulting decisions it informs
(e.g., reporting, disclosure, and mitigation). In light of our findings, we
argue that while red-teaming may be a valuable big-tent idea for characterizing
a broad set of activities and attitudes aimed at improving the behavior of
GenAI models, gestures towards red-teaming as a panacea for every possible risk
verge on security theater. To move toward a more robust toolbox of evaluations
for generative AI, we synthesize our recommendations into a question bank meant
to guide and scaffold future AI red-teaming practices.</div><div><a href='http://arxiv.org/abs/2401.15897v1'>2401.15897v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07506v1")'>NeuralSentinel: Safeguarding Neural Network Reliability and
  Trustworthiness</div>
<div id='2402.07506v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T09:24:34Z</div><div>Authors: Xabier Echeberria-Barrio, Mikel Gorricho, Selene Valencia, Francesco Zola</div><div style='padding-top: 10px; width: 80ex'>The usage of Artificial Intelligence (AI) systems has increased
exponentially, thanks to their ability to reduce the amount of data to be
analyzed, the user efforts and preserving a high rate of accuracy. However,
introducing this new element in the loop has converted them into attacked
points that can compromise the reliability of the systems. This new scenario
has raised crucial challenges regarding the reliability and trustworthiness of
the AI models, as well as about the uncertainties in their response decisions,
becoming even more crucial when applied in critical domains such as healthcare,
chemical, electrical plants, etc. To contain these issues, in this paper, we
present NeuralSentinel (NS), a tool able to validate the reliability and
trustworthiness of AI models. This tool combines attack and defence strategies
and explainability concepts to stress an AI model and help non-expert staff
increase their confidence in this new system by understanding the model
decisions. NS provide a simple and easy-to-use interface for helping humans in
the loop dealing with all the needed information. This tool was deployed and
used in a Hackathon event to evaluate the reliability of a skin cancer image
detector. During the event, experts and non-experts attacked and defended the
detector, learning which factors were the most important for model
misclassification and which techniques were the most efficient. The event was
also used to detect NS's limitations and gather feedback for further
improvements.</div><div><a href='http://arxiv.org/abs/2402.07506v1'>2402.07506v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09011v1")'>Inductive Models for Artificial Intelligence Systems are Insufficient
  without Good Explanations</div>
<div id='2401.09011v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T07:14:04Z</div><div>Authors: Udesh Habaraduwa</div><div style='padding-top: 10px; width: 80ex'>This paper discusses the limitations of machine learning (ML), particularly
deep artificial neural networks (ANNs), which are effective at approximating
complex functions but often lack transparency and explanatory power. It
highlights the `problem of induction' : the philosophical issue that past
observations may not necessarily predict future events, a challenge that ML
models face when encountering new, unseen data. The paper argues for the
importance of not just making predictions but also providing good explanations,
a feature that current models often fail to deliver. It suggests that for AI to
progress, we must seek models that offer insights and explanations, not just
predictions.</div><div><a href='http://arxiv.org/abs/2401.09011v1'>2401.09011v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07035v1")'>Distilling Symbolic Priors for Concept Learning into Neural Networks</div>
<div id='2402.07035v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T20:06:26Z</div><div>Authors: Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths</div><div style='padding-top: 10px; width: 80ex'>Humans can learn new concepts from a small number of examples by drawing on
their inductive biases. These inductive biases have previously been captured by
using Bayesian models defined over symbolic hypothesis spaces. Is it possible
to create a neural network that displays the same inductive biases? We show
that inductive biases that enable rapid concept learning can be instantiated in
artificial neural networks by distilling a prior distribution from a symbolic
Bayesian model via meta-learning, an approach for extracting the common
structure from a set of tasks. By generating the set of tasks used in
meta-learning from the prior distribution of a Bayesian model, we are able to
transfer that prior into a neural network. We use this approach to create a
neural network with an inductive bias towards concepts expressed as short
logical formulas. Analyzing results from previous behavioral experiments in
which people learned logical concepts from a few examples, we find that our
meta-trained models are highly aligned with human performance.</div><div><a href='http://arxiv.org/abs/2402.07035v1'>2402.07035v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09046v1")'>Inference of Abstraction for a Unified Account of Reasoning and Learning</div>
<div id='2402.09046v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T09:43:35Z</div><div>Authors: Hiroyuki Kido</div><div style='padding-top: 10px; width: 80ex'>Inspired by Bayesian approaches to brain function in neuroscience, we give a
simple theory of probabilistic inference for a unified account of reasoning and
learning. We simply model how data cause symbolic knowledge in terms of its
satisfiability in formal logic. The underlying idea is that reasoning is a
process of deriving symbolic knowledge from data via abstraction, i.e.,
selective ignorance. The logical consequence relation is discussed for its
proof-based theoretical correctness. The MNIST dataset is discussed for its
experiment-based empirical correctness.</div><div><a href='http://arxiv.org/abs/2402.09046v1'>2402.09046v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18426v1")'>A Relational Inductive Bias for Dimensional Abstraction in Neural
  Networks</div>
<div id='2402.18426v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:51:05Z</div><div>Authors: Declan Campbell, Jonathan D. Cohen</div><div style='padding-top: 10px; width: 80ex'>The human cognitive system exhibits remarkable flexibility and generalization
capabilities, partly due to its ability to form low-dimensional, compositional
representations of the environment. In contrast, standard neural network
architectures often struggle with abstract reasoning tasks, overfitting, and
requiring extensive data for training. This paper investigates the impact of
the relational bottleneck -- a mechanism that focuses processing on relations
among inputs -- on the learning of factorized representations conducive to
compositional coding and the attendant flexibility of processing. We
demonstrate that such a bottleneck not only improves generalization and
learning efficiency, but also aligns network performance with human-like
behavioral biases. Networks trained with the relational bottleneck developed
orthogonal representations of feature dimensions latent in the dataset,
reflecting the factorized structure thought to underlie human cognitive
flexibility. Moreover, the relational network mimics human biases towards
regularity without pre-specified symbolic primitives, suggesting that the
bottleneck fosters the emergence of abstract representations that confer
flexibility akin to symbols.</div><div><a href='http://arxiv.org/abs/2402.18426v1'>2402.18426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15332v1")'>Categorical Deep Learning: An Algebraic Theory of Architectures</div>
<div id='2402.15332v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:01:53Z</div><div>Authors: Bruno GavranoviÄ, Paul Lessard, Andrew Dudzik, Tamara von Glehn, JoÃ£o G. M. AraÃºjo, Petar VeliÄkoviÄ</div><div style='padding-top: 10px; width: 80ex'>We present our position on the elusive quest for a general-purpose framework
for specifying and studying deep learning architectures. Our opinion is that
the key attempts made so far lack a coherent bridge between specifying
constraints which models must satisfy and specifying their implementations.
Focusing on building a such a bridge, we propose to apply category theory --
precisely, the universal algebra of monads valued in a 2-category of parametric
maps -- as a single theory elegantly subsuming both of these flavours of neural
network design. To defend our position, we show how this theory recovers
constraints induced by geometric deep learning, as well as implementations of
many architectures drawn from the diverse landscape of neural networks, such as
RNNs. We also illustrate how the theory naturally encodes many standard
constructs in computer science and automata theory.</div><div><a href='http://arxiv.org/abs/2402.15332v1'>2402.15332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14461v1")'>Marabou 2.0: A Versatile Formal Analyzer of Neural Networks</div>
<div id='2401.14461v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T19:00:25Z</div><div>Authors: Haoze Wu, Omri Isac, Aleksandar ZeljiÄ, Teruhiro Tagomori, Matthew Daggitt, Wen Kokke, Idan Refaeli, Guy Amir, Kyle Julian, Shahaf Bassan, Pei Huang, Ori Lahav, Min Wu, Min Zhang, Ekaterina Komendantskaya, Guy Katz, Clark Barrett</div><div style='padding-top: 10px; width: 80ex'>This paper serves as a comprehensive system description of version 2.0 of the
Marabou framework for formal analysis of neural networks. We discuss the tool's
architectural design and highlight the major features and components introduced
since its initial release.</div><div><a href='http://arxiv.org/abs/2401.14461v1'>2401.14461v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11739v1")'>A Transition System Abstraction Framework for Neural Network Dynamical
  System Models</div>
<div id='2402.11739v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T23:49:18Z</div><div>Authors: Yejiang Yang, Zihao Mo, Hoang-Dung Tran, Weiming Xiang</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a transition system abstraction framework for neural
network dynamical system models to enhance the model interpretability, with
applications to complex dynamical systems such as human behavior learning and
verification. To begin with, the localized working zone will be segmented into
multiple localized partitions under the data-driven Maximum Entropy (ME)
partitioning method. Then, the transition matrix will be obtained based on the
set-valued reachability analysis of neural networks. Finally, applications to
human handwriting dynamics learning and verification are given to validate our
proposed abstraction framework, which demonstrates the advantages of enhancing
the interpretability of the black-box model, i.e., our proposed framework is
able to abstract a data-driven neural network model into a transition system,
making the neural network model interpretable through verifying specifications
described in Computational Tree Logic (CTL) languages.</div><div><a href='http://arxiv.org/abs/2402.11739v1'>2402.11739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11995v1")'>Network Inversion of Binarised Neural Nets</div>
<div id='2402.11995v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:39:54Z</div><div>Authors: Pirzada Suhail, Supratik Chakraborty, Amit Sethi</div><div style='padding-top: 10px; width: 80ex'>While the deployment of neural networks, yielding impressive results, becomes
more prevalent in various applications, their interpretability and
understanding remain a critical challenge. Network inversion, a technique that
aims to reconstruct the input space from the model's learned internal
representations, plays a pivotal role in unraveling the black-box nature of
input to output mappings in neural networks. In safety-critical scenarios,
where model outputs may influence pivotal decisions, the integrity of the
corresponding input space is paramount, necessitating the elimination of any
extraneous "garbage" to ensure the trustworthiness of the network. Binarised
Neural Networks (BNNs), characterized by binary weights and activations, offer
computational efficiency and reduced memory requirements, making them suitable
for resource-constrained environments. This paper introduces a novel approach
to invert a trained BNN by encoding it into a CNF formula that captures the
network's structure, allowing for both inference and inversion.</div><div><a href='http://arxiv.org/abs/2402.11995v1'>2402.11995v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08225v1")'>Efficient and Mathematically Robust Operations for Certified Neural
  Networks Inference</div>
<div id='2401.08225v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T09:22:38Z</div><div>Authors: Fabien Geyer, Johannes Freitag, Tobias Schulz, Sascha Uhrig</div><div style='padding-top: 10px; width: 80ex'>In recent years, machine learning (ML) and neural networks (NNs) have gained
widespread use and attention across various domains, particularly in
transportation for achieving autonomy, including the emergence of flying taxis
for urban air mobility (UAM). However, concerns about certification have come
up, compelling the development of standardized processes encompassing the
entire ML and NN pipeline. This paper delves into the inference stage and the
requisite hardware, highlighting the challenges associated with IEEE 754
floating-point arithmetic and proposing alternative number representations. By
evaluating diverse summation and dot product algorithms, we aim to mitigate
issues related to non-associativity. Additionally, our exploration of
fixed-point arithmetic reveals its advantages over floating-point methods,
demonstrating significant hardware efficiencies. Employing an empirical
approach, we ascertain the optimal bit-width necessary to attain an acceptable
level of accuracy, considering the inherent complexity of bit-width
optimization.</div><div><a href='http://arxiv.org/abs/2401.08225v1'>2401.08225v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12664v1")'>Discriminant Distance-Aware Representation on Deterministic Uncertainty
  Quantification Methods</div>
<div id='2402.12664v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T02:26:48Z</div><div>Authors: Jiaxin Zhang, Kamalika Das, Sricharan Kumar</div><div style='padding-top: 10px; width: 80ex'>Uncertainty estimation is a crucial aspect of deploying dependable deep
learning models in safety-critical systems. In this study, we introduce a novel
and efficient method for deterministic uncertainty estimation called
Discriminant Distance-Awareness Representation (DDAR). Our approach involves
constructing a DNN model that incorporates a set of prototypes in its latent
representations, enabling us to analyze valuable feature information from the
input data. By leveraging a distinction maximization layer over optimal
trainable prototypes, DDAR can learn a discriminant distance-awareness
representation. We demonstrate that DDAR overcomes feature collapse by relaxing
the Lipschitz constraint that hinders the practicality of deterministic
uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a
flexible and architecture-agnostic method that can be easily integrated as a
pluggable layer with distance-sensitive metrics, outperforming state-of-the-art
uncertainty estimation methods on multiple benchmark problems.</div><div><a href='http://arxiv.org/abs/2402.12664v1'>2402.12664v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.07744v2")'>Combining Machine Learning and Ontology: A Systematic Literature Review</div>
<div id='2401.07744v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T14:56:04Z</div><div>Authors: Sarah Ghidalia, Ouassila Labbani Narsis, AurÃ©lie Bertaux, Christophe Nicolle</div><div style='padding-top: 10px; width: 80ex'>Motivated by the desire to explore the process of combining inductive and
deductive reasoning, we conducted a systematic literature review of articles
that investigate the integration of machine learning and ontologies. The
objective was to identify diverse techniques that incorporate both inductive
reasoning (performed by machine learning) and deductive reasoning (performed by
ontologies) into artificial intelligence systems. Our review, which included
the analysis of 128 studies, allowed us to identify three main categories of
hybridization between machine learning and ontologies: learning-enhanced
ontologies, semantic data mining, and learning and reasoning systems. We
provide a comprehensive examination of all these categories, emphasizing the
various machine learning algorithms utilized in the studies. Furthermore, we
compared our classification with similar recent work in the field of hybrid AI
and neuro-symbolic approaches.</div><div><a href='http://arxiv.org/abs/2401.07744v2'>2401.07744v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14726v1")'>Incorporating Expert Rules into Neural Networks in the Framework of
  Concept-Based Learning</div>
<div id='2402.14726v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:33:49Z</div><div>Authors: Andrei V. Konstantinov, Lev V. Utkin</div><div style='padding-top: 10px; width: 80ex'>A problem of incorporating the expert rules into machine learning models for
extending the concept-based learning is formulated in the paper. It is proposed
how to combine logical rules and neural networks predicting the concept
probabilities. The first idea behind the combination is to form constraints for
a joint probability distribution over all combinations of concept values to
satisfy the expert rules. The second idea is to represent a feasible set of
probability distributions in the form of a convex polytope and to use its
vertices or faces. We provide several approaches for solving the stated problem
and for training neural networks which guarantee that the output probabilities
of concepts would not violate the expert rules. The solution of the problem can
be viewed as a way for combining the inductive and deductive learning. Expert
rules are used in a broader sense when any logical function that connects
concepts and class labels or just concepts with each other can be regarded as a
rule. This feature significantly expands the class of the proposed results.
Numerical examples illustrate the approaches. The code of proposed algorithms
is publicly available.</div><div><a href='http://arxiv.org/abs/2402.14726v1'>2402.14726v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13019v1")'>Improving Neural-based Classification with Logical Background Knowledge</div>
<div id='2402.13019v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T14:01:26Z</div><div>Authors: Arthur Ledaguenel, CÃ©line Hudelot, Mostepha Khouadjia</div><div style='padding-top: 10px; width: 80ex'>Neurosymbolic AI is a growing field of research aiming to combine neural
networks learning capabilities with the reasoning abilities of symbolic
systems. This hybridization can take many shapes. In this paper, we propose a
new formalism for supervised multi-label classification with propositional
background knowledge. We introduce a new neurosymbolic technique called
semantic conditioning at inference, which only constrains the system during
inference while leaving the training unaffected. We discuss its theoritical and
practical advantages over two other popular neurosymbolic techniques: semantic
conditioning and semantic regularization. We develop a new multi-scale
methodology to evaluate how the benefits of a neurosymbolic technique evolve
with the scale of the network. We then evaluate experimentally and compare the
benefits of all three techniques across model scales on several datasets. Our
results demonstrate that semantic conditioning at inference can be used to
build more accurate neural-based systems with fewer resources while
guaranteeing the semantic consistency of outputs.</div><div><a href='http://arxiv.org/abs/2402.13019v1'>2402.13019v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09206v1")'>Upper Bound of Bayesian Generalization Error in Partial Concept
  Bottleneck Model (CBM): Partial CBM outperforms naive CBM</div>
<div id='2403.09206v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T09:19:50Z</div><div>Authors: Naoki Hayashi, Yoshihide Sawada</div><div style='padding-top: 10px; width: 80ex'>Concept Bottleneck Model (CBM) is a methods for explaining neural networks.
In CBM, concepts which correspond to reasons of outputs are inserted in the
last intermediate layer as observed values. It is expected that we can
interpret the relationship between the output and concept similar to linear
regression. However, this interpretation requires observing all concepts and
decreases the generalization performance of neural networks. Partial CBM
(PCBM), which uses partially observed concepts, has been devised to resolve
these difficulties. Although some numerical experiments suggest that the
generalization performance of PCBMs is almost as high as that of the original
neural networks, the theoretical behavior of its generalization error has not
been yet clarified since PCBM is singular statistical model. In this paper, we
reveal the Bayesian generalization error in PCBM with a three-layered and
linear architecture. The result indcates that the structure of partially
observed concepts decreases the Bayesian generalization error compared with
that of CBM (full-observed concepts).</div><div><a href='http://arxiv.org/abs/2403.09206v1'>2403.09206v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02870v1")'>Statistics without Interpretation: A Sober Look at Explainable Machine
  Learning</div>
<div id='2402.02870v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:36:48Z</div><div>Authors: Sebastian Bordt, Ulrike von Luxburg</div><div style='padding-top: 10px; width: 80ex'>In the rapidly growing literature on explanation algorithms, it often remains
unclear what precisely these algorithms are for and how they should be used. We
argue that this is because explanation algorithms are often mathematically
complex but don't admit a clear interpretation. Unfortunately, complex
statistical methods that don't have a clear interpretation are bound to lead to
errors in interpretation, a fact that has become increasingly apparent in the
literature. In order to move forward, papers on explanation algorithms should
make clear how precisely the output of the algorithms should be interpreted.
They should also clarify what questions about the function can and cannot be
answered given the explanations. Our argument is based on the distinction
between statistics and their interpretation. It also relies on parallels
between explainable machine learning and applied statistics.</div><div><a href='http://arxiv.org/abs/2402.02870v1'>2402.02870v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18863v2")'>Probabilistic Lipschitzness and the Stable Rank for Comparing
  Explanation Models</div>
<div id='2402.18863v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T05:25:23Z</div><div>Authors: Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew</div><div style='padding-top: 10px; width: 80ex'>Explainability models are now prevalent within machine learning to address
the black-box nature of neural networks. The question now is which
explainability model is most effective. Probabilistic Lipschitzness has
demonstrated that the smoothness of a neural network is fundamentally linked to
the quality of post hoc explanations. In this work, we prove theoretical lower
bounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and
SmoothGrad. We propose a novel metric using probabilistic Lipschitzness,
normalised astuteness, to compare the robustness of explainability models.
Further, we prove a link between the local Lipschitz constant of a neural
network and its stable rank. We then demonstrate that the stable rank of a
neural network provides a heuristic for the robustness of explainability
models.</div><div><a href='http://arxiv.org/abs/2402.18863v2'>2402.18863v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09947v1")'>Explaining Probabilistic Models with Distributional Values</div>
<div id='2402.09947v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T13:50:00Z</div><div>Authors: Luca Franceschi, Michele Donini, CÃ©dric Archambeau, Matthias Seeger</div><div style='padding-top: 10px; width: 80ex'>A large branch of explainable machine learning is grounded in cooperative
game theory. However, research indicates that game-theoretic explanations may
mislead or be hard to interpret. We argue that often there is a critical
mismatch between what one wishes to explain (e.g. the output of a classifier)
and what current methods such as SHAP explain (e.g. the scalar probability of a
class). This paper addresses such gap for probabilistic models by generalising
cooperative games and value operators. We introduce the distributional values,
random variables that track changes in the model output (e.g. flipping of the
predicted class) and derive their analytic expressions for games with Gaussian,
Bernoulli and Categorical payoffs. We further establish several characterising
properties, and show that our framework provides fine-grained and insightful
explanations with case studies on vision and language models.</div><div><a href='http://arxiv.org/abs/2402.09947v1'>2402.09947v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08733v1")'>Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs</div>
<div id='2402.08733v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:01:45Z</div><div>Authors: Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison</div><div style='padding-top: 10px; width: 80ex'>Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the
stochastic real-world process $p(Y|X)$ it was trained on is important to ensure
it avoids producing incorrect or "hallucinated" answers or taking unsafe
actions. But this is difficult for generative models because probabilistic
predictions do not distinguish between per-response noise (aleatoric
uncertainty) and lack of knowledge about the process (epistemic uncertainty),
and existing epistemic uncertainty quantification techniques tend to be
overconfident when the model underfits. We propose a general strategy for
teaching a model to both approximate $p(Y|X)$ and also estimate the remaining
gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict
pairs of independent responses drawn from the true conditional distribution,
allow it to "cheat" by observing one response while predicting the other, then
measure how much it cheats. Remarkably, we prove that being good at cheating
(i.e. cheating whenever it improves your prediction) is equivalent to being
second-order calibrated, a principled extension of ordinary calibration that
allows us to construct provably-correct frequentist confidence intervals for
$p(Y|X)$ and detect incorrect responses with high probability. We demonstrate
empirically that our approach accurately estimates how much models don't know
across ambiguous image classification, (synthetic) language modeling, and
partially-observable navigation tasks, outperforming existing techniques.</div><div><a href='http://arxiv.org/abs/2402.08733v1'>2402.08733v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13740v1")'>Uncertainty-Aware Explanations Through Probabilistic Self-Explainable
  Neural Networks</div>
<div id='2403.13740v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T16:47:28Z</div><div>Authors: Jon Vadillo, Roberto Santana, Jose A. Lozano, Marta Kwiatkowska</div><div style='padding-top: 10px; width: 80ex'>The lack of transparency of Deep Neural Networks continues to be a limitation
that severely undermines their reliability and usage in high-stakes
applications. Promising approaches to overcome such limitations are
Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions
rely on the similarity between the input at hand and a set of prototypical
representations of the output classes, offering therefore a deep, yet
transparent-by-design, architecture. So far, such models have been designed by
considering pointwise estimates for the prototypes, which remain fixed after
the learning phase of the model. In this paper, we introduce a probabilistic
reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for
the prototypes with probability distributions over their values. This provides
not only a more flexible framework for an end-to-end learning of prototypes,
but can also capture the explanatory uncertainty of the model, which is a
missing feature in previous approaches. In addition, since the prototypes
determine both the explanation and the prediction, Prob-PSENNs allow us to
detect when the model is making uninformed or uncertain predictions, and to
obtain valid explanations for them. Our experiments demonstrate that
Prob-PSENNs provide more meaningful and robust explanations than their
non-probabilistic counterparts, thus enhancing the explainability and
reliability of the models.</div><div><a href='http://arxiv.org/abs/2403.13740v1'>2403.13740v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01549v1")'>Towards Modeling Uncertainties of Self-explaining Neural Networks via
  Conformal Prediction</div>
<div id='2401.01549v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T05:51:49Z</div><div>Authors: Wei Qian, Chenxu Zhao, Yangyi Li, Fenglong Ma, Chao Zhang, Mengdi Huai</div><div style='padding-top: 10px; width: 80ex'>Despite the recent progress in deep neural networks (DNNs), it remains
challenging to explain the predictions made by DNNs. Existing explanation
methods for DNNs mainly focus on post-hoc explanations where another
explanatory model is employed to provide explanations. The fact that post-hoc
methods can fail to reveal the actual original reasoning process of DNNs raises
the need to build DNNs with built-in interpretability. Motivated by this, many
self-explaining neural networks have been proposed to generate not only
accurate predictions but also clear and intuitive insights into why a
particular decision was made. However, existing self-explaining networks are
limited in providing distribution-free uncertainty quantification for the two
simultaneously generated prediction outcomes (i.e., a sample's final prediction
and its corresponding explanations for interpreting that prediction).
Importantly, they also fail to establish a connection between the confidence
values assigned to the generated explanations in the interpretation layer and
those allocated to the final predictions in the ultimate prediction layer. To
tackle the aforementioned challenges, in this paper, we design a novel
uncertainty modeling framework for self-explaining networks, which not only
demonstrates strong distribution-free uncertainty modeling performance for the
generated explanations in the interpretation layer but also excels in producing
efficient and effective prediction sets for the final predictions based on the
informative high-level basis explanations. We perform the theoretical analysis
for the proposed framework. Extensive experimental evaluation demonstrates the
effectiveness of the proposed uncertainty framework.</div><div><a href='http://arxiv.org/abs/2401.01549v1'>2401.01549v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14443v1")'>Language Models Can Reduce Asymmetry in Information Markets</div>
<div id='2403.14443v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:48:37Z</div><div>Authors: Nasim Rahaman, Martin Weiss, Manuel WÃ¼thrich, Yoshua Bengio, Li Erran Li, Chris Pal, Bernhard SchÃ¶lkopf</div><div style='padding-top: 10px; width: 80ex'>This work addresses the buyer's inspection paradox for information markets.
The paradox is that buyers need to access information to determine its value,
while sellers need to limit access to prevent theft. To study this, we
introduce an open-source simulated digital marketplace where intelligent
agents, powered by language models, buy and sell information on behalf of
external participants. The central mechanism enabling this marketplace is the
agents' dual capabilities: they not only have the capacity to assess the
quality of privileged information but also come equipped with the ability to
forget. This ability to induce amnesia allows vendors to grant temporary access
to proprietary information, significantly reducing the risk of unauthorized
retention while enabling agents to accurately gauge the information's relevance
to specific queries or tasks. To perform well, agents must make rational
decisions, strategically explore the marketplace through generated sub-queries,
and synthesize answers from purchased information. Concretely, our experiments
(a) uncover biases in language models leading to irrational behavior and
evaluate techniques to mitigate these biases, (b) investigate how price affects
demand in the context of informational goods, and (c) show that inspection and
higher budgets both lead to higher quality outcomes.</div><div><a href='http://arxiv.org/abs/2403.14443v1'>2403.14443v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17200v1")'>NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble
  Techniques</div>
<div id='2401.17200v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:33:35Z</div><div>Authors: Weronika Hryniewska-Guzik, Bartosz Sawicki, PrzemysÅaw Biecek</div><div style='padding-top: 10px; width: 80ex'>This paper presents a comprehensive comparative analysis of explainable
artificial intelligence (XAI) ensembling methods. Our research brings three
significant contributions. Firstly, we introduce a novel ensembling method,
NormEnsembleXAI, that leverages minimum, maximum, and average functions in
conjunction with normalization techniques to enhance interpretability.
Secondly, we offer insights into the strengths and weaknesses of XAI ensemble
methods. Lastly, we provide a library, facilitating the practical
implementation of XAI ensembling, thus promoting the adoption of transparent
and interpretable deep learning models.</div><div><a href='http://arxiv.org/abs/2401.17200v1'>2401.17200v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10373v1")'>Towards a general framework for improving the performance of classifiers
  using XAI methods</div>
<div id='2403.10373v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:04:20Z</div><div>Authors: Andrea Apicella, Salvatore Giugliano, Francesco IsgrÃ², Roberto Prevete</div><div style='padding-top: 10px; width: 80ex'>Modern Artificial Intelligence (AI) systems, especially Deep Learning (DL)
models, poses challenges in understanding their inner workings by AI
researchers. eXplainable Artificial Intelligence (XAI) inspects internal
mechanisms of AI models providing explanations about their decisions. While
current XAI research predominantly concentrates on explaining AI systems, there
is a growing interest in using XAI techniques to automatically improve the
performance of AI systems themselves. This paper proposes a general framework
for automatically improving the performance of pre-trained DL classifiers using
XAI methods, avoiding the computational overhead associated with retraining
complex models from scratch. In particular, we outline the possibility of two
different learning strategies for implementing this architecture, which we will
call auto-encoder-based and encoder-decoder-based, and discuss their key
aspects.</div><div><a href='http://arxiv.org/abs/2403.10373v1'>2403.10373v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01629v1")'>Synthetic Data in AI: Challenges, Applications, and Ethical Implications</div>
<div id='2401.01629v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T09:03:30Z</div><div>Authors: Shuang Hao, Wenfeng Han, Tao Jiang, Yiping Li, Haonan Wu, Chunlin Zhong, Zhangjun Zhou, He Tang</div><div style='padding-top: 10px; width: 80ex'>In the rapidly evolving field of artificial intelligence, the creation and
utilization of synthetic datasets have become increasingly significant. This
report delves into the multifaceted aspects of synthetic data, particularly
emphasizing the challenges and potential biases these datasets may harbor. It
explores the methodologies behind synthetic data generation, spanning
traditional statistical models to advanced deep learning techniques, and
examines their applications across diverse domains. The report also critically
addresses the ethical considerations and legal implications associated with
synthetic datasets, highlighting the urgent need for mechanisms to ensure
fairness, mitigate biases, and uphold ethical standards in AI development.</div><div><a href='http://arxiv.org/abs/2401.01629v1'>2401.01629v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02933v1")'>InterpretCC: Conditional Computation for Inherently Interpretable Neural
  Networks</div>
<div id='2402.02933v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:55:50Z</div><div>Authors: Vinitra Swamy, Julian Blackwell, Jibril Frej, Martin Jaggi, Tanja KÃ¤ser</div><div style='padding-top: 10px; width: 80ex'>Real-world interpretability for neural networks is a tradeoff between three
concerns: 1) it requires humans to trust the explanation approximation (e.g.
post-hoc approaches), 2) it compromises the understandability of the
explanation (e.g. automatically identified feature masks), and 3) it
compromises the model performance (e.g. decision trees). These shortcomings are
unacceptable for human-facing domains, like education, healthcare, or natural
language, which require trustworthy explanations, actionable interpretations,
and accurate predictions. In this work, we present InterpretCC (interpretable
conditional computation), a family of interpretable-by-design neural networks
that guarantee human-centric interpretability while maintaining comparable
performance to state-of-the-art models by adaptively and sparsely activating
features before prediction. We extend this idea into an interpretable
mixture-of-experts model, that allows humans to specify topics of interest,
discretely separates the feature space for each data point into topical
subnetworks, and adaptively and sparsely activates these topical subnetworks.
We demonstrate variations of the InterpretCC architecture for text and tabular
data across several real-world benchmarks: six online education courses, news
classification, breast cancer diagnosis, and review sentiment.</div><div><a href='http://arxiv.org/abs/2402.02933v1'>2402.02933v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07965v1")'>Conditional computation in neural networks: principles and research
  trends</div>
<div id='2403.07965v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T11:56:38Z</div><div>Authors: Simone Scardapane, Alessandro Baiocchi, Alessio Devoto, Valerio Marsocci, Pasquale Minervini, Jary Pomponi</div><div style='padding-top: 10px; width: 80ex'>This article summarizes principles and ideas from the emerging area of
applying \textit{conditional computation} methods to the design of neural
networks. In particular, we focus on neural networks that can dynamically
activate or de-activate parts of their computational graph conditionally on
their input. Examples include the dynamic selection of, e.g., input tokens,
layers (or sets of layers), and sub-modules inside each layer (e.g., channels
in a convolutional filter). We first provide a general formalism to describe
these techniques in an uniform way. Then, we introduce three notable
implementations of these principles: mixture-of-experts (MoEs) networks, token
selection mechanisms, and early-exit neural networks. The paper aims to provide
a tutorial-like introduction to this growing field. To this end, we analyze the
benefits of these modular designs in terms of efficiency, explainability, and
transfer learning, with a focus on emerging applicative areas ranging from
automated scientific discovery to semantic communication.</div><div><a href='http://arxiv.org/abs/2403.07965v1'>2403.07965v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09863v1")'>A Conceptual Framework For White Box Neural Networks</div>
<div id='2403.09863v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T20:50:03Z</div><div>Authors: Maciej Satkiewicz</div><div style='padding-top: 10px; width: 80ex'>This paper introduces semantic features as a general conceptual framework for
fully explainable neural network layers. A well-motivated proof of concept
model for relevant subproblem of MNIST consists of 4 such layers with the total
of 4.8K learnable parameters. The model is easily interpretable, achieves
human-level adversarial test accuracy with no form of adversarial training,
requires little hyperparameter tuning and can be quickly trained on a single
CPU. The general nature of the technique bears promise for a paradigm shift
towards radically democratised and truly generalizable white box neural
networks. The code is available at
https://github.com/314-Foundation/white-box-nn</div><div><a href='http://arxiv.org/abs/2403.09863v1'>2403.09863v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14200v1")'>Debiasing surgeon: fantastic weights and how to find them</div>
<div id='2403.14200v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T07:50:45Z</div><div>Authors: RÃ©mi Nahon, Ivan Luiz De Moura Matos, Van-Tam Nguyen, Enzo Tartaglione</div><div style='padding-top: 10px; width: 80ex'>Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic
biases that can lead to unfair models, emerges. Several debiasing approaches
have been proposed in the realm of deep learning, employing more or less
sophisticated approaches to discourage these models from massively employing
these biases. However, a question emerges: is this extra complexity really
necessary? Is a vanilla-trained model already embodying some ``unbiased
sub-networks'' that can be used in isolation and propose a solution without
relying on the algorithmic biases? In this work, we show that such a
sub-network typically exists, and can be extracted from a vanilla-trained model
without requiring additional training. We further validate that such specific
architecture is incapable of learning a specific bias, suggesting that there
are possible architectural countermeasures to the problem of biases in deep
neural networks.</div><div><a href='http://arxiv.org/abs/2403.14200v1'>2403.14200v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06957v1")'>Architectural Neural Backdoors from First Principles</div>
<div id='2402.06957v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T13:57:51Z</div><div>Authors: Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas Papernot</div><div style='padding-top: 10px; width: 80ex'>While previous research backdoored neural networks by changing their
parameters, recent work uncovered a more insidious threat: backdoors embedded
within the definition of the network's architecture. This involves injecting
common architectural components, such as activation functions and pooling
layers, to subtly introduce a backdoor behavior that persists even after (full
re-)training. However, the full scope and implications of architectural
backdoors have remained largely unexplored. Bober-Irizar et al. [2023]
introduced the first architectural backdoor; they showed how to create a
backdoor for a checkerboard pattern, but never explained how to target an
arbitrary trigger pattern of choice. In this work we construct an arbitrary
trigger detector which can be used to backdoor an architecture with no human
supervision. This leads us to revisit the concept of architecture backdoors and
taxonomise them, describing 12 distinct types. To gauge the difficulty of
detecting such backdoors, we conducted a user study, revealing that ML
developers can only identify suspicious components in common model definitions
as backdoors in 37% of cases, while they surprisingly preferred backdoored
models in 33% of cases. To contextualize these results, we find that language
models outperform humans at the detection of backdoors. Finally, we discuss
defenses against architectural backdoors, emphasizing the need for robust and
comprehensive strategies to safeguard the integrity of ML systems.</div><div><a href='http://arxiv.org/abs/2402.06957v1'>2402.06957v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06122v1")'>Manipulating Feature Visualizations with Gradient Slingshots</div>
<div id='2401.06122v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:57:17Z</div><div>Authors: Dilyara Bareeva, Marina M. -C. HÃ¶hne, Alexander Warnecke, Lukas Pirch, Klaus-Robert MÃ¼ller, Konrad Rieck, Kirill Bykov</div><div style='padding-top: 10px; width: 80ex'>Deep Neural Networks (DNNs) are capable of learning complex and versatile
representations, however, the semantic nature of the learned concepts remains
unknown. A common method used to explain the concepts learned by DNNs is
Activation Maximization (AM), which generates a synthetic input signal that
maximally activates a particular neuron in the network. In this paper, we
investigate the vulnerability of this approach to adversarial model
manipulations and introduce a novel method for manipulating feature
visualization without altering the model architecture or significantly
impacting the model's decision-making process. We evaluate the effectiveness of
our method on several neural network models and demonstrate its capabilities to
hide the functionality of specific neurons by masking the original explanations
of neurons with chosen target explanations during model auditing. As a remedy,
we propose a protective measure against such manipulations and provide
quantitative evidence which substantiates our findings.</div><div><a href='http://arxiv.org/abs/2401.06122v1'>2401.06122v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13388v3")'>Transformer tricks: Precomputing the first layer</div>
<div id='2402.13388v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T21:34:56Z</div><div>Authors: Nils Graef</div><div style='padding-top: 10px; width: 80ex'>This micro-paper describes a trick to speed up inference of transformers with
RoPE (such as LLaMA, Mistral, PaLM, and Gemma). For these models, a large
portion of the first transformer layer can be precomputed, which results in
slightly lower latency and lower cost-per-token. Because this trick optimizes
only one layer, the relative savings depend on the total number of layers. For
example, the maximum savings for a model with only 4 layers (such as Whisper
tiny) is limited to 25%, while a 32-layer model is limited to 3% savings. See
https://github.com/OpenMachine-ai/transformer-tricks for code and more
transformer tricks.</div><div><a href='http://arxiv.org/abs/2402.13388v3'>2402.13388v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12961v1")'>Chatterbox: Robust Transport for LLM Token Streaming under Unstable
  Network</div>
<div id='2401.12961v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T18:45:27Z</div><div>Authors: Hanchen Li, Yuhan Liu, Yihua Cheng, Siddhant Ray, Kuntai Du, Junchen Jiang</div><div style='padding-top: 10px; width: 80ex'>To render each generated token in real time, the LLM server generates
response tokens one by one and streams each generated token (or group of a few
tokens) through the network to the user right after it is generated, which we
refer to as LLM token streaming. However, under unstable network conditions,
the LLM token streaming experience could suffer greatly from stalls since one
packet loss could block the rendering of tokens contained in subsequent packets
even if they arrive on time. With a real-world measurement study, we show that
current applications including ChatGPT, Claude, and Bard all suffer from
increased stall under unstable network.
  For this emerging token streaming problem in LLM Chatbots, we propose a novel
transport layer scheme, called Chatterbox, which puts new generated tokens as
well as currently unacknowledged tokens in the next outgoing packet. This
ensures that each packet contains some new tokens and can be independently
rendered when received, thus avoiding aforementioned stalls caused by missing
packets. Through simulation under various network conditions, we show
Chatterbox reduces stall ratio (proportion of token rendering wait time) by
71.0% compared to the token streaming method commonly used by real chatbot
applications and by 31.6% compared to a custom packet duplication scheme. By
tailoring Chatterbox to fit the token-by-token generation of LLM, we enable the
Chatbots to respond like an eloquent speaker for users to better enjoy
pervasive AI.</div><div><a href='http://arxiv.org/abs/2401.12961v1'>2401.12961v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01790v1")'>An introduction to graphical tensor notation for mechanistic
  interpretability</div>
<div id='2402.01790v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:56:01Z</div><div>Authors: Jordan K. Taylor</div><div style='padding-top: 10px; width: 80ex'>Graphical tensor notation is a simple way of denoting linear operations on
tensors, originating from physics. Modern deep learning consists almost
entirely of operations on or between tensors, so easily understanding tensor
operations is quite important for understanding these systems. This is
especially true when attempting to reverse-engineer the algorithms learned by a
neural network in order to understand its behavior: a field known as
mechanistic interpretability. It's often easy to get confused about which
operations are happening between tensors and lose sight of the overall
structure, but graphical tensor notation makes it easier to parse things at a
glance and see interesting equivalences. The first half of this document
introduces the notation and applies it to some decompositions (SVD, CP, Tucker,
and tensor network decompositions), while the second half applies it to some
existing some foundational approaches for mechanistically understanding
language models, loosely following ``A Mathematical Framework for Transformer
Circuits'', then constructing an example ``induction head'' circuit in
graphical tensor notation.</div><div><a href='http://arxiv.org/abs/2402.01790v1'>2402.01790v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07809v1")'>pyvene: A Library for Understanding and Improving PyTorch Models via
  Interventions</div>
<div id='2403.07809v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T16:46:54Z</div><div>Authors: Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, Christopher Potts</div><div style='padding-top: 10px; width: 80ex'>Interventions on model-internal states are fundamental operations in many
areas of AI, including model editing, steering, robustness, and
interpretability. To facilitate such research, we introduce $\textbf{pyvene}$,
an open-source Python library that supports customizable interventions on a
range of different PyTorch modules. $\textbf{pyvene}$ supports complex
intervention schemes with an intuitive configuration format, and its
interventions can be static or include trainable parameters. We show how
$\textbf{pyvene}$ provides a unified and extensible framework for performing
interventions on neural models and sharing the intervened upon models with
others. We illustrate the power of the library via interpretability analyses
using causal abstraction and knowledge localization. We publish our library
through Python Package Index (PyPI) and provide code, documentation, and
tutorials at https://github.com/stanfordnlp/pyvene.</div><div><a href='http://arxiv.org/abs/2403.07809v1'>2403.07809v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07526v1")'>Editing Arbitrary Propositions in LLMs without Subject Labels</div>
<div id='2401.07526v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T08:08:24Z</div><div>Authors: Itai Feigenbaum, Devansh Arpit, Huan Wang, Shelby Heinecke, Juan Carlos Niebles, Weiran Yao, Caiming Xiong, Silvio Savarese</div><div style='padding-top: 10px; width: 80ex'>Large Language Model (LLM) editing modifies factual information in LLMs.
Locate-and-Edit (L\&amp;E) methods accomplish this by finding where relevant
information is stored within the neural network, and editing the weights at
that location. The goal of editing is to modify the response of an LLM to a
proposition independently of its phrasing, while not modifying its response to
other related propositions. Existing methods are limited to binary
propositions, which represent straightforward binary relations between a
subject and an object. Furthermore, existing methods rely on semantic subject
labels, which may not be available or even be well-defined in practice. In this
paper, we show that both of these issues can be effectively skirted with a
simple and fast localization method called Gradient Tracing (GT). This
localization method allows editing arbitrary propositions instead of just
binary ones, and does so without the need for subject labels. As propositions
always have a truth value, our experiments prompt an LLM as a boolean
classifier, and edit its T/F response to propositions. Our method applies GT
for location tracing, and then edit the model at that location using a mild
variant of Rank-One Model Editing (ROME). On datasets of binary propositions
derived from the CounterFact dataset, we show that our method -- without access
to subject labels -- performs close to state-of-the-art L\&amp;E methods which has
access subject labels. We then introduce a new dataset, Factual Accuracy
Classification Test (FACT), which includes non-binary propositions and for
which subject labels are not generally applicable, and therefore is beyond the
scope of existing L\&amp;E methods. Nevertheless, we show that with our method
editing is possible on FACT.</div><div><a href='http://arxiv.org/abs/2401.07526v1'>2401.07526v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10723v1")'>Conformalized Credal Set Predictors</div>
<div id='2402.10723v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T14:30:12Z</div><div>Authors: Alireza Javanmardi, David Stutz, Eyke HÃ¼llermeier</div><div style='padding-top: 10px; width: 80ex'>Credal sets are sets of probability distributions that are considered as
candidates for an imprecisely known ground-truth distribution. In machine
learning, they have recently attracted attention as an appealing formalism for
uncertainty representation, in particular due to their ability to represent
both the aleatoric and epistemic uncertainty in a prediction. However, the
design of methods for learning credal set predictors remains a challenging
problem. In this paper, we make use of conformal prediction for this purpose.
More specifically, we propose a method for predicting credal sets in the
classification task, given training data labeled by probability distributions.
Since our method inherits the coverage guarantees of conformal prediction, our
conformal credal sets are guaranteed to be valid with high probability (without
any assumptions on model or distribution). We demonstrate the applicability of
our method to natural language inference, a highly ambiguous natural language
task where it is common to obtain multiple annotations per example.</div><div><a href='http://arxiv.org/abs/2402.10723v1'>2402.10723v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03366v1")'>Uncertainty-Aware Explainable Recommendation with Large Language Models</div>
<div id='2402.03366v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T14:06:26Z</div><div>Authors: Yicui Peng, Hao Chen, Chingsheng Lin, Guo Huang, Jinrong Hu, Hui Guo, Bin Kong, Shu Hu, Xi Wu, Xin Wang</div><div style='padding-top: 10px; width: 80ex'>Providing explanations within the recommendation system would boost user
satisfaction and foster trust, especially by elaborating on the reasons for
selecting recommended items tailored to the user. The predominant approach in
this domain revolves around generating text-based explanations, with a notable
emphasis on applying large language models (LLMs). However, refining LLMs for
explainable recommendations proves impractical due to time constraints and
computing resource limitations. As an alternative, the current approach
involves training the prompt rather than the LLM. In this study, we developed a
model that utilizes the ID vectors of user and item inputs as prompts for
GPT-2. We employed a joint training mechanism within a multi-task learning
framework to optimize both the recommendation task and explanation task. This
strategy enables a more effective exploration of users' interests, improving
recommendation effectiveness and user satisfaction. Through the experiments,
our method achieving 1.59 DIV, 0.57 USR and 0.41 FCR on the Yelp, TripAdvisor
and Amazon dataset respectively, demonstrates superior performance over four
SOTA methods in terms of explainability evaluation metric. In addition, we
identified that the proposed model is able to ensure stable textual quality on
the three public datasets.</div><div><a href='http://arxiv.org/abs/2402.03366v1'>2402.03366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04260v1")'>Can Small Language Models be Good Reasoners for Sequential
  Recommendation?</div>
<div id='2403.04260v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T06:49:37Z</div><div>Authors: Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, Xiao Wang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) open up new horizons for sequential
recommendations, owing to their remarkable language comprehension and
generation capabilities. However, there are still numerous challenges that
should be addressed to successfully implement sequential recommendations
empowered by LLMs. Firstly, user behavior patterns are often complex, and
relying solely on one-step reasoning from LLMs may lead to incorrect or
task-irrelevant responses. Secondly, the prohibitively resource requirements of
LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real
sequential recommender systems. In this paper, we propose a novel Step-by-step
knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising
path for sequential recommenders to enjoy the exceptional reasoning
capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We
introduce CoT prompting based on user behavior sequences for the larger teacher
model. The rationales generated by the teacher model are then utilized as
labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In
this way, the student model acquires the step-by-step reasoning capabilities in
recommendation tasks. We encode the generated rationales from the student model
into a dense vector, which empowers recommendation in both ID-based and
ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of
SLIM over state-of-the-art baselines, and further analysis showcasing its
ability to generate meaningful recommendation reasoning at affordable costs.</div><div><a href='http://arxiv.org/abs/2403.04260v1'>2403.04260v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16539v1")'>Integrating Large Language Models with Graphical Session-Based
  Recommendation</div>
<div id='2402.16539v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T12:55:51Z</div><div>Authors: Naicheng Guo, Hongwei Cheng, Qianqiao Liang, Linxun Chen, Bing Han</div><div style='padding-top: 10px; width: 80ex'>With the rapid development of Large Language Models (LLMs), various
explorations have arisen to utilize LLMs capability of context understanding on
recommender systems. While pioneering strategies have primarily transformed
traditional recommendation tasks into challenges of natural language
generation, there has been a relative scarcity of exploration in the domain of
session-based recommendation (SBR) due to its specificity. SBR has been
primarily dominated by Graph Neural Networks, which have achieved many
successful outcomes due to their ability to capture both the implicit and
explicit relationships between adjacent behaviors. The structural nature of
graphs contrasts with the essence of natural language, posing a significant
adaptation gap for LLMs. In this paper, we introduce large language models with
graphical Session-Based recommendation, named LLMGR, an effective framework
that bridges the aforementioned gap by harmoniously integrating LLMs with Graph
Neural Networks (GNNs) for SBR tasks. This integration seeks to leverage the
complementary strengths of LLMs in natural language understanding and GNNs in
relational data processing, leading to a more powerful session-based
recommender system that can understand and recommend items within a session.
Moreover, to endow the LLM with the capability to empower SBR tasks, we design
a series of prompts for both auxiliary and major instruction tuning tasks.
These prompts are crafted to assist the LLM in understanding graph-structured
data and align textual information with nodes, effectively translating nuanced
user interactions into a format that can be understood and utilized by LLM
architectures. Extensive experiments on three real-world datasets demonstrate
that LLMGR outperforms several competitive baselines, indicating its
effectiveness in enhancing SBR tasks and its potential as a research direction
for future exploration.</div><div><a href='http://arxiv.org/abs/2402.16539v1'>2402.16539v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13630v1")'>UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural
  Language</div>
<div id='2402.13630v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:06:31Z</div><div>Authors: Yufei He, Bryan Hooi</div><div style='padding-top: 10px; width: 80ex'>Foundation models like ChatGPT and GPT-4 have revolutionized artificial
intelligence, exhibiting remarkable abilities to generalize across a wide array
of tasks and applications beyond their initial training objectives. However,
when this concept is applied to graph learning, a stark contrast emerges. Graph
learning has predominantly focused on single-graph models, tailored to specific
tasks or datasets, lacking the ability to transfer learned knowledge to
different domains. This limitation stems from the inherent complexity and
diversity of graph structures, along with the different feature and label
spaces specific to graph data. In this paper, we present our UniGraph
framework, designed to train a graph foundation model capable of generalizing
to unseen graphs and tasks across diverse domains. Unlike single-graph models
that use pre-computed node features of varying dimensions as input, our
approach leverages Text-Attributed Graphs (TAGs) for unifying node
representations. We propose a cascaded architecture of Language Models (LMs)
and Graph Neural Networks (GNNs) as backbone networks with a self-supervised
training objective based on Masked Graph Modeling (MGM). We introduce graph
instruction tuning using Large Language Models (LLMs) to enable zero-shot
prediction ability. Our comprehensive experiments across various graph learning
tasks and domains demonstrate the model's effectiveness in self-supervised
representation learning on unseen graphs, few-shot in-context transfer, and
zero-shot transfer, even surpassing or matching the performance of GNNs that
have undergone supervised training on target datasets.</div><div><a href='http://arxiv.org/abs/2402.13630v1'>2402.13630v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08170v2")'>LLaGA: Large Language and Graph Assistant</div>
<div id='2402.08170v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T02:03:26Z</div><div>Authors: Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have empowered the advance in graph-structured
data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4
has heralded a new era in deep learning. However, their application to graph
data poses distinct challenges due to the inherent difficulty of translating
graph structures to language. To this end, we introduce the Large Language and
Graph Assistant (LLaGA), an innovative model that effectively integrates LLM
capabilities to handle the complexities of graph-structured data. LLaGA retains
the general-purpose nature of LLMs while adapting graph data into a format
compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to
structure-aware sequences and then mapping these into the token embedding space
through a versatile projector. LLaGA excels in versatility, generalizability
and interpretability, allowing it to perform consistently well across different
datasets and tasks, extend its ability to unseen datasets or tasks, and provide
explanations for graphs. Our extensive experiments across popular graph
benchmarks show that LLaGA delivers outstanding performance across four
datasets and three tasks using one single model, surpassing state-of-the-art
graph models in both supervised and zero-shot scenarios. Our code is available
at \url{https://github.com/VITA-Group/LLaGA}.</div><div><a href='http://arxiv.org/abs/2402.08170v2'>2402.08170v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15183v4")'>GraphEdit: Large Language Models for Graph Structure Learning</div>
<div id='2402.15183v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:29:42Z</div><div>Authors: Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang</div><div style='padding-top: 10px; width: 80ex'>Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies
and interactions among nodes in graph-structured data by generating novel graph
structures. Graph Neural Networks (GNNs) have emerged as promising GSL
solutions, utilizing recursive message passing to encode node-wise
inter-dependencies. However, many existing GSL methods heavily depend on
explicit graph structural information as supervision signals, leaving them
susceptible to challenges such as data noise and sparsity. In this work, we
propose GraphEdit, an approach that leverages large language models (LLMs) to
learn complex node relationships in graph-structured data. By enhancing the
reasoning capabilities of LLMs through instruction-tuning over graph
structures, we aim to overcome the limitations associated with explicit graph
structural information and enhance the reliability of graph structure learning.
Our approach not only effectively denoises noisy connections but also
identifies node-wise dependencies from a global perspective, providing a
comprehensive understanding of the graph structure. We conduct extensive
experiments on multiple benchmark datasets to demonstrate the effectiveness and
robustness of GraphEdit across various settings. We have made our model
implementation available at: https://github.com/HKUDS/GraphEdit.</div><div><a href='http://arxiv.org/abs/2402.15183v4'>2402.15183v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13496v1")'>HetTree: Heterogeneous Tree Graph Neural Network</div>
<div id='2402.13496v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T03:14:45Z</div><div>Authors: Mingyu Guan, Jack W. Stokes, Qinlong Luo, Fuchen Liu, Purvanshi Mehta, Elnaz Nouri, Taesoo Kim</div><div style='padding-top: 10px; width: 80ex'>The recent past has seen an increasing interest in Heterogeneous Graph Neural
Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from
citation graphs to email graphs. However, existing methods ignore a tree
hierarchy among metapaths, which is naturally constituted by different node
types and relation types. In this paper, we present HetTree, a novel
heterogeneous tree graph neural network that models both the graph structure
and heterogeneous aspects in a scalable and effective manner. Specifically,
HetTree builds a semantic tree data structure to capture the hierarchy among
metapaths. Existing tree encoding techniques aggregate children nodes by
weighting the contribution of children nodes based on similarity to the parent
node. However, we find that this tree encoding fails to capture the entire
parent-children hierarchy by only considering the parent node. Hence, HetTree
uses a novel subtree attention mechanism to emphasize metapaths that are more
helpful in encoding parent-children relationships. Moreover, instead of
separating feature learning from label learning or treating features and labels
equally by projecting them to the same latent space, HetTree proposes to match
them carefully based on corresponding metapaths, which provides more accurate
and richer information between node features and labels. Our evaluation of
HetTree on a variety of real-world datasets demonstrates that it outperforms
all existing baselines on open benchmarks and efficiently scales to large
real-world graphs with millions of nodes and edges.</div><div><a href='http://arxiv.org/abs/2402.13496v1'>2402.13496v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07309v2")'>HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node
  Classification on Text-Attributed Hypergraphs</div>
<div id='2402.07309v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T21:16:26Z</div><div>Authors: AdriÃ¡n Bazaga, Pietro LiÃ², Gos Micklem</div><div style='padding-top: 10px; width: 80ex'>Hypergraphs are marked by complex topology, expressing higher-order
interactions among multiple entities with hyperedges. Lately, hypergraph-based
deep learning methods to learn informative data representations for the problem
of node classification on text-attributed hypergraphs have garnered increasing
research attention. However, existing methods struggle to simultaneously
capture the full extent of hypergraph structural information and the rich
linguistic attributes inherent in the nodes attributes, which largely hampers
their effectiveness and generalizability. To overcome these challenges, we
explore ways to further augment a pretrained BERT model with specialized
hypergraph-aware layers for the task of node classification. Such layers
introduce higher-order structural inductive bias into the language model, thus
improving the model's capacity to harness both higher-order context information
from the hypergraph structure and semantic information present in text. In this
paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model
which simultaneously models hypergraph relational structure while maintaining
the high-quality text encoding capabilities of a pre-trained BERT. Notably,
HyperBERT presents results that achieve a new state-of-the-art on five
challenging text-attributed hypergraph node classification benchmarks.</div><div><a href='http://arxiv.org/abs/2402.07309v2'>2402.07309v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06064v1")'>L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node
  Classification</div>
<div id='2403.06064v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T02:16:13Z</div><div>Authors: Qiuyu Liang, Weihua Wang, Feilong Bao, Guanglai Gao</div><div style='padding-top: 10px; width: 80ex'>Linear Graph Convolutional Networks (GCNs) are used to classify the node in
the graph data. However, we note that most existing linear GCN models perform
neural network operations in Euclidean space, which do not explicitly capture
the tree-like hierarchical structure exhibited in real-world datasets that
modeled as graphs. In this paper, we attempt to introduce hyperbolic space into
linear GCN and propose a novel framework for Lorentzian linear GCN.
Specifically, we map the learned features of graph nodes into hyperbolic space,
and then perform a Lorentzian linear feature transformation to capture the
underlying tree-like structure of data. Experimental results on standard
citation networks datasets with semi-supervised learning show that our approach
yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and
81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be
trained up to two orders of magnitude faster than other nonlinear GCN models on
PubMed dataset. Our code is publicly available at
https://github.com/llqy123/LLGC-master.</div><div><a href='http://arxiv.org/abs/2403.06064v1'>2403.06064v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18875v1")'>Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks</div>
<div id='2402.18875v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T05:44:41Z</div><div>Authors: Zhen Hao Wong, Hansi Yang, Xiaoyi Fu, Quanming Yao</div><div style='padding-top: 10px; width: 80ex'>Heterogeneous Graph Neural Networks (HGNNs) are a class of deep learning
models designed specifically for heterogeneous graphs, which are graphs that
contain different types of nodes and edges. This paper investigates the
application of curriculum learning techniques to improve the performance and
robustness of Heterogeneous Graph Neural Networks (GNNs). To better classify
the quality of the data, we design a loss-aware training schedule, named LTS
that measures the quality of every nodes of the data and incorporate the
training dataset into the model in a progressive manner that increases
difficulty step by step. LTS can be seamlessly integrated into various
frameworks, effectively reducing bias and variance, mitigating the impact of
noisy data, and enhancing overall accuracy. Our findings demonstrate the
efficacy of curriculum learning in enhancing HGNNs capabilities for analyzing
complex graph-structured data. The code is public at https:
//github.com/LARS-research/CLGNN/.</div><div><a href='http://arxiv.org/abs/2402.18875v1'>2402.18875v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02143v1")'>Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy
  and Directions</div>
<div id='2401.02143v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T08:49:10Z</div><div>Authors: Cheng-Te Li, Yu-Che Tsai, Chih-Yao Chen, Jay Chiehen Liao</div><div style='padding-top: 10px; width: 80ex'>In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural
Networks (GNNs), a domain where deep learning-based approaches have
increasingly shown superior performance in both classification and regression
tasks compared to traditional methods. The survey highlights a critical gap in
deep neural TDL methods: the underrepresentation of latent correlations among
data instances and feature values. GNNs, with their innate capability to model
intricate relationships and interactions between diverse elements of tabular
data, have garnered significant interest and application across various TDL
domains. Our survey provides a systematic review of the methods involved in
designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed
investigation into the foundational aspects and an overview of GNN-based TDL
methods, offering insights into their evolving landscape. We present a
comprehensive taxonomy focused on constructing graph structures and
representation learning within GNN-based TDL methods. In addition, the survey
examines various training plans, emphasizing the integration of auxiliary tasks
to enhance the effectiveness of instance representations. A critical part of
our discussion is dedicated to the practical application of GNNs across a
spectrum of GNN4TDL scenarios, demonstrating their versatility and impact.
Lastly, we discuss the limitations and propose future research directions,
aiming to spur advancements in GNN4TDL. This survey serves as a resource for
researchers and practitioners, offering a thorough understanding of GNNs' role
in revolutionizing TDL and pointing towards future innovations in this
promising area.</div><div><a href='http://arxiv.org/abs/2401.02143v1'>2401.02143v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02862v1")'>Graph Neural Machine: A New Model for Learning with Tabular Data</div>
<div id='2402.02862v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:22:15Z</div><div>Authors: Giannis Nikolentzos, Siyun Wang, Johannes Lutzeyer, Michalis Vazirgiannis</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a growing interest in mapping data from
different domains to graph structures. Among others, neural network models such
as the multi-layer perceptron (MLP) can be modeled as graphs. In fact, MLPs can
be represented as directed acyclic graphs. Graph neural networks (GNNs) have
recently become the standard tool for performing machine learning tasks on
graphs. In this work, we show that an MLP is equivalent to an asynchronous
message passing GNN model which operates on the MLP's graph representation. We
then propose a new machine learning model for tabular data, the so-called Graph
Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a
nearly complete graph and which employs a synchronous message passing scheme.
We show that a single GNM model can simulate multiple MLP models. We evaluate
the proposed model in several classification and regression datasets. In most
cases, the GNM model outperforms the MLP architecture.</div><div><a href='http://arxiv.org/abs/2402.02862v1'>2402.02862v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13849v1")'>Graphs Unveiled: Graph Neural Networks and Graph Generation</div>
<div id='2403.13849v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:37:27Z</div><div>Authors: LÃ¡szlÃ³ KovÃ¡cs, Ali Jlidi</div><div style='padding-top: 10px; width: 80ex'>One of the hot topics in machine learning is the field of GNN. The complexity
of graph data has imposed significant challenges on existing machine learning
algorithms. Recently, many studies on extending deep learning approaches for
graph data have emerged. This paper represents a survey, providing a
comprehensive overview of Graph Neural Networks (GNNs). We discuss the
applications of graph neural networks across various domains. Finally, we
present an advanced field in GNNs: graph generation.</div><div><a href='http://arxiv.org/abs/2403.13849v1'>2403.13849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01626v2")'>On the Expressive Power of Graph Neural Networks</div>
<div id='2401.01626v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T08:54:56Z</div><div>Authors: Ashwin Nalwade, Kelly Marshall, Axel Eladi, Umang Sharma</div><div style='padding-top: 10px; width: 80ex'>The study of Graph Neural Networks has received considerable interest in the
past few years. By extending deep learning to graph-structured data, GNNs can
solve a diverse set of tasks in fields including social science, chemistry, and
medicine. The development of GNN architectures has largely been focused on
improving empirical performance on tasks like node or graph classification.
However, a line of recent work has instead sought to find GNN architectures
that have desirable theoretical properties - by studying their expressive power
and designing architectures that maximize this expressiveness.
  While there is no consensus on the best way to define the expressiveness of a
GNN, it can be viewed from several well-motivated perspectives. Perhaps the
most natural approach is to study the universal approximation properties of
GNNs, much in the way that this has been studied extensively for MLPs. Another
direction focuses on the extent to which GNNs can distinguish between different
graph structures, relating this to the graph isomorphism test. Besides, a GNN's
ability to compute graph properties such as graph moments has been suggested as
another form of expressiveness. All of these different definitions are
complementary and have yielded different recommendations for GNN architecture
choices. In this paper, we would like to give an overview of the notion of
"expressive power" of GNNs and provide some valuable insights regarding the
design choices of GNNs.</div><div><a href='http://arxiv.org/abs/2401.01626v2'>2401.01626v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08514v1")'>Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN
  Expressiveness</div>
<div id='2401.08514v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:23:23Z</div><div>Authors: Bohang Zhang, Jingchu Gai, Yiheng Du, Qiwei Ye, Di He, Liwei Wang</div><div style='padding-top: 10px; width: 80ex'>Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in
the graph learning community. So far, GNN expressiveness has been primarily
assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an
expressivity measure has notable limitations: it is inherently coarse,
qualitative, and may not well reflect practical requirements (e.g., the ability
to encode substructures). In this paper, we introduce a unified framework for
quantitatively studying the expressiveness of GNN architectures, addressing all
the above limitations. Specifically, we identify a fundamental expressivity
measure termed homomorphism expressivity, which quantifies the ability of GNN
models to count graphs under homomorphism. Homomorphism expressivity offers a
complete and practical assessment tool: the completeness enables direct
expressivity comparisons between GNN models, while the practicality allows for
understanding concrete GNN abilities such as subgraph counting. By examining
four classes of prominent GNNs as case studies, we derive simple, unified, and
elegant descriptions of their homomorphism expressivity for both invariant and
equivariant settings. Our results provide novel insights into a series of
previous work, unify the landscape of different subareas in the community, and
settle several open questions. Empirically, extensive experiments on both
synthetic and real-world tasks verify our theory, showing that the practical
performance of GNN models aligns well with the proposed metric.</div><div><a href='http://arxiv.org/abs/2401.08514v1'>2401.08514v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08595v3")'>Homomorphism Counts for Graph Neural Networks: All About That Basis</div>
<div id='2402.08595v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:57:06Z</div><div>Authors: Emily Jin, Michael Bronstein, Ismail Ilkan Ceylan, Matthias Lanzinger</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks are architectures for learning invariant functions over
graphs. A large body of work has investigated the properties of graph neural
networks and identified several limitations, particularly pertaining to their
expressive power. Their inability to count certain patterns (e.g., cycles) in a
graph lies at the heart of such limitations, since many functions to be learned
rely on the ability of counting such patterns. Two prominent paradigms aim to
address this limitation by enriching the graph features with subgraph or
homomorphism pattern counts. In this work, we show that both of these
approaches are sub-optimal in a certain sense and argue for a more fine-grained
approach, which incorporates the homomorphism counts of all structures in the
"basis" of the target pattern. This yields strictly more expressive
architectures without incurring any additional overhead in terms of
computational complexity compared to existing approaches. We prove a series of
theoretical results on node-level and graph-level motif parameters and
empirically validate them on standard benchmark datasets.</div><div><a href='http://arxiv.org/abs/2402.08595v3'>2402.08595v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12143v2")'>Graph Neural Networks for Learning Equivariant Representations of Neural
  Networks</div>
<div id='2403.12143v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T18:01:01Z</div><div>Authors: Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang</div><div style='padding-top: 10px; width: 80ex'>Neural networks that process the parameters of other neural networks find
applications in domains as diverse as classifying implicit neural
representations, generating neural network weights, and predicting
generalization errors. However, existing approaches either overlook the
inherent permutation symmetry in the neural network or rely on intricate
weight-sharing patterns to achieve equivariance, while ignoring the impact of
the network architecture itself. In this work, we propose to represent neural
networks as computational graphs of parameters, which allows us to harness
powerful graph neural networks and transformers that preserve permutation
symmetry. Consequently, our approach enables a single model to encode neural
computational graphs with diverse architectures. We showcase the effectiveness
of our method on a wide range of tasks, including classification and editing of
implicit neural representations, predicting generalization performance, and
learning to optimize, while consistently outperforming state-of-the-art
methods. The source code is open-sourced at
https://github.com/mkofinas/neural-graphs.</div><div><a href='http://arxiv.org/abs/2403.12143v2'>2403.12143v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03332v1")'>Cyclic Neural Network</div>
<div id='2402.03332v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T07:31:53Z</div><div>Authors: Liangwei Yang, Hengrui Zhang, Zihe Song, Jiawei Zhang, Weizhi Zhang, Jing Ma, Philip S. Yu</div><div style='padding-top: 10px; width: 80ex'>This paper answers a fundamental question in artificial neural network (ANN)
design: We do not need to build ANNs layer-by-layer sequentially to guarantee
the Directed Acyclic Graph (DAG) property. Drawing inspiration from biological
intelligence (BI), where neurons form a complex, graph-structured network, we
introduce the groundbreaking Cyclic Neural Networks (Cyclic NNs). It emulates
the flexible and dynamic graph nature of biological neural systems, allowing
neuron connections in any graph-like structure, including cycles. This offers
greater adaptability compared to the DAG structure of current ANNs. We further
develop the Graph Over Multi-layer Perceptron, which is the first detailed
model based on this new design paradigm. Experimental validation of the Cyclic
NN's advantages on widely tested datasets in most generalized cases,
demonstrating its superiority over current BP training methods through the use
of a forward-forward (FF) training algorithm. This research illustrates a
totally new ANN design paradigm, which is a significant departure from current
ANN designs, potentially leading to more biologically plausible AI systems.</div><div><a href='http://arxiv.org/abs/2402.03332v1'>2402.03332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11100v1")'>Graph Expansion in Pruned Recurrent Neural Network Layers Preserve
  Performance</div>
<div id='2403.11100v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T06:08:08Z</div><div>Authors: Suryam Arnav Kalra, Arindam Biswas, Pabitra Mitra, Biswajit Basu</div><div style='padding-top: 10px; width: 80ex'>Expansion property of a graph refers to its strong connectivity as well as
sparseness. It has been reported that deep neural networks can be pruned to a
high degree of sparsity while maintaining their performance. Such pruning is
essential for performing real time sequence learning tasks using recurrent
neural networks in resource constrained platforms. We prune recurrent networks
such as RNNs and LSTMs, maintaining a large spectral gap of the underlying
graphs and ensuring their layerwise expansion properties. We also study the
time unfolded recurrent network graphs in terms of the properties of their
bipartite layers. Experimental results for the benchmark sequence MNIST,
CIFAR-10, and Google speech command data show that expander graph properties
are key to preserving classification accuracy of RNN and LSTM.</div><div><a href='http://arxiv.org/abs/2403.11100v1'>2403.11100v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06079v1")'>Generalization of Graph Neural Networks through the Lens of Homomorphism</div>
<div id='2403.06079v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T03:51:59Z</div><div>Authors: Shouheng Li, Dongwoo Kim, Qing Wang</div><div style='padding-top: 10px; width: 80ex'>Despite the celebrated popularity of Graph Neural Networks (GNNs) across
numerous applications, the ability of GNNs to generalize remains less explored.
In this work, we propose to study the generalization of GNNs through a novel
perspective - analyzing the entropy of graph homomorphism. By linking graph
homomorphism with information-theoretic measures, we derive generalization
bounds for both graph and node classifications. These bounds are capable of
capturing subtleties inherent in various graph structures, including but not
limited to paths, cycles and cliques. This enables a data-dependent
generalization analysis with robust theoretical guarantees. To shed light on
the generality of of our proposed bounds, we present a unifying framework that
can characterize a broad spectrum of GNN models through the lens of graph
homomorphism. We validate the practical applicability of our theoretical
findings by showing the alignment between the proposed bounds and the
empirically observed generalization gaps over both real-world and synthetic
datasets.</div><div><a href='http://arxiv.org/abs/2403.06079v1'>2403.06079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07025v1")'>Generalization Error of Graph Neural Networks in the Mean-field Regime</div>
<div id='2402.07025v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T19:12:31Z</div><div>Authors: Gholamali Aminian, Yixuan He, Gesine Reinert, Åukasz Szpruch, Samuel N. Cohen</div><div style='padding-top: 10px; width: 80ex'>This work provides a theoretical framework for assessing the generalization
error of graph classification tasks via graph neural networks in the
over-parameterized regime, where the number of parameters surpasses the
quantity of data points. We explore two widely utilized types of graph neural
networks: graph convolutional neural networks and message passing graph neural
networks. Prior to this study, existing bounds on the generalization error in
the over-parametrized regime were uninformative, limiting our understanding of
over-parameterized network performance. Our novel approach involves deriving
upper bounds within the mean-field regime for evaluating the generalization
error of these graph neural networks. We establish upper bounds with a
convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These
upper bounds offer a theoretical assurance of the networks' performance on
unseen data in the challenging over-parameterized regime and overall contribute
to our understanding of their performance.</div><div><a href='http://arxiv.org/abs/2402.07025v1'>2402.07025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12362v1")'>VC dimension of Graph Neural Networks with Pfaffian activation functions</div>
<div id='2401.12362v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T21:11:22Z</div><div>Authors: Giuseppe Alessio D'Inverno, Monica Bianchini, Franco Scarselli</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool
to learn tasks across a wide range of graph domains in a data-driven fashion;
based on a message passing mechanism, GNNs have gained increasing popularity
due to their intuitive formulation, closely linked with the Weisfeiler-Lehman
(WL) test for graph isomorphism, to which they have proven equivalent. From a
theoretical point of view, GNNs have been shown to be universal approximators,
and their generalization capability (namely, bounds on the Vapnik Chervonekis
(VC) dimension) has recently been investigated for GNNs with piecewise
polynomial activation functions. The aim of our work is to extend this analysis
on the VC dimension of GNNs to other commonly used activation functions, such
as sigmoid and hyperbolic tangent, using the framework of Pfaffian function
theory. Bounds are provided with respect to architecture parameters (depth,
number of neurons, input size) as well as with respect to the number of colors
resulting from the 1-WL test applied on the graph domain. The theoretical
analysis is supported by a preliminary experimental study.</div><div><a href='http://arxiv.org/abs/2401.12362v1'>2401.12362v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17805v1")'>Graph Neural Networks and Arithmetic Circuits</div>
<div id='2402.17805v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T11:04:06Z</div><div>Authors: Timon Barlag, Vivian Holzapfel, Laura Strieker, Jonni Virtema, Heribert Vollmer</div><div style='padding-top: 10px; width: 80ex'>We characterize the computational power of neural networks that follow the
graph neural network (GNN) architecture, not restricted to aggregate-combine
GNNs or other particular types. We establish an exact correspondence between
the expressivity of GNNs using diverse activation functions and arithmetic
circuits over real numbers. In our results the activation function of the
network becomes a gate type in the circuit. Our result holds for families of
constant depth circuits and networks, both uniformly and non-uniformly, for all
common activation functions.</div><div><a href='http://arxiv.org/abs/2402.17805v1'>2402.17805v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09469v1")'>Fourier Circuits in Neural Networks: Unlocking the Potential of Large
  Language Models in Mathematical Reasoning and Modular Arithmetic</div>
<div id='2402.09469v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T05:52:06Z</div><div>Authors: Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou</div><div style='padding-top: 10px; width: 80ex'>In the evolving landscape of machine learning, a pivotal challenge lies in
deciphering the internal representations harnessed by neural networks and
Transformers. Building on recent progress toward comprehending how networks
execute distinct target functions, our study embarks on an exploration of the
underlying reasons behind networks adopting specific computational strategies.
We direct our focus to the complex algebraic learning task of modular addition
involving $k$ inputs. Our research presents a thorough analytical
characterization of the features learned by stylized one-hidden layer neural
networks and one-layer Transformers in addressing this task.
  A cornerstone of our theoretical framework is the elucidation of how the
principle of margin maximization shapes the features adopted by one-hidden
layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of
modular arithmetic with $k$ inputs and $m$ denote the network width. We
demonstrate that a neuron count of $ m \geq 2^{2k-2} \cdot (p-1) $, these
networks attain a maximum $ L_{2,k+1} $-margin on the dataset $ D_p $.
Furthermore, we establish that each hidden-layer neuron aligns with a specific
Fourier spectrum, integral to solving modular addition problems.
  By correlating our findings with the empirical observations of similar
studies, we contribute to a deeper comprehension of the intrinsic computational
mechanisms of neural networks. Furthermore, we observe similar computational
mechanisms in the attention matrix of the Transformer. This research stands as
a significant stride in unraveling their operation complexities, particularly
in the realm of complex algebraic tasks.</div><div><a href='http://arxiv.org/abs/2402.09469v1'>2402.09469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16726v2")'>Interpreting Grokked Transformers in Complex Modular Arithmetic</div>
<div id='2402.16726v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T16:48:12Z</div><div>Authors: Hiroki Furuta, Gouki Minegishi, Yusuke Iwasawa, Yutaka Matsuo</div><div style='padding-top: 10px; width: 80ex'>Grokking has been actively explored to reveal the mystery of delayed
generalization. Identifying interpretable algorithms inside the grokked models
is a suggestive hint to understanding its mechanism. In this work, beyond the
simplest and well-studied modular addition, we observe the internal circuits
learned through grokking in complex modular arithmetic via interpretable
reverse engineering, which highlights the significant difference in their
dynamics: subtraction poses a strong asymmetry on Transformer; multiplication
requires cosine-biased components at all the frequencies in a Fourier domain;
polynomials often result in the superposition of the patterns from elementary
arithmetic, but clear patterns do not emerge in challenging cases; grokking can
easily occur even in higher-degree formulas with basic symmetric and
alternating expressions. We also introduce the novel progress measure for
modular arithmetic; Fourier Frequency Sparsity and Fourier Coefficient Ratio,
which not only indicate the late generalization but also characterize
distinctive internal representations of grokked models per modular operation.
Our empirical analysis emphasizes the importance of holistic evaluation among
various combinations.</div><div><a href='http://arxiv.org/abs/2402.16726v2'>2402.16726v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13001v2")'>A unifying primary framework for quantum graph neural networks from
  quantum graph states</div>
<div id='2402.13001v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T13:32:00Z</div><div>Authors: Ammar Daskin</div><div style='padding-top: 10px; width: 80ex'>Graph states are used to represent mathematical graphs as quantum states on
quantum computers. They can be formulated through stabilizer codes or directly
quantum gates and quantum states. In this paper we show that a quantum graph
neural network model can be understood and realized based on graph states. We
show that they can be used either as a parameterized quantum circuits to
represent neural networks or as an underlying structure to construct graph
neural networks on quantum computers.</div><div><a href='http://arxiv.org/abs/2402.13001v2'>2402.13001v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00485v1")'>A Survey of Geometric Graph Neural Networks: Data Structures, Models and
  Applications</div>
<div id='2403.00485v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T12:13:04Z</div><div>Authors: Jiaqi Han, Jiacheng Cen, Liming Wu, Zongzhao Li, Xiangzhe Kong, Rui Jiao, Ziyang Yu, Tingyang Xu, Fandi Wu, Zihe Wang, Hongteng Xu, Zhewei Wei, Yang Liu, Yu Rong, Wenbing Huang</div><div style='padding-top: 10px; width: 80ex'>Geometric graph is a special kind of graph with geometric features, which is
vital to model many scientific problems. Unlike generic graphs, geometric
graphs often exhibit physical symmetries of translations, rotations, and
reflections, making them ineffectively processed by current Graph Neural
Networks (GNNs). To tackle this issue, researchers proposed a variety of
Geometric Graph Neural Networks equipped with invariant/equivariant properties
to better characterize the geometry and topology of geometric graphs. Given the
current progress in this field, it is imperative to conduct a comprehensive
survey of data structures, models, and applications related to geometric GNNs.
In this paper, based on the necessary but concise mathematical preliminaries,
we provide a unified view of existing models from the geometric message passing
perspective. Additionally, we summarize the applications as well as the related
datasets to facilitate later research for methodology development and
experimental evaluation. We also discuss the challenges and future potential
directions of Geometric GNNs at the end of this survey.</div><div><a href='http://arxiv.org/abs/2403.00485v1'>2403.00485v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08871v1")'>Position Paper: Challenges and Opportunities in Topological Deep
  Learning</div>
<div id='2402.08871v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T00:35:10Z</div><div>Authors: Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson, Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro LiÃ², Paolo Di Lorenzo, Vasileios Maroulas, Nina Miolane, Farzana Nasrin, Karthikeyan Natesan Ramamurthy, Bastian Rieck, Simone Scardapane, Michael T. Schaub, Petar VeliÄkoviÄ, Bei Wang, Yusu Wang, Guo-Wei Wei, Ghada Zamzmi</div><div style='padding-top: 10px; width: 80ex'>Topological deep learning (TDL) is a rapidly evolving field that uses
topological features to understand and design deep learning models. This paper
posits that TDL may complement graph representation learning and geometric deep
learning by incorporating topological concepts, and can thus provide a natural
choice for various machine learning settings. To this end, this paper discusses
open problems in TDL, ranging from practical benefits to theoretical
foundations. For each problem, it outlines potential solutions and future
research opportunities. At the same time, this paper serves as an invitation to
the scientific community to actively participate in TDL research to unlock the
potential of this emerging field.</div><div><a href='http://arxiv.org/abs/2402.08871v1'>2402.08871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08480v1")'>Revealing Decurve Flows for Generalized Graph Propagation</div>
<div id='2402.08480v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T14:13:17Z</div><div>Authors: Chen Lin, Liheng Ma, Yiyang Chen, Wanli Ouyang, Michael M. Bronstein, Philip H. S. Torr</div><div style='padding-top: 10px; width: 80ex'>This study addresses the limitations of the traditional analysis of
message-passing, central to graph learning, by defining {\em
\textbf{generalized propagation}} with directed and weighted graphs. The
significance manifest in two ways. \textbf{Firstly}, we propose {\em
Generalized Propagation Neural Networks} (\textbf{GPNNs}), a framework that
unifies most propagation-based graph neural networks. By generating
directed-weighted propagation graphs with adjacency function and connectivity
function, GPNNs offer enhanced insights into attention mechanisms across
various graph models. We delve into the trade-offs within the design space with
empirical experiments and emphasize the crucial role of the adjacency function
for model expressivity via theoretical analysis. \textbf{Secondly}, we propose
the {\em Continuous Unified Ricci Curvature} (\textbf{CURC}), an extension of
celebrated {\em Ollivier-Ricci Curvature} for directed and weighted graphs.
Theoretically, we demonstrate that CURC possesses continuity, scale invariance,
and a lower bound connection with the Dirichlet isoperimetric constant
validating bottleneck analysis for GPNNs. We include a preliminary exploration
of learned propagation patterns in datasets, a first in the field. We observe
an intriguing ``{\em \textbf{decurve flow}}'' - a curvature reduction during
training for models with learnable propagation, revealing the evolution of
propagation over time and a deeper connection to over-smoothing and bottleneck
trade-off.</div><div><a href='http://arxiv.org/abs/2402.08480v1'>2402.08480v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07568v1")'>Weisfeiler-Leman at the margin: When more expressivity matters</div>
<div id='2402.07568v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:03:52Z</div><div>Authors: Billy J. Franks, Christopher Morris, Ameya Velingker, Floris Geerts</div><div style='padding-top: 10px; width: 80ex'>The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the
graph isomorphism problem. Recently, the algorithm has played a prominent role
in understanding the expressive power of message-passing graph neural networks
(MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL
faces challenges in distinguishing non-isomorphic graphs, leading to the
development of more expressive MPNN and kernel architectures. However, the
relationship between enhanced expressivity and improved generalization
performance remains unclear. Here, we show that an architecture's expressivity
offers limited insights into its generalization performance when viewed through
graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with
subgraph information and employ classical margin theory to investigate the
conditions under which an architecture's increased expressivity aligns with
improved generalization performance. In addition, we show that gradient flow
pushes the MPNN's weights toward the maximum margin solution. Further, we
introduce variations of expressive $1$-WL-based kernel and MPNN architectures
with provable generalization properties. Our empirical study confirms the
validity of our theoretical findings.</div><div><a href='http://arxiv.org/abs/2402.07568v1'>2402.07568v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02287v1")'>Future Directions in Foundations of Graph Machine Learning</div>
<div id='2402.02287v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T22:55:31Z</div><div>Authors: Christopher Morris, Nadav Dym, Haggai Maron, Ä°smail Ä°lkan Ceylan, Fabrizio Frasca, Ron Levie, Derek Lim, Michael Bronstein, Martin Grohe, Stefanie Jegelka</div><div style='padding-top: 10px; width: 80ex'>Machine learning on graphs, especially using graph neural networks (GNNs),
has seen a surge in interest due to the wide availability of graph data across
a broad spectrum of disciplines, from life to social and engineering sciences.
Despite their practical success, our theoretical understanding of the
properties of GNNs remains highly incomplete. Recent theoretical advancements
primarily focus on elucidating the coarse-grained expressive power of GNNs,
predominantly employing combinatorial techniques. However, these studies do not
perfectly align with practice, particularly in understanding the generalization
behavior of GNNs when trained with stochastic first-order optimization
techniques. In this position paper, we argue that the graph machine learning
community needs to shift its attention to developing a more balanced theory of
graph machine learning, focusing on a more thorough understanding of the
interplay of expressive power, generalization, and optimization.</div><div><a href='http://arxiv.org/abs/2402.02287v1'>2402.02287v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04763v1")'>BloomGML: Graph Machine Learning through the Lens of Bilevel
  Optimization</div>
<div id='2403.04763v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:57:46Z</div><div>Authors: Amber Yijia Zheng, Tong He, Yixuan Qiu, Minjie Wang, David Wipf</div><div style='padding-top: 10px; width: 80ex'>Bilevel optimization refers to scenarios whereby the optimal solution of a
lower-level energy function serves as input features to an upper-level
objective of interest. These optimal features typically depend on tunable
parameters of the lower-level energy in such a way that the entire bilevel
pipeline can be trained end-to-end. Although not generally presented as such,
this paper demonstrates how a variety of graph learning techniques can be
recast as special cases of bilevel optimization or simplifications thereof. In
brief, building on prior work we first derive a more flexible class of energy
functions that, when paired with various descent steps (e.g., gradient descent,
proximal methods, momentum, etc.), form graph neural network (GNN)
message-passing layers; critically, we also carefully unpack where any residual
approximation error lies with respect to the underlying constituent
message-passing functions. We then probe several simplifications of this
framework to derive close connections with non-GNN-based graph learning
approaches, including knowledge graph embeddings, various forms of label
propagation, and efficient graph-regularized MLP models. And finally, we
present supporting empirical results that demonstrate the versatility of the
proposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel
Optimization Offers More Graph Machine Learning. Our code is available at
https://github.com/amberyzheng/BloomGML. Let graph ML bloom.</div><div><a href='http://arxiv.org/abs/2403.04763v1'>2403.04763v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06687v1")'>Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and
  Attention Mechanism Approach for Heterogeneous Graph-Structured Data</div>
<div id='2403.06687v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T13:04:21Z</div><div>Authors: Jinghan Huang, Qiufeng Chen, Yijun Bian, Pengli Zhu, Nanguang Chen, Moo K. Chung, Anqi Qiu</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have proven effective in capturing relationships
among nodes in a graph. This study introduces a novel perspective by
considering a graph as a simplicial complex, encompassing nodes, edges,
triangles, and $k$-simplices, enabling the definition of graph-structured data
on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous
graph attention network (HL-HGAT), designed to learn heterogeneous signal
representations across $k$-simplices. The HL-HGAT incorporates three key
components: HL convolutional filters (HL-filters), simplicial projection (SP),
and simplicial attention pooling (SAP) operators, applied to $k$-simplices.
HL-filters leverage the unique topology of $k$-simplices encoded by the
Hodge-Laplacian (HL) operator, operating within the spectral domain of the
$k$-th HL operator. To address computation challenges, we introduce a
polynomial approximation for HL-filters, exhibiting spatial localization
properties. Additionally, we propose a pooling operator to coarsen
$k$-simplices, combining features through simplicial attention mechanisms of
self-attention and cross-attention via transformers and SP operators, capturing
topological interconnections across multiple dimensions of simplices. The
HL-HGAT is comprehensively evaluated across diverse graph applications,
including NP-hard problems, graph multi-label and classification challenges,
and graph regression tasks in logistics, computer vision, biology, chemistry,
and neuroscience. The results demonstrate the model's efficacy and versatility
in handling a wide range of graph-based scenarios.</div><div><a href='http://arxiv.org/abs/2403.06687v1'>2403.06687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13221v2")'>CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset
  for Advancing Graph Machine Learning</div>
<div id='2402.13221v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:32:27Z</div><div>Authors: Ulrik Friis-Jensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam, Kirsten M. Ã. Jensen, Raghavendra Selvan</div><div style='padding-top: 10px; width: 80ex'>Advances in graph machine learning (ML) have been driven by applications in
chemistry as graphs have remained the most expressive representations of
molecules. While early graph ML methods focused primarily on small organic
molecules, recently, the scope of graph ML has expanded to include inorganic
materials. Modelling the periodicity and symmetry of inorganic crystalline
materials poses unique challenges, which existing graph ML methods are unable
to address. Moving to inorganic nanomaterials increases complexity as the scale
of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of
existing graph ML focuses on characterising molecules and materials by
predicting target properties with graphs as input. However, the most exciting
applications of graph ML will be in their generative capabilities, which is
currently not at par with other domains such as images or text.
  We invite the graph ML community to address these open challenges by
presenting two new chemically-informed large-scale inorganic (CHILI)
nanomaterials datasets: A medium-scale dataset (with overall &gt;6M nodes, &gt;49M
edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal
types (CHILI-3K) and a large-scale dataset (with overall &gt;183M nodes, &gt;1.2B
edges) of nanomaterials generated from experimentally determined crystal
structures (CHILI-100K). We define 11 property prediction tasks and 6 structure
prediction tasks, which are of special interest for nanomaterial research. We
benchmark the performance of a wide array of baseline methods and use these
benchmarking results to highlight areas which need future work. To the best of
our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial
datasets of this scale -- both on the individual graph level and of the dataset
as a whole -- and the only nanomaterials datasets with high structural and
elemental diversity.</div><div><a href='http://arxiv.org/abs/2402.13221v2'>2402.13221v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11857v1")'>Complete and Efficient Graph Transformers for Crystal Material Property
  Prediction</div>
<div id='2403.11857v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:06:37Z</div><div>Authors: Keqiang Yan, Cong Fu, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji</div><div style='padding-top: 10px; width: 80ex'>Crystal structures are characterized by atomic bases within a primitive unit
cell that repeats along a regular lattice throughout 3D space. The periodic and
infinite nature of crystals poses unique challenges for geometric graph
representation learning. Specifically, constructing graphs that effectively
capture the complete geometric information of crystals and handle chiral
crystals remains an unsolved and challenging problem. In this paper, we
introduce a novel approach that utilizes the periodic patterns of unit cells to
establish the lattice-based representation for each atom, enabling efficient
and expressive graph representations of crystals. Furthermore, we propose
ComFormer, a SE(3) transformer designed specifically for crystalline materials.
ComFormer includes two variants; namely, iComFormer that employs invariant
geometric descriptors of Euclidean distances and angles, and eComFormer that
utilizes equivariant vector representations. Experimental results demonstrate
the state-of-the-art predictive accuracy of ComFormer variants on various tasks
across three widely-used crystal benchmarks. Our code is publicly available as
part of the AIRS library (https://github.com/divelab/AIRS).</div><div><a href='http://arxiv.org/abs/2403.11857v1'>2403.11857v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15089v1")'>Accelerating Material Property Prediction using Generically Complete
  Isometry Invariants</div>
<div id='2401.15089v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T15:14:22Z</div><div>Authors: Jonathan Balasingham, Viktor Zamaraev, Vitaliy Kurlin</div><div style='padding-top: 10px; width: 80ex'>Material or crystal property prediction using machine learning has grown
popular in recent years as it provides a computationally efficient replacement
to classical simulation methods. A crucial first step for any of these
algorithms is the representation used for a periodic crystal. While similar
objects like molecules and proteins have a finite number of atoms and their
representation can be built based upon a finite point cloud interpretation,
periodic crystals are unbounded in size, making their representation more
challenging. In the present work, we adapt the Pointwise Distance Distribution
(PDD), a continuous and generically complete isometry invariant for periodic
point sets, as a representation for our learning algorithm. While the PDD is
effective in distinguishing periodic point sets up to isometry, there is no
consideration for the composition of the underlying material. We develop a
transformer model with a modified self-attention mechanism that can utilize the
PDD and incorporate compositional information via a spatial encoding method.
This model is tested on the crystals of the Materials Project and Jarvis-DFT
databases and shown to produce accuracy on par with state-of-the-art methods
while being several times faster in both training and prediction time.</div><div><a href='http://arxiv.org/abs/2401.15089v1'>2401.15089v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11686v1")'>Crystalformer: Infinitely Connected Attention for Periodic Structure
  Encoding</div>
<div id='2403.11686v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T11:37:42Z</div><div>Authors: Tatsunori Taniai, Ryo Igarashi, Yuta Suzuki, Naoya Chiba, Kotaro Saito, Yoshitaka Ushiku, Kanta Ono</div><div style='padding-top: 10px; width: 80ex'>Predicting physical properties of materials from their crystal structures is
a fundamental problem in materials science. In peripheral areas such as the
prediction of molecular properties, fully connected attention networks have
been shown to be successful. However, unlike these finite atom arrangements,
crystal structures are infinitely repeating, periodic arrangements of atoms,
whose fully connected attention results in infinitely connected attention. In
this work, we show that this infinitely connected attention can lead to a
computationally tractable formulation, interpreted as neural potential
summation, that performs infinite interatomic potential summations in a deeply
learned feature space. We then propose a simple yet effective Transformer-based
encoder architecture for crystal structures called Crystalformer. Compared to
an existing Transformer-based model, the proposed model requires only 29.4% of
the number of parameters, with minimal modifications to the original
Transformer architecture. Despite the architectural simplicity, the proposed
method outperforms state-of-the-art methods for various property regression
tasks on the Materials Project and JARVIS-DFT datasets.</div><div><a href='http://arxiv.org/abs/2403.11686v1'>2403.11686v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00744v6")'>Harmonizing Covariance and Expressiveness for Deep Hamiltonian
  Regression in Crystalline Material Research: a Hybrid Cascaded Regression
  Framework</div>
<div id='2401.00744v6' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T12:57:15Z</div><div>Authors: Shi Yin, Xinyang Pan, Xudong Zhu, Tianyu Gao, Haochong Zhang, Feng Wu, Lixin He</div><div style='padding-top: 10px; width: 80ex'>Deep learning for Hamiltonian regression of quantum systems in material
research necessitates satisfying the covariance laws, among which achieving
SO(3)-equivariance without sacrificing the expressiveness capability of
networks remains unsolved due to the restriction on non-linear mappings in
assuring theoretical equivariance. To alleviate the covariance-expressiveness
dilemma, we make an exploration on non-linear covariant deep learning with a
hybrid framework consisting of two cascaded regression stages. The first stage,
i.e., a theoretically-guaranteed covariant neural network modeling symmetry
properties of 3D atom systems, predicts baseline Hamiltonians with
theoretically covariant features extracted, assisting the second stage in
learning covariance. Meanwhile, the second stage, powered by a non-linear 3D
graph Transformer network we propose for structural modeling of atomic systems,
refines the first stage's output as a fine-grained prediction of Hamiltonians
with better expressiveness capability. The novel combination of a theoretically
covariant yet inevitably less expressive model with a highly expressive
non-linear network enables precise, generalizable predictions while maintaining
robust covariance under coordinate transformations. We achieve state-of-the-art
performance in Hamiltonian prediction, confirmed through experiments on six
crystalline material databases.</div><div><a href='http://arxiv.org/abs/2401.00744v6'>2401.00744v6</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08032v1")'>Structure-based out-of-distribution (OOD) materials property prediction:
  a benchmark study</div>
<div id='2401.08032v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T01:03:39Z</div><div>Authors: Sadman Sadeed Omee, Nihang Fu, Rongzhi Dong, Ming Hu, Jianjun Hu</div><div style='padding-top: 10px; width: 80ex'>In real-world material research, machine learning (ML) models are usually
expected to predict and discover novel exceptional materials that deviate from
the known materials. It is thus a pressing question to provide an objective
evaluation of ML model performances in property prediction of
out-of-distribution (OOD) materials that are different from the training set
distribution. Traditional performance evaluation of materials property
prediction models through random splitting of the dataset frequently results in
artificially high performance assessments due to the inherent redundancy of
typical material datasets. Here we present a comprehensive benchmark study of
structure-based graph neural networks (GNNs) for extrapolative OOD materials
property prediction. We formulate five different categories of OOD ML problems
for three benchmark datasets from the MatBench study. Our extensive experiments
show that current state-of-the-art GNN algorithms significantly underperform
for the OOD property prediction tasks on average compared to their baselines in
the MatBench study, demonstrating a crucial generalization gap in realistic
material prediction tasks. We further examine the latent physical spaces of
these GNN models and identify the sources of CGCNN, ALIGNN, and DeeperGATGNN's
significantly more robust OOD performance than those of the current best models
in the MatBench study (coGN and coNGN), and provide insights to improve their
performance.</div><div><a href='http://arxiv.org/abs/2401.08032v1'>2401.08032v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10042v1")'>Accurate and Data-Efficient Micro-XRD Phase Identification Using
  Multi-Task Learning: Application to Hydrothermal Fluids</div>
<div id='2403.10042v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T06:23:30Z</div><div>Authors: Yanfei Li, Juejing Liu, Xiaodong Zhao, Wenjun Liu, Tong Geng, Ang Li, Xin Zhang</div><div style='padding-top: 10px; width: 80ex'>Traditional analysis of highly distorted micro-X-ray diffraction ({\mu}-XRD)
patterns from hydrothermal fluid environments is a time-consuming process,
often requiring substantial data preprocessing and labeled experimental data.
This study demonstrates the potential of deep learning with a multitask
learning (MTL) architecture to overcome these limitations. We trained MTL
models to identify phase information in {\mu}-XRD patterns, minimizing the need
for labeled experimental data and masking preprocessing steps. Notably, MTL
models showed superior accuracy compared to binary classification CNNs.
Additionally, introducing a tailored cross-entropy loss function improved MTL
model performance. Most significantly, MTL models tuned to analyze raw and
unmasked XRD patterns achieved close performance to models analyzing
preprocessed data, with minimal accuracy differences. This work indicates that
advanced deep learning architectures like MTL can automate arduous data
handling tasks, streamline the analysis of distorted XRD patterns, and reduce
the reliance on labor-intensive experimental datasets.</div><div><a href='http://arxiv.org/abs/2403.10042v1'>2403.10042v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10682v2")'>Evaluation of GlassNet for physics-informed machine learning of glass
  stability and glass-forming ability</div>
<div id='2403.10682v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T21:03:34Z</div><div>Authors: Sarah I. Allec, Xiaonan Lu, Daniel R. Cassar, Xuan T. Nguyen, Vinay I. Hegde, Thiruvillamalai Mahadevan, Miroslava Peterson, Jincheng Du, Brian J. Riley, John D. Vienna, James E. Saal</div><div style='padding-top: 10px; width: 80ex'>Glasses form the basis of many modern applications and also hold great
potential for future medical and environmental applications. However, their
structural complexity and large composition space make design and optimization
challenging for certain applications. Of particular importance for glass
processing is an estimate of a given composition's glass-forming ability (GFA).
However, there remain many open questions regarding the physical mechanisms of
glass formation, especially in oxide glasses. It is apparent that a proxy for
GFA would be highly useful in glass processing and design, but identifying such
a surrogate property has proven itself to be difficult. Here, we explore the
application of an open-source pre-trained NN model, GlassNet, that can predict
the characteristic temperatures necessary to compute glass stability (GS) and
assess the feasibility of using these physics-informed ML (PIML)-predicted GS
parameters to estimate GFA. In doing so, we track the uncertainties at each
step of the computation - from the original ML prediction errors, to the
compounding of errors during GS estimation, and finally to the final estimation
of GFA. While GlassNet exhibits reasonable accuracy on all individual
properties, we observe a large compounding of error in the combination of these
individual predictions for the prediction of GS, finding that random forest
models offer similar accuracy to GlassNet. We also breakdown the ML performance
on different glass families and find that the error in GS prediction is
correlated with the error in crystallization peak temperature prediction.
Lastly, we utilize this finding to assess the relationship between
top-performing GS parameters and GFA for two ternary glass systems: sodium
borosilicate and sodium iron phosphate glasses. We conclude that to obtain true
ML predictive capability of GFA, significantly more data needs to be collected.</div><div><a href='http://arxiv.org/abs/2403.10682v2'>2403.10682v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12134v1")'>Molecule Generation and Optimization for Efficient Fragrance Creation</div>
<div id='2402.12134v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T13:32:30Z</div><div>Authors: Bruno C. L. Rodrigues, Vinicius V. Santana, Sandris Murins, Idelfonso B. R. Nogueira</div><div style='padding-top: 10px; width: 80ex'>This research introduces a Machine Learning-centric approach to replicate
olfactory experiences, validated through experimental quantification of perfume
perception. Key contributions encompass a hybrid model connecting perfume
molecular structure to human olfactory perception. This model includes an
AI-driven molecule generator (utilizing Graph and Generative Neural Networks),
quantification and prediction of odor intensity, and refinery of optimal
solvent and molecule combinations for desired fragrances. Additionally, a
thermodynamic-based model establishes a link between olfactory perception and
liquid-phase concentrations. The methodology employs Transfer Learning and
selects the most suitable molecules based on vapor pressure and fragrance
notes. Ultimately, a mathematical optimization problem is formulated to
minimize discrepancies between new and target olfactory experiences. The
methodology is validated by reproducing two distinct olfactory experiences
using available experimental data.</div><div><a href='http://arxiv.org/abs/2402.12134v1'>2402.12134v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.03369v2")'>Multi-Modal Representation Learning for Molecular Property Prediction:
  Sequence, Graph, Geometry</div>
<div id='2401.03369v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T02:18:00Z</div><div>Authors: Zeyu Wang, Tianyi Jiang, Jinhuan Wang, Qi Xuan</div><div style='padding-top: 10px; width: 80ex'>Molecular property prediction refers to the task of labeling molecules with
some biochemical properties, playing a pivotal role in the drug discovery and
design process. Recently, with the advancement of machine learning, deep
learning-based molecular property prediction has emerged as a solution to the
resource-intensive nature of traditional methods, garnering significant
attention. Among them, molecular representation learning is the key factor for
molecular property prediction performance. And there are lots of
sequence-based, graph-based, and geometry-based methods that have been
proposed. However, the majority of existing studies focus solely on one
modality for learning molecular representations, failing to comprehensively
capture molecular characteristics and information. In this paper, a novel
multi-modal representation learning model, which integrates the sequence,
graph, and geometry characteristics, is proposed for molecular property
prediction, called SGGRL. Specifically, we design a fusion layer to fusion the
representation of different modalities. Furthermore, to ensure consistency
across modalities, SGGRL is trained to maximize the similarity of
representations for the same molecule while minimizing similarity for different
molecules. To verify the effectiveness of SGGRL, seven molecular datasets, and
several baselines are used for evaluation and comparison. The experimental
results demonstrate that SGGRL consistently outperforms the baselines in most
cases. This further underscores the capability of SGGRL to comprehensively
capture molecular information. Overall, the proposed SGGRL model showcases its
potential to revolutionize molecular property prediction by leveraging
multi-modal representation learning to extract diverse and comprehensive
molecular insights. Our code is released at
https://github.com/Vencent-Won/SGGRL.</div><div><a href='http://arxiv.org/abs/2401.03369v2'>2401.03369v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17615v2")'>Graph Multi-Similarity Learning for Molecular Property Prediction</div>
<div id='2401.17615v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T05:59:38Z</div><div>Authors: Hao Xu, Zhengyang Zhou, Pengyu Hong</div><div style='padding-top: 10px; width: 80ex'>Enhancing accurate molecular property prediction relies on effective and
proficient representation learning. It is crucial to incorporate diverse
molecular relationships characterized by multi-similarity (self-similarity and
relative similarities) between molecules. However, current molecular
representation learning methods fall short in exploring multi-similarity and
often underestimate the complexity of relationships between molecules.
Additionally, previous multi-similarity approaches require the specification of
positive and negative pairs to attribute distinct predefined weights to
different relative similarities, which can introduce potential bias. In this
work, we introduce Graph Multi-Similarity Learning for Molecular Property
Prediction (GraphMSL) framework, along with a novel approach to formulate a
generalized multi-similarity metric without the need to define positive and
negative pairs. In each of the chemical modality spaces (e.g.,molecular
depiction image, fingerprint, NMR, and SMILES) under consideration, we first
define a self-similarity metric (i.e., similarity between an anchor molecule
and another molecule), and then transform it into a generalized
multi-similarity metric for the anchor through a pair weighting function.
GraphMSL validates the efficacy of the multi-similarity metric across
MoleculeNet datasets. Furthermore, these metrics of all modalities are
integrated into a multimodal multi-similarity metric, which showcases the
potential to improve the performance. Moreover, the focus of the model can be
redirected or customized by altering the fusion function. Last but not least,
GraphMSL proves effective in drug discovery evaluations through post-hoc
analyses of the learnt representations.</div><div><a href='http://arxiv.org/abs/2401.17615v2'>2401.17615v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09416v1")'>Deep Manifold Transformation for Protein Representation Learning</div>
<div id='2402.09416v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:38:14Z</div><div>Authors: Bozhen Hu, Zelin Zang, Cheng Tan, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Protein representation learning is critical in various tasks in biology, such
as drug design and protein structure or function prediction, which has
primarily benefited from protein language models and graph neural networks.
These models can capture intrinsic patterns from protein sequences and
structures through masking and task-related losses. However, the learned
protein representations are usually not well optimized, leading to performance
degradation due to limited data, difficulty adapting to new tasks, etc. To
address this, we propose a new \underline{d}eep \underline{m}anifold
\underline{t}ransformation approach for universal \underline{p}rotein
\underline{r}epresentation \underline{l}earning (DMTPRL). It employs manifold
learning strategies to improve the quality and adaptability of the learned
embeddings. Specifically, we apply a novel manifold learning loss during
training based on the graph inter-node similarity. Our proposed DMTPRL method
outperforms state-of-the-art baselines on diverse downstream tasks across
popular datasets. This validates our approach for learning universal and robust
protein representations. We promise to release the code after acceptance.</div><div><a href='http://arxiv.org/abs/2402.09416v1'>2402.09416v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14736v1")'>NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein
  Classification in Graph Neural Networks</div>
<div id='2403.14736v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T13:27:57Z</div><div>Authors: Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho</div><div style='padding-top: 10px; width: 80ex'>Protein classification tasks are essential in drug discovery. Real-world
protein structures are dynamic, which will determine the properties of
proteins. However, the existing machine learning methods, like ProNet (Wang et
al., 2022a), only access limited conformational characteristics and protein
side-chain features, leading to impractical protein structure and inaccuracy of
protein classes in their predictions. In this paper, we propose novel semantic
data augmentation methods, Novel Augmentation of New Node Attributes (NaNa),
and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate
backbone chemical and side-chain biophysical information into protein
classification tasks and a co-embedding residual learning framework.
Specifically, we leverage molecular biophysical, secondary structure, chemical
bonds, and ionic features of proteins to facilitate protein classification
tasks. Furthermore, our semantic augmentation methods and the co-embedding
residual learning framework can improve the performance of GIN (Xu et al.,
2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%
and 11.33% respectively. Our code is available at
https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.</div><div><a href='http://arxiv.org/abs/2403.14736v1'>2403.14736v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13418v1")'>EvolMPNN: Predicting Mutational Effect on Homologous Proteins by
  Evolution Encoding</div>
<div id='2402.13418v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T23:06:21Z</div><div>Authors: Zhiqiang Zhong, Davide Mottin</div><div style='padding-top: 10px; width: 80ex'>Predicting protein properties is paramount for biological and medical
advancements. Current protein engineering mutates on a typical protein, called
the wild-type, to construct a family of homologous proteins and study their
properties. Yet, existing methods easily neglect subtle mutations, failing to
capture the effect on the protein properties. To this end, we propose EvolMPNN,
Evolution-aware Message Passing Neural Network, to learn evolution-aware
protein embeddings. EvolMPNN samples sets of anchor proteins, computes
evolutionary information by means of residues and employs a differentiable
evolution-aware aggregation scheme over these sampled anchors. This way
EvolMPNNcan capture the mutation effect on proteins with respect to the anchor
proteins. Afterwards, the aggregated evolution-aware embeddings are integrated
with sequence embeddings to generate final comprehensive protein embeddings.
Our model shows up to 6.4% better than state-of-the-art methods and attains 36x
inference speedup in comparison with large pre-trained models.</div><div><a href='http://arxiv.org/abs/2402.13418v1'>2402.13418v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08198v1")'>PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for
  Efficient and Generalizable Compound-Protein Interaction Prediction</div>
<div id='2402.08198v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T03:51:10Z</div><div>Authors: Lirong Wu, Yufei Huang, Cheng Tan, Zhangyang Gao, Bozhen Hu, Haitao Lin, Zicheng Liu, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Compound-Protein Interaction (CPI) prediction aims to predict the pattern and
strength of compound-protein interactions for rational drug discovery. Existing
deep learning-based methods utilize only the single modality of protein
sequences or structures and lack the co-modeling of the joint distribution of
the two modalities, which may lead to significant performance drops in complex
real-world scenarios due to various factors, e.g., modality missing and domain
shifting. More importantly, these methods only model protein sequences and
structures at a single fixed scale, neglecting more fine-grained multi-scale
information, such as those embedded in key protein fragments. In this paper, we
propose a novel multi-scale Protein Sequence-structure Contrasting framework
for CPI prediction (PSC-CPI), which captures the dependencies between protein
sequences and structures through both intra-modality and cross-modality
contrasting. We further apply length-variable protein augmentation to allow
contrasting to be performed at different scales, from the amino acid level to
the sequence level. Finally, in order to more fairly evaluate the model
generalizability, we split the test data into four settings based on whether
compounds and proteins have been observed during the training stage. Extensive
experiments have shown that PSC-CPI generalizes well in all four settings,
particularly in the more challenging ``Unseen-Both" setting, where neither
compounds nor proteins have been observed during training. Furthermore, even
when encountering a situation of modality missing, i.e., inference with only
single-modality protein data, PSC-CPI still exhibits comparable or even better
performance than previous approaches.</div><div><a href='http://arxiv.org/abs/2402.08198v1'>2402.08198v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03675v1")'>Effective Protein-Protein Interaction Exploration with PPIretrieval</div>
<div id='2402.03675v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T03:57:06Z</div><div>Authors: Chenqing Hua, Connor Coley, Guy Wolf, Doina Precup, Shuangjia Zheng</div><div style='padding-top: 10px; width: 80ex'>Protein-protein interactions (PPIs) are crucial in regulating numerous
cellular functions, including signal transduction, transportation, and immune
defense. As the accuracy of multi-chain protein complex structure prediction
improves, the challenge has shifted towards effectively navigating the vast
complex universe to identify potential PPIs. Herein, we propose PPIretrieval,
the first deep learning-based model for protein-protein interaction
exploration, which leverages existing PPI data to effectively search for
potential PPIs in an embedding space, capturing rich geometric and chemical
information of protein surfaces. When provided with an unseen query protein
with its associated binding site, PPIretrieval effectively identifies a
potential binding partner along with its corresponding binding site in an
embedding space, facilitating the formation of protein-protein complexes.</div><div><a href='http://arxiv.org/abs/2402.03675v1'>2402.03675v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14391v1")'>MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction
  Prediction via Microenvironment-Aware Protein Embedding</div>
<div id='2402.14391v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:04:41Z</div><div>Authors: Lirong Wu, Yijun Tian, Yufei Huang, Siyuan Li, Haitao Lin, Nitesh V Chawla, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Protein-Protein Interactions (PPIs) are fundamental in various biological
processes and play a key role in life activities. The growing demand and cost
of experimental PPI assays require computational methods for efficient PPI
prediction. While existing methods rely heavily on protein sequence for PPI
prediction, it is the protein structure that is the key to determine the
interactions. To take both protein modalities into account, we define the
microenvironment of an amino acid residue by its sequence and structural
contexts, which describe the surrounding chemical properties and geometric
features. In addition, microenvironments defined in previous work are largely
based on experimentally assayed physicochemical properties, for which the
"vocabulary" is usually extremely small. This makes it difficult to cover the
diversity and complexity of microenvironments. In this paper, we propose
Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which
encodes microenvironments into chemically meaningful discrete codes via a
sufficiently large microenvironment "vocabulary" (i.e., codebook). Moreover, we
propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM),
to capture the dependencies between different microenvironments by randomly
masking the codebook and reconstructing the input. With the learned
microenvironment codebook, we can reuse it as an off-the-shelf tool to
efficiently and effectively encode proteins of different sizes and functions
for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can
scale to PPI prediction with millions of PPIs with superior trade-offs between
effectiveness and computational efficiency than the state-of-the-art
competitors.</div><div><a href='http://arxiv.org/abs/2402.14391v1'>2402.14391v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10144v1")'>Exploiting Hierarchical Interactions for Protein Surface Learning</div>
<div id='2401.10144v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T14:10:40Z</div><div>Authors: Yiqun Lin, Liang Pan, Yi Li, Ziwei Liu, Xiaomeng Li</div><div style='padding-top: 10px; width: 80ex'>Predicting interactions between proteins is one of the most important yet
challenging problems in structural bioinformatics. Intrinsically, potential
function sites in protein surfaces are determined by both geometric and
chemical features. However, existing works only consider handcrafted or
individually learned chemical features from the atom type and extract geometric
features independently. Here, we identify two key properties of effective
protein surface learning: 1) relationship among atoms: atoms are linked with
each other by covalent bonds to form biomolecules instead of appearing alone,
leading to the significance of modeling the relationship among atoms in
chemical feature learning. 2) hierarchical feature interaction: the neighboring
residue effect validates the significance of hierarchical feature interaction
among atoms and between surface points and atoms (or residues). In this paper,
we present a principled framework based on deep learning techniques, namely
Hierarchical Chemical and Geometric Feature Interaction Network (HCGNet), for
protein surface analysis by bridging chemical and geometric features with
hierarchical interactions. Extensive experiments demonstrate that our method
outperforms the prior state-of-the-art method by 2.3% in site prediction task
and 3.2% in interaction matching task, respectively. Our code is available at
https://github.com/xmed-lab/HCGNet.</div><div><a href='http://arxiv.org/abs/2401.10144v1'>2401.10144v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05602v1")'>Extracting Protein-Protein Interactions (PPIs) from Biomedical
  Literature using Attention-based Relational Context Information</div>
<div id='2403.05602v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T01:43:21Z</div><div>Authors: Gilchan Park, Sean McCorkle, Carlos Soto, Ian Blaby, Shinjae Yoo</div><div style='padding-top: 10px; width: 80ex'>Because protein-protein interactions (PPIs) are crucial to understand living
systems, harvesting these data is essential to probe disease development and
discern gene/protein functions and biological processes. Some curated datasets
contain PPI data derived from the literature and other sources (e.g., IntAct,
BioGrid, DIP, and HPRD). However, they are far from exhaustive, and their
maintenance is a labor-intensive process. On the other hand, machine learning
methods to automate PPI knowledge extraction from the scientific literature
have been limited by a shortage of appropriate annotated data. This work
presents a unified, multi-source PPI corpora with vetted interaction
definitions augmented by binary interaction type labels and a Transformer-based
deep learning method that exploits entities' relational context information for
relation representation to improve relation classification performance. The
model's performance is evaluated on four widely studied biomedical relation
extraction datasets, as well as this work's target PPI datasets, to observe the
effectiveness of the representation to relation extraction tasks in various
data. Results show the model outperforms prior state-of-the-art models. The
code and data are available at:
https://github.com/BNLNLP/PPI-Relation-Extraction</div><div><a href='http://arxiv.org/abs/2403.05602v1'>2403.05602v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02124v1")'>ACP-ESM: A novel framework for classification of anticancer peptides
  using protein-oriented transformer approach</div>
<div id='2401.02124v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T08:19:27Z</div><div>Authors: Zeynep Hilal Kilimci, Mustafa Yalcin</div><div style='padding-top: 10px; width: 80ex'>Anticancer peptides (ACPs) are a class of molecules that have gained
significant attention in the field of cancer research and therapy. ACPs are
short chains of amino acids, the building blocks of proteins, and they possess
the ability to selectively target and kill cancer cells. One of the key
advantages of ACPs is their ability to selectively target cancer cells while
sparing healthy cells to a greater extent. This selectivity is often attributed
to differences in the surface properties of cancer cells compared to normal
cells. That is why ACPs are being investigated as potential candidates for
cancer therapy. ACPs may be used alone or in combination with other treatment
modalities like chemotherapy and radiation therapy. While ACPs hold promise as
a novel approach to cancer treatment, there are challenges to overcome,
including optimizing their stability, improving selectivity, and enhancing
their delivery to cancer cells, continuous increasing in number of peptide
sequences, developing a reliable and precise prediction model. In this work, we
propose an efficient transformer-based framework to identify anticancer
peptides for by performing accurate a reliable and precise prediction model.
For this purpose, four different transformer models, namely ESM, ProtBert,
BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid
sequences. To demonstrate the contribution of the proposed framework, extensive
experiments are carried on widely-used datasets in the literature, two versions
of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of
proposed model enhances classification accuracy when compared to the
state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of
accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and
88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.</div><div><a href='http://arxiv.org/abs/2401.02124v1'>2401.02124v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05014v1")'>Simple Multigraph Convolution Networks</div>
<div id='2403.05014v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T03:27:58Z</div><div>Authors: Danyang Wu, Xinjie Shen, Jitao Lu, Jin Xu, Feiping Nie</div><div style='padding-top: 10px; width: 80ex'>Existing multigraph convolution methods either ignore the cross-view
interaction among multiple graphs, or induce extremely high computational cost
due to standard cross-view polynomial operators. To alleviate this problem,
this paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which
first extracts consistent cross-view topology from multigraphs including
edge-level and subgraph-level topology, then performs polynomial expansion
based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes
the consistent topologies in polynomial expansion rather than standard
cross-view polynomial expansion, which performs credible cross-view spatial
message-passing, follows the spectral convolution paradigm, and effectively
reduces the complexity of standard polynomial expansion. In the simulations,
experimental results demonstrate that SMGCN achieves state-of-the-art
performance on ACM and DBLP multigraph benchmark datasets. Our codes are
available at https://github.com/frinkleko/SMGCN.</div><div><a href='http://arxiv.org/abs/2403.05014v1'>2403.05014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07954v1")'>Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace
  Approach</div>
<div id='2403.07954v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T06:26:17Z</div><div>Authors: Keke Huang, Wencai Cao, Hoang Ta, Xiaokui Xiao, Pietro LiÃ²</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs), known as spectral graph filters, find a wide
range of applications in web networks. To bypass eigendecomposition, polynomial
graph filters are proposed to approximate graph filters by leveraging various
polynomial bases for filter training. However, no existing studies have
explored the diverse polynomial graph filters from a unified perspective for
optimization.
  In this paper, we first unify polynomial graph filters, as well as the
optimal filters of identical degrees into the Krylov subspace of the same
order, thus providing equivalent expressive power theoretically. Next, we
investigate the asymptotic convergence property of polynomials from the unified
Krylov subspace perspective, revealing their limited adaptability in graphs
with varying heterophily degrees. Inspired by those facts, we design a novel
adaptive Krylov subspace approach to optimize polynomial bases with provable
controllability over the graph spectrum so as to adapt various heterophily
graphs. Subsequently, we propose AdaptKry, an optimized polynomial graph filter
utilizing bases from the adaptive Krylov subspaces. Meanwhile, in light of the
diverse spectral properties of complex graphs, we extend AdaptKry by leveraging
multiple adaptive Krylov bases without incurring extra training costs. As a
consequence, extended AdaptKry is able to capture the intricate characteristics
of graphs and provide insights into their inherent complexity. We conduct
extensive experiments across a series of real-world datasets. The experimental
results demonstrate the superior filtering capability of AdaptKry, as well as
the optimized efficacy of the adaptive Krylov basis.</div><div><a href='http://arxiv.org/abs/2403.07954v1'>2403.07954v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15603v2")'>Improving Expressive Power of Spectral Graph Neural Networks with
  Eigenvalue Correction</div>
<div id='2401.15603v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T08:12:00Z</div><div>Authors: Kangkang Lu, Yanhua Yu, Hao Fei, Xuan Li, Zixuan Yang, Zirui Guo, Meiyu Liang, Mengran Yin, Tat-Seng Chua</div><div style='padding-top: 10px; width: 80ex'>In recent years, spectral graph neural networks, characterized by polynomial
filters, have garnered increasing attention and have achieved remarkable
performance in tasks such as node classification. These models typically assume
that eigenvalues for the normalized Laplacian matrix are distinct from each
other, thus expecting a polynomial filter to have a high fitting ability.
However, this paper empirically observes that normalized Laplacian matrices
frequently possess repeated eigenvalues. Moreover, we theoretically establish
that the number of distinguishable eigenvalues plays a pivotal role in
determining the expressive power of spectral graph neural networks. In light of
this observation, we propose an eigenvalue correction strategy that can free
polynomial filters from the constraints of repeated eigenvalue inputs.
Concretely, the proposed eigenvalue correction strategy enhances the uniform
distribution of eigenvalues, thus mitigating repeated eigenvalues, and
improving the fitting capacity and expressive power of polynomial filters.
Extensive experimental results on both synthetic and real-world datasets
demonstrate the superiority of our method.</div><div><a href='http://arxiv.org/abs/2401.15603v2'>2401.15603v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08653v3")'>SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds</div>
<div id='2402.08653v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:33:45Z</div><div>Authors: Wuxinlin Cheng, Chenhui Deng, Ali Aghdaei, Zhiru Zhang, Zhuo Feng</div><div style='padding-top: 10px; width: 80ex'>Modern graph neural networks (GNNs) can be sensitive to changes in the input
graph structure and node features, potentially resulting in unpredictable
behavior and degraded performance. In this work, we introduce a spectral
framework known as SAGMAN for examining the stability of GNNs. This framework
assesses the distance distortions that arise from the nonlinear mappings of
GNNs between the input and output manifolds: when two nearby nodes on the input
manifold are mapped (through a GNN model) to two distant ones on the output
manifold, it implies a large distance distortion and thus a poor GNN stability.
We propose a distance-preserving graph dimension reduction (GDR) approach that
utilizes spectral graph embedding and probabilistic graphical models (PGMs) to
create low-dimensional input/output graph-based manifolds for meaningful
stability analysis. Our empirical evaluations show that SAGMAN effectively
assesses the stability of each node when subjected to various edge or feature
perturbations, offering a scalable approach for evaluating the stability of
GNNs, extending to applications within recommendation systems. Furthermore, we
illustrate its utility in downstream tasks, notably in enhancing GNN stability
and facilitating adversarial targeted attacks.</div><div><a href='http://arxiv.org/abs/2402.08653v3'>2402.08653v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03880v1")'>Graph neural network outputs are almost surely asymptotically constant</div>
<div id='2403.03880v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:40:26Z</div><div>Authors: Sam Adam-Day, Michael Benedikt, Ä°smail Ä°lkan Ceylan, Ben Finkelshtein</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) are the predominant architectures for a variety
of learning tasks on graphs. We present a new angle on the expressive power of
GNNs by studying how the predictions of a GNN probabilistic classifier evolve
as we apply it on larger graphs drawn from some random graph model. We show
that the output converges to a constant function, which upper-bounds what these
classifiers can express uniformly. This convergence phenomenon applies to a
very wide class of GNNs, including state of the art models, with aggregates
including mean and the attention-based mechanism of graph transformers. Our
results apply to a broad class of random graph models, including the (sparse)
Erd\H{o}s-R\'enyi model and the stochastic block model. We empirically validate
these findings, observing that the convergence phenomenon already manifests
itself on graphs of relatively modest size.</div><div><a href='http://arxiv.org/abs/2403.03880v1'>2403.03880v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06425v1")'>A Differential Geometric View and Explainability of GNN on Evolving
  Graphs</div>
<div id='2403.06425v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T04:26:18Z</div><div>Authors: Yazheng Liu, Xi Zhang, Sihong Xie</div><div style='padding-top: 10px; width: 80ex'>Graphs are ubiquitous in social networks and biochemistry, where Graph Neural
Networks (GNN) are the state-of-the-art models for prediction. Graphs can be
evolving and it is vital to formally model and understand how a trained GNN
responds to graph evolution. We propose a smooth parameterization of the GNN
predicted distributions using axiomatic attribution, where the distributions
are on a low-dimensional manifold within a high-dimensional embedding space. We
exploit the differential geometric viewpoint to model distributional evolution
as smooth curves on the manifold. We reparameterize families of curves on the
manifold and design a convex optimization problem to find a unique curve that
concisely approximates the distributional evolution for human interpretation.
Extensive experiments on node classification, link prediction, and graph
classification tasks with evolving graphs demonstrate the better sparsity,
faithfulness, and intuitiveness of the proposed method over the
state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.06425v1'>2403.06425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06817v1")'>Are Targeted Messages More Effective?</div>
<div id='2403.06817v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T15:34:57Z</div><div>Authors: Martin Grohe, Eran Rosenbluth</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNN) are deep learning architectures for graphs.
Essentially, a GNN is a distributed message passing algorithm, which is
controlled by parameters learned from data. It operates on the vertices of a
graph: in each iteration, vertices receive a message on each incoming edge,
aggregate these messages, and then update their state based on their current
state and the aggregated messages. The expressivity of GNNs can be
characterised in terms of certain fragments of first-order logic with counting
and the Weisfeiler-Lehman algorithm.
  The core GNN architecture comes in two different versions. In the first
version, a message only depends on the state of the source vertex, whereas in
the second version it depends on the states of the source and target vertices.
In practice, both of these versions are used, but the theory of GNNs so far
mostly focused on the first one. On the logical side, the two versions
correspond to two fragments of first-order logic with counting that we call
modal and guarded.
  The question whether the two versions differ in their expressivity has been
mostly overlooked in the GNN literature and has only been asked recently
(Grohe, LICS'23). We answer this question here. It turns out that the answer is
not as straightforward as one might expect. By proving that the modal and
guarded fragment of first-order logic with counting have the same expressivity
over labelled undirected graphs, we show that in a non-uniform setting the two
GNN versions have the same expressivity. However, we also prove that in a
uniform setting the second version is strictly more expressive.</div><div><a href='http://arxiv.org/abs/2403.06817v1'>2403.06817v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13937v1")'>Verifying message-passing neural networks via topology-based bounds
  tightening</div>
<div id='2402.13937v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T17:05:27Z</div><div>Authors: Christopher Hojny, Shiqiang Zhang, Juan S. Campos, Ruth Misener</div><div style='padding-top: 10px; width: 80ex'>Since graph neural networks (GNNs) are often vulnerable to attack, we need to
know when we can trust them. We develop a computationally effective approach
towards providing robust certificates for message-passing neural networks
(MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our
work builds on mixed-integer optimization, it encodes a wide variety of
subproblems, for example it admits (i) both adding and removing edges, (ii)
both global and local budgets, and (iii) both topological perturbations and
feature modifications. Our key technology, topology-based bounds tightening,
uses graph structure to tighten bounds. We also experiment with aggressive
bounds tightening to dynamically change the optimization constraints by
tightening variable bounds. To demonstrate the effectiveness of these
strategies, we implement an extension to the open-source branch-and-cut solver
SCIP. We test on both node and graph classification problems and consider
topological attacks that both add and remove edges.</div><div><a href='http://arxiv.org/abs/2402.13937v1'>2402.13937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09193v1")'>GNN-LoFI: a Novel Graph Neural Network through Localized Feature-based
  Histogram Intersection</div>
<div id='2401.09193v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T13:04:23Z</div><div>Authors: Alessandro Bicciato, Luca Cosmo, Giorgia Minello, Luca Rossi, Andrea Torsello</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks are increasingly becoming the framework of choice for
graph-based machine learning. In this paper, we propose a new graph neural
network architecture that substitutes classical message passing with an
analysis of the local distribution of node features. To this end, we extract
the distribution of features in the egonet for each local neighbourhood and
compare them against a set of learned label distributions by taking the
histogram intersection kernel. The similarity information is then propagated to
other nodes in the network, effectively creating a message passing-like
mechanism where the message is determined by the ensemble of the features. We
perform an ablation study to evaluate the network's performance under different
choices of its hyper-parameters. Finally, we test our model on standard graph
classification and regression benchmarks, and we find that it outperforms
widely used alternative approaches, including both graph kernels and graph
neural networks.</div><div><a href='http://arxiv.org/abs/2401.09193v1'>2401.09193v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01233v1")'>Graph Elimination Networks</div>
<div id='2401.01233v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T14:58:59Z</div><div>Authors: Shuo Wang, Ge Cheng, Yun Zhang</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) are widely applied across various domains, yet
they perform poorly in deep layers. Existing research typically attributes this
problem to node over-smoothing, where node representations become
indistinguishable after multiple rounds of propagation. In this paper, we delve
into the neighborhood propagation mechanism of GNNs and discover that the real
root cause of GNNs' performance degradation in deep layers lies in ineffective
neighborhood feature propagation. This propagation leads to an exponential
growth of a node's current representation at every propagation step, making it
extremely challenging to capture valuable dependencies between long-distance
nodes. To address this issue, we introduce Graph Elimination Networks (GENs),
which employ a specific algorithm to eliminate redundancies during neighborhood
propagation. We demonstrate that GENs can enhance nodes' perception of distant
neighborhoods and extend the depth of network propagation. Extensive
experiments show that GENs outperform the state-of-the-art methods on various
graph-level and node-level datasets.</div><div><a href='http://arxiv.org/abs/2401.01233v1'>2401.01233v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12529v1")'>Contextualized Messages Boost Graph Representations</div>
<div id='2403.12529v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T08:05:49Z</div><div>Authors: Brian Godwin Lim</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have gained significant interest in recent years
due to their ability to handle arbitrarily structured data represented as
graphs. GNNs generally follow the message-passing scheme to locally update node
feature representations. A graph readout function is then employed to create a
representation for the entire graph. Several studies proposed different GNNs by
modifying the aggregation and combination strategies of the message-passing
framework, often inspired by heuristics. Nevertheless, several studies have
begun exploring GNNs from a theoretical perspective based on the graph
isomorphism problem which inherently assumes countable node feature
representations. Yet, there are only a few theoretical works exploring GNNs
with uncountable node feature representations. This paper presents a new
perspective on the representational capabilities of GNNs across all levels -
node-level, neighborhood-level, and graph-level - when the space of node
feature representation is uncountable. From the results, a novel
soft-isomorphic relational graph convolution network (SIR-GCN) is proposed that
emphasizes non-linear and contextualized transformations of neighborhood
feature representations. The mathematical relationship of SIR-GCN and three
widely used GNNs is explored to highlight the contribution. Validation on
synthetic datasets then demonstrates that SIR-GCN outperforms comparable models
even in simple node and graph property prediction tasks.</div><div><a href='http://arxiv.org/abs/2403.12529v1'>2403.12529v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01384v1")'>Strong Transitivity Relations and Graph Neural Networks</div>
<div id='2401.01384v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T13:53:50Z</div><div>Authors: Yassin Mohamadi, Mostafa Haghir Chehreghani</div><div style='padding-top: 10px; width: 80ex'>Local neighborhoods play a crucial role in embedding generation in
graph-based learning. It is commonly believed that nodes ought to have
embeddings that resemble those of their neighbors. In this research, we try to
carefully expand the concept of similarity from nearby neighborhoods to the
entire graph. We provide an extension of similarity that is based on
transitivity relations, which enables Graph Neural Networks (GNNs) to capture
both global similarities and local similarities over the whole graph. We
introduce Transitivity Graph Neural Network (TransGNN), which more than local
node similarities, takes into account global similarities by distinguishing
strong transitivity relations from weak ones and exploiting them. We evaluate
our model over several real-world datasets and showed that it considerably
improves the performance of several well-known GNN models, for tasks such as
node classification.</div><div><a href='http://arxiv.org/abs/2401.01384v1'>2401.01384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10543v1")'>Distinguishing Neighborhood Representations Through Reverse Process of
  GNNs for Heterophilic Graphs</div>
<div id='2403.10543v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T08:48:54Z</div><div>Authors: MoonJeong Park, Jaeseung Heo, Dongwoo Kim</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Network (GNN) resembles the diffusion process, leading to the
over-smoothing of learned representations when stacking many layers. Hence, the
reverse process of message passing can sharpen the node representations by
inverting the forward message propagation. The sharpened representations can
help us to better distinguish neighboring nodes with different labels, such as
in heterophilic graphs. In this work, we apply the design principle of the
reverse process to the three variants of the GNNs. Through the experiments on
heterophilic graph data, where adjacent nodes need to have different
representations for successful classification, we show that the reverse process
significantly improves the prediction performance in many cases. Additional
analysis reveals that the reverse mechanism can mitigate the over-smoothing
over hundreds of layers.</div><div><a href='http://arxiv.org/abs/2403.10543v1'>2403.10543v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09125v1")'>Understanding Heterophily for Graph Neural Networks</div>
<div id='2401.09125v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T11:01:28Z</div><div>Authors: Junfu Wang, Yuanfang Guo, Liang Yang, Yunhong Wang</div><div style='padding-top: 10px; width: 80ex'>Graphs with heterophily have been regarded as challenging scenarios for Graph
Neural Networks (GNNs), where nodes are connected with dissimilar neighbors
through various patterns. In this paper, we present theoretical understandings
of the impacts of different heterophily patterns for GNNs by incorporating the
graph convolution (GC) operations into fully connected networks via the
proposed Heterophilous Stochastic Block Models (HSBM), a general random graph
model that can accommodate diverse heterophily patterns. Firstly, we show that
by applying a GC operation, the separability gains are determined by two
factors, i.e., the Euclidean distance of the neighborhood distributions and
$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where
$\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It
reveals that the impact of heterophily on classification needs to be evaluated
alongside the averaged node degree. Secondly, we show that the topological
noise has a detrimental impact on separability, which is equivalent to
degrading $\mathbb{E}\left[\operatorname{deg}\right]$. Finally, when applying
multiple GC operations, we show that the separability gains are determined by
the normalized distance of the $l$-powered neighborhood distributions. It
indicates that the nodes still possess separability as $l$ goes to infinity in
a wide range of regimes. Extensive experiments on both synthetic and real-world
data verify the effectiveness of our theory.</div><div><a href='http://arxiv.org/abs/2401.09125v1'>2401.09125v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03676v1")'>Simplified PCNet with Robustness</div>
<div id='2403.03676v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:57:48Z</div><div>Authors: Bingheng Li, Xuanting Xie, Haoxiang Lei, Ruiyi Fang, Zhao Kang</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have garnered significant attention for their
success in learning the representation of homophilic or heterophilic graphs.
However, they cannot generalize well to real-world graphs with different levels
of homophily. In response, the Possion-Charlier Network (PCNet)
\cite{li2024pc}, the previous work, allows graph representation to be learned
from heterophily to homophily. Although PCNet alleviates the heterophily issue,
there remain some challenges in further improving the efficacy and efficiency.
In this paper, we simplify PCNet and enhance its robustness. We first extend
the filter order to continuous values and reduce its parameters. Two variants
with adaptive neighborhood sizes are implemented. Theoretical analysis shows
our model's robustness to graph structure perturbations or adversarial attacks.
We validate our approach through semi-supervised learning tasks on various
datasets representing both homophilic and heterophilic graphs.</div><div><a href='http://arxiv.org/abs/2403.03676v1'>2403.03676v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10339v1")'>Generation is better than Modification: Combating High Class Homophily
  Variance in Graph Anomaly Detection</div>
<div id='2403.10339v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T14:26:53Z</div><div>Authors: Rui Zhang, Dawei Cheng, Xin Liu, Jie Yang, Yi Ouyang, Xian Wu, Yefeng Zheng</div><div style='padding-top: 10px; width: 80ex'>Graph-based anomaly detection is currently an important research topic in the
field of graph neural networks (GNNs). We find that in graph anomaly detection,
the homophily distribution differences between different classes are
significantly greater than those in homophilic and heterophilic graphs. For the
first time, we introduce a new metric called Class Homophily Variance, which
quantitatively describes this phenomenon. To mitigate its impact, we propose a
novel GNN model named Homophily Edge Generation Graph Neural Network (HedGe).
Previous works typically focused on pruning, selecting or connecting on
original relationships, and we refer to these methods as modifications.
Different from these works, our method emphasizes generating new relationships
with low class homophily variance, using the original relationships as an
auxiliary. HedGe samples homophily adjacency matrices from scratch using a
self-attention mechanism, and leverages nodes that are relevant in the feature
space but not directly connected in the original graph. Additionally, we modify
the loss function to punish the generation of unnecessary heterophilic edges by
the model. Extensive comparison experiments demonstrate that HedGe achieved the
best performance across multiple benchmark datasets, including anomaly
detection and edgeless node classification. The proposed model also improves
the robustness under the novel Heterophily Attack with increased class
homophily variance on other graph classification tasks.</div><div><a href='http://arxiv.org/abs/2403.10339v1'>2403.10339v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14155v1")'>Alleviating Structural Distribution Shift in Graph Anomaly Detection</div>
<div id='2401.14155v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T13:07:34Z</div><div>Authors: Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, Yongdong Zhang</div><div style='padding-top: 10px; width: 80ex'>Graph anomaly detection (GAD) is a challenging binary classification problem
due to its different structural distribution between anomalies and normal nodes
-- abnormal nodes are a minority, therefore holding high heterophily and low
homophily compared to normal nodes. Furthermore, due to various time factors
and the annotation preferences of human experts, the heterophily and homophily
can change across training and testing data, which is called structural
distribution shift (SDS) in this paper. The mainstream methods are built on
graph neural networks (GNNs), benefiting the classification of normals from
aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and
suffering from poor generalization.
  This work solves the problem from a feature view. We observe that the degree
of SDS varies between anomalies and normal nodes. Hence to address the issue,
the key lies in resisting high heterophily for anomalies meanwhile benefiting
the learning of normals from homophily. We tease out the anomaly features on
which we constrain to mitigate the effect of heterophilous neighbors and make
them invariant. We term our proposed framework as Graph Decomposition Network
(GDN). Extensive experiments are conducted on two benchmark datasets, and the
proposed framework achieves a remarkable performance boost in GAD, especially
in an SDS environment where anomalies have largely different structural
distribution across training and testing environments. Codes are open-sourced
in https://github.com/blacksingular/wsdm_GDN.</div><div><a href='http://arxiv.org/abs/2401.14155v1'>2401.14155v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13210v1")'>Multitask Active Learning for Graph Anomaly Detection</div>
<div id='2401.13210v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T03:43:45Z</div><div>Authors: Wenjing Chang, Kay Liu, Kaize Ding, Philip S. Yu, Jianjun Yu</div><div style='padding-top: 10px; width: 80ex'>In the web era, graph machine learning has been widely used on ubiquitous
graph-structured data. As a pivotal component for bolstering web security and
enhancing the robustness of graph-based applications, the significance of graph
anomaly detection is continually increasing. While Graph Neural Networks (GNNs)
have demonstrated efficacy in supervised and semi-supervised graph anomaly
detection, their performance is contingent upon the availability of sufficient
ground truth labels. The labor-intensive nature of identifying anomalies from
complex graph structures poses a significant challenge in real-world
applications. Despite that, the indirect supervision signals from other tasks
(e.g., node classification) are relatively abundant. In this paper, we propose
a novel MultItask acTIve Graph Anomaly deTEction framework, namely MITIGATE.
Firstly, by coupling node classification tasks, MITIGATE obtains the capability
to detect out-of-distribution nodes without known anomalies. Secondly, MITIGATE
quantifies the informativeness of nodes by the confidence difference across
tasks, allowing samples with conflicting predictions to provide informative yet
not excessively challenging information for subsequent training. Finally, to
enhance the likelihood of selecting representative nodes that are distant from
known patterns, MITIGATE adopts a masked aggregation mechanism for distance
measurement, considering both inherent features of nodes and current labeled
status. Empirical studies on four datasets demonstrate that MITIGATE
significantly outperforms the state-of-the-art methods for anomaly detection.
Our code is publicly available at: https://github.com/AhaChang/MITIGATE.</div><div><a href='http://arxiv.org/abs/2401.13210v1'>2401.13210v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12761v1")'>FGAD: Self-boosted Knowledge Distillation for An Effective Federated
  Graph Anomaly Detection Framework</div>
<div id='2402.12761v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T07:03:59Z</div><div>Authors: Jinyu Cai, Yunhe Zhang, Zhoumin Lu, Wenzhong Guo, See-kiong Ng</div><div style='padding-top: 10px; width: 80ex'>Graph anomaly detection (GAD) aims to identify anomalous graphs that
significantly deviate from other ones, which has raised growing attention due
to the broad existence and complexity of graph-structured data in many
real-world scenarios. However, existing GAD methods usually execute with
centralized training, which may lead to privacy leakage risk in some sensitive
cases, thereby impeding collaboration among organizations seeking to
collectively develop robust GAD models. Although federated learning offers a
promising solution, the prevalent non-IID problems and high communication costs
present significant challenges, particularly pronounced in collaborations with
graph data distributed among different participants. To tackle these
challenges, we propose an effective federated graph anomaly detection framework
(FGAD). We first introduce an anomaly generator to perturb the normal graphs to
be anomalous, and train a powerful anomaly detector by distinguishing generated
anomalous graphs from normal ones. Then, we leverage a student model to distill
knowledge from the trained anomaly detector (teacher model), which aims to
maintain the personality of local models and alleviate the adverse impact of
non-IID problems. Moreover, we design an effective collaborative learning
mechanism that facilitates the personalization preservation of local models and
significantly reduces communication costs among clients. Empirical results of
the GAD tasks on non-IID graphs compared with state-of-the-art baselines
demonstrate the superiority and efficiency of the proposed FGAD method.</div><div><a href='http://arxiv.org/abs/2402.12761v1'>2402.12761v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15988v1")'>Towards Fair Graph Anomaly Detection: Problem, New Datasets, and
  Evaluation</div>
<div id='2402.15988v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T05:00:20Z</div><div>Authors: Neng Kai Nigel Neo, Yeon-Chang Lee, Yiqiao Jin, Sang-Wook Kim, Srijan Kumar</div><div style='padding-top: 10px; width: 80ex'>The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect
anomalous nodes in an input graph while ensuring fairness and avoiding biased
predictions against individuals from sensitive subgroups such as gender or
political leanings. Fairness in graphs is particularly crucial in anomaly
detection areas such as misinformation detection in search/ranking systems,
where decision outcomes can significantly affect individuals. However, the
current literature does not comprehensively discuss this problem, nor does it
provide realistic datasets that encompass actual graph structures, anomaly
labels, and sensitive attributes for research in FairGAD. To bridge this gap,
we introduce a formal definition of the FairGAD problem and present two novel
graph datasets constructed from the globally prominent social media platforms
Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges
associated with 9,000 and 47,000 nodes, respectively, and leverage political
leanings as sensitive attributes and misinformation spreaders as anomaly
labels. We demonstrate that our FairGAD datasets significantly differ from the
synthetic datasets used currently by the research community. These new datasets
offer significant values for FairGAD by providing realistic data that captures
the intricacies of social networks. Using our datasets, we investigate the
performance-fairness trade-off in eleven existing GAD and non-graph AD methods
on five state-of-the-art fairness methods, which sheds light on their
effectiveness and limitations in addressing the FairGAD problem.</div><div><a href='http://arxiv.org/abs/2402.15988v1'>2402.15988v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12399v1")'>Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for
  Community Canvassing</div>
<div id='2403.12399v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T03:14:24Z</div><div>Authors: Saurabh Sharma, Ambuj SIngh</div><div style='padding-top: 10px; width: 80ex'>The problem of online social network manipulation for community canvassing is
of real concern in today's world. Motivated by the study of voter models,
opinion and polarization dynamics on networks, we model community canvassing as
a dynamic process over a network enabled via gradient-based attacks on GNNs.
Existing attacks on GNNs are all single-step and do not account for the dynamic
cascading nature of information diffusion in networks. We consider the
realistic scenario where an adversary uses a GNN as a proxy to predict and
manipulate voter preferences, especially uncertain voters. Gradient-based
attacks on the GNN inform the adversary of strategic manipulations that can be
made to proselytize targeted voters. In particular, we explore $\textit{minimum
budget attacks for community canvassing}$ (MBACC). We show that the MBACC
problem is NP-Hard and propose Dynamic Multi-Step Adversarial Community
Canvassing (MAC) to address it. MAC makes dynamic local decisions based on the
heuristic of low budget and high second-order influence to convert and perturb
target voters. MAC is a dynamic multi-step attack that discovers low-budget and
high-influence targets from which efficient cascading attacks can happen. We
evaluate MAC against single-step baselines on the MBACC problem with multiple
underlying networks and GNN models. Our experiments show the superiority of MAC
which is able to discover efficient multi-hop attacks for adversarial community
canvassing. Our code implementation and data is available at
https://github.com/saurabhsharma1993/mac.</div><div><a href='http://arxiv.org/abs/2403.12399v1'>2403.12399v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04010v1")'>Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message
  Passing and Hyperbolic Neural Networks</div>
<div id='2403.04010v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T19:42:34Z</div><div>Authors: Jing Gu, Dongmian Zou</div><div style='padding-top: 10px; width: 80ex'>Graph anomaly detection plays a vital role for identifying abnormal instances
in complex networks. Despite advancements of methodology based on deep learning
in recent years, existing benchmarking approaches exhibit limitations that
hinder a comprehensive comparison. In this paper, we revisit datasets and
approaches for unsupervised node-level graph anomaly detection tasks from three
aspects. Firstly, we introduce outlier injection methods that create more
diverse and graph-based anomalies in graph datasets. Secondly, we compare
methods employing message passing against those without, uncovering the
unexpected decline in performance associated with message passing. Thirdly, we
explore the use of hyperbolic neural networks, specifying crucial architecture
and loss design that contribute to enhanced performance. Through rigorous
experiments and evaluations, our study sheds light on general strategies for
improving node-level graph anomaly detection methods.</div><div><a href='http://arxiv.org/abs/2403.04010v1'>2403.04010v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11933v1")'>SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via
  Self-Supervised Learning</div>
<div id='2402.11933v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:19:26Z</div><div>Authors: Jongha Lee, Sunwoo Kim, Kijung Shin</div><div style='padding-top: 10px; width: 80ex'>To detect anomalies in real-world graphs, such as social, email, and
financial networks, various approaches have been developed. While they
typically assume static input graphs, most real-world graphs grow over time,
naturally represented as edge streams. In this context, we aim to achieve three
goals: (a) instantly detecting anomalies as they occur, (b) adapting to
dynamically changing states, and (c) handling the scarcity of dynamic anomaly
labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly
Detection in Edge Streams) for rapid detection of dynamic anomalies in edge
streams, without relying on labels. SLADE detects the shifts of nodes into
abnormal states by observing deviations in their interaction patterns over
time. To this end, it trains a deep neural network to perform two
self-supervised tasks: (a) minimizing drift in node representations and (b)
generating long-term interaction patterns from short-term ones. Failure in
these tasks for a node signals its deviation from the norm. Notably, the neural
network and tasks are carefully designed so that all required operations can be
performed in constant time (w.r.t. the graph size) in response to each new edge
in the input stream. In dynamic anomaly detection across four real-world
datasets, SLADE outperforms nine competing methods, even those leveraging label
supervision.</div><div><a href='http://arxiv.org/abs/2402.11933v1'>2402.11933v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00775v1")'>Detecting Anomalous Events in Object-centric Business Processes via
  Graph Neural Networks</div>
<div id='2403.00775v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T14:17:56Z</div><div>Authors: Alessandro Niro, Michael Werner</div><div style='padding-top: 10px; width: 80ex'>Detecting anomalies is important for identifying inefficiencies, errors, or
fraud in business processes. Traditional process mining approaches focus on
analyzing 'flattened', sequential, event logs based on a single case notion.
However, many real-world process executions exhibit a graph-like structure,
where events can be associated with multiple cases. Flattening event logs
requires selecting a single case identifier which creates a gap with the real
event data and artificially introduces anomalies in the event logs.
Object-centric process mining avoids these limitations by allowing events to be
related to different cases. This study proposes a novel framework for anomaly
detection in business processes that exploits graph neural networks and the
enhanced information offered by object-centric process mining. We first
reconstruct and represent the process dependencies of the object-centric event
logs as attributed graphs and then employ a graph convolutional autoencoder
architecture to detect anomalous events. Our results show that our approach
provides promising performance in detecting anomalies at the activity type and
attributes level, although it struggles to detect anomalies in the temporal
order of events.</div><div><a href='http://arxiv.org/abs/2403.00775v1'>2403.00775v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11887v2")'>Generative Semi-supervised Graph Anomaly Detection</div>
<div id='2402.11887v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T06:55:50Z</div><div>Authors: Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, Guansong Pang</div><div style='padding-top: 10px; width: 80ex'>This work considers a practical semi-supervised graph anomaly detection (GAD)
scenario, where part of the nodes in a graph are known to be normal,
contrasting to the unsupervised setting in most GAD studies with a fully
unlabeled graph. As expected, we find that having access to these normal nodes
helps enhance the detection performance of existing unsupervised GAD methods
when they are adapted to the semi-supervised setting. However, their
utilization of these normal nodes is limited. In this paper, we propose a novel
Generative GAD approach (GGAD) for the semi-supervised scenario to better
exploit the normal nodes. The key idea is to generate outlier nodes that
assimilate anomaly nodes in both local structure and node representations for
providing effective negative node samples in training a discriminative
one-class classifier. There have been many generative anomaly detection
approaches, but they are designed for non-graph data, and as a result, they
fail to take account of the graph structure information. Our approach tackles
this problem by generating graph structure-aware outlier nodes that have
asymmetric affinity separability from normal nodes while being enforced to
achieve egocentric closeness to normal nodes in the node representation space.
Comprehensive experiments on four real-world datasets are performed to
establish a benchmark for semi-supervised GAD and show that GGAD substantially
outperforms state-of-the-art unsupervised and semi-supervised GAD methods with
varying numbers of training normal nodes. Code will be made available at
https://github.com/mala-lab/GGAD.</div><div><a href='http://arxiv.org/abs/2402.11887v2'>2402.11887v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04468v1")'>A Survey of Graph Neural Networks in Real world: Imbalance, Noise,
  Privacy and OOD Challenges</div>
<div id='2403.04468v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T13:10:37Z</div><div>Authors: Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang Gu, Yifang Qin, Nan Yin, Senzhang Wang, Xinwang Liu, Xiao Luo, Philip S. Yu, Ming Zhang</div><div style='padding-top: 10px; width: 80ex'>Graph-structured data exhibits universality and widespread applicability
across diverse domains, such as social network analysis, biochemistry,
financial fraud detection, and network security. Significant strides have been
made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success
in these areas. However, in real-world scenarios, the training environment for
models is often far from ideal, leading to substantial performance degradation
of GNN models due to various unfavorable factors, including imbalance in data
distribution, the presence of noise in erroneous data, privacy protection of
sensitive information, and generalization capability for out-of-distribution
(OOD) scenarios. To tackle these issues, substantial efforts have been devoted
to improving the performance of GNN models in practical real-world scenarios,
as well as enhancing their reliability and robustness. In this paper, we
present a comprehensive survey that systematically reviews existing GNN models,
focusing on solutions to the four mentioned real-world challenges including
imbalance, noise, privacy, and OOD in practical scenarios that many existing
reviews have not considered. Specifically, we first highlight the four key
challenges faced by existing GNNs, paving the way for our exploration of
real-world GNN models. Subsequently, we provide detailed discussions on these
four aspects, dissecting how these solutions contribute to enhancing the
reliability and robustness of GNN models. Last but not least, we outline
promising directions and offer future perspectives in the field.</div><div><a href='http://arxiv.org/abs/2403.04468v1'>2403.04468v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15444v1")'>Towards Causal Classification: A Comprehensive Study on Graph Neural
  Networks</div>
<div id='2401.15444v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T15:35:05Z</div><div>Authors: Simi Job, Xiaohui Tao, Taotao Cai, Lin Li, Haoran Xie, Jianming Yong</div><div style='padding-top: 10px; width: 80ex'>The exploration of Graph Neural Networks (GNNs) for processing
graph-structured data has expanded, particularly their potential for causal
analysis due to their universal approximation capabilities. Anticipated to
significantly enhance common graph-based tasks such as classification and
prediction, the development of a causally enhanced GNN framework is yet to be
thoroughly investigated. Addressing this shortfall, our study delves into nine
benchmark graph classification models, testing their strength and versatility
across seven datasets spanning three varied domains to discern the impact of
causality on the predictive prowess of GNNs. This research offers a detailed
assessment of these models, shedding light on their efficiency, and flexibility
in different data environments, and highlighting areas needing advancement. Our
findings are instrumental in furthering the understanding and practical
application of GNNs in diverse datacentric fields</div><div><a href='http://arxiv.org/abs/2401.15444v1'>2401.15444v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07185v1")'>Uncertainty in Graph Neural Networks: A Survey</div>
<div id='2403.07185v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T21:54:52Z</div><div>Authors: Fangxin Wang, Yuqing Liu, Kay Liu, Yibo Wang, Sourav Medya, Philip S. Yu</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have been extensively used in various real-world
applications. However, the predictive uncertainty of GNNs stemming from diverse
sources such as inherent randomness in data and model training errors can lead
to unstable and erroneous predictions. Therefore, identifying, quantifying, and
utilizing uncertainty are essential to enhance the performance of the model for
the downstream tasks as well as the reliability of the GNN predictions. This
survey aims to provide a comprehensive overview of the GNNs from the
perspective of uncertainty with an emphasis on its integration in graph
learning. We compare and summarize existing graph uncertainty theory and
methods, alongside the corresponding downstream tasks. Thereby, we bridge the
gap between theory and practice, meanwhile connecting different GNN
communities. Moreover, our work provides valuable insights into promising
directions in this field.</div><div><a href='http://arxiv.org/abs/2403.07185v1'>2403.07185v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03350v1")'>Accurate and Scalable Estimation of Epistemic Uncertainty for Graph
  Neural Networks</div>
<div id='2401.03350v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T00:58:33Z</div><div>Authors: Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, Jayaraman J. Thiagarajan</div><div style='padding-top: 10px; width: 80ex'>While graph neural networks (GNNs) are widely used for node and graph
representation learning tasks, the reliability of GNN uncertainty estimates
under distribution shifts remains relatively under-explored. Indeed, while
post-hoc calibration strategies can be used to improve in-distribution
calibration, they need not also improve calibration under distribution shift.
However, techniques which produce GNNs with better intrinsic uncertainty
estimates are particularly valuable, as they can always be combined with
post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a
novel training framework designed to improve intrinsic GNN uncertainty
estimates. Our framework adapts the principle of stochastic data centering to
graph data through novel graph anchoring strategies, and is able to support
partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic
networks are necessary to obtain reliable estimates, we find that the
functional diversity induced by our anchoring strategies when sampling
hypotheses renders this unnecessary and allows us to support G-$\Delta$UQ on
pretrained models. Indeed, through extensive evaluation under covariate,
concept and graph size shifts, we show that G-$\Delta$UQ leads to better
calibrated GNNs for node and graph classification. Further, it also improves
performance on the uncertainty-based tasks of out-of-distribution detection and
generalization gap estimation. Overall, our work provides insights into
uncertainty estimation for GNNs, and demonstrates the utility of G-$\Delta$UQ
in obtaining reliable estimates.</div><div><a href='http://arxiv.org/abs/2401.03350v1'>2401.03350v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11494v1")'>Graph Out-of-Distribution Generalization via Causal Intervention</div>
<div id='2402.11494v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T07:49:22Z</div><div>Authors: Qitian Wu, Fan Nie, Chenxiao Yang, Tianyi Bao, Junchi Yan</div><div style='padding-top: 10px; width: 80ex'>Out-of-distribution (OOD) generalization has gained increasing attentions for
learning on graphs, as graph neural networks (GNNs) often exhibit performance
degradation with distribution shifts. The challenge is that distribution shifts
on graphs involve intricate interconnections between nodes, and the environment
labels are often absent in data. In this paper, we adopt a bottom-up
data-generative perspective and reveal a key observation through causal
analysis: the crux of GNNs' failure in OOD generalization lies in the latent
confounding bias from the environment. The latter misguides the model to
leverage environment-sensitive correlations between ego-graph features and
target nodes' labels, resulting in undesirable generalization on new unseen
nodes. Built upon this analysis, we introduce a conceptually simple yet
principled approach for training robust GNNs under node-level distribution
shifts, without prior knowledge of environment labels. Our method resorts to a
new learning objective derived from causal inference that coordinates an
environment estimator and a mixture-of-expert GNN predictor. The new approach
can counteract the confounding bias in training data and facilitate learning
generalizable predictive relations. Extensive experiment demonstrates that our
model can effectively enhance generalization with various types of distribution
shifts and yield up to 27.4\% accuracy improvement over state-of-the-arts on
graph OOD generalization benchmarks. Source codes are available at
https://github.com/fannie1208/CaNet.</div><div><a href='http://arxiv.org/abs/2402.11494v1'>2402.11494v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06239v1")'>Cooperative Classification and Rationalization for Graph Generalization</div>
<div id='2403.06239v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T15:38:20Z</div><div>Authors: Linan Yue, Qi Liu, Ye Liu, Weibo Gao, Fangzhou Yao, Wenfeng Li</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have achieved impressive results in graph
classification tasks, but they struggle to generalize effectively when faced
with out-of-distribution (OOD) data. Several approaches have been proposed to
address this problem. Among them, one solution is to diversify training
distributions in vanilla classification by modifying the data environment, yet
accessing the environment information is complex. Besides, another promising
approach involves rationalization, extracting invariant rationales for
predictions. However, extracting rationales is difficult due to limited
learning signals, resulting in less accurate rationales and diminished
predictions. To address these challenges, in this paper, we propose a
Cooperative Classification and Rationalization (C2R) method, consisting of the
classification and the rationalization module. Specifically, we first assume
that multiple environments are available in the classification module. Then, we
introduce diverse training distributions using an environment-conditional
generative network, enabling robust graph representations. Meanwhile, the
rationalization module employs a separator to identify relevant rationale
subgraphs while the remaining non-rationale subgraphs are de-correlated with
labels. Next, we align graph representations from the classification module
with rationale subgraph representations using the knowledge distillation
methods, enhancing the learning signal for rationales. Finally, we infer
multiple environments by gathering non-rationale representations and
incorporate them into the classification module for cooperative learning.
Extensive experimental results on both benchmarks and synthetic datasets
demonstrate the effectiveness of C2R. Code is available at
https://github.com/yuelinan/Codes-of-C2R.</div><div><a href='http://arxiv.org/abs/2403.06239v1'>2403.06239v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02036v1")'>Interpreting Graph Neural Networks with In-Distributed Proxies</div>
<div id='2402.02036v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:19:02Z</div><div>Authors: Zhuomin Chen, Jiaxing Zhang, Jingchao Ni, Xiaoting Li, Yuchen Bian, Md Mezbahul Islam, Ananda Mohan Mondal, Hua Wei, Dongsheng Luo</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have become a building block in graph data
processing, with wide applications in critical domains. The growing needs to
deploy GNNs in high-stakes applications necessitate explainability for users in
the decision-making processes. A popular paradigm for the explainability of
GNNs is to identify explainable subgraphs by comparing their labels with the
ones of original graphs. This task is challenging due to the substantial
distributional shift from the original graphs in the training set to the set of
explainable subgraphs, which prevents accurate prediction of labels with the
subgraphs. To address it, in this paper, we propose a novel method that
generates proxy graphs for explainable subgraphs that are in the distribution
of training data. We introduce a parametric method that employs graph
generators to produce proxy graphs. A new training objective based on
information theory is designed to ensure that proxy graphs not only adhere to
the distribution of training data but also preserve essential explanatory
factors. Such generated proxy graphs can be reliably used for approximating the
predictions of the true labels of explainable subgraphs. Empirical evaluations
across various datasets demonstrate our method achieves more accurate
explanations for GNNs.</div><div><a href='http://arxiv.org/abs/2402.02036v1'>2402.02036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00633v1")'>On Discprecncies between Perturbation Evaluations of Graph Neural
  Network Attributions</div>
<div id='2401.00633v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T02:03:35Z</div><div>Authors: Razieh Rezaei, Alireza Dizaji, Ashkan Khakzar, Anees Kazi, Nassir Navab, Daniel Rueckert</div><div style='padding-top: 10px; width: 80ex'>Neural networks are increasingly finding their way into the realm of graphs
and modeling relationships between features. Concurrently graph neural network
explanation approaches are being invented to uncover relationships between the
nodes of the graphs. However, there is a disparity between the existing
attribution methods, and it is unclear which attribution to trust. Therefore
research has introduced evaluation experiments that assess them from different
perspectives. In this work, we assess attribution methods from a perspective
not previously explored in the graph domain: retraining. The core idea is to
retrain the network on important (or not important) relationships as identified
by the attributions and evaluate how networks can generalize based on these
relationships. We reformulate the retraining framework to sidestep issues
lurking in the previous formulation and propose guidelines for correct
analysis. We run our analysis on four state-of-the-art GNN attribution methods
and five synthetic and real-world graph classification datasets. The analysis
reveals that attributions perform variably depending on the dataset and the
network. Most importantly, we observe that the famous GNNExplainer performs
similarly to an arbitrary designation of edge importance. The study concludes
that the retraining evaluation cannot be used as a generalized benchmark and
recommends it as a toolset to evaluate attributions on a specifically addressed
network, dataset, and sparsity.</div><div><a href='http://arxiv.org/abs/2401.00633v1'>2401.00633v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14578v1")'>GOAt: Explaining Graph Neural Networks via Graph Output Attribution</div>
<div id='2401.14578v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T00:32:58Z</div><div>Authors: Shengyao Lu, Keith G. Mills, Jiao He, Bang Liu, Di Niu</div><div style='padding-top: 10px; width: 80ex'>Understanding the decision-making process of Graph Neural Networks (GNNs) is
crucial to their interpretability. Most existing methods for explaining GNNs
typically rely on training auxiliary models, resulting in the explanations
remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a
novel method to attribute graph outputs to input graph features, creating GNN
explanations that are faithful, discriminative, as well as stable across
similar samples. By expanding the GNN as a sum of scalar products involving
node features, edge features and activation patterns, we propose an efficient
analytical method to compute contribution of each node or edge feature to each
scalar product and aggregate the contributions from all scalar products in the
expansion form to derive the importance of each node and edge. Through
extensive experiments on synthetic and real-world data, we show that our method
not only outperforms various state-ofthe-art GNN explainers in terms of the
commonly used fidelity metric, but also exhibits stronger discriminability, and
stability by a remarkable margin.</div><div><a href='http://arxiv.org/abs/2401.14578v1'>2401.14578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01744v2")'>Unveiling Molecular Moieties through Hierarchical Graph Explainability</div>
<div id='2402.01744v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:23:25Z</div><div>Authors: Paolo Sortino, Salvatore Contino, Ugo Perricone, Roberto Pirrone</div><div style='padding-top: 10px; width: 80ex'>Background: Graph Neural Networks (GNN) have emerged in very recent years as
a powerful tool for supporting in silico Virtual Screening. In this work we
present a GNN which uses Graph Convolutional architectures to achieve very
accurate multi-target screening. We also devised a hierarchical Explainable
Artificial Intelligence (XAI) technique to catch information directly at atom,
ring, and whole molecule level by leveraging the message passing mechanism. In
this way, we find the most relevant moieties involved in bioactivity
prediction. Results: We report a state-of-the-art GNN classifier on twenty
Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms
previous SOTA approaches proposed by the authors. Moreover, a CDK1-only
high-sensitivity version of the GNN has been designed to use our explainer in
order to avoid the inherent bias of multi-class models. The hierarchical
explainer has been validated by an expert chemist on 19 approved drugs on CDK1.
Our explainer provided information in accordance to the docking analysis for 17
out of the 19 test drugs. Conclusion: Our approach is a valid support for
shortening both the screening and the hit-to-lead phase. Detailed knowledge
about the molecular substructures that play a role in the inhibitory action,
can help the computational chemist to gain insights into the pharmacophoric
function of the molecule also for repurposing purposes.</div><div><a href='http://arxiv.org/abs/2402.01744v2'>2402.01744v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14702v1")'>FairSample: Training Fair and Accurate Graph Convolutional Neural
  Networks Efficiently</div>
<div id='2401.14702v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T08:17:12Z</div><div>Authors: Zicun Cong, Shi Baoxu, Shan Li, Jaewon Yang, Qi He, Jian Pei</div><div style='padding-top: 10px; width: 80ex'>Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and
more important concern as GCNs are adopted in many crucial applications.
Societal biases against sensitive groups may exist in many real world graphs.
GCNs trained on those graphs may be vulnerable to being affected by such
biases. In this paper, we adopt the well-known fairness notion of demographic
parity and tackle the challenge of training fair and accurate GCNs efficiently.
We present an in-depth analysis on how graph structure bias, node attribute
bias, and model parameters may affect the demographic parity of GCNs. Our
insights lead to FairSample, a framework that jointly mitigates the three types
of biases. We employ two intuitive strategies to rectify graph structures.
First, we inject edges across nodes that are in different sensitive groups but
similar in node features. Second, to enhance model fairness and retain model
quality, we develop a learnable neighbor sampling policy using reinforcement
learning. To address the bias in node features and model parameters, FairSample
is complemented by a regularization objective to optimize fairness.</div><div><a href='http://arxiv.org/abs/2401.14702v1'>2401.14702v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06017v1")'>Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New
  Benchmark</div>
<div id='2403.06017v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:33:26Z</div><div>Authors: Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, Suhang Wang, Yao Ma</div><div style='padding-top: 10px; width: 80ex'>Fair graph learning plays a pivotal role in numerous practical applications.
Recently, many fair graph learning methods have been proposed; however, their
evaluation often relies on poorly constructed semi-synthetic datasets or
substandard real-world datasets. In such cases, even a basic Multilayer
Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility
and fairness. In this work, we illustrate that many datasets fail to provide
meaningful information in the edges, which may challenge the necessity of using
graph structures in these problems. To address these issues, we develop and
introduce a collection of synthetic, semi-synthetic, and real-world datasets
that fulfill a broad spectrum of requirements. These datasets are thoughtfully
designed to include relevant graph structures and bias information crucial for
the fair evaluation of models. The proposed synthetic and semi-synthetic
datasets offer the flexibility to create data with controllable bias
parameters, thereby enabling the generation of desired datasets with
user-defined bias values with ease. Moreover, we conduct systematic evaluations
of these proposed datasets and establish a unified evaluation approach for fair
graph learning models. Our extensive experimental results with fair graph
learning methods across our datasets demonstrate their effectiveness in
benchmarking the performance of these methods. Our datasets and the code for
reproducing our experiments are available at
https://github.com/XweiQ/Benchmark-GraphFairness.</div><div><a href='http://arxiv.org/abs/2403.06017v1'>2403.06017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12161v2")'>Endowing Pre-trained Graph Models with Provable Fairness</div>
<div id='2402.12161v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T14:16:08Z</div><div>Authors: Zhongjian Zhang, Mengmei Zhang, Yue Yu, Cheng Yang, Jiawei Liu, Chuan Shi</div><div style='padding-top: 10px; width: 80ex'>Pre-trained graph models (PGMs) aim to capture transferable inherent
structural properties and apply them to different downstream tasks. Similar to
pre-trained language models, PGMs also inherit biases from human society,
resulting in discriminatory behavior in downstream applications. The debiasing
process of existing fair methods is generally coupled with parameter
optimization of GNNs. However, different downstream tasks may be associated
with different sensitive attributes in reality, directly employing existing
methods to improve the fairness of PGMs is inflexible and inefficient.
Moreover, most of them lack a theoretical guarantee, i.e., provable lower
bounds on the fairness of model predictions, which directly provides assurance
in a practical scenario. To overcome these limitations, we propose a novel
adapter-tuning framework that endows pre-trained graph models with provable
fairness (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains
a parameter-efficient adapter to flexibly improve the fairness of PGMs in
downstream tasks. Specifically, we design a sensitive semantic augmenter on
node representations, to extend the node representations with different
sensitive attribute semantics for each node. The extended representations will
be used to further train an adapter, to prevent the propagation of sensitive
attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR,
we quantify whether the fairness of each node is provable, i.e., predictions
are always fair within a certain range of sensitive attribute semantics.
Experimental evaluations on real-world datasets demonstrate that GraphPAR
achieves state-of-the-art prediction performance and fairness on node
classification task. Furthermore, based on our GraphPAR, around 90\% nodes have
provable fairness.</div><div><a href='http://arxiv.org/abs/2402.12161v2'>2402.12161v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12474v1")'>FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive
  Information Neutralization</div>
<div id='2403.12474v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T06:22:58Z</div><div>Authors: Cheng Yang, Jixi Liu, Yunhe Yan, Chuan Shi</div><div style='padding-top: 10px; width: 80ex'>Despite the remarkable success of graph neural networks (GNNs) in modeling
graph-structured data, like other machine learning models, GNNs are also
susceptible to making biased predictions based on sensitive attributes, such as
race and gender. For fairness consideration, recent state-of-the-art (SOTA)
methods propose to filter out sensitive information from inputs or
representations, e.g., edge dropping or feature masking. However, we argue that
such filtering-based strategies may also filter out some non-sensitive feature
information, leading to a sub-optimal trade-off between predictive performance
and fairness. To address this issue, we unveil an innovative
neutralization-based paradigm, where additional Fairness-facilitating Features
(F3) are incorporated into node features or representations before message
passing. The F3 are expected to statistically neutralize the sensitive bias in
node representations and provide additional nonsensitive information. We also
provide theoretical explanations for our rationale, concluding that F3 can be
realized by emphasizing the features of each node's heterogeneous neighbors
(neighbors with different sensitive attributes). We name our method as FairSIN,
and present three implementation variants from both data-centric and
model-centric perspectives. Experimental results on five benchmark datasets
with three different GNN backbones show that FairSIN significantly improves
fairness metrics while maintaining high prediction accuracies.</div><div><a href='http://arxiv.org/abs/2403.12474v1'>2403.12474v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16784v1")'>Graph Fairness Learning under Distribution Shifts</div>
<div id='2401.16784v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:51:24Z</div><div>Authors: Yibo Li, Xiao Wang, Yujie Xing, Shaohua Fan, Ruijia Wang, Yaoqi Liu, Chuan Shi</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have achieved remarkable performance on
graph-structured data. However, GNNs may inherit prejudice from the training
data and make discriminatory predictions based on sensitive attributes, such as
gender and race. Recently, there has been an increasing interest in ensuring
fairness on GNNs, but all of them are under the assumption that the training
and testing data are under the same distribution, i.e., training data and
testing data are from the same graph. Will graph fairness performance decrease
under distribution shifts? How does distribution shifts affect graph fairness
learning? All these open questions are largely unexplored from a theoretical
perspective. To answer these questions, we first theoretically identify the
factors that determine bias on a graph. Subsequently, we explore the factors
influencing fairness on testing graphs, with a noteworthy factor being the
representation distances of certain groups between the training and testing
graph. Motivated by our theoretical analysis, we propose our framework
FatraGNN. Specifically, to guarantee fairness performance on unknown testing
graphs, we propose a graph generator to produce numerous graphs with
significant bias and under different distributions. Then we minimize the
representation distances for each certain group between the training graph and
generated graphs. This empowers our model to achieve high classification and
fairness performance even on generated graphs with significant bias, thereby
effectively handling unknown testing graphs. Experiments on real-world and
semi-synthetic datasets demonstrate the effectiveness of our model in terms of
both accuracy and fairness.</div><div><a href='http://arxiv.org/abs/2401.16784v1'>2401.16784v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12937v1")'>GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural
  Networks</div>
<div id='2402.12937v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T11:38:52Z</div><div>Authors: Anuj Kumar Sirohi, Anjali Gupta, Sayan Ranu, Sandeep Kumar, Amitabha Bagchi</div><div style='padding-top: 10px; width: 80ex'>We address the growing apprehension that GNNs, in the absence of fairness
constraints, might produce biased decisions that disproportionately affect
underprivileged groups or individuals. Departing from previous work, we
introduce for the first time a method for incorporating the Gini coefficient as
a measure of fairness to be used within the GNN framework. Our proposal,
GRAPHGINI, works with the two different goals of individual and group fairness
in a single system, while maintaining high prediction accuracy. GRAPHGINI
enforces individual fairness through learnable attention scores that help in
aggregating more information through similar nodes. A heuristic-based maximum
Nash social welfare constraint ensures the maximum possible group fairness.
Both the individual fairness constraint and the group fairness constraint are
stated in terms of a differentiable approximation of the Gini coefficient. This
approximation is a contribution that is likely to be of interest even beyond
the scope of the problem studied in this paper. Unlike other state-of-the-art,
GRAPHGINI automatically balances all three optimization objectives (utility,
individual, and group fairness) of the GNN and is free from any manual tuning
of weight parameters. Extensive experimentation on real-world datasets
showcases the efficacy of GRAPHGINI in making significant improvements in
individual fairness compared to all currently available state-of-the-art
methods while maintaining utility and group equality.</div><div><a href='http://arxiv.org/abs/2402.12937v1'>2402.12937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12824v1")'>MAPPING: Debiasing Graph Neural Networks for Fair Node Classification
  with Limited Sensitive Information Leakage</div>
<div id='2401.12824v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:59:46Z</div><div>Authors: Ying Song, Balaji Palanisamy</div><div style='padding-top: 10px; width: 80ex'>Despite remarkable success in diverse web-based applications, Graph Neural
Networks(GNNs) inherit and further exacerbate historical discrimination and
social stereotypes, which critically hinder their deployments in high-stake
domains such as online clinical diagnosis, financial crediting, etc. However,
current fairness research that primarily craft on i.i.d data, cannot be
trivially replicated to non-i.i.d. graph structures with topological dependence
among samples. Existing fair graph learning typically favors pairwise
constraints to achieve fairness but fails to cast off dimensional limitations
and generalize them into multiple sensitive attributes; besides, most studies
focus on in-processing techniques to enforce and calibrate fairness,
constructing a model-agnostic debiasing GNN framework at the pre-processing
stage to prevent downstream misuses and improve training reliability is still
largely under-explored. Furthermore, previous work on GNNs tend to enhance
either fairness or privacy individually but few probe into their interplays. In
this paper, we propose a novel model-agnostic debiasing framework named MAPPING
(\underline{M}asking \underline{A}nd \underline{P}runing and
Message-\underline{P}assing train\underline{ING}) for fair node classification,
in which we adopt the distance covariance($dCov$)-based fairness constraints to
simultaneously reduce feature and topology biases in arbitrary dimensions, and
combine them with adversarial debiasing to confine the risks of attribute
inference attacks. Experiments on real-world datasets with different GNN
variants demonstrate the effectiveness and flexibility of MAPPING. Our results
show that MAPPING can achieve better trade-offs between utility and fairness,
and mitigate privacy risks of sensitive information leakage.</div><div><a href='http://arxiv.org/abs/2401.12824v1'>2401.12824v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04829v3")'>GNNShap: Scalable and Accurate GNN Explanation using Shapley Values</div>
<div id='2401.04829v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T21:26:33Z</div><div>Authors: Selahattin Akkas, Ariful Azad</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) are popular machine learning models for graphs
with many applications across scientific domains. However, GNNs are considered
black box models, and it is challenging to understand how the model makes
predictions. Game theoric Shapley value approaches are popular explanation
methods in other domains but are not well-studied for graphs. Some studies have
proposed Shapley value based GNN explanations, yet they have several
limitations: they consider limited samples to approximate Shapley values; some
mainly focus on small and large coalition sizes, and they are an order of
magnitude slower than other explanation methods, making them inapplicable to
even moderate-size graphs. In this work, we propose GNNShap, which provides
explanations for edges since they provide more natural explanations for graphs
and more fine-grained explanations. We overcome the limitations by sampling
from all coalition sizes, parallelizing the sampling on GPUs, and speeding up
model predictions by batching. GNNShap gives better fidelity scores and faster
explanations than baselines on real-world datasets. The code is available at
https://github.com/HipGraph/GNNShap.</div><div><a href='http://arxiv.org/abs/2401.04829v3'>2401.04829v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04133v1")'>SynHIN: Generating Synthetic Heterogeneous Information Network for
  Explainable AI</div>
<div id='2401.04133v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T04:43:36Z</div><div>Authors: Ming-Yi Hong, Yi-Hsiang Huang, You-Chen Teng, Chih-Yu Wang, Che Lin</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) excel in various domains, from detecting
e-commerce spam to social network classification problems. However, the lack of
public graph datasets hampers research progress, particularly in heterogeneous
information networks (HIN). The demand for datasets for fair HIN comparisons is
growing due to advancements in GNN interpretation models. In response, we
propose SynHIN, a unique method for generating synthetic heterogeneous
information networks. SynHIN identifies motifs in real-world datasets,
summarizes graph statistics, and constructs a synthetic network. Our approach
utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN
from primary motif clusters. After In/Our-Cluster mergers and a post-pruning
process fitting the real dataset constraints, we ensure the synthetic graph
statistics align closely with the reference one. SynHIN generates a synthetic
heterogeneous graph dataset for node classification tasks, using the primary
motif as the explanation ground truth. It can adapt and address the lack of
heterogeneous graph datasets and motif ground truths, proving beneficial for
assessing heterogeneous graph neural network explainers. We further present a
benchmark dataset for future heterogeneous graph explainer model research. Our
work marks a significant step towards explainable AI in HGNNs.</div><div><a href='http://arxiv.org/abs/2401.04133v1'>2401.04133v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02290v1")'>Path-based Explanation for Knowledge Graph Completion</div>
<div id='2401.02290v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T14:19:37Z</div><div>Authors: Heng Chang, Jiangnan Ye, Alejo Lopez Avila, Jinhua Du, Jia Li</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph
Completion (KGC) by modelling how entities and relations interact in recent
years. However, the explanation of the predicted facts has not caught the
necessary attention. Proper explanations for the results of GNN-based KGC
models increase model transparency and help researchers develop more reliable
models. Existing practices for explaining KGC tasks rely on
instance/subgraph-based approaches, while in some scenarios, paths can provide
more user-friendly and interpretable explanations. Nonetheless, the methods for
generating path-based explanations for KGs have not been well-explored. To
address this gap, we propose Power-Link, the first path-based KGC explainer
that explores GNN-based models. We design a novel simplified graph-powering
technique, which enables the generation of path-based explanations with a fully
parallelisable and memory-efficient training scheme. We further introduce three
new metrics for quantitative evaluation of the explanations, together with a
qualitative human evaluation. Extensive experiments demonstrate that Power-Link
outperforms the SOTA baselines in interpretability, efficiency, and
scalability.</div><div><a href='http://arxiv.org/abs/2401.02290v1'>2401.02290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06936v1")'>Counterfactual Reasoning with Knowledge Graph Embeddings</div>
<div id='2403.06936v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:21:39Z</div><div>Authors: Lena Zellinger, Andreas Stephan, Benjamin Roth</div><div style='padding-top: 10px; width: 80ex'>Knowledge graph embeddings (KGEs) were originally developed to infer true but
missing facts in incomplete knowledge repositories. In this paper, we link
knowledge graph completion and counterfactual reasoning via our new task CFKGR.
We model the original world state as a knowledge graph, hypothetical scenarios
as edges added to the graph, and plausible changes to the graph as inferences
from logical rules. We create corresponding benchmark datasets, which contain
diverse hypothetical scenarios with plausible changes to the original knowledge
graph and facts that should be retained. We develop COULDD, a general method
for adapting existing knowledge graph embeddings given a hypothetical premise,
and evaluate it on our benchmark. Our results indicate that KGEs learn patterns
in the graph without explicit training. We further observe that KGEs adapted
with COULDD solidly detect plausible counterfactual changes to the graph that
follow these patterns. An evaluation on human-annotated data reveals that KGEs
adapted with COULDD are mostly unable to recognize changes to the graph that do
not follow learned inference rules. In contrast, ChatGPT mostly outperforms
KGEs in detecting plausible changes to the graph but has poor knowledge
retention. In summary, CFKGR connects two previously distinct areas, namely KG
completion and counterfactual reasoning.</div><div><a href='http://arxiv.org/abs/2403.06936v1'>2403.06936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17585v1")'>Propagation and Pitfalls: Reasoning-based Assessment of Knowledge
  Editing through Counterfactual Tasks</div>
<div id='2401.17585v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T04:12:59Z</div><div>Authors: Wenyue Hua, Jiang Guo, Mingwen Dong, Henghui Zhu, Patrick Ng, Zhiguo Wang</div><div style='padding-top: 10px; width: 80ex'>Current approaches of knowledge editing struggle to effectively propagate
updates to interconnected facts. In this work, we delve into the barriers that
hinder the appropriate propagation of updated knowledge within these models for
accurate reasoning. To support our analysis, we introduce a novel
reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing
dataset) -- which covers six common reasoning schemes in real world. We conduct
a thorough analysis of existing knowledge editing techniques, including input
augmentation, finetuning, and locate-and-edit. We found that all model editing
methods show notably low performance on this dataset, especially in certain
reasoning schemes. Our analysis over the chain-of-thought generation of edited
models further uncover key reasons behind the inadequacy of existing knowledge
editing methods from a reasoning standpoint, involving aspects on fact-wise
editing, fact recall ability, and coherence in generation. We will make our
benchmark publicly available.</div><div><a href='http://arxiv.org/abs/2401.17585v1'>2401.17585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03732v1")'>Deep Outdated Fact Detection in Knowledge Graphs</div>
<div id='2402.03732v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T05:58:15Z</div><div>Authors: Huiling Tu, Shuo Yu, Vidya Saikrishna, Feng Xia, Karin Verspoor</div><div style='padding-top: 10px; width: 80ex'>Knowledge graphs (KGs) have garnered significant attention for their vast
potential across diverse domains. However, the issue of outdated facts poses a
challenge to KGs, affecting their overall quality as real-world information
evolves. Existing solutions for outdated fact detection often rely on manual
recognition. In response, this paper presents DEAN (Deep outdatEd fAct
detectioN), a novel deep learning-based framework designed to identify outdated
facts within KGs. DEAN distinguishes itself by capturing implicit structural
information among facts through comprehensive modeling of both entities and
relations. To effectively uncover latent out-of-date information, DEAN employs
a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph,
weighted by the number of entities. Experimental results demonstrate the
effectiveness and superiority of DEAN over state-of-the-art baseline methods.</div><div><a href='http://arxiv.org/abs/2402.03732v1'>2402.03732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08824v1")'>Disambiguated Node Classification with Graph Neural Networks</div>
<div id='2402.08824v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T22:07:57Z</div><div>Authors: Tianxiang Zhao, Xiang Zhang, Suhang Wang</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data across various domains. Despite their great
successful, one critical challenge is often overlooked by existing works, i.e.,
the learning of message propagation that can generalize effectively to
underrepresented graph regions. These minority regions often exhibit irregular
homophily/heterophily patterns and diverse neighborhood class distributions,
resulting in ambiguity. In this work, we investigate the ambiguity problem
within GNNs, its impact on representation learning, and the development of
richer supervision signals to fight against this problem. We conduct a
fine-grained evaluation of GNN, analyzing the existence of ambiguity in
different graph regions and its relation with node positions. To disambiguate
node embeddings, we propose a novel method, {\method}, which exploits
additional optimization guidance to enhance representation learning,
particularly for nodes in ambiguous regions. {\method} identifies ambiguous
nodes based on temporal inconsistency of predictions and introduces a
disambiguation regularization by employing contrastive learning in a
topology-aware manner. {\method} promotes discriminativity of node
representations and can alleviating semantic mixing caused by message
propagation, effectively addressing the ambiguity problem. Empirical results
validate the efficiency of {\method} and highlight its potential to improve GNN
performance in underrepresented graph regions.</div><div><a href='http://arxiv.org/abs/2402.08824v1'>2402.08824v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14340v3")'>Estimation of partially known Gaussian graphical models with score-based
  structural priors</div>
<div id='2401.14340v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T17:39:47Z</div><div>Authors: MartÃ­n Sevilla, Antonio GarcÃ­a Marques, Santiago Segarra</div><div style='padding-top: 10px; width: 80ex'>We propose a novel algorithm for the support estimation of partially known
Gaussian graphical models that incorporates prior information about the
underlying graph. In contrast to classical approaches that provide a point
estimate based on a maximum likelihood or a maximum a posteriori criterion
using (simple) priors on the precision matrix, we consider a prior on the graph
and rely on annealed Langevin diffusion to generate samples from the posterior
distribution. Since the Langevin sampler requires access to the score function
of the underlying graph prior, we use graph neural networks to effectively
estimate the score from a graph dataset (either available beforehand or
generated from a known distribution). Numerical experiments demonstrate the
benefits of our approach.</div><div><a href='http://arxiv.org/abs/2401.14340v3'>2401.14340v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11292v1")'>Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction</div>
<div id='2403.11292v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T18:08:22Z</div><div>Authors: Asma Sattar, Georgios Deligiorgis, Marco Trincavelli, Davide Bacciu</div><div style='padding-top: 10px; width: 80ex'>Dynamic multi-relational graphs are an expressive relational representation
for data enclosing entities and relations of different types, and where
relationships are allowed to vary in time. Addressing predictive tasks over
such data requires the ability to find structure embeddings that capture the
diversity of the relationships involved, as well as their dynamic evolution. In
this work, we establish a novel class of challenging tasks for dynamic
multi-relational graphs involving out-of-domain link prediction, where the
relationship being predicted is not available in the input graph. We then
introduce a novel Graph Neural Network model, named GOOD, designed specifically
to tackle the out-of-domain generalization problem. GOOD introduces a novel
design concept for multi-relation embedding aggregation, based on the idea that
good representations are such when it is possible to disentangle the mixing
proportions of the different relational embeddings that have produced it. We
also propose five benchmarks based on two retail domains, where we show that
GOOD can effectively generalize predictions out of known relationship types and
achieve state-of-the-art results. Most importantly, we provide insights into
problems where out-of-domain prediction might be preferred to an in-domain
formulation, that is, where the relationship to be predicted has very few
positive examples.</div><div><a href='http://arxiv.org/abs/2403.11292v1'>2403.11292v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17472v1")'>Fraud Detection with Binding Global and Local Relational Interaction</div>
<div id='2402.17472v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T12:53:15Z</div><div>Authors: Haolin Li, Shuyang Jiang, Lifeng Zhang, Siyuan Du, Guangnan Ye, Hongfeng Chai</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Network has been proved to be effective for fraud detection for
its capability to encode node interaction and aggregate features in a holistic
view. Recently, Transformer network with great sequence encoding ability, has
also outperformed other GNN-based methods in literatures. However, both
GNN-based and Transformer-based networks only encode one perspective of the
whole graph, while GNN encodes global features and Transformer network encodes
local ones. Furthermore, previous works ignored encoding global interaction
features of the heterogeneous graph with separate networks, thus leading to
suboptimal performance. In this work, we present a novel framework called
Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds
local and global features into a target node. The simple yet effective network
applies a modified GAGA module where each transformer layer is followed by a
cross-relation aggregation layer, to encode local embeddings and node
interactions across different relations. Apart from the Transformer-based
network, we further introduce a Relation-Aware GNN module to learn global
embeddings, which is later merged into the local embeddings by an attention
fusion module and a skip connection. Extensive experiments on two popular
public datasets and an industrial dataset demonstrate that RAGFormer achieves
the state-of-the-art performance. Substantial analysis experiments validate the
effectiveness of each submodule of RAGFormer and its high efficiency in
utilizing small-scale data and low hyper-parameter sensitivity.</div><div><a href='http://arxiv.org/abs/2402.17472v1'>2402.17472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14708v1")'>CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph
  Neural Networks</div>
<div id='2402.14708v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:08:09Z</div><div>Authors: Yifan Duan, Guibin Zhang, Shilong Wang, Xiaojiang Peng, Wang Ziqi, Junyuan Mao, Hao Wu, Xinke Jiang, Kun Wang</div><div style='padding-top: 10px; width: 80ex'>Credit card fraud poses a significant threat to the economy. While Graph
Neural Network (GNN)-based fraud detection methods perform well, they often
overlook the causal effect of a node's local structure on predictions. This
paper introduces a novel method for credit card fraud detection, the
\textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal
\textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork
(CaT-GNN), which leverages causal invariant learning to reveal inherent
correlations within transaction data. By decomposing the problem into discovery
and intervention phases, CaT-GNN identifies causal nodes within the transaction
graph and applies a causal mixup strategy to enhance the model's robustness and
interpretability. CaT-GNN consists of two key components: Causal-Inspector and
Causal-Intervener. The Causal-Inspector utilizes attention weights in the
temporal attention mechanism to identify causal and environment nodes without
introducing additional parameters. Subsequently, the Causal-Intervener performs
a causal mixup enhancement on environment nodes based on the set of nodes.
Evaluated on three datasets, including a private financial dataset and two
public datasets, CaT-GNN demonstrates superior performance over existing
state-of-the-art methods. Our findings highlight the potential of integrating
causal reasoning with graph neural networks to improve fraud detection
capabilities in financial transactions.</div><div><a href='http://arxiv.org/abs/2402.14708v1'>2402.14708v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00299v1")'>Attention-based Dynamic Multilayer Graph Neural Networks for Loan
  Default Prediction</div>
<div id='2402.00299v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T03:20:53Z</div><div>Authors: Sahab Zandi, Kamesh Korangi, MarÃ­a ÃskarsdÃ³ttir, Christophe Mues, CristiÃ¡n Bravo</div><div style='padding-top: 10px; width: 80ex'>Whereas traditional credit scoring tends to employ only individual borrower-
or loan-level predictors, it has been acknowledged for some time that
connections between borrowers may result in default risk propagating over a
network. In this paper, we present a model for credit risk assessment
leveraging a dynamic multilayer network built from a Graph Neural Network and a
Recurrent Neural Network, each layer reflecting a different source of network
connection. We test our methodology in a behavioural credit scoring context
using a dataset provided by U.S. mortgage financier Freddie Mac, in which
different types of connections arise from the geographical location of the
borrower and their choice of mortgage provider. The proposed model considers
both types of connections and the evolution of these connections over time. We
enhance the model by using a custom attention mechanism that weights the
different time snapshots according to their importance. After testing multiple
configurations, a model with GAT, LSTM, and the attention mechanism provides
the best results. Empirical results demonstrate that, when it comes to
predicting probability of default for the borrowers, our proposed model brings
both better results and novel insights for the analysis of the importance of
connections and timestamps, compared to traditional methods.</div><div><a href='http://arxiv.org/abs/2402.00299v1'>2402.00299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13429v1")'>Detecting and Triaging Spoofing using Temporal Convolutional Networks</div>
<div id='2403.13429v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T09:17:12Z</div><div>Authors: Kaushalya Kularatnam, Tania Stathaki</div><div style='padding-top: 10px; width: 80ex'>As algorithmic trading and electronic markets continue to transform the
landscape of financial markets, detecting and deterring rogue agents to
maintain a fair and efficient marketplace is crucial. The explosion of large
datasets and the continually changing tricks of the trade make it difficult to
adapt to new market conditions and detect bad actors. To that end, we propose a
framework that can be adapted easily to various problems in the space of
detecting market manipulation. Our approach entails initially employing a
labelling algorithm which we use to create a training set to learn a weakly
supervised model to identify potentially suspicious sequences of order book
states. The main goal here is to learn a representation of the order book that
can be used to easily compare future events. Subsequently, we posit the
incorporation of expert assessment to scrutinize specific flagged order book
states. In the event of an expert's unavailability, recourse is taken to the
application of a more complex algorithm on the identified suspicious order book
states. We then conduct a similarity search between any new representation of
the order book against the expert labelled representations to rank the results
of the weak learner. We show some preliminary results that are promising to
explore further in this direction</div><div><a href='http://arxiv.org/abs/2403.13429v1'>2403.13429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14033v1")'>VN Network: Embedding Newly Emerging Entities with Virtual Neighbors</div>
<div id='2402.14033v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T03:04:34Z</div><div>Authors: Yongquan He, Zihan Wang, Peng Zhang, Zhaopeng Tu, Zhaochun Ren</div><div style='padding-top: 10px; width: 80ex'>Embedding entities and relations into continuous vector spaces has attracted
a surge of interest in recent years. Most embedding methods assume that all
test entities are available during training, which makes it time-consuming to
retrain embeddings for newly emerging entities. To address this issue, recent
works apply the graph neural network on the existing neighbors of the unseen
entities. In this paper, we propose a novel framework, namely Virtual Neighbor
(VN) network, to address three key challenges. Firstly, to reduce the neighbor
sparsity problem, we introduce the concept of the virtual neighbors inferred by
rules. And we assign soft labels to these neighbors by solving a
rule-constrained problem, rather than simply regarding them as unquestionably
true. Secondly, many existing methods only use one-hop or two-hop neighbors for
aggregation and ignore the distant information that may be helpful. Instead, we
identify both logic and symmetric path rules to capture complex patterns.
Finally, instead of one-time injection of rules, we employ an iterative
learning scheme between the embedding method and virtual neighbor prediction to
capture the interactions within. Experimental results on two knowledge graph
completion tasks demonstrate that our VN network significantly outperforms
state-of-the-art baselines. Furthermore, results on Subject/Object-R show that
our proposed VN network is highly robust to the neighbor sparsity problem.</div><div><a href='http://arxiv.org/abs/2402.14033v1'>2402.14033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01809v1")'>PhenoLinker: Phenotype-Gene Link Prediction and Explanation using
  Heterogeneous Graph Neural Networks</div>
<div id='2402.01809v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:35:21Z</div><div>Authors: Jose L. Mellina Andreu, Luis Bernal, Antonio F. Skarmeta, Mina Ryten, Sara Ãlvarez, Alejandro Cisterna GarcÃ­a, Juan A. BotÃ­a</div><div style='padding-top: 10px; width: 80ex'>The association of a given human phenotype to a genetic variant remains a
critical challenge for biology. We present a novel system called PhenoLinker
capable of associating a score to a phenotype-gene relationship by using
heterogeneous information networks and a convolutional neural network-based
model for graphs, which can provide an explanation for the predictions. This
system can aid in the discovery of new associations and in the understanding of
the consequences of human genetic variation.</div><div><a href='http://arxiv.org/abs/2402.01809v1'>2402.01809v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00030v2")'>GraphPub: Generation of Differential Privacy Graph with High
  Availability</div>
<div id='2403.00030v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T20:02:55Z</div><div>Authors: Wanghan Xu, Bin Shi, Ao Liu, Jiqiang Zhang, Bo Dong</div><div style='padding-top: 10px; width: 80ex'>In recent years, with the rapid development of graph neural networks (GNN),
more and more graph datasets have been published for GNN tasks. However, when
an upstream data owner publishes graph data, there are often many privacy
concerns, because many real-world graph data contain sensitive information like
person's friend list. Differential privacy (DP) is a common method to protect
privacy, but due to the complex topological structure of graph data, applying
DP on graphs often affects the message passing and aggregation of GNN models,
leading to a decrease in model accuracy. In this paper, we propose a novel
graph edge protection framework, graph publisher (GraphPub), which can protect
graph topology while ensuring that the availability of data is basically
unchanged. Through reverse learning and the encoder-decoder mechanism, we
search for some false edges that do not have a large negative impact on the
aggregation of node features, and use them to replace some real edges. The
modified graph will be published, which is difficult to distinguish between
real and false data. Sufficient experiments prove that our framework achieves
model accuracy close to the original graph with an extremely low privacy
budget.</div><div><a href='http://arxiv.org/abs/2403.00030v2'>2403.00030v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10995v1")'>Edge Private Graph Neural Networks with Singular Value Perturbation</div>
<div id='2403.10995v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T18:44:56Z</div><div>Authors: Tingting Tang, Yue Niu, Salman Avestimehr, Murali Annavaram</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) play a key role in learning representations from
graph-structured data and are demonstrated to be useful in many applications.
However, the GNN training pipeline has been shown to be vulnerable to node
feature leakage and edge extraction attacks. This paper investigates a scenario
where an attacker aims to recover private edge information from a trained GNN
model. Previous studies have employed differential privacy (DP) to add noise
directly to the adjacency matrix or a compact graph representation. The added
perturbations cause the graph structure to be substantially morphed, reducing
the model utility. We propose a new privacy-preserving GNN training algorithm,
Eclipse, that maintains good model utility while providing strong privacy
protection on edges. Eclipse is based on two key observations. First, adjacency
matrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains
GNNs with a low-rank format of the graph via singular values decomposition
(SVD), rather than the original graph. Using the low-rank format, Eclipse
preserves the primary graph topology and removes the remaining residual edges.
Eclipse adds noise to the low-rank singular values instead of the entire graph,
thereby preserving the graph privacy while still maintaining enough of the
graph structure to maintain model utility. We theoretically show Eclipse
provide formal DP guarantee on edges. Experiments on benchmark graph datasets
show that Eclipse achieves significantly better privacy-utility tradeoff
compared to existing privacy-preserving GNN training methods. In particular,
under strong privacy constraints ($\epsilon$ &lt; 4), Eclipse shows significant
gains in the model utility by up to 46%. We further demonstrate that Eclipse
also has better resilience against common edge attacks (e.g., LPA), lowering
the attack AUC by up to 5% compared to other state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2403.10995v1'>2403.10995v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08593v1")'>Graph Feature Preprocessor: Real-time Extraction of Subgraph-based
  Features from Transaction Graphs</div>
<div id='2402.08593v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:53:48Z</div><div>Authors: Jovan BlanuÅ¡a, Maximo Cravero Baraja, Andreea Anghel, Luc von NiederhÃ¤usern, Erik Altman, Haris Pozidis, Kubilay Atasu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present "Graph Feature Preprocessor", a software library
for detecting typical money laundering and fraud patterns in financial
transaction graphs in real time. These patterns are used to produce a rich set
of transaction features for downstream machine learning training and inference
tasks such as money laundering detection. We show that our enriched transaction
features dramatically improve the prediction accuracy of
gradient-boosting-based machine learning models. Our library exploits multicore
parallelism, maintains a dynamic in-memory graph, and efficiently mines
subgraph patterns in the incoming transaction stream, which enables it to be
operated in a streaming manner. We evaluate our library using highly-imbalanced
synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets.
In these datasets, the proportion of illicit transactions is very small, which
makes the learning process challenging. Our solution, which combines our Graph
Feature Preprocessor and gradient-boosting-based machine learning models, is
able to detect these illicit transactions with higher minority-class F1 scores
than standard graph neural networks. In addition, the end-to-end throughput
rate of our solution executed on a multicore CPU outperforms the graph neural
network baselines executed on a powerful V100 GPU. Overall, the combination of
high accuracy, a high throughput rate, and low latency of our solution
demonstrates the practical value of our library in real-world applications.
Graph Feature Preprocessor has been integrated into IBM mainframe software
products, namely "IBM Cloud Pak for Data on Z" and "AI Toolkit for IBM Z and
LinuxONE".</div><div><a href='http://arxiv.org/abs/2402.08593v1'>2402.08593v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09495v2")'>On the Potential of Network-Based Features for Fraud Detection</div>
<div id='2402.09495v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:20:09Z</div><div>Authors: Catayoun Azarm, Erman Acar, Mickey van Zeelt</div><div style='padding-top: 10px; width: 80ex'>Online transaction fraud presents substantial challenges to businesses and
consumers, risking significant financial losses. Conventional rule-based
systems struggle to keep pace with evolving fraud tactics, leading to high
false positive rates and missed detections. Machine learning techniques offer a
promising solution by leveraging historical data to identify fraudulent
patterns. This article explores using the personalised PageRank (PPR) algorithm
to capture the social dynamics of fraud by analysing relationships between
financial accounts. The primary objective is to compare the performance of
traditional features with the addition of PPR in fraud detection models.
Results indicate that integrating PPR enhances the model's predictive power,
surpassing the baseline model. Additionally, the PPR feature provides unique
and valuable information, evidenced by its high feature importance score.
Feature stability analysis confirms consistent feature distributions across
training and test datasets.</div><div><a href='http://arxiv.org/abs/2402.09495v2'>2402.09495v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06482v1")'>Financial Default Prediction via Motif-preserving Graph Neural Network
  with Curriculum Learning</div>
<div id='2403.06482v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T07:44:56Z</div><div>Authors: Daixin Wang, Zhiqiang Zhang, Yeyu Zhao, Kai Huang, Yulin Kang, Jun Zhou</div><div style='padding-top: 10px; width: 80ex'>User financial default prediction plays a critical role in credit risk
forecasting and management. It aims at predicting the probability that the user
will fail to make the repayments in the future. Previous methods mainly extract
a set of user individual features regarding his own profiles and behaviors and
build a binary-classification model to make default predictions. However, these
methods cannot get satisfied results, especially for users with limited
information. Although recent efforts suggest that default prediction can be
improved by social relations, they fail to capture the higher-order topology
structure at the level of small subgraph patterns. In this paper, we fill in
this gap by proposing a motif-preserving Graph Neural Network with curriculum
learning (MotifGNN) to jointly learn the lower-order structures from the
original graph and higherorder structures from multi-view motif-based graphs
for financial default prediction. Specifically, to solve the problem of weak
connectivity in motif-based graphs, we design the motif-based gating mechanism.
It utilizes the information learned from the original graph with good
connectivity to strengthen the learning of the higher-order structure. And
considering that the motif patterns of different samples are highly unbalanced,
we propose a curriculum learning mechanism on the whole learning process to
more focus on the samples with uncommon motif distributions. Extensive
experiments on one public dataset and two industrial datasets all demonstrate
the effectiveness of our proposed method.</div><div><a href='http://arxiv.org/abs/2403.06482v1'>2403.06482v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17363v1")'>CGGM: A conditional graph generation model with adaptive sparsity for
  node anomaly detection in IoT networks</div>
<div id='2402.17363v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T09:55:34Z</div><div>Authors: Xianshi Su, Munan Li, Tongbang Jiang, Hao Long</div><div style='padding-top: 10px; width: 80ex'>Dynamic graphs are extensively employed for detecting anomalous behavior in
nodes within the Internet of Things (IoT). Generative models are often used to
address the issue of imbalanced node categories in dynamic graphs.
Nevertheless, the constraints it faces include the monotonicity of adjacency
relationships, the difficulty in constructing multi-dimensional features for
nodes, and the lack of a method for end-to-end generation of multiple
categories of nodes. This paper presents a novel graph generation model, called
CGGM, designed specifically to generate a larger number of nodes belonging to
the minority class. The mechanism for generating an adjacency matrix, through
adaptive sparsity, enhances flexibility in its structure. The feature
generation module, called multidimensional features generator (MFG) to generate
node features along with topological information. Labels are transformed into
embedding vectors, serving as conditional constraints to control the generation
of synthetic data across multiple categories. Using a multi-stage loss, the
distribution of synthetic data is adjusted to closely resemble that of real
data. In extensive experiments, we show that CGGM's synthetic data outperforms
state-of-the-art methods across various metrics. Our results demonstrate
efficient generation of diverse data categories, robustly enhancing
multi-category classification model performance.</div><div><a href='http://arxiv.org/abs/2402.17363v1'>2402.17363v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17599v2")'>DAGnosis: Localized Identification of Data Inconsistencies using
  Structures</div>
<div id='2402.17599v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T11:29:16Z</div><div>Authors: Nicolas Huynh, Jeroen Berrevoets, Nabeel Seedat, Jonathan CrabbÃ©, Zhaozhi Qian, Mihaela van der Schaar</div><div style='padding-top: 10px; width: 80ex'>Identification and appropriate handling of inconsistencies in data at
deployment time is crucial to reliably use machine learning models. While
recent data-centric methods are able to identify such inconsistencies with
respect to the training set, they suffer from two key limitations: (1)
suboptimality in settings where features exhibit statistical independencies,
due to their usage of compressive representations and (2) lack of localization
to pin-point why a sample might be flagged as inconsistent, which is important
to guide future data collection. We solve these two fundamental limitations
using directed acyclic graphs (DAGs) to encode the training set's features
probability distribution and independencies as a structure. Our method, called
DAGnosis, leverages these structural interactions to bring valuable and
insightful data-centric conclusions. DAGnosis unlocks the localization of the
causes of inconsistencies on a DAG, an aspect overlooked by previous
approaches. Moreover, we show empirically that leveraging these interactions
(1) leads to more accurate conclusions in detecting inconsistencies, as well as
(2) provides more detailed insights into why some samples are flagged.</div><div><a href='http://arxiv.org/abs/2402.17599v2'>2402.17599v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07022v1")'>Edge-Enabled Anomaly Detection and Information Completion for Social
  Network Knowledge Graphs</div>
<div id='2401.07022v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T09:27:37Z</div><div>Authors: Fan Lu, Quan Qi, Huaibin Qin</div><div style='padding-top: 10px; width: 80ex'>In the rapidly advancing information era, various human behaviors are being
precisely recorded in the form of data, including identity information,
criminal records, and communication data. Law enforcement agencies can
effectively maintain social security and precisely combat criminal activities
by analyzing the aforementioned data. In comparison to traditional data
analysis methods, deep learning models, relying on the robust computational
power in cloud centers, exhibit higher accuracy in extracting data features and
inferring data. However, within the architecture of cloud centers, the
transmission of data from end devices introduces significant latency, hindering
real-time inference of data. Furthermore, low-latency edge computing
architectures face limitations in direct deployment due to relatively weak
computing and storage capacities of nodes. To address these challenges, a
lightweight distributed knowledge graph completion architecture is proposed.
Firstly, we introduce a lightweight distributed knowledge graph completion
architecture that utilizes knowledge graph embedding for data analysis.
Subsequently, to filter out substandard data, a personnel data quality
assessment method named PDQA is proposed. Lastly, we present a model pruning
algorithm that significantly reduces the model size while maximizing
performance, enabling lightweight deployment. In experiments, we compare the
effects of 11 advanced models on completing the knowledge graph of public
security personnel information. The results indicate that the RotatE model
outperforms other models significantly in knowledge graph completion, with the
pruned model size reduced by 70\%, and hits@10 reaching 86.97\%.}</div><div><a href='http://arxiv.org/abs/2401.07022v1'>2401.07022v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14609v2")'>FedCQA: Answering Complex Queries on Multi-Source Knowledge Graphs via
  Federated Learning</div>
<div id='2402.14609v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T14:57:44Z</div><div>Authors: Qi Hu, Weifeng Jiang, Haoran Li, Zihao Wang, Jiaxin Bai, Qianren Mao, Yangqiu Song, Lixin Fan, Jianxin Li</div><div style='padding-top: 10px; width: 80ex'>Complex logical query answering is a challenging task in knowledge graphs
(KGs) that has been widely studied. The ability to perform complex logical
reasoning is essential and supports various graph reasoning-based downstream
tasks, such as search engines. Recent approaches are proposed to represent KG
entities and logical queries into embedding vectors and find answers to logical
queries from the KGs. However, existing proposed methods mainly focus on
querying a single KG and cannot be applied to multiple graphs. In addition,
directly sharing KGs with sensitive information may incur privacy risks, making
it impractical to share and construct an aggregated KG for reasoning to
retrieve query answers. Thus, it remains unknown how to answer queries on
multi-source KGs. An entity can be involved in various knowledge graphs and
reasoning on multiple KGs and answering complex queries on multi-source KGs is
important in discovering knowledge cross graphs. Fortunately, federated
learning is utilized in knowledge graphs to collaboratively learn
representations with privacy preserved. Federated knowledge graph embeddings
enrich the relations in knowledge graphs to improve the representation quality.
However, these methods only focus on one-hop relations and cannot perform
complex reasoning tasks. In this paper, we apply federated learning to complex
query-answering tasks to reason over multi-source knowledge graphs while
preserving privacy. We propose a Federated Complex Query Answering framework
(FedCQA), to reason over multi-source KGs avoiding sensitive raw data
transmission to protect privacy. We conduct extensive experiments on three
real-world datasets and evaluate retrieval performance on various types of
complex queries.</div><div><a href='http://arxiv.org/abs/2402.14609v2'>2402.14609v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10110v1")'>Meta Operator for Complex Query Answering on Knowledge Graphs</div>
<div id='2403.10110v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T08:54:25Z</div><div>Authors: Hang Yin, Zihao Wang, Yangqiu Song</div><div style='padding-top: 10px; width: 80ex'>Knowledge graphs contain informative factual knowledge but are considered
incomplete. To answer complex queries under incomplete knowledge,
learning-based Complex Query Answering (CQA) models are proposed to directly
learn from the query-answer samples to avoid the direct traversal of incomplete
graph data. Existing works formulate the training of complex query answering
models as multi-task learning and require a large number of training samples.
In this work, we explore the compositional structure of complex queries and
argue that the different logical operator types, rather than the different
complex query types, are the key to improving generalizability. Accordingly, we
propose a meta-learning algorithm to learn the meta-operators with limited data
and adapt them to different instances of operators under various complex
queries. Empirical results show that learning meta-operators is more effective
than learning original CQA or meta-CQA models.</div><div><a href='http://arxiv.org/abs/2403.10110v1'>2403.10110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12646v1")'>Prompt-fused framework for Inductive Logical Query Answering</div>
<div id='2403.12646v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T11:30:30Z</div><div>Authors: Zezhong Xu, Peng Ye, Lei Liang, Huajun Chen, Wen Zhang</div><div style='padding-top: 10px; width: 80ex'>Answering logical queries on knowledge graphs (KG) poses a significant
challenge for machine reasoning. The primary obstacle in this task stems from
the inherent incompleteness of KGs. Existing research has predominantly focused
on addressing the issue of missing edges in KGs, thereby neglecting another
aspect of incompleteness: the emergence of new entities. Furthermore, most of
the existing methods tend to reason over each logical operator separately,
rather than comprehensively analyzing the query as a whole during the reasoning
process. In this paper, we propose a query-aware prompt-fused framework named
Pro-QE, which could incorporate existing query embedding methods and address
the embedding of emerging entities through contextual information aggregation.
Additionally, a query prompt, which is generated by encoding the symbolic
query, is introduced to gather information relevant to the query from a
holistic perspective. To evaluate the efficacy of our model in the inductive
setting, we introduce two new challenging benchmarks. Experimental results
demonstrate that our model successfully handles the issue of unseen entities in
logical queries. Furthermore, the ablation study confirms the efficacy of the
aggregator and prompt components.</div><div><a href='http://arxiv.org/abs/2403.12646v1'>2403.12646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12954v1")'>Conditional Logical Message Passing Transformer for Complex Query
  Answering</div>
<div id='2402.12954v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T12:17:01Z</div><div>Authors: Chongzhi Zhang, Zhiping Peng, Junhao Zheng, Qianli Ma</div><div style='padding-top: 10px; width: 80ex'>Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging
task. Given that KGs are usually incomplete, neural models are proposed to
solve CQA by performing multi-hop logical reasoning. However, most of them
cannot perform well on both one-hop and multi-hop queries simultaneously.
Recent work proposes a logical message passing mechanism based on the
pre-trained neural link predictors. While effective on both one-hop and
multi-hop queries, it ignores the difference between the constant and variable
nodes in a query graph. In addition, during the node embedding update stage,
this mechanism cannot dynamically measure the importance of different messages,
and whether it can capture the implicit logical dependencies related to a node
and received messages remains unclear. In this paper, we propose Conditional
Logical Message Passing Transformer (CLMPT), which considers the difference
between constants and variables in the case of using pre-trained neural link
predictors and performs message passing conditionally on the node type. We
empirically verified that this approach can reduce computational costs without
affecting performance. Furthermore, CLMPT uses the transformer to aggregate
received messages and update the corresponding node embedding. Through the
self-attention mechanism, CLMPT can assign adaptive weights to elements in an
input set consisting of received messages and the corresponding node and
explicitly model logical dependencies between various elements. Experimental
results show that CLMPT is a new state-of-the-art neural CQA model.</div><div><a href='http://arxiv.org/abs/2402.12954v1'>2402.12954v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01625v2")'>SCALA: Sparsification-based Contrastive Learning for Anomaly Detection
  on Attributed Networks</div>
<div id='2401.01625v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T08:51:18Z</div><div>Authors: Enbo He, Yitong Hao, Yue Zhang, Guisheng Yin, Lina Yao</div><div style='padding-top: 10px; width: 80ex'>Anomaly detection on attributed networks aims to find the nodes whose
behaviors are significantly different from other majority nodes. Generally,
network data contains information about relationships between entities, and the
anomaly is usually embodied in these relationships. Therefore, how to
comprehensively model complex interaction patterns in networks is still a major
focus. It can be observed that anomalies in networks violate the homophily
assumption. However, most existing studies only considered this phenomenon
obliquely rather than explicitly. Besides, the node representation of normal
entities can be perturbed easily by the noise relationships introduced by
anomalous nodes. To address the above issues, we present a novel contrastive
learning framework for anomaly detection on attributed networks,
\textbf{SCALA}, aiming to improve the embedding quality of the network and
provide a new measurement of qualifying the anomaly score for each node by
introducing sparsification into the conventional method. Extensive experiments
are conducted on five benchmark real-world datasets and the results show that
SCALA consistently outperforms all baseline methods significantly.</div><div><a href='http://arxiv.org/abs/2401.01625v2'>2401.01625v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10547v1")'>PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology
  Optimization</div>
<div id='2401.10547v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T08:13:10Z</div><div>Authors: Ziqi Yuan, Haoyi Zhou, Tianyu Chen, Jianxin Li</div><div style='padding-top: 10px; width: 80ex'>A multitude of toxic online behaviors, ranging from network attacks to
anonymous traffic and spam, have severely disrupted the smooth operation of
networks. Due to the inherent sender-receiver nature of network behaviors,
graph-based frameworks are commonly used for detecting anomalous behaviors.
However, in real-world scenarios, the boundary between normal and anomalous
behaviors tends to be ambiguous. The local heterophily of graphs interferes
with the detection, and existing methods based on nodes or edges introduce
unwanted noise into representation results, thereby impacting the effectiveness
of detection. To address these issues, we propose PhoGAD, a graph-based anomaly
detection framework. PhoGAD leverages persistent homology optimization to
clarify behavioral boundaries. Building upon this, the weights of adjacent
edges are designed to mitigate the effects of local heterophily. Subsequently,
to tackle the noise problem, we conduct a formal analysis and propose a
disentangled representation-based explicit embedding method, ultimately
achieving anomaly behavior detection. Experiments on intrusion, traffic, and
spam datasets verify that PhoGAD has surpassed the performance of
state-of-the-art (SOTA) frameworks in detection efficacy. Notably, PhoGAD
demonstrates robust detection even with diminished anomaly proportions,
highlighting its applicability to real-world scenarios. The analysis of
persistent homology demonstrates its effectiveness in capturing the topological
structure formed by normal edge features. Additionally, ablation experiments
validate the effectiveness of the innovative mechanisms integrated within
PhoGAD.</div><div><a href='http://arxiv.org/abs/2401.10547v1'>2401.10547v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02682v1")'>Homophily-Related: Adaptive Hybrid Graph Filter for Multi-View Graph
  Clustering</div>
<div id='2401.02682v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T07:27:29Z</div><div>Authors: Zichen Wen, Yawen Ling, Yazhou Ren, Tianyi Wu, Jianpeng Chen, Xiaorong Pu, Zhifeng Hao, Lifang He</div><div style='padding-top: 10px; width: 80ex'>Recently there is a growing focus on graph data, and multi-view graph
clustering has become a popular area of research interest. Most of the existing
methods are only applicable to homophilous graphs, yet the extensive real-world
graph data can hardly fulfill the homophily assumption, where the connected
nodes tend to belong to the same class. Several studies have pointed out that
the poor performance on heterophilous graphs is actually due to the fact that
conventional graph neural networks (GNNs), which are essentially low-pass
filters, discard information other than the low-frequency information on the
graph. Nevertheless, on certain graphs, particularly heterophilous ones,
neglecting high-frequency information and focusing solely on low-frequency
information impedes the learning of node representations. To break this
limitation, our motivation is to perform graph filtering that is closely
related to the homophily degree of the given graph, with the aim of fully
leveraging both low-frequency and high-frequency signals to learn
distinguishable node embedding. In this work, we propose Adaptive Hybrid Graph
Filter for Multi-View Graph Clustering (AHGFC). Specifically, a graph joint
process and graph joint aggregation matrix are first designed by using the
intrinsic node features and adjacency relationship, which makes the low and
high-frequency signals on the graph more distinguishable. Then we design an
adaptive hybrid graph filter that is related to the homophily degree, which
learns the node embedding based on the graph joint aggregation matrix. After
that, the node embedding of each view is weighted and fused into a consensus
embedding for the downstream task. Experimental results show that our proposed
model performs well on six datasets containing homophilous and heterophilous
graphs.</div><div><a href='http://arxiv.org/abs/2401.02682v1'>2401.02682v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03666v1")'>Provable Filter for Real-world Graph Clustering</div>
<div id='2403.03666v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:37:49Z</div><div>Authors: Xuanting Xie, Erlin Pan, Zhao Kang, Wenyu Chen, Bingheng Li</div><div style='padding-top: 10px; width: 80ex'>Graph clustering, an important unsupervised problem, has been shown to be
more resistant to advances in Graph Neural Networks (GNNs). In addition, almost
all clustering methods focus on homophilic graphs and ignore heterophily. This
significantly limits their applicability in practice, since real-world graphs
exhibit a structural disparity and cannot simply be classified as homophily and
heterophily. Thus, a principled way to handle practical graphs is urgently
needed. To fill this gap, we provide a novel solution with theoretical support.
Interestingly, we find that most homophilic and heterophilic edges can be
correctly identified on the basis of neighbor information. Motivated by this
finding, we construct two graphs that are highly homophilic and heterophilic,
respectively. They are used to build low-pass and high-pass filters to capture
holistic information. Important features are further enhanced by the
squeeze-and-excitation block. We validate our approach through extensive
experiments on both homophilic and heterophilic graphs. Empirical results
demonstrate the superiority of our method compared to state-of-the-art
clustering methods.</div><div><a href='http://arxiv.org/abs/2403.03666v1'>2403.03666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03659v1")'>Robust Graph Structure Learning under Heterophily</div>
<div id='2403.03659v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:29:13Z</div><div>Authors: Xuanting Xie, Zhao Kang, Wenyu Chen</div><div style='padding-top: 10px; width: 80ex'>Graph is a fundamental mathematical structure in characterizing relations
between different objects and has been widely used on various learning tasks.
Most methods implicitly assume a given graph to be accurate and complete.
However, real data is inevitably noisy and sparse, which will lead to inferior
results. Despite the remarkable success of recent graph representation learning
methods, they inherently presume that the graph is homophilic, and largely
overlook heterophily, where most connected nodes are from different classes. In
this regard, we propose a novel robust graph structure learning method to
achieve a high-quality graph from heterophilic data for downstream tasks. We
first apply a high-pass filter to make each node more distinctive from its
neighbors by encoding structure information into the node features. Then, we
learn a robust graph with an adaptive norm characterizing different levels of
noise. Afterwards, we propose a novel regularizer to further refine the graph
structure. Clustering and semi-supervised classification experiments on
heterophilic graphs verify the effectiveness of our method.</div><div><a href='http://arxiv.org/abs/2403.03659v1'>2403.03659v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14876v2")'>Cross-Space Adaptive Filter: Integrating Graph Topology and Node
  Attributes for Alleviating the Over-smoothing Problem</div>
<div id='2401.14876v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T14:02:29Z</div><div>Authors: Chen Huang, Haoyang Li, Yifan Zhang, Wenqiang Lei, Jiancheng Lv</div><div style='padding-top: 10px; width: 80ex'>The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to
extract low-frequency signals from graph topology, which may lead to the
over-smoothing problem when GCN goes deep. To this end, various methods have
been proposed to create an adaptive filter by incorporating an extra filter
(e.g., a high-pass filter) extracted from the graph topology. However, these
methods heavily rely on topological information and ignore the node attribute
space, which severely sacrifices the expressive power of the deep GCNs,
especially when dealing with disassortative graphs. In this paper, we propose a
cross-space adaptive filter, called CSF, to produce the adaptive-frequency
information extracted from both the topology and attribute spaces.
Specifically, we first derive a tailored attribute-based high-pass filter that
can be interpreted theoretically as a minimizer for semi-supervised kernel
ridge regression. Then, we cast the topology-based low-pass filter as a
Mercer's kernel within the context of GCNs. This serves as a foundation for
combining it with the attribute-based filter to capture the adaptive-frequency
information. Finally, we derive the cross-space filter via an effective
multiple-kernel learning strategy, which unifies the attribute-based high-pass
filter and the topology-based low-pass filter. This helps to address the
over-smoothing problem while maintaining effectiveness. Extensive experiments
demonstrate that CSF not only successfully alleviates the over-smoothing
problem but also promotes the effectiveness of the node classification task.</div><div><a href='http://arxiv.org/abs/2401.14876v2'>2401.14876v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10756v1")'>Towards Cohesion-Fairness Harmony: Contrastive Regularization in
  Individual Fair Graph Clustering</div>
<div id='2402.10756v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T15:25:56Z</div><div>Authors: Siamak Ghodsi, Seyed Amjad Seyedi, Eirini Ntoutsi</div><div style='padding-top: 10px; width: 80ex'>Conventional fair graph clustering methods face two primary challenges: i)
They prioritize balanced clusters at the expense of cluster cohesion by
imposing rigid constraints, ii) Existing methods of both individual and
group-level fairness in graph partitioning mostly rely on eigen decompositions
and thus, generally lack interpretability. To address these issues, we propose
iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model
with contrastive fairness regularization that achieves balanced and cohesive
clusters. By introducing fairness regularization, our model allows for
customizable accuracy-fairness trade-offs, thereby enhancing user autonomy
without compromising the interpretability provided by nonnegative matrix
tri-factorization. Experimental evaluations on real and synthetic datasets
demonstrate the superior flexibility of iFairNMTF in achieving fairness and
clustering performance.</div><div><a href='http://arxiv.org/abs/2402.10756v1'>2402.10756v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10572v1")'>Discovering Invariant Neighborhood Patterns for Heterophilic Graphs</div>
<div id='2403.10572v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T02:25:45Z</div><div>Authors: Ruihao Zhang, Zhengyu Chen, Teng Xiao, Yueyang Wang, Kun Kuang</div><div style='padding-top: 10px; width: 80ex'>This paper studies the problem of distribution shifts on non-homophilous
graphs Mosting existing graph neural network methods rely on the homophilous
assumption that nodes from the same class are more likely to be linked.
However, such assumptions of homophily do not always hold in real-world graphs,
which leads to more complex distribution shifts unaccounted for in previous
methods. The distribution shifts of neighborhood patterns are much more diverse
on non-homophilous graphs. We propose a novel Invariant Neighborhood Pattern
Learning (INPL) to alleviate the distribution shifts problem on non-homophilous
graphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP)
module to capture the adaptive neighborhood information, which could alleviate
the neighborhood pattern distribution shifts problem on non-homophilous graphs.
We propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrain
the ANP and learn invariant graph representation on non-homophilous graphs.
Extensive experimental results on real-world non-homophilous graphs show that
INPL could achieve state-of-the-art performance for learning on large
non-homophilous graphs.</div><div><a href='http://arxiv.org/abs/2403.10572v1'>2403.10572v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14802v1")'>Link Prediction under Heterophily: A Physics-Inspired Graph Neural
  Network Approach</div>
<div id='2402.14802v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:56:31Z</div><div>Authors: Andrea Giuseppe Di Francesco, Francesco Caso, Maria Sofia Bucarelli, Fabrizio Silvestri</div><div style='padding-top: 10px; width: 80ex'>In the past years, Graph Neural Networks (GNNs) have become the `de facto'
standard in various deep learning domains, thanks to their flexibility in
modeling real-world phenomena represented as graphs. However, the
message-passing mechanism of GNNs faces challenges in learnability and
expressivity, hindering high performance on heterophilic graphs, where adjacent
nodes frequently have different labels. Most existing solutions addressing
these challenges are primarily confined to specific benchmarks focused on node
classification tasks. This narrow focus restricts the potential impact that
link prediction under heterophily could offer in several applications,
including recommender systems. For example, in social networks, two users may
be connected for some latent reason, making it challenging to predict such
connections in advance. Physics-Inspired GNNs such as GRAFF provided a
significant contribution to enhance node classification performance under
heterophily, thanks to the adoption of physics biases in the message-passing.
Drawing inspiration from these findings, we advocate that the methodology
employed by GRAFF can improve link prediction performance as well. To further
explore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link
prediction. We evaluate its efficacy within a recent collection of heterophilic
graphs, establishing a new benchmark for link prediction under heterophily. Our
approach surpasses previous methods, in most of the datasets, showcasing a
strong flexibility in different contexts, and achieving relative AUROC
improvements of up to 26.7%.</div><div><a href='http://arxiv.org/abs/2402.14802v1'>2402.14802v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00755v1")'>Saliency-Aware Regularized Graph Neural Network</div>
<div id='2401.00755v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T13:44:16Z</div><div>Authors: Wenjie Pei, Weina Xu, Zongze Wu, Weichao Li, Jinfan Wang, Guangming Lu, Xiangrong Wang</div><div style='padding-top: 10px; width: 80ex'>The crux of graph classification lies in the effective representation
learning for the entire graph. Typical graph neural networks focus on modeling
the local dependencies when aggregating features of neighboring nodes, and
obtain the representation for the entire graph by aggregating node features.
Such methods have two potential limitations: 1) the global node saliency w.r.t.
graph classification is not explicitly modeled, which is crucial since
different nodes may have different semantic relevance to graph classification;
2) the graph representation directly aggregated from node features may have
limited effectiveness to reflect graph-level information. In this work, we
propose the Saliency-Aware Regularized Graph Neural Network (SAR-GNN) for graph
classification, which consists of two core modules: 1) a traditional graph
neural network serving as the backbone for learning node features and 2) the
Graph Neural Memory designed to distill a compact graph representation from
node features of the backbone. We first estimate the global node saliency by
measuring the semantic similarity between the compact graph representation and
node features. Then the learned saliency distribution is leveraged to
regularize the neighborhood aggregation of the backbone, which facilitates the
message passing of features for salient nodes and suppresses the less relevant
nodes. Thus, our model can learn more effective graph representation. We
demonstrate the merits of SAR-GNN by extensive experiments on seven datasets
across various types of graph data. Code will be released.</div><div><a href='http://arxiv.org/abs/2401.00755v1'>2401.00755v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03465v1")'>Self-Attention Empowered Graph Convolutional Network for Structure
  Learning and Node Embedding</div>
<div id='2403.03465v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T05:00:31Z</div><div>Authors: Mengying Jiang, Guizhong Liu, Yuanchao Su, Xinliang Wu</div><div style='padding-top: 10px; width: 80ex'>In representation learning on graph-structured data, many popular graph
neural networks (GNNs) fail to capture long-range dependencies, leading to
performance degradation. Furthermore, this weakness is magnified when the
concerned graph is characterized by heterophily (low homophily). To solve this
issue, this paper proposes a novel graph learning framework called the graph
convolutional network with self-attention (GCN-SA). The proposed scheme
exhibits an exceptional generalization capability in node-level representation
learning. The proposed GCN-SA contains two enhancements corresponding to edges
and node features. For edges, we utilize a self-attention mechanism to design a
stable and effective graph-structure-learning module that can capture the
internal correlation between any pair of nodes. This graph-structure-learning
module can identify reliable neighbors for each node from the entire graph.
Regarding the node features, we modify the transformer block to make it more
applicable to enable GCN to fuse valuable information from the entire graph.
These two enhancements work in distinct ways to help our GCN-SA capture
long-range dependencies, enabling it to perform representation learning on
graphs with varying levels of homophily. The experimental results on benchmark
datasets demonstrate the effectiveness of the proposed GCN-SA. Compared to
other outstanding GNN counterparts, the proposed GCN-SA is competitive.</div><div><a href='http://arxiv.org/abs/2403.03465v1'>2403.03465v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17612v2")'>IGCN: Integrative Graph Convolutional Networks for Multi-modal Data</div>
<div id='2401.17612v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T05:52:11Z</div><div>Authors: Cagri Ozdemir, Mohammad Al Olaimat, Yashu Vashishath, Serdar Bozdag, Alzheimer's Disease Neuroimaging Initiative</div><div style='padding-top: 10px; width: 80ex'>Recent advances in Graph Neural Networks (GNN) have led to a considerable
growth in graph data modeling for multi-modal data which contains various types
of nodes and edges. Although some integrative prediction solutions have been
developed recently for network-structured data, these methods have some
restrictions. For a node classification task involving multi-modal data,
certain data modalities may perform better when predicting one class, while
others might excel in predicting a different class. Thus, to obtain a better
learning representation, advanced computational methodologies are required for
the integrative analysis of multi-modal data. Moreover, existing integrative
tools lack a comprehensive and cohesive understanding of the rationale behind
their specific predictions, making them unsuitable for enhancing model
interpretability. Addressing these restrictions, we introduce a novel
integrative neural network approach for multi-modal data networks, named
Integrative Graph Convolutional Networks (IGCN). IGCN learns node embeddings
from multiple topologies and fuses the multiple node embeddings into a weighted
form by assigning attention coefficients to the node embeddings. Our proposed
attention mechanism helps identify which types of data receive more emphasis
for each sample to predict a certain class. Therefore, IGCN has the potential
to unravel previously unknown characteristics within different node
classification tasks. We benchmarked IGCN on several datasets from different
domains, including a multi-omics dataset to predict cancer subtypes and a
multi-modal clinical dataset to predict the progression of Alzheimer's disease.
Experimental results show that IGCN outperforms or is on par with the
state-of-the-art and baseline methods.</div><div><a href='http://arxiv.org/abs/2401.17612v2'>2401.17612v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07268v1")'>Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker
  Identification with PathFormer</div>
<div id='2402.07268v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T18:23:54Z</div><div>Authors: Zehao Dong, Qihang Zhao, Philip R. O. Payne, Michael A Province, Carlos Cruchaga, Muhan Zhang, Tianyu Zhao, Yixin Chen, Fuhai Li</div><div style='padding-top: 10px; width: 80ex'>Biomarker identification is critical for precise disease diagnosis and
understanding disease pathogenesis in omics data analysis, like using fold
change and regression analysis. Graph neural networks (GNNs) have been the
dominant deep learning model for analyzing graph-structured data. However, we
found two major limitations of existing GNNs in omics data analysis, i.e.,
limited-prediction (diagnosis) accuracy and limited-reproducible biomarker
identification capacity across multiple datasets. The root of the challenges is
the unique graph structure of biological signaling pathways, which consists of
a large number of targets and intensive and complex signaling interactions
among these targets. To resolve these two challenges, in this study, we
presented a novel GNN model architecture, named PathFormer, which
systematically integrate signaling network, priori knowledge and omics data to
rank biomarkers and predict disease diagnosis. In the comparison results,
PathFormer outperformed existing GNN models significantly in terms of highly
accurate prediction capability ( 30% accuracy improvement in disease diagnosis
compared with existing GNN models) and high reproducibility of biomarker
ranking across different datasets. The improvement was confirmed using two
independent Alzheimer's Disease (AD) and cancer transcriptomic datasets. The
PathFormer model can be directly applied to other omics data analysis studies.</div><div><a href='http://arxiv.org/abs/2402.07268v1'>2402.07268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15478v1")'>Product Manifold Representations for Learning on Biological Pathways</div>
<div id='2401.15478v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T18:46:19Z</div><div>Authors: Daniel McNeela, Frederic Sala, Anthony Gitter</div><div style='padding-top: 10px; width: 80ex'>Machine learning models that embed graphs in non-Euclidean spaces have shown
substantial benefits in a variety of contexts, but their application has not
been studied extensively in the biological domain, particularly with respect to
biological pathway graphs. Such graphs exhibit a variety of complex network
structures, presenting challenges to existing embedding approaches. Learning
high-quality embeddings for biological pathway graphs is important for
researchers looking to understand the underpinnings of disease and train
high-quality predictive models on these networks. In this work, we investigate
the effects of embedding pathway graphs in non-Euclidean mixed-curvature spaces
and compare against traditional Euclidean graph representation learning models.
We then train a supervised model using the learned node embeddings to predict
missing protein-protein interactions in pathway graphs. We find large
reductions in distortion and boosts on in-distribution edge prediction
performance as a result of using mixed-curvature embeddings and their
corresponding graph neural network models. However, we find that
mixed-curvature representations underperform existing baselines on
out-of-distribution edge prediction performance suggesting that these
representations may overfit to the training graph topology. We provide our
mixed-curvature product GCN code at
https://github.com/mcneela/Mixed-Curvature-GCN and our pathway analysis code at
https://github.com/mcneela/Mixed-Curvature-Pathways.</div><div><a href='http://arxiv.org/abs/2401.15478v1'>2401.15478v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06432v1")'>Joint-Embedding Masked Autoencoder for Self-supervised Learning of
  Dynamic Functional Connectivity from the Human Brain</div>
<div id='2403.06432v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T04:49:41Z</div><div>Authors: Jungwon Choi, Hyungi Lee, Byung-Hoon Kim, Juho Lee</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have shown promise in learning dynamic
functional connectivity for distinguishing phenotypes from human brain
networks. However, obtaining extensive labeled clinical data for training is
often resource-intensive, making practical application difficult. Leveraging
unlabeled data thus becomes crucial for representation learning in a
label-scarce setting. Although generative self-supervised learning techniques,
especially masked autoencoders, have shown promising results in representation
learning in various domains, their application to dynamic graphs for dynamic
functional connectivity remains underexplored, facing challenges in capturing
high-level semantic representations. Here, we introduce the Spatio-Temporal
Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the
Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA
employs a JEPA-inspired strategy for reconstructing dynamic graphs, which
enables the learning of higher-level semantic representations considering
temporal perspectives, addressing the challenges in fMRI data representation
learning. Utilizing the large-scale UK Biobank dataset for self-supervised
learning, ST-JEMA shows exceptional representation learning performance on
dynamic functional connectivity demonstrating superiority over previous methods
in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI
datasets even with limited samples and effectiveness of temporal reconstruction
on missing data scenarios. These findings highlight the potential of our
approach as a robust representation learning method for leveraging label-scarce
fMRI data.</div><div><a href='http://arxiv.org/abs/2403.06432v1'>2403.06432v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01383v2")'>Predicting Infant Brain Connectivity with Federated Multi-Trajectory
  GNNs using Scarce Data</div>
<div id='2401.01383v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T10:20:01Z</div><div>Authors: Michalis Pistos, Gang Li, Weili Lin, Dinggang Shen, Islem Rekik</div><div style='padding-top: 10px; width: 80ex'>The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital's local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.</div><div><a href='http://arxiv.org/abs/2401.01383v2'>2401.01383v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14484v1")'>HyperGALE: ASD Classification via Hypergraph Gated Attention with
  Learnable Hyperedges</div>
<div id='2403.14484v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T15:31:28Z</div><div>Authors: Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju Surampudi</div><div style='padding-top: 10px; width: 80ex'>Autism Spectrum Disorder (ASD) is a neurodevelopmental condition
characterized by varied social cognitive challenges and repetitive behavioral
patterns. Identifying reliable brain imaging-based biomarkers for ASD has been
a persistent challenge due to the spectrum's diverse symptomatology. Existing
baselines in the field have made significant strides in this direction, yet
there remains room for improvement in both performance and interpretability. We
propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating
learned hyperedges and gated attention mechanisms. This approach has led to
substantial improvements in the model's ability to interpret complex brain
graph data, offering deeper insights into ASD biomarker characterization.
Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves
interpretability but also demonstrates statistically significant enhancements
in key performance metrics compared to both previous baselines and the
foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD
research highlights the potential of sophisticated graph-based techniques in
neurodevelopmental studies. The source code and implementation instructions are
available at GitHub:https://github.com/mehular0ra/HyperGALE.</div><div><a href='http://arxiv.org/abs/2403.14484v1'>2403.14484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05818v2")'>PR-NET: Leveraging Pathway Refined Network Structures for Prostate
  Cancer Patient Condition Prediction</div>
<div id='2403.05818v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T06:58:21Z</div><div>Authors: R. Li, J. Liu, X. L. Deng, X. Liu, J. C. Guo, W. Y. Wu, L. Yang</div><div style='padding-top: 10px; width: 80ex'>The diagnosis and monitoring of Castrate Resistant Prostate Cancer (CRPC) are
crucial for cancer patients, but the current models (such as P-NET) have
limitations in terms of parameter count, generalization, and cost. To address
the issue, we develop a more accurate and efficient Prostate Cancer patient
condition prediction model, named PR-NET. By compressing and optimizing the
network structure of P-NET, the model complexity is reduced while maintaining
high accuracy and interpretability. The PR-NET demonstrated superior
performance in predicting prostate cancer patient outcomes, outshining P-NET
and six other traditional models with a significant margin. In our rigorous
evaluation, PR-NET not only achieved impressive average AUC and Recall scores
of 0.94 and 0.83, respectively, on known data but also maintained robust
generalizability on five unknown datasets with a higher average AUC of 0.73 and
Recall of 0.72, compared to P-NET's 0.68 and 0.5. PR-NET's efficiency was
evidenced by its shorter average training and inference times, and its
gene-level analysis revealed 46 key genes, demonstrating its enhanced
predictive power and efficiency in identifying critical biomarkers for prostate
cancer. Future research can further expand its application domains and optimize
the model's performance and reliability.</div><div><a href='http://arxiv.org/abs/2403.05818v2'>2403.05818v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17807v1")'>Exploring Gene Regulatory Interaction Networks and predicting
  therapeutic molecules for Hypopharyngeal Cancer and EGFR-mutated lung
  adenocarcinoma</div>
<div id='2402.17807v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T11:29:36Z</div><div>Authors: Abanti Bhattacharjya, Md Manowarul Islam, Md Ashraf Uddin, Md. Alamin Talukder, AKM Azad, Sunil Aryal, Bikash Kumar Paul, Wahia Tasnim, Muhammad Ali Abdulllah Almoyad, Mohammad Ali Moni</div><div style='padding-top: 10px; width: 80ex'>With the advent of Information technology, the Bioinformatics research field
is becoming increasingly attractive to researchers and academicians. The recent
development of various Bioinformatics toolkits has facilitated the rapid
processing and analysis of vast quantities of biological data for human
perception. Most studies focus on locating two connected diseases and making
some observations to construct diverse gene regulatory interaction networks, a
forerunner to general drug design for curing illness. For instance,
Hypopharyngeal cancer is a disease that is associated with EGFR-mutated lung
adenocarcinoma. In this study, we select EGFR-mutated lung adenocarcinoma and
Hypopharyngeal cancer by finding the Lung metastases in hypopharyngeal cancer.
To conduct this study, we collect Mircorarray datasets from GEO (Gene
Expression Omnibus), an online database controlled by NCBI. Differentially
expressed genes, common genes, and hub genes between the selected two diseases
are detected for the succeeding move. Our research findings have suggested
common therapeutic molecules for the selected diseases based on 10 hub genes
with the highest interactions according to the degree topology method and the
maximum clique centrality (MCC). Our suggested therapeutic molecules will be
fruitful for patients with those two diseases simultaneously.</div><div><a href='http://arxiv.org/abs/2402.17807v1'>2402.17807v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05478v1")'>Population Graph Cross-Network Node Classification for Autism Detection
  Across Sample Groups</div>
<div id='2401.05478v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T18:04:12Z</div><div>Authors: Anna Stephens, Francisco Santos, Pang-Ning Tan, Abdol-Hossein Esfahanian</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNN) are a powerful tool for combining imaging and
non-imaging medical information for node classification tasks. Cross-network
node classification extends GNN techniques to account for domain drift,
allowing for node classification on an unlabeled target network. In this paper
we present OTGCN, a powerful, novel approach to cross-network node
classification. This approach leans on concepts from graph convolutional
networks to harness insights from graph data structures while simultaneously
applying strategies rooted in optimal transport to correct for the domain drift
that can occur between samples from different data collection sites. This
blended approach provides a practical solution for scenarios with many distinct
forms of data collected across different locations and equipment. We
demonstrate the effectiveness of this approach at classifying Autism Spectrum
Disorder subjects using a blend of imaging and non-imaging data.</div><div><a href='http://arxiv.org/abs/2401.05478v1'>2401.05478v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10158v1")'>Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights</div>
<div id='2403.10158v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T10:01:19Z</div><div>Authors: Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, CÃ©cile Rosseau, Alessandra Pascale, John Dinsmore</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.</div><div><a href='http://arxiv.org/abs/2403.10158v1'>2403.10158v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09290v1")'>SELECTOR: Heterogeneous graph network with convolutional masked
  autoencoder for multimodal robust prediction of cancer survival</div>
<div id='2403.09290v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T11:23:39Z</div><div>Authors: Liangrui Pan, Yijun Peng, Yan Li, Xiang Wang, Wenjuan Liu, Liwen Xu, Qingchun Liang, Shaoliang Peng</div><div style='padding-top: 10px; width: 80ex'>Accurately predicting the survival rate of cancer patients is crucial for
aiding clinicians in planning appropriate treatment, reducing cancer-related
medical expenses, and significantly enhancing patients' quality of life.
Multimodal prediction of cancer patient survival offers a more comprehensive
and precise approach. However, existing methods still grapple with challenges
related to missing multimodal data and information interaction within
modalities. This paper introduces SELECTOR, a heterogeneous graph-aware network
based on convolutional mask encoders for robust multimodal prediction of cancer
patient survival. SELECTOR comprises feature edge reconstruction, convolutional
mask encoder, feature cross-fusion, and multimodal survival prediction modules.
Initially, we construct a multimodal heterogeneous graph and employ the
meta-path method for feature edge reconstruction, ensuring comprehensive
incorporation of feature information from graph edges and effective embedding
of nodes. To mitigate the impact of missing features within the modality on
prediction accuracy, we devised a convolutional masked autoencoder (CMAE) to
process the heterogeneous graph post-feature reconstruction. Subsequently, the
feature cross-fusion module facilitates communication between modalities,
ensuring that output features encompass all features of the modality and
relevant information from other modalities. Extensive experiments and analysis
on six cancer datasets from TCGA demonstrate that our method significantly
outperforms state-of-the-art methods in both modality-missing and
intra-modality information-confirmed cases. Our codes are made available at
https://github.com/panliangrui/Selector.</div><div><a href='http://arxiv.org/abs/2403.09290v1'>2403.09290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15584v1")'>DGNN: Decoupled Graph Neural Networks with Structural Consistency
  between Attribute and Graph Embedding Representations</div>
<div id='2401.15584v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T06:43:13Z</div><div>Authors: Jinlu Wang, Jipeng Guo, Yanfeng Sun, Junbin Gao, Shaofan Wang, Yachao Yang, Baocai Yin</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) demonstrate a robust capability for
representation learning on graphs with complex structures, showcasing superior
performance in various applications. The majority of existing GNNs employ a
graph convolution operation by using both attribute and structure information
through coupled learning. In essence, GNNs, from an optimization perspective,
seek to learn a consensus and compromise embedding representation that balances
attribute and graph information, selectively exploring and retaining valid
information. To obtain a more comprehensive embedding representation of nodes,
a novel GNNs framework, dubbed Decoupled Graph Neural Networks (DGNN), is
introduced. DGNN explores distinctive embedding representations from the
attribute and graph spaces by decoupled terms. Considering that semantic graph,
constructed from attribute feature space, consists of different node connection
information and provides enhancement for the topological graph, both
topological and semantic graphs are combined for the embedding representation
learning. Further, structural consistency among attribute embedding and graph
embeddings is promoted to effectively remove redundant information and
establish soft connection. This involves promoting factor sharing for adjacency
reconstruction matrices, facilitating the exploration of a consensus and
high-level correlation. Finally, a more powerful and complete representation is
achieved through the concatenation of these embeddings. Experimental results
conducted on several graph benchmark datasets verify its superiority in node
classification task.</div><div><a href='http://arxiv.org/abs/2401.15584v1'>2401.15584v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13033v1")'>Enhancing Real-World Complex Network Representations with Hyperedge
  Augmentation</div>
<div id='2402.13033v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T14:18:43Z</div><div>Authors: Xiangyu Zhao, Zehui Li, Mingzhu Shen, Guy-Bart Stan, Pietro LiÃ², Yiren Zhao</div><div style='padding-top: 10px; width: 80ex'>Graph augmentation methods play a crucial role in improving the performance
and enhancing generalisation capabilities in Graph Neural Networks (GNNs).
Existing graph augmentation methods mainly perturb the graph structures and are
usually limited to pairwise node relations. These methods cannot fully address
the complexities of real-world large-scale networks that often involve
higher-order node relations beyond only being pairwise. Meanwhile, real-world
graph datasets are predominantly modelled as simple graphs, due to the scarcity
of data that can be used to form higher-order edges. Therefore, reconfiguring
the higher-order edges as an integration into graph augmentation strategies
lights up a promising research path to address the aforementioned issues. In
this paper, we present Hyperedge Augmentation (HyperAug), a novel graph
augmentation method that constructs virtual hyperedges directly form the raw
data, and produces auxiliary node features by extracting from the virtual
hyperedge information, which are used for enhancing GNN performances on
downstream tasks. We design three diverse virtual hyperedge construction
strategies to accompany the augmentation scheme: (1) via graph statistics, (2)
from multiple data perspectives, and (3) utilising multi-modality. Furthermore,
to facilitate HyperAug evaluation, we provide 23 novel real-world graph
datasets across various domains including social media, biology, and
e-commerce. Our empirical study shows that HyperAug consistently and
significantly outperforms GNN baselines and other graph augmentation methods,
across a variety of application contexts, which clearly indicates that it can
effectively incorporate higher-order node relations into graph augmentation
methods for real-world complex networks.</div><div><a href='http://arxiv.org/abs/2402.13033v1'>2402.13033v1</a></div>
</div></div>
    <div><a href="arxiv_20.html">Prev (20)</a></div>
    <div><a href="arxiv_22.html">Next (22)</a></div>
    