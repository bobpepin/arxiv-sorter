
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01920v1")'>Preference Poisoning Attacks on Reward Model Learning</div>
<div id='2402.01920v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T21:45:24Z</div><div>Authors: Junlin Wu, Jiongxiao Wang, Chaowei Xiao, Chenguang Wang, Ning Zhang, Yevgeniy Vorobeychik</div><div style='padding-top: 10px; width: 80ex'>Learning utility, or reward, models from pairwise comparisons is a
fundamental component in a number of application domains. These approaches
inherently entail collecting preference information from people, with feedback
often provided anonymously. Since preferences are subjective, there is no gold
standard to compare against; yet, reliance of high-impact systems on preference
learning creates a strong motivation for malicious actors to skew data
collected in this fashion to their ends. We investigate the nature and extent
of this vulnerability systematically by considering a threat model in which an
attacker can flip a small subset of preference comparisons with the goal of
either promoting or demoting a target outcome. First, we propose two classes of
algorithmic approaches for these attacks: a principled gradient-based
framework, and several variants of rank-by-distance methods. Next, we
demonstrate the efficacy of best attacks in both these classes in successfully
achieving malicious goals on datasets from three diverse domains: autonomous
control, recommendation system, and textual prompt-response preference
learning. We find that the best attacks are often highly successful, achieving
in the most extreme case 100% success rate with only 0.3% of the data poisoned.
However, which attack is best can vary significantly across domains,
demonstrating the value of our comprehensive vulnerability analysis that
involves several classes of attack algorithms. In addition, we observe that the
simpler and more scalable rank-by-distance approaches are often competitive
with the best, and on occasion significantly outperform gradient-based methods.
Finally, we show that several state-of-the-art defenses against other classes
of poisoning attacks exhibit, at best, limited efficacy in our setting.</div><div><a href='http://arxiv.org/abs/2402.01920v1'>2402.01920v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09660v2")'>User Modeling and User Profiling: A Comprehensive Survey</div>
<div id='2402.09660v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T02:06:06Z</div><div>Authors: Erasmo Purificato, Ludovico Boratto, Ernesto William De Luca</div><div style='padding-top: 10px; width: 80ex'>The integration of artificial intelligence (AI) into daily life, particularly
through information retrieval and recommender systems, has necessitated
advanced user modeling and profiling techniques to deliver personalized
experiences. These techniques aim to construct accurate user representations
based on the rich amounts of data generated through interactions with these
systems. This paper presents a comprehensive survey of the current state,
evolution, and future directions of user modeling and profiling research. We
provide a historical overview, tracing the development from early stereotype
models to the latest deep learning techniques, and propose a novel taxonomy
that encompasses all active topics in this research area, including recent
trends. Our survey highlights the paradigm shifts towards more sophisticated
user profiling methods, emphasizing implicit data collection, multi-behavior
modeling, and the integration of graph data structures. We also address the
critical need for privacy-preserving techniques and the push towards
explainability and fairness in user modeling approaches. By examining the
definitions of core terminology, we aim to clarify ambiguities and foster a
clearer understanding of the field by proposing two novel encyclopedic
definitions of the main terms. Furthermore, we explore the application of user
modeling in various domains, such as fake news detection, cybersecurity, and
personalized education. This survey serves as a comprehensive resource for
researchers and practitioners, offering insights into the evolution of user
modeling and profiling and guiding the development of more personalized,
ethical, and effective AI systems.</div><div><a href='http://arxiv.org/abs/2402.09660v2'>2402.09660v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11495v1")'>URLBERT:A Contrastive and Adversarial Pre-trained Model for URL
  Classification</div>
<div id='2402.11495v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T07:51:20Z</div><div>Authors: Yujie Li, Yanbin Wang, Haitao Xu, Zhenhao Guo, Zheng Cao, Lun Zhang</div><div style='padding-top: 10px; width: 80ex'>URLs play a crucial role in understanding and categorizing web content,
particularly in tasks related to security control and online recommendations.
While pre-trained models are currently dominating various fields, the domain of
URL analysis still lacks specialized pre-trained models. To address this gap,
this paper introduces URLBERT, the first pre-trained representation learning
model applied to a variety of URL classification or detection tasks. We first
train a URL tokenizer on a corpus of billions of URLs to address URL data
tokenization. Additionally, we propose two novel pre-training tasks: (1)
self-supervised contrastive learning tasks, which strengthen the model's
understanding of URL structure and the capture of category differences by
distinguishing different variants of the same URL; (2) virtual adversarial
training, aimed at improving the model's robustness in extracting semantic
features from URLs. Finally, our proposed methods are evaluated on tasks
including phishing URL detection, web page classification, and ad filtering,
achieving state-of-the-art performance. Importantly, we also explore multi-task
learning with URLBERT, and experimental results demonstrate that multi-task
learning model based on URLBERT exhibit equivalent effectiveness compared to
independently fine-tuned models, showing the simplicity of URLBERT in handling
complex task requirements. The code for our work is available at
https://github.com/Davidup1/URLBERT.</div><div><a href='http://arxiv.org/abs/2402.11495v1'>2402.11495v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13177v1")'>Deep Learning Model Reuse in the HuggingFace Community: Challenges,
  Benefit and Trends</div>
<div id='2401.13177v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T01:50:29Z</div><div>Authors: Mina Taraghi, Gianolli Dorcelus, Armstrong Foundjem, Florian Tambon, Foutse Khomh</div><div style='padding-top: 10px; width: 80ex'>The ubiquity of large-scale Pre-Trained Models (PTMs) is on the rise,
sparking interest in model hubs, and dedicated platforms for hosting PTMs.
Despite this trend, a comprehensive exploration of the challenges that users
encounter and how the community leverages PTMs remains lacking. To address this
gap, we conducted an extensive mixed-methods empirical study by focusing on
discussion forums and the model hub of HuggingFace, the largest public model
hub. Based on our qualitative analysis, we present a taxonomy of the challenges
and benefits associated with PTM reuse within this community. We then conduct a
quantitative study to track model-type trends and model documentation evolution
over time. Our findings highlight prevalent challenges such as limited guidance
for beginner users, struggles with model output comprehensibility in training
or inference, and a lack of model understanding. We also identified interesting
trends among models where some models maintain high upload rates despite a
decline in topics related to them. Additionally, we found that despite the
introduction of model documentation tools, its quantity has not increased over
time, leading to difficulties in model comprehension and selection among users.
Our study sheds light on new challenges in reusing PTMs that were not reported
before and we provide recommendations for various stakeholders involved in PTM
reuse.</div><div><a href='http://arxiv.org/abs/2401.13177v1'>2401.13177v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16898v2")'>MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex
  Influence Maximization</div>
<div id='2402.16898v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T03:48:22Z</div><div>Authors: Nguyen Do, Tanmoy Chowdhury, Chen Ling, Liang Zhao, My T. Thai</div><div style='padding-top: 10px; width: 80ex'>Multiplex influence maximization (MIM) asks us to identify a set of seed
users such as to maximize the expected number of influenced users in a
multiplex network. MIM has been one of central research topics, especially in
nowadays social networking landscape where users participate in multiple online
social networks (OSNs) and their influences can propagate among several OSNs
simultaneously. Although there exist a couple combinatorial algorithms to MIM,
learning-based solutions have been desired due to its generalization ability to
heterogeneous networks and their diversified propagation characteristics. In
this paper, we introduce MIM-Reasoner, coupling reinforcement learning with
probabilistic graphical model, which effectively captures the complex
propagation process within and between layers of a given multiplex network,
thereby tackling the most challenging problem in MIM. We establish a
theoretical guarantee for MIM-Reasoner as well as conduct extensive analyses on
both synthetic and real-world datasets to validate our MIM-Reasoner's
performance.</div><div><a href='http://arxiv.org/abs/2402.16898v2'>2402.16898v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08062v1")'>Avoiding Catastrophe in Continuous Spaces by Asking for Help</div>
<div id='2402.08062v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T21:12:11Z</div><div>Authors: Benjamin Plaut, Hanlin Zhu, Stuart Russell</div><div style='padding-top: 10px; width: 80ex'>Most reinforcement learning algorithms with formal regret guarantees assume
all mistakes are reversible and rely on essentially trying all possible
options. This approach leads to poor outcomes when some mistakes are
irreparable or even catastrophic. We propose a variant of the contextual bandit
problem where the goal is to minimize the chance of catastrophe. Specifically,
we assume that the payoff each round represents the chance of avoiding
catastrophe that round, and try to maximize the product of payoffs (the overall
chance of avoiding catastrophe). To give the agent some chance of success, we
allow a limited number of queries to a mentor and assume a Lipschitz continuous
payoff function. We present an algorithm whose regret and rate of querying the
mentor both approach 0 as the time horizon grows, assuming a continuous 1D
state space and a relatively "simple" payoff function. We also provide a
matching lower bound: without the simplicity assumption: any algorithm either
constantly asks for help or is nearly guaranteed to cause catastrophe. Finally,
we identify the key obstacle to generalizing our algorithm to a
multi-dimensional state space.</div><div><a href='http://arxiv.org/abs/2402.08062v1'>2402.08062v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14923v1")'>Reinforcement Learning Interventions on Boundedly Rational Human Agents
  in Frictionful Tasks</div>
<div id='2401.14923v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T14:59:48Z</div><div>Authors: Eura Nofshin, Siddharth Swaroop, Weiwei Pan, Susan Murphy, Finale Doshi-Velez</div><div style='padding-top: 10px; width: 80ex'>Many important behavior changes are frictionful; they require individuals to
expend effort over a long period with little immediate gratification. Here, an
artificial intelligence (AI) agent can provide personalized interventions to
help individuals stick to their goals. In these settings, the AI agent must
personalize rapidly (before the individual disengages) and interpretably, to
help us understand the behavioral interventions. In this paper, we introduce
Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent
intervenes on the parameters of a Markov Decision Process (MDP) belonging to a
boundedly rational human agent. Our formulation of the human decision-maker as
a planning agent allows us to attribute undesirable human policies (ones that
do not lead to the goal) to their maladapted MDP parameters, such as an
extremely low discount factor. Furthermore, we propose a class of tractable
human models that captures fundamental behaviors in frictionful tasks.
Introducing a notion of MDP equivalence specific to BMRL, we theoretically and
empirically show that AI planning with our human models can lead to helpful
policies on a wide range of more complex, ground-truth humans.</div><div><a href='http://arxiv.org/abs/2401.14923v1'>2401.14923v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05794v2")'>Bounds on the price of feedback for mistake-bounded online learning</div>
<div id='2401.05794v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T09:56:08Z</div><div>Authors: Jesse Geneson, Linus Tang</div><div style='padding-top: 10px; width: 80ex'>We improve several worst-case bounds for various online learning scenarios
from (Auer and Long, Machine Learning, 1999). In particular, we sharpen an
upper bound for delayed ambiguous reinforcement learning by a factor of 2 and
an upper bound for learning compositions of families of functions by a factor
of 2.41. We also improve a lower bound from the same paper for learning
compositions of $k$ families of functions by a factor of $\Theta(\ln{k})$,
matching the upper bound up to a constant factor. In addition, we solve a
problem from (Long, Theoretical Computer Science, 2020) on the price of bandit
feedback with respect to standard feedback for multiclass learning, and we
improve an upper bound from (Feng et al., Theoretical Computer Science, 2023)
on the price of $r$-input delayed ambiguous reinforcement learning by a factor
of $r$, matching a lower bound from the same paper up to the leading term.</div><div><a href='http://arxiv.org/abs/2401.05794v2'>2401.05794v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04568v1")'>Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit
  Feedback and Unknown Transition</div>
<div id='2403.04568v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T15:03:50Z</div><div>Authors: Long-Fei Li, Peng Zhao, Zhi-Hua Zhou</div><div style='padding-top: 10px; width: 80ex'>We study reinforcement learning with linear function approximation, unknown
transition, and adversarial losses in the bandit feedback setting.
Specifically, we focus on linear mixture MDPs whose transition kernel is a
linear mixture model. We propose a new algorithm that attains an
$\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$ regret with high probability,
where $d$ is the dimension of feature mappings, $S$ is the size of state space,
$A$ is the size of action space, $H$ is the episode length and $K$ is the
number of episodes. Our result strictly improves the previous best-known
$\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$ result in Zhao et al. (2023a)
since $H \leq S$ holds by the layered MDP structure. Our advancements are
primarily attributed to (i) a new least square estimator for the transition
parameter that leverages the visit information of all states, as opposed to
only one state in prior work, and (ii) a new self-normalized concentration
tailored specifically to handle non-independent noises, originally proposed in
the dynamic assortment area and firstly applied in reinforcement learning to
handle correlations between different states.</div><div><a href='http://arxiv.org/abs/2403.04568v1'>2403.04568v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14585v1")'>Bandits with Abstention under Expert Advice</div>
<div id='2402.14585v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T14:38:52Z</div><div>Authors: Stephen Pasteris, Alberto Rumi, Maximilian Thiessen, Shota Saito, Atsushi Miyauchi, Fabio Vitale, Mark Herbster</div><div style='padding-top: 10px; width: 80ex'>We study the classic problem of prediction with expert advice under bandit
feedback. Our model assumes that one action, corresponding to the learner's
abstention from play, has no reward or loss on every trial. We propose the CBA
algorithm, which exploits this assumption to obtain reward bounds that can
significantly improve those of the classical Exp4 algorithm. We can view our
problem as the aggregation of confidence-rated predictors when the learner has
the option of abstention from play. Importantly, we are the first to achieve
bounds on the expected cumulative reward for general confidence-rated
predictors. In the special case of specialists we achieve a novel reward bound,
significantly improving previous bounds of SpecialistExp (treating abstention
as another action). As an example application, we discuss learning unions of
balls in a finite metric space. In this contextual setting, we devise an
efficient implementation of CBA, reducing the runtime from quadratic to almost
linear in the number of contexts. Preliminary experiments show that CBA
improves over existing bandit algorithms.</div><div><a href='http://arxiv.org/abs/2402.14585v1'>2402.14585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17235v1")'>Stochastic Gradient Succeeds for Bandits</div>
<div id='2402.17235v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T06:05:01Z</div><div>Authors: Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, Dale Schuurmans</div><div style='padding-top: 10px; width: 80ex'>We show that the \emph{stochastic gradient} bandit algorithm converges to a
\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \emph{constant}
step size. Remarkably, global convergence of the stochastic gradient bandit
algorithm has not been previously established, even though it is an old
algorithm known to be applicable to bandits. The new result is achieved by
establishing two novel technical findings: first, the noise of the stochastic
updates in the gradient bandit algorithm satisfies a strong ``growth
condition'' property, where the variance diminishes whenever progress becomes
small, implying that additional noise control via diminishing step sizes is
unnecessary; second, a form of ``weak exploration'' is automatically achieved
through the stochastic gradient updates, since they prevent the action
probabilities from decaying faster than $O(1/t)$, thus ensuring that every
action is sampled infinitely often with probability $1$. These two findings can
be used to show that the stochastic gradient update is already ``sufficient''
for bandits in the sense that exploration versus exploitation is automatically
balanced in a manner that ensures almost sure convergence to a global optimum.
These novel theoretical findings are further verified by experimental results.</div><div><a href='http://arxiv.org/abs/2402.17235v1'>2402.17235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11156v1")'>Efficient Low-Rank Matrix Estimation, Experimental Design, and
  Arm-Set-Dependent Low-Rank Bandits</div>
<div id='2402.11156v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:51:29Z</div><div>Authors: Kyoungseok Jang, Chicheng Zhang, Kwang-Sung Jun</div><div style='padding-top: 10px; width: 80ex'>We study low-rank matrix trace regression and the related problem of low-rank
matrix bandits. Assuming access to the distribution of the covariates, we
propose a novel low-rank matrix estimation method called LowPopArt and provide
its recovery guarantee that depends on a novel quantity denoted by B(Q) that
characterizes the hardness of the problem, where Q is the covariance matrix of
the measurement distribution. We show that our method can provide tighter
recovery guarantees than classical nuclear norm penalized least squares
(Koltchinskii et al., 2011) in several problems. To perform efficient
estimation with a limited number of measurements from an arbitrarily given
measurement set A, we also propose a novel experimental design criterion that
minimizes B(Q) with computational efficiency. We leverage our novel estimator
and design of experiments to derive two low-rank linear bandit algorithms for
general arm sets that enjoy improved regret upper bounds. This improves over
previous works on low-rank bandits, which make somewhat restrictive assumptions
that the arm set is the unit ball or that an efficient exploration distribution
is given. To our knowledge, our experimental design criterion is the first one
tailored to low-rank matrix estimation beyond the naive reduction to linear
regression, which can be of independent interest.</div><div><a href='http://arxiv.org/abs/2402.11156v1'>2402.11156v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06971v1")'>A representation-learning game for classes of prediction tasks</div>
<div id='2403.06971v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:54:42Z</div><div>Authors: Neria Uzan, Nir Weinberger</div><div style='padding-top: 10px; width: 80ex'>We propose a game-based formulation for learning dimensionality-reducing
representations of feature vectors, when only a prior knowledge on future
prediction tasks is available. In this game, the first player chooses a
representation, and then the second player adversarially chooses a prediction
task from a given class, representing the prior knowledge. The first player
aims is to minimize, and the second player to maximize, the regret: The minimal
prediction loss using the representation, compared to the same loss using the
original features. For the canonical setting in which the representation, the
response to predict and the predictors are all linear functions, and under the
mean squared error loss function, we derive the theoretically optimal
representation in pure strategies, which shows the effectiveness of the prior
knowledge, and the optimal regret in mixed strategies, which shows the
usefulness of randomizing the representation. For general representations and
loss functions, we propose an efficient algorithm to optimize a randomized
representation. The algorithm only requires the gradients of the loss function,
and is based on incrementally adding a representation rule to a mixture of such
rules.</div><div><a href='http://arxiv.org/abs/2403.06971v1'>2403.06971v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01047v1")'>Sharp Analysis of Power Iteration for Tensor PCA</div>
<div id='2401.01047v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T05:55:27Z</div><div>Authors: Yuchen Wu, Kangjie Zhou</div><div style='padding-top: 10px; width: 80ex'>We investigate the power iteration algorithm for the tensor PCA model
introduced in Richard and Montanari (2014). Previous work studying the
properties of tensor power iteration is either limited to a constant number of
iterations, or requires a non-trivial data-independent initialization. In this
paper, we move beyond these limitations and analyze the dynamics of randomly
initialized tensor power iteration up to polynomially many steps. Our
contributions are threefold: First, we establish sharp bounds on the number of
iterations required for power method to converge to the planted signal, for a
broad range of the signal-to-noise ratios. Second, our analysis reveals that
the actual algorithmic threshold for power iteration is smaller than the one
conjectured in literature by a polylog(n) factor, where n is the ambient
dimension. Finally, we propose a simple and effective stopping criterion for
power iteration, which provably outputs a solution that is highly correlated
with the true signal. Extensive numerical experiments verify our theoretical
results.</div><div><a href='http://arxiv.org/abs/2401.01047v1'>2401.01047v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15285v1")'>Generative Modelling with Tensor Train approximations of
  Hamilton--Jacobi--Bellman equations</div>
<div id='2402.15285v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T12:30:20Z</div><div>Authors: David Sommer, Robert Gruhlke, Max Kirstein, Martin Eigel, Claudia Schillings</div><div style='padding-top: 10px; width: 80ex'>Sampling from probability densities is a common challenge in fields such as
Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in
particular, the use of reverse-time diffusion processes depending on the
log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling
tool. In Berner et al. [2022] the authors point out that these log-densities
can be obtained by solution of a \textit{Hamilton-Jacobi-Bellman} (HJB)
equation known from stochastic optimal control. While this HJB equation is
usually treated with indirect methods such as policy iteration and unsupervised
training of black-box architectures like Neural Networks, we propose instead to
solve the HJB equation by direct time integration, using compressed polynomials
represented in the Tensor Train (TT) format for spatial discretization.
Crucially, this method is sample-free, agnostic to normalization constants and
can avoid the curse of dimensionality due to the TT compression. We provide a
complete derivation of the HJB equation's action on Tensor Train polynomials
and demonstrate the performance of the proposed time-step-, rank- and
degree-adaptive integration method on a nonlinear sampling task in 20
dimensions.</div><div><a href='http://arxiv.org/abs/2402.15285v1'>2402.15285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16435v1")'>Training Implicit Generative Models via an Invariant Statistical Loss</div>
<div id='2402.16435v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T09:32:28Z</div><div>Authors: José Manuel de Frutos, Pablo M. Olmos, Manuel A. Vázquez, Joaquín Míguez</div><div style='padding-top: 10px; width: 80ex'>Implicit generative models have the capability to learn arbitrary complex
data distributions. On the downside, training requires telling apart real data
from artificially-generated ones using adversarial discriminators, leading to
unstable training and mode-dropping issues. As reported by Zahee et al. (2017),
even in the one-dimensional (1D) case, training a generative adversarial
network (GAN) is challenging and often suboptimal. In this work, we develop a
discriminator-free method for training one-dimensional (1D) generative implicit
models and subsequently expand this method to accommodate multivariate cases.
Our loss function is a discrepancy measure between a suitably chosen
transformation of the model samples and a uniform distribution; hence, it is
invariant with respect to the true distribution of the data. We first formulate
our method for 1D random variables, providing an effective solution for
approximate reparameterization of arbitrary complex distributions. Then, we
consider the temporal setting (both univariate and multivariate), in which we
model the conditional distribution of each sample given the history of the
process. We demonstrate through numerical simulations that this new method
yields promising results, successfully learning true distributions in a variety
of scenarios and mitigating some of the well-known problems that
state-of-the-art implicit methods present.</div><div><a href='http://arxiv.org/abs/2402.16435v1'>2402.16435v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.17148v1")'>Time series generation for option pricing on quantum computers using
  tensor network</div>
<div id='2402.17148v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T02:29:24Z</div><div>Authors: Nozomu Kobayashi, Yoshiyuki Suimon, Koichi Miyamoto</div><div style='padding-top: 10px; width: 80ex'>Finance, especially option pricing, is a promising industrial field that
might benefit from quantum computing. While quantum algorithms for option
pricing have been proposed, it is desired to devise more efficient
implementations of costly operations in the algorithms, one of which is
preparing a quantum state that encodes a probability distribution of the
underlying asset price. In particular, in pricing a path-dependent option, we
need to generate a state encoding a joint distribution of the underlying asset
price at multiple time points, which is more demanding. To address these
issues, we propose a novel approach using Matrix Product State (MPS) as a
generative model for time series generation. To validate our approach, taking
the Heston model as a target, we conduct numerical experiments to generate time
series in the model. Our findings demonstrate the capability of the MPS model
to generate paths in the Heston model, highlighting its potential for
path-dependent option pricing on quantum computers.</div><div><a href='http://arxiv.org/abs/2402.17148v1'>2402.17148v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17077v1")'>Dynamical Survival Analysis with Controlled Latent States</div>
<div id='2401.17077v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T14:57:32Z</div><div>Authors: Linus Bleistein, Van-Tuan Nguyen, Adeline Fermanian, Agathe Guilloux</div><div style='padding-top: 10px; width: 80ex'>We consider the task of learning individual-specific intensities of counting
processes from a set of static variables and irregularly sampled time series.
We introduce a novel modelization approach in which the intensity is the
solution to a controlled differential equation. We first design a neural
estimator by building on neural controlled differential equations. In a second
time, we show that our model can be linearized in the signature space under
sufficient regularity conditions, yielding a signature-based estimator which we
call CoxSig. We provide theoretical learning guarantees for both estimators,
before showcasing the performance of our models on a vast array of simulated
and real-world datasets from finance, predictive maintenance and food supply
chain management.</div><div><a href='http://arxiv.org/abs/2401.17077v1'>2401.17077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08150v1")'>Differentially Private Sliced Inverse Regression: Minimax Optimality and
  Algorithm</div>
<div id='2401.08150v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T06:47:43Z</div><div>Authors: Xintao Xia, Linjun Zhang, Zhanrui Cai</div><div style='padding-top: 10px; width: 80ex'>Privacy preservation has become a critical concern in high-dimensional data
analysis due to the growing prevalence of data-driven applications. Proposed by
Li (1991), sliced inverse regression has emerged as a widely utilized
statistical technique for reducing covariate dimensionality while maintaining
sufficient statistical information. In this paper, we propose optimally
differentially private algorithms specifically designed to address privacy
concerns in the context of sufficient dimension reduction. We proceed to
establish lower bounds for differentially private sliced inverse regression in
both the low and high-dimensional settings. Moreover, we develop differentially
private algorithms that achieve the minimax lower bounds up to logarithmic
factors. Through a combination of simulations and real data analysis, we
illustrate the efficacy of these differentially private algorithms in
safeguarding privacy while preserving vital information within the reduced
dimension space. As a natural extension, we can readily offer analogous lower
and upper bounds for differentially private sparse principal component
analysis, a topic that may also be of potential interest to the statistical and
machine learning community.</div><div><a href='http://arxiv.org/abs/2401.08150v1'>2401.08150v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01294v1")'>Efficient Sparse Least Absolute Deviation Regression with Differential
  Privacy</div>
<div id='2401.01294v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T17:13:34Z</div><div>Authors: Weidong Liu, Xiaojun Mao, Xiaofei Zhang, Xin Zhang</div><div style='padding-top: 10px; width: 80ex'>In recent years, privacy-preserving machine learning algorithms have
attracted increasing attention because of their important applications in many
scientific fields. However, in the literature, most privacy-preserving
algorithms demand learning objectives to be strongly convex and Lipschitz
smooth, which thus cannot cover a wide class of robust loss functions (e.g.,
quantile/least absolute loss). In this work, we aim to develop a fast
privacy-preserving learning solution for a sparse robust regression problem.
Our learning loss consists of a robust least absolute loss and an $\ell_1$
sparse penalty term. To fast solve the non-smooth loss under a given privacy
budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)
algorithm for least absolute deviation regression. Our algorithm achieves a
fast estimation by reformulating the sparse LAD problem as a penalized least
square estimation problem and adopts a three-stage noise injection to guarantee
the $(\epsilon,\delta)$-differential privacy. We show that our algorithm can
achieve better privacy and statistical accuracy trade-off compared with the
state-of-the-art privacy-preserving regression algorithms. In the end, we
conduct experiments to verify the efficiency of our proposed FRAPPE algorithm.</div><div><a href='http://arxiv.org/abs/2401.01294v1'>2401.01294v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08468v2")'>Keep or toss? A nonparametric score to evaluate solutions for noisy ICA</div>
<div id='2401.08468v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T16:18:17Z</div><div>Authors: Syamantak Kumar, Purnamrita Sarkar, Peter Bickel, Derek Bean</div><div style='padding-top: 10px; width: 80ex'>Independent Component Analysis (ICA) was introduced in the 1980's as a model
for Blind Source Separation (BSS), which refers to the process of recovering
the sources underlying a mixture of signals, with little knowledge about the
source signals or the mixing process. While there are many sophisticated
algorithms for estimation, different methods have different shortcomings. In
this paper, we develop a nonparametric score to adaptively pick the right
algorithm for ICA with arbitrary Gaussian noise. The novelty of this score
stems from the fact that it just assumes a finite second moment of the data and
uses the characteristic function to evaluate the quality of the estimated
mixing matrix without any knowledge of the parameters of the noise
distribution. In addition, we propose some new contrast functions and
algorithms that enjoy the same fast computability as existing algorithms like
FASTICA and JADE but work in domains where the former may fail. While these
also may have weaknesses, our proposed diagnostic, as shown by our simulations,
can remedy them. Finally, we propose a theoretical framework to analyze the
local and global convergence properties of our algorithms.</div><div><a href='http://arxiv.org/abs/2401.08468v2'>2401.08468v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15292v4")'>Adaptive Block Sparse Regularization under Arbitrary Linear Transform</div>
<div id='2401.15292v4' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T04:24:19Z</div><div>Authors: Takanobu Furuhashi, Hidekata Hontani, Tatsuya Yokota</div><div style='padding-top: 10px; width: 80ex'>We propose a convex and fast signal reconstruction method for block sparsity
under arbitrary linear transform with unknown block structure. The proposed
method is a generalization of the similar existing method and can reconstruct
signals with block sparsity under non-invertible transforms, unlike the
existing method. Our work broadens the scope of block sparse regularization,
enabling more versatile and powerful applications across various signal
processing domains. We derive an iterative algorithm for solving proposed
method and provide conditions for its convergence to the optimal solution.
Numerical experiments demonstrate the effectiveness of the proposed method.</div><div><a href='http://arxiv.org/abs/2401.15292v4'>2401.15292v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03168v1")'>Learning Explicitly Conditioned Sparsifying Transforms</div>
<div id='2403.03168v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:03:51Z</div><div>Authors: Andrei Pătraşcu, Cristian Rusu, Paul Irofti</div><div style='padding-top: 10px; width: 80ex'>Sparsifying transforms became in the last decades widely known tools for
finding structured sparse representations of signals in certain transform
domains. Despite the popularity of classical transforms such as DCT and
Wavelet, learning optimal transforms that guarantee good representations of
data into the sparse domain has been recently analyzed in a series of papers.
Typically, the conditioning number and representation ability are complementary
key features of learning square transforms that may not be explicitly
controlled in a given optimization model. Unlike the existing approaches from
the literature, in our paper, we consider a new sparsifying transform model
that enforces explicit control over the data representation quality and the
condition number of the learned transforms. We confirm through numerical
experiments that our model presents better numerical behavior than the
state-of-the-art.</div><div><a href='http://arxiv.org/abs/2403.03168v1'>2403.03168v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16612v1")'>Learning a Gaussian Mixture for Sparsity Regularization in Inverse
  Problems</div>
<div id='2401.16612v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T22:52:57Z</div><div>Authors: Giovanni S. Alberti, Luca Ratti, Matteo Santacesaria, Silvia Sciutto</div><div style='padding-top: 10px; width: 80ex'>In inverse problems, it is widely recognized that the incorporation of a
sparsity prior yields a regularization effect on the solution. This approach is
grounded on the a priori assumption that the unknown can be appropriately
represented in a basis with a limited number of significant components, while
most coefficients are close to zero. This occurrence is frequently observed in
real-world scenarios, such as with piecewise smooth signals. In this study, we
propose a probabilistic sparsity prior formulated as a mixture of degenerate
Gaussians, capable of modeling sparsity with respect to a generic basis. Under
this premise, we design a neural network that can be interpreted as the Bayes
estimator for linear inverse problems. Additionally, we put forth both a
supervised and an unsupervised training strategy to estimate the parameters of
this network. To evaluate the effectiveness of our approach, we conduct a
numerical comparison with commonly employed sparsity-promoting regularization
techniques, namely LASSO, group LASSO, iterative hard thresholding, and sparse
coding/dictionary learning. Notably, our reconstructions consistently exhibit
lower mean square error values across all $1$D datasets utilized for the
comparisons, even in cases where the datasets significantly deviate from a
Gaussian mixture model.</div><div><a href='http://arxiv.org/abs/2401.16612v1'>2401.16612v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07026v1")'>Whiteness-based bilevel learning of regularization parameters in imaging</div>
<div id='2403.07026v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T15:45:39Z</div><div>Authors: Carlo Santambrogio, Monica Pragliola, Alessandro Lanza, Marco Donatelli, Luca Calatroni</div><div style='padding-top: 10px; width: 80ex'>We consider an unsupervised bilevel optimization strategy for learning
regularization parameters in the context of imaging inverse problems in the
presence of additive white Gaussian noise. Compared to supervised and
semi-supervised metrics relying either on the prior knowledge of reference data
and/or on some (partial) knowledge on the noise statistics, the proposed
approach optimizes the whiteness of the residual between the observed data and
the observation model with no need of ground-truth data.We validate the
approach on standard Total Variation-regularized image deconvolution problems
which show that the proposed quality metric provides estimates close to the
mean-square error oracle and to discrepancy-based principles.</div><div><a href='http://arxiv.org/abs/2403.07026v1'>2403.07026v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12072v1")'>Robustness and Exploration of Variational and Machine Learning
  Approaches to Inverse Problems: An Overview</div>
<div id='2402.12072v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T11:48:11Z</div><div>Authors: Alexander Auras, Kanchana Vaishnavi Gandikota, Hannah Droege, Michael Moeller</div><div style='padding-top: 10px; width: 80ex'>This paper attempts to provide an overview of current approaches for solving
inverse problems in imaging using variational methods and machine learning. A
special focus lies on point estimators and their robustness against adversarial
perturbations. In this context results of numerical experiments for a
one-dimensional toy problem are provided, showing the robustness of different
approaches and empirically verifying theoretical guarantees. Another focus of
this review is the exploration of the subspace of data consistent solutions
through explicit guidance to satisfy specific semantic or textural properties.</div><div><a href='http://arxiv.org/abs/2402.12072v1'>2402.12072v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15635v1")'>Bagged Deep Image Prior for Recovering Images in the Presence of Speckle
  Noise</div>
<div id='2402.15635v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T22:36:07Z</div><div>Authors: Xi Chen, Zhewen Hou, Christopher A. Metzler, Arian Maleki, Shirin Jalali</div><div style='padding-top: 10px; width: 80ex'>We investigate both the theoretical and algorithmic aspects of
likelihood-based methods for recovering a complex-valued signal from multiple
sets of measurements, referred to as looks, affected by speckle
(multiplicative) noise. Our theoretical contributions include establishing the
first existing theoretical upper bound on the Mean Squared Error (MSE) of the
maximum likelihood estimator under the deep image prior hypothesis. Our
theoretical results capture the dependence of MSE upon the number of parameters
in the deep image prior, the number of looks, the signal dimension, and the
number of measurements per look. On the algorithmic side, we introduce the
concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with
projected gradient descent. Furthermore, we show how employing Newton-Schulz
algorithm for calculating matrix inverses within the iterations of PGD reduces
the computational complexity of the algorithm. We will show that this method
achieves the state-of-the-art performance.</div><div><a href='http://arxiv.org/abs/2402.15635v1'>2402.15635v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02964v1")'>Mixed Noise and Posterior Estimation with Conditional DeepGEM</div>
<div id='2402.02964v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:42:21Z</div><div>Authors: Paul Hagemann, Johannes Hertrich, Maren Casfor, Sebastian Heidenreich, Gabriele Steidl</div><div style='padding-top: 10px; width: 80ex'>Motivated by indirect measurements and applications from nanometrology with a
mixed noise model, we develop a novel algorithm for jointly estimating the
posterior and the noise parameters in Bayesian inverse problems. We propose to
solve the problem by an expectation maximization (EM) algorithm. Based on the
current noise parameters, we learn in the E-step a conditional normalizing flow
that approximates the posterior. In the M-step, we propose to find the noise
parameter updates again by an EM algorithm, which has analytical formulas. We
compare the training of the conditional normalizing flow with the forward and
reverse KL, and show that our model is able to incorporate information from
many measurements, unlike previous approaches.</div><div><a href='http://arxiv.org/abs/2402.02964v1'>2402.02964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03345v1")'>Weakly supervised covariance matrices alignment through Stiefel matrices
  estimation for MEG applications</div>
<div id='2402.03345v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T19:04:49Z</div><div>Authors: Antoine Collas, Rémi Flamary, Alexandre Gramfort</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel domain adaptation technique for time series
data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the
challenge of limited labeled signals in the target dataset. Leveraging a
domain-dependent mixing model and the optimal transport domain adaptation
assumption, we exploit abundant unlabeled data in the target domain to ensure
effective prediction by establishing pairwise correspondence with equivalent
signal variances between domains. Theoretical foundations are laid for
identifying crucial Stiefel matrices, essential for recovering underlying
signal variances from a Riemannian representation of observed signal
covariances. We propose an integrated cost function that simultaneously learns
these matrices, pairwise domain relationships, and a predictor, classifier, or
regressor, depending on the task. Applied to neuroscience problems, MSA
outperforms recent methods in brain-age regression with task variations using
magnetoencephalography (MEG) signals from the Cam-CAN dataset.</div><div><a href='http://arxiv.org/abs/2402.03345v1'>2402.03345v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08948v1")'>Mean-Field Analysis for Learning Subspace-Sparse Polynomials with
  Gaussian Input</div>
<div id='2402.08948v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T05:34:24Z</div><div>Authors: Ziang Chen, Rong Ge</div><div style='padding-top: 10px; width: 80ex'>In this work, we study the mean-field flow for learning subspace-sparse
polynomials using stochastic gradient descent and two-layer neural networks,
where the input distribution is standard Gaussian and the output only depends
on the projection of the input onto a low-dimensional subspace. We propose a
basis-free generalization of the merged-staircase property in Abbe et al.
(2022) and establish a necessary condition for the SGD-learnability. In
addition, we prove that the condition is almost sufficient, in the sense that a
condition slightly stronger than the necessary condition can guarantee the
exponential decay of the loss functional to zero.</div><div><a href='http://arxiv.org/abs/2402.08948v1'>2402.08948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08493v1")'>Sparsity via Sparse Group $k$-max Regularization</div>
<div id='2402.08493v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T14:41:28Z</div><div>Authors: Qinghua Tao, Xiangming Xi, Jun Xu, Johan A. K. Suykens</div><div style='padding-top: 10px; width: 80ex'>For the linear inverse problem with sparsity constraints, the $l_0$
regularized problem is NP-hard, and existing approaches either utilize greedy
algorithms to find almost-optimal solutions or to approximate the $l_0$
regularization with its convex counterparts. In this paper, we propose a novel
and concise regularization, namely the sparse group $k$-max regularization,
which can not only simultaneously enhance the group-wise and in-group sparsity,
but also casts no additional restraints on the magnitude of variables in each
group, which is especially important for variables at different scales, so that
it approximate the $l_0$ norm more closely. We also establish an iterative soft
thresholding algorithm with local optimality conditions and complexity analysis
provided. Through numerical experiments on both synthetic and real-world
datasets, we verify the effectiveness and flexibility of the proposed method.</div><div><a href='http://arxiv.org/abs/2402.08493v1'>2402.08493v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02463v1")'>A Fast Method for Lasso and Logistic Lasso</div>
<div id='2402.02463v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T12:24:52Z</div><div>Authors: Siu-Wing Cheng, Man Ting Wong</div><div style='padding-top: 10px; width: 80ex'>We propose a fast method for solving compressed sensing, Lasso regression,
and Logistic Lasso regression problems that iteratively runs an appropriate
solver using an active set approach. We design a strategy to update the active
set that achieves a large speedup over a single call of several solvers,
including gradient projection for sparse reconstruction (GPSR), lassoglm of
Matlab, and glmnet. For compressed sensing, the hybrid of our method and GPSR
is 31.41 times faster than GPSR on average for Gaussian ensembles and 25.64
faster on average for binary ensembles. For Lasso regression, the hybrid of our
method and GPSR achieves a 30.67-fold average speedup in our experiments. In
our experiments on Logistic Lasso regression, the hybrid of our method and
lassoglm gives an 11.95-fold average speedup, and the hybrid of our method and
glmnet gives a 1.40-fold average speedup.</div><div><a href='http://arxiv.org/abs/2402.02463v1'>2402.02463v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12609v1")'>Fast Semi-supervised Unmixing using Non-convex Optimization</div>
<div id='2401.12609v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T10:07:41Z</div><div>Authors: Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce a novel linear model tailored for
semisupervised/library-based unmixing. Our model incorporates considerations
for library mismatch while enabling the enforcement of the abundance sum-to-one
constraint (ASC). Unlike conventional sparse unmixing methods, this model
involves nonconvex optimization, presenting significant computational
challenges. We demonstrate the efficacy of Alternating Methods of Multipliers
(ADMM) in cyclically solving these intricate problems. We propose two
semisupervised unmixing approaches, each relying on distinct priors applied to
the new model in addition to the ASC: sparsity prior and convexity constraint.
Our experimental results validate that enforcing the convexity constraint
outperforms the sparsity prior for the endmember library. These results are
corroborated across three simulated datasets (accounting for spectral
variability and varying pixel purity levels) and the Cuprite dataset.
Additionally, our comparison with conventional sparse unmixing methods
showcases considerable advantages of our proposed model, which entails
nonconvex optimization. Notably, our implementations of the proposed
algorithms-fast semisupervised unmixing (FaSUn) and sparse unmixing using
soft-shrinkage (SUnS)-prove considerably more efficient than traditional sparse
unmixing methods. SUnS and FaSUn were implemented using PyTorch and provided in
a dedicated Python package called Fast Semisupervised Unmixing (FUnmix), which
is open-source and available at https://github.com/BehnoodRasti/FUnmix</div><div><a href='http://arxiv.org/abs/2401.12609v1'>2401.12609v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04526v1")'>Hyperspectral unmixing for Raman spectroscopy via physics-constrained
  autoencoders</div>
<div id='2403.04526v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:27:08Z</div><div>Authors: Dimitar Georgiev, Álvaro Fernández-Galiana, Simon Vilms Pedersen, Georgios Papadopoulos, Ruoxiao Xie, Molly M. Stevens, Mauricio Barahona</div><div style='padding-top: 10px; width: 80ex'>Raman spectroscopy is widely used across scientific domains to characterize
the chemical composition of samples in a non-destructive, label-free manner.
Many applications entail the unmixing of signals from mixtures of molecular
species to identify the individual components present and their proportions,
yet conventional methods for chemometrics often struggle with complex mixture
scenarios encountered in practice. Here, we develop hyperspectral unmixing
algorithms based on autoencoder neural networks, and we systematically validate
them using both synthetic and experimental benchmark datasets created in-house.
Our results demonstrate that unmixing autoencoders provide improved accuracy,
robustness and efficiency compared to standard unmixing methods. We also
showcase the applicability of autoencoders to complex biological settings by
showing improved biochemical characterization of volumetric Raman imaging data
from a monocytic cell.</div><div><a href='http://arxiv.org/abs/2403.04526v1'>2403.04526v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00851v1")'>Data Augmentation Scheme for Raman Spectra with Highly Correlated
  Annotations</div>
<div id='2402.00851v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:46:28Z</div><div>Authors: Christoph Lange, Isabel Thiele, Lara Santolin, Sebastian L. Riedel, Maxim Borisyak, Peter Neubauer, M. Nicolas Cruz Bournazou</div><div style='padding-top: 10px; width: 80ex'>In biotechnology Raman Spectroscopy is rapidly gaining popularity as a
process analytical technology (PAT) that measures cell densities, substrate-
and product concentrations. As it records vibrational modes of molecules it
provides that information non-invasively in a single spectrum. Typically,
partial least squares (PLS) is the model of choice to infer information about
variables of interest from the spectra. However, biological processes are known
for their complexity where convolutional neural networks (CNN) present a
powerful alternative. They can handle non-Gaussian noise and account for beam
misalignment, pixel malfunctions or the presence of additional substances.
However, they require a lot of data during model training, and they pick up
non-linear dependencies in the process variables. In this work, we exploit the
additive nature of spectra in order to generate additional data points from a
given dataset that have statistically independent labels so that a network
trained on such data exhibits low correlations between the model predictions.
We show that training a CNN on these generated data points improves the
performance on datasets where the annotations do not bear the same correlation
as the dataset that was used for model training. This data augmentation
technique enables us to reuse spectra as training data for new contexts that
exhibit different correlations. The additional data allows for building a
better and more robust model. This is of interest in scenarios where large
amounts of historical data are available but are currently not used for model
training. We demonstrate the capabilities of the proposed method using
synthetic spectra of Ralstonia eutropha batch cultivations to monitor
substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations
during of the experiments.</div><div><a href='http://arxiv.org/abs/2402.00851v1'>2402.00851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18830v2")'>Training-set-free two-stage deep learning for spectroscopic data
  de-noising</div>
<div id='2402.18830v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T03:31:41Z</div><div>Authors: Dongchen Huang, Junde Liu, Tian Qian, Hongming Weng</div><div style='padding-top: 10px; width: 80ex'>De-noising is a prominent step in the spectra post-processing procedure.
Previous machine learning-based methods are fast but mostly based on supervised
learning and require a training set that may be typically expensive in real
experimental measurements. Unsupervised learning-based algorithms are slow and
require many iterations to achieve convergence. Here, we bridge this gap by
proposing a training-set-free two-stage deep learning method. We show that the
fuzzy fixed input in previous methods can be improved by introducing an
adaptive prior. Combined with more advanced optimization techniques, our
approach can achieve five times acceleration compared to previous work.
Theoretically, we study the landscape of a corresponding non-convex linear
problem, and our results indicates that this problem has benign geometry for
first-order algorithms to converge.</div><div><a href='http://arxiv.org/abs/2402.18830v2'>2402.18830v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00197v1")'>Determination of Trace Organic Contaminant Concentration via Machine
  Classification of Surface-Enhanced Raman Spectra</div>
<div id='2402.00197v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T21:49:40Z</div><div>Authors: Vishnu Jayaprakash, Jae Bem You, Chiranjeevi Kanike, Jinfeng Liu, Christopher McCallum, Xuehua Zhang</div><div style='padding-top: 10px; width: 80ex'>Accurate detection and analysis of traces of persistent organic pollutants in
water is important in many areas, including environmental monitoring and food
quality control, due to their long environmental stability and potential
bioaccumulation. While conventional analysis of organic pollutants requires
expensive equipment, surface enhanced Raman spectroscopy (SERS) has
demonstrated great potential for accurate detection of these contaminants.
However, SERS analytical difficulties, such as spectral preprocessing,
denoising, and substrate-based spectral variation, have hindered widespread use
of the technique. Here, we demonstrate an approach for predicting the
concentration of sample pollutants from messy, unprocessed Raman data using
machine learning. Frequency domain transform methods, including the Fourier and
Walsh Hadamard transforms, are applied to sets of Raman spectra of three model
micropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are
then used to train machine learning algorithms. Using standard machine learning
models, the concentration of sample pollutants are predicted with more than 80
percent cross-validation accuracy from raw Raman data. cross-validation
accuracy of 85 percent was achieved using deep learning for a moderately sized
dataset (100 spectra), and 70 to 80 percent cross-validation accuracy was
achieved even for very small datasets (50 spectra). Additionally, standard
models were shown to accurately identify characteristic peaks via analysis of
their importance scores. The approach shown here has the potential to be
applied to facilitate accurate detection and analysis of persistent organic
pollutants by surface-enhanced Raman spectroscopy.</div><div><a href='http://arxiv.org/abs/2402.00197v1'>2402.00197v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14045v1")'>Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical
  Imaging: A Systematic Literature Review and Future Directions</div>
<div id='2402.14045v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T13:06:48Z</div><div>Authors: Sisipho Hamlomo, Marcellin Atemkeng, Yusuf Brima, Chuneeta Nunhokee, Jeremy Baxter</div><div style='padding-top: 10px; width: 80ex'>The large volume and complexity of medical imaging datasets are bottlenecks
for storage, transmission, and processing. To tackle these challenges, the
application of low-rank matrix approximation (LRMA) and its derivative, local
LRMA (LLRMA) has demonstrated potential.
  This paper conducts a systematic literature review to showcase works applying
LRMA and LLRMA in medical imaging. A detailed analysis of the literature
identifies LRMA and LLRMA methods applied to various imaging modalities. This
paper addresses the challenges and limitations associated with existing LRMA
and LLRMA methods.
  We note a significant shift towards a preference for LLRMA in the medical
imaging field since 2015, demonstrating its potential and effectiveness in
capturing complex structures in medical data compared to LRMA. Acknowledging
the limitations of shallow similarity methods used with LLRMA, we suggest
advanced semantic image segmentation for similarity measure, explaining in
detail how it can measure similar patches and their feasibility.
  We note that LRMA and LLRMA are mainly applied to unstructured medical data,
and we propose extending their application to different medical data types,
including structured and semi-structured. This paper also discusses how LRMA
and LLRMA can be applied to regular data with missing entries and the impact of
inaccuracies in predicting missing values and their effects. We discuss the
impact of patch size and propose the use of random search (RS) to determine the
optimal patch size. To enhance feasibility, a hybrid approach using Bayesian
optimization and RS is proposed, which could improve the application of LRMA
and LLRMA in medical imaging.</div><div><a href='http://arxiv.org/abs/2402.14045v1'>2402.14045v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02531v1")'>Density-based Isometric Mapping</div>
<div id='2403.02531v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T22:51:51Z</div><div>Authors: Bardia Yousefi, Mélina Khansari, Ryan Trask, Patrick Tallon, Carina Carino, Arman Afrasiyabi, Vikas Kundra, Lan Ma, Lei Ren, Keyvan Farahani, Michelle Hershman</div><div style='padding-top: 10px; width: 80ex'>The isometric mapping method employs the shortest path algorithm to estimate
the Euclidean distance between points on High dimensional (HD) manifolds. This
may not be sufficient for weakly uniformed HD data as it could lead to
overestimating distances between far neighboring points, resulting in
inconsistencies between the intrinsic (local) and extrinsic (global) distances
during the projection. To address this issue, we modify the shortest path
algorithm by adding a novel constraint inspired by the Parzen-Rosenblatt (PR)
window, which helps to maintain the uniformity of the constructed shortest-path
graph in Isomap. Multiple imaging datasets overall of 72,236 cases, 70,000
MINST data, 1596 from multiple Chest-XRay pneumonia datasets, and three NSCLC
CT/PET datasets with a total of 640 lung cancer patients, were used to
benchmark and validate PR-Isomap. 431 imaging biomarkers were extracted from
each modality. Our results indicate that PR-Isomap projects HD attributes into
a lower-dimensional (LD) space while preserving information, visualized by the
MNIST dataset indicating the maintaining local and global distances. PR-Isomap
achieved the highest comparative accuracies of 80.9% (STD:5.8) for pneumonia
and 78.5% (STD:4.4), 88.4% (STD:1.4), and 61.4% (STD:11.4) for three NSCLC
datasets, with a confidence interval of 95% for outcome prediction. Similarly,
the multivariate Cox model showed higher overall survival, measured with
c-statistics and log-likelihood test, of PR-Isomap compared to other
dimensionality reduction methods. Kaplan Meier survival curve also signifies
the notable ability of PR-Isomap to distinguish between high-risk and low-risk
patients using multimodal imaging biomarkers preserving HD imaging
characteristics for precision medicine.</div><div><a href='http://arxiv.org/abs/2403.02531v1'>2403.02531v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06056v1")'>Absence of spurious solutions far from ground truth: A low-rank analysis
  with high-order losses</div>
<div id='2403.06056v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T01:07:22Z</div><div>Authors: Ziye Ma, Ying Chen, Javad Lavaei, Somayeh Sojoudi</div><div style='padding-top: 10px; width: 80ex'>Matrix sensing problems exhibit pervasive non-convexity, plaguing
optimization with a proliferation of suboptimal spurious solutions. Avoiding
convergence to these critical points poses a major challenge. This work
provides new theoretical insights that help demystify the intricacies of the
non-convex landscape. In this work, we prove that under certain conditions,
critical points sufficiently distant from the ground truth matrix exhibit
favorable geometry by being strict saddle points rather than troublesome local
minima. Moreover, we introduce the notion of higher-order losses for the matrix
sensing problem and show that the incorporation of such losses into the
objective function amplifies the negative curvature around those distant
critical points. This implies that increasing the complexity of the objective
function via high-order losses accelerates the escape from such critical points
and acts as a desirable alternative to increasing the complexity of the
optimization problem via over-parametrization. By elucidating key
characteristics of the non-convex optimization landscape, this work makes
progress towards a comprehensive framework for tackling broader machine
learning objectives plagued by non-convexity.</div><div><a href='http://arxiv.org/abs/2403.06056v1'>2403.06056v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17215v1")'>Multidimensional unstructured sparse recovery via eigenmatrix</div>
<div id='2402.17215v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T05:11:14Z</div><div>Authors: Lexing Ying</div><div style='padding-top: 10px; width: 80ex'>This note considers the multidimensional unstructured sparse recovery
problems. Examples include Fourier inversion and sparse deconvolution. The
eigenmatrix is a data-driven construction with desired approximate eigenvalues
and eigenvectors proposed for the one-dimensional problems. This note extends
the eigenmatrix approach to multidimensional problems. Numerical results are
provided to demonstrate the performance of the proposed method.</div><div><a href='http://arxiv.org/abs/2402.17215v1'>2402.17215v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03169v1")'>A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation</div>
<div id='2402.03169v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:38:30Z</div><div>Authors: Hugo Lebeau, Florent Chatelain, Romain Couillet</div><div style='padding-top: 10px; width: 80ex'>This work presents a comprehensive understanding of the estimation of a
planted low-rank signal from a general spiked tensor model near the
computational threshold. Relying on standard tools from the theory of large
random matrices, we characterize the large-dimensional spectral behavior of the
unfoldings of the data tensor and exhibit relevant signal-to-noise ratios
governing the detectability of the principal directions of the signal. These
results allow to accurately predict the reconstruction performance of truncated
multilinear SVD (MLSVD) in the non-trivial regime. This is particularly
important since it serves as an initialization of the higher-order orthogonal
iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank
approximation depends entirely on its initialization. We give a sufficient
condition for the convergence of HOOI and show that the number of iterations
before convergence tends to $1$ in the large-dimensional limit.</div><div><a href='http://arxiv.org/abs/2402.03169v1'>2402.03169v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03468v1")'>Exact Tensor Completion Powered by Arbitrary Linear Transforms</div>
<div id='2402.03468v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:26:38Z</div><div>Authors: Li Ge, Xue Jiang, Lin Chen</div><div style='padding-top: 10px; width: 80ex'>In this work, a tensor completion problem is studied, which aims to perfectly
recover the tensor from partial observations. Existing theoretical guarantee
requires the involved transform to be orthogonal, which hinders its
applications. In this paper, jumping out of the constraints of isotropy or
self-adjointness, the theoretical guarantee of exact tensor completion with
arbitrary linear transforms is established. To that end, we define a new
tensor-tensor product, which leads us to a new definition of the tensor nuclear
norm. Equipped with these tools, an efficient algorithm based on alternating
direction of multipliers is designed to solve the transformed tensor completion
program and the theoretical bound is obtained. Our model and proof greatly
enhance the flexibility of tensor completion and extensive experiments validate
the superiority of the proposed method.</div><div><a href='http://arxiv.org/abs/2402.03468v1'>2402.03468v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01842v1")'>Wasserstein Nonnegative Tensor Factorization with Manifold
  Regularization</div>
<div id='2401.01842v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T17:20:27Z</div><div>Authors: Jianyu Wang, Linruize Tang</div><div style='padding-top: 10px; width: 80ex'>Nonnegative tensor factorization (NTF) has become an important tool for
feature extraction and part-based representation with preserved intrinsic
structure information from nonnegative high-order data. However, the original
NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss
function which treats each feature equally leading to the neglect of the
side-information of features. To utilize correlation information of features
and manifold information of samples, we introduce Wasserstein manifold
nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein
distance between the distribution of input tensorial data and the distribution
of reconstruction. Although some researches about Wasserstein distance have
been proposed in nonnegative matrix factorization (NMF), they ignore the
spatial structure information of higher-order data. We use Wasserstein distance
(a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and
add a graph regularizer to a latent factor. Experimental results demonstrate
the effectiveness of the proposed method compared with other NMF and NTF
methods.</div><div><a href='http://arxiv.org/abs/2401.01842v1'>2401.01842v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16836v1")'>Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition</div>
<div id='2401.16836v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T09:22:37Z</div><div>Authors: Juefei Chen, Longxiu Huang, Yimin Wei</div><div style='padding-top: 10px; width: 80ex'>Nonnegative Matrix Factorization (NMF) is an important unsupervised learning
method to extract meaningful features from data. To address the NMF problem
within a polynomial time framework, researchers have introduced a separability
assumption, which has recently evolved into the concept of coseparability. This
advancement offers a more efficient core representation for the original data.
However, in the real world, the data is more natural to be represented as a
multi-dimensional array, such as images or videos. The NMF's application to
high-dimensional data involves vectorization, which risks losing essential
multi-dimensional correlations. To retain these inherent correlations in the
data, we turn to tensors (multidimensional arrays) and leverage the tensor
t-product. This approach extends the coseparable NMF to the tensor setting,
creating what we term coseparable Nonnegative Tensor Factorization (NTF). In
this work, we provide an alternating index selection method to select the
coseparable core. Furthermore, we validate the t-CUR sampling theory and
integrate it with the tensor Discrete Empirical Interpolation Method (t-DEIM)
to introduce an alternative, randomized index selection process. These methods
have been tested on both synthetic and facial analysis datasets. The results
demonstrate the efficiency of coseparable NTF when compared to coseparable NMF.</div><div><a href='http://arxiv.org/abs/2401.16836v1'>2401.16836v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06153v2")'>The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data</div>
<div id='2403.06153v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T09:54:56Z</div><div>Authors: John Hood, Aaron Schein</div><div style='padding-top: 10px; width: 80ex'>This paper introduces AL$\ell_0$CORE, a new form of probabilistic
non-negative tensor decomposition. AL$\ell_0$CORE is a Tucker decomposition
where the number of non-zero elements (i.e., the $\ell_0$-norm) of the core
tensor is constrained to a preset value $Q$ much smaller than the size of the
core. While the user dictates the total budget $Q$, the locations and values of
the non-zero elements are latent variables and allocated across the core tensor
during inference. AL$\ell_0$CORE -- i.e., $allo$cated $\ell_0$-$co$nstrained
$core$-- thus enjoys both the computational tractability of CP decomposition
and the qualitatively appealing latent structure of Tucker. In a suite of
real-data experiments, we demonstrate that AL$\ell_0$CORE typically requires
only tiny fractions (e.g.,~1%) of the full core to achieve the same results as
full Tucker decomposition at only a correspondingly tiny fraction of the cost.</div><div><a href='http://arxiv.org/abs/2403.06153v2'>2403.06153v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07711v1")'>Efficient Nonparametric Tensor Decomposition for Binary and Count Data</div>
<div id='2401.07711v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T14:27:03Z</div><div>Authors: Zerui Tao, Toshihisa Tanaka, Qibin Zhao</div><div style='padding-top: 10px; width: 80ex'>In numerous applications, binary reactions or event counts are observed and
stored within high-order tensors. Tensor decompositions (TDs) serve as a
powerful tool to handle such high-dimensional and sparse data. However, many
traditional TDs are explicitly or implicitly designed based on the Gaussian
distribution, which is unsuitable for discrete data. Moreover, most TDs rely on
predefined multi-linear structures, such as CP and Tucker formats. Therefore,
they may not be effective enough to handle complex real-world datasets. To
address these issues, we propose ENTED, an \underline{E}fficient
\underline{N}onparametric \underline{TE}nsor \underline{D}ecomposition for
binary and count tensors. Specifically, we first employ a nonparametric
Gaussian process (GP) to replace traditional multi-linear structures. Next, we
utilize the \pg augmentation which provides a unified framework to establish
conjugate models for binary and count distributions. Finally, to address the
computational issue of GPs, we enhance the model by incorporating sparse
orthogonal variational inference of inducing points, which offers a more
effective covariance approximation within GPs and stochastic natural gradient
updates for nonparametric models. We evaluate our model on several real-world
tensor completion tasks, considering binary and count datasets. The results
manifest both better performance and computational advantages of the proposed
model.</div><div><a href='http://arxiv.org/abs/2401.07711v1'>2401.07711v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15345v1")'>Fourier Basis Density Model</div>
<div id='2402.15345v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:26:12Z</div><div>Authors: Alfredo De la Fuente, Saurabh Singh, Johannes Ballé</div><div style='padding-top: 10px; width: 80ex'>We introduce a lightweight, flexible and end-to-end trainable probability
density model parameterized by a constrained Fourier basis. We assess its
performance at approximating a range of multi-modal 1D densities, which are
generally difficult to fit. In comparison to the deep factorized model
introduced in [1], our model achieves a lower cross entropy at a similar
computational budget. In addition, we also evaluate our method on a toy
compression task, demonstrating its utility in learned compression.</div><div><a href='http://arxiv.org/abs/2402.15345v1'>2402.15345v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10910v1")'>Graph Regularized NMF with L20-norm for Unsupervised Feature Learning</div>
<div id='2403.10910v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T12:10:01Z</div><div>Authors: Zhen Wang, Wenwen Min</div><div style='padding-top: 10px; width: 80ex'>Nonnegative Matrix Factorization (NMF) is a widely applied technique in the
fields of machine learning and data mining. Graph Regularized Non-negative
Matrix Factorization (GNMF) is an extension of NMF that incorporates graph
regularization constraints. GNMF has demonstrated exceptional performance in
clustering and dimensionality reduction, effectively discovering inherent
low-dimensional structures embedded within high-dimensional spaces. However,
the sensitivity of GNMF to noise limits its stability and robustness in
practical applications. In order to enhance feature sparsity and mitigate the
impact of noise while mining row sparsity patterns in the data for effective
feature selection, we introduce the $\ell_{2,0}$-norm constraint as the
sparsity constraints for GNMF. We propose an unsupervised feature learning
framework based on GNMF\_$\ell_{20}$ and devise an algorithm based on PALM and
its accelerated version to address this problem. Additionally, we establish the
convergence of the proposed algorithms and validate the efficacy and
superiority of our approach through experiments conducted on both simulated and
real image data.</div><div><a href='http://arxiv.org/abs/2403.10910v1'>2403.10910v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15335v1")'>Low-Rank Representations Meets Deep Unfolding: A Generalized and
  Interpretable Network for Hyperspectral Anomaly Detection</div>
<div id='2402.15335v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:15:58Z</div><div>Authors: Chenyu Li, Bing Zhang, Danfeng Hong, Jing Yao, Jocelyn Chanussot</div><div style='padding-top: 10px; width: 80ex'>Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from
low resolution, simple background, and small size of the detection data. These
factors also limit the performance of the well-known low-rank representation
(LRR) models in terms of robustness on the separation of background and target
features and the reliance on manual parameter selection. To this end, we build
a new set of HAD benchmark datasets for improving the robustness of the HAD
algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a
generalized and interpretable HAD network by deeply unfolding a
dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of
spectrally decoupling the background structure and object properties in a more
generalized fashion and eliminating the bias introduced by vital interference
targets concurrently. In addition, LRR-Net$^+$ integrates the solution process
of the Alternating Direction Method of Multipliers (ADMM) optimizer with the
deep network, guiding its search process and imparting a level of
interpretability to parameter optimization. Additionally, the integration of
physical models with DL techniques eliminates the need for manual parameter
tuning. The manually tuned parameters are seamlessly transformed into trainable
parameters for deep neural networks, facilitating a more efficient and
automated optimization process. Extensive experiments conducted on the AIR-HAD
dataset show the superiority of our LRR-Net$^+$ in terms of detection
performance and generalization ability, compared to top-performing rivals.
Furthermore, the compilable codes and our AIR-HAD benchmark datasets in this
paper will be made available freely and openly at
\url{https://sites.google.com/view/danfeng-hong}.</div><div><a href='http://arxiv.org/abs/2402.15335v1'>2402.15335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02592v1")'>Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery</div>
<div id='2401.02592v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T01:17:16Z</div><div>Authors: Zhen Qin, Michael B. Wakin, Zhihui Zhu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we provide the first convergence guarantee for the
factorization approach. Specifically, to avoid the scaling ambiguity and to
facilitate theoretical analysis, we optimize over the so-called left-orthogonal
TT format which enforces orthonormality among most of the factors. To ensure
the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for
optimizing those factors over the Stiefel manifold. We first delve into the TT
factorization problem and establish the local linear convergence of RGD.
Notably, the rate of convergence only experiences a linear decline as the
tensor order increases. We then study the sensing problem that aims to recover
a TT format tensor from linear measurements. Assuming the sensing operator
satisfies the restricted isometry property (RIP), we show that with a proper
initialization, which could be obtained through spectral initialization, RGD
also converges to the ground-truth tensor at a linear rate. Furthermore, we
expand our analysis to encompass scenarios involving Gaussian noise in the
measurements. We prove that RGD can reliably recover the ground truth at a
linear rate, with the recovery error exhibiting only polynomial growth in
relation to the tensor order. We conduct various experiments to validate our
theoretical findings.</div><div><a href='http://arxiv.org/abs/2401.02592v1'>2401.02592v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02890v1")'>Black-Box Approximation and Optimization with Hierarchical Tucker
  Decomposition</div>
<div id='2402.02890v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:59:12Z</div><div>Authors: Gleb Ryzhakov, Andrei Chertkov, Artem Basharin, Ivan Oseledets</div><div style='padding-top: 10px; width: 80ex'>We develop a new method HTBB for the multidimensional black-box approximation
and gradient-free optimization, which is based on the low-rank hierarchical
Tucker decomposition with the use of the MaxVol indices selection procedure.
Numerical experiments for 14 complex model problems demonstrate the robustness
of the proposed method for dimensions up to 1000, while it shows significantly
more accurate results than classical gradient-free optimization methods, as
well as approximation and optimization methods based on the popular tensor
train decomposition, which represents a simpler case of a tensor network.</div><div><a href='http://arxiv.org/abs/2402.02890v1'>2402.02890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01090v1")'>Scalable Higher-Order Tensor Product Spline Models</div>
<div id='2402.01090v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T01:18:48Z</div><div>Authors: David Rügamer</div><div style='padding-top: 10px; width: 80ex'>In the current era of vast data and transparent machine learning, it is
essential for techniques to operate at a large scale while providing a clear
mathematical comprehension of the internal workings of the method. Although
there already exist interpretable semi-parametric regression methods for
large-scale applications that take into account non-linearity in the data, the
complexity of the models is still often limited. One of the main challenges is
the absence of interactions in these models, which are left out for the sake of
better interpretability but also due to impractical computational costs. To
overcome this limitation, we propose a new approach using a factorization
method to derive a highly scalable higher-order tensor product spline model.
Our method allows for the incorporation of all (higher-order) interactions of
non-linear feature effects while having computational costs proportional to a
model without interactions. We further develop a meaningful penalization scheme
and examine the induced optimization problem. We conclude by evaluating the
predictive and estimation performance of our method.</div><div><a href='http://arxiv.org/abs/2402.01090v1'>2402.01090v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02524v2")'>Koopman operators with intrinsic observables in rigged reproducing
  kernel Hilbert spaces</div>
<div id='2403.02524v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T22:28:20Z</div><div>Authors: Isao Ishikawa, Yuka Hashimoto, Masahiro Ikeda, Yoshinobu Kawahara</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel approach for estimating the Koopman operator
defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We
propose an estimation method, what we call Jet Dynamic Mode Decomposition
(JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion
known as jets to enhance the estimation of the Koopman operator. This method
refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy,
especially in the numerical estimation of eigenvalues. This paper proves
JetDMD's superiority through explicit error bounds and convergence rate for
special positive definite kernels, offering a solid theoretical foundation for
its performance. We also delve into the spectral analysis of the Koopman
operator, proposing the notion of extended Koopman operator within a framework
of rigged Hilbert space. This notion leads to a deeper understanding of
estimated Koopman eigenfunctions and capturing them outside the original
function space. Through the theory of rigged Hilbert space, our study provides
a principled methodology to analyze the estimated spectrum and eigenfunctions
of Koopman operators, and enables eigendecomposition within a rigged RKHS. We
also propose a new effective method for reconstructing the dynamical system
from temporally-sampled trajectory data of the dynamical system with solid
theoretical guarantee. We conduct several numerical simulations using the van
der Pol oscillator, the Duffing oscillator, the H\'enon map, and the Lorenz
attractor, and illustrate the performance of JetDMD with clear numerical
computations of eigenvalues and accurate predictions of the dynamical systems.</div><div><a href='http://arxiv.org/abs/2403.02524v2'>2403.02524v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03192v1")'>On the Convergence of Hermitian Dynamic Mode Decomposition</div>
<div id='2401.03192v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T11:13:16Z</div><div>Authors: Nicolas Boullé, Matthew J. Colbrook</div><div style='padding-top: 10px; width: 80ex'>In this work, we study the convergence of Hermitian Dynamic Mode
Decomposition (DMD) to the spectral properties of self-adjoint Koopman
operators. Hermitian DMD is a data-driven method for approximating the Koopman
operator associated with an unknown nonlinear dynamical system from
discrete-time snapshots, while preserving the self-adjointness of the operator
on its finite-dimensional approximations. We show that, under suitable
conditions, the eigenvalues and eigenfunctions of HDMD converge to the spectral
properties of the underlying Koopman operator. Along the way, we establish a
general theorem on the convergence of spectral measures, and demonstrate our
results numerically on the two-dimensional Schr\"odinger equation.</div><div><a href='http://arxiv.org/abs/2401.03192v1'>2401.03192v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03669v2")'>Spectral Algorithms on Manifolds through Diffusion</div>
<div id='2403.03669v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:43:53Z</div><div>Authors: Weichun Xia, Lei Shi</div><div style='padding-top: 10px; width: 80ex'>The existing research on spectral algorithms, applied within a Reproducing
Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions,
often neglecting the inherent structure of the input feature space. Our paper
introduces a new perspective, asserting that input data are situated within a
low-dimensional manifold embedded in a higher-dimensional Euclidean space. We
study the convergence performance of spectral algorithms in the RKHSs,
specifically those generated by the heat kernels, known as diffusion spaces.
Incorporating the manifold structure of the input, we employ integral operator
techniques to derive tight convergence upper bounds concerning generalized
norms, which indicates that the estimators converge to the target function in
strong sense, entailing the simultaneous convergence of the function itself and
its derivatives. These bounds offer two significant advantages: firstly, they
are exclusively contingent on the intrinsic dimension of the input manifolds,
thereby providing a more focused analysis. Secondly, they enable the efficient
derivation of convergence rates for derivatives of any k-th order, all of which
can be accomplished within the ambit of the same spectral algorithms.
Furthermore, we establish minimax lower bounds to demonstrate the asymptotic
optimality of these conclusions in specific contexts. Our study confirms that
the spectral algorithms are practically significant in the broader context of
high-dimensional approximation.</div><div><a href='http://arxiv.org/abs/2403.03669v2'>2403.03669v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13118v1")'>Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process
  Regression</div>
<div id='2403.13118v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:47:02Z</div><div>Authors: Jiwoo Song, Daning Huang</div><div style='padding-top: 10px; width: 80ex'>Modal analysis has become an essential tool to understand the coherent
structure of complex flows. The classical modal analysis methods, such as
dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition
(SPOD), rely on a sufficient amount of data that is regularly sampled in time.
However, often one needs to deal with sparse temporally irregular data, e.g.,
due to experimental measurements and simulation algorithm. To overcome the
limitations of data scarcity and irregular sampling, we propose a novel modal
analysis technique using multi-variate Gaussian process regression (MVGPR). We
first establish the connection between MVGPR and the existing modal analysis
techniques, DMD and SPOD, from a linear system identification perspective.
Next, leveraging this connection, we develop a MVGPR-based modal analysis
technique that addresses the aforementioned limitations. The capability of
MVGPR is endowed by its judiciously designed kernel structure for correlation
function, that is derived from the assumed linear dynamics. Subsequently, the
proposed MVGPR method is benchmarked against DMD and SPOD on a range of
examples, from academic and synthesized data to unsteady airfoil aerodynamics.
The results demonstrate MVGPR as a promising alternative to classical modal
analysis methods, especially in the scenario of scarce and temporally irregular
data.</div><div><a href='http://arxiv.org/abs/2403.13118v1'>2403.13118v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11521v1")'>A Data-driven Approach for Rapid Detection of Aeroelastic Modes from
  Flutter Flight Test Based on Limited Sensor Measurements</div>
<div id='2403.11521v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T07:15:01Z</div><div>Authors: Arpan Das, Pier Marzocca, Giuliano Coppotelli, Oleg Levinski, Paul Taylor</div><div style='padding-top: 10px; width: 80ex'>Flutter flight test involves the evaluation of the airframes aeroelastic
stability by applying artificial excitation on the aircraft lifting surfaces.
The subsequent responses are captured and analyzed to extract the frequencies
and damping characteristics of the system. However, noise contamination,
turbulence, non-optimal excitation of modes, and sensor malfunction in one or
more sensors make it time-consuming and corrupt the extraction process. In
order to expedite the process of identifying and analyzing aeroelastic modes,
this study implements a time-delay embedded Dynamic Mode Decomposition
technique. This approach is complemented by Robust Principal Component Analysis
methodology, and a sparsity promoting criterion which enables the automatic and
optimal selection of sparse modes. The anonymized flutter flight test data,
provided by the fifth author of this research paper, is utilized in this
implementation. The methodology assumes no knowledge of the input excitation,
only deals with the responses captured by accelerometer channels, and rapidly
identifies the aeroelastic modes. By incorporating a compressed sensing
algorithm, the methodology gains the ability to identify aeroelastic modes,
even when the number of available sensors is limited. This augmentation greatly
enhances the methodology's robustness and effectiveness, making it an excellent
choice for real-time implementation during flutter test campaigns.</div><div><a href='http://arxiv.org/abs/2403.11521v1'>2403.11521v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.00691v2")'>Stochastic Gradient Descent for Additive Nonparametric Regression</div>
<div id='2401.00691v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T08:03:52Z</div><div>Authors: Xin Chen, Jason M. Klusowski</div><div style='padding-top: 10px; width: 80ex'>This paper introduces an iterative algorithm for training additive models
that enjoys favorable memory storage and computational requirements. The
algorithm can be viewed as the functional counterpart of stochastic gradient
descent, applied to the coefficients of a truncated basis expansion of the
component functions. We show that the resulting estimator satisfies an oracle
inequality that allows for model mis-specification. In the well-specified
setting, by choosing the learning rate carefully across three distinct stages
of training, we demonstrate that its risk is minimax optimal in terms of the
dependence on the dimensionality of the data and the size of the training
sample. We further illustrate the computational benefits by comparing the
approach with traditional backfitting on two real-world datasets.</div><div><a href='http://arxiv.org/abs/2401.00691v2'>2401.00691v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12272v1")'>Transfer Learning for Nonparametric Regression: Non-asymptotic Minimax
  Analysis and Adaptive Procedure</div>
<div id='2401.12272v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T16:24:04Z</div><div>Authors: T. Tony Cai, Hongming Pu</div><div style='padding-top: 10px; width: 80ex'>Transfer learning for nonparametric regression is considered. We first study
the non-asymptotic minimax risk for this problem and develop a novel estimator
called the confidence thresholding estimator, which is shown to achieve the
minimax optimal risk up to a logarithmic factor. Our results demonstrate two
unique phenomena in transfer learning: auto-smoothing and super-acceleration,
which differentiate it from nonparametric regression in a traditional setting.
We then propose a data-driven algorithm that adaptively achieves the minimax
risk up to a logarithmic factor across a wide range of parameter spaces.
Simulation studies are conducted to evaluate the numerical performance of the
adaptive transfer learning algorithm, and a real-world example is provided to
demonstrate the benefits of the proposed method.</div><div><a href='http://arxiv.org/abs/2401.12272v1'>2401.12272v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14966v1")'>Smoothness Adaptive Hypothesis Transfer Learning</div>
<div id='2402.14966v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T21:02:19Z</div><div>Authors: Haotian Lin, Matthew Reimherr</div><div style='padding-top: 10px; width: 80ex'>Many existing two-phase kernel-based hypothesis transfer learning algorithms
employ the same kernel regularization across phases and rely on the known
smoothness of functions to obtain optimality. Therefore, they fail to adapt to
the varying and unknown smoothness between the target/source and their offset
in practice. In this paper, we address these problems by proposing Smoothness
Adaptive Transfer Learning (SATL), a two-phase kernel ridge
regression(KRR)-based algorithm. We first prove that employing the misspecified
fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax
optimality and derive an adaptive procedure to the unknown Sobolev smoothness.
Leveraging these results, SATL employs Gaussian kernels in both phases so that
the estimators can adapt to the unknown smoothness of the target/source and
their offset function. We derive the minimax lower bound of the learning
problem in excess risk and show that SATL enjoys a matching upper bound up to a
logarithmic factor. The minimax convergence rate sheds light on the factors
influencing transfer dynamics and demonstrates the superiority of SATL compared
to non-transfer learning settings. While our main objective is a theoretical
analysis, we also conduct several experiments to confirm our results.</div><div><a href='http://arxiv.org/abs/2402.14966v1'>2402.14966v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13565v1")'>AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for
  High-dimensional Regression</div>
<div id='2403.13565v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T12:58:46Z</div><div>Authors: Zelin He, Ying Sun, Jingyuan Liu, Runze Li</div><div style='padding-top: 10px; width: 80ex'>We consider the transfer learning problem in the high dimensional setting,
where the feature dimension is larger than the sample size. To learn
transferable information, which may vary across features or the source samples,
we propose an adaptive transfer learning method that can detect and aggregate
the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable
structures. We achieve this by employing a novel fused-penalty, coupled with
weights that can adapt according to the transferable structure. To choose the
weight, we propose a theoretically informed, data-driven procedure, enabling
F-AdaTrans to selectively fuse the transferable signals with the target while
filtering out non-transferable signals, and S-AdaTrans to obtain the optimal
combination of information transferred from each source sample. The
non-asymptotic rates are established, which recover existing near-minimax
optimal rates in special cases. The effectiveness of the proposed method is
validated using both synthetic and real data.</div><div><a href='http://arxiv.org/abs/2403.13565v1'>2403.13565v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16410v1")'>ReTaSA: A Nonparametric Functional Estimation Approach for Addressing
  Continuous Target Shift</div>
<div id='2401.16410v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:47:36Z</div><div>Authors: Hwanwoo Kim, Xin Zhang, Jiwei Zhao, Qinglong Tian</div><div style='padding-top: 10px; width: 80ex'>The presence of distribution shifts poses a significant challenge for
deploying modern machine learning models in real-world applications. This work
focuses on the target shift problem in a regression setting (Zhang et al.,
2013; Nguyen et al., 2016). More specifically, the target variable y (also
known as the response variable), which is continuous, has different marginal
distributions in the training source and testing domain, while the conditional
distribution of features x given y remains the same. While most literature
focuses on classification tasks with finite target space, the regression
problem has an infinite dimensional target space, which makes many of the
existing methods inapplicable. In this work, we show that the continuous target
shift problem can be addressed by estimating the importance weight function
from an ill-posed integral equation. We propose a nonparametric regularized
approach named ReTaSA to solve the ill-posed integral equation and provide
theoretical justification for the estimated importance weight function. The
effectiveness of the proposed method has been demonstrated with extensive
numerical studies on synthetic and real-world datasets.</div><div><a href='http://arxiv.org/abs/2401.16410v1'>2401.16410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07442v1")'>Proxy Methods for Domain Adaptation</div>
<div id='2403.07442v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T09:32:41Z</div><div>Authors: Katherine Tsai, Stephen R. Pfohl, Olawale Salaudeen, Nicole Chiou, Matt J. Kusner, Alexander D'Amour, Sanmi Koyejo, Arthur Gretton</div><div style='padding-top: 10px; width: 80ex'>We study the problem of domain adaptation under distribution shift, where the
shift is due to a change in the distribution of an unobserved, latent variable
that confounds both the covariates and the labels. In this setting, neither the
covariate shift nor the label shift assumptions apply. Our approach to
adaptation employs proximal causal learning, a technique for estimating causal
effects in settings where proxies of unobserved confounders are available. We
demonstrate that proxy variables allow for adaptation to distribution shift
without explicitly recovering or modeling latent variables. We consider two
settings, (i) Concept Bottleneck: an additional ''concept'' variable is
observed that mediates the relationship between the covariates and labels; (ii)
Multi-domain: training data from multiple source domains is available, where
each source domain exhibits a different distribution over the latent
confounder. We develop a two-stage kernel estimation approach to adapt to
complex distribution shifts in both settings. In our experiments, we show that
our approach outperforms other methods, notably those which explicitly recover
the latent confounder.</div><div><a href='http://arxiv.org/abs/2403.07442v1'>2403.07442v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02432v1")'>On the impact of measure pre-conditionings on general parametric ML
  models and transfer learning via domain adaptation</div>
<div id='2403.02432v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:26:39Z</div><div>Authors: Joaquín Sánchez García</div><div style='padding-top: 10px; width: 80ex'>We study a new technique for understanding convergence of learning agents
under small modifications of data. We show that such convergence can be
understood via an analogue of Fatou's lemma which yields gamma-convergence. We
show it's relevance and applications in general machine learning tasks and
domain adaptation transfer learning.</div><div><a href='http://arxiv.org/abs/2403.02432v1'>2403.02432v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11682v1")'>Learning Conditional Invariances through Non-Commutativity</div>
<div id='2402.11682v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T19:12:18Z</div><div>Authors: Abhra Chaudhuri, Serban Georgescu, Anjan Dutta</div><div style='padding-top: 10px; width: 80ex'>Invariance learning algorithms that conditionally filter out domain-specific
random variables as distractors, do so based only on the data semantics, and
not the target domain under evaluation. We show that a provably optimal and
sample-efficient way of learning conditional invariances is by relaxing the
invariance criterion to be non-commutatively directed towards the target
domain. Under domain asymmetry, i.e., when the target domain contains
semantically relevant information absent in the source, the risk of the encoder
$\varphi^*$ that is optimal on average across domains is strictly lower-bounded
by the risk of the target-specific optimal encoder $\Phi^*_\tau$. We prove that
non-commutativity steers the optimization towards $\Phi^*_\tau$ instead of
$\varphi^*$, bringing the $\mathcal{H}$-divergence between domains down to
zero, leading to a stricter bound on the target risk. Both our theory and
experiments demonstrate that non-commutative invariance (NCI) can leverage
source domain samples to meet the sample complexity needs of learning
$\Phi^*_\tau$, surpassing SOTA invariance learning algorithms for domain
adaptation, at times by over $2\%$, approaching the performance of an oracle.
Implementation is available at https://github.com/abhrac/nci.</div><div><a href='http://arxiv.org/abs/2402.11682v1'>2402.11682v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01420v1")'>The Implicit Bias of Heterogeneity towards Invariance and Causality</div>
<div id='2403.01420v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T07:38:24Z</div><div>Authors: Yang Xu, Yihong Gu, Cong Fang</div><div style='padding-top: 10px; width: 80ex'>It is observed empirically that the large language models (LLM), trained with
a variant of regression loss using numerous corpus from the Internet, can
unveil causal associations to some extent. This is contrary to the traditional
wisdom that ``association is not causation'' and the paradigm of traditional
causal inference in which prior causal knowledge should be carefully
incorporated into the design of methods. It is a mystery why causality, in a
higher layer of understanding, can emerge from the regression task that pursues
associations. In this paper, we claim the emergence of causality from
association-oriented training can be attributed to the coupling effects from
the heterogeneity of the source data, stochasticity of training algorithms, and
over-parameterization of the learning models. We illustrate such an intuition
using a simple but insightful model that learns invariance, a quasi-causality,
using regression loss. To be specific, we consider multi-environment low-rank
matrix sensing problems where the unknown r-rank ground-truth d*d matrices
diverge across the environments but contain a lower-rank invariant, causal
part. In this case, running pooled gradient descent will result in biased
solutions that only learn associations in general. We show that running
large-batch Stochastic Gradient Descent, whose each batch being linear
measurement samples randomly selected from a certain environment, can
successfully drive the solution towards the invariant, causal solution under
certain conditions. This step is related to the relatively strong heterogeneity
of the environments, the large step size and noises in the optimization
algorithm, and the over-parameterization of the model. In summary, we unveil
another implicit bias that is a result of the symbiosis between the
heterogeneity of data and modern algorithms, which is, to the best of our
knowledge, first in the literature.</div><div><a href='http://arxiv.org/abs/2403.01420v1'>2403.01420v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.14426v1")'>M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment
  Network for Uplift Modeling</div>
<div id='2401.14426v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T08:10:36Z</div><div>Authors: Zexu Sun, Xu Chen</div><div style='padding-top: 10px; width: 80ex'>Uplift modeling is a technique used to predict the effect of a treatment
(e.g., discounts) on an individual's response. Although several methods have
been proposed for multi-valued treatment, they are extended from binary
treatment methods. There are still some limitations. Firstly, existing methods
calculate uplift based on predicted responses, which may not guarantee a
consistent uplift distribution between treatment and control groups. Moreover,
this may cause cumulative errors for multi-valued treatment. Secondly, the
model parameters become numerous with many prediction heads, leading to reduced
efficiency. To address these issues, we propose a novel \underline{M}ulti-gate
\underline{M}ixture-of-Experts based \underline{M}ulti-valued
\underline{T}reatment \underline{N}etwork (M$^3$TN). M$^3$TN consists of two
components: 1) a feature representation module with Multi-gate
Mixture-of-Experts to improve the efficiency; 2) a reparameterization module by
modeling uplift explicitly to improve the effectiveness. We also conduct
extensive experiments to demonstrate the effectiveness and efficiency of our
M$^3$TN.</div><div><a href='http://arxiv.org/abs/2401.14426v1'>2401.14426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00019v1")'>Transformer-based Parameter Estimation in Statistics</div>
<div id='2403.00019v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T04:30:41Z</div><div>Authors: Xiaoxin Yin, David S. Yin</div><div style='padding-top: 10px; width: 80ex'>Parameter estimation is one of the most important tasks in statistics, and is
key to helping people understand the distribution behind a sample of
observations. Traditionally parameter estimation is done either by closed-form
solutions (e.g., maximum likelihood estimation for Gaussian distribution), or
by iterative numerical methods such as Newton-Raphson method when closed-form
solution does not exist (e.g., for Beta distribution).
  In this paper we propose a transformer-based approach to parameter
estimation. Compared with existing solutions, our approach does not require a
closed-form solution or any mathematical derivations. It does not even require
knowing the probability density function, which is needed by numerical methods.
After the transformer model is trained, only a single inference is needed to
estimate the parameters of the underlying distribution based on a sample of
observations. In the empirical study we compared our approach with maximum
likelihood estimation on commonly used distributions such as normal
distribution, exponential distribution and beta distribution. It is shown that
our approach achieves similar or better accuracy as measured by
mean-square-errors.</div><div><a href='http://arxiv.org/abs/2403.00019v1'>2403.00019v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13079v1")'>Mode Estimation with Partial Feedback</div>
<div id='2402.13079v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:24:21Z</div><div>Authors: Charles Arnal, Vivien Cabannes, Vianney Perchet</div><div style='padding-top: 10px; width: 80ex'>The combination of lightly supervised pre-training and online fine-tuning has
played a key role in recent AI developments. These new learning pipelines call
for new theoretical frameworks. In this paper, we formalize core aspects of
weakly supervised and active learning with a simple problem: the estimation of
the mode of a distribution using partial feedback. We show how entropy coding
allows for optimal information acquisition from partial feedback, develop
coarse sufficient statistics for mode identification, and adapt bandit
algorithms to our new setting. Finally, we combine those contributions into a
statistically and computationally efficient solution to our problem.</div><div><a href='http://arxiv.org/abs/2402.13079v1'>2402.13079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02952v1")'>On Least Squares Estimation in Softmax Gating Mixture of Experts</div>
<div id='2402.02952v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:31:18Z</div><div>Authors: Huy Nguyen, Nhat Ho, Alessandro Rinaldo</div><div style='padding-top: 10px; width: 80ex'>Mixture of experts (MoE) model is a statistical machine learning design that
aggregates multiple expert networks using a softmax gating function in order to
form a more intricate and expressive model. Despite being commonly used in
several applications owing to their scalability, the mathematical and
statistical properties of MoE models are complex and difficult to analyze. As a
result, previous theoretical works have primarily focused on probabilistic MoE
models by imposing the impractical assumption that the data are generated from
a Gaussian MoE model. In this work, we investigate the performance of the least
squares estimators (LSE) under a deterministic MoE model where the data are
sampled according to a regression model, a setting that has remained largely
unexplored. We establish a condition called strong identifiability to
characterize the convergence behavior of various types of expert functions. We
demonstrate that the rates for estimating strongly identifiable experts, namely
the widely used feed forward networks with activation functions
$\mathrm{sigmoid}(\cdot)$ and $\tanh(\cdot)$, are substantially faster than
those of polynomial experts, which we show to exhibit a surprising slow
estimation rate. Our findings have important practical implications for expert
selection.</div><div><a href='http://arxiv.org/abs/2402.02952v1'>2402.02952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03226v1")'>FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion</div>
<div id='2402.03226v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:37:46Z</div><div>Authors: Xing Han, Huy Nguyen, Carl Harris, Nhat Ho, Suchi Saria</div><div style='padding-top: 10px; width: 80ex'>As machine learning models in critical fields increasingly grapple with
multimodal data, they face the dual challenges of handling a wide array of
modalities, often incomplete due to missing elements, and the temporal
irregularity and sparsity of collected samples. Successfully leveraging this
complex data, while overcoming the scarcity of high-quality training samples,
is key to improving these models' predictive performance. We introduce
``FuseMoE'', a mixture-of-experts framework incorporated with an innovative
gating function. Designed to integrate a diverse number of modalities, FuseMoE
is effective in managing scenarios with missing modalities and irregularly
sampled data trajectories. Theoretically, our unique gating function
contributes to enhanced convergence rates, leading to better performance in
multiple downstream tasks. The practical utility of FuseMoE in real world is
validated by a challenging set of clinical risk prediction tasks.</div><div><a href='http://arxiv.org/abs/2402.03226v1'>2402.03226v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08088v1")'>Out-of-Distribution Detection and Data Drift Monitoring using
  Statistical Process Control</div>
<div id='2402.08088v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:10:06Z</div><div>Authors: Ghada Zamzmi, Kesavan Venkatesh, Brandon Nelson, Smriti Prathapan, Paul H. Yi, Berkman Sahiner, Jana G. Delfino</div><div style='padding-top: 10px; width: 80ex'>Background: Machine learning (ML) methods often fail with data that deviates
from their training distribution. This is a significant concern for ML-enabled
devices in clinical settings, where data drift may cause unexpected performance
that jeopardizes patient safety.
  Method: We propose a ML-enabled Statistical Process Control (SPC) framework
for out-of-distribution (OOD) detection and drift monitoring. SPC is
advantageous as it visually and statistically highlights deviations from the
expected distribution. To demonstrate the utility of the proposed framework for
monitoring data drift in radiological images, we investigated different design
choices, including methods for extracting feature representations, drift
quantification, and SPC parameter selection.
  Results: We demonstrate the effectiveness of our framework for two tasks: 1)
differentiating axial vs. non-axial computed tomography (CT) images and 2)
separating chest x-ray (CXR) from other modalities. For both tasks, we achieved
high accuracy in detecting OOD inputs, with 0.913 in CT and 0.995 in CXR, and
sensitivity of 0.980 in CT and 0.984 in CXR. Our framework was also adept at
monitoring data streams and identifying the time a drift occurred. In a
simulation with 100 daily CXR cases, we detected a drift in OOD input
percentage from 0-1% to 3-5% within two days, maintaining a low false-positive
rate. Through additional experimental results, we demonstrate the framework's
data-agnostic nature and independence from the underlying model's structure.
  Conclusion: We propose a framework for OOD detection and drift monitoring
that is agnostic to data, modality, and model. The framework is customizable
and can be adapted for specific applications.</div><div><a href='http://arxiv.org/abs/2402.08088v1'>2402.08088v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11858v1")'>Stochastic Hessian Fitting on Lie Group</div>
<div id='2402.11858v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T06:00:35Z</div><div>Authors: Xi-Lin Li</div><div style='padding-top: 10px; width: 80ex'>This paper studies the fitting of Hessian or its inverse with stochastic
Hessian-vector products. A Hessian fitting criterion, which can be used to
derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad,
etc., is used for the analysis. Our studies reveal different convergence rates
for different Hessian fitting methods, e.g., sublinear rates for gradient
descent in the Euclidean space and a commonly used closed-form solution, linear
rates for gradient descent on the manifold of symmetric positive definite (SPL)
matrices and certain Lie groups. The Hessian fitting problem is further shown
to be strongly convex under mild conditions on a specific yet general enough
Lie group. To confirm our analysis, these methods are tested under different
settings like noisy Hessian-vector products, time varying Hessians, and low
precision arithmetic. These findings are useful for stochastic second order
optimizations that rely on fast, robust and accurate Hessian estimations.</div><div><a href='http://arxiv.org/abs/2402.11858v1'>2402.11858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09081v1")'>Low-Rank Extragradient Methods for Scalable Semidefinite Optimization</div>
<div id='2402.09081v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T10:48:00Z</div><div>Authors: Dan Garber. Atara Kaplan</div><div style='padding-top: 10px; width: 80ex'>We consider several classes of highly important semidefinite optimization
problems that involve both a convex objective function (smooth or nonsmooth)
and additional linear or nonlinear smooth and convex constraints, which are
ubiquitous in statistics, machine learning, combinatorial optimization, and
other domains. We focus on high-dimensional and plausible settings in which the
problem admits a low-rank solution which also satisfies a low-rank
complementarity condition. We provide several theoretical results proving that,
under these circumstances, the well-known Extragradient method, when
initialized in the proximity of an optimal primal-dual solution, converges to a
solution of the constrained optimization problem with its standard convergence
rates guarantees, using only low-rank singular value decompositions (SVD) to
project onto the positive semidefinite cone, as opposed to
computationally-prohibitive full-rank SVDs required in worst-case. Our approach
is supported by numerical experiments conducted with a dataset of Max-Cut
instances.</div><div><a href='http://arxiv.org/abs/2402.09081v1'>2402.09081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12859v1")'>Primal Methods for Variational Inequality Problems with Functional
  Constraints</div>
<div id='2403.12859v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T16:03:03Z</div><div>Authors: Liang Zhang, Niao He, Michael Muehlebach</div><div style='padding-top: 10px; width: 80ex'>Constrained variational inequality problems are recognized for their broad
applications across various fields including machine learning and operations
research. First-order methods have emerged as the standard approach for solving
these problems due to their simplicity and scalability. However, they typically
rely on projection or linear minimization oracles to navigate the feasible set,
which becomes computationally expensive in practical scenarios featuring
multiple functional constraints. Existing efforts to tackle such functional
constrained variational inequality problems have centered on primal-dual
algorithms grounded in the Lagrangian function. These algorithms along with
their theoretical analysis often require the existence and prior knowledge of
the optimal Lagrange multipliers. In this work, we propose a simple primal
method, termed Constrained Gradient Method (CGM), for addressing functional
constrained variational inequality problems, without necessitating any
information on the optimal Lagrange multipliers. We establish a non-asymptotic
convergence analysis of the algorithm for variational inequality problems with
monotone operators under smooth constraints. Remarkably, our algorithms match
the complexity of projection-based methods in terms of operator queries for
both monotone and strongly monotone settings, while utilizing significantly
cheaper oracles based on quadratic programming. Furthermore, we provide several
numerical examples to evaluate the efficacy of our algorithms.</div><div><a href='http://arxiv.org/abs/2403.12859v1'>2403.12859v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03352v1")'>Zeroth-Order primal-dual Alternating Projection Gradient Algorithms for
  Nonconvex Minimax Problems with Coupled linear Constraints</div>
<div id='2402.03352v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T11:22:13Z</div><div>Authors: Huiling Zhang, Zi Xu, Yuhong Dai</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study zeroth-order algorithms for nonconvex minimax
problems with coupled linear constraints under the deterministic and stochastic
settings, which have attracted wide attention in machine learning, signal
processing and many other fields in recent years, e.g., adversarial attacks in
resource allocation problems and network flow problems etc. We propose two
single-loop algorithms, namely the zero-order primal-dual alternating projected
gradient (ZO-PDAPG) algorithm and the zero-order regularized momentum
primal-dual projected gradient algorithm (ZO-RMPDPG), for solving deterministic
and stochastic nonconvex-(strongly) concave minimax problems with coupled
linear constraints. The iteration complexity of the two proposed algorithms to
obtain an $\varepsilon$-stationary point are proved to be
$\mathcal{O}(\varepsilon ^{-2})$ (resp. $\mathcal{O}(\varepsilon ^{-4})$) for
solving nonconvex-strongly concave (resp. nonconvex-concave) minimax problems
with coupled linear constraints under deterministic settings and
$\tilde{\mathcal{O}}(\varepsilon ^{-3})$ (resp.
$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$) under stochastic settings
respectively. To the best of our knowledge, they are the first two zeroth-order
algorithms with iterative complexity guarantees for solving
nonconvex-(strongly) concave minimax problems with coupled linear constraints
under the deterministic and stochastic settings.</div><div><a href='http://arxiv.org/abs/2402.03352v1'>2402.03352v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16164v1")'>Constrained Bi-Level Optimization: Proximal Lagrangian Value function
  Approach and Hessian-free Algorithm</div>
<div id='2401.16164v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T13:50:56Z</div><div>Authors: Wei Yao, Chengming Yu, Shangzhi Zeng, Jin Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper presents a new approach and algorithm for solving a class of
constrained Bi-Level Optimization (BLO) problems in which the lower-level
problem involves constraints coupling both upper-level and lower-level
variables. Such problems have recently gained significant attention due to
their broad applicability in machine learning. However, conventional
gradient-based methods unavoidably rely on computationally intensive
calculations related to the Hessian matrix. To address this challenge, we begin
by devising a smooth proximal Lagrangian value function to handle the
constrained lower-level problem. Utilizing this construct, we introduce a
single-level reformulation for constrained BLOs that transforms the original
BLO problem into an equivalent optimization problem with smooth constraints.
Enabled by this reformulation, we develop a Hessian-free gradient-based
algorithm-termed proximal Lagrangian Value function-based Hessian-free Bi-level
Algorithm (LV-HBA)-that is straightforward to implement in a single loop
manner. Consequently, LV-HBA is especially well-suited for machine learning
applications. Furthermore, we offer non-asymptotic convergence analysis for
LV-HBA, eliminating the need for traditional strong convexity assumptions for
the lower-level problem while also being capable of accommodating non-singleton
scenarios. Empirical results substantiate the algorithm's superior practical
performance.</div><div><a href='http://arxiv.org/abs/2401.16164v1'>2401.16164v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09257v1")'>A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level
  Optimization</div>
<div id='2401.09257v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T15:03:37Z</div><div>Authors: Feiyang Ye, Baijiong Lin, Xiaofeng Cao, Yu Zhang, Ivor Tsang</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO)
problem, where the upper-level subproblem is a multi-objective optimization
problem and the lower-level subproblem is for scalar optimization. Existing
gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the
computational inefficient problem. To address this, we propose an efficient
first-order multi-gradient method for MOBLO, called FORUM. Specifically, we
reformulate MOBLO problems as a constrained multi-objective optimization (MOO)
problem via the value-function approach. Then we propose a novel multi-gradient
aggregation method to solve the challenging constrained MOO problem.
Theoretically, we provide the complexity analysis to show the efficiency of the
proposed method and a non-asymptotic convergence result. Empirically, extensive
experiments demonstrate the effectiveness and efficiency of the proposed FORUM
method in different learning problems. In particular, it achieves
state-of-the-art performance on three multi-task learning benchmark datasets.</div><div><a href='http://arxiv.org/abs/2401.09257v1'>2401.09257v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19078v2")'>Smooth Tchebycheff Scalarization for Multi-Objective Optimization</div>
<div id='2402.19078v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T12:03:05Z</div><div>Authors: Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, Qingfu Zhang</div><div style='padding-top: 10px; width: 80ex'>Multi-objective optimization problems can be found in many real-world
applications, where the objectives often conflict each other and cannot be
optimized by a single solution. In the past few decades, numerous methods have
been proposed to find Pareto solutions that represent different optimal
trade-offs among the objectives for a given problem. However, these existing
methods could have high computational complexity or may not have good
theoretical properties for solving a general differentiable multi-objective
optimization problem. In this work, by leveraging the smooth optimization
technique, we propose a novel and lightweight smooth Tchebycheff scalarization
approach for gradient-based multi-objective optimization. It has good
theoretical properties for finding all Pareto solutions with valid trade-off
preferences, while enjoying significantly lower computational complexity
compared to other methods. Experimental results on various real-world
application problems fully demonstrate the effectiveness of our proposed
method.</div><div><a href='http://arxiv.org/abs/2402.19078v2'>2402.19078v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03086v1")'>Dual Lagrangian Learning for Conic Optimization</div>
<div id='2402.03086v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:14:08Z</div><div>Authors: Mathieu Tanneau, Pascal Van Hentenryck</div><div style='padding-top: 10px; width: 80ex'>This paper presents Dual Lagrangian Learning (DLL), a principled learning
methodology that combines conic duality theory with the representation power of
ML models. DLL leverages conic duality to provide dual-feasible solutions, and
therefore valid Lagrangian dual bounds, for parametric linear and nonlinear
conic optimization problems. The paper introduces differentiable conic
projection layers, a systematic dual completion procedure, and a
self-supervised learning framework. The effectiveness of DLL is demonstrated on
linear and nonlinear parametric optimization problems for which DLL provides
valid dual bounds within 0.5% of optimality.</div><div><a href='http://arxiv.org/abs/2402.03086v1'>2402.03086v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03454v2")'>Learning Constrained Optimization with Deep Augmented Lagrangian Methods</div>
<div id='2403.03454v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T04:43:22Z</div><div>Authors: James Kotary, Ferdinando Fioretto</div><div style='padding-top: 10px; width: 80ex'>Learning to Optimize (LtO) is a problem setting in which a machine learning
(ML) model is trained to emulate a constrained optimization solver. Learning to
produce optimal and feasible solutions subject to complex constraints is a
difficult task, but is often made possible by restricting the input space to a
limited distribution of related problems. Most LtO methods focus on directly
learning solutions to the primal problem, and applying correction schemes or
loss function penalties to encourage feasibility. This paper proposes an
alternative approach, in which the ML model is trained instead to predict dual
solution estimates directly, from which primal estimates are constructed to
form dual-feasible solution pairs. This enables an end-to-end training scheme
is which the dual objective is maximized as a loss function, and solution
estimates iterate toward primal feasibility, emulating a Dual Ascent method.
First it is shown that the poor convergence properties of classical Dual Ascent
are reflected in poor convergence of the proposed training scheme. Then, by
incorporating techniques from practical Augmented Lagrangian methods, we show
how the training scheme can be improved to learn highly accurate constrained
optimization solvers, for both convex and nonconvex problems.</div><div><a href='http://arxiv.org/abs/2403.03454v2'>2403.03454v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01306v2")'>Learning solutions to some toy constrained optimization problems in
  infinite dimensional Hilbert spaces</div>
<div id='2401.01306v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T17:32:53Z</div><div>Authors: Pinak Mandal</div><div style='padding-top: 10px; width: 80ex'>In this work we present deep learning implementations of two popular
theoretical constrained optimization algorithms in infinite dimensional Hilbert
spaces, namely, the penalty and the augmented Lagrangian methods. We test these
algorithms on some toy problems originating in either calculus of variations or
physics. We demonstrate that both methods are able to produce decent
approximations for the test problems and are comparable in terms of different
errors produced. Leveraging the common occurrence of the Lagrange multiplier
update rule being computationally less expensive than solving subproblems in
the penalty method, we achieve significant speedups in cases when the output of
the constraint function is itself a function.</div><div><a href='http://arxiv.org/abs/2401.01306v2'>2401.01306v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03451v2")'>Optimization Over Trained Neural Networks: Taking a Relaxing Walk</div>
<div id='2401.03451v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T11:15:00Z</div><div>Authors: Jiatai Tong, Junyang Cai, Thiago Serra</div><div style='padding-top: 10px; width: 80ex'>Besides training, mathematical optimization is also used in deep learning to
model and solve formulations over trained neural networks for purposes such as
verification, compression, and optimization with learned constraints. However,
solving these formulations soon becomes difficult as the network size grows due
to the weak linear relaxation and dense constraint matrix. We have seen
improvements in recent years with cutting plane algorithms, reformulations, and
an heuristic based on Mixed-Integer Linear Programming (MILP). In this work, we
propose a more scalable heuristic based on exploring global and local linear
relaxations of the neural network model. Our heuristic is competitive with a
state-of-the-art MILP solver and the prior heuristic while producing better
solutions with increases in input, depth, and number of neurons.</div><div><a href='http://arxiv.org/abs/2401.03451v2'>2401.03451v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14033v1")'>Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted
  Activations</div>
<div id='2401.14033v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T09:23:31Z</div><div>Authors: Patricia Pauli, Aaron Havens, Alexandre Araujo, Siddharth Garg, Farshad Khorrami, Frank Allgöwer, Bin Hu</div><div style='padding-top: 10px; width: 80ex'>Recently, semidefinite programming (SDP) techniques have shown great promise
in providing accurate Lipschitz bounds for neural networks. Specifically, the
LipSDP approach (Fazlyab et al., 2019) has received much attention and provides
the least conservative Lipschitz upper bounds that can be computed with
polynomial time guarantees. However, one main restriction of LipSDP is that its
formulation requires the activation functions to be slope-restricted on
$[0,1]$, preventing its further use for more general activation functions such
as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for
example as residual ReLU networks. However, a direct application of LipSDP to
the resultant residual ReLU networks is conservative and even fails in
recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our
paper bridges this gap and extends LipSDP beyond slope-restricted activation
functions. To this end, we provide novel quadratic constraints for GroupSort,
MaxMin, and Householder activations via leveraging their underlying properties
such as sum preservation. Our proposed analysis is general and provides a
unified approach for estimating $\ell_2$ and $\ell_\infty$ Lipschitz bounds for
a rich class of neural network architectures, including non-residual and
residual neural networks and implicit models, with GroupSort, MaxMin, and
Householder activations. Finally, we illustrate the utility of our approach
with a variety of experiments and show that our proposed SDPs generate less
conservative Lipschitz bounds in comparison to existing approaches.</div><div><a href='http://arxiv.org/abs/2401.14033v1'>2401.14033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07094v1")'>FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning</div>
<div id='2403.07094v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T18:40:47Z</div><div>Authors: Xiang Meng, Wenyu Chen, Riade Benbaki, Rahul Mazumder</div><div style='padding-top: 10px; width: 80ex'>The increasing computational demands of modern neural networks present
deployment challenges on resource-constrained devices. Network pruning offers a
solution to reduce model size and computational cost while maintaining
performance. However, most current pruning methods focus primarily on improving
sparsity by reducing the number of nonzero parameters, often neglecting other
deployment costs such as inference time, which are closely related to the
number of floating-point operations (FLOPs). In this paper, we propose FALCON,
a novel combinatorial-optimization-based framework for network pruning that
jointly takes into account model accuracy (fidelity), FLOPs, and sparsity
constraints. A main building block of our approach is an integer linear program
(ILP) that simultaneously handles FLOP and sparsity constraints. We present a
novel algorithm to approximately solve the ILP. We propose a novel first-order
method for our optimization framework which makes use of our ILP solver. Using
problem structure (e.g., the low-rank structure of approx. Hessian), we can
address instances with millions of parameters. Our experiments demonstrate that
FALCON achieves superior accuracy compared to other pruning approaches within a
fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs
retained, our approach improves the accuracy by 48% relative to
state-of-the-art. Furthermore, in gradual pruning settings with re-training
between pruning steps, our framework outperforms existing pruning methods,
emphasizing the significance of incorporating both FLOP and sparsity
constraints for effective network pruning.</div><div><a href='http://arxiv.org/abs/2403.07094v1'>2403.07094v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07339v1")'>IM-Unpack: Training and Inference with Arbitrarily Low Precision
  Integers</div>
<div id='2403.07339v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T05:44:27Z</div><div>Authors: Zhanpeng Zeng, Karthikeyan Sankaralingam, Vikas Singh</div><div style='padding-top: 10px; width: 80ex'>GEneral Matrix Multiply (GEMM) is a central operation in deep learning and
corresponds to the largest chunk of the compute footprint. Therefore, improving
its efficiency is an active topic of ongoing research. A popular strategy is
the use of low bit-width integers to approximate the original entries in a
matrix. This allows efficiency gains, but often requires sophisticated
techniques to control the rounding error incurred. In this work, we first
verify/check that when the low bit-width restriction is removed, for a variety
of Transformer-based models, whether integers are sufficient for all GEMMs need
-- for {\em both} training and inference stages, and can achieve parity with
floating point counterparts. No sophisticated techniques are needed. We find
that while a large majority of entries in matrices (encountered in such models)
can be easily represented by {\em low} bit-width integers, the existence of a
few heavy hitter entries make it difficult to achieve efficiency gains via the
exclusive use of low bit-width GEMMs alone. To address this issue, we develop a
simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\em unpack} a
matrix with large integer entries into a larger matrix whose entries all lie
within the representable range of arbitrarily low bit-width integers. This
allows {\em equivalence} with the original GEMM, i.e., the exact result can be
obtained using purely low bit-width integer GEMMs. This comes at the cost of
additional operations -- we show that for many popular models, this overhead is
quite small.</div><div><a href='http://arxiv.org/abs/2403.07339v1'>2403.07339v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03777v1")'>ENOT: Expectile Regularization for Fast and Accurate Training of Neural
  Optimal Transport</div>
<div id='2403.03777v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:15:42Z</div><div>Authors: Nazar Buzun, Maksim Bobrin, Dmitry V. Dylov</div><div style='padding-top: 10px; width: 80ex'>We present a new extension for Neural Optimal Transport (NOT) training
procedure, capable of accurately and efficiently estimating optimal
transportation plan via specific regularisation on conjugate potentials. The
main bottleneck of existing NOT solvers is associated with the procedure of
finding a near-exact approximation of the conjugate operator (i.e., the
c-transform), which is done either by optimizing over maximin objectives or by
the computationally-intensive fine-tuning of the initial approximated
prediction. We resolve both issues by proposing a new, theoretically justified
loss in the form of expectile regularization that enforces binding conditions
on the learning dual potentials. Such a regularization provides the upper bound
estimation over the distribution of possible conjugate potentials and makes the
learning stable, eliminating the need for additional extensive finetuning. We
formally justify the efficiency of our method, called Expectile-Regularised
Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art
approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a
3-fold improvement in quality and up to a 10-fold improvement in runtime).</div><div><a href='http://arxiv.org/abs/2403.03777v1'>2403.03777v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14991v1")'>Quantum Theory and Application of Contextual Optimal Transport</div>
<div id='2402.14991v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T22:03:16Z</div><div>Authors: Nicola Mariella, Albert Akhriev, Francesco Tacchino, Christa Zoufal, Juan Carlos Gonzalez-Espitia, Benedek Harsanyi, Eugene Koskin, Ivano Tavernelli, Stefan Woerner, Marianna Rapsomaniki, Sergiy Zhuk, Jannis Born</div><div style='padding-top: 10px; width: 80ex'>Optimal Transport (OT) has fueled machine learning (ML) applications across
many domains. In cases where paired data measurements ($\mu$, $\nu$) are
coupled to a context variable $p_i$ , one may aspire to learn a global
transportation map that can be parameterized through a potentially unseen
con-text. Existing approaches utilize Neural OT and largely rely on Brenier's
theorem. Here, we propose a first-of-its-kind quantum computing formulation for
amortized optimization of contextualized transportation plans. We exploit a
direct link between doubly stochastic matrices and unitary operators thus
finding a natural connection between OT and quantum computation. We verify our
method on synthetic and real data, by predicting variations in cell type
distributions parameterized through drug dosage as context. Our comparisons to
several baselines reveal that our method can capture dose-induced variations in
cell distributions, even to some extent when dosages are extrapolated and
sometimes with performance similar to the best classical models. In summary,
this is a first step toward learning to predict contextualized transportation
plans through quantum.</div><div><a href='http://arxiv.org/abs/2402.14991v1'>2402.14991v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08097v1")'>An Accelerated Gradient Method for Simple Bilevel Optimization with
  Convex Lower-level Problem</div>
<div id='2402.08097v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:34:53Z</div><div>Authors: Jincheng Cao, Ruichen Jiang, Erfan Yazdandoost Hamedani, Aryan Mokhtari</div><div style='padding-top: 10px; width: 80ex'>In this paper, we focus on simple bilevel optimization problems, where we
minimize a convex smooth objective function over the optimal solution set of
another convex smooth constrained optimization problem. We present a novel
bilevel optimization method that locally approximates the solution set of the
lower-level problem using a cutting plane approach and employs an accelerated
gradient-based update to reduce the upper-level objective function over the
approximated solution set. We measure the performance of our method in terms of
suboptimality and infeasibility errors and provide non-asymptotic convergence
guarantees for both error criteria. Specifically, when the feasible set is
compact, we show that our method requires at most
$\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ iterations to find a
solution that is $\epsilon_f$-suboptimal and $\epsilon_g$-infeasible. Moreover,
under the additional assumption that the lower-level objective satisfies the
$r$-th H\"olderian error bound, we show that our method achieves an iteration
complexity of
$\mathcal{O}(\max\{\epsilon_{f}^{-\frac{2r-1}{2r}},\epsilon_{g}^{-\frac{2r-1}{2r}}\})$,
which matches the optimal complexity of single-level convex constrained
optimization when $r=1$.</div><div><a href='http://arxiv.org/abs/2402.08097v1'>2402.08097v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07101v1")'>On the Complexity of First-Order Methods in Stochastic Bilevel
  Optimization</div>
<div id='2402.07101v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T04:26:35Z</div><div>Authors: Jeongyeol Kwon, Dohyun Kwon, Hanbaek Lyu</div><div style='padding-top: 10px; width: 80ex'>We consider the problem of finding stationary points in Bilevel optimization
when the lower-level problem is unconstrained and strongly convex. The problem
has been extensively studied in recent years; the main technical challenge is
to keep track of lower-level solutions $y^*(x)$ in response to the changes in
the upper-level variables $x$. Subsequently, all existing approaches tie their
analyses to a genie algorithm that knows lower-level solutions and, therefore,
need not query any points far from them. We consider a dual question to such
approaches: suppose we have an oracle, which we call $y^*$-aware, that returns
an $O(\epsilon)$-estimate of the lower-level solution, in addition to
first-order gradient estimators {\it locally unbiased} within the
$\Theta(\epsilon)$-ball around $y^*(x)$. We study the complexity of finding
stationary points with such an $y^*$-aware oracle: we propose a simple
first-order method that converges to an $\epsilon$ stationary point using
$O(\epsilon^{-6}), O(\epsilon^{-4})$ access to first-order $y^*$-aware oracles.
Our upper bounds also apply to standard unbiased first-order oracles, improving
the best-known complexity of first-order methods by $O(\epsilon)$ with minimal
assumptions. We then provide the matching $\Omega(\epsilon^{-6})$,
$\Omega(\epsilon^{-4})$ lower bounds without and with an additional smoothness
assumption on $y^*$-aware oracles, respectively. Our results imply that any
approach that simulates an algorithm with an $y^*$-aware oracle must suffer the
same lower bounds.</div><div><a href='http://arxiv.org/abs/2402.07101v1'>2402.07101v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03167v2")'>Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic
  Update and Transient Iteration Complexity</div>
<div id='2402.03167v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:35:30Z</div><div>Authors: Boao Kong, Shuchen Zhu, Songtao Lu, Xinmeng Huang, Kun Yuan</div><div style='padding-top: 10px; width: 80ex'>Stochastic bilevel optimization (SBO) is becoming increasingly essential in
machine learning due to its versatility in handling nested structures. To
address large-scale SBO, decentralized approaches have emerged as effective
paradigms in which nodes communicate with immediate neighbors without a central
server, thereby improving communication efficiency and enhancing algorithmic
robustness. However, current decentralized SBO algorithms face challenges,
including expensive inner-loop updates and unclear understanding of the
influence of network topology, data heterogeneity, and the nested bilevel
algorithmic structures. In this paper, we introduce a single-loop decentralized
SBO (D-SOBA) algorithm and establish its transient iteration complexity, which,
for the first time, clarifies the joint influence of network topology and data
heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the
state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and
transient iteration complexity under more relaxed assumptions compared to
existing methods. Numerical experiments validate our theoretical findings.</div><div><a href='http://arxiv.org/abs/2402.03167v2'>2402.03167v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16748v1")'>Enhancing Hypergradients Estimation: A Study of Preconditioning and
  Reparameterization</div>
<div id='2402.16748v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T17:09:18Z</div><div>Authors: Zhenzhang Ye, Gabriel Peyré, Daniel Cremers, Pierre Ablin</div><div style='padding-top: 10px; width: 80ex'>Bilevel optimization aims to optimize an outer objective function that
depends on the solution to an inner optimization problem. It is routinely used
in Machine Learning, notably for hyperparameter tuning. The conventional method
to compute the so-called hypergradient of the outer problem is to use the
Implicit Function Theorem (IFT). As a function of the error of the inner
problem resolution, we study the error of the IFT method. We analyze two
strategies to reduce this error: preconditioning the IFT formula and
reparameterizing the inner problem. We give a detailed account of the impact of
these two modifications on the error, highlighting the role played by
higher-order derivatives of the functionals at stake. Our theoretical findings
explain when super efficiency, namely reaching an error on the hypergradient
that depends quadratically on the error on the inner problem, is achievable and
compare the two approaches when this is impossible. Numerical evaluations on
hyperparameter tuning for regression problems substantiate our theoretical
findings.</div><div><a href='http://arxiv.org/abs/2402.16748v1'>2402.16748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09807v1")'>Two trust region type algorithms for solving nonconvex-strongly concave
  minimax problems</div>
<div id='2402.09807v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T09:13:59Z</div><div>Authors: Tongliang Yao, Zi Xu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a Minimax Trust Region (MINIMAX-TR) algorithm and a
Minimax Trust Region Algorithm with Contractions and Expansions(MINIMAX-TRACE)
algorithm for solving nonconvex-strongly concave minimax problems. Both
algorithms can find an $(\epsilon, \sqrt{\epsilon})$-second order stationary
point(SSP) within $\mathcal{O}(\epsilon^{-1.5})$ iterations, which matches the
best well known iteration complexity.</div><div><a href='http://arxiv.org/abs/2402.09807v1'>2402.09807v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02356v1")'>Decentralized Sum-of-Nonconvex Optimization</div>
<div id='2402.02356v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:48:45Z</div><div>Authors: Zhuanghua Liu, Bryan Kian Hsiang Low</div><div style='padding-top: 10px; width: 80ex'>We consider the optimization problem of minimizing the sum-of-nonconvex
function, i.e., a convex function that is the average of nonconvex components.
The existing stochastic algorithms for such a problem only focus on a single
machine and the centralized scenario. In this paper, we study the
sum-of-nonconvex optimization in the decentralized setting. We present a new
theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the
linear convergence of their approach. However, the convergence rate of the
PMGT-SVRG algorithm has a linear dependency on the condition number, which is
undesirable for the ill-conditioned problem. To remedy this issue, we propose
an accelerated stochastic decentralized first-order algorithm by incorporating
the techniques of acceleration, gradient tracking, and multi-consensus mixing
into the SVRG algorithm. The convergence rate of the proposed method has a
square-root dependency on the condition number. The numerical experiments
validate the theoretical guarantee of our proposed algorithms on both synthetic
and real-world datasets.</div><div><a href='http://arxiv.org/abs/2402.02356v1'>2402.02356v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11565v1")'>Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex
  Optimization</div>
<div id='2403.11565v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T08:35:17Z</div><div>Authors: Siyuan Zhang, Nachuan Xiao, Xin Liu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we concentrate on decentralized optimization problems with
nonconvex and nonsmooth objective functions, especially on the decentralized
training of nonsmooth neural networks. We introduce a unified framework, named
DSM, to analyze the global convergence of decentralized stochastic subgradient
methods. We prove the global convergence of our proposed framework under mild
conditions, by establishing that the generated sequence asymptotically
approximates the trajectories of its associated differential inclusion.
Furthermore, we establish that our proposed framework encompasses a wide range
of existing efficient decentralized subgradient methods, including
decentralized stochastic subgradient descent (DSGD), DSGD with
gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In
addition, we introduce SignSGD employing the sign map to regularize the update
directions in DSGDm, and show it is enclosed in our proposed framework.
Consequently, our convergence results establish, for the first time, global
convergence of these methods when applied to nonsmooth nonconvex objectives.
Preliminary numerical experiments demonstrate that our proposed framework
yields highly efficient decentralized subgradient methods with convergence
guarantees in the training of nonsmooth neural networks.</div><div><a href='http://arxiv.org/abs/2403.11565v1'>2403.11565v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16978v1")'>An inexact Bregman proximal point method and its acceleration version
  for unbalanced optimal transport</div>
<div id='2402.16978v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T19:25:21Z</div><div>Authors: Xiang Chen, Faqiang Wang, Jun Liu, Li Cui</div><div style='padding-top: 10px; width: 80ex'>The Unbalanced Optimal Transport (UOT) problem plays increasingly important
roles in computational biology, computational imaging and deep learning.
Scaling algorithm is widely used to solve UOT due to its convenience and good
convergence properties. However, this algorithm has lower accuracy for large
regularization parameters, and due to stability issues, small regularization
parameters can easily lead to numerical overflow. We address this challenge by
developing an inexact Bregman proximal point method for solving UOT. This
algorithm approximates the proximal operator using the Scaling algorithm at
each iteration. The algorithm (1) converges to the true solution of UOT, (2)
has theoretical guarantees and robust regularization parameter selection, (3)
mitigates numerical stability issues, and (4) can achieve comparable
computational complexity to the Scaling algorithm in specific practice.
Building upon this, we develop an accelerated version of inexact Bregman
proximal point method for solving UOT by using acceleration techniques of
Bregman proximal point method and provide theoretical guarantees and
experimental validation of convergence and acceleration.</div><div><a href='http://arxiv.org/abs/2402.16978v1'>2402.16978v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06646v1")'>Block Majorization Minimization with Extrapolation and Application to
  $β$-NMF</div>
<div id='2401.06646v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T15:52:02Z</div><div>Authors: Le Thi Khanh Hien, Valentin Leplat, Nicolas Gillis</div><div style='padding-top: 10px; width: 80ex'>We propose a Block Majorization Minimization method with Extrapolation (BMMe)
for solving a class of multi-convex optimization problems. The extrapolation
parameters of BMMe are updated using a novel adaptive update rule. By showing
that block majorization minimization can be reformulated as a block mirror
descent method, with the Bregman divergence adaptively updated at each
iteration, we establish subsequential convergence for BMMe. We use this method
to design efficient algorithms to tackle nonnegative matrix factorization
problems with the $\beta$-divergences ($\beta$-NMF) for $\beta\in [1,2]$. These
algorithms, which are multiplicative updates with extrapolation, benefit from
our novel results that offer convergence guarantees. We also empirically
illustrate the significant acceleration of BMMe for $\beta$-NMF through
extensive experiments.</div><div><a href='http://arxiv.org/abs/2401.06646v1'>2401.06646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02967v2")'>Non-Convex Stochastic Composite Optimization with Polyak Momentum</div>
<div id='2403.02967v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:43:58Z</div><div>Authors: Yuan Gao, Anton Rodomanov, Sebastian U. Stich</div><div style='padding-top: 10px; width: 80ex'>The stochastic proximal gradient method is a powerful generalization of the
widely used stochastic gradient descent (SGD) method and has found numerous
applications in Machine Learning. However, it is notoriously known that this
method fails to converge in non-convex settings where the stochastic noise is
significant (i.e. when only small or bounded batch sizes are used). In this
paper, we focus on the stochastic proximal gradient method with Polyak
momentum. We prove this method attains an optimal convergence rate for
non-convex composite optimization problems, regardless of batch size.
Additionally, we rigorously analyze the variance reduction effect of the Polyak
momentum in the composite optimization setting and we show the method also
converges when the proximal step can only be solved inexactly. Finally, we
provide numerical experiments to validate our theoretical results.</div><div><a href='http://arxiv.org/abs/2403.02967v2'>2403.02967v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03180v1")'>Shuffling Momentum Gradient Algorithm for Convex Optimization</div>
<div id='2403.03180v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:19:02Z</div><div>Authors: Trang H. Tran, Quoc Tran-Dinh, Lam M. Nguyen</div><div style='padding-top: 10px; width: 80ex'>The Stochastic Gradient Descent method (SGD) and its stochastic variants have
become methods of choice for solving finite-sum optimization problems arising
from machine learning and data science thanks to their ability to handle
large-scale applications and big datasets. In the last decades, researchers
have made substantial effort to study the theoretical performance of SGD and
its shuffling variants. However, only limited work has investigated its
shuffling momentum variants, including shuffling heavy-ball momentum schemes
for non-convex problems and Nesterov's momentum for convex settings. In this
work, we extend the analysis of the shuffling momentum gradient method
developed in [Tran et al (2021)] to both finite-sum convex and strongly convex
optimization problems. We provide the first analysis of shuffling
momentum-based methods for the strongly convex setting, attaining a convergence
rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number
of training epochs. Our analysis is a state-of-the-art, matching the best rates
of existing shuffling stochastic gradient algorithms in the literature.</div><div><a href='http://arxiv.org/abs/2403.03180v1'>2403.03180v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06738v1")'>Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum</div>
<div id='2401.06738v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:17:28Z</div><div>Authors: Anh Dang, Reza Babanezhad, Sharan Vaswani</div><div style='padding-top: 10px; width: 80ex'>We analyze the convergence of stochastic heavy ball (SHB) momentum in the
smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with
small mini-batches) cannot attain an accelerated rate of convergence even for
quadratics, and conjecture that the practical gain of SHB is a by-product of
mini-batching. We substantiate this claim by showing that SHB can obtain an
accelerated rate when the mini-batch size is larger than some threshold. In
particular, for strongly-convex quadratics with condition number $\kappa$, we
prove that SHB with the standard step-size and momentum parameters results in
an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate,
where $T$ is the number of iterations and $\sigma^2$ is the variance in the
stochastic gradients. To ensure convergence to the minimizer, we propose a
multi-stage approach that results in a noise-adaptive
$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$
rate. For general strongly-convex functions, we use the averaging
interpretation of SHB along with exponential step-sizes to prove an
$O\left(\exp\left(-\frac{T}{\kappa} \right) + \frac{\sigma^2}{T} \right)$
convergence to the minimizer in a noise-adaptive manner. Finally, we
empirically demonstrate the effectiveness of the proposed algorithms.</div><div><a href='http://arxiv.org/abs/2401.06738v1'>2401.06738v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00853v1")'>Distributed Momentum Methods Under Biased Gradient Estimations</div>
<div id='2403.00853v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:03:03Z</div><div>Authors: Ali Beikmohammadi, Sarit Khirirat, Sindri Magnússon</div><div style='padding-top: 10px; width: 80ex'>Distributed stochastic gradient methods are gaining prominence in solving
large-scale machine learning problems that involve data distributed across
multiple nodes. However, obtaining unbiased stochastic gradients, which have
been the focus of most theoretical research, is challenging in many distributed
machine learning applications. The gradient estimations easily become biased,
for example, when gradients are compressed or clipped, when data is shuffled,
and in meta-learning and reinforcement learning. In this work, we establish
non-asymptotic convergence bounds on distributed momentum methods under biased
gradient estimation on both general non-convex and $\mu$-PL non-convex
problems. Our analysis covers general distributed optimization problems, and we
work out the implications for special cases where gradient estimates are
biased, i.e., in meta-learning and when the gradients are compressed or
clipped. Our numerical experiments on training deep neural networks with
Top-$K$ sparsification and clipping verify faster convergence performance of
momentum methods than traditional biased gradient descent.</div><div><a href='http://arxiv.org/abs/2403.00853v1'>2403.00853v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17675v1")'>Convergence analysis of t-SNE as a gradient flow for point cloud on a
  manifold</div>
<div id='2401.17675v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T08:52:45Z</div><div>Authors: Seonghyeon Jeong, Hau-Tieng Wu</div><div style='padding-top: 10px; width: 80ex'>We present a theoretical foundation regarding the boundedness of the t-SNE
algorithm. t-SNE employs gradient descent iteration with Kullback-Leibler (KL)
divergence as the objective function, aiming to identify a set of points that
closely resemble the original data points in a high-dimensional space,
minimizing KL divergence. Investigating t-SNE properties such as perplexity and
affinity under a weak convergence assumption on the sampled dataset, we examine
the behavior of points generated by t-SNE under continuous gradient flow.
Demonstrating that points generated by t-SNE remain bounded, we leverage this
insight to establish the existence of a minimizer for KL divergence.</div><div><a href='http://arxiv.org/abs/2401.17675v1'>2401.17675v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15146v1")'>On the Convergence of Adam under Non-uniform Smoothness: Separability
  from SGDM and Beyond</div>
<div id='2403.15146v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T11:57:51Z</div><div>Authors: Bohan Wang, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-Ming Ma, Wei Chen</div><div style='padding-top: 10px; width: 80ex'>This paper aims to clearly distinguish between Stochastic Gradient Descent
with Momentum (SGDM) and Adam in terms of their convergence rates. We
demonstrate that Adam achieves a faster convergence compared to SGDM under the
condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in
deterministic environments, Adam can attain the known lower bound for the
convergence rate of deterministic first-order optimizers, whereas the
convergence rate of Gradient Descent with Momentum (GDM) has higher order
dependence on the initial function value; (2) in stochastic setting, Adam's
convergence rate upper bound matches the lower bounds of stochastic first-order
optimizers, considering both the initial function value and the final error,
whereas there are instances where SGDM fails to converge with any learning
rate. These insights distinctly differentiate Adam and SGDM regarding their
convergence rates. Additionally, by introducing a novel stopping-time based
technique, we further prove that if we consider the minimum gradient norm
during iterations, the corresponding convergence rate can match the lower
bounds across all problem hyperparameters. The technique can also help proving
that Adam with a specific hyperparameter scheduler is parameter-agnostic, which
hence can be of independent interest.</div><div><a href='http://arxiv.org/abs/2403.15146v1'>2403.15146v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02857v1")'>Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation</div>
<div id='2402.02857v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:17:36Z</div><div>Authors: Sobihan Surendran, Antoine Godichon-Baggioni, Adeline Fermanian, Sylvain Le Corff</div><div style='padding-top: 10px; width: 80ex'>Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for
training deep neural networks. Most theoretical results assume access to
unbiased gradient estimators, which is not the case in several recent deep
learning and reinforcement learning applications that use Monte Carlo methods.
This paper provides a comprehensive non-asymptotic analysis of SGD with biased
gradients and adaptive steps for convex and non-convex smooth functions. Our
study incorporates time-dependent bias and emphasizes the importance of
controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In
particular, we establish that Adagrad and RMSProp with biased gradients
converge to critical points for smooth non-convex functions at a rate similar
to existing results in the literature for the unbiased case. Finally, we
provide experimental results using Variational Autoenconders (VAE) that
illustrate our convergence results and show how the effect of bias can be
reduced by appropriate hyperparameter tuning.</div><div><a href='http://arxiv.org/abs/2402.02857v1'>2402.02857v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15125v1")'>Accelerating Convergence of Stein Variational Gradient Descent via Deep
  Unfolding</div>
<div id='2402.15125v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:24:57Z</div><div>Authors: Yuya Kawamura, Satoshi Takabe</div><div style='padding-top: 10px; width: 80ex'>Stein variational gradient descent (SVGD) is a prominent particle-based
variational inference method used for sampling a target distribution. SVGD has
attracted interest for application in machine-learning techniques such as
Bayesian inference. In this paper, we propose novel trainable algorithms that
incorporate a deep-learning technique called deep unfolding,into SVGD. This
approach facilitates the learning of the internal parameters of SVGD, thereby
accelerating its convergence speed. To evaluate the proposed trainable SVGD
algorithms, we conducted numerical simulations of three tasks: sampling a
one-dimensional Gaussian mixture, performing Bayesian logistic regression, and
learning Bayesian neural networks. The results show that our proposed
algorithms exhibit faster convergence than the conventional variants of SVGD.</div><div><a href='http://arxiv.org/abs/2402.15125v1'>2402.15125v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16408v1")'>Stable Training of Normalizing Flows for High-dimensional Variational
  Inference</div>
<div id='2402.16408v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T09:04:07Z</div><div>Authors: Daniel Andrade</div><div style='padding-top: 10px; width: 80ex'>Variational inference with normalizing flows (NFs) is an increasingly popular
alternative to MCMC methods. In particular, NFs based on coupling layers (Real
NVPs) are frequently used due to their good empirical performance. In theory,
increasing the depth of normalizing flows should lead to more accurate
posterior approximations. However, in practice, training deep normalizing flows
for approximating high-dimensional posterior distributions is often infeasible
due to the high variance of the stochastic gradients. In this work, we show
that previous methods for stabilizing the variance of stochastic gradient
descent can be insufficient to achieve stable training of Real NVPs. As the
source of the problem, we identify that, during training, samples often exhibit
unusual high values. As a remedy, we propose a combination of two methods: (1)
soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log
transformation of the samples. We evaluate these and other previously proposed
modification on several challenging target distributions, including a
high-dimensional horseshoe logistic regression model. Our experiments show that
with our modifications, stable training of Real NVPs for posteriors with
several thousand dimensions is possible, allowing for more accurate marginal
likelihood estimation via importance sampling. Moreover, we evaluate several
common training techniques and architecture choices and provide practical
advise for training NFs for high-dimensional variational inference.</div><div><a href='http://arxiv.org/abs/2402.16408v1'>2402.16408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13875v1")'>Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?</div>
<div id='2401.13875v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T01:09:09Z</div><div>Authors: Huy Nguyen, Pedram Akbarian, Nhat Ho</div><div style='padding-top: 10px; width: 80ex'>Dense-to-sparse gating mixture of experts (MoE) has recently become an
effective alternative to a well-known sparse MoE. Rather than fixing the number
of activated experts as in the latter model, which could limit the
investigation of potential experts, the former model utilizes the temperature
to control the softmax weight distribution and the sparsity of the MoE during
training in order to stabilize the expert specialization. Nevertheless, while
there are previous attempts to theoretically comprehend the sparse MoE, a
comprehensive analysis of the dense-to-sparse gating MoE has remained elusive.
Therefore, we aim to explore the impacts of the dense-to-sparse gate on the
maximum likelihood estimation under the Gaussian MoE in this paper. We
demonstrate that due to interactions between the temperature and other model
parameters via some partial differential equations, the convergence rates of
parameter estimations are slower than any polynomial rates, and could be as
slow as $\mathcal{O}(1/\log(n))$, where $n$ denotes the sample size. To address
this issue, we propose using a novel activation dense-to-sparse gate, which
routes the output of a linear layer to an activation function before delivering
them to the softmax function. By imposing linearly independence conditions on
the activation function and its derivatives, we show that the parameter
estimation rates are significantly improved to polynomial rates.</div><div><a href='http://arxiv.org/abs/2401.13875v1'>2401.13875v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16776v1")'>Leveraging Nested MLMC for Sequential Neural Posterior Estimation with
  Intractable Likelihoods</div>
<div id='2401.16776v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:29:41Z</div><div>Authors: Xiliang Yang, Yifei Xiong, Zhijian He</div><div style='padding-top: 10px; width: 80ex'>Sequential neural posterior estimation (SNPE) techniques have been recently
proposed for dealing with simulation-based models with intractable likelihoods.
They are devoted to learning the posterior from adaptively proposed simulations
using neural network-based conditional density estimators. As a SNPE technique,
the automatic posterior transformation (APT) method proposed by Greenberg et
al. (2019) performs notably and scales to high dimensional data. However, the
APT method bears the computation of an expectation of the logarithm of an
intractable normalizing constant, i.e., a nested expectation. Although atomic
APT was proposed to solve this by discretizing the normalizing constant, it
remains challenging to analyze the convergence of learning. In this paper, we
propose a nested APT method to estimate the involved nested expectation
instead. This facilitates establishing the convergence analysis. Since the
nested estimators for the loss function and its gradient are biased, we make
use of unbiased multi-level Monte Carlo (MLMC) estimators for debiasing. To
further reduce the excessive variance of the unbiased estimators, this paper
also develops some truncated MLMC estimators by taking account of the trade-off
between the bias and the average cost. Numerical experiments for approximating
complex posteriors with multimodal in moderate dimensions are provided.</div><div><a href='http://arxiv.org/abs/2401.16776v1'>2401.16776v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02413v1")'>Simulation-Based Inference with Quantile Regression</div>
<div id='2401.02413v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T18:53:50Z</div><div>Authors: He Jia</div><div style='padding-top: 10px; width: 80ex'>We present Neural Quantile Estimation (NQE), a novel Simulation-Based
Inference (SBI) method based on conditional quantile regression. NQE
autoregressively learns individual one dimensional quantiles for each posterior
dimension, conditioned on the data and previous posterior dimensions. Posterior
samples are obtained by interpolating the predicted quantiles using monotonic
cubic Hermite spline, with specific treatment for the tail behavior and
multi-modal distributions. We introduce an alternative definition for the
Bayesian credible region using the local Cumulative Density Function (CDF),
offering substantially faster evaluation than the traditional Highest Posterior
Density Region (HPDR). In case of limited simulation budget and/or known model
misspecification, a post-processing broadening step can be integrated into NQE
to ensure the unbiasedness of the posterior estimation with negligible
additional computational cost. We demonstrate that the proposed NQE method
achieves state-of-the-art performance on a variety of benchmark problems.</div><div><a href='http://arxiv.org/abs/2401.02413v1'>2401.02413v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16639v1")'>Differentiable Particle Filtering using Optimal Placement Resampling</div>
<div id='2402.16639v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T15:09:56Z</div><div>Authors: Domonkos Csuzdi, Olivér Törő, Tamás Bécsi</div><div style='padding-top: 10px; width: 80ex'>Particle filters are a frequent choice for inference tasks in nonlinear and
non-Gaussian state-space models. They can either be used for state inference by
approximating the filtering distribution or for parameter inference by
approximating the marginal data (observation) likelihood. A good proposal
distribution and a good resampling scheme are crucial to obtain low variance
estimates. However, traditional methods like multinomial resampling introduce
nondifferentiability in PF-based loss functions for parameter estimation,
prohibiting gradient-based learning tasks. This work proposes a differentiable
resampling scheme by deterministic sampling from an empirical cumulative
distribution function. We evaluate our method on parameter inference tasks and
proposal learning.</div><div><a href='http://arxiv.org/abs/2402.16639v1'>2402.16639v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06302v1")'>Nonparametric Automatic Differentiation Variational Inference with
  Spline Approximation</div>
<div id='2403.06302v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T20:22:06Z</div><div>Authors: Yuda Shao, Shan Yu, Tianshu Feng</div><div style='padding-top: 10px; width: 80ex'>Automatic Differentiation Variational Inference (ADVI) is efficient in
learning probabilistic models. Classic ADVI relies on the parametric approach
to approximate the posterior. In this paper, we develop a spline-based
nonparametric approximation approach that enables flexible posterior
approximation for distributions with complicated structures, such as skewness,
multimodality, and bounded support. Compared with widely-used nonparametric
variational inference methods, the proposed method is easy to implement and
adaptive to various data structures. By adopting the spline approximation, we
derive a lower bound of the importance weighted autoencoder and establish the
asymptotic consistency. Experiments demonstrate the efficiency of the proposed
method in approximating complex posterior distributions and improving the
performance of generative models with incomplete data.</div><div><a href='http://arxiv.org/abs/2403.06302v1'>2403.06302v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17699v1")'>Gradient-based Discrete Sampling with Automatic Cyclical Scheduling</div>
<div id='2402.17699v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:23:40Z</div><div>Authors: Patrick Pynadath, Riddhiman Bhattacharya, Arun Hariharan, Ruqi Zhang</div><div style='padding-top: 10px; width: 80ex'>Discrete distributions, particularly in high-dimensional deep models, are
often highly multimodal due to inherent discontinuities. While gradient-based
discrete sampling has proven effective, it is susceptible to becoming trapped
in local modes due to the gradient information. To tackle this challenge, we
propose an automatic cyclical scheduling, designed for efficient and accurate
sampling in multimodal discrete distributions. Our method contains three key
components: (1) a cyclical step size schedule where large steps discover new
modes and small steps exploit each mode; (2) a cyclical balancing schedule,
ensuring ``balanced" proposals for given step sizes and high efficiency of the
Markov chain; and (3) an automatic tuning scheme for adjusting the
hyperparameters in the cyclical schedules, allowing adaptability across diverse
datasets with minimal tuning. We prove the non-asymptotic convergence and
inference guarantee for our method in general discrete distributions. Extensive
experiments demonstrate the superiority of our method in sampling complex
multimodal discrete distributions.</div><div><a href='http://arxiv.org/abs/2402.17699v1'>2402.17699v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10610v1")'>Sequential Monte Carlo for Inclusive KL Minimization in Amortized
  Variational Inference</div>
<div id='2403.10610v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T18:13:48Z</div><div>Authors: Declan McNamara, Jackson Loper, Jeffrey Regier</div><div style='padding-top: 10px; width: 80ex'>For training an encoder network to perform amortized variational inference,
the Kullback-Leibler (KL) divergence from the exact posterior to its
approximation, known as the inclusive or forward KL, is an increasingly popular
choice of variational objective due to the mass-covering property of its
minimizer. However, minimizing this objective is challenging. A popular
existing approach, Reweighted Wake-Sleep (RWS), suffers from heavily biased
gradients and a circular pathology that results in highly concentrated
variational distributions. As an alternative, we propose SMC-Wake, a procedure
for fitting an amortized variational approximation that uses
likelihood-tempered sequential Monte Carlo samplers to estimate the gradient of
the inclusive KL divergence. We propose three gradient estimators, all of which
are asymptotically unbiased in the number of iterations and two of which are
strongly consistent. Our method interleaves stochastic gradient updates, SMC
samplers, and iterative improvement to an estimate of the normalizing constant
to reduce bias from self-normalization. In experiments with both simulated and
real datasets, SMC-Wake fits variational distributions that approximate the
posterior more accurately than existing methods.</div><div><a href='http://arxiv.org/abs/2403.10610v1'>2403.10610v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05293v1")'>Leveraging Continuous Time to Understand Momentum When Training Diagonal
  Linear Networks</div>
<div id='2403.05293v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T13:21:07Z</div><div>Authors: Hristo Papazov, Scott Pesme, Nicolas Flammarion</div><div style='padding-top: 10px; width: 80ex'>In this work, we investigate the effect of momentum on the optimisation
trajectory of gradient descent. We leverage a continuous-time approach in the
analysis of momentum gradient descent with step size $\gamma$ and momentum
parameter $\beta$ that allows us to identify an intrinsic quantity $\lambda =
\frac{ \gamma }{ (1 - \beta)^2 }$ which uniquely defines the optimisation path
and provides a simple acceleration rule. When training a $2$-layer diagonal
linear network in an overparametrised regression setting, we characterise the
recovered solution through an implicit regularisation problem. We then prove
that small values of $\lambda$ help to recover sparse solutions. Finally, we
give similar but weaker results for stochastic momentum gradient descent. We
provide numerical experiments which support our claims.</div><div><a href='http://arxiv.org/abs/2403.05293v1'>2403.05293v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02325v1")'>Role of Momentum in Smoothing Objective Function in Implicit Graduated
  Optimization</div>
<div id='2402.02325v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T02:48:28Z</div><div>Authors: Naoki Sato, Hideaki Iiduka</div><div style='padding-top: 10px; width: 80ex'>While stochastic gradient descent (SGD) with momentum has fast convergence
and excellent generalizability, a theoretical explanation for this is lacking.
In this paper, we show that SGD with momentum smooths the objective function,
the degree of which is determined by the learning rate, the batch size, the
momentum factor, the variance of the stochastic gradient, and the upper bound
of the gradient norm. This theoretical finding reveals why momentum improves
generalizability and provides new insights into the role of the
hyperparameters, including momentum factor. We also present an implicit
graduated optimization algorithm that exploits the smoothing properties of SGD
with momentum and provide experimental results supporting our assertion that
SGD with momentum smooths the objective function.</div><div><a href='http://arxiv.org/abs/2402.02325v1'>2402.02325v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12033v1")'>Momentum-SAM: Sharpness Aware Minimization without Computational
  Overhead</div>
<div id='2401.12033v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T15:19:18Z</div><div>Authors: Marlon Becker, Frederick Altrock, Benjamin Risse</div><div style='padding-top: 10px; width: 80ex'>The recently proposed optimization algorithm for deep neural networks
Sharpness Aware Minimization (SAM) suggests perturbing parameters before
gradient calculation by a gradient ascent step to guide the optimization into
parameter space regions of flat loss. While significant generalization
improvements and thus reduction of overfitting could be demonstrated, the
computational costs are doubled due to the additionally needed gradient
calculation, making SAM unfeasible in case of limited computationally
capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose
Momentum-SAM (MSAM), which perturbs parameters in the direction of the
accumulated momentum vector to achieve low sharpness without significant
computational overhead or memory demands over SGD or Adam. We evaluate MSAM in
detail and reveal insights on separable mechanisms of NAG, SAM and MSAM
regarding training optimization and generalization. Code is available at
https://github.com/MarlonBecker/MSAM.</div><div><a href='http://arxiv.org/abs/2401.12033v1'>2401.12033v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13704v1")'>Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer
  through an Implicit-Explicit (IMEX) time-stepping approach</div>
<div id='2403.13704v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T16:08:27Z</div><div>Authors: Abhinab Bhattacharjee, Andrey A. Popov, Arash Sarshar, Adrian Sandu</div><div style='padding-top: 10px; width: 80ex'>The Adam optimizer, often used in Machine Learning for neural network
training, corresponds to an underlying ordinary differential equation (ODE) in
the limit of very small learning rates. This work shows that the classical Adam
algorithm is a first order implicit-explicit (IMEX) Euler discretization of the
underlying ODE. Employing the time discretization point of view, we propose new
extensions of the Adam scheme obtained by using higher order IMEX methods to
solve the ODE. Based on this approach, we derive a new optimization algorithm
for neural network training that performs better than classical Adam on several
regression and classification problems.</div><div><a href='http://arxiv.org/abs/2403.13704v1'>2403.13704v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11215v1")'>AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods</div>
<div id='2402.11215v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T07:49:50Z</div><div>Authors: Tim Tsz-Kit Lau, Han Liu, Mladen Kolar</div><div style='padding-top: 10px; width: 80ex'>The choice of batch sizes in stochastic gradient optimizers is critical for
model training. However, the practice of varying batch sizes throughout the
training process is less explored compared to other hyperparameters. We
investigate adaptive batch size strategies derived from adaptive sampling
methods, traditionally applied only in stochastic gradient descent. Given the
significant interplay between learning rates and batch sizes, and considering
the prevalence of adaptive gradient methods in deep learning, we emphasize the
need for adaptive batch size strategies in these contexts. We introduce
AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase
batch sizes during training, while model updates are performed using AdaGrad
and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a
rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth
nonconvex functions within $K$ iterations. AdaGrad also demonstrates similar
convergence properties when integrated with a novel coordinate-wise variant of
our adaptive batch size strategies. Our theoretical claims are supported by
numerical experiments on various image classification tasks, highlighting the
enhanced adaptability of progressive batching protocols in deep learning and
the potential of such adaptive batch size strategies with adaptive gradient
optimizers in large-scale model training.</div><div><a href='http://arxiv.org/abs/2402.11215v1'>2402.11215v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15344v1")'>Iteration and Stochastic First-order Oracle Complexities of Stochastic
  Gradient Descent using Constant and Decaying Learning Rates</div>
<div id='2402.15344v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:24:45Z</div><div>Authors: Kento Imaizumi, Hideaki Iiduka</div><div style='padding-top: 10px; width: 80ex'>The performance of stochastic gradient descent (SGD), which is the simplest
first-order optimizer for training deep neural networks, depends on not only
the learning rate but also the batch size. They both affect the number of
iterations and the stochastic first-order oracle (SFO) complexity needed for
training. In particular, the previous numerical results indicated that, for SGD
using a constant learning rate, the number of iterations needed for training
decreases when the batch size increases, and the SFO complexity needed for
training is minimized at a critical batch size and that it increases once the
batch size exceeds that size. Here, we study the relationship between batch
size and the iteration and SFO complexities needed for nonconvex optimization
in deep learning with SGD using constant or decaying learning rates and show
that SGD using the critical batch size minimizes the SFO complexity. We also
provide numerical comparisons of SGD with the existing first-order optimizers
and show the usefulness of SGD using a critical batch size. Moreover, we show
that measured critical batch sizes are close to the sizes estimated from our
theoretical results.</div><div><a href='http://arxiv.org/abs/2402.15344v1'>2402.15344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02833v1")'>SOFIM: Stochastic Optimization Using Regularized Fisher Information
  Matrix</div>
<div id='2403.02833v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T10:09:31Z</div><div>Authors: Gayathri C, Mrinmay Sen, A. K. Qin, Raghu Kishore N, Yen-Wei Chen, Balasubramanian Raman</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a new stochastic optimization method based on the
regularized Fisher information matrix (FIM), named SOFIM, which can efficiently
utilize the FIM to approximate the Hessian matrix for finding Newton's gradient
update in large-scale stochastic optimization of machine learning models. It
can be viewed as a variant of natural gradient descent (NGD), where the
challenge of storing and calculating the full FIM is addressed through making
use of the regularized FIM and directly finding the gradient update direction
via Sherman-Morrison matrix inversion. Additionally, like the popular Adam
method, SOFIM uses the first moment of the gradient to address the issue of
non-stationary objectives across mini-batches due to heterogeneous data. The
utilization of the regularized FIM and Sherman-Morrison matrix inversion leads
to the improved convergence rate with the same space and time complexities as
stochastic gradient descent (SGD) with momentum. The extensive experiments on
training deep learning models on several benchmark image classification
datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and
several state-of-the-art Newton optimization methods, such as Nystrom-SGD,
L-BFGS, and AdaHessian, in term of the convergence speed for achieving the
pre-specified objectives of training and test losses as well as test accuracy.</div><div><a href='http://arxiv.org/abs/2403.02833v1'>2403.02833v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03473v1")'>Inverse-Free Fast Natural Gradient Descent Method for Deep Learning</div>
<div id='2403.03473v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T05:13:28Z</div><div>Authors: Xinwei Ou, Ce Zhu, Xiaolin Huang, Yipeng Liu</div><div style='padding-top: 10px; width: 80ex'>Second-order methods can converge much faster than first-order methods by
incorporating second-order derivates or statistics, but they are far less
prevalent in deep learning due to their computational inefficiency. To handle
this, many of the existing solutions focus on reducing the size of the matrix
to be inverted. However, it is still needed to perform the inverse operator in
each iteration. In this paper, we present a fast natural gradient descent
(FNGD) method, which only requires computing the inverse during the first
epoch. Firstly, we reformulate the gradient preconditioning formula in the
natural gradient descent (NGD) as a weighted sum of per-sample gradients using
the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the
iterative inverse operation involved in computing coefficients, the weighted
coefficients are shared across epochs without affecting the empirical
performance.
  FNGD approximates the NGD as a fixed-coefficient weighted sum, akin to the
average sum in first-order methods. Consequently, the computational complexity
of FNGD can approach that of first-order methods. To demonstrate the efficiency
of the proposed FNGD, we perform empirical evaluations on image classification
and machine translation tasks. For training ResNet-18 on the CIFAR-100 dataset,
FNGD can achieve a speedup of 2.05$\times$ compared with KFAC. For training
Transformer on Multi30K, FNGD outperforms AdamW by 24 BLEU score while
requiring almost the same training time.</div><div><a href='http://arxiv.org/abs/2403.03473v1'>2403.03473v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03496v3")'>Can We Remove the Square-Root in Adaptive Gradient Methods? A
  Second-Order Perspective</div>
<div id='2402.03496v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T20:15:19Z</div><div>Authors: Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner, Alireza Makhzani</div><div style='padding-top: 10px; width: 80ex'>Adaptive gradient optimizers like Adam(W) are the default training algorithms
for many deep learning architectures, such as transformers. Their diagonal
preconditioner is based on the gradient outer product which is incorporated
into the parameter update via a square root. While these methods are often
motivated as approximate second-order methods, the square root represents a
fundamental difference. In this work, we investigate how the behavior of
adaptive methods changes when we remove the root, i.e. strengthen their
second-order motivation. Surprisingly, we find that such square-root-free
adaptive methods close the generalization gap to SGD on convolutional
architectures, while maintaining their root-based counterpart's performance on
transformers. The second-order perspective also has practical benefits for the
development of adaptive methods with non-diagonal preconditioner. In contrast
to root-based counterparts like Shampoo, they do not require numerically
unstable matrix square roots and therefore work well in low precision, which we
demonstrate empirically. This raises important questions regarding the
currently overlooked role of adaptivity for the success of adaptive methods
since the success is often attributed to sign descent induced by the root.</div><div><a href='http://arxiv.org/abs/2402.03496v3'>2402.03496v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07114v1")'>Towards Quantifying the Preconditioning Effect of Adam</div>
<div id='2402.07114v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T06:21:18Z</div><div>Authors: Rudrajit Das, Naman Agarwal, Sujay Sanghavi, Inderjit S. Dhillon</div><div style='padding-top: 10px; width: 80ex'>There is a notable dearth of results characterizing the preconditioning
effect of Adam and showing how it may alleviate the curse of ill-conditioning
-- an issue plaguing gradient descent (GD). In this work, we perform a detailed
analysis of Adam's preconditioning effect for quadratic functions and quantify
to what extent Adam can mitigate the dependence on the condition number of the
Hessian. Our key finding is that Adam can suffer less from the condition number
but at the expense of suffering a dimension-dependent quantity. Specifically,
for a $d$-dimensional quadratic with a diagonal Hessian having condition number
$\kappa$, we show that the effective condition number-like quantity controlling
the iteration complexity of Adam without momentum is $\mathcal{O}(\min(d,
\kappa))$. For a diagonally dominant Hessian, we obtain a bound of
$\mathcal{O}(\min(d \sqrt{d \kappa}, \kappa))$ for the corresponding quantity.
Thus, when $d &lt; \mathcal{O}(\kappa^p)$ where $p = 1$ for a diagonal Hessian and
$p = 1/3$ for a diagonally dominant Hessian, Adam can outperform GD (which has
an $\mathcal{O}(\kappa)$ dependence). On the negative side, our results suggest
that Adam can be worse than GD for a sufficiently non-diagonal Hessian even if
$d \ll \mathcal{O}(\kappa^{1/3})$; we corroborate this with empirical evidence.
Finally, we extend our analysis to functions satisfying per-coordinate
Lipschitz smoothness and a modified version of the Polyak-\L ojasiewicz
condition.</div><div><a href='http://arxiv.org/abs/2402.07114v1'>2402.07114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03295v1")'>Ginger: An Efficient Curvature Approximation with Linear Complexity for
  General Neural Networks</div>
<div id='2402.03295v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:51:17Z</div><div>Authors: Yongchang Hao, Yanshuai Cao, Lili Mou</div><div style='padding-top: 10px; width: 80ex'>Second-order optimization approaches like the generalized Gauss-Newton method
are considered more powerful as they utilize the curvature information of the
objective function with preconditioning matrices. Albeit offering tempting
theoretical benefits, they are not easily applicable to modern deep learning.
The major reason is due to the quadratic memory and cubic time complexity to
compute the inverse of the matrix. These requirements are infeasible even with
state-of-the-art hardware. In this work, we propose Ginger, an
eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our
method enjoys efficient linear memory and time complexity for each iteration.
Instead of approximating the conditioning matrix, we directly maintain its
inverse to make the approximation more accurate. We provide the convergence
result of Ginger for non-convex objectives. Our experiments on different tasks
with different model architectures verify the effectiveness of our method. Our
code is publicly available.</div><div><a href='http://arxiv.org/abs/2402.03295v1'>2402.03295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07193v1")'>The Implicit Bias of Gradient Noise: A Symmetry Perspective</div>
<div id='2402.07193v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T13:00:04Z</div><div>Authors: Liu Ziyin, Mingze Wang, Lei Wu</div><div style='padding-top: 10px; width: 80ex'>We characterize the learning dynamics of stochastic gradient descent (SGD)
when continuous symmetry exists in the loss function, where the divergence
between SGD and gradient descent is dramatic. We show that depending on how the
symmetry affects the learning dynamics, we can divide a family of symmetry into
two classes. For one class of symmetry, SGD naturally converges to solutions
that have a balanced and aligned gradient noise. For the other class of
symmetry, SGD will almost always diverge. Then, we show that our result remains
applicable and can help us understand the training dynamics even when the
symmetry is not present in the loss function. Our main result is universal in
the sense that it only depends on the existence of the symmetry and is
independent of the details of the loss function. We demonstrate that the
proposed theory offers an explanation of progressive sharpening and flattening
and can be applied to common practical problems such as representation
normalization, matrix factorization, and the use of warmup.</div><div><a href='http://arxiv.org/abs/2402.07193v1'>2402.07193v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17722v1")'>Taming Nonconvex Stochastic Mirror Descent with General Bregman
  Divergence</div>
<div id='2402.17722v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:56:49Z</div><div>Authors: Ilyas Fatkhullin, Niao He</div><div style='padding-top: 10px; width: 80ex'>This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the
contemporary nonconvex optimization setting. Existing results for batch-free
nonconvex SMD restrict the choice of the distance generating function (DGF) to
be differentiable with Lipschitz continuous gradients, thereby excluding
important setups such as Shannon entropy. In this work, we present a new
convergence analysis of nonconvex SMD supporting general DGF, that overcomes
the above limitations and relies solely on the standard assumptions. Moreover,
our convergence is established with respect to the Bregman Forward-Backward
envelope, which is a stronger measure than the commonly used squared norm of
gradient mapping. We further extend our results to guarantee high probability
convergence under sub-Gaussian noise and global convergence under the
generalized Bregman Proximal Polyak-{\L}ojasiewicz condition. Additionally, we
illustrate the advantages of our improved SMD theory in various nonconvex
machine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of
nonconvex differentially private (DP) learning, our theory yields a simple
algorithm with a (nearly) dimension-independent utility bound. For the problem
of training linear neural networks, we develop provably convergent stochastic
algorithms.</div><div><a href='http://arxiv.org/abs/2402.17722v1'>2402.17722v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02912v1")'>Mirror Descent Algorithms with Nearly Dimension-Independent Rates for
  Differentially-Private Stochastic Saddle-Point Problems</div>
<div id='2403.02912v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T12:28:00Z</div><div>Authors: Tomás González, Cristóbal Guzmán, Courtney Paquette</div><div style='padding-top: 10px; width: 80ex'>We study the problem of differentially-private (DP) stochastic
(convex-concave) saddle-points in the polyhedral setting. We propose
$(\varepsilon, \delta)$-DP algorithms based on stochastic mirror descent that
attain nearly dimension-independent convergence rates for the expected duality
gap, a type of guarantee that was known before only for bilinear objectives.
For convex-concave and first-order-smooth stochastic objectives, our algorithms
attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$,
where $d$ is the dimension of the problem and $n$ the dataset size. Under an
additional second-order-smoothness assumption, we improve the rate on the
expected gap to $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{2/5}$.
Under this additional assumption, we also show, by using bias-reduced gradient
estimators, that the duality gap is bounded by $\log(d)/\sqrt{n} +
\log(d)/[n\varepsilon]^{1/2}$ with constant success probability. This result
provides evidence of the near-optimality of the approach. Finally, we show that
combining our methods with acceleration techniques from online learning leads
to the first algorithm for DP Stochastic Convex Optimization in the polyhedral
setting that is not based on Frank-Wolfe methods. For convex and
first-order-smooth stochastic objectives, our algorithms attain an excess risk
of $\sqrt{\log(d)/n} + \log(d)^{7/10}/[n\varepsilon]^{2/5}$, and when
additionally assuming second-order-smoothness, we improve the rate to
$\sqrt{\log(d)/n} + \log(d)/\sqrt{n\varepsilon}$. Instrumental to all of these
results are various extensions of the classical Maurey Sparsification Lemma,
which may be of independent interest.</div><div><a href='http://arxiv.org/abs/2403.02912v1'>2403.02912v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00664v1")'>New Sample Complexity Bounds for (Regularized) Sample Average
  Approximation in Several Heavy-Tailed, Non-Lipschitzian, and High-Dimensional
  Cases</div>
<div id='2401.00664v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T04:35:53Z</div><div>Authors: Hongcheng Liu, Jindong Tong</div><div style='padding-top: 10px; width: 80ex'>We study the sample complexity of sample average approximation (SAA) and its
simple variations, referred to as the regularized SAA (RSAA), in solving convex
and strongly convex stochastic programming (SP) problems under
heavy-tailed-ness, non-Lipschitz-ness, and/or high dimensionality. The presence
of such irregularities underscores critical vacua in the literature. In
response, this paper presents three sets of results: First, we show that the
(R)SAA is effective even if the objective function is not necessarily Lipschitz
and the underlying distribution admits some bounded central moments only at
(near-)optimal solutions. Second, when the SP's objective function is the sum
of a smooth term and a Lipschitz term, we prove that the (R)SAA's sample
complexity is completely independent from any complexity measures (e.g., the
covering number) of the feasible region. Third, we explicate the (R)SAA's
sample complexities with regard to the dependence on dimensionality $d$: When
some $p$th ($p\geq 2$) central moment of the underlying distribution is
bounded, we show that the required sample size grows at a rate no worse than
$\mathcal O\left(p d^{2/p}\right)$ under any one of the three structural
assumptions: (i) strong convexity w.r.t. the $q$-norm ($q\geq 1$); (ii) the
combination of restricted strong convexity and sparsity; and (iii) a
dimension-insensitive $q$-norm of an optimal solution. In both cases of (i) and
(iii), it is further required that $p\leq q/(q-1)$. As a direct implication,
the (R)SAA's complexity becomes (poly-)logarithmic in $d$, whenever $p\geq
c\cdot \ln d$ is admissible for some constant $c&gt;0$. These new results deviate
from the SAA's typical sample complexities that grow polynomially with $d$.
Part of our proof is based on the average-replace-one (RO) stability, which
appears to be novel for the (R)SAA's analyses.</div><div><a href='http://arxiv.org/abs/2401.00664v1'>2401.00664v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11844v1")'>Near-Optimal Solutions of Constrained Learning Problems</div>
<div id='2403.11844v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:55:45Z</div><div>Authors: Juan Elenter, Luiz F. O. Chamon, Alejandro Ribeiro</div><div style='padding-top: 10px; width: 80ex'>With the widespread adoption of machine learning systems, the need to curtail
their behavior has become increasingly apparent. This is evidenced by recent
advancements towards developing models that satisfy robustness, safety, and
fairness requirements. These requirements can be imposed (with generalization
guarantees) by formulating constrained learning problems that can then be
tackled by dual ascent algorithms. Yet, though these algorithms converge in
objective value, even in non-convex settings, they cannot guarantee that their
outcome is feasible. Doing so requires randomizing over all iterates, which is
impractical in virtually any modern applications. Still, final iterates have
been observed to perform well in practice. In this work, we address this gap
between theory and practice by characterizing the constraint violation of
Lagrangian minimizers associated with optimal dual variables, despite lack of
convexity. To do this, we leverage the fact that non-convex, finite-dimensional
constrained learning problems can be seen as parametrizations of convex,
functional problems. Our results show that rich parametrizations effectively
mitigate the issue of feasibility in dual methods, shedding light on prior
empirical successes of dual learning. We illustrate our findings in fair
learning tasks.</div><div><a href='http://arxiv.org/abs/2403.11844v1'>2403.11844v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04535v1")'>Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection
  and Beyond</div>
<div id='2401.04535v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T13:10:30Z</div><div>Authors: Zhao Ding, Chenguang Duan, Yuling Jiao, Jerry Zhijian Yang</div><div style='padding-top: 10px; width: 80ex'>We propose SDORE, a semi-supervised deep Sobolev regressor, for the
nonparametric estimation of the underlying regression function and its
gradient. SDORE employs deep neural networks to minimize empirical risk with
gradient norm regularization, allowing computation of the gradient norm on
unlabeled data. We conduct a comprehensive analysis of the convergence rates of
SDORE and establish a minimax optimal rate for the regression function.
Crucially, we also derive a convergence rate for the associated plug-in
gradient estimator, even in the presence of significant domain shift. These
theoretical findings offer valuable prior guidance for selecting regularization
parameters and determining the size of the neural network, while showcasing the
provable advantage of leveraging unlabeled data in semi-supervised learning. To
the best of our knowledge, SDORE is the first provable neural network-based
approach that simultaneously estimates the regression function and its
gradient, with diverse applications including nonparametric variable selection
and inverse problems. The effectiveness of SDORE is validated through an
extensive range of numerical simulations and real data analysis.</div><div><a href='http://arxiv.org/abs/2401.04535v1'>2401.04535v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04236v1")'>Regularized DeepIV with Model Selection</div>
<div id='2403.04236v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T05:38:56Z</div><div>Authors: Zihao Li, Hui Lan, Vasilis Syrgkanis, Mengdi Wang, Masatoshi Uehara</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study nonparametric estimation of instrumental variable
(IV) regressions. While recent advancements in machine learning have introduced
flexible methods for IV estimation, they often encounter one or more of the
following limitations: (1) restricting the IV regression to be uniquely
identified; (2) requiring minimax computation oracle, which is highly unstable
in practice; (3) absence of model selection procedure. In this paper, we
present the first method and analysis that can avoid all three limitations,
while still enabling general function approximation. Specifically, we propose a
minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can
converge to the least-norm IV solution. Our method consists of two stages:
first, we learn the conditional distribution of covariates, and by utilizing
the learned distribution, we learn the estimator by minimizing a
Tikhonov-regularized loss function. We further show that our method allows
model selection procedures that can achieve the oracle rates in the
misspecified regime. When extended to an iterative estimator, our method
matches the current state-of-the-art convergence rate. Our method is a Tikhonov
regularized variant of the popular DeepIV method with a non-parametric MLE
first-stage estimator, and our results provide the first rigorous guarantees
for this empirically used method, showcasing the importance of regularization
which was absent from the original work.</div><div><a href='http://arxiv.org/abs/2403.04236v1'>2403.04236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03214v1")'>Understanding Representation Learnability of Nonlinear Self-Supervised
  Learning</div>
<div id='2401.03214v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T13:23:26Z</div><div>Authors: Ruofeng Yang, Xiangyuan Li, Bo Jiang, Shuai Li</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning (SSL) has empirically shown its data representation
learnability in many downstream tasks. There are only a few theoretical works
on data representation learnability, and many of those focus on final data
representation, treating the nonlinear neural network as a ``black box".
However, the accurate learning results of neural networks are crucial for
describing the data distribution features learned by SSL models. Our paper is
the first to analyze the learning results of the nonlinear SSL model
accurately. We consider a toy data distribution that contains two features: the
label-related feature and the hidden feature. Unlike previous linear setting
work that depends on closed-form solutions, we use the gradient descent
algorithm to train a 1-layer nonlinear SSL model with a certain initialization
region and prove that the model converges to a local minimum. Furthermore,
different from the complex iterative analysis, we propose a new analysis
process which uses the exact version of Inverse Function Theorem to accurately
describe the features learned by the local minimum. With this local minimum, we
prove that the nonlinear SSL model can capture the label-related feature and
hidden feature at the same time. In contrast, the nonlinear supervised learning
(SL) model can only learn the label-related feature. We also present the
learning processes and results of the nonlinear SSL and SL model via simulation
experiments.</div><div><a href='http://arxiv.org/abs/2401.03214v1'>2401.03214v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08808v1")'>Sample Relationship from Learning Dynamics Matters for Generalisation</div>
<div id='2401.08808v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T20:20:10Z</div><div>Authors: Shangmin Guo, Yi Ren, Stefano V. Albrecht, Kenny Smith</div><div style='padding-top: 10px; width: 80ex'>Although much research has been done on proposing new models or loss
functions to improve the generalisation of artificial neural networks (ANNs),
less attention has been directed to the impact of the training data on
generalisation. In this work, we start from approximating the interaction
between samples, i.e. how learning one sample would modify the model's
prediction on other samples. Through analysing the terms involved in weight
updates in supervised learning, we find that labels influence the interaction
between samples. Therefore, we propose the labelled pseudo Neural Tangent
Kernel (lpNTK) which takes label information into consideration when measuring
the interactions between samples. We first prove that lpNTK asymptotically
converges to the empirical neural tangent kernel in terms of the Frobenius norm
under certain assumptions. Secondly, we illustrate how lpNTK helps to
understand learning phenomena identified in previous work, specifically the
learning difficulty of samples and forgetting events during learning. Moreover,
we also show that using lpNTK to identify and remove poisoning training samples
does not hurt the generalisation performance of ANNs.</div><div><a href='http://arxiv.org/abs/2401.08808v1'>2401.08808v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15898v2")'>Information-based Transductive Active Learning</div>
<div id='2402.15898v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T09:22:45Z</div><div>Authors: Jonas Hübotter, Bhavya Sukhija, Lenart Treven, Yarden As, Andreas Krause</div><div style='padding-top: 10px; width: 80ex'>We generalize active learning to address real-world settings where sampling
is restricted to an accessible region of the domain, while prediction targets
may lie outside this region. To this end, we propose ITL, short for
information-based transductive learning, an approach which samples adaptively
to maximize the information gained about specified prediction targets. We show,
under general regularity assumptions, that ITL converges uniformly to the
smallest possible uncertainty obtainable from the accessible data. We
demonstrate ITL in two key applications: Few-shot fine-tuning of large neural
networks and safe Bayesian optimization, and in both cases, ITL significantly
outperforms the state-of-the-art.</div><div><a href='http://arxiv.org/abs/2402.15898v2'>2402.15898v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15441v2")'>Active Few-Shot Fine-Tuning</div>
<div id='2402.15441v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T09:19:05Z</div><div>Authors: Jonas Hübotter, Bhavya Sukhija, Lenart Treven, Yarden As, Andreas Krause</div><div style='padding-top: 10px; width: 80ex'>We study the active few-shot fine-tuning of large neural networks to
downstream tasks. We show that few-shot fine-tuning is an instance of a
generalization of classical active learning, transductive active learning, and
we propose ITL, short for information-based transductive learning, an approach
which samples adaptively to maximize the information gained about specified
downstream tasks. Under general regularity assumptions, we prove that ITL
converges uniformly to the smallest possible uncertainty obtainable from the
accessible data. To the best of our knowledge, we are the first to derive
generalization bounds of this kind, and they may be of independent interest for
active learning. We apply ITL to the few-shot fine-tuning of large neural
networks and show that ITL substantially improves upon the state-of-the-art.</div><div><a href='http://arxiv.org/abs/2402.15441v2'>2402.15441v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01401v2")'>Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization</div>
<div id='2402.01401v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:33:30Z</div><div>Authors: Jack Foster, Kyle Fogarty, Stefan Schoepf, Cengiz Öztireli, Alexandra Brintrup</div><div style='padding-top: 10px; width: 80ex'>To comply with AI and data regulations, the need to forget private or
copyrighted information from trained machine learning models is increasingly
important. The key challenge in unlearning is forgetting the necessary data in
a timely manner, while preserving model performance. In this work, we address
the zero-shot unlearning scenario, whereby an unlearning algorithm must be able
to remove data given only a trained model and the data to be forgotten. Under
such a definition, existing state-of-the-art methods are insufficient. Building
on the concepts of Lipschitz continuity, we present a method that induces
smoothing of the forget sample's output, with respect to perturbations of that
sample. We show this smoothing successfully results in forgetting while
preserving general model performance. We perform extensive empirical evaluation
of our method over a range of contemporary benchmarks, verifying that our
method achieves state-of-the-art performance under the strict constraints of
zero-shot unlearning.</div><div><a href='http://arxiv.org/abs/2402.01401v2'>2402.01401v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01922v1")'>A General Framework for Learning from Weak Supervision</div>
<div id='2402.01922v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T21:48:50Z</div><div>Authors: Hao Chen, Jindong Wang, Lei Feng, Xiang Li, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj</div><div style='padding-top: 10px; width: 80ex'>Weakly supervised learning generally faces challenges in applicability to
various scenarios with diverse weak supervision and in scalability due to the
complexity of existing algorithms, thereby hindering the practical deployment.
This paper introduces a general framework for learning from weak supervision
(GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization
(EM) formulation, adeptly accommodating various weak supervision sources,
including instance partial labels, aggregate statistics, pairwise observations,
and unlabeled data. We further present an advanced algorithm that significantly
simplifies the EM computational demands using a Non-deterministic Finite
Automaton (NFA) along with a forward-backward algorithm, which effectively
reduces time complexity from quadratic or factorial often required in existing
solutions to linear scale. The problem of learning from arbitrary weak
supervision is therefore converted to the NFA modeling of them. GLWS not only
enhances the scalability of machine learning models but also demonstrates
superior performance and versatility across 11 weak supervision scenarios. We
hope our work paves the way for further advancements and practical deployment
in this field.</div><div><a href='http://arxiv.org/abs/2402.01922v1'>2402.01922v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17176v1")'>DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</div>
<div id='2402.17176v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T03:24:54Z</div><div>Authors: Hongyu Shen, Yici Yan, Zhizhen Zhao</div><div style='padding-top: 10px; width: 80ex'>Model-X knockoff, among various feature selection methods, received much
attention recently due to its guarantee on false discovery rate (FDR) control.
Subsequent to its introduction in parametric design, knockoff is advanced to
handle arbitrary data distributions using deep learning-based generative
modeling. However, we observed that current implementations of the deep Model-X
knockoff framework exhibit limitations. Notably, the "swap property" that
knockoffs necessitate frequently encounter challenges on sample level, leading
to a diminished selection power. To overcome, we develop "Deep Dependency
Regularized Knockoff (DeepDRK)", a distribution-free deep learning method that
strikes a balance between FDR and power. In DeepDRK, a generative model
grounded in a transformer architecture is introduced to better achieve the
"swap property". Novel efficient regularization techniques are also proposed to
reach higher power. Our model outperforms other benchmarks in synthetic,
semi-synthetic, and real-world data, especially when sample size is small and
data distribution is complex.</div><div><a href='http://arxiv.org/abs/2402.17176v1'>2402.17176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16661v1")'>Penalized Generative Variable Selection</div>
<div id='2402.16661v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T15:35:10Z</div><div>Authors: Tong Wang, Jian Huang, Shuangge Ma</div><div style='padding-top: 10px; width: 80ex'>Deep networks are increasingly applied to a wide variety of data, including
data with high-dimensional predictors. In such analysis, variable selection can
be needed along with estimation/model building. Many of the existing deep
network studies that incorporate variable selection have been limited to
methodological and numerical developments. In this study, we consider
modeling/estimation using the conditional Wasserstein Generative Adversarial
networks. Group Lasso penalization is applied for variable selection, which may
improve model estimation/prediction, interpretability, stability, etc.
Significantly advancing from the existing literature, the analysis of censored
survival data is also considered. We establish the convergence rate for
variable selection while considering the approximation error, and obtain a more
efficient distribution estimation. Simulations and the analysis of real
experimental data demonstrate satisfactory practical utility of the proposed
analysis.</div><div><a href='http://arxiv.org/abs/2402.16661v1'>2402.16661v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11288v1")'>Long-Term Fair Decision Making through Deep Generative Models</div>
<div id='2401.11288v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T17:44:50Z</div><div>Authors: Yaowei Hu, Yongkai Wu, Lu Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper studies long-term fair machine learning which aims to mitigate
group disparity over the long term in sequential decision-making systems. To
define long-term fairness, we leverage the temporal causal graph and use the
1-Wasserstein distance between the interventional distributions of different
demographic groups at a sufficiently large time step as the quantitative
metric. Then, we propose a three-phase learning framework where the decision
model is trained on high-fidelity data generated by a deep generative model. We
formulate the optimization problem as a performative risk minimization and
adopt the repeated gradient descent algorithm for learning. The empirical
evaluation shows the efficacy of the proposed method using both synthetic and
semi-synthetic datasets.</div><div><a href='http://arxiv.org/abs/2401.11288v1'>2401.11288v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01476v1")'>Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian
  Processes</div>
<div id='2402.01476v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T15:05:13Z</div><div>Authors: Yingyi Chen, Qinghua Tao, Francesco Tonin, Johan A. K. Suykens</div><div style='padding-top: 10px; width: 80ex'>While the great capability of Transformers significantly boosts prediction
accuracy, it could also yield overconfident predictions and require calibrated
uncertainty estimation, which can be commonly tackled by Gaussian processes
(GPs). Existing works apply GPs with symmetric kernels under variational
inference to the attention kernel; however, omitting the fact that attention
kernels are in essence asymmetric. Moreover, the complexity of deriving the GP
posteriors remains high for large-scale data. In this work, we propose
Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building
uncertainty-aware self-attention where the asymmetry of attention kernels is
tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through
KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from
KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using
only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP
posteriors can be based on the inversion of a diagonal matrix containing
singular values, contributing to a reduction in time complexity; iii) an
evidence lower bound is derived so that variational parameters can be optimized
towards this objective. Experiments verify our excellent performances and
efficiency on in-distribution, distribution-shift and out-of-distribution
benchmarks.</div><div><a href='http://arxiv.org/abs/2402.01476v1'>2402.01476v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07626v1")'>Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution
  for Weak Features</div>
<div id='2402.07626v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T13:11:11Z</div><div>Authors: Rodrigo Veiga, Anastasia Remizova, Nicolas Macris</div><div style='padding-top: 10px; width: 80ex'>We investigate the test risk of continuous-time stochastic gradient flow
dynamics in learning theory. Using a path integral formulation we provide, in
the regime of a small learning rate, a general formula for computing the
difference between test risk curves of pure gradient and stochastic gradient
flows. We apply the general theory to a simple model of weak features, which
displays the double descent phenomenon, and explicitly compute the corrections
brought about by the added stochastic term in the dynamics, as a function of
time and model parameters. The analytical results are compared to simulations
of discrete-time stochastic gradient descent and show good agreement.</div><div><a href='http://arxiv.org/abs/2402.07626v1'>2402.07626v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10929v1")'>Function-space Parameterization of Neural Networks for Sequential
  Learning</div>
<div id='2403.10929v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T14:00:04Z</div><div>Authors: Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin</div><div style='padding-top: 10px; width: 80ex'>Sequential learning paradigms pose challenges for gradient-based deep
learning due to difficulties incorporating new data and retaining prior
knowledge. While Gaussian processes elegantly tackle these problems, they
struggle with scalability and handling rich inputs, such as images. To address
these issues, we introduce a technique that converts neural networks from
weight space to function space, through a dual parameterization. Our
parameterization offers: (i) a way to scale function-space methods to large
data sets via sparsification, (ii) retention of prior knowledge when access to
past data is limited, and (iii) a mechanism to incorporate new data without
retraining. Our experiments demonstrate that we can retain knowledge in
continual learning and incorporate new data efficiently. We further show its
strengths in uncertainty quantification and guiding exploration in model-based
RL. Further information and code is available on the project website.</div><div><a href='http://arxiv.org/abs/2403.10929v1'>2403.10929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03220v2")'>The Benefits of Reusing Batches for Gradient Descent in Two-Layer
  Networks: Breaking the Curse of Information and Leap Exponents</div>
<div id='2402.03220v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:30:42Z</div><div>Authors: Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborová, Florent Krzakala</div><div style='padding-top: 10px; width: 80ex'>We investigate the training dynamics of two-layer neural networks when
learning multi-index target functions. We focus on multi-pass gradient descent
(GD) that reuses the batches multiple times and show that it significantly
changes the conclusion about which functions are learnable compared to
single-pass gradient descent. In particular, multi-pass GD with finite stepsize
is found to overcome the limitations of gradient flow and single-pass GD given
by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et
al., 2023) of the target function. We show that upon re-using batches, the
network achieves in just two time steps an overlap with the target subspace
even for functions not satisfying the staircase property (Abbe et al., 2021).
We characterize the (broad) class of functions efficiently learned in finite
time. The proof of our results is based on the analysis of the Dynamical
Mean-Field Theory (DMFT). We further provide a closed-form description of the
dynamical process of the low-dimensional projections of the weights, and
numerical experiments illustrating the theory.</div><div><a href='http://arxiv.org/abs/2402.03220v2'>2402.03220v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10791v1")'>Early alignment in two-layer networks training is a two-edged sword</div>
<div id='2401.10791v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:23:53Z</div><div>Authors: Etienne Boursier, Nicolas Flammarion</div><div style='padding-top: 10px; width: 80ex'>Training neural networks with first order optimisation methods is at the core
of the empirical success of deep learning. The scale of initialisation is a
crucial factor, as small initialisations are generally associated to a feature
learning regime, for which gradient descent is implicitly biased towards simple
solutions. This work provides a general and quantitative description of the
early alignment phase, originally introduced by Maennel et al. (2018) . For
small initialisation and one hidden ReLU layer networks, the early stage of the
training dynamics leads to an alignment of the neurons towards key directions.
This alignment induces a sparse representation of the network, which is
directly related to the implicit bias of gradient flow at convergence. This
sparsity inducing alignment however comes at the expense of difficulties in
minimising the training objective: we also provide a simple data example for
which overparameterised networks fail to converge towards global minima and
only converge to a spurious stationary point instead.</div><div><a href='http://arxiv.org/abs/2401.10791v1'>2401.10791v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03579v1")'>Deconstructing the Goldilocks Zone of Neural Network Initialization</div>
<div id='2402.03579v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T23:06:48Z</div><div>Authors: Artem Vysogorets, Anna Dawid, Julia Kempe</div><div style='padding-top: 10px; width: 80ex'>The second-order properties of the training loss have a massive impact on the
optimization dynamics of deep learning models. Fort &amp; Scherlis (2019)
discovered that a high positive curvature and local convexity of the loss
Hessian are associated with highly trainable initial points located in a region
coined the "Goldilocks zone". Only a handful of subsequent studies touched upon
this relationship, so it remains largely unexplained. In this paper, we present
a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous
neural networks. In particular, we derive the fundamental condition resulting
in non-zero positive curvature of the loss Hessian and argue that it is only
incidentally related to the initialization norm, contrary to prior beliefs.
Further, we relate high positive curvature to model confidence, low initial
loss, and a previously unknown type of vanishing cross-entropy loss gradient.
To understand the importance of positive curvature for trainability of deep
networks, we optimize both fully-connected and convolutional architectures
outside the Goldilocks zone and analyze the emergent behaviors. We find that
strong model performance is not necessarily aligned with the Goldilocks zone,
which questions the practical significance of this concept.</div><div><a href='http://arxiv.org/abs/2402.03579v1'>2402.03579v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13728v1")'>M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via
  Multiplier Induced Loss Landscape Scheduling</div>
<div id='2403.13728v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T16:38:26Z</div><div>Authors: Xudong Sun, Nutan Chen, Alexej Gossmann, Yu Xing, Carla Feistner, Emilio Dorigatt, Felix Drost, Daniele Scarcella, Lisa Beer, Carsten Marr</div><div style='padding-top: 10px; width: 80ex'>When a neural network parameterized loss function consists of many terms, the
combinatorial choice of weight multipliers during the optimization process
forms a challenging problem. To address this, we proposed a probabilistic
graphical model (PGM) for the joint model parameter and multiplier evolution
process, with a hypervolume based likelihood that promotes multi-objective
descent of each loss term. The corresponding parameter and multiplier
estimation as a sequential decision process is then cast into an optimal
control problem, where the multi-objective descent goal is dispatched
hierarchically into a series of constraint optimization sub-problems. The
sub-problem constraint automatically adapts itself according to Pareto
dominance and serves as the setpoint for the low level multiplier controller to
schedule loss landscapes via output feedback of each loss term. Our method is
multiplier-free and operates at the timescale of epochs, thus saves tremendous
computational resources compared to full training cycle multiplier tuning. We
applied it to domain invariant variational auto-encoding with 6 loss terms on
the PACS domain generalization task, and observed robust performance across a
range of controller hyperparameters, as well as different multiplier initial
conditions, outperforming other multiplier scheduling methods. We offered
modular implementation of our method, admitting custom definition of many loss
terms for applying our multi-objective hierarchical output feedback training
scheme to other deep learning fields.</div><div><a href='http://arxiv.org/abs/2403.13728v1'>2403.13728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05610v1")'>Evidence, Definitions and Algorithms regarding the Existence of
  Cohesive-Convergence Groups in Neural Network Optimization</div>
<div id='2403.05610v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T13:23:42Z</div><div>Authors: Thien An L. Nguyen</div><div style='padding-top: 10px; width: 80ex'>Understanding the convergence process of neural networks is one of the most
complex and crucial issues in the field of machine learning. Despite the close
association of notable successes in this domain with the convergence of
artificial neural networks, this concept remains predominantly theoretical. In
reality, due to the non-convex nature of the optimization problems that
artificial neural networks tackle, very few trained networks actually achieve
convergence. To expand recent research efforts on artificial-neural-network
convergence, this paper will discuss a different approach based on observations
of cohesive-convergence groups emerging during the optimization process of an
artificial neural network.</div><div><a href='http://arxiv.org/abs/2403.05610v1'>2403.05610v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06898v1")'>Always-Sparse Training by Growing Connections with Guided Stochastic
  Exploration</div>
<div id='2401.06898v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T21:32:04Z</div><div>Authors: Mike Heddes, Narayan Srinivasa, Tony Givargis, Alexandru Nicolau</div><div style='padding-top: 10px; width: 80ex'>The excessive computational requirements of modern artificial neural networks
(ANNs) are posing limitations on the machines that can run them. Sparsification
of ANNs is often motivated by time, memory and energy savings only during model
inference, yielding no benefits during training. A growing body of work is now
focusing on providing the benefits of model sparsification also during
training. While these methods greatly improve the training efficiency, the
training algorithms yielding the most accurate models still materialize the
dense weights, or compute dense gradients during training. We propose an
efficient, always-sparse training algorithm with excellent scaling to larger
and sparser models, supported by its linear time complexity with respect to the
model width during training and inference. Moreover, our guided stochastic
exploration algorithm improves over the accuracy of previous sparse training
methods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG,
and ViT models, and compare it against a range of sparsification methods.</div><div><a href='http://arxiv.org/abs/2401.06898v1'>2401.06898v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14729v1")'>Auto-Train-Once: Controller Network Guided Automatic Network Pruning
  from Scratch</div>
<div id='2403.14729v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T02:33:37Z</div><div>Authors: Xidong Wu, Shangqian Gao, Zeyu Zhang, Zhenzhen Li, Runxue Bao, Yanfu Zhang, Xiaoqian Wang, Heng Huang</div><div style='padding-top: 10px; width: 80ex'>Current techniques for deep neural network (DNN) pruning often involve
intricate multi-step processes that require domain-specific expertise, making
their widespread adoption challenging. To address the limitation, the
Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for
additional fine-tuning steps by directly training and compressing a general DNN
from scratch. Nevertheless, the static design of optimizers (in OTO) can lead
to convergence issues of local optima. In this paper, we proposed the
Auto-Train-Once (ATO), an innovative network pruning algorithm designed to
automatically reduce the computational and storage costs of DNNs. During the
model training phase, our approach not only trains the target model but also
leverages a controller network as an architecture generator to guide the
learning of target model weights. Furthermore, we developed a novel stochastic
gradient algorithm that enhances the coordination between model training and
controller network training, thereby improving pruning performance. We provide
a comprehensive convergence analysis as well as extensive experiments, and the
results show that our approach achieves state-of-the-art performance across
various model architectures (including ResNet18, ResNet34, ResNet50, ResNet56,
and MobileNetv2) on standard benchmark datasets (CIFAR-10, CIFAR-100, and
ImageNet).</div><div><a href='http://arxiv.org/abs/2403.14729v1'>2403.14729v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07688v1")'>Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of
  Neurons</div>
<div id='2403.07688v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T14:28:06Z</div><div>Authors: Simon Dufort-Labbé, Pierluca D'Oro, Evgenii Nikishin, Razvan Pascanu, Pierre-Luc Bacon, Aristide Baratin</div><div style='padding-top: 10px; width: 80ex'>When training deep neural networks, the phenomenon of $\textit{dying
neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero
during training$\unicode{x2013}$ has traditionally been viewed as undesirable,
linked with optimization challenges, and contributing to plasticity loss in
continual learning scenarios. In this paper, we reassess this phenomenon,
focusing on sparsity and pruning. By systematically exploring the impact of
various hyperparameter configurations on dying neurons, we unveil their
potential to facilitate simple yet effective structured pruning algorithms. We
introduce $\textit{Demon Pruning}$ (DemP), a method that controls the
proliferation of dead neurons, dynamically leading to network sparsity.
Achieved through a combination of noise injection on active units and a
one-cycled schedule regularization strategy, DemP stands out for its simplicity
and broad applicability. Experiments on CIFAR10 and ImageNet datasets
demonstrate that DemP surpasses existing structured pruning techniques,
showcasing superior accuracy-sparsity tradeoffs and training speedups. These
findings suggest a novel perspective on dying neurons as a valuable resource
for efficient model compression and optimization.</div><div><a href='http://arxiv.org/abs/2403.07688v1'>2403.07688v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07839v2")'>Towards Meta-Pruning via Optimal Transport</div>
<div id='2402.07839v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:50:56Z</div><div>Authors: Alexander Theus, Olin Geimer, Friedrich Wicke, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh</div><div style='padding-top: 10px; width: 80ex'>Structural pruning of neural networks conventionally relies on identifying
and discarding less important neurons, a practice often resulting in
significant accuracy loss that necessitates subsequent fine-tuning efforts.
This paper introduces a novel approach named Intra-Fusion, challenging this
prevailing pruning paradigm. Unlike existing methods that focus on designing
meaningful neuron importance metrics, Intra-Fusion redefines the overlying
pruning procedure. Through utilizing the concepts of model fusion and Optimal
Transport, we leverage an agnostically given importance metric to arrive at a
more effective sparse model representation. Notably, our approach achieves
substantial accuracy recovery without the need for resource-intensive
fine-tuning, making it an efficient and promising tool for neural network
compression.
  Additionally, we explore how fusion can be added to the pruning process to
significantly decrease the training time while maintaining competitive
performance. We benchmark our results for various networks on commonly used
datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that
the proposed Intra-Fusion approach invigorates exploration into a fresh
alternative to the predominant compression approaches. Our code is available
here: https://github.com/alexandertheus/Intra-Fusion.</div><div><a href='http://arxiv.org/abs/2402.07839v2'>2402.07839v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00084v1")'>EPSD: Early Pruning with Self-Distillation for Efficient Model
  Compression</div>
<div id='2402.00084v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T05:39:55Z</div><div>Authors: Dong Chen, Ning Liu, Yichen Zhu, Zhengping Che, Rui Ma, Fachao Zhang, Xiaofeng Mou, Yi Chang, Jian Tang</div><div style='padding-top: 10px; width: 80ex'>Neural network compression techniques, such as knowledge distillation (KD)
and network pruning, have received increasing attention. Recent work `Prune,
then Distill' reveals that a pruned student-friendly teacher network can
benefit the performance of KD. However, the conventional teacher-student
pipeline, which entails cumbersome pre-training of the teacher and complicated
compression steps, makes pruning with KD less efficient. In addition to
compressing models, recent compression techniques also emphasize the aspect of
efficiency. Early pruning demands significantly less computational cost in
comparison to the conventional pruning methods as it does not require a large
pre-trained model. Likewise, a special case of KD, known as self-distillation
(SD), is more efficient since it requires no pre-training or student-teacher
pair selection. This inspires us to collaborate early pruning with SD for
efficient model compression. In this work, we propose the framework named Early
Pruning with Self-Distillation (EPSD), which identifies and preserves
distillable weights in early pruning for a given SD task. EPSD efficiently
combines early pruning and self-distillation in a two-step process, maintaining
the pruned network's trainability for compression. Instead of a simple
combination of pruning and SD, EPSD enables the pruned network to favor SD by
keeping more distillable weights before training to ensure better distillation
of the pruned network. We demonstrated that EPSD improves the training of
pruned networks, supported by visual and quantitative analyses. Our evaluation
covered diverse benchmarks (CIFAR-10/100, Tiny-ImageNet, full ImageNet,
CUB-200-2011, and Pascal VOC), with EPSD outperforming advanced pruning and SD
techniques.</div><div><a href='http://arxiv.org/abs/2402.00084v1'>2402.00084v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00999v1")'>Distributional Dataset Distillation with Subtask Decomposition</div>
<div id='2403.00999v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T21:49:34Z</div><div>Authors: Tian Qin, Zhiwei Deng, David Alvarez-Melis</div><div style='padding-top: 10px; width: 80ex'>What does a neural network learn when training from a task-specific dataset?
Synthesizing this knowledge is the central idea behind Dataset Distillation,
which recent work has shown can be used to compress large datasets into a small
set of input-label pairs ($\textit{prototypes}$) that capture essential aspects
of the original dataset. In this paper, we make the key observation that
existing methods distilling into explicit prototypes are very often suboptimal,
incurring in unexpected storage cost from distilled labels. In response, we
propose $\textit{Distributional Dataset Distillation}$ (D3), which encodes the
data using minimal sufficient per-class statistics and paired with a decoder,
we distill dataset into a compact distributional representation that is more
memory-efficient compared to prototype-based methods. To scale up the process
of learning these representations, we propose $\textit{Federated
distillation}$, which decomposes the dataset into subsets, distills them in
parallel using sub-task experts and then re-aggregates them. We thoroughly
evaluate our algorithm on a three-dimensional metric and show that our method
achieves state-of-the-art results on TinyImageNet and ImageNet-1K.
Specifically, we outperform the prior art by $6.9\%$ on ImageNet-1K under the
storage budget of 2 images per class.</div><div><a href='http://arxiv.org/abs/2403.00999v1'>2403.00999v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11106v1")'>Self-Supervised Quantization-Aware Knowledge Distillation</div>
<div id='2403.11106v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T06:20:28Z</div><div>Authors: Kaiqi Zhao, Ming Zhao</div><div style='padding-top: 10px; width: 80ex'>Quantization-aware training (QAT) and Knowledge Distillation (KD) are
combined to achieve competitive performance in creating low-bit deep learning
models. However, existing works applying KD to QAT require tedious
hyper-parameter tuning to balance the weights of different loss terms, assume
the availability of labeled training data, and require complex, computationally
intensive training procedures for good performance. To address these
limitations, this paper proposes a novel Self-Supervised Quantization-Aware
Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and
backward dynamics of various quantization functions, making it flexible for
incorporating various QAT works. Then it formulates QAT as a co-optimization
problem that simultaneously minimizes the KL-Loss between the full-precision
and low-bit models for KD and the discretization error for quantization,
without supervision from labels. A comprehensive evaluation shows that SQAKD
substantially outperforms the state-of-the-art QAT and KD works for a variety
of model architectures. Our code is at: https://github.com/kaiqi123/SQAKD.git.</div><div><a href='http://arxiv.org/abs/2403.11106v1'>2403.11106v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09441v1")'>Adversarial Fine-tuning of Compressed Neural Networks for Joint
  Improvement of Robustness and Efficiency</div>
<div id='2403.09441v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T14:34:25Z</div><div>Authors: Hallgrimur Thorsteinsson, Valdemar J Henriksen, Tong Chen, Raghavendra Selvan</div><div style='padding-top: 10px; width: 80ex'>As deep learning (DL) models are increasingly being integrated into our
everyday lives, ensuring their safety by making them robust against adversarial
attacks has become increasingly critical. DL models have been found to be
susceptible to adversarial attacks which can be achieved by introducing small,
targeted perturbations to disrupt the input data. Adversarial training has been
presented as a mitigation strategy which can result in more robust models. This
adversarial robustness comes with additional computational costs required to
design adversarial attacks during training. The two objectives -- adversarial
robustness and computational efficiency -- then appear to be in conflict of
each other. In this work, we explore the effects of two different model
compression methods -- structured weight pruning and quantization -- on
adversarial robustness. We specifically explore the effects of fine-tuning on
compressed models, and present the trade-off between standard fine-tuning and
adversarial fine-tuning. Our results show that compression does not inherently
lead to loss in model robustness and adversarial fine-tuning of a compressed
model can yield large improvement to the robustness performance of models. We
present experiments on two benchmark datasets showing that adversarial
fine-tuning of compressed models can achieve robustness performance comparable
to adversarially trained models, while also improving computational efficiency.</div><div><a href='http://arxiv.org/abs/2403.09441v1'>2403.09441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00155v1")'>Towards Explaining Deep Neural Network Compression Through a
  Probabilistic Latent Space</div>
<div id='2403.00155v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T22:13:12Z</div><div>Authors: Mahsa Mozafari-Nia, Salimeh Yasaei Sekeh</div><div style='padding-top: 10px; width: 80ex'>Despite the impressive performance of deep neural networks (DNNs), their
computational complexity and storage space consumption have led to the concept
of network compression. While DNN compression techniques such as pruning and
low-rank decomposition have been extensively studied, there has been
insufficient attention paid to their theoretical explanation. In this paper, we
propose a novel theoretical framework that leverages a probabilistic latent
space of DNN weights and explains the optimal network sparsity by using the
information-theoretic divergence measures. We introduce new analogous projected
patterns (AP2) and analogous-in-probability projected patterns (AP3) notions
for DNNs and prove that there exists a relationship between AP3/AP2 property of
layers in the network and its performance. Further, we provide a theoretical
analysis that explains the training process of the compressed network. The
theoretical results are empirically validated through experiments conducted on
standard pre-trained benchmarks, including AlexNet, ResNet50, and VGG16, using
CIFAR10 and CIFAR100 datasets. Through our experiments, we highlight the
relationship of AP3 and AP2 properties with fine-tuning pruned DNNs and
sparsity levels.</div><div><a href='http://arxiv.org/abs/2403.00155v1'>2403.00155v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00258v1")'>"Lossless" Compression of Deep Neural Networks: A High-dimensional
  Neural Tangent Kernel Approach</div>
<div id='2403.00258v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T03:46:28Z</div><div>Authors: Lingyu Gu, Yongqi Du, Yuan Zhang, Di Xie, Shiliang Pu, Robert C. Qiu, Zhenyu Liao</div><div style='padding-top: 10px; width: 80ex'>Modern deep neural networks (DNNs) are extremely powerful; however, this
comes at the price of increased depth and having more parameters per layer,
making their training and inference more computationally challenging. In an
attempt to address this key limitation, efforts have been devoted to the
compression (e.g., sparsification and/or quantization) of these large-scale
machine learning models, so that they can be deployed on low-power IoT devices.
In this paper, building upon recent advances in neural tangent kernel (NTK) and
random matrix theory (RMT), we provide a novel compression approach to wide and
fully-connected \emph{deep} neural nets. Specifically, we demonstrate that in
the high-dimensional regime where the number of data points $n$ and their
dimension $p$ are both large, and under a Gaussian mixture model for the data,
there exists \emph{asymptotic spectral equivalence} between the NTK matrices
for a large family of DNN models. This theoretical result enables "lossless"
compression of a given DNN to be performed, in the sense that the compressed
network yields asymptotically the same NTK as the original (dense and
unquantized) network, with its weights and activations taking values
\emph{only} in $\{ 0, \pm 1 \}$ up to a scaling. Experiments on both synthetic
and real-world data are conducted to support the advantages of the proposed
compression scheme, with code available at
\url{https://github.com/Model-Compression/Lossless_Compression}.</div><div><a href='http://arxiv.org/abs/2403.00258v1'>2403.00258v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11740v1")'>Extraction of nonlinearity in neural networks and model compression with
  Koopman operator</div>
<div id='2402.11740v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T23:54:35Z</div><div>Authors: Naoki Sugishita, Kayo Kinjo, Jun Ohkubo</div><div style='padding-top: 10px; width: 80ex'>Nonlinearity plays a crucial role in deep neural networks. In this paper, we
first investigate the degree to which the nonlinearity of the neural network is
essential. For this purpose, we employ the Koopman operator, extended dynamic
mode decomposition, and the tensor-train format. The results imply that
restricted nonlinearity is enough for the classification of handwritten
numbers. Then, we propose a model compression method for deep neural networks,
which could be beneficial to handling large networks in resource-constrained
environments. Leveraging the Koopman operator, the proposed method enables us
to use linear algebra in the internal processing of neural networks. We
numerically show that the proposed method performs comparably or better than
conventional methods in highly compressed model settings for the handwritten
number recognition task.</div><div><a href='http://arxiv.org/abs/2402.11740v1'>2402.11740v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11737v1")'>Compression Repair for Feedforward Neural Networks Based on Model
  Equivalence Evaluation</div>
<div id='2402.11737v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T23:41:38Z</div><div>Authors: Zihao Mo, Yejiang Yang, Shuaizheng Lu, Weiming Xiang</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a method of repairing compressed Feedforward Neural
Networks (FNNs) based on equivalence evaluation of two neural networks. In the
repairing framework, a novel neural network equivalence evaluation method is
developed to compute the output discrepancy between two neural networks. The
output discrepancy can quantitatively characterize the output difference
produced by compression procedures. Based on the computed output discrepancy,
the repairing method first initializes a new training set for the compressed
networks to narrow down the discrepancy between the two neural networks and
improve the performance of the compressed network. Then, we repair the
compressed FNN by re-training based on the training set. We apply our developed
method to the MNIST dataset to demonstrate the effectiveness and advantages of
our proposed repair method.</div><div><a href='http://arxiv.org/abs/2402.11737v1'>2402.11737v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01089v1")'>No Free Prune: Information-Theoretic Barriers to Pruning at
  Initialization</div>
<div id='2402.01089v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T01:13:16Z</div><div>Authors: Tanishq Kumar, Kevin Luo, Mark Sellke</div><div style='padding-top: 10px; width: 80ex'>The existence of "lottery tickets" arXiv:1803.03635 at or near initialization
raises the tantalizing question of whether large models are necessary in deep
learning, or whether sparse networks can be quickly identified and trained
without ever training the dense models that contain them. However, efforts to
find these sparse subnetworks without training the dense model ("pruning at
initialization") have been broadly unsuccessful arXiv:2009.08576. We put
forward a theoretical explanation for this, based on the model's effective
parameter count, $p_\text{eff}$, given by the sum of the number of non-zero
weights in the final network and the mutual information between the sparsity
mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to
sparse networks with the usual parameter count replaced by $p_\text{eff}$,
meaning a sparse neural network which robustly interpolates noisy data requires
a heavily data-dependent mask. We posit that pruning during and after training
outputs masks with higher mutual information than those produced by pruning at
initialization. Thus two networks may have the same sparsities, but differ in
effective parameter count based on how they were trained. This suggests that
pruning near initialization may be infeasible and explains why lottery tickets
exist, but cannot be found fast (i.e. without training the full network).
Experiments on neural networks confirm that information gained during training
may indeed affect model capacity.</div><div><a href='http://arxiv.org/abs/2402.01089v1'>2402.01089v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15022v1")'>Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude
  Pruning</div>
<div id='2403.15022v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T08:11:14Z</div><div>Authors: Tausifa Jan Saleem, Ramanjit Ahuja, Surendra Prasad, Brejesh Lall</div><div style='padding-top: 10px; width: 80ex'>Lottery ticket hypothesis for deep neural networks emphasizes the importance
of initialization used to re-train the sparser networks obtained using the
iterative magnitude pruning process. An explanation for why the specific
initialization proposed by the lottery ticket hypothesis tends to work better
in terms of generalization (and training) performance has been lacking.
Moreover, the underlying principles in iterative magnitude pruning, like the
pruning of smaller magnitude weights and the role of the iterative process,
lack full understanding and explanation. In this work, we attempt to provide
insights into these phenomena by empirically studying the volume/geometry and
loss landscape characteristics of the solutions obtained at various stages of
the iterative magnitude pruning process.</div><div><a href='http://arxiv.org/abs/2403.15022v1'>2403.15022v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19262v1")'>Masks, Signs, And Learning Rate Rewinding</div>
<div id='2402.19262v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:32:02Z</div><div>Authors: Advait Gadhikar, Rebekka Burkholz</div><div style='padding-top: 10px; width: 80ex'>Learning Rate Rewinding (LRR) has been established as a strong variant of
Iterative Magnitude Pruning (IMP) to find lottery tickets in deep
overparameterized neural networks. While both iterative pruning schemes couple
structure and parameter learning, understanding how LRR excels in both aspects
can bring us closer to the design of more flexible deep learning algorithms
that can optimize diverse sets of sparse architectures. To this end, we conduct
experiments that disentangle the effect of mask learning and parameter
optimization and how both benefit from overparameterization. The ability of LRR
to flip parameter signs early and stay robust to sign perturbations seems to
make it not only more effective in mask identification but also in optimizing
diverse sets of masks, including random ones. In support of this hypothesis, we
prove in a simplified single hidden neuron setting that LRR succeeds in more
cases than IMP, as it can escape initially problematic sign configurations.</div><div><a href='http://arxiv.org/abs/2402.19262v1'>2402.19262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07968v1")'>Do Deep Neural Network Solutions Form a Star Domain?</div>
<div id='2403.07968v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T13:59:23Z</div><div>Authors: Ankit Sonthalia, Alexander Rubinstein, Ehsan Abbasnejad, Seong Joon Oh</div><div style='padding-top: 10px; width: 80ex'>Entezari et al. (2022) conjectured that neural network solution sets
reachable via stochastic gradient descent (SGD) are convex, considering
permutation invariances. This means that two independent solutions can be
connected by a linear path with low loss, given one of them is appropriately
permuted. However, current methods to test this theory often fail to eliminate
loss barriers between two independent solutions (Ainsworth et al., 2022;
Benzing et al., 2022). In this work, we conjecture that a more relaxed claim
holds: the SGD solution set is a star domain that contains a star model that is
linearly connected to all the other solutions via paths with low loss values,
modulo permutations. We propose the Starlight algorithm that finds a star model
of a given learning task. We validate our claim by showing that this star model
is linearly connected with other independently found solutions. As an
additional benefit of our study, we demonstrate better uncertainty estimates on
Bayesian Model Averaging over the obtained star domain. Code is available at
https://github.com/aktsonthalia/starlight.</div><div><a href='http://arxiv.org/abs/2403.07968v1'>2403.07968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11237v1")'>Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in
  Deep Learning</div>
<div id='2402.11237v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T10:02:22Z</div><div>Authors: Hadi M. Dolatabadi, Sarah M. Erfani, Christopher Leckie</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than
learning the intended task, they tend to draw inconclusive relationships
between their inputs and outputs. Shortcut learning is ubiquitous among many
failure cases of neural networks, and traces of this phenomenon can be seen in
their generalizability issues, domain shift, adversarial vulnerability, and
even bias towards majority groups. In this paper, we argue that this
commonality in the cause of various DNN issues creates a significant
opportunity that should be leveraged to find a unified solution for shortcut
learning. To this end, we outline the recent advances in topological data
analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified
roadmap for detecting shortcuts in deep learning. We demonstrate our arguments
by investigating the topological features of computational graphs in DNNs using
two cases of unlearnable examples and bias in decision-making as our test
studies. Our analysis of these two failure cases of DNNs reveals that finding a
unified solution for shortcut learning in DNNs is not out of reach, and TDA can
play a significant role in forming such a framework.</div><div><a href='http://arxiv.org/abs/2402.11237v1'>2402.11237v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15978v1")'>Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural
  Networks Using the Marginal Likelihood</div>
<div id='2402.15978v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T03:48:13Z</div><div>Authors: Rayen Dhahri, Alexander Immer, Betrand Charpentier, Stephan Günnemann, Vincent Fortuin</div><div style='padding-top: 10px; width: 80ex'>Neural network sparsification is a promising avenue to save computational
time and memory costs, especially in an age where many successful AI models are
becoming too large to na\"ively deploy on consumer hardware. While much work
has focused on different weight pruning criteria, the overall sparsifiability
of the network, i.e., its capacity to be pruned without quality loss, has often
been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM),
a pruning framework that highlights the effectiveness of using the Bayesian
marginal likelihood in conjunction with sparsity-inducing priors for making
neural networks more sparsifiable. Our approach implements an automatic Occam's
razor that selects the most sparsifiable model that still explains the data
well, both for structured and unstructured sparsification. In addition, we
demonstrate that the pre-computed posterior Hessian approximation used in the
Laplace approximation can be re-used to define a cheap pruning criterion, which
outperforms many existing (more expensive) approaches. We demonstrate the
effectiveness of our framework, especially at high sparsity levels, across a
range of different neural network architectures and datasets.</div><div><a href='http://arxiv.org/abs/2402.15978v1'>2402.15978v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08830v1")'>Stochastic Subnetwork Annealing: A Regularization Technique for Fine
  Tuning Pruned Subnetworks</div>
<div id='2401.08830v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T21:07:04Z</div><div>Authors: Tim Whitaker, Darrell Whitley</div><div style='padding-top: 10px; width: 80ex'>Pruning methods have recently grown in popularity as an effective way to
reduce the size and computational complexity of deep neural networks. Large
numbers of parameters can be removed from trained models with little
discernible loss in accuracy after a small number of continued training epochs.
However, pruning too many parameters at once often causes an initial steep drop
in accuracy which can undermine convergence quality. Iterative pruning
approaches mitigate this by gradually removing a small number of parameters
over multiple epochs. However, this can still lead to subnetworks that overfit
local regions of the loss landscape. We introduce a novel and effective
approach to tuning subnetworks through a regularization technique we call
Stochastic Subnetwork Annealing. Instead of removing parameters in a discrete
manner, we instead represent subnetworks with stochastic masks where each
parameter has a probabilistic chance of being included or excluded on any given
forward pass. We anneal these probabilities over time such that subnetwork
structure slowly evolves as mask values become more deterministic, allowing for
a smoother and more robust optimization of subnetworks at high levels of
sparsity.</div><div><a href='http://arxiv.org/abs/2401.08830v1'>2401.08830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07498v1")'>Accelerated Smoothing: A Scalable Approach to Randomized Smoothing</div>
<div id='2402.07498v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T09:07:54Z</div><div>Authors: Devansh Bhardwaj, Kshitiz Kaushik, Sarthak Gupta</div><div style='padding-top: 10px; width: 80ex'>Randomized smoothing has emerged as a potent certifiable defense against
adversarial attacks by employing smoothing noises from specific distributions
to ensure the robustness of a smoothed classifier. However, the utilization of
Monte Carlo sampling in this process introduces a compute-intensive element,
which constrains the practicality of randomized smoothing on a larger scale. To
address this limitation, we propose a novel approach that replaces Monte Carlo
sampling with the training of a surrogate neural network. Through extensive
experimentation in various settings, we demonstrate the efficacy of our
approach in approximating the smoothed classifier with remarkable precision.
Furthermore, we demonstrate that our approach significantly accelerates the
robust radius certification process, providing nearly $600$X improvement in
computation time, overcoming the computational bottlenecks associated with
traditional randomized smoothing.</div><div><a href='http://arxiv.org/abs/2402.07498v1'>2402.07498v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17902v1")'>SequentialAttention++ for Block Sparsification: Differentiable Pruning
  Meets Combinatorial Optimization</div>
<div id='2402.17902v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:42:18Z</div><div>Authors: Taisuke Yasuda, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni, Vahab Mirrokni</div><div style='padding-top: 10px; width: 80ex'>Neural network pruning is a key technique towards engineering large yet
scalable, interpretable, and generalizable models. Prior work on the subject
has developed largely along two orthogonal directions: (1) differentiable
pruning for efficiently and accurately scoring the importance of parameters,
and (2) combinatorial optimization for efficiently searching over the space of
sparse models. We unite the two approaches, both theoretically and empirically,
to produce a coherent framework for structured neural network pruning in which
differentiable pruning guides combinatorial optimization algorithms to select
the most important sparse set of parameters. Theoretically, we show how many
existing differentiable pruning techniques can be understood as nonconvex
regularization for group sparse optimization, and prove that for a wide class
of nonconvex regularizers, the global optimum is unique, group-sparse, and
provably yields an approximate solution to a sparse convex optimization
problem. The resulting algorithm that we propose, SequentialAttention++,
advances the state of the art in large-scale neural network block-wise pruning
tasks on the ImageNet and Criteo datasets.</div><div><a href='http://arxiv.org/abs/2402.17902v1'>2402.17902v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12419v1")'>EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs</div>
<div id='2402.12419v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:55:32Z</div><div>Authors: Song Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, Rongrong Ji</div><div style='padding-top: 10px; width: 80ex'>Existing methods for fine-tuning sparse LLMs often suffer from
resource-intensive requirements and high retraining costs. Additionally, many
fine-tuning methods often rely on approximations or heuristic optimization
strategies, which may lead to suboptimal solutions. To address these issues, we
propose an efficient and fast framework for fine-tuning sparse LLMs based on
minimizing reconstruction error. Our approach involves sampling a small dataset
for calibration and utilizing backpropagation to iteratively optimize
block-wise reconstruction error, on a block-by-block basis, aiming for optimal
solutions. Extensive experiments on various benchmarks consistently demonstrate
the superiority of our method over other baselines. For instance, on the
Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a
perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of
75.14. Moreover, with a structured sparsity ratio of 26\%, EBFT achieves a
perplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the
fine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,
and the entire framework can be executed on a single 16GB GPU. The source code
is available at https://github.com/sunggo/EBFT.</div><div><a href='http://arxiv.org/abs/2402.12419v1'>2402.12419v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14225v1")'>Posterior concentrations of fully-connected Bayesian neural networks
  with general priors on the weights</div>
<div id='2403.14225v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T08:31:36Z</div><div>Authors: Insung Kong, Yongdai Kim</div><div style='padding-top: 10px; width: 80ex'>Bayesian approaches for training deep neural networks (BNNs) have received
significant interest and have been effectively utilized in a wide range of
applications. There have been several studies on the properties of posterior
concentrations of BNNs. However, most of these studies only demonstrate results
in BNN models with sparse or heavy-tailed priors. Surprisingly, no theoretical
results currently exist for BNNs using Gaussian priors, which are the most
commonly used one. The lack of theory arises from the absence of approximation
results of Deep Neural Networks (DNNs) that are non-sparse and have bounded
parameters. In this paper, we present a new approximation theory for non-sparse
DNNs with bounded parameters. Additionally, based on the approximation theory,
we show that BNNs with non-sparse general priors can achieve near-minimax
optimal posterior concentration rates to the true model.</div><div><a href='http://arxiv.org/abs/2403.14225v1'>2403.14225v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13106v1")'>On Generalization Bounds for Deep Compound Gaussian Neural Networks</div>
<div id='2402.13106v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T16:01:39Z</div><div>Authors: Carter Lyons, Raghu G. Raj, Margaret Cheney</div><div style='padding-top: 10px; width: 80ex'>Algorithm unfolding or unrolling is the technique of constructing a deep
neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide
better interpretability and superior empirical performance over standard DNNs
in signal estimation tasks. An important theoretical question, which has only
recently received attention, is the development of generalization error bounds
for unrolled DNNs. These bounds deliver theoretical and practical insights into
the performance of a DNN on empirical datasets that are distinct from, but
sampled from, the probability density generating the DNN training data. In this
paper, we develop novel generalization error bounds for a class of unrolled
DNNs that are informed by a compound Gaussian prior. These compound Gaussian
networks have been shown to outperform comparative standard and unfolded deep
neural networks in compressive sensing and tomographic imaging problems. The
generalization error bound is formulated by bounding the Rademacher complexity
of the class of compound Gaussian network estimates with Dudley's integral.
Under realistic conditions, we show that, at worst, the generalization error
scales $\mathcal{O}(n\sqrt{\ln(n)})$ in the signal dimension and
$\mathcal{O}(($Network Size$)^{3/2})$ in network size.</div><div><a href='http://arxiv.org/abs/2402.13106v1'>2402.13106v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03542v3")'>DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE
  Pre-Training</div>
<div id='2403.03542v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:38:34Z</div><div>Authors: Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>Pre-training has been investigated to improve the efficiency and performance
of training neural operators in data-scarce settings. However, it is largely in
its infancy due to the inherent complexity and diversity, such as long
trajectories, multiple scales and varying dimensions of partial differential
equations (PDEs) data. In this paper, we present a new auto-regressive
denoising pre-training strategy, which allows for more stable and efficient
pre-training on PDE data and generalizes to various downstream tasks. Moreover,
by designing a flexible and scalable model architecture based on Fourier
attention, we can easily scale up the model for large-scale pre-training. We
train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets
with more than 100k trajectories. Extensive experiments show that we achieve
SOTA on these benchmarks and validate the strong generalizability of our model
to significantly enhance performance on diverse downstream PDE tasks like 3D
data. Code is available at \url{https://github.com/thu-ml/DPOT}.</div><div><a href='http://arxiv.org/abs/2403.03542v3'>2403.03542v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03104v1")'>When To Grow? A Fitting Risk-Aware Policy for Layer Growing in Deep
  Neural Networks</div>
<div id='2401.03104v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T01:15:51Z</div><div>Authors: Haihang Wu, Wei Wang, Tamasha Malepathirana, Damith Senanayake, Denny Oetomo, Saman Halgamuge</div><div style='padding-top: 10px; width: 80ex'>Neural growth is the process of growing a small neural network to a large
network and has been utilized to accelerate the training of deep neural
networks. One crucial aspect of neural growth is determining the optimal growth
timing. However, few studies investigate this systematically. Our study reveals
that neural growth inherently exhibits a regularization effect, whose intensity
is influenced by the chosen policy for growth timing. While this regularization
effect may mitigate the overfitting risk of the model, it may lead to a notable
accuracy drop when the model underfits. Yet, current approaches have not
addressed this issue due to their lack of consideration of the regularization
effect from neural growth. Motivated by these findings, we propose an
under/over fitting risk-aware growth timing policy, which automatically adjusts
the growth timing informed by the level of potential under/overfitting risks to
address both risks. Comprehensive experiments conducted using CIFAR-10/100 and
ImageNet datasets show that the proposed policy achieves accuracy improvements
of up to 1.3% in models prone to underfitting while achieving similar
accuracies in models suffering from overfitting compared to the existing
methods.</div><div><a href='http://arxiv.org/abs/2401.03104v1'>2401.03104v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16720v1")'>SmartFRZ: An Efficient Training Framework using Attention-Based Layer
  Freezing</div>
<div id='2401.16720v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T03:34:27Z</div><div>Authors: Sheng Li, Geng Yuan, Yue Dai, Youtao Zhang, Yanzhi Wang, Xulong Tang</div><div style='padding-top: 10px; width: 80ex'>There has been a proliferation of artificial intelligence applications, where
model training is key to promising high-quality services for these
applications. However, the model training process is both time-intensive and
energy-intensive, inevitably affecting the user's demand for application
efficiency. Layer freezing, an efficient model training technique, has been
proposed to improve training efficiency. Although existing layer freezing
methods demonstrate the great potential to reduce model training costs, they
still remain shortcomings such as lacking generalizability and compromised
accuracy. For instance, existing layer freezing methods either require the
freeze configurations to be manually defined before training, which does not
apply to different networks, or use heuristic freezing criteria that is hard to
guarantee decent accuracy in different scenarios. Therefore, there lacks a
generic and smart layer freezing method that can automatically perform
``in-situation'' layer freezing for different networks during training
processes. To this end, we propose a generic and efficient training framework
(SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer
freezing, which can automatically select the appropriate layers to freeze
without compromising accuracy. Experimental results show that SmartFRZ
effectively reduces the amount of computation in training and achieves
significant training acceleration, and outperforms the state-of-the-art layer
freezing approaches.</div><div><a href='http://arxiv.org/abs/2401.16720v1'>2401.16720v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04161v3")'>SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS</div>
<div id='2403.04161v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T02:40:42Z</div><div>Authors: Yameng Peng, Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun Chang</div><div style='padding-top: 10px; width: 80ex'>Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid
resource-intensive neural network training, especially in Neural Architecture
Search (NAS). Recent studies show that existing training-free metrics have
several limitations, such as limited correlation and poor generalisation across
different search spaces and tasks. Hence, we propose Sample-Wise Activation
Patterns and its derivative, SWAP-Score, a novel high-performance training-free
metric. It measures the expressivity of networks over a batch of input samples.
The SWAP-Score is strongly correlated with ground-truth performance across
various search spaces and tasks, outperforming 15 existing training-free
metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be
further enhanced by regularisation, which leads to even higher correlations in
cell-based search space and enables model size control during the search. For
example, Spearman's rank correlation coefficient between regularised SWAP-Score
and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90,
significantly higher than 0.80 from the second-best metric, NWOT. When
integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves
competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and
9 minutes of GPU time respectively.</div><div><a href='http://arxiv.org/abs/2403.04161v3'>2403.04161v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05123v1")'>ECToNAS: Evolutionary Cross-Topology Neural Architecture Search</div>
<div id='2403.05123v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T07:36:46Z</div><div>Authors: Elisabeth J. Schiessler, Roland C. Aydin, Christian J. Cyron</div><div style='padding-top: 10px; width: 80ex'>We present ECToNAS, a cost-efficient evolutionary cross-topology neural
architecture search algorithm that does not require any pre-trained meta
controllers. Our framework is able to select suitable network architectures for
different tasks and hyperparameter settings, independently performing
cross-topology optimisation where required. It is a hybrid approach that fuses
training and topology optimisation together into one lightweight,
resource-friendly process. We demonstrate the validity and power of this
approach with six standard data sets (CIFAR-10, CIFAR-100, EuroSAT, Fashion
MNIST, MNIST, SVHN), showcasing the algorithm's ability to not only optimise
the topology within an architectural type, but also to dynamically add and
remove convolutional cells when and where required, thus crossing boundaries
between different network types. This enables researchers without a background
in machine learning to make use of appropriate model types and topologies and
to apply machine learning methods in their domains, with a computationally
cheap, easy-to-use cross-topology neural architecture search framework that
fully encapsulates the topology optimisation within the training process.</div><div><a href='http://arxiv.org/abs/2403.05123v1'>2403.05123v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02484v1")'>Encodings for Prediction-based Neural Architecture Search</div>
<div id='2403.02484v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T21:05:52Z</div><div>Authors: Yash Akhauri, Mohamed S. Abdelfattah</div><div style='padding-top: 10px; width: 80ex'>Predictor-based methods have substantially enhanced Neural Architecture
Search (NAS) optimization. The efficacy of these predictors is largely
influenced by the method of encoding neural network architectures. While
traditional encodings used an adjacency matrix describing the graph structure
of a neural network, novel encodings embrace a variety of approaches from
unsupervised pretraining of latent representations to vectors of zero-cost
proxies. In this paper, we categorize and investigate neural encodings from
three main types: structural, learned, and score-based. Furthermore, we extend
these encodings and introduce \textit{unified encodings}, that extend NAS
predictors to multiple search spaces. Our analysis draws from experiments
conducted on over 1.5 million neural network architectures on NAS spaces such
as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and
TransNASBench-101. Building on our study, we present our predictor
\textbf{FLAN}: \textbf{Fl}ow \textbf{A}ttention for \textbf{N}AS. FLAN
integrates critical insights on predictor design, transfer learning, and
\textit{unified encodings} to enable more than an order of magnitude cost
reduction for training NAS accuracy predictors. Our implementation and
encodings for all neural networks are open-sourced at
\href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan\_nas}.</div><div><a href='http://arxiv.org/abs/2403.02484v1'>2403.02484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18213v1")'>Multi-objective Differentiable Neural Architecture Search</div>
<div id='2402.18213v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T10:09:04Z</div><div>Authors: Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley, Josif Grabocka, Frank Hutter</div><div style='padding-top: 10px; width: 80ex'>Pareto front profiling in multi-objective optimization (MOO), i.e. finding a
diverse set of Pareto optimal solutions, is challenging, especially with
expensive objectives like neural network training. Typically, in MOO neural
architecture search (NAS), we aim to balance performance and hardware metrics
across devices. Prior NAS approaches simplify this task by incorporating
hardware constraints into the objective function, but profiling the Pareto
front necessitates a search for each constraint. In this work, we propose a
novel NAS algorithm that encodes user preferences for the trade-off between
performance and hardware metrics, and yields representative and diverse
architectures across multiple devices in just one search run. To this end, we
parameterize the joint architectural distribution across devices and multiple
objectives via a hypernetwork that can be conditioned on hardware features and
preference vectors, enabling zero-shot transferability to new devices.
Extensive experiments with up to 19 hardware devices and 3 objectives showcase
the effectiveness and scalability of our method. Finally, we show that, without
additional costs, our method outperforms existing MOO NAS methods across
qualitatively different search spaces and datasets, including MobileNetV3 on
ImageNet-1k and a Transformer space on machine translation.</div><div><a href='http://arxiv.org/abs/2402.18213v1'>2402.18213v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02446v1")'>On Latency Predictors for Neural Architecture Search</div>
<div id='2403.02446v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:59:32Z</div><div>Authors: Yash Akhauri, Mohamed S. Abdelfattah</div><div style='padding-top: 10px; width: 80ex'>Efficient deployment of neural networks (NN) requires the co-optimization of
accuracy and latency. For example, hardware-aware neural architecture search
has been used to automatically find NN architectures that satisfy a latency
constraint on a specific hardware device. Central to these search algorithms is
a prediction model that is designed to provide a hardware latency estimate for
a candidate NN architecture. Recent research has shown that the sample
efficiency of these predictive models can be greatly improved through
pre-training on some \textit{training} devices with many samples, and then
transferring the predictor on the \textit{test} (target) device. Transfer
learning and meta-learning methods have been used for this, but often exhibit
significant performance variability. Additionally, the evaluation of existing
latency predictors has been largely done on hand-crafted training/test device
sets, making it difficult to ascertain design features that compose a robust
and general latency predictor. To address these issues, we introduce a
comprehensive suite of latency prediction tasks obtained in a principled way
through automated partitioning of hardware device sets. We then design a
general latency predictor to comprehensively study (1) the predictor
architecture, (2) NN sample selection methods, (3) hardware device
representations, and (4) NN operation encoding schemes. Building on conclusions
from our study, we present an end-to-end latency predictor training strategy
that outperforms existing methods on 11 out of 12 difficult latency prediction
tasks, improving latency prediction by 22.5\% on average, and up to to 87.6\%
on the hardest tasks. Focusing on latency prediction, our HW-Aware NAS reports
a $5.8\times$ speedup in wall-clock time. Our code is available on
\href{https://github.com/abdelfattah-lab/nasflat_latency}{https://github.com/abdelfattah-lab/nasflat\_latency}.</div><div><a href='http://arxiv.org/abs/2403.02446v1'>2403.02446v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13204v1")'>SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural
  Architecture Search</div>
<div id='2402.13204v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:15:11Z</div><div>Authors: Halima Bouzidi, Smail Niar, Hamza Ouarnoughi, El-Ghazali Talbi</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in Artificial Intelligence (AI), driven by Neural
Networks (NN), demand innovative neural architecture designs, particularly
within the constrained environments of Internet of Things (IoT) systems, to
balance performance and efficiency. HW-aware Neural Architecture Search
(HW-aware NAS) emerges as an attractive strategy to automate the design of NN
using multi-objective optimization approaches, such as evolutionary algorithms.
However, the intricate relationship between NN design parameters and HW-aware
NAS optimization objectives remains an underexplored research area, overlooking
opportunities to effectively leverage this knowledge to guide the search
process accordingly. Furthermore, the large amount of evaluation data produced
during the search holds untapped potential for refining the optimization
strategy and improving the approximation of the Pareto front. Addressing these
issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware
NAS. Our method leverages adaptive evolutionary operators guided by the learned
importance of NN design parameters. Specifically, through tree-based surrogate
models and a Reinforcement Learning agent, we aspire to gather knowledge on
'How' and 'When' to evolve NN architectures. Comprehensive evaluations across
various NAS search spaces and hardware devices on the ImageNet-1k dataset have
shown the merit of SONATA with up to 0.25% improvement in accuracy and up to
2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Pareto
dominance over the native NSGA-II, further stipulating the importance of
self-adaptive evolution operators in HW-aware NAS.</div><div><a href='http://arxiv.org/abs/2402.13204v1'>2402.13204v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07035v1")'>Multiple Population Alternate Evolution Neural Architecture Search</div>
<div id='2403.07035v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T08:05:01Z</div><div>Authors: Juan Zou, Han Chu, Yizhang Xia, Junwen Xu, Yuan Liu, Zhanglu Hou</div><div style='padding-top: 10px; width: 80ex'>The effectiveness of Evolutionary Neural Architecture Search (ENAS) is
influenced by the design of the search space. Nevertheless, common methods
including the global search space, scalable search space and hierarchical
search space have certain limitations. Specifically, the global search space
requires a significant amount of computational resources and time, the scalable
search space sacrifices the diversity of network structures and the
hierarchical search space increases the search cost in exchange for network
diversity. To address above limitation, we propose a novel paradigm of
searching neural network architectures and design the Multiple Population
Alternate Evolution Neural Architecture Search (MPAE), which can achieve module
diversity with a smaller search cost. MPAE converts the search space into L
interconnected units and sequentially searches the units, then the above search
of the entire network be cycled several times to reduce the impact of previous
units on subsequent units. To accelerate the population evolution process, we
also propose the the population migration mechanism establishes an excellent
migration archive and transfers the excellent knowledge and experience in the
migration archive to new populations. The proposed method requires only 0.3 GPU
days to search a neural network on the CIFAR dataset and achieves the
state-of-the-art results.</div><div><a href='http://arxiv.org/abs/2403.07035v1'>2403.07035v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11173v1")'>Multi-Objective Evolutionary Neural Architecture Search for Recurrent
  Neural Networks</div>
<div id='2403.11173v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T11:19:45Z</div><div>Authors: Reinhard Booysen, Anna Sergeevna Bosman</div><div style='padding-top: 10px; width: 80ex'>Artificial neural network (NN) architecture design is a nontrivial and
time-consuming task that often requires a high level of human expertise. Neural
architecture search (NAS) serves to automate the design of NN architectures and
has proven to be successful in automatically finding NN architectures that
outperform those manually designed by human experts. NN architecture
performance can be quantified based on multiple objectives, which include model
accuracy and some NN architecture complexity objectives, among others. The
majority of modern NAS methods that consider multiple objectives for NN
architecture performance evaluation are concerned with automated feed forward
NN architecture design, which leaves multi-objective automated recurrent neural
network (RNN) architecture design unexplored. RNNs are important for modeling
sequential datasets, and prominent within the natural language processing
domain. It is often the case in real world implementations of machine learning
and NNs that a reasonable trade-off is accepted for marginally reduced model
accuracy in favour of lower computational resources demanded by the model. This
paper proposes a multi-objective evolutionary algorithm-based RNN architecture
search method. The proposed method relies on approximate network morphisms for
RNN architecture complexity optimisation during evolution. The results show
that the proposed method is capable of finding novel RNN architectures with
comparable performance to state-of-the-art manually designed RNN architectures,
but with reduced computational demand.</div><div><a href='http://arxiv.org/abs/2403.11173v1'>2403.11173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13330v1")'>NACHOS: Neural Architecture Search for Hardware Constrained Early Exit
  Neural Networks</div>
<div id='2401.13330v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T09:48:12Z</div><div>Authors: Matteo Gambella, Jary Pomponi, Simone Scardapane, Manuel Roveri</div><div style='padding-top: 10px; width: 80ex'>Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)
with Early Exit Classifiers (EECs), to provide predictions at intermediate
points of the processing when enough confidence in classification is achieved.
This leads to many benefits in terms of effectiveness and efficiency.
Currently, the design of EENNs is carried out manually by experts, a complex
and time-consuming task that requires accounting for many aspects, including
the correct placement, the thresholding, and the computational overhead of the
EECs. For this reason, the research is exploring the use of Neural Architecture
Search (NAS) to automatize the design of EENNs. Currently, few comprehensive
NAS solutions for EENNs have been proposed in the literature, and a fully
automated, joint design strategy taking into consideration both the backbone
and the EECs remains an open problem. To this end, this work presents Neural
Architecture Search for Hardware Constrained Early Exit Neural Networks
(NACHOS), the first NAS framework for the design of optimal EENNs satisfying
constraints on the accuracy and the number of Multiply and Accumulate (MAC)
operations performed by the EENNs at inference time. In particular, this
provides the joint design of backbone and EECs to select a set of admissible
(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best
tradeoff between the accuracy and number of MACs. The results show that the
models designed by NACHOS are competitive with the state-of-the-art EENNs.
Additionally, this work investigates the effectiveness of two novel
regularization terms designed for the optimization of the auxiliary classifiers
of the EENN</div><div><a href='http://arxiv.org/abs/2401.13330v1'>2401.13330v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10686v1")'>AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs</div>
<div id='2403.10686v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T21:14:44Z</div><div>Authors: Md Rubel Ahmed, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang</div><div style='padding-top: 10px; width: 80ex'>High-level synthesis (HLS) is a design flow that leverages modern language
features and flexibility, such as complex data structures, inheritance,
templates, etc., to prototype hardware designs rapidly. However, exploring
various design space parameters can take much time and effort for hardware
engineers to meet specific design specifications. This paper proposes a novel
framework called AutoHLS, which integrates a deep neural network (DNN) with
Bayesian optimization (BO) to accelerate HLS hardware design optimization. Our
tool focuses on HLS pragma exploration and operation transformation. It
utilizes integrated DNNs to predict synthesizability within a given FPGA
resource budget. We also investigate the potential of emerging quantum neural
networks (QNNs) instead of classical DNNs for the AutoHLS pipeline. Our
experimental results demonstrate up to a 70-fold speedup in exploration time.</div><div><a href='http://arxiv.org/abs/2403.10686v1'>2403.10686v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12205v1")'>Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization</div>
<div id='2401.12205v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:46:30Z</div><div>Authors: Animesh Basak Chowdhury, Marco Romanelli, Benjamin Tan, Ramesh Karri, Siddharth Garg</div><div style='padding-top: 10px; width: 80ex'>Logic synthesis, a pivotal stage in chip design, entails optimizing chip
specifications encoded in hardware description languages like Verilog into
highly efficient implementations using Boolean logic gates. The process
involves a sequential application of logic minimization heuristics (``synthesis
recipe"), with their arrangement significantly impacting crucial metrics such
as area and delay. Addressing the challenge posed by the broad spectrum of
design complexities - from variations of past designs (e.g., adders and
multipliers) to entirely novel configurations (e.g., innovative processor
instructions) - requires a nuanced `synthesis recipe` guided by human expertise
and intuition. This study conducts a thorough examination of learning and
search techniques for logic synthesis, unearthing a surprising revelation:
pre-trained agents, when confronted with entirely novel designs, may veer off
course, detrimentally affecting the search trajectory. We present ABC-RL, a
meticulously tuned $\alpha$ parameter that adeptly adjusts recommendations from
pre-trained agents during the search process. Computed based on similarity
scores through nearest neighbor retrieval from the training dataset, ABC-RL
yields superior synthesis recipes tailored for a wide array of hardware
designs. Our findings showcase substantial enhancements in the
Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to
24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an
impressive up to 9x reduction in runtime (iso-QoR) when compared to current
state-of-the-art methodologies.</div><div><a href='http://arxiv.org/abs/2401.12205v1'>2401.12205v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07781v1")'>IR-Aware ECO Timing Optimization Using Reinforcement Learning</div>
<div id='2402.07781v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:47:08Z</div><div>Authors: Vidya A. Chhabria, Wenjing Jiang, Sachin S. Sapatnekar</div><div style='padding-top: 10px; width: 80ex'>Engineering change orders (ECOs) in late stages make minimal design fixes to
recover from timing shifts due to excessive IR drops. This paper integrates
IR-drop-aware timing analysis and ECO timing optimization using reinforcement
learning (RL). The method operates after physical design and power grid
synthesis, and rectifies IR-drop-induced timing degradation through gate
sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel
RL framework, which trains a relational graph convolutional network (R-GCN)
agent to sequentially size gates to fix timing violations. The R-GCN agent
outperforms a classical LR-only algorithm: in an open 45nm technology, it (a)
moves the Pareto front of the delay-area tradeoff curve to the left and (b)
saves runtime over the classical method by running fast inference using trained
models at iso-quality. The RL model is transferable across timing
specifications, and transferable to unseen designs with zero-shot learning or
fine tuning.</div><div><a href='http://arxiv.org/abs/2402.07781v1'>2402.07781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14236v1")'>Automated Design and Optimization of Distributed Filtering Circuits via
  Reinforcement Learning</div>
<div id='2402.14236v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T02:36:14Z</div><div>Authors: Peng Gao, Tao Yu, Fei Wang, Ru-Yue Yuan</div><div style='padding-top: 10px; width: 80ex'>Designing distributed filtering circuits (DFCs) is complex and
time-consuming, with the circuit performance relying heavily on the expertise
and experience of electronics engineers. However, manual design methods tend to
have exceedingly low-efficiency. This study proposes a novel end-to-end
automated method for fabricating circuits to improve the design of DFCs. The
proposed method harnesses reinforcement learning (RL) algorithms, eliminating
the dependence on the design experience of engineers. Thus, it significantly
reduces the subjectivity and constraints associated with circuit design. The
experimental findings demonstrate clear improvements in both design efficiency
and quality when comparing the proposed method with traditional engineer-driven
methods. In particular, the proposed method achieves superior performance when
designing complex or rapidly evolving DFCs. Furthermore, compared to existing
circuit automation design techniques, the proposed method demonstrates superior
design efficiency, highlighting the substantial potential of RL in circuit
design automation.</div><div><a href='http://arxiv.org/abs/2402.14236v1'>2402.14236v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.01851v2")'>The Power of Training: How Different Neural Network Setups Influence the
  Energy Demand</div>
<div id='2401.01851v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T17:44:17Z</div><div>Authors: Daniel Geißler, Bo Zhou, Mengxi Liu, Sungho Suh, Paul Lukowicz</div><div style='padding-top: 10px; width: 80ex'>This work offers a heuristic evaluation of the effects of variations in
machine learning training regimes and learning paradigms on the energy
consumption of computing, especially HPC hardware with a life-cycle aware
perspective. While increasing data availability and innovation in
high-performance hardware fuels the training of sophisticated models, it also
fosters the fading perception of energy consumption and carbon emission.
Therefore, the goal of this work is to raise awareness about the energy impact
of general training parameters and processes, from learning rate over batch
size to knowledge transfer. Multiple setups with different hyperparameter
configurations are evaluated on three different hardware systems. Among many
results, we have found out that even with the same model and hardware to reach
the same accuracy, improperly set training hyperparameters consume up to 5
times the energy of the optimal setup. We also extensively examined the
energy-saving benefits of learning paradigms including recycling knowledge
through pretraining and sharing knowledge through multitask training.</div><div><a href='http://arxiv.org/abs/2401.01851v2'>2401.01851v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17408v1")'>Solving Boltzmann Optimization Problems with Deep Learning</div>
<div id='2401.17408v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T19:52:02Z</div><div>Authors: Fiona Knoll, John T. Daly, Jess J. Meyer</div><div style='padding-top: 10px; width: 80ex'>Decades of exponential scaling in high performance computing (HPC) efficiency
is coming to an end. Transistor based logic in complementary metal-oxide
semiconductor (CMOS) technology is approaching physical limits beyond which
further miniaturization will be impossible. Future HPC efficiency gains will
necessarily rely on new technologies and paradigms of compute. The Ising model
shows particular promise as a future framework for highly energy efficient
computation. Ising systems are able to operate at energies approaching
thermodynamic limits for energy consumption of computation. Ising systems can
function as both logic and memory. Thus, they have the potential to
significantly reduce energy costs inherent to CMOS computing by eliminating
costly data movement. The challenge in creating Ising-based hardware is in
optimizing useful circuits that produce correct results on fundamentally
nondeterministic hardware. The contribution of this paper is a novel machine
learning approach, a combination of deep neural networks and random forests,
for efficiently solving optimization problems that minimize sources of error in
the Ising model. In addition, we provide a process to express a Boltzmann
probability optimization problem as a supervised machine learning problem.</div><div><a href='http://arxiv.org/abs/2401.17408v1'>2401.17408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01996v1")'>Mean-Field Assisted Deep Boltzmann Learning with Probabilistic Computers</div>
<div id='2401.01996v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T22:19:57Z</div><div>Authors: Shuvro Chowdhury, Shaila Niazi, Kerem Y. Camsari</div><div style='padding-top: 10px; width: 80ex'>Despite their appeal as physics-inspired, energy-based and generative nature,
general Boltzmann Machines (BM) are considered intractable to train. This
belief led to simplified models of BMs with restricted intralayer connections
or layer-by-layer training of deep BMs. Recent developments in domain-specific
hardware -- specifically probabilistic computers (p-computer) with
probabilistic bits (p-bit) -- may change established wisdom on the tractability
of deep BMs. In this paper, we show that deep and unrestricted BMs can be
trained using p-computers generating hundreds of billions of Markov Chain Monte
Carlo (MCMC) samples per second, on sparse networks developed originally for
use in D-Wave's annealers. To maximize the efficiency of learning the
p-computer, we introduce two families of Mean-Field Theory assisted learning
algorithms, or xMFTs (x = Naive and Hierarchical). The xMFTs are used to
estimate the averages and correlations during the positive phase of the
contrastive divergence (CD) algorithm and our custom-designed p-computer is
used to estimate the averages and correlations in the negative phase. A custom
Field-Programmable-Gate Array (FPGA) emulation of the p-computer architecture
takes up to 45 billion flips per second, allowing the implementation of CD-$n$
where $n$ can be of the order of millions, unlike RBMs where $n$ is typically 1
or 2. Experiments on the full MNIST dataset with the combined algorithm show
that the positive phase can be efficiently computed by xMFTs without much
degradation when the negative phase is computed by the p-computer. Our
algorithm can be used in other scalable Ising machines and its variants can be
used to train BMs, previously thought to be intractable.</div><div><a href='http://arxiv.org/abs/2401.01996v1'>2401.01996v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18746v1")'>Accelerating Computer Architecture Simulation through Machine Learning</div>
<div id='2402.18746v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T23:00:57Z</div><div>Authors: Wajid Ali, Ayaz Akram</div><div style='padding-top: 10px; width: 80ex'>This paper presents our approach to accelerate computer architecture
simulation by leveraging machine learning techniques. Traditional computer
architecture simulations are time-consuming, making it challenging to explore
different design choices efficiently. Our proposed model utilizes a combination
of application features and micro-architectural features to predict the
performance of an application. These features are derived from simulations of a
small portion of the application. We demonstrate the effectiveness of our
approach by building and evaluating a machine learning model that offers
significant speedup in architectural exploration. This model demonstrates the
ability to predict IPC values for the testing data with a root mean square
error of less than 0.1.</div><div><a href='http://arxiv.org/abs/2402.18746v1'>2402.18746v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07145v1")'>Scalable and Efficient Methods for Uncertainty Estimation and Reduction
  in Deep Learning</div>
<div id='2401.07145v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T19:30:34Z</div><div>Authors: Soyed Tuhin Ahmed</div><div style='padding-top: 10px; width: 80ex'>Neural networks (NNs) can achieved high performance in various fields such as
computer vision, and natural language processing. However, deploying NNs in
resource-constrained safety-critical systems has challenges due to uncertainty
in the prediction caused by out-of-distribution data, and hardware
non-idealities. To address the challenges of deploying NNs in
resource-constrained safety-critical systems, this paper summarizes the (4th
year) PhD thesis work that explores scalable and efficient methods for
uncertainty estimation and reduction in deep learning, with a focus on
Computation-in-Memory (CIM) using emerging resistive non-volatile memories. We
tackle the inherent uncertainties arising from out-of-distribution inputs and
hardware non-idealities, crucial in maintaining functional safety in automated
decision-making systems. Our approach encompasses problem-aware training
algorithms, novel NN topologies, and hardware co-design solutions, including
dropout-based \emph{binary} Bayesian Neural Networks leveraging spintronic
devices and variational inference techniques. These innovations significantly
enhance OOD data detection, inference accuracy, and energy efficiency, thereby
contributing to the reliability and robustness of NN implementations.</div><div><a href='http://arxiv.org/abs/2401.07145v1'>2401.07145v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04744v1")'>Testing Spintronics Implemented Monte Carlo Dropout-Based Bayesian
  Neural Networks</div>
<div id='2401.04744v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T09:42:27Z</div><div>Authors: Soyed Tuhin Ahmed, Michael Hefenbrock, Guillaume Prenat, Lorena Anghel, Mehdi B. Tahoori</div><div style='padding-top: 10px; width: 80ex'>Bayesian Neural Networks (BayNNs) can inherently estimate predictive
uncertainty, facilitating informed decision-making. Dropout-based BayNNs are
increasingly implemented in spintronics-based computation-in-memory
architectures for resource-constrained yet high-performance safety-critical
applications. Although uncertainty estimation is important, the reliability of
Dropout generation and BayNN computation is equally important for target
applications but is overlooked in existing works. However, testing BayNNs is
significantly more challenging compared to conventional NNs, due to their
stochastic nature. In this paper, we present for the first time the model of
the non-idealities of the spintronics-based Dropout module and analyze their
impact on uncertainty estimates and accuracy. Furthermore, we propose a testing
framework based on repeatability ranking for Dropout-based BayNN with up to
$100\%$ fault coverage while using only $0.2\%$ of training data as test
vectors.</div><div><a href='http://arxiv.org/abs/2401.04744v1'>2401.04744v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12416v1")'>Enhancing Reliability of Neural Networks at the Edge: Inverted
  Normalization with Stochastic Affine Transformations</div>
<div id='2401.12416v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T00:27:31Z</div><div>Authors: Soyed Tuhin Ahmed, Kamal Danouchi, Guillaume Prenat, Lorena Anghel, Mehdi B. Tahoori</div><div style='padding-top: 10px; width: 80ex'>Bayesian Neural Networks (BayNNs) naturally provide uncertainty in their
predictions, making them a suitable choice in safety-critical applications.
Additionally, their realization using memristor-based in-memory computing (IMC)
architectures enables them for resource-constrained edge applications. In
addition to predictive uncertainty, however, the ability to be inherently
robust to noise in computation is also essential to ensure functional safety.
In particular, memristor-based IMCs are susceptible to various sources of
non-idealities such as manufacturing and runtime variations, drift, and
failure, which can significantly reduce inference accuracy. In this paper, we
propose a method to inherently enhance the robustness and inference accuracy of
BayNNs deployed in IMC architectures. To achieve this, we introduce a novel
normalization layer combined with stochastic affine transformations. Empirical
results in various benchmark datasets show a graceful degradation in inference
accuracy, with an improvement of up to $58.11\%$.</div><div><a href='http://arxiv.org/abs/2401.12416v1'>2401.12416v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01458v1")'>Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint</div>
<div id='2401.01458v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T23:05:07Z</div><div>Authors: Soyed Tuhin Ahmed, Mehdi B. tahoori</div><div style='padding-top: 10px; width: 80ex'>Neural networks (NNs) are increasingly used in always-on safety-critical
applications deployed on hardware accelerators (NN-HAs) employing various
memory technologies. Reliable continuous operation of NN is essential for
safety-critical applications. During online operation, NNs are susceptible to
single and multiple permanent and soft errors due to factors such as radiation,
aging, and thermal effects. Explicit NN-HA testing methods cannot detect
transient faults during inference, are unsuitable for always-on applications,
and require extensive test vector generation and storage. Therefore, in this
paper, we propose the \emph{uncertainty fingerprint} approach representing the
online fault status of NN. Furthermore, we propose a dual head NN topology
specifically designed to produce uncertainty fingerprints and the primary
prediction of the NN in \emph{a single shot}. During the online operation, by
matching the uncertainty fingerprint, we can concurrently self-test NNs with up
to $100\%$ coverage with a low false positive rate while maintaining a similar
performance of the primary task. Compared to existing works, memory overhead is
reduced by up to $243.7$ MB, multiply and accumulate (MAC) operation is reduced
by up to $10000\times$, and false-positive rates are reduced by up to $89\%$.</div><div><a href='http://arxiv.org/abs/2401.01458v1'>2401.01458v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02946v1")'>SAFFIRA: a Framework for Assessing the Reliability of
  Systolic-Array-Based DNN Accelerators</div>
<div id='2403.02946v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:17:09Z</div><div>Authors: Mahdi Taheri, Masoud Daneshtalab, Jaan Raik, Maksim Jenihhin, Salvatore Pappalardo, Paul Jimenez, Bastien Deveautour, Alberto Bosio</div><div style='padding-top: 10px; width: 80ex'>Systolic array has emerged as a prominent architecture for Deep Neural
Network (DNN) hardware accelerators, providing high-throughput and low-latency
performance essential for deploying DNNs across diverse applications. However,
when used in safety-critical applications, reliability assessment is mandatory
to guarantee the correct behavior of DNN accelerators. While fault injection
stands out as a well-established practical and robust method for reliability
assessment, it is still a very time-consuming process. This paper addresses the
time efficiency issue by introducing a novel hierarchical software-based
hardware-aware fault injection strategy tailored for systolic array-based DNN
accelerators.</div><div><a href='http://arxiv.org/abs/2403.02946v1'>2403.02946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09509v1")'>Exploration of Activation Fault Reliability in Quantized Systolic
  Array-Based DNN Accelerators</div>
<div id='2401.09509v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T12:55:17Z</div><div>Authors: Mahdi Taheri, Natalia Cherezova, Mohammad Saeed Ansari, Maksim Jenihhin, Ali Mahani, Masoud Daneshtalab, Jaan Raik</div><div style='padding-top: 10px; width: 80ex'>The stringent requirements for the Deep Neural Networks (DNNs) accelerator's
reliability stand along with the need for reducing the computational burden on
the hardware platforms, i.e. reducing the energy consumption and execution time
as well as increasing the efficiency of DNN accelerators. Moreover, the growing
demand for specialized DNN accelerators with tailored requirements,
particularly for safety-critical applications, necessitates a comprehensive
design space exploration to enable the development of efficient and robust
accelerators that meet those requirements. Therefore, the trade-off between
hardware performance, i.e. area and delay, and the reliability of the DNN
accelerator implementation becomes critical and requires tools for analysis.
This paper presents a comprehensive methodology for exploring and enabling a
holistic assessment of the trilateral impact of quantization on model accuracy,
activation fault reliability, and hardware efficiency. A fully automated
framework is introduced that is capable of applying various quantization-aware
techniques, fault injection, and hardware implementation, thus enabling the
measurement of hardware parameters. Moreover, this paper proposes a novel
lightweight protection technique integrated within the framework to ensure the
dependable deployment of the final systolic-array-based FPGA implementation.
The experiments on established benchmarks demonstrate the analysis flow and the
profound implications of quantization on reliability, hardware performance, and
network accuracy, particularly concerning the transient faults in the network's
activations.</div><div><a href='http://arxiv.org/abs/2401.09509v1'>2401.09509v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.09479v2")'>Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep
  Learning</div>
<div id='2401.09479v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T05:45:51Z</div><div>Authors: Rahul Vishwakarma, Amin Rezaei</div><div style='padding-top: 10px; width: 80ex'>The risk of hardware Trojans being inserted at various stages of chip
production has increased in a zero-trust fabless era. To counter this, various
machine learning solutions have been developed for the detection of hardware
Trojans. While most of the focus has been on either a statistical or deep
learning approach, the limited number of Trojan-infected benchmarks affects the
detection accuracy and restricts the possibility of detecting zero-day Trojans.
To close the gap, we first employ generative adversarial networks to amplify
our data in two alternative representation modalities, a graph and a tabular,
ensuring that the dataset is distributed in a representative manner. Further,
we propose a multimodal deep learning approach to detect hardware Trojans and
evaluate the results from both early fusion and late fusion strategies. We also
estimate the uncertainty quantification metrics of each prediction for
risk-aware decision-making. The outcomes not only confirms the efficacy of our
proposed hardware Trojan detection method but also opens a new door for future
studies employing multimodality and uncertainty quantification to address other
hardware security challenges.</div><div><a href='http://arxiv.org/abs/2401.09479v2'>2401.09479v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02342v1")'>Evasive Hardware Trojan through Adversarial Power Trace</div>
<div id='2401.02342v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T16:28:15Z</div><div>Authors: Behnam Omidi, Khaled N. Khasawneh, Ihsen Alouani</div><div style='padding-top: 10px; width: 80ex'>The globalization of the Integrated Circuit (IC) supply chain, driven by
time-to-market and cost considerations, has made ICs vulnerable to hardware
Trojans (HTs). Against this threat, a promising approach is to use Machine
Learning (ML)-based side-channel analysis, which has the advantage of being a
non-intrusive method, along with efficiently detecting HTs under golden
chip-free settings. In this paper, we question the trustworthiness of ML-based
HT detection via side-channel analysis. We introduce a HT obfuscation (HTO)
approach to allow HTs to bypass this detection method. Rather than
theoretically misleading the model by simulated adversarial traces, a key
aspect of our approach is the design and implementation of adversarial noise as
part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs
and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly,
we found that HTO can be implemented with only a single transistor for ASIC
designs to generate adversarial power traces that can fool the defense with
100% efficiency. We also efficiently implemented our approach on a Spartan 6
Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii)
ring-oscillator-based design. Additionally, we assess the efficiency of
countermeasures like spectral domain analysis, and we show that an adaptive
attacker can still design evasive HTOs by constraining the design with a
spectral noise budget. In addition, while adversarial training (AT) offers
higher protection against evasive HTs, AT models suffer from a considerable
utility loss, potentially rendering them unsuitable for such security
application. We believe this research represents a significant step in
understanding and exploiting ML vulnerabilities in a hardware security context,
and we make all resources and designs openly available online:
https://dev.d18uu4lqwhbmka.amplifyapp.com</div><div><a href='http://arxiv.org/abs/2401.02342v1'>2401.02342v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17918v1")'>The Seeker's Dilemma: Realistic Formulation and Benchmarking for
  Hardware Trojan Detection</div>
<div id='2402.17918v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T22:14:01Z</div><div>Authors: Amin Sarihi, Ahmad Patooghy, Abdel-Hameed A. Badawy, Peter Jamieson</div><div style='padding-top: 10px; width: 80ex'>This work focuses on advancing security research in the hardware design space
by formally defining the realistic problem of Hardware Trojan (HT) detection.
The goal is to model HT detection more closely to the real world, i.e.,
describing the problem as "The Seeker's Dilemma" (an extension of Hide&amp;Seek on
a graph), where a detecting agent is unaware of whether circuits are infected
by HTs or not. Using this theoretical problem formulation, we create a
benchmark that consists of a mixture of HT-free and HT-infected restructured
circuits while preserving their original functionalities. The restructured
circuits are randomly infected by HTs, causing a situation where the defender
is uncertain if a circuit is infected or not. We believe that our innovative
dataset will help the community better judge the detection quality of different
methods by comparing their success rates in circuit classification. We use our
developed benchmark to evaluate three state-of-the-art HT detection tools to
show baseline results for this approach. We use Principal Component Analysis to
assess the strength of our benchmark, where we observe that some restructured
HT-infected circuits are mapped closely to HT-free circuits, leading to
significant label misclassification by detectors.</div><div><a href='http://arxiv.org/abs/2402.17918v1'>2402.17918v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01299v1")'>A Photonic Physically Unclonable Function's Resilience to
  Multiple-Valued Machine Learning Attacks</div>
<div id='2403.01299v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T19:44:19Z</div><div>Authors: Jessie M. Henderson, Elena R. Henderson, Clayton A. Harper, Hiva Shahoei, William V. Oxford, Eric C. Larson, Duncan L. MacFarlane, Mitchell A. Thornton</div><div style='padding-top: 10px; width: 80ex'>Physically unclonable functions (PUFs) identify integrated circuits using
nonlinearly-related challenge-response pairs (CRPs). Ideally, the relationship
between challenges and corresponding responses is unpredictable, even if a
subset of CRPs is known. Previous work developed a photonic PUF offering
improved security compared to non-optical counterparts. Here, we investigate
this PUF's susceptibility to Multiple-Valued-Logic-based machine learning
attacks. We find that approximately 1,000 CRPs are necessary to train models
that predict response bits better than random chance. Given the significant
challenge of acquiring a vast number of CRPs from a photonic PUF, our results
demonstrate photonic PUF resilience against such attacks.</div><div><a href='http://arxiv.org/abs/2403.01299v1'>2403.01299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02846v1")'>Machine Learning Resistant Amorphous Silicon Physically Unclonable
  Functions (PUFs)</div>
<div id='2402.02846v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:00:28Z</div><div>Authors: Velat Kilic, Neil Macfarlane, Jasper Stround, Samuel Metais, Milad Alemohammad, A. Brinton Cooper, Amy C. Foster, Mark A. Foster</div><div style='padding-top: 10px; width: 80ex'>We investigate usage of nonlinear wave chaotic amorphous silicon (a-Si)
cavities as physically unclonable functions (PUF). Machine learning attacks on
integrated electronic PUFs have been demonstrated to be very effective at
modeling PUF behavior. Such attacks on integrated a-Si photonic PUFs are
investigated through application of algorithms including linear regression,
k-nearest neighbor, decision tree ensembles (random forests and gradient
boosted trees), and deep neural networks (DNNs). We found that DNNs performed
the best among all the algorithms studied but still failed to completely break
the a-Si PUF security which we quantify through a private information metric.
Furthermore, machine learning resistance of a-Si PUFs were found to be directly
related to the strength of their nonlinear response.</div><div><a href='http://arxiv.org/abs/2402.02846v1'>2402.02846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13946v2")'>AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement
  Learning</div>
<div id='2402.13946v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T17:18:25Z</div><div>Authors: Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran</div><div style='padding-top: 10px; width: 80ex'>Machine learning has shown great promise in addressing several critical
hardware security problems. In particular, researchers have developed novel
graph neural network (GNN)-based techniques for detecting intellectual property
(IP) piracy, detecting hardware Trojans (HTs), and reverse engineering
circuits, to name a few. These techniques have demonstrated outstanding
accuracy and have received much attention in the community. However, since
these techniques are used for security applications, it is imperative to
evaluate them thoroughly and ensure they are robust and do not compromise the
security of integrated circuits.
  In this work, we propose AttackGNN, the first red-team attack on GNN-based
techniques in hardware security. To this end, we devise a novel reinforcement
learning (RL) agent that generates adversarial examples, i.e., circuits,
against the GNN-based techniques. We overcome three challenges related to
effectiveness, scalability, and generality to devise a potent RL agent. We
target five GNN-based techniques for four crucial classes of problems in
hardware security: IP piracy, detecting/localizing HTs, reverse engineering,
and hardware obfuscation. Through our approach, we craft circuits that fool all
GNNs considered in this work. For instance, to evade IP piracy detection, we
generate adversarial pirated circuits that fool the GNN-based defense into
classifying our crafted circuits as not pirated. For attacking HT localization
GNN, our attack generates HT-infested circuits that fool the defense on all
tested circuits. We obtain a similar 100% success rate against GNNs for all
classes of problems.</div><div><a href='http://arxiv.org/abs/2402.13946v2'>2402.13946v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07549v1")'>A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit
  for Analog In-Memory Computing</div>
<div id='2402.07549v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T10:30:45Z</div><div>Authors: Elena Ferro, Athanasios Vasilopoulos, Corey Lammie, Manuel Le Gallo, Luca Benini, Irem Boybat, Abu Sebastian</div><div style='padding-top: 10px; width: 80ex'>Analog In-Memory Computing (AIMC) is an emerging technology for fast and
energy-efficient Deep Learning (DL) inference. However, a certain amount of
digital post-processing is required to deal with circuit mismatches and
non-idealities associated with the memory devices. Efficient near-memory
digital logic is critical to retain the high area/energy efficiency and low
latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic
with limited parallelization capability and high latency. To overcome these
limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on
fixed-point arithmetic. It achieves competitive accuracy and higher computing
throughput than previous approaches while minimizing the area overhead.
Moreover, the NMPU supports standard DL activation steps, such as ReLU and
Batch Normalization. We perform a physical implementation of the NMPU design in
a 14 nm CMOS technology and provide detailed performance, power, and area
assessments. We validate the efficacy of the NMPU by using data from an AIMC
chip and demonstrate that a simulated AIMC system with the proposed NMPU
outperforms existing FP16-based implementations, providing 139$\times$
speed-up, 7.8$\times$ smaller area, and a competitive power consumption.
Additionally, our approach achieves an inference accuracy of 86.65 %/65.06 %,
with an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when
benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100
datasets, respectively.</div><div><a href='http://arxiv.org/abs/2402.07549v1'>2402.07549v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13082v1")'>Pruning for Improved ADC Efficiency in Crossbar-based Analog In-memory
  Accelerators</div>
<div id='2403.13082v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T18:26:45Z</div><div>Authors: Timur Ibrayev, Isha Garg, Indranil Chakraborty, Kaushik Roy</div><div style='padding-top: 10px; width: 80ex'>Deep learning has proved successful in many applications but suffers from
high computational demands and requires custom accelerators for deployment.
Crossbar-based analog in-memory architectures are attractive for acceleration
of deep neural networks (DNN), due to their high data reuse and high efficiency
enabled by combining storage and computation in memory. However, they require
analog-to-digital converters (ADCs) to communicate crossbar outputs. ADCs
consume a significant portion of energy and area of every crossbar processing
unit, thus diminishing the potential efficiency benefits. Pruning is a
well-studied technique to improve the efficiency of DNNs but requires
modifications to be effective for crossbars. In this paper, we motivate
crossbar-attuned pruning to target ADC-specific inefficiencies. This is
achieved by identifying three key properties (dubbed D.U.B.) that induce
sparsity that can be utilized to reduce ADC energy without sacrificing
accuracy. The first property ensures that sparsity translates effectively to
hardware efficiency by restricting sparsity levels to Discrete powers of 2. The
other 2 properties encourage columns in the same crossbar to achieve both
Unstructured and Balanced sparsity in order to amortize the accuracy drop. The
desired D.U.B. sparsity is then achieved by regularizing the variance of
$L_{0}$ norms of neighboring columns within the same crossbar. Our proposed
implementation allows it to be directly used in end-to-end gradient-based
training. We apply the proposed algorithm to convolutional layers of VGG11 and
ResNet18 models, trained on CIFAR-10 and ImageNet datasets, and achieve up to
7.13x and 1.27x improvement, respectively, in ADC energy with less than 1% drop
in accuracy.</div><div><a href='http://arxiv.org/abs/2403.13082v1'>2403.13082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06195v1")'>NeuSpin: Design of a Reliable Edge Neuromorphic System Based on
  Spintronics for Green AI</div>
<div id='2401.06195v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T13:27:19Z</div><div>Authors: Soyed Tuhin Ahmed, Kamal Danouchi, Guillaume Prenat, Lorena Anghel, Mehdi B. Tahoori</div><div style='padding-top: 10px; width: 80ex'>Internet of Things (IoT) and smart wearable devices for personalized
healthcare will require storing and computing ever-increasing amounts of data.
The key requirements for these devices are ultra-low-power, high-processing
capabilities, autonomy at low cost, as well as reliability and accuracy to
enable Green AI at the edge. Artificial Intelligence (AI) models, especially
Bayesian Neural Networks (BayNNs) are resource-intensive and face challenges
with traditional computing architectures due to the memory wall problem.
Computing-in-Memory (CIM) with emerging resistive memories offers a solution by
combining memory blocks and computing units for higher efficiency and lower
power consumption. However, implementing BayNNs on CIM hardware, particularly
with spintronic technologies, presents technical challenges due to variability
and manufacturing defects. The NeuSPIN project aims to address these challenges
through full-stack hardware and software co-design, developing novel
algorithmic and circuit design approaches to enhance the performance,
energy-efficiency and robustness of BayNNs on sprintronic-based CIM platforms.</div><div><a href='http://arxiv.org/abs/2401.06195v1'>2401.06195v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07387v1")'>Optimising network interactions through device agnostic models</div>
<div id='2401.07387v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T22:46:53Z</div><div>Authors: Luca Manneschi, Ian T. Vidamour, Kilian D. Stenning, Jack C. Gartside, Charles Swindells, Guru Venkat, David Griffin, Susan Stepney, Will R. Branford, Thomas Hayward, Matt O Ellis, Eleni Vasilaki</div><div style='padding-top: 10px; width: 80ex'>Physically implemented neural networks hold the potential to achieve the
performance of deep learning models by exploiting the innate physical
properties of devices as computational tools. This exploration of physical
processes for computation requires to also consider their intrinsic dynamics,
which can serve as valuable resources to process information. However, existing
computational methods are unable to extend the success of deep learning
techniques to parameters influencing device dynamics, which often lack a
precise mathematical description. In this work, we formulate a universal
framework to optimise interactions with dynamic physical systems in a fully
data-driven fashion. The framework adopts neural stochastic differential
equations as differentiable digital twins, effectively capturing both
deterministic and stochastic behaviours of devices. Employing differentiation
through the trained models provides the essential mathematical estimates for
optimizing a physical neural network, harnessing the intrinsic temporal
computation abilities of its physical nodes. To accurately model real devices'
behaviours, we formulated neural-SDE variants that can operate under a variety
of experimental settings. Our work demonstrates the framework's applicability
through simulations and physical implementations of interacting dynamic
devices, while highlighting the importance of accurately capturing system
stochasticity for the successful deployment of a physically defined neural
network.</div><div><a href='http://arxiv.org/abs/2401.07387v1'>2401.07387v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04337v1")'>Explainable AI for Embedded Systems Design: A Case Study of Static
  Redundant NVM Memory Write Prediction</div>
<div id='2403.04337v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T09:02:11Z</div><div>Authors: Abdoulaye Gamatié, Yuyang Wang</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the application of eXplainable Artificial
Intelligence (XAI) in the design of embedded systems using machine learning
(ML). As a case study, it addresses the challenging problem of static silent
store prediction. This involves identifying redundant memory writes based only
on static program features. Eliminating such stores enhances performance and
energy efficiency by reducing memory access and bus traffic, especially in the
presence of emerging non-volatile memory technologies. To achieve this, we
propose a methodology consisting of: 1) the development of relevant ML models
for explaining silent store prediction, and 2) the application of XAI to
explain these models. We employ two state-of-the-art model-agnostic XAI methods
to analyze the causes of silent stores. Through the case study, we evaluate the
effectiveness of the methods. We find that these methods provide explanations
for silent store predictions, which are consistent with known causes of silent
store occurrences from previous studies. Typically, this allows us to confirm
the prevalence of silent stores in operations that write the zero constant into
memory, or the absence of silent stores in operations involving loop induction
variables. This suggests the potential relevance of XAI in analyzing ML models'
decision in embedded system design. From the case study, we share some valuable
insights and pitfalls we encountered. More generally, this study aims to lay
the groundwork for future research in the emerging field of XAI for embedded
system design.</div><div><a href='http://arxiv.org/abs/2403.04337v1'>2403.04337v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06020v2")'>Multi-conditioned Graph Diffusion for Neural Architecture Search</div>
<div id='2403.06020v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:45:31Z</div><div>Authors: Rohan Asthana, Joschua Conrad, Youssef Dawoud, Maurits Ortmanns, Vasileios Belagiannis</div><div style='padding-top: 10px; width: 80ex'>Neural architecture search automates the design of neural network
architectures usually by exploring a large and thus complex architecture search
space. To advance the architecture search, we present a graph diffusion-based
NAS approach that uses discrete conditional graph diffusion processes to
generate high-performing neural network architectures. We then propose a
multi-conditioned classifier-free guidance approach applied to graph diffusion
networks to jointly impose constraints such as high accuracy and low hardware
latency. Unlike the related work, our method is completely differentiable and
requires only a single model training. In our evaluations, we show promising
results on six standard benchmarks, yielding novel and unique architectures at
a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we
demonstrate the generalisability and efficiency of our method through
experiments on ImageNet dataset.</div><div><a href='http://arxiv.org/abs/2403.06020v2'>2403.06020v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13293v1")'>Building Optimal Neural Architectures using Interpretable Knowledge</div>
<div id='2403.13293v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T04:18:38Z</div><div>Authors: Keith G. Mills, Fred X. Han, Mohammad Salameh, Shengyao Lu, Chunhua Zhou, Jiao He, Fengyu Sun, Di Niu</div><div style='padding-top: 10px; width: 80ex'>Neural Architecture Search is a costly practice. The fact that a search space
can span a vast number of design choices with each architecture evaluation
taking nontrivial overhead makes it hard for an algorithm to sufficiently
explore candidate networks. In this paper, we propose AutoBuild, a scheme which
learns to align the latent embeddings of operations and architecture modules
with the ground-truth performance of the architectures they appear in. By doing
so, AutoBuild is capable of assigning interpretable importance scores to
architecture modules, such as individual operation features and larger macro
operation sequences such that high-performance neural networks can be
constructed without any need for search. Through experiments performed on
state-of-the-art image classification, segmentation, and Stable Diffusion
models, we show that by mining a relatively small set of evaluated
architectures, AutoBuild can learn to build high-quality architectures directly
or help to reduce search space to focus on relevant areas, finding better
architectures that outperform both the original labeled ones and ones found by
search baselines. Code available at
https://github.com/Ascend-Research/AutoBuild</div><div><a href='http://arxiv.org/abs/2403.13293v1'>2403.13293v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02433v1")'>Uncertainty-Aware Perceiver</div>
<div id='2402.02433v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:11:27Z</div><div>Authors: EuiYul Song</div><div style='padding-top: 10px; width: 80ex'>The Perceiver makes few architectural assumptions about the relationship
among its inputs with quadratic scalability on its memory and computation time.
Indeed, the Perceiver model outpaces or is competitive with ResNet-50 and ViT
in terms of accuracy to some degree. However, the Perceiver does not take
predictive uncertainty and calibration into account. The Perceiver also
generalizes its performance on three datasets, three models, one evaluation
metric, and one hyper-parameter setting. Worst of all, the Perceiver's relative
performance improvement against other models is marginal. Furthermore, its
reduction of architectural prior is not substantial; is not equivalent to its
quality. Thereby, I invented five mutations of the Perceiver, the
Uncertainty-Aware Perceivers, that obtain uncertainty estimates and measured
their performance on three metrics. Experimented with CIFAR-10 and CIFAR-100,
the Uncertainty-Aware Perceivers make considerable performance enhancement
compared to the Perceiver.</div><div><a href='http://arxiv.org/abs/2402.02433v1'>2402.02433v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03781v1")'>Neural Architecture Search using Particle Swarm and Ant Colony
  Optimization</div>
<div id='2403.03781v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:23:26Z</div><div>Authors: Séamus Lankford, Diarmuid Grimes</div><div style='padding-top: 10px; width: 80ex'>Neural network models have a number of hyperparameters that must be chosen
along with their architecture. This can be a heavy burden on a novice user,
choosing which architecture and what values to assign to parameters. In most
cases, default hyperparameters and architectures are used. Significant
improvements to model accuracy can be achieved through the evaluation of
multiple architectures. A process known as Neural Architecture Search (NAS) may
be applied to automatically evaluate a large number of such architectures. A
system integrating open source tools for Neural Architecture Search (OpenNAS),
in the classification of images, has been developed as part of this research.
OpenNAS takes any dataset of grayscale, or RBG images, and generates
Convolutional Neural Network (CNN) architectures based on a range of
metaheuristics using either an AutoKeras, a transfer learning or a Swarm
Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony
Optimization (ACO) are used as the SI algorithms. Furthermore, models developed
through such metaheuristics may be combined using stacking ensembles. In the
context of this paper, we focus on training and optimizing CNNs using the Swarm
Intelligence (SI) components of OpenNAS. Two major types of SI algorithms,
namely PSO and ACO, are compared to see which is more effective in generating
higher model accuracies. It is shown, with our experimental design, that the
PSO algorithm performs better than ACO. The performance improvement of PSO is
most notable with a more complex dataset. As a baseline, the performance of
fine-tuned pre-trained models is also evaluated.</div><div><a href='http://arxiv.org/abs/2403.03781v1'>2403.03781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00865v1")'>Fast and Efficient Local Search for Genetic Programming Based Loss
  Function Learning</div>
<div id='2403.00865v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T02:20:04Z</div><div>Authors: Christian Raymond, Qi Chen, Bing Xue, Mengjie Zhang</div><div style='padding-top: 10px; width: 80ex'>In this paper, we develop upon the topic of loss function learning, an
emergent meta-learning paradigm that aims to learn loss functions that
significantly improve the performance of the models trained under them.
Specifically, we propose a new meta-learning framework for task and
model-agnostic loss function learning via a hybrid search approach. The
framework first uses genetic programming to find a set of symbolic loss
functions. Second, the set of learned loss functions is subsequently
parameterized and optimized via unrolled differentiation. The versatility and
performance of the proposed framework are empirically validated on a diverse
set of supervised learning tasks. Results show that the learned loss functions
bring improved convergence, sample efficiency, and inference performance on
tabulated, computer vision, and natural language processing problems, using a
variety of task-specific neural network architectures.</div><div><a href='http://arxiv.org/abs/2403.00865v1'>2403.00865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13572v1")'>On the Expressive Power of a Variant of the Looped Transformer</div>
<div id='2402.13572v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T07:07:54Z</div><div>Authors: Yihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li, Michael K. Ng, Zhenguo Li, Zhaoqiang Liu</div><div style='padding-top: 10px; width: 80ex'>Besides natural language processing, transformers exhibit extraordinary
performance in solving broader applications, including scientific computing and
computer vision. Previous works try to explain this from the expressive power
and capability perspectives that standard transformers are capable of
performing some algorithms. To empower transformers with algorithmic
capabilities and motivated by the recently proposed looped transformer (Yang et
al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed
Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard
transformer and vanilla looped transformer, the proposed AlgoFormer can achieve
significantly higher expressiveness in algorithm representation when using the
same number of parameters. In particular, inspired by the structure of
human-designed learning algorithms, our transformer block consists of a
pre-transformer that is responsible for task pre-processing, a looped
transformer for iterative optimization algorithms, and a post-transformer for
producing the desired results after post-processing. We provide theoretical
evidence of the expressive power of the AlgoFormer in solving some challenging
problems, mirroring human-designed algorithms. Furthermore, some theoretical
and empirical results are presented to show that the designed transformer has
the potential to be smarter than human-designed algorithms. Experimental
results demonstrate the empirical superiority of the proposed transformer in
that it outperforms the standard transformer and vanilla looped transformer in
some challenging tasks.</div><div><a href='http://arxiv.org/abs/2402.13572v1'>2402.13572v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15734v1")'>Data-Efficient Operator Learning via Unsupervised Pretraining and
  In-Context Learning</div>
<div id='2402.15734v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T06:27:33Z</div><div>Authors: Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, Michael W. Mahoney</div><div style='padding-top: 10px; width: 80ex'>Recent years have witnessed the promise of coupling machine learning methods
and physical domain-specific insight for solving scientific problems based on
partial differential equations (PDEs). However, being data-intensive, these
methods still require a large amount of PDE data. This reintroduces the need
for expensive numerical PDE solutions, partially undermining the original goal
of avoiding these expensive simulations. In this work, seeking data efficiency,
we design unsupervised pretraining and in-context learning methods for PDE
operator learning. To reduce the need for training data with simulated
solutions, we pretrain neural operators on unlabeled PDE data using
reconstruction-based proxy tasks. To improve out-of-distribution performance,
we further assist neural operators in flexibly leveraging in-context learning
methods, without incurring extra training costs or designs. Extensive empirical
evaluations on a diverse set of PDEs demonstrate that our method is highly
data-efficient, more generalizable, and even outperforms conventional
vision-pretrained models.</div><div><a href='http://arxiv.org/abs/2402.15734v1'>2402.15734v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07364v2")'>PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar
  Nonlinear Conservation Laws</div>
<div id='2401.07364v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T20:41:36Z</div><div>Authors: Liu Yang, Stanley J. Osher</div><div style='padding-top: 10px; width: 80ex'>Can we build a single large model for a wide range of PDE-related scientific
learning tasks? Can this model generalize to new PDEs, even of new forms,
without any fine-tuning? In-context operator learning and the corresponding
model In-Context Operator Networks (ICON) represent an initial exploration of
these questions. The capability of ICON regarding the first question has been
demonstrated previously. In this paper, we present a detailed methodology for
solving PDE problems with ICON, and show how a single ICON model can make
forward and reverse predictions for different equations with different strides,
provided with appropriately designed data prompts. We show the positive
evidence to the second question, i.e., ICON can generalize well to some PDEs
with new forms without any fine-tuning. This is exemplified through a study on
1D scalar nonlinear conservation laws, a family of PDEs with temporal
evolution. We also show how to broaden the range of problems that an ICON model
can address, by transforming functions and equations to ICON's capability
scope. We believe that the progress in this paper is a significant step towards
the goal of training a foundation model for PDE-related tasks under the
in-context operator learning framework.</div><div><a href='http://arxiv.org/abs/2401.07364v2'>2401.07364v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16014v1")'>Building Flexible Machine Learning Models for Scientific Computing at
  Scale</div>
<div id='2402.16014v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T07:19:01Z</div><div>Authors: Tianyu Chen, Haoyi Zhou, Ying Li, Hao Wang, Chonghan Gao, Shanghang Zhang, Jianxin Li</div><div style='padding-top: 10px; width: 80ex'>Foundation models have revolutionized knowledge acquisition across domains,
and our study introduces OmniArch, a paradigm-shifting approach designed for
building foundation models in multi-physics scientific computing. OmniArch's
pre-training involves a versatile pipeline that processes multi-physics
spatio-temporal data, casting forward problem learning into scalable
auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning
(PIRL) technique during fine-tuning ensures alignment with physical laws.
Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new
performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional
adaptability to new physics via few-shot and zero-shot learning approaches. The
model's representations further extend to inverse problem-solving, highlighting
the transformative potential of AI-enabled Scientific Computing(AI4SC)
foundation models for engineering applications and physics discovery.</div><div><a href='http://arxiv.org/abs/2402.16014v1'>2402.16014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12424v2")'>DALex: Lexicase-like Selection via Diverse Aggregation</div>
<div id='2401.12424v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T01:20:15Z</div><div>Authors: Andrew Ni, Li Ding, Lee Spector</div><div style='padding-top: 10px; width: 80ex'>Lexicase selection has been shown to provide advantages over other selection
algorithms in several areas of evolutionary computation and machine learning.
In its standard form, lexicase selection filters a population or other
collection based on randomly ordered training cases that are considered one at
a time. This iterated filtering process can be time-consuming, particularly in
settings with large numbers of training cases. In this paper, we propose a new
method that is nearly equivalent to lexicase selection in terms of the
individuals that it selects, but which does so significantly more quickly. The
new method, called DALex (for Diversely Aggregated Lexicase), selects the best
individual with respect to a weighted sum of training case errors, where the
weights are randomly sampled. This allows us to formulate the core computation
required for selection as matrix multiplication instead of recursive loops of
comparisons, which in turn allows us to take advantage of optimized and
parallel algorithms designed for matrix multiplication for speedup.
Furthermore, we show that we can interpolate between the behavior of lexicase
selection and its "relaxed" variants, such as epsilon or batch lexicase
selection, by adjusting a single hyperparameter, named "particularity
pressure," which represents the importance granted to each individual training
case. Results on program synthesis, deep learning, symbolic regression, and
learning classifier systems demonstrate that DALex achieves significant
speedups over lexicase selection and its relaxed variants while maintaining
almost identical problem-solving performance. Under a fixed computational
budget, these savings free up resources that can be directed towards increasing
population size or the number of generations, enabling the potential for
solving more difficult problems.</div><div><a href='http://arxiv.org/abs/2401.12424v2'>2401.12424v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.18443v1")'>LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs</div>
<div id='2402.18443v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T16:13:44Z</div><div>Authors: Md Hafizur Rahman, Prabuddha Chakraborty</div><div style='padding-top: 10px; width: 80ex'>Building efficient neural network architectures can be a time-consuming task
requiring extensive expert knowledge. This task becomes particularly
challenging for edge devices because one has to consider parameters such as
power consumption during inferencing, model size, inferencing speed, and CO2
emissions. In this article, we introduce a novel framework designed to
automatically discover new neural network architectures based on user-defined
parameters, an expert system, and an LLM trained on a large amount of
open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be
used by non-AI experts, does not require a predetermined neural architecture
search space, and considers a large set of edge device-specific parameters. We
implement and validate this proposed neural architecture discovery framework
using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo
and Gemini as the LLM component. We observe that the proposed framework can
rapidly (within hours) discover intricate neural network models that perform
extremely well across a diverse set of application settings defined by the
user.</div><div><a href='http://arxiv.org/abs/2402.18443v1'>2402.18443v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16694v2")'>EdgeOL: Efficient in-situ Online Learning on Edge Devices</div>
<div id='2401.16694v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T02:41:05Z</div><div>Authors: Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Chao Wu, Alex K. Jones, Jingtong Hu, Yanzhi Wang, Xulong Tang</div><div style='padding-top: 10px; width: 80ex'>Emerging applications, such as robot-assisted eldercare and object
recognition, generally employ deep learning neural networks (DNNs) and
naturally require: i) handling streaming-in inference requests and ii) adapting
to possible deployment scenario changes. Online model fine-tuning is widely
adopted to satisfy these needs. However, an inappropriate fine-tuning scheme
could involve significant energy consumption, making it challenging to deploy
on edge devices. In this paper, we propose EdgeOL, an edge online learning
framework that optimizes inference accuracy, fine-tuning execution time, and
energy efficiency through both inter-tuning and intra-tuning optimizations.
Experimental results show that, on average, EdgeOL reduces overall fine-tuning
execution time by 64%, energy consumption by 52%, and improves average
inference accuracy by 1.75% over the immediate online learning strategy.</div><div><a href='http://arxiv.org/abs/2401.16694v2'>2401.16694v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07923v1")'>The Fusion of Deep Reinforcement Learning and Edge Computing for
  Real-time Monitoring and Control Optimization in IoT Environments</div>
<div id='2403.07923v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T12:01:06Z</div><div>Authors: Jingyu Xu, Weixiang Wan, Linying Pan, Wenjian Sun, Yuxiang Liu</div><div style='padding-top: 10px; width: 80ex'>In response to the demand for real-time performance and control quality in
industrial Internet of Things (IoT) environments, this paper proposes an
optimization control system based on deep reinforcement learning and edge
computing. The system leverages cloud-edge collaboration, deploys lightweight
policy networks at the edge, predicts system states, and outputs controls at a
high frequency, enabling monitoring and optimization of industrial objectives.
Additionally, a dynamic resource allocation mechanism is designed to ensure
rational scheduling of edge computing resources, achieving global optimization.
Results demonstrate that this approach reduces cloud-edge communication
latency, accelerates response to abnormal situations, reduces system failure
rates, extends average equipment operating time, and saves costs for manual
maintenance and replacement. This ensures real-time and stable control.</div><div><a href='http://arxiv.org/abs/2403.07923v1'>2403.07923v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07905v1")'>Enhancing Kubernetes Automated Scheduling with Deep Learning and
  Reinforcement Techniques for Large-Scale Cloud Computing Optimization</div>
<div id='2403.07905v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T13:12:44Z</div><div>Authors: Zheng Xu, Yulu Gong, Yanlin Zhou, Qiaozhi Bao, Wenpin Qian</div><div style='padding-top: 10px; width: 80ex'>With the continuous expansion of the scale of cloud computing applications,
artificial intelligence technologies such as Deep Learning and Reinforcement
Learning have gradually become the key tools to solve the automated task
scheduling of large-scale cloud computing systems. Aiming at the complexity and
real-time requirement of task scheduling in large-scale cloud computing system,
this paper proposes an automatic task scheduling scheme based on deep learning
and reinforcement learning. Firstly, the deep learning technology is used to
monitor and predict the parameters in the cloud computing system in real time
to obtain the system status information. Then, combined with reinforcement
learning algorithm, the task scheduling strategy is dynamically adjusted
according to the real-time system state and task characteristics to achieve the
optimal utilization of system resources and the maximum of task execution
efficiency. This paper verifies the effectiveness and performance advantages of
the proposed scheme in experiments, and proves the potential and application
prospect of deep learning and reinforcement learning in automatic task
scheduling in large-scale cloud computing systems.</div><div><a href='http://arxiv.org/abs/2403.07905v1'>2403.07905v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17216v1")'>Application of Machine Learning Optimization in Cloud Computing Resource
  Scheduling and Management</div>
<div id='2402.17216v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T05:14:27Z</div><div>Authors: Yifan Zhang, Bo Liu, Yulu Gong, Jiaxin Huang, Jingyu Xu, Weixiang Wan</div><div style='padding-top: 10px; width: 80ex'>In recent years, cloud computing has been widely used. Cloud computing refers
to the centralized computing resources, users through the access to the
centralized resources to complete the calculation, the cloud computing center
will return the results of the program processing to the user. Cloud computing
is not only for individual users, but also for enterprise users. By purchasing
a cloud server, users do not have to buy a large number of computers, saving
computing costs. According to a report by China Economic News Network, the
scale of cloud computing in China has reached 209.1 billion yuan. At present,
the more mature cloud service providers in China are Ali Cloud, Baidu Cloud,
Huawei Cloud and so on. Therefore, this paper proposes an innovative approach
to solve complex problems in cloud computing resource scheduling and management
using machine learning optimization techniques. Through in-depth study of
challenges such as low resource utilization and unbalanced load in the cloud
environment, this study proposes a comprehensive solution, including
optimization methods such as deep learning and genetic algorithm, to improve
system performance and efficiency, and thus bring new breakthroughs and
progress in the field of cloud computing resource management.Rational
allocation of resources plays a crucial role in cloud computing. In the
resource allocation of cloud computing, the cloud computing center has limited
cloud resources, and users arrive in sequence. Each user requests the cloud
computing center to use a certain number of cloud resources at a specific time.</div><div><a href='http://arxiv.org/abs/2402.17216v1'>2402.17216v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.12116v1")'>Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target</div>
<div id='2403.12116v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T16:14:28Z</div><div>Authors: Dongshu Liu, Jérémie Laydevant, Adrien Pontlevy, Damien Querlioz, Julie Grollier</div><div style='padding-top: 10px; width: 80ex'>Current unsupervised learning methods depend on end-to-end training via deep
learning techniques such as self-supervised learning, with high computational
requirements, or employ layer-by-layer training using bio-inspired approaches
like Hebbian learning, using local learning rules incompatible with supervised
learning. Both approaches are problematic for edge AI hardware that relies on
sparse computational resources and would strongly benefit from alternating
between unsupervised and supervised learning phases - thus leveraging widely
available unlabeled data from the environment as well as labeled training
datasets. To solve this challenge, in this work, we introduce a 'self-defined
target' that uses Winner-Take-All (WTA) selectivity at the network's final
layer, complemented by regularization through biologically inspired homeostasis
mechanism. This approach, framework-agnostic and compatible with both global
(Backpropagation) and local (Equilibrium propagation) learning rules, achieves
a 97.6% test accuracy on the MNIST dataset. Furthermore, we demonstrate that
incorporating a hidden layer enhances classification accuracy and the quality
of learned features across all training methods, showcasing the advantages of
end-to-end unsupervised training. Extending to semi-supervised learning, our
method dynamically adjusts the target according to data availability, reaching
a 96.6% accuracy with just 600 labeled MNIST samples. This result highlights
our 'unsupervised target' strategy's efficacy and flexibility in scenarios
ranging from abundant to no labeled data availability.</div><div><a href='http://arxiv.org/abs/2403.12116v1'>2403.12116v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12690v2")'>LNPT: Label-free Network Pruning and Training</div>
<div id='2403.12690v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T12:49:09Z</div><div>Authors: Jinying Xiao, Ping Li, Zhe Tang, Jie Nie</div><div style='padding-top: 10px; width: 80ex'>Pruning before training enables the deployment of neural networks on smart
devices. By retaining weights conducive to generalization, pruned networks can
be accommodated on resource-constrained smart devices. It is commonly held that
the distance on weight norms between the initialized and the fully-trained
networks correlates with generalization performance. However, as we have
uncovered, inconsistency between this metric and generalization during training
processes, which poses an obstacle to determine the pruned structures on smart
devices in advance. In this paper, we introduce the concept of the learning
gap, emphasizing its accurate correlation with generalization. Experiments show
that the learning gap, in the form of feature maps from the penultimate layer
of networks, aligns with variations of generalization performance. We propose a
novel learning framework, LNPT, which enables mature networks on the cloud to
provide online guidance for network pruning and learning on smart devices with
unlabeled data. Our results demonstrate the superiority of this approach over
supervised training.</div><div><a href='http://arxiv.org/abs/2403.12690v2'>2403.12690v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04759v1")'>Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing</div>
<div id='2403.04759v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:56:33Z</div><div>Authors: Xiaofan Yu, Anthony Thomas, Ivannia Gomez Moreno, Louis Gutierrez, Tajana Rosing</div><div style='padding-top: 10px; width: 80ex'>On-device learning has emerged as a prevailing trend that avoids the slow
response time and costly communication of cloud-based learning. The ability to
learn continuously and indefinitely in a changing environment, and with
resource constraints, is critical for real sensor deployments. However,
existing designs are inadequate for practical scenarios with (i) streaming data
input, (ii) lack of supervision and (iii) limited on-board resources. In this
paper, we design and deploy the first on-device lifelong learning system called
LifeHD for general IoT applications with limited supervision. LifeHD is
designed based on a novel neurally-inspired and lightweight learning paradigm
called Hyperdimensional Computing (HDC). We utilize a two-tier associative
memory organization to intelligently store and manage high-dimensional,
low-precision vectors, which represent the historical patterns as cluster
centroids. We additionally propose two variants of LifeHD to cope with scarce
labeled inputs and power constraints. We implement LifeHD on off-the-shelf edge
platforms and perform extensive evaluations across three scenarios. Our
measurements show that LifeHD improves the unsupervised clustering accuracy by
up to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong
learning baselines with as much as 34.3x better energy efficiency. Our code is
available at https://github.com/Orienfish/LifeHD.</div><div><a href='http://arxiv.org/abs/2403.04759v1'>2403.04759v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02043v1")'>A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data
  Transmission</div>
<div id='2402.02043v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:41:39Z</div><div>Authors: Wenjun Huang, Arghavan Rezvani, Hanning Chen, Yang Ni, Sanggeon Yun, Sungheon Jeong, Mohsen Imani</div><div style='padding-top: 10px; width: 80ex'>Applications in the Internet of Things (IoT) utilize machine learning to
analyze sensor-generated data. However, a major challenge lies in the lack of
targeted intelligence in current sensing systems, leading to vast data
generation and increased computational and communication costs. To address this
challenge, we propose a novel sensing module to equip sensing frameworks with
intelligent data transmission capabilities by integrating a highly efficient
machine learning model placed near the sensor. This model provides prompt
feedback for the sensing system to transmit only valuable data while discarding
irrelevant information by regulating the frequency of data transmission. The
near-sensor model is quantized and optimized for real-time sensor control. To
enhance the framework's performance, the training process is customized and a
"lazy" sensor deactivation strategy utilizing temporal information is
introduced. The suggested method is orthogonal to other IoT frameworks and can
be considered as a plugin for selective data transmission. The framework is
implemented, encompassing both software and hardware components. The
experiments demonstrate that the framework utilizing the suggested module
achieves over 85% system efficiency in terms of energy consumption and storage,
with negligible impact on performance. This methodology has the potential to
significantly reduce data output from sensors, benefiting a wide range of IoT
applications.</div><div><a href='http://arxiv.org/abs/2402.02043v1'>2402.02043v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09068v1")'>DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning</div>
<div id='2401.09068v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T09:01:50Z</div><div>Authors: Lixiang Han, Zhen Xiao, Zhenjiang Li</div><div style='padding-top: 10px; width: 80ex'>DTMM is a library designed for efficient deployment and execution of machine
learning models on weak IoT devices such as microcontroller units (MCUs). The
motivation for designing DTMM comes from the emerging field of tiny machine
learning (TinyML), which explores extending the reach of machine learning to
many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak
capability of embedded devices, it is necessary to compress models by pruning
enough weights before deploying. Although pruning has been studied extensively
on many computing platforms, two key issues with pruning methods are
exacerbated on MCUs: models need to be deeply compressed without significantly
compromising accuracy, and they should perform efficiently after pruning.
Current solutions only achieve one of these objectives, but not both. In this
paper, we find that pruned models have great potential for efficient deployment
and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,
pre-execution pruning optimizations, runtime acceleration, and post-execution
low-cost storage to fill the gap for efficient deployment and execution of
pruned models. It can be integrated into commercial ML frameworks for practical
deployment, and a prototype system has been developed. Extensive experiments on
various models show promising gains compared to state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2401.09068v1'>2401.09068v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14120v1")'>Advancing IIoT with Over-the-Air Federated Learning: The Role of
  Iterative Magnitude Pruning</div>
<div id='2403.14120v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T04:15:56Z</div><div>Authors: Fazal Muhammad Ali Khan, Hatem Abou-Zeid, Aryan Kaushik, Syed Ali Hassan</div><div style='padding-top: 10px; width: 80ex'>The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of
interconnected smart devices where data-driven insights and machine learning
(ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is
the integration of federated learning (FL), which addresses data privacy and
security among devices. FL enables edge sensors, also known as peripheral
intelligence units (PIUs) to learn and adapt using their data locally, without
explicit sharing of confidential data, to facilitate a collaborative yet
confidential learning process. However, the lower memory footprint and
computational power of PIUs inherently require deep neural network (DNN) models
that have a very compact size. Model compression techniques such as pruning can
be used to reduce the size of DNN models by removing unnecessary connections
that have little impact on the model's performance, thus making the models more
suitable for the limited resources of PIUs. Targeting the notion of compact yet
robust DNN models, we propose the integration of iterative magnitude pruning
(IMP) of the DNN model being trained in an over-the-air FL (OTA-FL) environment
for IIoT. We provide a tutorial overview and also present a case study of the
effectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present
future directions for enhancing and optimizing these deep compression
techniques further, aiming to push the boundaries of IIoT capabilities in
acquiring compact yet robust and high-performing DNN models.</div><div><a href='http://arxiv.org/abs/2403.14120v1'>2403.14120v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10984v1")'>IoTCO2: Assessing the End-To-End Carbon Footprint of
  Internet-of-Things-Enabled Deep Learning</div>
<div id='2403.10984v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T17:32:59Z</div><div>Authors: Ahmad Faiz, Shahzeen Attari, Gayle Buck, Fan Chen, Lei Jiang</div><div style='padding-top: 10px; width: 80ex'>To improve privacy and ensure quality-of-service (QoS), deep learning (DL)
models are increasingly deployed on Internet of Things (IoT) devices for data
processing, significantly increasing the carbon footprint associated with DL on
IoT, covering both operational and embodied aspects. Existing operational
energy predictors often overlook quantized DL models and emerging neural
processing units (NPUs), while embodied carbon footprint modeling tools neglect
non-computing hardware components common in IoT devices, creating a gap in
accurate carbon footprint modeling tools for IoT-enabled DL. This paper
introduces \textit{\carb}, an end-to-end modeling tool for precise carbon
footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21\%$
deviation in carbon footprint values compared to actual measurements across
various DL models. Additionally, practical applications of \carb are showcased
through multiple user case studies.</div><div><a href='http://arxiv.org/abs/2403.10984v1'>2403.10984v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04888v1")'>RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing</div>
<div id='2402.04888v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T20:30:23Z</div><div>Authors: Borna Barahimi, Hakam Singh, Hina Tabassum, Omer Waqar, Mohammad Omer</div><div style='padding-top: 10px; width: 80ex'>WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere
communication devices to sensing instruments, leveraging Channel State
Information (CSI) extraction capabilities. Nevertheless, resource-constrained
IoT devices and the intricacies of deep neural networks necessitate
transmitting CSI to cloud servers for sensing. Although feasible, this leads to
considerable communication overhead. In this context, this paper develops a
novel Real-time Sensing and Compression Network (RSCNet) which enables sensing
with compressed CSI; thereby reducing the communication overheads. RSCNet
facilitates optimization across CSI windows composed of a few CSI frames. Once
transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to
harness data from prior windows, thus bolstering both the sensing accuracy and
CSI reconstruction. RSCNet adeptly balances the trade-off between CSI
compression and sensing precision, thus streamlining real-time cloud-based WiFi
sensing with reduced communication costs. Numerical findings demonstrate the
gains of RSCNet over the existing benchmarks like SenseFi, showcasing a sensing
accuracy of 97.4% with minimal CSI reconstruction error. Numerical results also
show a computational analysis of the proposed RSCNet as a function of the
number of CSI frames.</div><div><a href='http://arxiv.org/abs/2402.04888v1'>2402.04888v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09035v1")'>DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers</div>
<div id='2403.09035v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T02:11:38Z</div><div>Authors: Xiao Ma, Shengfeng He, Hezhe Qiao, Dong Ma</div><div style='padding-top: 10px; width: 80ex'>Enabling efficient and accurate deep neural network (DNN) inference on
microcontrollers is non-trivial due to the constrained on-chip resources.
Current methodologies primarily focus on compressing larger models yet at the
expense of model accuracy. In this paper, we rethink the problem from the
inverse perspective by constructing small/weak models directly and improving
their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference
framework with a selector-classifiers architecture, where the selector routes
each input sample to the appropriate classifier for classification. DiTMoS is
grounded on a key insight: a composition of weak models can exhibit high
diversity and the union of them can significantly boost the accuracy upper
bound. To approach the upper bound, DiTMoS introduces three strategies
including diverse training data splitting to increase the classifiers'
diversity, adversarial selector-classifiers training to ensure synergistic
interactions thereby maximizing their complementarity, and heterogeneous
feature aggregation to improve the capacity of classifiers. We further propose
a network slicing technique to alleviate the extra memory overhead incurred by
feature aggregation. We deploy DiTMoS on the Neucleo STM32F767ZI board and
evaluate it based on three time-series datasets for human activity recognition,
keywords spotting, and emotion recognition, respectively. The experiment
results manifest that: (a) DiTMoS achieves up to 13.4% accuracy improvement
compared to the best baseline; (b) network slicing almost completely eliminates
the memory overhead incurred by feature aggregation with a marginal increase of
latency.</div><div><a href='http://arxiv.org/abs/2403.09035v1'>2403.09035v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10284v1")'>MorpheusNet: Resource efficient sleep stage classifier for embedded
  on-line systems</div>
<div id='2401.10284v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T17:52:08Z</div><div>Authors: Ali Kavoosi, Morgan P. Mitchell, Raveen Kariyawasam, John E. Fleming, Penny Lewis, Heidi Johansen-Berg, Hayriye Cagnan, Timothy Denison</div><div style='padding-top: 10px; width: 80ex'>Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts
to examine hours of electrophysiological recordings for manual classification.
This is a limiting factor when it comes to leveraging sleep stages for
therapeutic purposes. With increasing affordability and expansion of wearable
devices, automating SSC may enable deployment of sleep-based therapies at
scale. Deep Learning has gained increasing attention as a potential method to
automate this process. Previous research has shown accuracy comparable to
manual expert scores. However, previous approaches require sizable amount of
memory and computational resources. This constrains the ability to classify in
real time and deploy models on the edge. To address this gap, we aim to provide
a model capable of predicting sleep stages in real-time, without requiring
access to external computational sources (e.g., mobile phone, cloud). The
algorithm is power efficient to enable use on embedded battery powered systems.
Our compact sleep stage classifier can be deployed on most off-the-shelf
microcontrollers (MCU) with constrained hardware settings. This is due to the
memory footprint of our approach requiring significantly fewer operations. The
model was tested on three publicly available data bases and achieved
performance comparable to the state of the art, whilst reducing model
complexity by orders of magnitude (up to 280 times smaller compared to state of
the art). We further optimized the model with quantization of parameters to 8
bits with only an average drop of 0.95% in accuracy. When implemented in
firmware, the quantized model achieves a latency of 1.6 seconds on an Arm
CortexM4 processor, allowing its use for on-line SSC-based therapies.</div><div><a href='http://arxiv.org/abs/2401.10284v1'>2401.10284v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13714v1")'>Value-Driven Mixed-Precision Quantization for Patch-Based Inference on
  Microcontrollers</div>
<div id='2401.13714v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T04:21:41Z</div><div>Authors: Wei Tao, Shenglin He, Kai Lu, Xiaoyang Qu, Guokuan Li, Jiguang Wan, Jianzong Wang, Jing Xiao</div><div style='padding-top: 10px; width: 80ex'>Deploying neural networks on microcontroller units (MCUs) presents
substantial challenges due to their constrained computation and memory
resources. Previous researches have explored patch-based inference as a
strategy to conserve memory without sacrificing model accuracy. However, this
technique suffers from severe redundant computation overhead, leading to a
substantial increase in execution latency. A feasible solution to address this
issue is mixed-precision quantization, but it faces the challenges of accuracy
degradation and a time-consuming search time. In this paper, we propose
QuantMCU, a novel patch-based inference method that utilizes value-driven
mixed-precision quantization to reduce redundant computation. We first utilize
value-driven patch classification (VDPC) to maintain the model accuracy. VDPC
classifies patches into two classes based on whether they contain outlier
values. For patches containing outlier values, we apply 8-bit quantization to
the feature maps on the dataflow branches that follow. In addition, for patches
without outlier values, we utilize value-driven quantization search (VDQS) on
the feature maps of their following dataflow branches to reduce search time.
Specifically, VDQS introduces a novel quantization search metric that takes
into account both computation and accuracy, and it employs entropy as an
accuracy representation to avoid additional training. VDQS also adopts an
iterative approach to determine the bitwidth of each feature map to further
accelerate the search process. Experimental results on real-world MCU devices
show that QuantMCU can reduce computation by 2.2x on average while maintaining
comparable model accuracy compared to the state-of-the-art patch-based
inference methods.</div><div><a href='http://arxiv.org/abs/2401.13714v1'>2401.13714v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05106v1")'>Simulating Battery-Powered TinyML Systems Optimised using Reinforcement
  Learning in Image-Based Anomaly Detection</div>
<div id='2403.05106v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T07:09:56Z</div><div>Authors: Jared M. Ping, Ken J. Nixon</div><div style='padding-top: 10px; width: 80ex'>Advances in Tiny Machine Learning (TinyML) have bolstered the creation of
smart industry solutions, including smart agriculture, healthcare and smart
cities. Whilst related research contributes to enabling TinyML solutions on
constrained hardware, there is a need to amplify real-world applications by
optimising energy consumption in battery-powered systems. The work presented
extends and contributes to TinyML research by optimising battery-powered
image-based anomaly detection Internet of Things (IoT) systems. Whilst previous
work in this area has yielded the capabilities of on-device inferencing and
training, there has yet to be an investigation into optimising the management
of such capabilities using machine learning approaches, such as Reinforcement
Learning (RL), to improve the deployment battery life of such systems. Using
modelled simulations, the battery life effects of an RL algorithm are
benchmarked against static and dynamic optimisation approaches, with the
foundation laid for a hardware benchmark to follow. It is shown that using RL
within a TinyML-enabled IoT system to optimise the system operations, including
cloud anomaly processing and on-device training, yields an improved battery
life of 22.86% and 10.86% compared to static and dynamic optimisation
approaches respectively. The proposed solution can be deployed to
resource-constrained hardware, given its low memory footprint of 800 B, which
could be further reduced. This further facilitates the real-world deployment of
such systems, including key sectors such as smart agriculture.</div><div><a href='http://arxiv.org/abs/2403.05106v1'>2403.05106v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10371v1")'>An Energy-Efficient Ensemble Approach for Mitigating Data Incompleteness
  in IoT Applications</div>
<div id='2403.10371v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:01:48Z</div><div>Authors: Yousef AlShehri, Lakshmish Ramaswamy</div><div style='padding-top: 10px; width: 80ex'>Machine Learning (ML) is becoming increasingly important for IoT-based
applications. However, the dynamic and ad-hoc nature of many IoT ecosystems
poses unique challenges to the efficacy of ML algorithms. One such challenge is
data incompleteness, which is manifested as missing sensor readings. Many
factors, including sensor failures and/or network disruption, can cause data
incompleteness. Furthermore, most IoT systems are severely power-constrained.
It is important that we build IoT-based ML systems that are robust against data
incompleteness while simultaneously being energy efficient. This paper presents
an empirical study of SECOE - a recent technique for alleviating data
incompleteness in IoT - with respect to its energy bottlenecks. Towards
addressing the energy bottlenecks of SECOE, we propose ENAMLE - a proactive,
energy-aware technique for mitigating the impact of concurrent missing data.
ENAMLE is unique in the sense that it builds an energy-aware ensemble of
sub-models, each trained with a subset of sensors chosen carefully based on
their correlations. Furthermore, at inference time, ENAMLE adaptively alters
the number of the ensemble of models based on the amount of missing data rate
and the energy-accuracy trade-off. ENAMLE's design includes several novel
mechanisms for minimizing energy consumption while maintaining accuracy. We
present extensive experimental studies on two distinct datasets that
demonstrate the energy efficiency of ENAMLE and its ability to alleviate sensor
failures.</div><div><a href='http://arxiv.org/abs/2403.10371v1'>2403.10371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09488v1")'>Intelligent Agricultural Greenhouse Control System Based on Internet of
  Things and Machine Learning</div>
<div id='2402.09488v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T09:07:00Z</div><div>Authors: Cangqing Wang</div><div style='padding-top: 10px; width: 80ex'>This study endeavors to conceptualize and execute a sophisticated
agricultural greenhouse control system grounded in the amalgamation of the
Internet of Things (IoT) and machine learning. Through meticulous monitoring of
intrinsic environmental parameters within the greenhouse and the integration of
machine learning algorithms, the conditions within the greenhouse are aptly
modulated. The envisaged outcome is an enhancement in crop growth efficiency
and yield, accompanied by a reduction in resource wastage. In the backdrop of
escalating global population figures and the escalating exigencies of climate
change, agriculture confronts unprecedented challenges. Conventional
agricultural paradigms have proven inadequate in addressing the imperatives of
food safety and production efficiency. Against this backdrop, greenhouse
agriculture emerges as a viable solution, proffering a controlled milieu for
crop cultivation to augment yields, refine quality, and diminish reliance on
natural resources [b1]. Nevertheless, greenhouse agriculture contends with a
gamut of challenges. Traditional greenhouse management strategies, often
grounded in experiential knowledge and predefined rules, lack targeted
personalized regulation, thereby resulting in resource inefficiencies. The
exigencies of real-time monitoring and precise control of the greenhouse's
internal environment gain paramount importance with the burgeoning scale of
agriculture. To redress this challenge, the study introduces IoT technology and
machine learning algorithms into greenhouse agriculture, aspiring to institute
an intelligent agricultural greenhouse control system conducive to augmenting
the efficiency and sustainability of agricultural production.</div><div><a href='http://arxiv.org/abs/2402.09488v1'>2402.09488v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16258v1")'>MosquIoT: A System Based on IoT and Machine Learning for the Monitoring
  of Aedes aegypti (Diptera: Culicidae)</div>
<div id='2401.16258v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T16:08:18Z</div><div>Authors: Javier Aira, Teresa Olivares Montes, Francisco M. Delicado, Darìo Vezzani</div><div style='padding-top: 10px; width: 80ex'>Millions of people around the world are infected with mosquito-borne diseases
each year. One of the most dangerous species is Aedes aegypti, the main vector
of viruses such as dengue, yellow fever, chikungunya, and Zika, among others.
Mosquito prevention and eradication campaigns are essential to avoid major
public health consequences. In this respect, entomological surveillance is an
important tool. At present, this traditional monitoring tool is executed
manually and requires digital transformation to help authorities make better
decisions, improve their planning efforts, speed up execution, and better
manage available resources. Therefore, new technological tools based on proven
techniques need to be designed and developed. However, such tools should also
be cost-effective, autonomous, reliable, and easy to implement, and should be
enabled by connectivity and multi-platform software applications. This paper
presents the design, development, and testing of an innovative system named
MosquIoT. It is based on traditional ovitraps with embedded Internet of Things
(IoT) and Tiny Machine Learning (TinyML) technologies, which enable the
detection and quantification of Ae. aegypti eggs. This innovative and promising
solution may help dynamically understand the behavior of Ae. aegypti
populations in cities, shifting from the current reactive entomological
monitoring model to a proactive and predictive digital one.</div><div><a href='http://arxiv.org/abs/2401.16258v1'>2401.16258v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.03233v2")'>Convergence Rate Maximization for Split Learning-based Control of EMG
  Prosthetic Devices</div>
<div id='2401.03233v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T15:05:49Z</div><div>Authors: Matea Marinova, Daniel Denkovski, Hristijan Gjoreski, Zoran Hadzi-Velkov, Valentin Rakovic</div><div style='padding-top: 10px; width: 80ex'>Split Learning (SL) is a promising Distributed Learning approach in
electromyography (EMG) based prosthetic control, due to its applicability
within resource-constrained environments. Other learning approaches, such as
Deep Learning and Federated Learning (FL), provide suboptimal solutions, since
prosthetic devices are extremely limited in terms of processing power and
battery life. The viability of implementing SL in such scenarios is caused by
its inherent model partitioning, with clients executing the smaller model
segment. However, selecting an inadequate cut layer hinders the training
process in SL systems. This paper presents an algorithm for optimal cut layer
selection in terms of maximizing the convergence rate of the model. The
performance evaluation demonstrates that the proposed algorithm substantially
accelerates the convergence in an EMG pattern recognition task for improving
prosthetic device control.</div><div><a href='http://arxiv.org/abs/2401.03233v2'>2401.03233v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.00306v2")'>An Accurate and Low-Parameter Machine Learning Architecture for Next
  Location Prediction</div>
<div id='2402.00306v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T03:39:15Z</div><div>Authors: Calvin Jary, Nafiseh Kahani</div><div style='padding-top: 10px; width: 80ex'>Next location prediction is a discipline that involves predicting a users
next location. Its applications include resource allocation, quality of
service, energy efficiency, and traffic management. This paper proposes an
energy-efficient, small, and low parameter machine learning (ML) architecture
for accurate next location prediction, deployable on modest base stations and
edge devices. To accomplish this we ran a hundred hyperparameter experiments on
the full human mobility patterns of an entire city, to determine an exact ML
architecture that reached a plateau of accuracy with the least amount of model
parameters. We successfully achieved a reduction in the number of model
parameters within published ML architectures from 202 million down to 2
million. This reduced the total size of the model parameters from 791 MB down
to 8 MB. Additionally, this decreased the training time by a factor of four,
the amount of graphics processing unit (GPU) memory needed for training by a
factor of twenty, and the overall accuracy was increased from 80.16% to 82.54%.
This improvement allows for modest base stations and edge devices which do not
have a large amount of memory or storage, to deploy and utilize the proposed ML
architecture for next location prediction.</div><div><a href='http://arxiv.org/abs/2402.00306v2'>2402.00306v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08081v1")'>Predicting Next Useful Location With Context-Awareness: The
  State-Of-The-Art</div>
<div id='2401.08081v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T03:15:52Z</div><div>Authors: Alireza Nezhadettehad, Arkady Zaslavsky, Rakib Abdur, Siraj Ahmed Shaikh, Seng W. Loke, Guang-Li Huang, Alireza Hassani</div><div style='padding-top: 10px; width: 80ex'>Predicting the future location of mobile objects reinforces location-aware
services with proactive intelligence and helps businesses and decision-makers
with better planning and near real-time scheduling in different applications
such as traffic congestion control, location-aware advertisements, and
monitoring public health and well-being. The recent developments in the
smartphone and location sensors technology and the prevalence of using
location-based social networks alongside the improvements in artificial
intelligence and machine learning techniques provide an excellent opportunity
to exploit massive amounts of historical and real-time contextual information
to recognise mobility patterns and achieve more accurate and intelligent
predictions. This survey provides a comprehensive overview of the next useful
location prediction problem with context-awareness. First, we explain the
concepts of context and context-awareness and define the next location
prediction problem. Then we analyse nearly thirty studies in this field
concerning the prediction method, the challenges addressed, the datasets and
metrics used for training and evaluating the model, and the types of context
incorporated. Finally, we discuss the advantages and disadvantages of different
approaches, focusing on the usefulness of the predicted location and
identifying the open challenges and future work on this subject by introducing
two potential use cases of next location prediction in the automotive industry.</div><div><a href='http://arxiv.org/abs/2401.08081v1'>2401.08081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07958v1")'>Temporal Decisions: Leveraging Temporal Correlation for Efficient
  Decisions in Early Exit Neural Networks</div>
<div id='2403.07958v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T08:28:27Z</div><div>Authors: Max Sponner, Lorenzo Servadei, Bernd Waschneck, Robert Wille, Akash Kumar</div><div style='padding-top: 10px; width: 80ex'>Deep Learning is becoming increasingly relevant in Embedded and
Internet-of-things applications. However, deploying models on embedded devices
poses a challenge due to their resource limitations. This can impact the
model's inference accuracy and latency. One potential solution are Early Exit
Neural Networks, which adjust model depth dynamically through additional
classifiers attached between their hidden layers. However, the real-time
termination decision mechanism is critical for the system's efficiency,
latency, and sustained accuracy.
  This paper introduces Difference Detection and Temporal Patience as decision
mechanisms for Early Exit Neural Networks. They leverage the temporal
correlation present in sensor data streams to efficiently terminate the
inference. We evaluate their effectiveness in health monitoring, image
classification, and wake-word detection tasks. Our novel contributions were
able to reduce the computational footprint compared to established decision
mechanisms significantly while maintaining higher accuracy scores. We achieved
a reduction of mean operations per inference by up to 80% while maintaining
accuracy levels within 5% of the original model.
  These findings highlight the importance of considering temporal correlation
in sensor data to improve the termination decision.</div><div><a href='http://arxiv.org/abs/2403.07958v1'>2403.07958v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04326v1")'>Edge-based Parametric Digital Twins for Intelligent Building Indoor
  Climate Modeling</div>
<div id='2403.04326v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T08:45:31Z</div><div>Authors: Zhongjun Ni, Chi Zhang, Magnus Karlsson, Shaofang Gong</div><div style='padding-top: 10px; width: 80ex'>Digital transformation in the built environment generates vast data for
developing data-driven models to optimize building operations. This study
presents an integrated solution utilizing edge computing, digital twins, and
deep learning to enhance the understanding of climate in buildings. Parametric
digital twins, created using an ontology, ensure consistent data representation
across diverse service systems equipped by different buildings. Based on
created digital twins and collected data, deep learning methods are employed to
develop predictive models for identifying patterns in indoor climate and
providing insights. Both the parametric digital twin and deep learning models
are deployed on edge for low latency and privacy compliance. As a
demonstration, a case study was conducted in a historic building in
\"Osterg\"otland, Sweden, to compare the performance of five deep learning
architectures. The results indicate that the time-series dense encoder model
exhibited strong competitiveness in performing multi-horizon forecasts of
indoor temperature and relative humidity with low computational costs.</div><div><a href='http://arxiv.org/abs/2403.04326v1'>2403.04326v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02909v1")'>Digital Twin for Grey Box modeling of Multistory residential building
  thermal dynamics</div>
<div id='2402.02909v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:25:42Z</div><div>Authors: Lina Morkunaite, Justas Kardoka, Darius Pupeikis, Paris Fokaides, Vangelis Angelakis</div><div style='padding-top: 10px; width: 80ex'>Buildings energy efficiency is a widely researched topic, which is rapidly
gaining popularity due to rising environmental concerns and the need for energy
independence. In Northern Europe heating energy alone accounts for up to 70
percent of the total building energy consumption. Industry 4.0 technologies
such as IoT, big data, cloud computing and machine learning, along with the
creation of predictive and proactive digital twins, can help to reduce this
number. However, buildings thermal dynamics is a very complex process that
depends on many variables. As a result, commonly used physics-based white box
models are time-consuming and require vast expertise. On the contrary, black
box forecasting models, which rely primarily on building energy consumption
data, lack fundamental insights and hinder re-use. In this study we propose an
architecture to facilitate grey box modelling of building thermal dynamics
while integrating real time IoT data with 3D representation of buildings. The
architecture is validated in a case study creating a digital twin platform that
enables users to define the thermal dynamics of buildings based on physical
laws and real data, thus facilitating informed decision making for the best
heating energy optimization strategy. Also, the created user interface enables
stakeholders such as facility managers, energy providers or governing bodies to
analyse, compare and evaluate buildings thermal dynamics without extensive
expertise or time resources.</div><div><a href='http://arxiv.org/abs/2402.02909v1'>2402.02909v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10726v2")'>Empowering Aggregators with Practical Data-Driven Tools: Harnessing
  Aggregated and Disaggregated Flexibility for Demand Response</div>
<div id='2401.10726v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T14:43:04Z</div><div>Authors: Costas Mylonas, Donata Boric, Leila Luttenberger Maric, Alexandros Tsitsanis, Eleftheria Petrianou, Magda Foti</div><div style='padding-top: 10px; width: 80ex'>This study explores the crucial interplay between aggregators and building
occupants in activating flexibility through Demand Response (DR) programs, with
a keen focus on achieving robust decarbonization and fortifying the resilience
of the energy system amidst the uncertainties presented by Renewable Energy
Sources (RES). Firstly, it introduces a methodology of optimizing aggregated
flexibility provision strategies in environments with limited data, utilizing
Discrete Fourier Transformation (DFT) and clustering techniques to identify
building occupant's activity patterns. Secondly, the study assesses the
disaggregated flexibility provision of Heating Ventilation and Air Conditioning
(HVAC) systems during DR events, employing machine learning and optimization
techniques for precise, device-level analysis. The first approach offers a
non-intrusive pathway for aggregators to provide flexibility services in
environments of a single smart meter for the whole building's consumption,
while the second approach carefully considers building occupants' thermal
comfort profiles, while maximizing flexibility in case of existence of
dedicated smart meters to the HVAC systems. Through the application of
data-driven techniques and encompassing case studies from both industrial and
residential buildings, this paper not only unveils pivotal opportunities for
aggregators in the balancing and emerging flexibility markets but also
successfully develops end-to-end practical tools for aggregators. Furthermore,
the efficacy of this tool is validated through detailed case studies,
substantiating its operational capability and contributing to the evolution of
a resilient and efficient energy system.</div><div><a href='http://arxiv.org/abs/2401.10726v2'>2401.10726v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13752v1")'>AI-Powered Predictions for Electricity Load in Prosumer Communities</div>
<div id='2402.13752v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:23:09Z</div><div>Authors: Aleksei Kychkin, Georgios C. Chasparis</div><div style='padding-top: 10px; width: 80ex'>The flexibility in electricity consumption and production in communities of
residential buildings, including those with renewable energy sources and energy
storage (a.k.a., prosumers), can effectively be utilized through the
advancement of short-term demand response mechanisms. It is known that
flexibility can further be increased if demand response is performed at the
level of communities of prosumers, since aggregated groups can better
coordinate electricity consumption. However, the effectiveness of such
short-term optimization is highly dependent on the accuracy of electricity load
forecasts both for each building as well as for the whole community. Structural
variations in the electricity load profile can be associated with different
exogenous factors, such as weather conditions, calendar information and day of
the week, as well as user behavior. In this paper, we review a wide range of
electricity load forecasting techniques, that can provide significant
assistance in optimizing load consumption in prosumer communities. We present
and test artificial intelligence (AI) powered short-term load forecasting
methodologies that operate with black-box time series models, such as
Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based
SARIMA and smoothing Holt-Winters models; and empirical regression-based models
that utilize domain knowledge. The integration of weather forecasts into
data-driven time series forecasts is also tested. Results show that the
combination of persistent and regression terms (adapted to the load forecasting
task) achieves the best forecast accuracy.</div><div><a href='http://arxiv.org/abs/2402.13752v1'>2402.13752v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13628v1")'>Improving Building Temperature Forecasting: A Data-driven Approach with
  System Scenario Clustering</div>
<div id='2402.13628v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:04:45Z</div><div>Authors: Dafang Zhao, Zheng Chen, Zhengmao Li, Xiaolei Yuan, Ittetsu Taniguchi</div><div style='padding-top: 10px; width: 80ex'>Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in
maintaining a comfortable thermal environment and cost approximately 40% of
primary energy usage in the building sector. For smart energy management in
buildings, usage patterns and their resulting profiles allow the improvement of
control systems with prediction capabilities. However, for large-scale HVAC
system management, it is difficult to construct a detailed model for each
subsystem. In this paper, a new data-driven room temperature prediction model
is proposed based on the k-means clustering method. The proposed data-driven
temperature prediction approach extracts the system operation feature through
historical data analysis and further simplifies the system-level model to
improve generalization and computational efficiency. We evaluate the proposed
approach in the real world. The results demonstrated that our approach can
significantly reduce modeling time without reducing prediction accuracy.</div><div><a href='http://arxiv.org/abs/2402.13628v1'>2402.13628v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04751v1")'>Identifying Best Practice Melting Patterns in Induction Furnaces: A
  Data-Driven Approach Using Time Series KMeans Clustering and Multi-Criteria
  Decision Making</div>
<div id='2401.04751v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T14:00:42Z</div><div>Authors: Daniel Anthony Howard, Bo Nørregaard Jørgensen, Zheng Ma</div><div style='padding-top: 10px; width: 80ex'>Improving energy efficiency in industrial production processes is crucial for
competitiveness, and compliance with climate policies. This paper introduces a
data-driven approach to identify optimal melting patterns in induction
furnaces. Through time-series K-means clustering the melting patterns could be
classified into distinct clusters based on temperature profiles. Using the
elbow method, 12 clusters were identified, representing the range of melting
patterns. Performance parameters such as melting time, energy-specific
performance, and carbon cost were established for each cluster, indicating
furnace efficiency and environmental impact. Multiple criteria decision-making
methods including Simple Additive Weighting, Multiplicative Exponential
Weighting, Technique for Order of Preference by Similarity to Ideal Solution,
modified TOPSIS, and VlseKriterijumska Optimizacija I Kompromisno Resenje were
utilized to determine the best-practice cluster. The study successfully
identified the cluster with the best performance. Implementing the best
practice operation resulted in an 8.6 % reduction in electricity costs,
highlighting the potential energy savings in the foundry.</div><div><a href='http://arxiv.org/abs/2401.04751v1'>2401.04751v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.06643v1")'>Spatial features of CO2 for occupancy detection in a naturally
  ventilated school building</div>
<div id='2403.06643v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T12:04:28Z</div><div>Authors: Qirui Huang, Marc Syndicus, Jérôme Frisch, Christoph van Treeck</div><div style='padding-top: 10px; width: 80ex'>Accurate occupancy information helps to improve building energy efficiency
and occupant comfort. Occupancy detection methods based on CO2 sensors have
received attention due to their low cost and low intrusiveness. In naturally
ventilated buildings, the accuracy of CO2-based occupancy detection is
generally low in related studies due to the complex ventilation behavior and
the difficulty in measuring the actual air exchange through windows. In this
study, we present two novel features for occupancy detection based on the
spatial distribution of the CO2 concentration. After a quantitative analysis
with Support Vector Machine (SVM) as classifier, it was found that the accuracy
of occupancy state detection in naturally ventilated rooms could be improved by
up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1
score 0.84) without any ventilation information. With ventilation information,
the accuracy reached 87.6 % (F1 score 0.89). The performance of occupancy
quantity detection was significantly improved by up to 25.3 percentage points
versus baseline, reaching 56 %, with root mean square error (RMSE) of 11.44
occupants, using only CO2-related features. Additional ventilation information
further enhanced the performance to 61.8 % (RMSE 9.02 occupants). By
incorporating spatial features, the model using only CO2-related features
revealed similar performance as the model containing additional ventilation
information, resulting in a better low-cost occupancy detection method for
naturally ventilated buildings.</div><div><a href='http://arxiv.org/abs/2403.06643v1'>2403.06643v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02963v1")'>One-class anomaly detection through color-to-thermal AI for building
  envelope inspection</div>
<div id='2402.02963v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:41:30Z</div><div>Authors: Polina Kurtser, Kailun Feng, Thomas Olofsson, Aitor De Andres</div><div style='padding-top: 10px; width: 80ex'>We present a label-free method for detecting anomalies during thermographic
inspection of building envelopes. It is based on the AI-driven prediction of
thermal distributions from color images. Effectively the method performs as a
one-class classifier of the thermal image regions with high mismatch between
the predicted and actual thermal distributions. The algorithm can learn to
identify certain features as normal or anomalous by selecting the target sample
used for training. We demonstrated this principle by training the algorithm
with data collected at different outdoors temperature, which lead to the
detection of thermal bridges. The method can be implemented to assist human
professionals during routine building inspections or combined with mobile
platforms for automating examination of large areas.</div><div><a href='http://arxiv.org/abs/2402.02963v1'>2402.02963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10566v1")'>Cooling-Guide Diffusion Model for Battery Cell Arrangement</div>
<div id='2403.10566v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T16:51:51Z</div><div>Authors: Nicholas Sung, Liu Zheng, Pingfeng Wang, Faez Ahmed</div><div style='padding-top: 10px; width: 80ex'>Our study introduces a Generative AI method that employs a cooling-guided
diffusion model to optimize the layout of battery cells, a crucial step for
enhancing the cooling performance and efficiency of battery thermal management
systems. Traditional design processes, which rely heavily on iterative
optimization and extensive guesswork, are notoriously slow and inefficient,
often leading to suboptimal solutions. In contrast, our innovative method uses
a parametric denoising diffusion probabilistic model (DDPM) with classifier and
cooling guidance to generate optimized cell layouts with enhanced cooling
paths, significantly lowering the maximum temperature of the cells. By
incorporating position-based classifier guidance, we ensure the feasibility of
generated layouts. Meanwhile, cooling guidance directly optimizes
cooling-efficiency, making our approach uniquely effective. When compared to
two advanced models, the Tabular Denoising Diffusion Probabilistic Model
(TabDDPM) and the Conditional Tabular GAN (CTGAN), our cooling-guided diffusion
model notably outperforms both. It is five times more effective than TabDDPM
and sixty-six times better than CTGAN across key metrics such as feasibility,
diversity, and cooling efficiency. This research marks a significant leap
forward in the field, aiming to optimize battery cell layouts for superior
cooling efficiency, thus setting the stage for the development of more
effective and dependable battery thermal management systems.</div><div><a href='http://arxiv.org/abs/2403.10566v1'>2403.10566v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02426v1")'>Digital Twins and Civil Engineering Phases: Reorienting Adoption
  Strategies</div>
<div id='2403.02426v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:18:53Z</div><div>Authors: Taiwo A. Adebiyi, Nafeezat A. Ajenifuja, Ruda Zhang</div><div style='padding-top: 10px; width: 80ex'>Digital twin (DT) technology has received immense attention over the years
due to the promises it presents to various stakeholders in science and
engineering. As a result, different thematic areas of DT have been explored.
This is no different in specific fields such as manufacturing, automation, oil
and gas, and civil engineering, leading to fragmented approaches for
field-specific applications. The civil engineering industry is further
disadvantaged in this regard as it relies on external techniques by other
engineering fields for its DT adoption. A rising consequence of these
extensions is a concentrated application of DT to the operations and
maintenance phase. On another spectrum, Building Information Modeling (BIM) are
pervasively utilized in the planning/design phase, and the transient nature of
the construction phase remains a challenge for its DT adoption. In this paper,
we present a phase-based development of DT in the Architecture, Engineering,
and Construction industry. We commence by presenting succinct expositions on DT
as a concept and as a service and establish a five-level scale system.
Furthermore, we present separately a systematic literature review of the
conventional techniques employed at each civil engineering phase. In this
regard, we identified enabling technologies such as computer vision for
extended sensing and the Internet of Things for reliable integration.
Ultimately, we attempt to reveal DT as an important tool across the entire life
cycle of civil engineering projects and nudge researchers to think more
holistically in their quest for the integration of DT for civil engineering
applications.</div><div><a href='http://arxiv.org/abs/2403.02426v1'>2403.02426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16449v1")'>AI in Energy Digital Twining: A Reinforcement Learning-based Adaptive
  Digital Twin Model for Green Cities</div>
<div id='2401.16449v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T20:38:58Z</div><div>Authors: Lal Verda Cakir, Kubra Duran, Craig Thomson, Matthew Broadbent, Berk Canberk</div><div style='padding-top: 10px; width: 80ex'>Digital Twins (DT) have become crucial to achieve sustainable and effective
smart urban solutions. However, current DT modelling techniques cannot support
the dynamicity of these smart city environments. This is caused by the lack of
right-time data capturing in traditional approaches, resulting in inaccurate
modelling and high resource and energy consumption challenges. To fill this
gap, we explore spatiotemporal graphs and propose the Reinforcement
Learning-based Adaptive Twining (RL-AT) mechanism with Deep Q Networks (DQN).
By doing so, our study contributes to advancing Green Cities and showcases
tangible benefits in accuracy, synchronisation, resource optimization, and
energy efficiency. As a result, we note the spatiotemporal graphs are able to
offer a consistent accuracy and 55% higher querying performance when
implemented using graph databases. In addition, our model demonstrates
right-time data capturing with 20% lower overhead and 25% lower energy
consumption.</div><div><a href='http://arxiv.org/abs/2401.16449v1'>2401.16449v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07591v1")'>Robustifying and Boosting Training-Free Neural Architecture Search</div>
<div id='2403.07591v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T12:24:11Z</div><div>Authors: Zhenfeng He, Yao Shu, Zhongxiang Dai, Bryan Kian Hsiang Low</div><div style='padding-top: 10px; width: 80ex'>Neural architecture search (NAS) has become a key component of AutoML and a
standard tool to automate the design of deep neural networks. Recently,
training-free NAS as an emerging paradigm has successfully reduced the search
costs of standard training-based NAS by estimating the true architecture
performance with only training-free metrics. Nevertheless, the estimation
ability of these metrics typically varies across different tasks, making it
challenging to achieve robust and consistently good search performance on
diverse tasks with only a single training-free metric. Meanwhile, the
estimation gap between training-free metrics and the true architecture
performances limits training-free NAS to achieve superior performance. To
address these challenges, we propose the robustifying and boosting
training-free NAS (RoBoT) algorithm which (a) employs the optimized combination
of existing training-free metrics explored from Bayesian optimization to
develop a robust and consistently better-performing metric on diverse tasks,
and (b) applies greedy search, i.e., the exploitation, on the newly developed
metric to bridge the aforementioned gap and consequently to boost the search
performance of standard training-free NAS further. Remarkably, the expected
performance of our RoBoT can be theoretically guaranteed, which improves over
the existing training-free NAS under mild conditions with additional
interesting insights. Our extensive experiments on various NAS benchmark tasks
yield substantial empirical evidence to support our theoretical results.</div><div><a href='http://arxiv.org/abs/2403.07591v1'>2403.07591v1</a></div>
</div></div>
    <div><a href="arxiv_13.html">Prev (13)</a></div>
    <div><a href="arxiv_15.html">Next (15)</a></div>
    