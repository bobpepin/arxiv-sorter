
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06452v1")'>Automated Machine Learning for Positive-Unlabelled Learning</div>
<div id='2401.06452v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T08:54:34Z</div><div>Authors: Jack D. Saunders, Alex A. Freitas</div><div style='padding-top: 10px; width: 80ex'>Positive-Unlabelled (PU) learning is a growing field of machine learning that
aims to learn classifiers from data consisting of labelled positive and
unlabelled instances, which can be in reality positive or negative, but whose
label is unknown. An extensive number of methods have been proposed to address
PU learning over the last two decades, so many so that selecting an optimal
method for a given PU learning task presents a challenge. Our previous work has
addressed this by proposing GA-Auto-PU, the first Automated Machine Learning
(Auto-ML) system for PU learning. In this work, we propose two new Auto-ML
systems for PU learning: BO-Auto-PU, based on a Bayesian Optimisation approach,
and EBO-Auto-PU, based on a novel evolutionary/Bayesian optimisation approach.
We also present an extensive evaluation of the three Auto-ML systems, comparing
them to each other and to well-established PU learning methods across 60
datasets (20 real-world datasets, each with 3 versions in terms of PU learning
characteristics).</div><div><a href='http://arxiv.org/abs/2401.06452v1'>2401.06452v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13370v1")'>Counting Network for Learning from Majority Label</div>
<div id='2403.13370v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T08:04:00Z</div><div>Authors: Kaito Shiku, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise</div><div style='padding-top: 10px; width: 80ex'>The paper proposes a novel problem in multi-class Multiple-Instance Learning
(MIL) called Learning from the Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag's label. LML aims to classify
instances using bag-level majority classes. This problem is valuable in various
applications. Existing MIL methods are unsuitable for LML due to aggregating
confidences, which may lead to inconsistency between the bag-level label and
the label obtained by counting the number of instances for each class. This may
lead to incorrect instance-level classification. We propose a counting network
trained to produce the bag-level majority labels estimated by counting the
number of instances for each class. This led to the consistency of the majority
class between the network outputs and one obtained by counting the number of
instances. Experimental results show that our counting network outperforms
conventional MIL methods on four datasets The code is publicly available at
https://github.com/Shiku-Kaito/Counting-Network-for-Learning-from-Majority-Label.</div><div><a href='http://arxiv.org/abs/2403.13370v1'>2403.13370v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07352v1")'>Data Distribution-based Curriculum Learning</div>
<div id='2402.07352v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T01:02:22Z</div><div>Authors: Shonal Chaudhry, Anuraganand Sharma</div><div style='padding-top: 10px; width: 80ex'>The order of training samples can have a significant impact on the
performance of a classifier. Curriculum learning is a method of ordering
training samples from easy to hard. This paper proposes the novel idea of a
curriculum learning approach called Data Distribution-based Curriculum Learning
(DDCL). DDCL uses the data distribution of a dataset to build a curriculum
based on the order of samples. Two types of scoring methods known as DDCL
(Density) and DDCL (Point) are used to score training samples thus determining
their training order. DDCL (Density) uses the sample density to assign scores
while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the
proposed DDCL approach by conducting experiments on multiple datasets using a
neural network, support vector machine and random forest classifier. Evaluation
results show that the application of DDCL improves the average classification
accuracy for all datasets compared to standard evaluation without any
curriculum. Moreover, analysis of the error losses for a single training epoch
reveals that convergence is faster when using DDCL over the no curriculum
method.</div><div><a href='http://arxiv.org/abs/2402.07352v1'>2402.07352v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03502v1")'>How Does Unlabeled Data Provably Help Out-of-Distribution Detection?</div>
<div id='2402.03502v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T20:36:33Z</div><div>Authors: Xuefeng Du, Zhen Fang, Ilias Diakonikolas, Yixuan Li</div><div style='padding-top: 10px; width: 80ex'>Using unlabeled data to regularize the machine learning models has
demonstrated promise for improving safety and reliability in detecting
out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild
data is non-trivial due to the heterogeneity of both in-distribution (ID) and
OOD data. This lack of a clean set of OOD samples poses significant challenges
in learning an optimal OOD classifier. Currently, there is a lack of research
on formally understanding how unlabeled data helps OOD detection. This paper
bridges the gap by introducing a new learning framework SAL (Separate And
Learn) that offers both strong theoretical guarantees and empirical
effectiveness. The framework separates candidate outliers from the unlabeled
data and then trains an OOD classifier using the candidate outliers and the
labeled ID data. Theoretically, we provide rigorous error bounds from the lens
of separability and learnability, formally justifying the two components in our
algorithm. Our theory shows that SAL can separate the candidate outliers with
small error rates, which leads to a generalization guarantee for the learned
OOD classifier. Empirically, SAL achieves state-of-the-art performance on
common benchmarks, reinforcing our theoretical insights. Code is publicly
available at https://github.com/deeplearning-wisc/sal.</div><div><a href='http://arxiv.org/abs/2402.03502v1'>2402.03502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02690v1")'>Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy
  Label Learning</div>
<div id='2403.02690v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:20:49Z</div><div>Authors: HeeSun Bae, Seungjae Shin, Byeonghu Na, Il-Chul Moon</div><div style='padding-top: 10px; width: 80ex'>For learning with noisy labels, the transition matrix, which explicitly
models the relation between noisy label distribution and clean label
distribution, has been utilized to achieve the statistical consistency of
either the classifier or the risk. Previous researches have focused more on how
to estimate this transition matrix well, rather than how to utilize it. We
propose good utilization of the transition matrix is crucial and suggest a new
utilization method based on resampling, coined RENT. Specifically, we first
demonstrate current utilizations can have potential limitations for
implementation. As an extension to Reweighting, we suggest the Dirichlet
distribution-based per-sample Weight Sampling (DWS) framework, and compare
reweighting and resampling under DWS framework. With the analyses from DWS, we
propose RENT, a REsampling method with Noise Transition matrix. Empirically,
RENT consistently outperforms existing transition matrix utilization methods,
which includes reweighting, on various benchmark datasets. Our code is
available at \url{https://github.com/BaeHeeSun/RENT}.</div><div><a href='http://arxiv.org/abs/2403.02690v1'>2403.02690v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07821v1")'>On Computationally Efficient Multi-Class Calibration</div>
<div id='2402.07821v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:25:23Z</div><div>Authors: Parikshit Gopalan, Lunjia Hu, Guy N. Rothblum</div><div style='padding-top: 10px; width: 80ex'>Consider a multi-class labelling problem, where the labels can take values in
$[k]$, and a predictor predicts a distribution over the labels. In this work,
we study the following foundational question: Are there notions of multi-class
calibration that give strong guarantees of meaningful predictions and can be
achieved in time and sample complexities polynomial in $k$? Prior notions of
calibration exhibit a tradeoff between computational efficiency and
expressivity: they either suffer from having sample complexity exponential in
$k$, or needing to solve computationally intractable problems, or give rather
weak guarantees.
  Our main contribution is a notion of calibration that achieves all these
desiderata: we formulate a robust notion of projected smooth calibration for
multi-class predictions, and give new recalibration algorithms for efficiently
calibrating predictors under this definition with complexity polynomial in $k$.
Projected smooth calibration gives strong guarantees for all downstream
decision makers who want to use the predictor for binary classification
problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g.
is this an image of an animal? It ensures that the probabilities predicted by
summing the probabilities assigned to labels in $T$ are close to some perfectly
calibrated binary predictor for that task. We also show that natural
strengthenings of our definition are computationally hard to achieve: they run
into information theoretic barriers or computational intractability. Underlying
both our upper and lower bounds is a tight connection that we prove between
multi-class calibration and the well-studied problem of agnostic learning in
the (standard) binary prediction setting.</div><div><a href='http://arxiv.org/abs/2402.07821v1'>2402.07821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10889v1")'>List Sample Compression and Uniform Convergence</div>
<div id='2403.10889v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T10:49:27Z</div><div>Authors: Steve Hanneke, Shay Moran, Tom Waknine</div><div style='padding-top: 10px; width: 80ex'>List learning is a variant of supervised classification where the learner
outputs multiple plausible labels for each instance rather than just one. We
investigate classical principles related to generalization within the context
of list learning. Our primary goal is to determine whether classical principles
in the PAC setting retain their applicability in the domain of list PAC
learning. We focus on uniform convergence (which is the basis of Empirical Risk
Minimization) and on sample compression (which is a powerful manifestation of
Occam's Razor). In classical PAC learning, both uniform convergence and sample
compression satisfy a form of `completeness': whenever a class is learnable, it
can also be learned by a learning rule that adheres to these principles. We ask
whether the same completeness holds true in the list learning setting.
  We show that uniform convergence remains equivalent to learnability in the
list PAC learning setting. In contrast, our findings reveal surprising results
regarding sample compression: we prove that when the label space is
$Y=\{0,1,2\}$, then there are 2-list-learnable classes that cannot be
compressed. This refutes the list version of the sample compression conjecture
by Littlestone and Warmuth (1986). We prove an even stronger impossibility
result, showing that there are $2$-list-learnable classes that cannot be
compressed even when the reconstructed function can work with lists of
arbitrarily large size. We prove a similar result for (1-list) PAC learnable
classes when the label space is unbounded. This generalizes a recent result by
arXiv:2308.06424.</div><div><a href='http://arxiv.org/abs/2403.10889v1'>2403.10889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07724v1")'>Balancing Fairness and Accuracy in Data-Restricted Binary Classification</div>
<div id='2403.07724v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T15:01:27Z</div><div>Authors: Zachary McBride Lazri, Danial Dervovic, Antigoni Polychroniadou, Ivan Brugere, Dana Dachman-Soled, Min Wu</div><div style='padding-top: 10px; width: 80ex'>Applications that deal with sensitive information may have restrictions
placed on the data available to a machine learning (ML) classifier. For
example, in some applications, a classifier may not have direct access to
sensitive attributes, affecting its ability to produce accurate and fair
decisions. This paper proposes a framework that models the trade-off between
accuracy and fairness under four practical scenarios that dictate the type of
data available for analysis. Prior works examine this trade-off by analyzing
the outputs of a scoring function that has been trained to implicitly learn the
underlying distribution of the feature vector, class label, and sensitive
attribute of a dataset. In contrast, our framework directly analyzes the
behavior of the optimal Bayesian classifier on this underlying distribution by
constructing a discrete approximation it from the dataset itself. This approach
enables us to formulate multiple convex optimization problems, which allow us
to answer the question: How is the accuracy of a Bayesian classifier affected
in different data restricting scenarios when constrained to be fair? Analysis
is performed on a set of fairness definitions that include group and individual
fairness. Experiments on three datasets demonstrate the utility of the proposed
framework as a tool for quantifying the trade-offs among different fairness
notions and their distributional dependencies.</div><div><a href='http://arxiv.org/abs/2403.07724v1'>2403.07724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15603v1")'>Differentially Private Fair Binary Classifications</div>
<div id='2402.15603v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T20:52:59Z</div><div>Authors: Hrad Ghoukasian, Shahab Asoodeh</div><div style='padding-top: 10px; width: 80ex'>In this work, we investigate binary classification under the constraints of
both differential privacy and fairness. We first propose an algorithm based on
the decoupling technique for learning a classifier with only fairness
guarantee. This algorithm takes in classifiers trained on different demographic
groups and generates a single classifier satisfying statistical parity. We then
refine this algorithm to incorporate differential privacy. The performance of
the final algorithm is rigorously examined in terms of privacy, fairness, and
utility guarantees. Empirical evaluations conducted on the Adult and Credit
Card datasets illustrate that our algorithm outperforms the state-of-the-art in
terms of fairness guarantees, while maintaining the same level of privacy and
utility.</div><div><a href='http://arxiv.org/abs/2402.15603v1'>2402.15603v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03011v1")'>On the Impact of Output Perturbation on Fairness in Binary Linear
  Classification</div>
<div id='2402.03011v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:50:08Z</div><div>Authors: Vitalii Emelianov, Michaël Perrot</div><div style='padding-top: 10px; width: 80ex'>We theoretically study how differential privacy interacts with both
individual and group fairness in binary linear classification. More precisely,
we focus on the output perturbation mechanism, a classic approach in
privacy-preserving machine learning. We derive high-probability bounds on the
level of individual and group fairness that the perturbed models can achieve
compared to the original model. Hence, for individual fairness, we prove that
the impact of output perturbation on the level of fairness is bounded but grows
with the dimension of the model. For group fairness, we show that this impact
is determined by the distribution of so-called angular margins, that is signed
margins of the non-private model re-scaled by the norm of each example.</div><div><a href='http://arxiv.org/abs/2402.03011v1'>2402.03011v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07174v1")'>On the (In)Compatibility between Group Fairness and Individual Fairness</div>
<div id='2401.07174v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T23:38:10Z</div><div>Authors: Shizhou Xu, Thomas Strohmer</div><div style='padding-top: 10px; width: 80ex'>We study the compatibility between the optimal statistical parity solutions
and individual fairness. While individual fairness seeks to treat similar
individuals similarly, optimal statistical parity aims to provide similar
treatment to individuals who share relative similarity within their respective
sensitive groups. The two fairness perspectives, while both desirable from a
fairness perspective, often come into conflict in applications. Our goal in
this work is to analyze the existence of this conflict and its potential
solution. In particular, we establish sufficient (sharp) conditions for the
compatibility between the optimal (post-processing) statistical parity $L^2$
learning and the ($K$-Lipschitz or $(\epsilon,\delta)$) individual fairness
requirements. Furthermore, when there exists a conflict between the two, we
first relax the former to the Pareto frontier (or equivalently the optimal
trade-off) between $L^2$ error and statistical disparity, and then analyze the
compatibility between the frontier and the individual fairness requirements.
Our analysis identifies regions along the Pareto frontier that satisfy
individual fairness requirements. (Lastly, we provide individual fairness
guarantees for the composition of a trained model and the optimal
post-processing step so that one can determine the compatibility of the
post-processed model.) This provides practitioners with a valuable approach to
attain Pareto optimality for statistical parity while adhering to the
constraints of individual fairness.</div><div><a href='http://arxiv.org/abs/2401.07174v1'>2401.07174v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02183v1")'>FairGridSearch: A Framework to Compare Fairness-Enhancing Models</div>
<div id='2401.02183v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T10:29:02Z</div><div>Authors: Shih-Chi Ma, Tatiana Ermakova, Benjamin Fabian</div><div style='padding-top: 10px; width: 80ex'>Machine learning models are increasingly used in critical decision-making
applications. However, these models are susceptible to replicating or even
amplifying bias present in real-world data. While there are various bias
mitigation methods and base estimators in the literature, selecting the optimal
model for a specific application remains challenging.
  This paper focuses on binary classification and proposes FairGridSearch, a
novel framework for comparing fairness-enhancing models. FairGridSearch enables
experimentation with different model parameter combinations and recommends the
best one. The study applies FairGridSearch to three popular datasets (Adult,
COMPAS, and German Credit) and analyzes the impacts of metric selection, base
estimator choice, and classification threshold on model fairness.
  The results highlight the significance of selecting appropriate accuracy and
fairness metrics for model evaluation. Additionally, different base estimators
and classification threshold values affect the effectiveness of bias mitigation
methods and fairness stability respectively, but the effects are not consistent
across all datasets. Based on these findings, future research on fairness in
machine learning should consider a broader range of factors when building fair
models, going beyond bias mitigation methods alone.</div><div><a href='http://arxiv.org/abs/2401.02183v1'>2401.02183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01811v1")'>A Distributionally Robust Optimisation Approach to Fair Credit Scoring</div>
<div id='2402.01811v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:43:59Z</div><div>Authors: Pablo Casas, Christophe Mues, Huan Yu</div><div style='padding-top: 10px; width: 80ex'>Credit scoring has been catalogued by the European Commission and the
Executive Office of the US President as a high-risk classification task, a key
concern being the potential harms of making loan approval decisions based on
models that would be biased against certain groups. To address this concern,
recent credit scoring research has considered a range of fairness-enhancing
techniques put forward by the machine learning community to reduce bias and
unfair treatment in classification systems. While the definition of fairness or
the approach they follow to impose it may vary, most of these techniques,
however, disregard the robustness of the results. This can create situations
where unfair treatment is effectively corrected in the training set, but when
producing out-of-sample classifications, unfair treatment is incurred again.
Instead, in this paper, we will investigate how to apply Distributionally
Robust Optimisation (DRO) methods to credit scoring, thereby empirically
evaluating how they perform in terms of fairness, ability to classify
correctly, and the robustness of the solution against changes in the marginal
proportions. In so doing, we find DRO methods to provide a substantial
improvement in terms of fairness, with almost no loss in performance. These
results thus indicate that DRO can improve fairness in credit scoring, provided
that further advances are made in efficiently implementing these systems. In
addition, our analysis suggests that many of the commonly used fairness metrics
are unsuitable for a credit scoring setting, as they depend on the choice of
classification threshold.</div><div><a href='http://arxiv.org/abs/2402.01811v1'>2402.01811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10652v1")'>Improving Fairness in Credit Lending Models using Subgroup Threshold
  Optimization</div>
<div id='2403.10652v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T19:36:56Z</div><div>Authors: Cecilia Ying, Stephen Thomas</div><div style='padding-top: 10px; width: 80ex'>In an effort to improve the accuracy of credit lending decisions, many
financial intuitions are now using predictions from machine learning models.
While such predictions enjoy many advantages, recent research has shown that
the predictions have the potential to be biased and unfair towards certain
subgroups of the population. To combat this, several techniques have been
introduced to help remove the bias and improve the overall fairness of the
predictions. We introduce a new fairness technique, called \textit{Subgroup
Threshold Optimizer} (\textit{STO}), that does not require any alternations to
the input training data nor does it require any changes to the underlying
machine learning algorithm, and thus can be used with any existing machine
learning pipeline. STO works by optimizing the classification thresholds for
individual subgroups in order to minimize the overall discrimination score
between them. Our experiments on a real-world credit lending dataset show that
STO can reduce gender discrimination by over 90\%.</div><div><a href='http://arxiv.org/abs/2403.10652v1'>2403.10652v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03785v1")'>A machine learning workflow to address credit default prediction</div>
<div id='2403.03785v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:30:41Z</div><div>Authors: Rambod Rahmani, Marco Parola, Mario G. C. A. Cimino</div><div style='padding-top: 10px; width: 80ex'>Due to the recent increase in interest in Financial Technology (FinTech),
applications like credit default prediction (CDP) are gaining significant
industrial and academic attention. In this regard, CDP plays a crucial role in
assessing the creditworthiness of individuals and businesses, enabling lenders
to make informed decisions regarding loan approvals and risk management. In
this paper, we propose a workflow-based approach to improve CDP, which refers
to the task of assessing the probability that a borrower will default on his or
her credit obligations. The workflow consists of multiple steps, each designed
to leverage the strengths of different techniques featured in machine learning
pipelines and, thus best solve the CDP task. We employ a comprehensive and
systematic approach starting with data preprocessing using Weight of Evidence
encoding, a technique that ensures in a single-shot data scaling by removing
outliers, handling missing values, and making data uniform for models working
with different data types. Next, we train several families of learning models,
introducing ensemble techniques to build more robust models and hyperparameter
optimization via multi-objective genetic algorithms to consider both predictive
accuracy and financial aspects. Our research aims at contributing to the
FinTech industry in providing a tool to move toward more accurate and reliable
credit risk assessment, benefiting both lenders and borrowers.</div><div><a href='http://arxiv.org/abs/2403.03785v1'>2403.03785v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17979v1")'>Ensemble Methodology:Innovations in Credit Default Prediction Using
  LightGBM, XGBoost, and LocalEnsemble</div>
<div id='2402.17979v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T01:48:54Z</div><div>Authors: Mengran Zhu, Ye Zhang, Yulu Gong, Kaijuan Xing, Xu Yan, Jintong Song</div><div style='padding-top: 10px; width: 80ex'>In the realm of consumer lending, accurate credit default prediction stands
as a critical element in risk mitigation and lending decision optimization.
Extensive research has sought continuous improvement in existing models to
enhance customer experiences and ensure the sound economic functioning of
lending institutions. This study responds to the evolving landscape of credit
default prediction, challenging conventional models and introducing innovative
approaches. By building upon foundational research and recent innovations, our
work aims to redefine the standards of accuracy in credit default prediction,
setting a new benchmark for the industry. To overcome these challenges, we
present an Ensemble Methods framework comprising LightGBM, XGBoost, and
LocalEnsemble modules, each making unique contributions to amplify diversity
and improve generalization. By utilizing distinct feature sets, our methodology
directly tackles limitations identified in previous studies, with the
overarching goal of establishing a novel standard for credit default prediction
accuracy. Our experimental findings validate the effectiveness of the ensemble
model on the dataset, signifying substantial contributions to the field. This
innovative approach not only addresses existing obstacles but also sets a
precedent for advancing the accuracy and robustness of credit default
prediction models.</div><div><a href='http://arxiv.org/abs/2402.17979v1'>2402.17979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07234v1")'>The Effects of Data Imbalance Under a Federated Learning Approach for
  Credit Risk Forecasting</div>
<div id='2401.07234v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T09:15:10Z</div><div>Authors: Shuyao Zhang, Jordan Tay, Pedro Baiz</div><div style='padding-top: 10px; width: 80ex'>Credit risk forecasting plays a crucial role for commercial banks and other
financial institutions in granting loans to customers and minimise the
potential loss. However, traditional machine learning methods require the
sharing of sensitive client information with an external server to build a
global model, potentially posing a risk of security threats and privacy
leakage. A newly developed privacy-preserving distributed machine learning
technique known as Federated Learning (FL) allows the training of a global
model without the necessity of accessing private local data directly. This
investigation examined the feasibility of federated learning in credit risk
assessment and showed the effects of data imbalance on model performance. Two
neural network architectures, Multilayer Perceptron (MLP) and Long Short-Term
Memory (LSTM), and one tree ensemble architecture, Extreme Gradient Boosting
(XGBoost), were explored across three different datasets under various
scenarios involving different numbers of clients and data distribution
configurations. We demonstrate that federated models consistently outperform
local models on non-dominant clients with smaller datasets. This trend is
especially pronounced in highly imbalanced data scenarios, yielding a
remarkable average improvement of 17.92% in model performance. However, for
dominant clients (clients with more data), federated models may not exhibit
superior performance, suggesting the need for special incentives for this type
of clients to encourage their participation.</div><div><a href='http://arxiv.org/abs/2401.07234v1'>2401.07234v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11621v1")'>A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and
  XGBoost for Speculative Stock Price Forecasting</div>
<div id='2401.11621v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T17:13:30Z</div><div>Authors: Riaz Ud Din, Salman Ahmed, Saddam Hussain Khan</div><div style='padding-top: 10px; width: 80ex'>Forecasting speculative stock prices is essential for effective investment
risk management that drives the need for the development of innovative
algorithms. However, the speculative nature, volatility, and complex sequential
dependencies within financial markets present inherent challenges which
necessitate advanced techniques. This paper proposes a novel framework, CAB-XDE
(customized attention BiLSTM-XGB decision ensemble), for predicting the daily
closing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework
integrates a customized bi-directional long short-term memory (BiLSTM) with the
attention mechanism and the XGBoost algorithm. The customized BiLSTM leverages
its learning capabilities to capture the complex sequential dependencies and
speculative market trends. Additionally, the new attention mechanism
dynamically assigns weights to influential features, thereby enhancing
interpretability, and optimizing effective cost measures and volatility
forecasting. Moreover, XGBoost handles nonlinear relationships and contributes
to the proposed CAB-XDE framework robustness. Additionally, the weight
determination theory-error reciprocal method further refines predictions. This
refinement is achieved by iteratively adjusting model weights. It is based on
discrepancies between theoretical expectations and actual errors in individual
customized attention BiLSTM and XGBoost models to enhance performance. Finally,
the predictions from both XGBoost and customized attention BiLSTM models are
concatenated to achieve diverse prediction space and are provided to the
ensemble classifier to enhance the generalization capabilities of CAB-XDE. The
proposed CAB-XDE framework is empirically validated on volatile Bitcoin market,
sourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE
of 0.0037, MAE of 84.40, and RMSE of 106.14.</div><div><a href='http://arxiv.org/abs/2401.11621v1'>2401.11621v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03606v1")'>Enhancing Price Prediction in Cryptocurrency Using Transformer Neural
  Network and Technical Indicators</div>
<div id='2403.03606v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T10:53:12Z</div><div>Authors: Mohammad Ali Labbaf Khaniki, Mohammad Manthouri</div><div style='padding-top: 10px; width: 80ex'>This study presents an innovative approach for predicting cryptocurrency time
series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The
methodology integrates the use of technical indicators, a Performer neural
network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal
dynamics and extract significant features from raw cryptocurrency data. The
application of technical indicators, such facilitates the extraction of
intricate patterns, momentum, volatility, and trends. The Performer neural
network, employing Fast Attention Via positive Orthogonal Random features
(FAVOR+), has demonstrated superior computational efficiency and scalability
compared to the traditional Multi-head attention mechanism in Transformer
models. Additionally, the integration of BiLSTM in the feedforward network
enhances the model's capacity to capture temporal dynamics in the data,
processing it in both forward and backward directions. This is particularly
advantageous for time series data where past and future data points can
influence the current state. The proposed method has been applied to the hourly
and daily timeframes of the major cryptocurrencies and its performance has been
benchmarked against other methods documented in the literature. The results
underscore the potential of the proposed method to outperform existing models,
marking a significant progression in the field of cryptocurrency price
prediction.</div><div><a href='http://arxiv.org/abs/2403.03606v1'>2403.03606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08077v1")'>Transformer-based approach for Ethereum Price Prediction Using
  Crosscurrency correlation and Sentiment Analysis</div>
<div id='2401.08077v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T03:03:39Z</div><div>Authors: Shubham Singh, Mayur Bhat</div><div style='padding-top: 10px; width: 80ex'>The research delves into the capabilities of a transformer-based neural
network for Ethereum cryptocurrency price forecasting. The experiment runs
around the hypothesis that cryptocurrency prices are strongly correlated with
other cryptocurrencies and the sentiments around the cryptocurrency. The model
employs a transformer architecture for several setups from single-feature
scenarios to complex configurations incorporating volume, sentiment, and
correlated cryptocurrency prices. Despite a smaller dataset and less complex
architecture, the transformer model surpasses ANN and MLP counterparts on some
parameters. The conclusion presents a hypothesis on the illusion of causality
in cryptocurrency price movements driven by sentiments.</div><div><a href='http://arxiv.org/abs/2401.08077v1'>2401.08077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03410v1")'>Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial
  Regression</div>
<div id='2403.03410v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T02:37:26Z</div><div>Authors: Novan Fauzi Al Giffary, Feri Sulianta</div><div style='padding-top: 10px; width: 80ex'>The rapid development of information technology, especially the Internet, has
facilitated users with a quick and easy way to seek information. With these
convenience offered by internet services, many individuals who initially
invested in gold and precious metals are now shifting into digital investments
in form of cryptocurrencies. However, investments in crypto coins are filled
with uncertainties and fluctuation in daily basis. This risk posed as
significant challenges for coin investors that could result in substantial
investment losses. The uncertainty of the value of these crypto coins is a
critical issue in the field of coin investment. Forecasting, is one of the
methods used to predict the future value of these crypto coins. By utilizing
the models of Long Short Term Memory, Support Vector Machine, and Polynomial
Regression algorithm for forecasting, a performance comparison is conducted to
determine which algorithm model is most suitable for predicting crypto currency
prices. The mean square error is employed as a benchmark for the comparison. By
applying those three constructed algorithm models, the Support Vector Machine
uses a linear kernel to produce the smallest mean square error compared to the
Long Short Term Memory and Polynomial Regression algorithm models, with a mean
square error value of 0.02. Keywords: Cryptocurrency, Forecasting, Long Short
Term Memory, Mean Square Error, Polynomial Regression, Support Vector Machine</div><div><a href='http://arxiv.org/abs/2403.03410v1'>2403.03410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10931v1")'>Forecasting Cryptocurrency Staking Rewards</div>
<div id='2401.10931v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T21:09:39Z</div><div>Authors: Sauren Gupta, Apoorva Hathi Katharaki, Yifan Xu, Bhaskar Krishnamachari, Rajarshi Gupta</div><div style='padding-top: 10px; width: 80ex'>This research explores a relatively unexplored area of predicting
cryptocurrency staking rewards, offering potential insights to researchers and
investors. We investigate two predictive methodologies: a) a straightforward
sliding-window average, and b) linear regression models predicated on
historical data. The findings reveal that ETH staking rewards can be forecasted
with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day
look-aheads respectively, using a 7-day sliding-window average approach.
Additionally, we discern diverse prediction accuracies across various
cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is
identified as superior to the moving-window average for perdicting in the short
term for XTZ and ATOM. The results underscore the generally stable and
predictable nature of staking rewards for most assets, with MATIC presenting a
noteworthy exception.</div><div><a href='http://arxiv.org/abs/2401.10931v1'>2401.10931v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11728v1")'>Numerical Claim Detection in Finance: A New Financial Dataset,
  Weak-Supervision Model, and Market Analysis</div>
<div id='2402.11728v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T22:55:26Z</div><div>Authors: Agam Shah, Arnav Hiray, Pratvi Shah, Arkaprabha Banerjee, Anushka Singh, Dheeraj Eidnani, Bhaskar Chaudhury, Sudheer Chava</div><div style='padding-top: 10px; width: 80ex'>In this paper, we investigate the influence of claims in analyst reports and
earnings calls on financial market returns, considering them as significant
quarterly events for publicly traded companies. To facilitate a comprehensive
analysis, we construct a new financial dataset for the claim detection task in
the financial domain. We benchmark various language models on this dataset and
propose a novel weak-supervision model that incorporates the knowledge of
subject matter experts (SMEs) in the aggregation function, outperforming
existing approaches. Furthermore, we demonstrate the practical utility of our
proposed model by constructing a novel measure ``optimism". Furthermore, we
observed the dependence of earnings surprise and return on our optimism
measure. Our dataset, models, and code will be made publicly (under CC BY 4.0
license) available on GitHub and Hugging Face.</div><div><a href='http://arxiv.org/abs/2402.11728v1'>2402.11728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09916v1")'>BUSTER: a "BUSiness Transaction Entity Recognition" dataset</div>
<div id='2402.09916v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T12:39:57Z</div><div>Authors: Andrea Zugarini, Andrew Zamai, Marco Ernandes, Leonardo Rigutini</div><div style='padding-top: 10px; width: 80ex'>Albeit Natural Language Processing has seen major breakthroughs in the last
few years, transferring such advances into real-world business cases can be
challenging. One of the reasons resides in the displacement between popular
benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data
and long documents often affect real problems in vertical domains such as
finance, law and health. To support industry-oriented research, we present
BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists
of 3779 manually annotated documents on financial transactions. We establish
several baselines exploiting both general-purpose and domain-specific language
models. The best performing model is also used to automatically annotate 6196
documents, which we release as an additional silver corpus to BUSTER.</div><div><a href='http://arxiv.org/abs/2402.09916v1'>2402.09916v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07843v1")'>A Machine learning and Empirical Bayesian Approach for Predictive Buying
  in B2B E-commerce</div>
<div id='2403.07843v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:32:52Z</div><div>Authors: Tuhin Subhra De, Pranjal Singh, Alok Patel</div><div style='padding-top: 10px; width: 80ex'>In the context of developing nations like India, traditional business to
business (B2B) commerce heavily relies on the establishment of robust
relationships, trust, and credit arrangements between buyers and sellers.
Consequently, ecommerce enterprises frequently. Established in 2016 with a
vision to revolutionize trade in India through technology, Udaan is the
countrys largest business to business ecommerce platform. Udaan operates across
diverse product categories, including lifestyle, electronics, home and employ
telecallers to cultivate buyer relationships, streamline order placement
procedures, and promote special promotions. The accurate anticipation of buyer
order placement behavior emerges as a pivotal factor for attaining sustainable
growth, heightening competitiveness, and optimizing the efficiency of these
telecallers. To address this challenge, we have employed an ensemble approach
comprising XGBoost and a modified version of Poisson Gamma model to predict
customer order patterns with precision. This paper provides an in-depth
exploration of the strategic fusion of machine learning and an empirical
Bayesian approach, bolstered by the judicious selection of pertinent features.
This innovative approach has yielded a remarkable 3 times increase in customer
order rates, show casing its potential for transformative impact in the
ecommerce industry.</div><div><a href='http://arxiv.org/abs/2403.07843v1'>2403.07843v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01327v2")'>Supervised Algorithmic Fairness in Distribution Shifts: A Survey</div>
<div id='2402.01327v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:26:18Z</div><div>Authors: Yujie Lin, Dong Li, Chen Zhao, Xintao Wu, Qin Tian, Minglai Shao</div><div style='padding-top: 10px; width: 80ex'>Supervised fairness-aware machine learning under distribution shifts is an
emerging field that addresses the challenge of maintaining equitable and
unbiased predictions when faced with changes in data distributions from source
to target domains. In real-world applications, machine learning models are
often trained on a specific dataset but deployed in environments where the data
distribution may shift over time due to various factors. This shift can lead to
unfair predictions, disproportionately affecting certain groups characterized
by sensitive attributes, such as race and gender. In this survey, we provide a
summary of various types of distribution shifts and comprehensively investigate
existing methods based on these shifts, highlighting six commonly used
approaches in the literature. Additionally, this survey lists publicly
available datasets and evaluation metrics for empirical studies. We further
explore the interconnection with related research fields, discuss the
significant challenges, and identify potential directions for future studies.</div><div><a href='http://arxiv.org/abs/2402.01327v2'>2402.01327v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12722v2")'>Falcon: Fair Active Learning using Multi-armed Bandits</div>
<div id='2401.12722v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T12:48:27Z</div><div>Authors: Ki Hyun Tae, Hantian Zhang, Jaeyoung Park, Kexin Rong, Steven Euijong Whang</div><div style='padding-top: 10px; width: 80ex'>Biased data can lead to unfair machine learning models, highlighting the
importance of embedding fairness at the beginning of data analysis,
particularly during dataset curation and labeling. In response, we propose
Falcon, a scalable fair active learning framework. Falcon adopts a data-centric
approach that improves machine learning model fairness via strategic sample
selection. Given a user-specified group fairness measure, Falcon identifies
samples from "target groups" (e.g., (attribute=female, label=positive)) that
are the most informative for improving fairness. However, a challenge arises
since these target groups are defined using ground truth labels that are not
available during sample selection. To handle this, we propose a novel
trial-and-error method, where we postpone using a sample if the predicted label
is different from the expected one and falls outside the target group. We also
observe the trade-off that selecting more informative samples results in higher
likelihood of postponing due to undesired label prediction, and the optimal
balance varies per dataset. We capture the trade-off between informativeness
and postpone rate as policies and propose to automatically select the best
policy using adversarial multi-armed bandit methods, given their computational
efficiency and theoretical guarantees. Experiments show that Falcon
significantly outperforms existing fair active learning approaches in terms of
fairness and accuracy and is more efficient. In particular, only Falcon
supports a proper trade-off between accuracy and fairness where its maximum
fairness score is 1.8-4.5x higher than the second-best results.</div><div><a href='http://arxiv.org/abs/2401.12722v2'>2401.12722v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12789v1")'>Fair Classifiers Without Fair Training: An Influence-Guided Data
  Sampling Approach</div>
<div id='2402.12789v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T07:57:38Z</div><div>Authors: Jinlong Pang, Jialu Wang, Zhaowei Zhu, Yuanshun Yao, Chen Qian, Yang Liu</div><div style='padding-top: 10px; width: 80ex'>A fair classifier should ensure the benefit of people from different groups,
while the group information is often sensitive and unsuitable for model
training. Therefore, learning a fair classifier but excluding sensitive
attributes in the training dataset is important. In this paper, we study
learning fair classifiers without implementing fair training algorithms to
avoid possible leakage of sensitive information. Our theoretical analyses
validate the possibility of this approach, that traditional training on a
dataset with an appropriate distribution shift can reduce both the upper bound
for fairness disparity and model generalization error, indicating that fairness
and accuracy can be improved simultaneously with simply traditional training.
We then propose a tractable solution to progressively shift the original
training data during training by sampling influential data, where the sensitive
attribute of new data is not accessed in sampling or used in training.
Extensive experiments on real-world data demonstrate the effectiveness of our
proposed algorithm.</div><div><a href='http://arxiv.org/abs/2402.12789v1'>2402.12789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11338v1")'>Fair Classification with Partial Feedback: An Exploration-Based
  Data-Collection Approach</div>
<div id='2402.11338v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T17:09:19Z</div><div>Authors: Vijay Keswani, Anay Mehrotra, L. Elisa Celis</div><div style='padding-top: 10px; width: 80ex'>In many predictive contexts (e.g., credit lending), true outcomes are only
observed for samples that were positively classified in the past. These past
observations, in turn, form training datasets for classifiers that make future
predictions. However, such training datasets lack information about the
outcomes of samples that were (incorrectly) negatively classified in the past
and can lead to erroneous classifiers. We present an approach that trains a
classifier using available data and comes with a family of exploration
strategies to collect outcome data about subpopulations that otherwise would
have been ignored. For any exploration strategy, the approach comes with
guarantees that (1) all sub-populations are explored, (2) the fraction of false
positives is bounded, and (3) the trained classifier converges to a "desired"
classifier. The right exploration strategy is context-dependent; it can be
chosen to improve learning guarantees and encode context-specific group
fairness properties. Evaluation on real-world datasets shows that this approach
consistently boosts the quality of collected outcome data and improves the
fraction of true positives for all groups, with only a small reduction in
predictive utility.</div><div><a href='http://arxiv.org/abs/2402.11338v1'>2402.11338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14282v1")'>How to be fair? A study of label and selection bias</div>
<div id='2403.14282v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T10:43:55Z</div><div>Authors: Marco Favier, Toon Calders, Sam Pinxteren, Jonathan Meyer</div><div style='padding-top: 10px; width: 80ex'>It is widely accepted that biased data leads to biased and thus potentially
unfair models. Therefore, several measures for bias in data and model
predictions have been proposed, as well as bias mitigation techniques whose aim
is to learn models that are fair by design. Despite the myriad of mitigation
techniques developed in the past decade, however, it is still poorly understood
under what circumstances which methods work. Recently, Wick et al. showed, with
experiments on synthetic data, that there exist situations in which bias
mitigation techniques lead to more accurate models when measured on unbiased
data. Nevertheless, in the absence of a thorough mathematical analysis, it
remains unclear which techniques are effective under what circumstances. We
propose to address this problem by establishing relationships between the type
of bias and the effectiveness of a mitigation technique, where we categorize
the mitigation techniques by the bias measure they optimize. In this paper we
illustrate this principle for label and selection bias on the one hand, and
demographic parity and ``We're All Equal'' on the other hand. Our theoretical
analysis allows to explain the results of Wick et al. and we also show that
there are situations where minimizing fairness measures does not result in the
fairest possible distribution.</div><div><a href='http://arxiv.org/abs/2403.14282v1'>2403.14282v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08879v1")'>Inference for an Algorithmic Fairness-Accuracy Frontier</div>
<div id='2402.08879v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T00:56:09Z</div><div>Authors: Yiqi Liu, Francesca Molinari</div><div style='padding-top: 10px; width: 80ex'>Decision-making processes increasingly rely on the use of algorithms. Yet,
algorithms' predictive ability frequently exhibit systematic variation across
subgroups of the population. While both fairness and accuracy are desirable
properties of an algorithm, they often come at the cost of one another. What
should a fairness-minded policymaker do then, when confronted with finite data?
In this paper, we provide a consistent estimator for a theoretical
fairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose
inference methods to test hypotheses that have received much attention in the
fairness literature, such as (i) whether fully excluding a covariate from use
in training the algorithm is optimal and (ii) whether there are less
discriminatory alternatives to an existing algorithm. We also provide an
estimator for the distance between a given algorithm and the fairest point on
the frontier, and characterize its asymptotic distribution. We leverage the
fact that the fairness-accuracy frontier is part of the boundary of a convex
set that can be fully represented by its support function. We show that the
estimated support function converges to a tight Gaussian process as the sample
size increases, and then express policy-relevant hypotheses as restrictions on
the support function to construct valid test statistics.</div><div><a href='http://arxiv.org/abs/2402.08879v1'>2402.08879v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15561v1")'>Fair Multivariate Adaptive Regression Splines for Ensuring Equity and
  Transparency</div>
<div id='2402.15561v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T19:02:24Z</div><div>Authors: Parian Haghighat, Denisa G'andara, Lulu Kang, Hadis Anahideh</div><div style='padding-top: 10px; width: 80ex'>Predictive analytics is widely used in various domains, including education,
to inform decision-making and improve outcomes. However, many predictive models
are proprietary and inaccessible for evaluation or modification by researchers
and practitioners, limiting their accountability and ethical design. Moreover,
predictive models are often opaque and incomprehensible to the officials who
use them, reducing their trust and utility. Furthermore, predictive models may
introduce or exacerbate bias and inequity, as they have done in many sectors of
society. Therefore, there is a need for transparent, interpretable, and fair
predictive models that can be easily adopted and adapted by different
stakeholders. In this paper, we propose a fair predictive model based on
multivariate adaptive regression splines(MARS) that incorporates fairness
measures in the learning process. MARS is a non-parametric regression model
that performs feature selection, handles non-linear relationships, generates
interpretable decision rules, and derives optimal splitting criteria on the
variables. Specifically, we integrate fairness into the knot optimization
algorithm and provide theoretical and empirical evidence of how it results in a
fair knot placement. We apply our fairMARS model to real-world data and
demonstrate its effectiveness in terms of accuracy and equity. Our paper
contributes to the advancement of responsible and ethical predictive analytics
for social good.</div><div><a href='http://arxiv.org/abs/2402.15561v1'>2402.15561v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.16412v1")'>Learning to Manipulate under Limited Information</div>
<div id='2401.16412v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:49:50Z</div><div>Authors: Wesley H. Holliday, Alexander Kristoffersen, Eric Pacuit</div><div style='padding-top: 10px; width: 80ex'>By classic results in social choice theory, any reasonable preferential
voting method sometimes gives individuals an incentive to report an insincere
preference. The extent to which different voting methods are more or less
resistant to such strategic manipulation has become a key consideration for
comparing voting methods. Here we measure resistance to manipulation by whether
neural networks of varying sizes can learn to profitably manipulate a given
voting method in expectation, given different types of limited information
about how other voters will vote. We trained nearly 40,000 neural networks of
26 sizes to manipulate against 8 different voting methods, under 6 types of
limited information, in committee-sized elections with 5-21 voters and 3-6
candidates. We find that some voting methods, such as Borda, are highly
manipulable by networks with limited information, while others, such as Instant
Runoff, are not, despite being quite profitably manipulated by an ideal
manipulator with full information.</div><div><a href='http://arxiv.org/abs/2401.16412v1'>2401.16412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03577v1")'>Revisiting the Dataset Bias Problem from a Statistical Perspective</div>
<div id='2402.03577v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:58:06Z</div><div>Authors: Kien Do, Dung Nguyen, Hung Le, Thao Le, Dang Nguyen, Haripriya Harikumar, Truyen Tran, Santu Rana, Svetha Venkatesh</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the "dataset bias" problem from a statistical
standpoint, and identify the main cause of the problem as the strong
correlation between a class attribute u and a non-class attribute b in the
input x, represented by p(u|b) differing significantly from p(u). Since p(u|b)
appears as part of the sampling distributions in the standard maximum
log-likelihood (MLL) objective, a model trained on a biased dataset via MLL
inherently incorporates such correlation into its parameters, leading to poor
generalization to unbiased test data. From this observation, we propose to
mitigate dataset bias via either weighting the objective of each sample n by
\frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to
\frac{1}{p(u_{n}|b_{n})}. While both methods are statistically equivalent, the
former proves more stable and effective in practice. Additionally, we establish
a connection between our debiasing approach and causal reasoning, reinforcing
our method's theoretical foundation. However, when the bias label is
unavailable, computing p(u|b) exactly is difficult. To overcome this challenge,
we propose to approximate \frac{1}{p(u|b)} using a biased classifier trained
with "bias amplification" losses. Extensive experiments on various biased
datasets demonstrate the superiority of our method over existing debiasing
techniques in most settings, validating our theoretical analysis.</div><div><a href='http://arxiv.org/abs/2402.03577v1'>2402.03577v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15477v1")'>Debiasing Machine Learning Models by Using Weakly Supervised Learning</div>
<div id='2402.15477v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:11:32Z</div><div>Authors: Renan D. B. Brotto, Jean-Michel Loubes, Laurent Risser, Jean-Pierre Florens, Kenji Nose-Filho, João M. T. Romano</div><div style='padding-top: 10px; width: 80ex'>We tackle the problem of bias mitigation of algorithmic decisions in a
setting where both the output of the algorithm and the sensitive variable are
continuous. Most of prior work deals with discrete sensitive variables, meaning
that the biases are measured for subgroups of persons defined by a label,
leaving out important algorithmic bias cases, where the sensitive variable is
continuous. Typical examples are unfair decisions made with respect to the age
or the financial status. In our work, we then propose a bias mitigation
strategy for continuous sensitive variables, based on the notion of endogeneity
which comes from the field of econometrics. In addition to solve this new
problem, our bias mitigation strategy is a weakly supervised learning method
which requires that a small portion of the data can be measured in a fair
manner. It is model agnostic, in the sense that it does not make any hypothesis
on the prediction model. It also makes use of a reasonably large amount of
input observations and their corresponding predictions. Only a small fraction
of the true output predictions should be known. This therefore limits the need
for expert interventions. Results obtained on synthetic data show the
effectiveness of our approach for examples as close as possible to real-life
applications in econometrics.</div><div><a href='http://arxiv.org/abs/2402.15477v1'>2402.15477v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01928v1")'>Robust Counterfactual Explanations in Machine Learning: A Survey</div>
<div id='2402.01928v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T21:56:58Z</div><div>Authors: Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni</div><div style='padding-top: 10px; width: 80ex'>Counterfactual explanations (CEs) are advocated as being ideally suited to
providing algorithmic recourse for subjects affected by the predictions of
machine learning models. While CEs can be beneficial to affected individuals,
recent work has exposed severe issues related to the robustness of
state-of-the-art methods for obtaining CEs. Since a lack of robustness may
compromise the validity of CEs, techniques to mitigate this risk are in order.
In this survey, we review works in the rapidly growing area of robust CEs and
perform an in-depth analysis of the forms of robustness they consider. We also
discuss existing solutions and their limitations, providing a solid foundation
for future developments.</div><div><a href='http://arxiv.org/abs/2402.01928v1'>2402.01928v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03773v1")'>Verified Training for Counterfactual Explanation Robustness under Data
  Shift</div>
<div id='2403.03773v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:06:16Z</div><div>Authors: Anna P. Meyer, Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni</div><div style='padding-top: 10px; width: 80ex'>Counterfactual explanations (CEs) enhance the interpretability of machine
learning models by describing what changes to an input are necessary to change
its prediction to a desired class. These explanations are commonly used to
guide users' actions, e.g., by describing how a user whose loan application was
denied can be approved for a loan in the future. Existing approaches generate
CEs by focusing on a single, fixed model, and do not provide any formal
guarantees on the CEs' future validity. When models are updated periodically to
account for data shift, if the generated CEs are not robust to the shifts,
users' actions may no longer have the desired impacts on their predictions.
This paper introduces VeriTraCER, an approach that jointly trains a classifier
and an explainer to explicitly consider the robustness of the generated CEs to
small model shifts. VeriTraCER optimizes over a carefully designed loss
function that ensures the verifiable robustness of CEs to local model updates,
thus providing deterministic guarantees to CE validity. Our empirical
evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably
robust to small model updates and (2) display competitive robustness to
state-of-the-art approaches in handling empirical model updates including
random initialization, leave-one-out, and distribution shifts.</div><div><a href='http://arxiv.org/abs/2403.03773v1'>2403.03773v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09702v3")'>Sparse and Faithful Explanations Without Sparse Models</div>
<div id='2402.09702v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T04:36:52Z</div><div>Authors: Yiyang Sun, Zhi Chen, Vittorio Orlandi, Tong Wang, Cynthia Rudin</div><div style='padding-top: 10px; width: 80ex'>Even if a model is not globally sparse, it is possible for decisions made
from that model to be accurately and faithfully described by a small number of
features. For instance, an application for a large loan might be denied to
someone because they have no credit history, which overwhelms any evidence
towards their creditworthiness. In this work, we introduce the Sparse
Explanation Value (SEV), a new way of measuring sparsity in machine learning
models. In the loan denial example above, the SEV is 1 because only one factor
is needed to explain why the loan was denied. SEV is a measure of decision
sparsity rather than overall model sparsity, and we are able to show that many
machine learning models -- even if they are not sparse -- actually have low
decision sparsity, as measured by SEV. SEV is defined using movements over a
hypercube, allowing SEV to be defined consistently over various model classes,
with movement restrictions reflecting real-world constraints. We proposed the
algorithms that reduce SEV without sacrificing accuracy, providing sparse and
completely faithful explanations, even without globally sparse models.</div><div><a href='http://arxiv.org/abs/2402.09702v3'>2402.09702v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17441v1")'>Explaining Predictive Uncertainty by Exposing Second-Order Effects</div>
<div id='2401.17441v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T21:02:21Z</div><div>Authors: Florian Bley, Sebastian Lapuschkin, Wojciech Samek, Grégoire Montavon</div><div style='padding-top: 10px; width: 80ex'>Explainable AI has brought transparency into complex ML blackboxes, enabling,
in particular, to identify which features these models use for their
predictions. So far, the question of explaining predictive uncertainty, i.e.
why a model 'doubts', has been scarcely studied. Our investigation reveals that
predictive uncertainty is dominated by second-order effects, involving single
features or product interactions between them. We contribute a new method for
explaining predictive uncertainty based on these second-order effects.
Computationally, our method reduces to a simple covariance computation over a
collection of first-order explanations. Our method is generally applicable,
allowing for turning common attribution techniques (LRP, Gradient x Input,
etc.) into powerful second-order uncertainty explainers, which we call CovLRP,
CovGI, etc. The accuracy of the explanations our method produces is
demonstrated through systematic quantitative evaluations, and the overall
usefulness of our method is demonstrated via two practical showcases.</div><div><a href='http://arxiv.org/abs/2401.17441v1'>2401.17441v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07072v1")'>Explainable Learning with Gaussian Processes</div>
<div id='2403.07072v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T18:03:02Z</div><div>Authors: Kurt Butler, Guanchao Feng, Petar M. Djuric</div><div style='padding-top: 10px; width: 80ex'>The field of explainable artificial intelligence (XAI) attempts to develop
methods that provide insight into how complicated machine learning methods make
predictions. Many methods of explanation have focused on the concept of feature
attribution, a decomposition of the model's prediction into individual
contributions corresponding to each input feature. In this work, we explore the
problem of feature attribution in the context of Gaussian process regression
(GPR). We take a principled approach to defining attributions under model
uncertainty, extending the existing literature. We show that although GPR is a
highly flexible and non-parametric approach, we can derive interpretable,
closed-form expressions for the feature attributions. When using integrated
gradients as an attribution method, we show that the attributions of a GPR
model also follow a Gaussian process distribution, which quantifies the
uncertainty in attribution arising from uncertainty in the model. We
demonstrate, both through theory and experimentation, the versatility and
robustness of this approach. We also show that, when applicable, the exact
expressions for GPR attributions are both more accurate and less
computationally expensive than the approximations currently used in practice.
The source code for this project is freely available under MIT license at
https://github.com/KurtButler/2024_attributions_paper.</div><div><a href='http://arxiv.org/abs/2403.07072v1'>2403.07072v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12118v1")'>DualView: Data Attribution from the Dual Perspective</div>
<div id='2402.12118v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T13:13:16Z</div><div>Authors: Galip Ümit Yolcu, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</div><div style='padding-top: 10px; width: 80ex'>Local data attribution (or influence estimation) techniques aim at estimating
the impact that individual data points seen during training have on particular
predictions of an already trained Machine Learning model during test time.
Previous methods either do not perform well consistently across different
evaluation criteria from literature, are characterized by a high computational
demand, or suffer from both. In this work we present DualView, a novel method
for post-hoc data attribution based on surrogate modelling, demonstrating both
high computational efficiency, as well as good evaluation results. With a focus
on neural networks, we evaluate our proposed technique using suitable
quantitative evaluation strategies from the literature against related
principal local data attribution methods. We find that DualView requires
considerably lower computational resources than other methods, while
demonstrating comparable performance to competing approaches across evaluation
metrics. Futhermore, our proposed method produces sparse explanations, where
sparseness can be tuned via a hyperparameter. Finally, we showcase that with
DualView, we can now render explanations from local data attributions
compatible with established local feature attribution methods: For each
prediction on (test) data points explained in terms of impactful samples from
the training set, we are able to compute and visualize how the prediction on
(test) sample relates to each influential training sample in terms of features
recognized and by the model. We provide an Open Source implementation of
DualView online, together with implementations for all other local data
attribution methods we compare against, as well as the metrics reported here,
for full reproducibility.</div><div><a href='http://arxiv.org/abs/2402.12118v1'>2402.12118v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15866v1")'>Stochastic Amortization: A Unified Approach to Accelerate Feature and
  Data Attribution</div>
<div id='2401.15866v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T03:42:37Z</div><div>Authors: Ian Covert, Chanwoo Kim, Su-In Lee, James Zou, Tatsunori Hashimoto</div><div style='padding-top: 10px; width: 80ex'>Many tasks in explainable machine learning, such as data valuation and
feature attribution, perform expensive computation for each data point and can
be intractable for large datasets. These methods require efficient
approximations, and learning a network that directly predicts the desired
output, which is commonly known as amortization, is a promising solution.
However, training such models with exact labels is often intractable; we
therefore explore training with noisy labels and find that this is inexpensive
and surprisingly effective. Through theoretical analysis of the label noise and
experiments with various models and datasets, we show that this approach
significantly accelerates several feature attribution and data valuation
methods, often yielding an order of magnitude speedup over existing approaches.</div><div><a href='http://arxiv.org/abs/2401.15866v1'>2401.15866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03838v1")'>Feature Selection as Deep Sequential Generative Learning</div>
<div id='2403.03838v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T16:31:56Z</div><div>Authors: Wangyang Ying, Dongjie Wang, Haifeng Chen, Yanjie Fu</div><div style='padding-top: 10px; width: 80ex'>Feature selection aims to identify the most pattern-discriminative feature
subset. In prior literature, filter (e.g., backward elimination) and embedded
(e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding)
and tie to specific models, thus, hard to generalize; wrapper methods search a
feature subset in a huge discrete space and is computationally costly. To
transform the way of feature selection, we regard a selected feature subset as
a selection decision token sequence and reformulate feature selection as a deep
sequential generative learning task that distills feature knowledge and
generates decision sequences. Our method includes three steps: (1) We develop a
deep variational transformer model over a joint of sequential reconstruction,
variational, and performance evaluator losses. Our model can distill feature
selection knowledge and learn a continuous embedding space to map feature
selection decision sequences into embedding vectors associated with utility
scores. (2) We leverage the trained feature subset utility evaluator as a
gradient provider to guide the identification of the optimal feature subset
embedding;(3) We decode the optimal feature subset embedding to
autoregressively generate the best feature selection decision sequence with
autostop. Extensive experimental results show this generative perspective is
effective and generic, without large discrete search space and expert-specific
hyperparameters.</div><div><a href='http://arxiv.org/abs/2403.03838v1'>2403.03838v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08845v1")'>Feature Attribution with Necessity and Sufficiency via Dual-stage
  Perturbation Test for Causal Explanation</div>
<div id='2402.08845v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T23:25:01Z</div><div>Authors: Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</div><div style='padding-top: 10px; width: 80ex'>We investigate the problem of explainability in machine learning.To address
this problem, Feature Attribution Methods (FAMs) measure the contribution of
each feature through a perturbation test, where the difference in prediction is
compared under different perturbations.However, such perturbation tests may not
accurately distinguish the contributions of different features, when their
change in prediction is the same after perturbation.In order to enhance the
ability of FAMs to distinguish different features' contributions in this
challenging setting, we propose to utilize the probability (PNS) that
perturbing a feature is a necessary and sufficient cause for the prediction to
change as a measure of feature importance.Our approach, Feature Attribution
with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test
involving two stages (factual and interventional).In practice, to generate
counterfactual samples, we use a resampling-based approach on the observed
samples to approximate the required conditional distribution.Finally, we
combine FANS and gradient-based optimization to extract the subset with the
largest PNS.We demonstrate that FANS outperforms existing feature attribution
methods on six benchmarks.</div><div><a href='http://arxiv.org/abs/2402.08845v1'>2402.08845v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02439v1")'>Root Causing Prediction Anomalies Using Explainable AI</div>
<div id='2403.02439v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:38:50Z</div><div>Authors: Ramanathan Vishnampet, Rajesh Shenoy, Jianhui Chen, Anuj Gupta</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel application of explainable AI (XAI) for
root-causing performance degradation in machine learning models that learn
continuously from user engagement data. In such systems a single feature
corruption can cause cascading feature, label and concept drifts. We have
successfully applied this technique to improve the reliability of models used
in personalized advertising. Performance degradation in such systems manifest
as prediction anomalies in the models. These models are typically trained
continuously using features that are produced by hundreds of real time data
processing pipelines or derived from other upstream models. A failure in any of
these pipelines or an instability in any of the upstream models can cause
feature corruption, causing the model's predicted output to deviate from the
actual output and the training data to become corrupted. The causal
relationship between the features and the predicted output is complex, and
root-causing is challenging due to the scale and dynamism of the system. We
demonstrate how temporal shifts in the global feature importance distribution
can effectively isolate the cause of a prediction anomaly, with better recall
than model-to-feature correlation methods. The technique appears to be
effective even when approximating the local feature importance using a simple
perturbation-based method, and aggregating over a few thousand examples. We
have found this technique to be a model-agnostic, cheap and effective way to
monitor complex data pipelines in production and have deployed a system for
continuously analyzing the global feature importance distribution of
continuously trained models.</div><div><a href='http://arxiv.org/abs/2403.02439v1'>2403.02439v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16294v1")'>Dual feature-based and example-based explanation methods</div>
<div id='2401.16294v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T16:53:04Z</div><div>Authors: Andrei V. Konstantinov, Boris V. Kozlov, Stanislav R. Kirpichenko, Lev V. Utkin</div><div style='padding-top: 10px; width: 80ex'>A new approach to the local and global explanation is proposed. It is based
on selecting a convex hull constructed for the finite number of points around
an explained instance. The convex hull allows us to consider a dual
representation of instances in the form of convex combinations of extreme
points of a produced polytope. Instead of perturbing new instances in the
Euclidean feature space, vectors of convex combination coefficients are
uniformly generated from the unit simplex, and they form a new dual dataset. A
dual linear surrogate model is trained on the dual dataset. The explanation
feature importance values are computed by means of simple matrix calculations.
The approach can be regarded as a modification of the well-known model LIME.
The dual representation inherently allows us to get the example-based
explanation. The neural additive model is also considered as a tool for
implementing the example-based explanation approach. Many numerical experiments
with real datasets are performed for studying the approach. The code of
proposed algorithms is available.</div><div><a href='http://arxiv.org/abs/2401.16294v1'>2401.16294v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09473v1")'>One-for-many Counterfactual Explanations by Column Generation</div>
<div id='2402.09473v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T10:03:31Z</div><div>Authors: Andrea Lodi, Jasone Ramírez-Ayerbe</div><div style='padding-top: 10px; width: 80ex'>In this paper, we consider the problem of generating a set of counterfactual
explanations for a group of instances, with the one-for-many allocation rule,
where one explanation is allocated to a subgroup of the instances. For the
first time, we solve the problem of minimizing the number of explanations
needed to explain all the instances, while considering sparsity by limiting the
number of features allowed to be changed collectively in each explanation. A
novel column generation framework is developed to efficiently search for the
explanations. Our framework can be applied to any black-box classifier, like
neural networks. Compared with a simple adaptation of a mixed-integer
programming formulation from the literature, the column generation framework
dominates in terms of scalability, computational performance and quality of the
solutions.</div><div><a href='http://arxiv.org/abs/2402.09473v1'>2402.09473v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13940v1")'>Multi-criteria approach for selecting an explanation from the set of
  counterfactuals produced by an ensemble of explainers</div>
<div id='2403.13940v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T19:25:11Z</div><div>Authors: Ignacy Stępka, Mateusz Lango, Jerzy Stefanowski</div><div style='padding-top: 10px; width: 80ex'>Counterfactuals are widely used to explain ML model predictions by providing
alternative scenarios for obtaining the more desired predictions. They can be
generated by a variety of methods that optimize different, sometimes
conflicting, quality measures and produce quite different solutions. However,
choosing the most appropriate explanation method and one of the generated
counterfactuals is not an easy task. Instead of forcing the user to test many
different explanation methods and analysing conflicting solutions, in this
paper, we propose to use a multi-stage ensemble approach that will select
single counterfactual based on the multiple-criteria analysis. It offers a
compromise solution that scores well on several popular quality measures. This
approach exploits the dominance relation and the ideal point decision aid
method, which selects one counterfactual from the Pareto front. The conducted
experiments demonstrated that the proposed approach generates fully actionable
counterfactuals with attractive compromise values of the considered quality
measures.</div><div><a href='http://arxiv.org/abs/2403.13940v1'>2403.13940v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08379v1")'>The Duet of Representations and How Explanations Exacerbate It</div>
<div id='2402.08379v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T11:18:27Z</div><div>Authors: Charles Wan, Rodrigo Belo, Leid Zejnilović, Susana Lavado</div><div style='padding-top: 10px; width: 80ex'>An algorithm effects a causal representation of relations between features
and labels in the human's perception. Such a representation might conflict with
the human's prior belief. Explanations can direct the human's attention to the
conflicting feature and away from other relevant features. This leads to causal
overattribution and may adversely affect the human's information processing. In
a field experiment we implemented an XGBoost-trained model as a decision-making
aid for counselors at a public employment service to predict candidates' risk
of long-term unemployment. The treatment group of counselors was also provided
with SHAP. The results show that the quality of the human's decision-making is
worse when a feature on which the human holds a conflicting prior belief is
displayed as part of the explanation.</div><div><a href='http://arxiv.org/abs/2402.08379v1'>2402.08379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08290v1")'>The Effect of Data Poisoning on Counterfactual Explanations</div>
<div id='2402.08290v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T08:41:32Z</div><div>Authors: André Artelt, Shubham Sharma, Freddy Lecué, Barbara Hammer</div><div style='padding-top: 10px; width: 80ex'>Counterfactual explanations provide a popular method for analyzing the
predictions of black-box systems, and they can offer the opportunity for
computational recourse by suggesting actionable changes on how to change the
input to obtain a different (i.e. more favorable) system output. However,
recent work highlighted their vulnerability to different types of
manipulations. This work studies the vulnerability of counterfactual
explanations to data poisoning. We formalize data poisoning in the context of
counterfactual explanations for increasing the cost of recourse on three
different levels: locally for a single instance, or a sub-group of instances,
or globally for all instances. We demonstrate that state-of-the-art
counterfactual generation methods \&amp; toolboxes are vulnerable to such data
poisoning.</div><div><a href='http://arxiv.org/abs/2402.08290v1'>2402.08290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11168v2")'>Trust Regions for Explanations via Black-Box Probabilistic Certification</div>
<div id='2402.11168v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T02:26:14Z</div><div>Authors: Amit Dhurandhar, Swagatam Haldar, Dennis Wei, Karthikeyan Natesan Ramamurthy</div><div style='padding-top: 10px; width: 80ex'>Given the black box nature of machine learning models, a plethora of
explainability methods have been developed to decipher the factors behind
individual decisions. In this paper, we introduce a novel problem of black box
(probabilistic) explanation certification. We ask the question: Given a black
box model with only query access, an explanation for an example and a quality
metric (viz. fidelity, stability), can we find the largest hypercube (i.e.,
$\ell_{\infty}$ ball) centered at the example such that when the explanation is
applied to all examples within the hypercube, (with high probability) a quality
criterion is met (viz. fidelity greater than some value)? Being able to
efficiently find such a \emph{trust region} has multiple benefits: i) insight
into model behavior in a \emph{region}, with a \emph{guarantee}; ii)
ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse},
which can save time, energy and money by not having to find explanations for
every example; and iv) a possible \emph{meta-metric} to compare explanation
methods. Our contributions include formalizing this problem, proposing
solutions, providing theoretical guarantees for these solutions that are
computable, and experimentally showing their efficacy on synthetic and real
data.</div><div><a href='http://arxiv.org/abs/2402.11168v2'>2402.11168v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10330v1")'>Towards Non-Adversarial Algorithmic Recourse</div>
<div id='2403.10330v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T14:18:21Z</div><div>Authors: Tobias Leemann, Martin Pawelczyk, Bardh Prenkaj, Gjergji Kasneci</div><div style='padding-top: 10px; width: 80ex'>The streams of research on adversarial examples and counterfactual
explanations have largely been growing independently. This has led to several
recent works trying to elucidate their similarities and differences. Most
prominently, it has been argued that adversarial examples, as opposed to
counterfactual explanations, have a unique characteristic in that they lead to
a misclassification compared to the ground truth. However, the computational
goals and methodologies employed in existing counterfactual explanation and
adversarial example generation methods often lack alignment with this
requirement. Using formal definitions of adversarial examples and
counterfactual explanations, we introduce non-adversarial algorithmic recourse
and outline why in high-stakes situations, it is imperative to obtain
counterfactual explanations that do not exhibit adversarial characteristics. We
subsequently investigate how different components in the objective functions,
e.g., the machine learning model or cost function used to measure distance,
determine whether the outcome can be considered an adversarial example or not.
Our experiments on common datasets highlight that these design choices are
often more critical in deciding whether recourse is non-adversarial than
whether recourse or attack algorithms are used. Furthermore, we show that
choosing a robust and accurate machine learning model results in less
adversarial recourse desired in practice.</div><div><a href='http://arxiv.org/abs/2403.10330v1'>2403.10330v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17516v2")'>QUCE: The Minimisation and Quantification of Path-Based Uncertainty for
  Generative Counterfactual Explanations</div>
<div id='2402.17516v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T14:00:08Z</div><div>Authors: Jamie Duell, Hsuan Fu, Monika Seisenberger, Xiuyi Fan</div><div style='padding-top: 10px; width: 80ex'>Deep Neural Networks (DNNs) stand out as one of the most prominent approaches
within the Machine Learning (ML) domain. The efficacy of DNNs has surged
alongside recent increases in computational capacity, allowing these approaches
to scale to significant complexities for addressing predictive challenges in
big data. However, as the complexity of DNN models rises, interpretability
diminishes. In response to this challenge, explainable models such as
Adversarial Gradient Integration (AGI) leverage path-based gradients provided
by DNNs to elucidate their decisions. Yet the performance of path-based
explainers can be compromised when gradients exhibit irregularities during
out-of-distribution path traversal. In this context, we introduce Quantified
Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate
out-of-distribution traversal by minimizing path uncertainty. QUCE not only
quantifies uncertainty when presenting explanations but also generates more
certain counterfactual examples. We showcase the performance of the QUCE method
by comparing it with competing methods for both path-based explanations and
generative counterfactual examples. The code repository for the QUCE method is
available at: https://github.com/jamie-duell/QUCE.</div><div><a href='http://arxiv.org/abs/2402.17516v2'>2402.17516v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14539v1")'>Understanding Disparities in Post Hoc Machine Learning Explanation</div>
<div id='2401.14539v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T22:09:28Z</div><div>Authors: Vishwali Mhasawade, Salman Rahman, Zoe Haskell-Craig, Rumi Chunara</div><div style='padding-top: 10px; width: 80ex'>Previous work has highlighted that existing post-hoc explanation methods
exhibit disparities in explanation fidelity (across 'race' and 'gender' as
sensitive attributes), and while a large body of work focuses on mitigating
these issues at the explanation metric level, the role of the data generating
process and black box model in relation to explanation disparities remains
largely unexplored. Accordingly, through both simulations as well as
experiments on a real-world dataset, we specifically assess challenges to
explanation disparities that originate from properties of the data: limited
sample size, covariate shift, concept shift, omitted variable bias, and
challenges based on model properties: inclusion of the sensitive attribute and
appropriate functional form. Through controlled simulation analyses, our study
demonstrates that increased covariate shift, concept shift, and omission of
covariates increase explanation disparities, with the effect pronounced higher
for neural network models that are better able to capture the underlying
functional form in comparison to linear models. We also observe consistent
findings regarding the effect of concept shift and omitted variable bias on
explanation disparities in the Adult income dataset. Overall, results indicate
that disparities in model explanations can also depend on data and model
properties. Based on this systematic investigation, we provide recommendations
for the design of explanation methods that mitigate undesirable disparities.</div><div><a href='http://arxiv.org/abs/2401.14539v1'>2401.14539v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15826v1")'>Reward Design for Justifiable Sequential Decision-Making</div>
<div id='2402.15826v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T14:29:30Z</div><div>Authors: Aleksa Sukovic, Goran Radanovic</div><div style='padding-top: 10px; width: 80ex'>Equipping agents with the capacity to justify made decisions using supporting
evidence represents a cornerstone of accountable decision-making. Furthermore,
ensuring that justifications are in line with human expectations and societal
norms is vital, especially in high-stakes situations such as healthcare. In
this work, we propose the use of a debate-based reward model for reinforcement
learning agents, where the outcome of a zero-sum debate game quantifies the
justifiability of a decision in a particular state. This reward model is then
used to train a justifiable policy, whose decisions can be more easily
corroborated with supporting evidence. In the debate game, two argumentative
agents take turns providing supporting evidence for two competing decisions.
Given the proposed evidence, a proxy of a human judge evaluates which decision
is better justified. We demonstrate the potential of our approach in learning
policies for prescribing and justifying treatment decisions of septic patients.
We show that augmenting the reward with the feedback signal generated by the
debate-based reward model yields policies highly favored by the judge when
compared to the policy obtained solely from the environment rewards, while
hardly sacrificing any performance. Moreover, in terms of the overall
performance and justifiability of trained policies, the debate-based feedback
is comparable to the feedback obtained from an ideal judge proxy that evaluates
decisions using the full information encoded in the state. This suggests that
the debate game outputs key information contained in states that is most
relevant for evaluating decisions, which in turn substantiates the practicality
of combining our approach with human-in-the-loop evaluations. Lastly, we
showcase that agents trained via multi-agent debate learn to propose evidence
that is resilient to refutations and closely aligns with human preferences.</div><div><a href='http://arxiv.org/abs/2402.15826v1'>2402.15826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14937v1")'>SoK: Analyzing Adversarial Examples: A Framework to Study Adversary
  Knowledge</div>
<div id='2402.14937v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T19:44:19Z</div><div>Authors: Lucas Fenaux, Florian Kerschbaum</div><div style='padding-top: 10px; width: 80ex'>Adversarial examples are malicious inputs to machine learning models that
trigger a misclassification. This type of attack has been studied for close to
a decade, and we find that there is a lack of study and formalization of
adversary knowledge when mounting attacks. This has yielded a complex space of
attack research with hard-to-compare threat models and attacks. We focus on the
image classification domain and provide a theoretical framework to study
adversary knowledge inspired by work in order theory. We present an adversarial
example game, inspired by cryptographic games, to standardize attacks. We
survey recent attacks in the image classification domain and classify their
adversary's knowledge in our framework. From this systematization, we compile
results that both confirm existing beliefs about adversary knowledge, such as
the potency of information about the attacked model as well as allow us to
derive new conclusions on the difficulty associated with the white-box and
transferable threat models, for example, that transferable attacks might not be
as difficult as previously thought.</div><div><a href='http://arxiv.org/abs/2402.14937v1'>2402.14937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15808v2")'>Optimal Zero-Shot Detector for Multi-Armed Attacks</div>
<div id='2402.15808v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T13:08:39Z</div><div>Authors: Federica Granese, Marco Romanelli, Pablo Piantanida</div><div style='padding-top: 10px; width: 80ex'>This paper explores a scenario in which a malicious actor employs a
multi-armed attack strategy to manipulate data samples, offering them various
avenues to introduce noise into the dataset. Our central objective is to
protect the data by detecting any alterations to the input. We approach this
defensive strategy with utmost caution, operating in an environment where the
defender possesses significantly less information compared to the attacker.
Specifically, the defender is unable to utilize any data samples for training a
defense model or verifying the integrity of the channel. Instead, the defender
relies exclusively on a set of pre-existing detectors readily available "off
the shelf". To tackle this challenge, we derive an innovative
information-theoretic defense approach that optimally aggregates the decisions
made by these detectors, eliminating the need for any training data. We further
explore a practical use-case scenario for empirical evaluation, where the
attacker possesses a pre-trained classifier and launches well-known adversarial
attacks against it. Our experiments highlight the effectiveness of our proposed
solution, even in scenarios that deviate from the optimal setup.</div><div><a href='http://arxiv.org/abs/2402.15808v2'>2402.15808v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14283v1")'>Information Leakage Detection through Approximate Bayes-optimal
  Prediction</div>
<div id='2401.14283v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T16:15:27Z</div><div>Authors: Pritha Gupta, Marcel Wever, Eyke Hüllermeier</div><div style='padding-top: 10px; width: 80ex'>In today's data-driven world, the proliferation of publicly available
information intensifies the challenge of information leakage (IL), raising
security concerns. IL involves unintentionally exposing secret (sensitive)
information to unauthorized parties via systems' observable information.
Conventional statistical approaches, which estimate mutual information (MI)
between observable and secret information for detecting IL, face challenges
such as the curse of dimensionality, convergence, computational complexity, and
MI misestimation. Furthermore, emerging supervised machine learning (ML)
methods, though effective, are limited to binary system-sensitive information
and lack a comprehensive theoretical framework. To address these limitations,
we establish a theoretical framework using statistical learning theory and
information theory to accurately quantify and detect IL. We demonstrate that MI
can be accurately estimated by approximating the log-loss and accuracy of the
Bayes predictor. As the Bayes predictor is typically unknown in practice, we
propose to approximate it with the help of automated machine learning (AutoML).
First, we compare our MI estimation approaches against current baselines, using
synthetic data sets generated using the multivariate normal (MVN) distribution
with known MI. Second, we introduce a cut-off technique using one-sided
statistical tests to detect IL, employing the Holm-Bonferroni correction to
increase confidence in detection decisions. Our study evaluates IL detection
performance on real-world data sets, highlighting the effectiveness of the
Bayes predictor's log-loss estimation, and finds our proposed method to
effectively estimate MI on synthetic data sets and thus detect ILs accurately.</div><div><a href='http://arxiv.org/abs/2401.14283v1'>2401.14283v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02718v1")'>Calibration Attack: A Framework For Adversarial Attacks Targeting
  Calibration</div>
<div id='2401.02718v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T09:21:54Z</div><div>Authors: Stephen Obadinma, Xiaodan Zhu, Hongyu Guo</div><div style='padding-top: 10px; width: 80ex'>We introduce a new framework of adversarial attacks, named calibration
attacks, in which the attacks are generated and organized to trap victim models
to be miscalibrated without altering their original accuracy, hence seriously
endangering the trustworthiness of the models and any decision-making based on
their confidence scores. Specifically, we identify four novel forms of
calibration attacks: underconfidence attacks, overconfidence attacks, maximum
miscalibration attacks, and random confidence attacks, in both the black-box
and white-box setups. We then test these new attacks on typical victim models
with comprehensive datasets, demonstrating that even with a relatively low
number of queries, the attacks can create significant calibration mistakes. We
further provide detailed analyses to understand different aspects of
calibration attacks. Building on that, we investigate the effectiveness of
widely used adversarial defences and calibration methods against these types of
attacks, which then inspires us to devise two novel defences against such
calibration attacks.</div><div><a href='http://arxiv.org/abs/2401.02718v1'>2401.02718v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14090v1")'>A Modular Approach to Automatic Cyber Threat Attribution using Opinion
  Pools</div>
<div id='2401.14090v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:12:16Z</div><div>Authors: Koen T. W. Teuwen</div><div style='padding-top: 10px; width: 80ex'>Cyber threat attribution can play an important role in increasing resilience
against digital threats. Recent research focuses on automating the threat
attribution process and on integrating it with other efforts, such as threat
hunting. To support increasing automation of the cyber threat attribution
process, this paper proposes a modular architecture as an alternative to
current monolithic automated approaches. The modular architecture can utilize
opinion pools to combine the output of concrete attributors. The proposed
solution increases the tractability of the threat attribution problem and
offers increased usability and interpretability, as opposed to monolithic
alternatives. In addition, a Pairing Aggregator is proposed as an aggregation
method that forms pairs of attributors based on distinct features to produce
intermediary results before finally producing a single Probability Mass
Function (PMF) as output. The Pairing Aggregator sequentially applies both the
logarithmic opinion pool and the linear opinion pool. An experimental
validation suggests that the modular approach does not result in decreased
performance and can even enhance precision and recall compared to monolithic
alternatives. The results also suggest that the Pairing Aggregator can improve
precision over the linear and logarithmic opinion pools. Furthermore, the
improved k-accuracy in the experiment suggests that forensic experts can
leverage the resulting PMF during their manual attribution processes to enhance
their efficiency.</div><div><a href='http://arxiv.org/abs/2401.14090v1'>2401.14090v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.01423v1")'>Collective Certified Robustness against Graph Injection Attacks</div>
<div id='2403.01423v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T07:45:35Z</div><div>Authors: Yuni Lai, Bailin Pan, Kaihuang Chen, Yancheng Yuan, Kai Zhou</div><div style='padding-top: 10px; width: 80ex'>We investigate certified robustness for GNNs under graph injection attacks.
Existing research only provides sample-wise certificates by verifying each node
independently, leading to very limited certifying performance. In this paper,
we present the first collective certificate, which certifies a set of target
nodes simultaneously. To achieve it, we formulate the problem as a binary
integer quadratic constrained linear programming (BQCLP). We further develop a
customized linearization technique that allows us to relax the BQCLP into
linear programming (LP) that can be efficiently solved. Through comprehensive
experiments, we demonstrate that our collective certification scheme
significantly improves certification performance with minimal computational
overhead. For instance, by solving the LP within 1 minute on the Citeseer
dataset, we achieve a significant increase in the certified ratio from 0.0% to
81.2% when the injected node number is 5% of the graph size. Our step marks a
crucial step towards making provable defense more practical.</div><div><a href='http://arxiv.org/abs/2403.01423v1'>2403.01423v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08586v1")'>Faster Repeated Evasion Attacks in Tree Ensembles</div>
<div id='2402.08586v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:44:02Z</div><div>Authors: Lorenzo Cascioli, Laurens Devos, Ondřej Kuželka, Jesse Davis</div><div style='padding-top: 10px; width: 80ex'>Tree ensembles are one of the most widely used model classes. However, these
models are susceptible to adversarial examples, i.e., slightly perturbed
examples that elicit a misprediction. There has been significant research on
designing approaches to construct such examples for tree ensembles. But this is
a computationally challenging problem that often must be solved a large number
of times (e.g., for all examples in a training set). This is compounded by the
fact that current approaches attempt to find such examples from scratch. In
contrast, we exploit the fact that multiple similar problems are being solved.
Specifically, our approach exploits the insight that adversarial examples for
tree ensembles tend to perturb a consistent but relatively small set of
features. We show that we can quickly identify this set of features and use
this knowledge to speedup constructing adversarial examples.</div><div><a href='http://arxiv.org/abs/2402.08586v1'>2402.08586v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13866v1")'>The Bid Picture: Auction-Inspired Multi-player Generative Adversarial
  Networks Training</div>
<div id='2403.13866v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T11:47:42Z</div><div>Authors: Joo Yong Shim, Jean Seong Bjorn Choe, Jong-Kook Kim</div><div style='padding-top: 10px; width: 80ex'>This article proposes auction-inspired multi-player generative adversarial
networks training, which mitigates the mode collapse problem of GANs. Mode
collapse occurs when an over-fitted generator generates a limited range of
samples, often concentrating on a small subset of the data distribution.
Despite the restricted diversity of generated samples, the discriminator can
still be deceived into distinguishing these samples as real samples from the
actual distribution. In the absence of external standards, a model cannot
recognize its failure during the training phase. We extend the two-player game
of generative adversarial networks to the multi-player game. During the
training, the values of each model are determined by the bids submitted by
other players in an auction-like process.</div><div><a href='http://arxiv.org/abs/2403.13866v1'>2403.13866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08513v2")'>X Hacking: The Threat of Misguided AutoML</div>
<div id='2401.08513v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:21:33Z</div><div>Authors: Rahul Sharma, Sergey Redyuk, Sumantrak Mukherjee, Andrea Sipka, Sebastian Vollmer, David Selby</div><div style='padding-top: 10px; width: 80ex'>Explainable AI (XAI) and interpretable machine learning methods help to build
trust in model predictions and derived insights, yet also present a perverse
incentive for analysts to manipulate XAI metrics to support pre-specified
conclusions. This paper introduces the concept of X-hacking, a form of
p-hacking applied to XAI metrics such as Shap values. We show how an automated
machine learning pipeline can be used to search for 'defensible' models that
produce a desired explanation while maintaining superior predictive performance
to a common baseline. We formulate the trade-off between explanation and
accuracy as a multi-objective optimization problem and illustrate the
feasibility and severity of X-hacking empirically on familiar real-world
datasets. Finally, we suggest possible methods for detection and prevention,
and discuss ethical implications for the credibility and reproducibility of XAI
research.</div><div><a href='http://arxiv.org/abs/2401.08513v2'>2401.08513v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12731v1")'>The Distributional Uncertainty of the SHAP score in Explainable Machine
  Learning</div>
<div id='2401.12731v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T13:04:02Z</div><div>Authors: Santiago Cifuentes, Leopoldo Bertossi, Nina Pardal, Sergio Abriola, Maria Vanina Martinez, Miguel Romero</div><div style='padding-top: 10px; width: 80ex'>Attribution scores reflect how important the feature values in an input
entity are for the output of a machine learning model. One of the most popular
attribution scores is the SHAP score, which is an instantiation of the general
Shapley value used in coalition game theory. The definition of this score
relies on a probability distribution on the entity population. Since the exact
distribution is generally unknown, it needs to be assigned subjectively or be
estimated from data, which may lead to misleading feature scores. In this
paper, we propose a principled framework for reasoning on SHAP scores under
unknown entity population distributions. In our framework, we consider an
uncertainty region that contains the potential distributions, and the SHAP
score of a feature becomes a function defined over this region. We study the
basic problems of finding maxima and minima of this function, which allows us
to determine tight ranges for the SHAP scores of all features. In particular,
we pinpoint the complexity of these problems, and other related ones, showing
them to be NP-complete. Finally, we present experiments on a real-world
dataset, showing that our framework may contribute to a more robust feature
scoring.</div><div><a href='http://arxiv.org/abs/2401.12731v1'>2401.12731v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16754v2")'>AI Oversight and Human Mistakes: Evidence from Centre Court</div>
<div id='2401.16754v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T05:22:45Z</div><div>Authors: David Almog, Romain Gauriot, Lionel Page, Daniel Martin</div><div style='padding-top: 10px; width: 80ex'>Powered by the increasing predictive capabilities of machine learning
algorithms, artificial intelligence (AI) systems have begun to be used to
overrule human mistakes in many settings. We provide the first field evidence
this AI oversight carries psychological costs that can impact human
decision-making. We investigate one of the highest visibility settings in which
AI oversight has occurred: the Hawk-Eye review of umpires in top tennis
tournaments. We find that umpires lowered their overall mistake rate after the
introduction of Hawk-Eye review, in line with rational inattention given
psychological costs of being overruled by AI. We also find that umpires
increased the rate at which they called balls in, which produced a shift from
making Type II errors (calling a ball out when in) to Type I errors (calling a
ball in when out). We structurally estimate the psychological costs of being
overruled by AI using a model of rational inattentive umpires, and our results
suggest that because of these costs, umpires cared twice as much about Type II
errors under AI oversight.</div><div><a href='http://arxiv.org/abs/2401.16754v2'>2401.16754v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02062v1")'>U-Trustworthy Models.Reliability, Competence, and Confidence in
  Decision-Making</div>
<div id='2401.02062v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T04:58:02Z</div><div>Authors: Ritwik Vashistha, Arya Farahi</div><div style='padding-top: 10px; width: 80ex'>With growing concerns regarding bias and discrimination in predictive models,
the AI community has increasingly focused on assessing AI system
trustworthiness. Conventionally, trustworthy AI literature relies on the
probabilistic framework and calibration as prerequisites for trustworthiness.
In this work, we depart from this viewpoint by proposing a novel trust
framework inspired by the philosophy literature on trust. We present a precise
mathematical definition of trustworthiness, termed
$\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks
aimed at maximizing a utility function. We argue that a model's
$\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes
utility within this task subset. Our first set of results challenges the
probabilistic framework by demonstrating its potential to favor less
trustworthy models and introduce the risk of misleading trustworthiness
assessments. Within the context of $\mathcal{U}$-trustworthiness, we prove that
properly-ranked models are inherently $\mathcal{U}$-trustworthy. Furthermore,
we advocate for the adoption of the AUC metric as the preferred measure of
trustworthiness. By offering both theoretical guarantees and experimental
validation, AUC enables robust evaluation of trustworthiness, thereby enhancing
model selection and hyperparameter tuning to yield more trustworthy outcomes.</div><div><a href='http://arxiv.org/abs/2401.02062v1'>2401.02062v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14711v1")'>Human-in-the-Loop AI for Cheating Ring Detection</div>
<div id='2403.14711v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:25:57Z</div><div>Authors: Yong-Siang Shih, Manqian Liao, Ruidong Liu, Mirza Basim Baig</div><div style='padding-top: 10px; width: 80ex'>Online exams have become popular in recent years due to their accessibility.
However, some concerns have been raised about the security of the online exams,
particularly in the context of professional cheating services aiding malicious
test takers in passing exams, forming so-called "cheating rings". In this
paper, we introduce a human-in-the-loop AI cheating ring detection system
designed to detect and deter these cheating rings. We outline the underlying
logic of this human-in-the-loop AI system, exploring its design principles
tailored to achieve its objectives of detecting cheaters. Moreover, we
illustrate the methodologies used to evaluate its performance and fairness,
aiming to mitigate the unintended risks associated with the AI system. The
design and development of the system adhere to Responsible AI (RAI) standards,
ensuring that ethical considerations are integrated throughout the entire
development process.</div><div><a href='http://arxiv.org/abs/2403.14711v1'>2403.14711v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00773v1")'>Misconduct in Post-Selections and Deep Learning</div>
<div id='2403.00773v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T17:11:08Z</div><div>Authors: Juyang Weng</div><div style='padding-top: 10px; width: 80ex'>This is a theoretical paper on "Deep Learning" misconduct in particular and
Post-Selection in general. As far as the author knows, the first peer-reviewed
papers on Deep Learning misconduct are [32], [37], [36]. Regardless of learning
modes, e.g., supervised, reinforcement, adversarial, and evolutional, almost
all machine learning methods (except for a few methods that train a sole
system) are rooted in the same misconduct -- cheating and hiding -- (1)
cheating in the absence of a test and (2) hiding bad-looking data. It was
reasoned in [32], [37], [36] that authors must report at least the average
error of all trained networks, good and bad, on the validation set (called
general cross-validation in this paper). Better, report also five percentage
positions of ranked errors. From the new analysis here, we can see that the
hidden culprit is Post-Selection. This is also true for Post-Selection on
hand-tuned or searched hyperparameters, because they are random, depending on
random observation data. Does cross-validation on data splits rescue
Post-Selections from the Misconducts (1) and (2)? The new result here says: No.
Specifically, this paper reveals that using cross-validation for data splits is
insufficient to exonerate Post-Selections in machine learning. In general,
Post-Selections of statistical learners based on their errors on the validation
set are statistically invalid.</div><div><a href='http://arxiv.org/abs/2403.00773v1'>2403.00773v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12708v1")'>Deep Neural Network Benchmarks for Selective Classification</div>
<div id='2401.12708v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T12:15:47Z</div><div>Authors: Andrea Pugnana, Lorenzo Perini, Jesse Davis, Salvatore Ruggieri</div><div style='padding-top: 10px; width: 80ex'>With the increasing deployment of machine learning models in many
socially-sensitive tasks, there is a growing demand for reliable and
trustworthy predictions. One way to accomplish these requirements is to allow a
model to abstain from making a prediction when there is a high risk of making
an error. This requires adding a selection mechanism to the model, which
selects those examples for which the model will provide a prediction. The
selective classification framework aims to design a mechanism that balances the
fraction of rejected predictions (i.e., the proportion of examples for which
the model does not make a prediction) versus the improvement in predictive
performance on the selected predictions. Multiple selective classification
frameworks exist, most of which rely on deep neural network architectures.
However, the empirical evaluation of the existing approaches is still limited
to partial comparisons among methods and settings, providing practitioners with
little insight into their relative merits. We fill this gap by benchmarking 18
baselines on a diverse set of 44 datasets that includes both image and tabular
data. Moreover, there is a mix of binary and multiclass tasks. We evaluate
these approaches using several criteria, including selective error rate,
empirical coverage, distribution of rejected instance's classes, and
performance on out-of-distribution instances. The results indicate that there
is not a single clear winner among the surveyed baselines, and the best method
depends on the users' objectives.</div><div><a href='http://arxiv.org/abs/2401.12708v1'>2401.12708v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03208v1")'>Active Statistical Inference</div>
<div id='2403.03208v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:46:50Z</div><div>Authors: Tijana Zrnic, Emmanuel J. Candès</div><div style='padding-top: 10px; width: 80ex'>Inspired by the concept of active learning, we propose active
inference$\unicode{x2013}$a methodology for statistical inference with
machine-learning-assisted data collection. Assuming a budget on the number of
labels that can be collected, the methodology uses a machine learning model to
identify which data points would be most beneficial to label, thus effectively
utilizing the budget. It operates on a simple yet powerful intuition:
prioritize the collection of labels for data points where the model exhibits
uncertainty, and rely on the model's predictions where it is confident. Active
inference constructs provably valid confidence intervals and hypothesis tests
while leveraging any black-box machine learning model and handling any data
distribution. The key point is that it achieves the same level of accuracy with
far fewer samples than existing baselines relying on non-adaptively-collected
data. This means that for the same number of collected samples, active
inference enables smaller confidence intervals and more powerful p-values. We
evaluate active inference on datasets from public opinion research, census
analysis, and proteomics.</div><div><a href='http://arxiv.org/abs/2403.03208v1'>2403.03208v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17238v1")'>Does Negative Sampling Matter? A Review with Insights into its Theory
  and Applications</div>
<div id='2402.17238v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T06:13:02Z</div><div>Authors: Zhen Yang, Ming Ding, Tinglin Huang, Yukuo Cen, Junshuai Song, Bin Xu, Yuxiao Dong, Jie Tang</div><div style='padding-top: 10px; width: 80ex'>Negative sampling has swiftly risen to prominence as a focal point of
research, with wide-ranging applications spanning machine learning, computer
vision, natural language processing, data mining, and recommender systems. This
growing interest raises several critical questions: Does negative sampling
really matter? Is there a general framework that can incorporate all existing
negative sampling methods? In what fields is it applied? Addressing these
questions, we propose a general framework that leverages negative sampling.
Delving into the history of negative sampling, we trace the development of
negative sampling through five evolutionary paths. We dissect and categorize
the strategies used to select negative sample candidates, detailing global,
local, mini-batch, hop, and memory-based approaches. Our review categorizes
current negative sampling methods into five types: static, hard, GAN-based,
Auxiliary-based, and In-batch methods, providing a clear structure for
understanding negative sampling. Beyond detailed categorization, we highlight
the application of negative sampling in various areas, offering insights into
its practical benefits. Finally, we briefly discuss open problems and future
directions for negative sampling.</div><div><a href='http://arxiv.org/abs/2402.17238v1'>2402.17238v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09376v1")'>Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter
  Paradigm for Performance Estimation in Online and Static Settings</div>
<div id='2401.09376v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T17:46:10Z</div><div>Authors: Kevin Slote, Elaine Lee</div><div style='padding-top: 10px; width: 80ex'>In the realm of machine learning and statistical modeling, practitioners
often work under the assumption of accessible, static, labeled data for
evaluation and training. However, this assumption often deviates from reality
where data may be private, encrypted, difficult- to-measure, or unlabeled. In
this paper, we bridge this gap by adapting the Hui-Walter paradigm, a method
traditionally applied in epidemiology and medicine, to the field of machine
learning. This approach enables us to estimate key performance metrics such as
false positive rate, false negative rate, and priors in scenarios where no
ground truth is available. We further extend this paradigm for handling online
data, opening up new possibilities for dynamic data environments. Our
methodology involves partitioning data into latent classes to simulate multiple
data populations (if natural populations are unavailable) and independently
training models to replicate multiple tests. By cross-tabulating binary
outcomes across ensemble categorizers and multiple populations, we are able to
estimate unknown parameters through Gibbs sampling, eliminating the need for
ground-truth or labeled data. This paper showcases the potential of our
methodology to transform machine learning practices by allowing for accurate
model assessment under dynamic and uncertain data conditions.</div><div><a href='http://arxiv.org/abs/2401.09376v1'>2401.09376v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15365v1")'>Efficient semi-supervised inference for logistic regression under
  case-control studies</div>
<div id='2402.15365v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:55:58Z</div><div>Authors: Zhuojun Quan, Yuanyuan Lin, Kani Chen, Wen Yu</div><div style='padding-top: 10px; width: 80ex'>Semi-supervised learning has received increasingly attention in statistics
and machine learning. In semi-supervised learning settings, a labeled data set
with both outcomes and covariates and an unlabeled data set with covariates
only are collected. We consider an inference problem in semi-supervised
settings where the outcome in the labeled data is binary and the labeled data
is collected by case-control sampling. Case-control sampling is an effective
sampling scheme for alleviating imbalance structure in binary data. Under the
logistic model assumption, case-control data can still provide consistent
estimator for the slope parameter of the regression model. However, the
intercept parameter is not identifiable. Consequently, the marginal case
proportion cannot be estimated from case-control data. We find out that with
the availability of the unlabeled data, the intercept parameter can be
identified in semi-supervised learning setting. We construct the likelihood
function of the observed labeled and unlabeled data and obtain the maximum
likelihood estimator via an iterative algorithm. The proposed estimator is
shown to be consistent, asymptotically normal, and semiparametrically
efficient. Extensive simulation studies are conducted to show the finite sample
performance of the proposed method. The results imply that the unlabeled data
not only helps to identify the intercept but also improves the estimation
efficiency of the slope parameter. Meanwhile, the marginal case proportion can
be estimated accurately by the proposed method.</div><div><a href='http://arxiv.org/abs/2402.15365v1'>2402.15365v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12367v1")'>Semisupervised score based matching algorithm to evaluate the effect of
  public health interventions</div>
<div id='2403.12367v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T02:24:16Z</div><div>Authors: Hongzhe Zhang, Jiasheng Shi, Jing Huang</div><div style='padding-top: 10px; width: 80ex'>Multivariate matching algorithms "pair" similar study units in an
observational study to remove potential bias and confounding effects caused by
the absence of randomizations. In one-to-one multivariate matching algorithms,
a large number of "pairs" to be matched could mean both the information from a
large sample and a large number of tasks, and therefore, to best match the
pairs, such a matching algorithm with efficiency and comparatively limited
auxiliary matching knowledge provided through a "training" set of paired units
by domain experts, is practically intriguing.
  We proposed a novel one-to-one matching algorithm based on a quadratic score
function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights
$\beta$, which can be interpreted as a variable importance measure, are
designed to minimize the score difference between paired training units while
maximizing the score difference between unpaired training units. Further, in
the typical but intricate case where the training set is much smaller than the
unpaired set, we propose a \underline{s}emisupervised \underline{c}ompanion
\underline{o}ne-\underline{t}o-\underline{o}ne \underline{m}atching
\underline{a}lgorithm (SCOTOMA) that makes the best use of the unpaired units.
The proposed weight estimator is proved to be consistent when the truth
matching criterion is indeed the quadratic score function. When the model
assumptions are violated, we demonstrate that the proposed algorithm still
outperforms some popular competing matching algorithms through a series of
simulations. We applied the proposed algorithm to a real-world study to
investigate the effect of in-person schooling on community Covid-19
transmission rate for policy making purpose.</div><div><a href='http://arxiv.org/abs/2403.12367v1'>2403.12367v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00728v1")'>Dropout-Based Rashomon Set Exploration for Efficient Predictive
  Multiplicity Estimation</div>
<div id='2402.00728v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:25:00Z</div><div>Authors: Hsiang Hsu, Guihong Li, Shaohan Hu, Chun-Fu, Chen</div><div style='padding-top: 10px; width: 80ex'>Predictive multiplicity refers to the phenomenon in which classification
tasks may admit multiple competing models that achieve almost-equally-optimal
performance, yet generate conflicting outputs for individual samples. This
presents significant concerns, as it can potentially result in systemic
exclusion, inexplicable discrimination, and unfairness in practical
applications. Measuring and mitigating predictive multiplicity, however, is
computationally challenging due to the need to explore all such
almost-equally-optimal models, known as the Rashomon set, in potentially huge
hypothesis spaces. To address this challenge, we propose a novel framework that
utilizes dropout techniques for exploring models in the Rashomon set. We
provide rigorous theoretical derivations to connect the dropout parameters to
properties of the Rashomon set, and empirically evaluate our framework through
extensive experimentation. Numerical results show that our technique
consistently outperforms baselines in terms of the effectiveness of predictive
multiplicity metric estimation, with runtime speedup up to $20\times \sim
5000\times$. With efficient Rashomon set exploration and metric estimation,
mitigation of predictive multiplicity is then achieved through dropout ensemble
and model selection.</div><div><a href='http://arxiv.org/abs/2402.00728v1'>2402.00728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14058v1")'>Hypothesis-Driven Deep Learning for Out of Distribution Detection</div>
<div id='2403.14058v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T01:06:47Z</div><div>Authors: Yasith Jayawardana, Azeem Ahmad, Balpreet S. Ahluwalia, Rafi Ahmad, Sampath Jayarathna, Dushan N. Wadduwage</div><div style='padding-top: 10px; width: 80ex'>Predictions of opaque black-box systems are frequently deployed in
high-stakes applications such as healthcare. For such applications, it is
crucial to assess how models handle samples beyond the domain of training data.
While several metrics and tests exist to detect out-of-distribution (OoD) data
from in-distribution (InD) data to a deep neural network (DNN), their
performance varies significantly across datasets, models, and tasks, which
limits their practical use. In this paper, we propose a hypothesis-driven
approach to quantify whether a new sample is InD or OoD. Given a trained DNN
and some input, we first feed the input through the DNN and compute an ensemble
of OoD metrics, which we term latent responses. We then formulate the OoD
detection problem as a hypothesis test between latent responses of different
groups, and use permutation-based resampling to infer the significance of the
observed latent responses under a null hypothesis. We adapt our method to
detect an unseen sample of bacteria to a trained deep learning model, and show
that it reveals interpretable differences between InD and OoD latent responses.
Our work has implications for systematic novelty detection and informed
decision-making from classifiers trained on a subset of labels.</div><div><a href='http://arxiv.org/abs/2403.14058v1'>2403.14058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10686v1")'>Uncertainty, Calibration, and Membership Inference Attacks: An
  Information-Theoretic Perspective</div>
<div id='2402.10686v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T13:41:18Z</div><div>Authors: Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone</div><div style='padding-top: 10px; width: 80ex'>In a membership inference attack (MIA), an attacker exploits the
overconfidence exhibited by typical machine learning models to determine
whether a specific data point was used to train a target model. In this paper,
we analyze the performance of the state-of-the-art likelihood ratio attack
(LiRA) within an information-theoretical framework that allows the
investigation of the impact of the aleatoric uncertainty in the true data
generation process, of the epistemic uncertainty caused by a limited training
data set, and of the calibration level of the target model. We compare three
different settings, in which the attacker receives decreasingly informative
feedback from the target model: confidence vector (CV) disclosure, in which the
output probability vector is released; true label confidence (TLC) disclosure,
in which only the probability assigned to the true label is made available by
the model; and decision set (DS) disclosure, in which an adaptive prediction
set is produced as in conformal prediction. We derive bounds on the advantage
of an MIA adversary with the aim of offering insights into the impact of
uncertainty and calibration on the effectiveness of MIAs. Simulation results
demonstrate that the derived analytical bounds predict well the effectiveness
of MIAs.</div><div><a href='http://arxiv.org/abs/2402.10686v1'>2402.10686v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14069v1")'>Sampling Audit Evidence Using a Naive Bayes Classifier</div>
<div id='2403.14069v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T01:35:03Z</div><div>Authors: Guang-Yih Sheu, Nai-Ru Liu</div><div style='padding-top: 10px; width: 80ex'>Taiwan's auditors have suffered from processing excessive audit data,
including drawing audit evidence. This study advances sampling techniques by
integrating machine learning with sampling. This machine learning integration
helps avoid sampling bias, keep randomness and variability, and target risker
samples. We first classify data using a Naive Bayes classifier into some
classes. Next, a user-based, item-based, or hybrid approach is employed to draw
audit evidence. The representativeness index is the primary metric for
measuring its representativeness. The user-based approach samples data
symmetric around the median of a class as audit evidence. It may be equivalent
to a combination of monetary and variable samplings. The item-based approach
represents asymmetric sampling based on posterior probabilities for obtaining
risky samples as audit evidence. It may be identical to a combination of
non-statistical and monetary samplings. Auditors can hybridize those user-based
and item-based approaches to balance representativeness and riskiness in
selecting audit evidence. Three experiments show that sampling using machine
learning integration has the benefits of drawing unbiased samples, handling
complex patterns, correlations, and unstructured data, and improving efficiency
in sampling big data. However, the limitations are the classification accuracy
output by machine learning algorithms and the range of prior probabilities.</div><div><a href='http://arxiv.org/abs/2403.14069v1'>2403.14069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15472v1")'>FAIR: Filtering of Automatically Induced Rules</div>
<div id='2402.15472v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T18:04:54Z</div><div>Authors: Divya Jyoti Bajpai, Ayush Maheshwari, Manjesh Kumar Hanawal, Ganesh Ramakrishnan</div><div style='padding-top: 10px; width: 80ex'>The availability of large annotated data can be a critical bottleneck in
training machine learning algorithms successfully, especially when applied to
diverse domains. Weak supervision offers a promising alternative by
accelerating the creation of labeled training data using domain-specific rules.
However, it requires users to write a diverse set of high-quality rules to
assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches
circumvent this problem by automatically creating rules from features on a
small labeled set and filtering a final set of rules from them. In the ARI
approach, the crucial step is to filter out a set of a high-quality useful
subset of rules from the large set of automatically created rules. In this
paper, we propose an algorithm (Filtering of Automatically Induced Rules) to
filter rules from a large number of automatically induced rules using
submodular objective functions that account for the collective precision,
coverage, and conflicts of the rule set. We experiment with three ARI
approaches and five text classification datasets to validate the superior
performance of our algorithm with respect to several semi-supervised label
aggregation approaches. Further, we show that achieves statistically
significant results in comparison to existing rule-filtering approaches.</div><div><a href='http://arxiv.org/abs/2402.15472v1'>2402.15472v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08209v1")'>Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits</div>
<div id='2402.08209v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T04:17:48Z</div><div>Authors: Hiroyuki Namba, Shota Horiguchi, Masaki Hamamoto, Masashi Egi</div><div style='padding-top: 10px; width: 80ex'>Data cleansing aims to improve model performance by removing a set of harmful
instances from the training dataset. Data Shapley is a common theoretically
guaranteed method to evaluate the contribution of each instance to model
performance; however, it requires training on all subsets of the training data,
which is computationally expensive. In this paper, we propose an
iterativemethod to fast identify a subset of instances with low data Shapley
values by using the thresholding bandit algorithm. We provide a theoretical
guarantee that the proposed method can accurately select harmful instances if a
sufficiently large number of iterations is conducted. Empirical evaluation
using various models and datasets demonstrated that the proposed method
efficiently improved the computational speed while maintaining the model
performance.</div><div><a href='http://arxiv.org/abs/2402.08209v1'>2402.08209v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06419v1")'>Causal Multi-Label Feature Selection in Federated Setting</div>
<div id='2403.06419v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T04:11:48Z</div><div>Authors: Yukun Song, Dayuan Cao, Jiali Miao, Shuai Yang, Kui Yu</div><div style='padding-top: 10px; width: 80ex'>Multi-label feature selection serves as an effective mean for dealing with
high-dimensional multi-label data. To achieve satisfactory performance,
existing methods for multi-label feature selection often require the
centralization of substantial data from multiple sources. However, in Federated
setting, centralizing data from all sources and merging them into a single
dataset is not feasible. To tackle this issue, in this paper, we study a
challenging problem of causal multi-label feature selection in federated
setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS)
algorithm with three novel subroutines. Specifically, FedCMFS first uses the
FedCFL subroutine that considers the correlations among label-label,
label-feature, and feature-feature to learn the relevant features (candidate
parents and children) of each class label while preserving data privacy without
centralizing data. Second, FedCMFS employs the FedCFR subroutine to selectively
recover the missed true relevant features. Finally, FedCMFS utilizes the FedCFC
subroutine to remove false relevant features. The extensive experiments on 8
datasets have shown that FedCMFS is effect for causal multi-label feature
selection in federated setting.</div><div><a href='http://arxiv.org/abs/2403.06419v1'>2403.06419v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14294v1")'>High-arity PAC learning via exchangeability</div>
<div id='2402.14294v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T05:16:04Z</div><div>Authors: Leonardo N. Coregliano, Maryanthe Malliaris</div><div style='padding-top: 10px; width: 80ex'>We develop a theory of high-arity PAC learning, which is statistical learning
in the presence of "structured correlation". In this theory, hypotheses are
either graphs, hypergraphs or, more generally, structures in finite relational
languages, and i.i.d. sampling is replaced by sampling an induced substructure,
producing an exchangeable distribution. We prove a high-arity version of the
fundamental theorem of statistical learning by characterizing high-arity
(agnostic) PAC learnability in terms of finiteness of a purely combinatorial
dimension and in terms of an appropriate version of uniform convergence.</div><div><a href='http://arxiv.org/abs/2402.14294v1'>2402.14294v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13400v1")'>The Dimension of Self-Directed Learning</div>
<div id='2402.13400v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T21:59:41Z</div><div>Authors: Pramith Devulapalli, Steve Hanneke</div><div style='padding-top: 10px; width: 80ex'>Understanding the self-directed learning complexity has been an important
problem that has captured the attention of the online learning theory community
since the early 1990s. Within this framework, the learner is allowed to
adaptively choose its next data point in making predictions unlike the setting
in adversarial online learning.
  In this paper, we study the self-directed learning complexity in both the
binary and multi-class settings, and we develop a dimension, namely $SDdim$,
that exactly characterizes the self-directed learning mistake-bound for any
concept class. The intuition behind $SDdim$ can be understood as a two-player
game called the "labelling game". Armed with this two-player game, we calculate
$SDdim$ on a whole host of examples with notable results on axis-aligned
rectangles, VC dimension $1$ classes, and linear separators. We demonstrate
several learnability gaps with a central focus on self-directed learning and
offline sequence learning models that include either the best or worst
ordering. Finally, we extend our analysis to the self-directed binary agnostic
setting where we derive upper and lower bounds.</div><div><a href='http://arxiv.org/abs/2402.13400v1'>2402.13400v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08922v1")'>The Mirrored Influence Hypothesis: Efficient Data Influence Estimation
  by Harnessing Forward Passes</div>
<div id='2402.08922v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T03:43:05Z</div><div>Authors: Myeongseob Ko, Feiyang Kang, Weiyan Shi, Ming Jin, Zhou Yu, Ruoxi Jia</div><div style='padding-top: 10px; width: 80ex'>Large-scale black-box models have become ubiquitous across numerous
applications. Understanding the influence of individual training data sources
on predictions made by these models is crucial for improving their
trustworthiness. Current influence estimation techniques involve computing
gradients for every training point or repeated training on different subsets.
These approaches face obvious computational challenges when scaled up to large
datasets and models.
  In this paper, we introduce and explore the Mirrored Influence Hypothesis,
highlighting a reciprocal nature of influence between training and test data.
Specifically, it suggests that evaluating the influence of training data on
test predictions can be reformulated as an equivalent, yet inverse problem:
assessing how the predictions for training samples would be altered if the
model were trained on specific test samples. Through both empirical and
theoretical validations, we demonstrate the wide applicability of our
hypothesis. Inspired by this, we introduce a new method for estimating the
influence of training data, which requires calculating gradients for specific
test samples, paired with a forward pass for each training point. This approach
can capitalize on the common asymmetry in scenarios where the number of test
samples under concurrent examination is much smaller than the scale of the
training dataset, thus gaining a significant improvement in efficiency compared
to existing approaches.
  We demonstrate the applicability of our method across a range of scenarios,
including data attribution in diffusion models, data leakage detection,
analysis of memorization, mislabeled data detection, and tracing behavior in
language models. Our code will be made available at
https://github.com/ruoxi-jia-group/Forward-INF.</div><div><a href='http://arxiv.org/abs/2402.08922v1'>2402.08922v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16979v1")'>Algorithmic Arbitrariness in Content Moderation</div>
<div id='2402.16979v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T19:27:00Z</div><div>Authors: Juan Felipe Gomez, Caio Vieira Machado, Lucas Monteiro Paes, Flavio P. Calmon</div><div style='padding-top: 10px; width: 80ex'>Machine learning (ML) is widely used to moderate online content. Despite its
scalability relative to human moderation, the use of ML introduces unique
challenges to content moderation. One such challenge is predictive
multiplicity: multiple competing models for content classification may perform
equally well on average, yet assign conflicting predictions to the same
content. This multiplicity can result from seemingly innocuous choices during
model development, such as random seed selection for parameter initialization.
We experimentally demonstrate how content moderation tools can arbitrarily
classify samples as toxic, leading to arbitrary restrictions on speech. We
discuss these findings in terms of human rights set out by the International
Covenant on Civil and Political Rights (ICCPR), namely freedom of expression,
non-discrimination, and procedural justice. We analyze (i) the extent of
predictive multiplicity among state-of-the-art LLMs used for detecting toxic
content; (ii) the disparate impact of this arbitrariness across social groups;
and (iii) how model multiplicity compares to unambiguous human classifications.
Our findings indicate that the up-scaled algorithmic moderation risks
legitimizing an algorithmic leviathan, where an algorithm disproportionately
manages human rights. To mitigate such risks, our study underscores the need to
identify and increase the transparency of arbitrariness in content moderation
applications. Since algorithmic content moderation is being fueled by pressing
social concerns, such as disinformation and hate speech, our discussion on
harms raises concerns relevant to policy debates. Our findings also contribute
to content moderation and intermediary liability laws being discussed and
passed in many countries, such as the Digital Services Act in the European
Union, the Online Safety Act in the United Kingdom, and the Fake News Bill in
Brazil.</div><div><a href='http://arxiv.org/abs/2402.16979v1'>2402.16979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.11145v1")'>Document Set Expansion with Positive-Unlabeled Learning: A Density
  Estimation-based Approach</div>
<div id='2401.11145v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T06:52:14Z</div><div>Authors: Haiyang Zhang, Qiuyi Chen, Yuanjie Zou, Yushan Pan, Jia Wang, Mark Stevenson</div><div style='padding-top: 10px; width: 80ex'>Document set expansion aims to identify relevant documents from a large
collection based on a small set of documents that are on a fine-grained topic.
Previous work shows that PU learning is a promising method for this task.
However, some serious issues remain unresolved, i.e. typical challenges that PU
methods suffer such as unknown class prior and imbalanced data, and the need
for transductive experimental settings. In this paper, we propose a novel PU
learning framework based on density estimation, called puDE, that can handle
the above issues. The advantage of puDE is that it neither constrained to the
SCAR assumption and nor require any class prior knowledge. We demonstrate the
effectiveness of the proposed method using a series of real-world datasets and
conclude that our method is a better alternative for the DSE task.</div><div><a href='http://arxiv.org/abs/2401.11145v1'>2401.11145v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.12062v2")'>Causal Equal Protection as Algorithmic Fairness</div>
<div id='2402.12062v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T11:30:00Z</div><div>Authors: Marcello Di Bello, Nicolò Cangiotti, Michele Loi</div><div style='padding-top: 10px; width: 80ex'>Over the last ten years the literature in computer science and philosophy has
formulated different criteria of algorithmic fairness. One of the most
discussed, classification parity, requires that the erroneous classifications
of a predictive algorithm occur with equal frequency for groups picked out by
protected characteristics. Despite its intuitive appeal, classification parity
has come under attack. Multiple scenarios can be imagined in which -
intuitively - a predictive algorithm does not treat any individual unfairly,
and yet classification parity is violated. To make progress, we turn to a
related principle, equal protection, originally developed in the context of
criminal justice. Key to equal protection is equalizing the risks of erroneous
classifications (in a sense to be specified) as opposed to equalizing the rates
of erroneous classifications. We show that equal protection avoids many of the
counterexamples to classification parity, but also fails to model our moral
intuitions in a number of common scenarios, for example, when the predictor is
causally downstream relative to the protected characteristic. To address these
difficulties, we defend a novel principle, causal equal protection, that models
the fair allocation of the risks of erroneous classification through the lenses
of causality.</div><div><a href='http://arxiv.org/abs/2402.12062v2'>2402.12062v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07857v1")'>Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias</div>
<div id='2403.07857v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:48:08Z</div><div>Authors: Sierra Wyllie, Ilia Shumailov, Nicolas Papernot</div><div style='padding-top: 10px; width: 80ex'>Model-induced distribution shifts (MIDS) occur as previous model outputs
pollute new model training sets over generations of models. This is known as
model collapse in the case of generative models, and performative prediction or
unfairness feedback loops for supervised models. When a model induces a
distribution shift, it also encodes its mistakes, biases, and unfairnesses into
the ground truth of its data ecosystem. We introduce a framework that allows us
to track multiple MIDS over many generations, finding that they can lead to
loss in performance, fairness, and minoritized group representation, even in
initially unbiased datasets. Despite these negative consequences, we identify
how models might be used for positive, intentional, interventions in their data
ecosystems, providing redress for historical discrimination through a framework
called algorithmic reparation (AR). We simulate AR interventions by curating
representative training batches for stochastic gradient descent to demonstrate
how AR can improve upon the unfairnesses of models and data ecosystems subject
to other MIDS. Our work takes an important step towards identifying,
mitigating, and taking accountability for the unfair feedback loops enabled by
the idea that ML systems are inherently neutral and objective.</div><div><a href='http://arxiv.org/abs/2403.07857v1'>2403.07857v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11778v1")'>Towards Theoretical Understandings of Self-Consuming Generative Models</div>
<div id='2402.11778v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:08:09Z</div><div>Authors: Shi Fu, Sen Zhang, Yingjie Wang, Xinmei Tian, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>This paper tackles the emerging challenge of training generative models
within a self-consuming loop, wherein successive generations of models are
recursively trained on mixtures of real and synthetic data from previous
generations. We construct a theoretical framework to rigorously evaluate how
this training regimen impacts the data distributions learned by future models.
Specifically, we derive bounds on the total variation (TV) distance between the
synthetic data distributions produced by future models and the original real
data distribution under various mixed training scenarios. Our analysis
demonstrates that this distance can be effectively controlled under the
condition that mixed training dataset sizes or proportions of real data are
large enough. Interestingly, we further unveil a phase transition induced by
expanding synthetic data amounts, proving theoretically that while the TV
distance exhibits an initial ascent, it declines beyond a threshold point.
Finally, we specialize our general results to diffusion models, delivering
nuanced insights such as the efficacy of optimal early stopping within the
self-consuming loop.</div><div><a href='http://arxiv.org/abs/2402.11778v1'>2402.11778v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01520v2")'>S$^{2}$-DMs:Skip-Step Diffusion Models</div>
<div id='2401.01520v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T03:08:32Z</div><div>Authors: Yixuan Wang, Shuangyin Li</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.</div><div><a href='http://arxiv.org/abs/2401.01520v2'>2401.01520v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08018v1")'>Nearest Neighbour Score Estimators for Diffusion Generative Models</div>
<div id='2402.08018v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:27:30Z</div><div>Authors: Matthew Niedoba, Dylan Green, Saeid Naderiparizi, Vasileios Lioutas, Jonathan Wilder Lavington, Xiaoxuan Liang, Yunpeng Liu, Ke Zhang, Setareh Dabiri, Adam Ścibior, Berend Zwartsenberg, Frank Wood</div><div style='padding-top: 10px; width: 80ex'>Score function estimation is the cornerstone of both training and sampling
from diffusion generative models. Despite this fact, the most commonly used
estimators are either biased neural network approximations or high variance
Monte Carlo estimators based on the conditional score. We introduce a novel
nearest neighbour score function estimator which utilizes multiple samples from
the training set to dramatically decrease estimator variance. We leverage our
low variance estimator in two compelling applications. Training consistency
models with our estimator, we report a significant increase in both convergence
speed and sample quality. In diffusion models, we show that our estimator can
replace a learned network for probability-flow ODE integration, opening
promising new avenues of future research.</div><div><a href='http://arxiv.org/abs/2402.08018v1'>2402.08018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01189v1")'>Training Unbiased Diffusion Models From Biased Dataset</div>
<div id='2403.01189v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T12:06:42Z</div><div>Authors: Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim, Wanmo Kang, Il-Chul Moon</div><div style='padding-top: 10px; width: 80ex'>With significant advancements in diffusion models, addressing the potential
risks of dataset bias becomes increasingly important. Since generated outputs
directly suffer from dataset bias, mitigating latent bias becomes a key factor
in improving sample quality and proportion. This paper proposes time-dependent
importance reweighting to mitigate the bias for the diffusion models. We
demonstrate that the time-dependent density ratio becomes more precise than
previous approaches, thereby minimizing error propagation in generative
learning. While directly applying it to score-matching is intractable, we
discover that using the time-dependent density ratio both for reweighting and
score correction can lead to a tractable form of the objective function to
regenerate the unbiased data density. Furthermore, we theoretically establish a
connection with traditional score-matching, and we demonstrate its convergence
to an unbiased distribution. The experimental evidence supports the usefulness
of the proposed method, which outperforms baselines including time-independent
importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various
bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.</div><div><a href='http://arxiv.org/abs/2403.01189v1'>2403.01189v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09031v2")'>Data Attribution for Diffusion Models: Timestep-induced Bias in
  Influence Estimation</div>
<div id='2401.09031v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T07:58:18Z</div><div>Authors: Tong Xie, Haoyu Li, Andrew Bai, Cho-Jui Hsieh</div><div style='padding-top: 10px; width: 80ex'>Data attribution methods trace model behavior back to its training dataset,
offering an effective approach to better understand ''black-box'' neural
networks. While prior research has established quantifiable links between model
output and training data in diverse settings, interpreting diffusion model
outputs in relation to training samples remains underexplored. In particular,
diffusion models operate over a sequence of timesteps instead of instantaneous
input-output relationships in previous contexts, posing a significant challenge
to extend existing frameworks to diffusion models directly. Notably, we present
Diffusion-TracIn that incorporates this temporal dynamics and observe that
samples' loss gradient norms are highly dependent on timestep. This trend leads
to a prominent bias in influence estimation, and is particularly noticeable for
samples trained on large-norm-inducing timesteps, causing them to be generally
influential. To mitigate this effect, we introduce Diffusion-ReTrac as a
re-normalized adaptation that enables the retrieval of training samples more
targeted to the test sample of interest, facilitating a localized measurement
of influence and considerably more intuitive visualization. We demonstrate the
efficacy of our approach through various evaluation metrics and auxiliary
tasks, reducing the amount of generally influential samples to $\frac{1}{3}$ of
its original quantity.</div><div><a href='http://arxiv.org/abs/2401.09031v2'>2401.09031v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18153v1")'>Diffusion-based Neural Network Weights Generation</div>
<div id='2402.18153v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T08:34:23Z</div><div>Authors: Bedionita Soro, Bruno Andreis, Hayeon Lee, Song Chong, Frank Hutter, Sung Ju Hwang</div><div style='padding-top: 10px; width: 80ex'>Transfer learning is a topic of significant interest in recent deep learning
research because it enables faster convergence and improved performance on new
tasks. While the performance of transfer learning depends on the similarity of
the source data to the target data, it is costly to train a model on a large
number of datasets. Therefore, pretrained models are generally blindly selected
with the hope that they will achieve good performance on the given task. To
tackle such suboptimality of the pretrained models, we propose an efficient and
adaptive transfer learning scheme through dataset-conditioned pretrained
weights sampling. Specifically, we use a latent diffusion model with a
variational autoencoder that can reconstruct the neural network weights, to
learn the distribution of a set of pretrained weights conditioned on each
dataset for transfer learning on unseen datasets. By learning the distribution
of a neural network on a variety pretrained models, our approach enables
adaptive sampling weights for unseen datasets achieving faster convergence and
reaching competitive performance.</div><div><a href='http://arxiv.org/abs/2402.18153v1'>2402.18153v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07282v1")'>Enhancing Transfer Learning with Flexible Nonparametric Posterior
  Sampling</div>
<div id='2403.07282v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T03:26:58Z</div><div>Authors: Hyungi Lee, Giung Nam, Edwin Fong, Juho Lee</div><div style='padding-top: 10px; width: 80ex'>Transfer learning has recently shown significant performance across various
tasks involving deep neural networks. In these transfer learning scenarios, the
prior distribution for downstream data becomes crucial in Bayesian model
averaging (BMA). While previous works proposed the prior over the neural
network parameters centered around the pre-trained solution, such strategies
have limitations when dealing with distribution shifts between upstream and
downstream data. This paper introduces nonparametric transfer learning (NPTL),
a flexible posterior sampling method to address the distribution shift issue
within the context of nonparametric learning. The nonparametric learning (NPL)
method is a recent approach that employs a nonparametric prior for posterior
sampling, efficiently accounting for model misspecification scenarios, which is
suitable for transfer learning scenarios that may involve the distribution
shift between upstream and downstream tasks. Through extensive empirical
validations, we demonstrate that our approach surpasses other baselines in BMA
performance.</div><div><a href='http://arxiv.org/abs/2403.07282v1'>2403.07282v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07366v1")'>Bayesian Federated Learning Via Expectation Maximization and Turbo Deep
  Approximate Message Passing</div>
<div id='2402.07366v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T01:47:06Z</div><div>Authors: Wei Xu, An Liu, Yiting Zhang, Vincent Lau</div><div style='padding-top: 10px; width: 80ex'>Federated learning (FL) is a machine learning paradigm where the clients
possess decentralized training data and the central server handles aggregation
and scheduling. Typically, FL algorithms involve clients training their local
models using stochastic gradient descent (SGD), which carries drawbacks such as
slow convergence and being prone to getting stuck in suboptimal solutions. In
this work, we propose a message passing based Bayesian federated learning (BFL)
framework to avoid these drawbacks.Specifically, we formulate the problem of
deep neural network (DNN) learning and compression and as a sparse Bayesian
inference problem, in which group sparse prior is employed to achieve
structured model compression. Then, we propose an efficient BFL algorithm
called EMTDAMP, where expectation maximization (EM) and turbo deep approximate
message passing (TDAMP) are combined to achieve distributed learning and
compression. The central server aggregates local posterior distributions to
update global posterior distributions and update hyperparameters based on EM to
accelerate convergence. The clients perform TDAMP to achieve efficient
approximate message passing over DNN with joint prior distribution. We detail
the application of EMTDAMP to Boston housing price prediction and handwriting
recognition, and present extensive numerical results to demonstrate the
advantages of EMTDAMP.</div><div><a href='http://arxiv.org/abs/2402.07366v1'>2402.07366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17517v1")'>Label-Noise Robust Diffusion Models</div>
<div id='2402.17517v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T14:00:34Z</div><div>Authors: Byeonghu Na, Yeongmin Kim, HeeSun Bae, Jung Hyun Lee, Se Jung Kwon, Wanmo Kang, Il-Chul Moon</div><div style='padding-top: 10px; width: 80ex'>Conditional diffusion models have shown remarkable performance in various
generative tasks, but training them requires large-scale datasets that often
contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to
condition mismatch and quality degradation of generated data. This paper
proposes Transition-aware weighted Denoising Score Matching (TDSM) for training
conditional diffusion models with noisy labels, which is the first study in the
line of diffusion models. The TDSM objective contains a weighted sum of score
networks, incorporating instance-wise and time-dependent label transition
probabilities. We introduce a transition-aware weight estimator, which
leverages a time-dependent noisy-label classifier distinctively customized to
the diffusion process. Through experiments across various datasets and noisy
label settings, TDSM improves the quality of generated samples aligned with
given conditions. Furthermore, our method improves generation performance even
on prevalent benchmark datasets, which implies the potential noisy labels and
their risk of generative model learning. Finally, we show the improved
performance of TDSM on top of conventional noisy label corrections, which
empirically proving its contribution as a part of label-noise robust generative
models. Our code is available at: https://github.com/byeonghu-na/tdsm.</div><div><a href='http://arxiv.org/abs/2402.17517v1'>2402.17517v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07802v1")'>Towards a mathematical theory for consistency training in diffusion
  models</div>
<div id='2402.07802v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T17:07:02Z</div><div>Authors: Gen Li, Zhihan Huang, Yuting Wei</div><div style='padding-top: 10px; width: 80ex'>Consistency models, which were proposed to mitigate the high computational
overhead during the sampling phase of diffusion models, facilitate single-step
sampling while attaining state-of-the-art empirical performance. When
integrated into the training phase, consistency models attempt to train a
sequence of consistency functions capable of mapping any point at any time step
of the diffusion process to its starting point. Despite the empirical success,
a comprehensive theoretical understanding of consistency training remains
elusive. This paper takes a first step towards establishing theoretical
underpinnings for consistency models. We demonstrate that, in order to generate
samples within $\varepsilon$ proximity to the target in distribution (measured
by some Wasserstein metric), it suffices for the number of steps in consistency
learning to exceed the order of $d^{5/2}/\varepsilon$, with $d$ the data
dimension. Our theory offers rigorous insights into the validity and efficacy
of consistency models, illuminating their utility in downstream inference
tasks.</div><div><a href='http://arxiv.org/abs/2402.07802v1'>2402.07802v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01103v2")'>Compositional Generative Modeling: A Single Model is Not All You Need</div>
<div id='2402.01103v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:40:51Z</div><div>Authors: Yilun Du, Leslie Kaelbling</div><div style='padding-top: 10px; width: 80ex'>Large monolithic generative models trained on massive amounts of data have
become an increasingly dominant approach in AI research. In this paper, we
argue that we should instead construct large generative systems by composing
smaller generative models together. We show how such a compositional generative
approach enables us to learn distributions in a more data-efficient manner,
enabling generalization to parts of the data distribution unseen at training
time. We further show how this enables us to program and construct new
generative models for tasks completely unseen at training. Finally, we show
that in many cases, we can discover separate compositional components from
data.</div><div><a href='http://arxiv.org/abs/2402.01103v2'>2402.01103v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03305v1")'>Do Diffusion Models Learn Semantically Meaningful and Efficient
  Representations?</div>
<div id='2402.03305v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:58:38Z</div><div>Authors: Qiyao Liang, Ziming Liu, Ila Fiete</div><div style='padding-top: 10px; width: 80ex'>Diffusion models are capable of impressive feats of image generation with
uncommon juxtapositions such as astronauts riding horses on the moon with
properly placed shadows. These outputs indicate the ability to perform
compositional generalization, but how do the models do so? We perform
controlled experiments on conditional DDPMs learning to generate 2D spherical
Gaussian bumps centered at specified $x$- and $y$-positions. Our results show
that the emergence of semantically meaningful latent representations is key to
achieving high performance. En route to successful performance over learning,
the model traverses three distinct phases of latent representations: (phase A)
no latent structure, (phase B) a 2D manifold of disordered states, and (phase
C) a 2D ordered manifold. Corresponding to each of these phases, we identify
qualitatively different generation behaviors: 1) multiple bumps are generated,
2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is
generated at the correct $x$ and y location. Furthermore, we show that even
under imbalanced datasets where features ($x$- versus $y$-positions) are
represented with skewed frequencies, the learning process for $x$ and $y$ is
coupled rather than factorized, demonstrating that simple vanilla-flavored
diffusion models cannot learn efficient representations in which localization
in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest
the need for future work to find inductive biases that will push generative
models to discover and exploit factorizable independent structures in their
inputs, which will be required to vault these models into more data-efficient
regimes.</div><div><a href='http://arxiv.org/abs/2402.03305v1'>2402.03305v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16991v2")'>A Phase Transition in Diffusion Models Reveals the Hierarchical Nature
  of Data</div>
<div id='2402.16991v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T19:52:33Z</div><div>Authors: Antonio Sclocchi, Alessandro Favero, Matthieu Wyart</div><div style='padding-top: 10px; width: 80ex'>Understanding the structure of real data is paramount in advancing modern
deep-learning methodologies. Natural data such as images are believed to be
composed of features organised in a hierarchical and combinatorial manner,
which neural networks capture during learning. Recent advancements show that
diffusion models can generate high-quality images, hinting at their ability to
capture this underlying structure. We study this phenomenon in a hierarchical
generative model of data. We find that the backward diffusion process acting
after a time $t$ is governed by a phase transition at some threshold time,
where the probability of reconstructing high-level features, like the class of
an image, suddenly drops. Instead, the reconstruction of low-level features,
such as specific details of an image, evolves smoothly across the whole
diffusion process. This result implies that at times beyond the transition, the
class has changed but the generated sample may still be composed of low-level
elements of the initial image. We validate these theoretical insights through
numerical experiments on class-unconditional ImageNet diffusion models. Our
analysis characterises the relationship between time and scale in diffusion
models and puts forward generative models as powerful tools to model
combinatorial data properties.</div><div><a href='http://arxiv.org/abs/2402.16991v2'>2402.16991v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19369v1")'>Structure Preserving Diffusion Models</div>
<div id='2402.19369v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T17:16:20Z</div><div>Authors: Haoye Lu, Spencer Szabados, Yaoliang Yu</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have become the leading distribution-learning method in
recent years. Herein, we introduce structure-preserving diffusion processes, a
family of diffusion processes for learning distributions that possess
additional structure, such as group symmetries, by developing theoretical
conditions under which the diffusion transition steps preserve said symmetry.
While also enabling equivariant data sampling trajectories, we exemplify these
results by developing a collection of different symmetry equivariant diffusion
models capable of learning distributions that are inherently symmetric.
Empirical studies, over both synthetic and real-world datasets, are used to
validate the developed models adhere to the proposed theory and are capable of
achieving improved performance over existing methods in terms of sample
equality. We also show how the proposed models can be used to achieve
theoretically guaranteed equivariant image noise reduction without prior
knowledge of the image orientation.</div><div><a href='http://arxiv.org/abs/2402.19369v1'>2402.19369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02081v1")'>Risk-Sensitive Diffusion: Learning the Underlying Distribution from
  Noisy Samples</div>
<div id='2402.02081v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T08:41:51Z</div><div>Authors: Yangming Li, Max Ruiz Luyten, Mihaela van der Schaar</div><div style='padding-top: 10px; width: 80ex'>While achieving remarkable performances, we show that diffusion models are
fragile to the presence of noisy samples, limiting their potential in the vast
amount of settings where, unlike image synthesis, we are not blessed with clean
data. Motivated by our finding that such fragility originates from the
distribution gaps between noisy and clean samples along the diffusion process,
we introduce risk-sensitive SDE, a stochastic differential equation that is
parameterized by the risk (i.e., data "dirtiness") to adjust the distributions
of noisy samples, reducing misguidance while benefiting from their contained
information. The optimal expression for risk-sensitive SDE depends on the
specific noise distribution, and we derive its parameterizations that minimize
the misguidance of noisy samples for both Gaussian and general non-Gaussian
perturbations. We conduct extensive experiments on both synthetic and
real-world datasets (e.g., medical time series), showing that our model
effectively recovers the clean data distribution from noisy samples,
significantly outperforming conditional generation baselines.</div><div><a href='http://arxiv.org/abs/2402.02081v1'>2402.02081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10282v2")'>BioDiffusion: A Versatile Diffusion Model for Biomedical Signal
  Synthesis</div>
<div id='2401.10282v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T23:52:44Z</div><div>Authors: Xiaomin Li, Mykhailo Sakevych, Gentry Atkinson, Vangelis Metsis</div><div style='padding-top: 10px; width: 80ex'>Machine learning tasks involving biomedical signals frequently grapple with
issues such as limited data availability, imbalanced datasets, labeling
complexities, and the interference of measurement noise. These challenges often
hinder the optimal training of machine learning algorithms. Addressing these
concerns, we introduce BioDiffusion, a diffusion-based probabilistic model
optimized for the synthesis of multivariate biomedical signals. BioDiffusion
demonstrates excellence in producing high-fidelity, non-stationary,
multivariate signals for a range of tasks including unconditional,
label-conditional, and signal-conditional generation. Leveraging these
synthesized signals offers a notable solution to the aforementioned challenges.
Our research encompasses both qualitative and quantitative assessments of the
synthesized data quality, underscoring its capacity to bolster accuracy in
machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed
with current leading time-series generative models, empirical evidence suggests
that BioDiffusion outperforms them in biomedical signal generation quality.</div><div><a href='http://arxiv.org/abs/2401.10282v2'>2401.10282v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19455v1")'>Listening to the Noise: Blind Denoising with Gibbs Diffusion</div>
<div id='2402.19455v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:50:11Z</div><div>Authors: David Heurtel-Depeiges, Charles C. Margossian, Ruben Ohana, Bruno Régaldo-Saint Blancard</div><div style='padding-top: 10px; width: 80ex'>In recent years, denoising problems have become intertwined with the
development of deep generative models. In particular, diffusion models are
trained like denoisers, and the distribution they model coincide with denoising
priors in the Bayesian picture. However, denoising through diffusion-based
posterior sampling requires the noise level and covariance to be known,
preventing blind denoising. We overcome this limitation by introducing Gibbs
Diffusion (GDiff), a general methodology addressing posterior sampling of both
the signal and the noise parameters. Assuming arbitrary parametric Gaussian
noise, we develop a Gibbs algorithm that alternates sampling steps from a
conditional diffusion model trained to map the signal prior to the family of
noise distributions, and a Monte Carlo sampler to infer the noise parameters.
Our theoretical analysis highlights potential pitfalls, guides diagnostic
usage, and quantifies errors in the Gibbs stationary distribution caused by the
diffusion model. We showcase our method for 1) blind denoising of natural
images involving colored noises with unknown amplitude and spectral index, and
2) a cosmology problem, namely the analysis of cosmic microwave background
data, where Bayesian inference of "noise" parameters means constraining models
of the evolution of the Universe.</div><div><a href='http://arxiv.org/abs/2402.19455v1'>2402.19455v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07929v1")'>Sketching the Heat Kernel: Using Gaussian Processes to Embed Data</div>
<div id='2403.07929v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T22:56:19Z</div><div>Authors: Anna C. Gilbert, Kevin O'Neill</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a novel, non-deterministic method for embedding data in
low-dimensional Euclidean space based on computing realizations of a Gaussian
process depending on the geometry of the data. This type of embedding first
appeared in (Adler et al, 2018) as a theoretical model for a generic manifold
in high dimensions.
  In particular, we take the covariance function of the Gaussian process to be
the heat kernel, and computing the embedding amounts to sketching a matrix
representing the heat kernel. The Karhunen-Lo\`eve expansion reveals that the
straight-line distances in the embedding approximate the diffusion distance in
a probabilistic sense, avoiding the need for sharp cutoffs and maintaining some
of the smaller-scale structure.
  Our method demonstrates further advantage in its robustness to outliers. We
justify the approach with both theory and experiments.</div><div><a href='http://arxiv.org/abs/2403.07929v1'>2403.07929v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08740v1")'>SiT: Exploring Flow and Diffusion-based Generative Models with Scalable
  Interpolant Transformers</div>
<div id='2401.08740v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:55:25Z</div><div>Authors: Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, Saining Xie</div><div style='padding-top: 10px; width: 80ex'>We present Scalable Interpolant Transformers (SiT), a family of generative
models built on the backbone of Diffusion Transformers (DiT). The interpolant
framework, which allows for connecting two distributions in a more flexible way
than standard diffusion models, makes possible a modular study of various
design choices impacting generative models built on dynamical transport: using
discrete vs. continuous time learning, deciding the objective for the model to
learn, choosing the interpolant connecting the distributions, and deploying a
deterministic or stochastic sampler. By carefully introducing the above
ingredients, SiT surpasses DiT uniformly across model sizes on the conditional
ImageNet 256x256 benchmark using the exact same backbone, number of parameters,
and GFLOPs. By exploring various diffusion coefficients, which can be tuned
separately from learning, SiT achieves an FID-50K score of 2.06.</div><div><a href='http://arxiv.org/abs/2401.08740v1'>2401.08740v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08741v1")'>Fixed Point Diffusion Models</div>
<div id='2401.08741v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:55:54Z</div><div>Authors: Xingjian Bai, Luke Melas-Kyriazi</div><div style='padding-top: 10px; width: 80ex'>We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to
image generation that integrates the concept of fixed point solving into the
framework of diffusion-based generative modeling. Our approach embeds an
implicit fixed point solving layer into the denoising network of a diffusion
model, transforming the diffusion process into a sequence of closely-related
fixed point problems. Combined with a new stochastic training method, this
approach significantly reduces model size, reduces memory usage, and
accelerates training. Moreover, it enables the development of two new
techniques to improve sampling efficiency: reallocating computation across
timesteps and reusing fixed point solutions between timesteps. We conduct
extensive experiments with state-of-the-art models on ImageNet, FFHQ,
CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in
performance and efficiency. Compared to the state-of-the-art DiT model, FPDM
contains 87% fewer parameters, consumes 60% less memory during training, and
improves image generation quality in situations where sampling computation or
time is limited. Our code and pretrained models are available at
https://lukemelas.github.io/fixed-point-diffusion-models.</div><div><a href='http://arxiv.org/abs/2401.08741v1'>2401.08741v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15170v1")'>The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling</div>
<div id='2402.15170v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:05:23Z</div><div>Authors: Jiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu, Zhenguo Li, Zhi-Ming Ma, Kenji Kawaguchi</div><div style='padding-top: 10px; width: 80ex'>With the incorporation of the UNet architecture, diffusion probabilistic
models have become a dominant force in image generation tasks. One key design
in UNet is the skip connections between the encoder and decoder blocks.
Although skip connections have been shown to improve training stability and
model performance, we reveal that such shortcuts can be a limiting factor for
the complexity of the transformation. As the sampling steps decrease, the
generation process and the role of the UNet get closer to the push-forward
transformations from Gaussian distribution to the target, posing a challenge
for the network's complexity. To address this challenge, we propose
Skip-Tuning, a simple yet surprisingly effective training-free tuning method on
the skip connections. Our method can achieve 100% FID improvement for
pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of
ODE samplers regardless of sampling steps. Surprisingly, the improvement
persists when we increase the number of sampling steps and can even surpass the
best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive
exploratory experiments are conducted to shed light on the surprising
effectiveness. We observe that while Skip-Tuning increases the score-matching
losses in the pixel space, the losses in the feature space are reduced,
particularly at intermediate noise levels, which coincide with the most
effective range accounting for image quality improvement.</div><div><a href='http://arxiv.org/abs/2402.15170v1'>2402.15170v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14167v1")'>T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with
  Trajectory Stitching</div>
<div id='2402.14167v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T23:08:54Z</div><div>Authors: Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, Anima Anandkumar</div><div style='padding-top: 10px; width: 80ex'>Sampling from diffusion probabilistic models (DPMs) is often expensive for
high-quality image generation and typically requires many steps with a large
model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a
simple yet efficient technique to improve the sampling efficiency with little
or no generation degradation. Instead of solely using a large DPM for the
entire sampling trajectory, T-Stitch first leverages a smaller DPM in the
initial steps as a cheap drop-in replacement of the larger DPM and switches to
the larger DPM at a later stage. Our key insight is that different diffusion
models learn similar encodings under the same training data distribution and
smaller models are capable of generating good global structures in the early
steps. Extensive experiments demonstrate that T-Stitch is training-free,
generally applicable for different architectures, and complements most existing
fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL,
for example, 40% of the early timesteps can be safely replaced with a 10x
faster DiT-S without performance drop on class-conditional ImageNet generation.
We further show that our method can also be used as a drop-in technique to not
only accelerate the popular pretrained stable diffusion (SD) models but also
improve the prompt alignment of stylized SD models from the public model zoo.
Code is released at https://github.com/NVlabs/T-Stitch</div><div><a href='http://arxiv.org/abs/2402.14167v1'>2402.14167v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08815v1")'>Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive</div>
<div id='2401.08815v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T20:31:46Z</div><div>Authors: Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva</div><div style='padding-top: 10px; width: 80ex'>Despite the recent advances in large-scale diffusion models, little progress
has been made on the layout-to-image (L2I) synthesis task. Current L2I models
either suffer from poor editability via text or weak alignment between the
generated image and the input layout. This limits their usability in practice.
To mitigate this, we propose to integrate adversarial supervision into the
conventional training pipeline of L2I diffusion models (ALDM). Specifically, we
employ a segmentation-based discriminator which provides explicit feedback to
the diffusion generator on the pixel-level alignment between the denoised image
and the input layout. To encourage consistent adherence to the input layout
over the sampling steps, we further introduce the multistep unrolling strategy.
Instead of looking at a single timestep, we unroll a few steps recursively to
imitate the inference process, and ask the discriminator to assess the
alignment of denoised images with the layout over a certain time window. Our
experiments show that ALDM enables layout faithfulness of the generated images,
while allowing broad editability via text prompts. Moreover, we showcase its
usefulness for practical applications: by synthesizing target distribution
samples via text control, we improve domain generalization of semantic
segmentation models by a large margin (~12 mIoU points).</div><div><a href='http://arxiv.org/abs/2401.08815v1'>2401.08815v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13573v2")'>ToDo: Token Downsampling for Efficient Generation of High-Resolution
  Images</div>
<div id='2402.13573v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T07:10:28Z</div><div>Authors: Ethan Smith, Nayan Saxena, Aninda Saha</div><div style='padding-top: 10px; width: 80ex'>Attention mechanism has been crucial for image diffusion models, however,
their quadratic computational complexity limits the sizes of images we can
process within reasonable time and memory constraints. This paper investigates
the importance of dense attention in generative image models, which often
contain redundant features, making them suitable for sparser attention
mechanisms. We propose a novel training-free method ToDo that relies on token
downsampling of key and value tokens to accelerate Stable Diffusion inference
by up to 2x for common sizes and up to 4.5x or more for high resolutions like
2048x2048. We demonstrate that our approach outperforms previous methods in
balancing efficient throughput and fidelity.</div><div><a href='http://arxiv.org/abs/2402.13573v2'>2402.13573v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12404v1")'>Understanding Training-free Diffusion Guidance: Mechanisms and
  Limitations</div>
<div id='2403.12404v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T03:27:01Z</div><div>Authors: Yifei Shen, Xinyang Jiang, Yezhen Wang, Yifan Yang, Dongqi Han, Dongsheng Li</div><div style='padding-top: 10px; width: 80ex'>Adding additional control to pretrained diffusion models has become an
increasingly popular research area, with extensive applications in computer
vision, reinforcement learning, and AI for science. Recently, several studies
have proposed training-free diffusion guidance by using off-the-shelf networks
pretrained on clean images. This approach enables zero-shot conditional
generation for universal control formats, which appears to offer a free lunch
in diffusion guidance. In this paper, we aim to develop a deeper understanding
of the operational mechanisms and fundamental limitations of training-free
guidance. We offer a theoretical analysis that supports training-free guidance
from the perspective of optimization, distinguishing it from classifier-based
(or classifier-free) guidance. To elucidate their drawbacks, we theoretically
demonstrate that training-free methods are more susceptible to adversarial
gradients and exhibit slower convergence rates compared to classifier guidance.
We then introduce a collection of techniques designed to overcome the
limitations, accompanied by theoretical rationale and empirical evidence. Our
experiments in image and motion generation confirm the efficacy of these
techniques.</div><div><a href='http://arxiv.org/abs/2403.12404v1'>2403.12404v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04585v1")'>Enhanced Distribution Alignment for Post-Training Quantization of
  Diffusion Models</div>
<div id='2401.04585v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T14:42:49Z</div><div>Authors: Xuewen Liu, Zhikai Li, Junrui Xiao, Qingyi Gu</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have achieved great success in image generation tasks
through iterative noise estimation. However, the heavy denoising process and
complex neural networks hinder their low-latency applications in real-world
scenarios. Quantization can effectively reduce model complexity, and
post-training quantization (PTQ), which does not require fine-tuning, is highly
promising in accelerating the denoising process. Unfortunately, we find that
due to the highly dynamic distribution of activations in different denoising
steps, existing PTQ methods for diffusion models suffer from distribution
mismatch issues at both calibration sample level and reconstruction output
level, which makes the performance far from satisfactory, especially in low-bit
cases. In this paper, we propose Enhanced Distribution Alignment for
Post-Training Quantization of Diffusion Models (EDA-DM) to address the above
issues. Specifically, at the calibration sample level, we select calibration
samples based on the density and diversity in the latent space, thus
facilitating the alignment of their distribution with the overall samples; and
at the reconstruction output level, we propose Fine-grained Block
Reconstruction, which can align the outputs of the quantized model and the
full-precision model at different network granularity. Extensive experiments
demonstrate that EDA-DM outperforms the existing post-training quantization
frameworks in both unconditional and conditional generation scenarios. At
low-bit precision, the quantized models with our method even outperform the
full-precision models on most datasets.</div><div><a href='http://arxiv.org/abs/2401.04585v1'>2401.04585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17376v1")'>Accelerating Diffusion Sampling with Optimized Time Steps</div>
<div id='2402.17376v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T10:13:30Z</div><div>Authors: Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, Zhenguo Li</div><div style='padding-top: 10px; width: 80ex'>Diffusion probabilistic models (DPMs) have shown remarkable performance in
high-resolution image synthesis, but their sampling efficiency is still to be
desired due to the typically large number of sampling steps. Recent
advancements in high-order numerical ODE solvers for DPMs have enabled the
generation of high-quality images with much fewer sampling steps. While this is
a significant development, most sampling methods still employ uniform time
steps, which is not optimal when using a small number of steps. To address this
issue, we propose a general framework for designing an optimization problem
that seeks more appropriate time steps for a specific numerical ODE solver for
DPMs. This optimization problem aims to minimize the distance between the
ground-truth solution to the ODE and an approximate solution corresponding to
the numerical solver. It can be efficiently solved using the constrained trust
region method, taking less than $15$ seconds. Our extensive experiments on both
unconditional and conditional sampling using pixel- and latent-space DPMs
demonstrate that, when combined with the state-of-the-art sampling method
UniPC, our optimized time steps significantly improve image generation
performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet,
compared to using uniform time steps.</div><div><a href='http://arxiv.org/abs/2402.17376v1'>2402.17376v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11262v1")'>Understanding Diffusion Models by Feynman's Path Integral</div>
<div id='2403.11262v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T16:24:29Z</div><div>Authors: Yuji Hirono, Akinori Tanaka, Kenji Fukushima</div><div style='padding-top: 10px; width: 80ex'>Score-based diffusion models have proven effective in image generation and
have gained widespread usage; however, the underlying factors contributing to
the performance disparity between stochastic and deterministic (i.e., the
probability flow ODEs) sampling schemes remain unclear. We introduce a novel
formulation of diffusion models using Feynman's path integral, which is a
formulation originally developed for quantum physics. We find this formulation
providing comprehensive descriptions of score-based generative models, and
demonstrate the derivation of backward stochastic differential equations and
loss functions.The formulation accommodates an interpolating parameter
connecting stochastic and deterministic sampling schemes, and we identify this
parameter as a counterpart of Planck's constant in quantum physics. This
analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a
well-established technique in quantum physics, for evaluating the negative
log-likelihood to assess the performance disparity between stochastic and
deterministic sampling schemes.</div><div><a href='http://arxiv.org/abs/2403.11262v1'>2403.11262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13369v1")'>The Uncanny Valley: A Comprehensive Analysis of Diffusion Models</div>
<div id='2402.13369v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T20:49:22Z</div><div>Authors: Karam Ghanem, Danilo Bzdok</div><div style='padding-top: 10px; width: 80ex'>Through Diffusion Models (DMs), we have made significant advances in
generating high-quality images. Our exploration of these models delves deeply
into their core operational principles by systematically investigating key
aspects across various DM architectures: i) noise schedules, ii) samplers, and
iii) guidance. Our comprehensive examination of these models sheds light on
their hidden fundamental mechanisms, revealing the concealed foundational
elements that are essential for their effectiveness. Our analyses emphasize the
hidden key factors that determine model performance, offering insights that
contribute to the advancement of DMs. Past findings show that the configuration
of noise schedules, samplers, and guidance is vital to the quality of generated
images; however, models reach a stable level of quality across different
configurations at a remarkably similar point, revealing that the decisive
factors for optimal performance predominantly reside in the diffusion process
dynamics and the structural design of the model's network, rather than the
specifics of configuration details. Our comparative analysis reveals that
Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics
consistently outperform the Noise Conditioned Score Network (NCSN)-based ones,
not only when evaluated in their original forms but also when continuous
through Stochastic Differential Equation (SDE)-based implementations.</div><div><a href='http://arxiv.org/abs/2402.13369v1'>2402.13369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08563v2")'>Denoising Diffusion Restoration Tackles Forward and Inverse Problems for
  the Laplace Operator</div>
<div id='2402.08563v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:04:41Z</div><div>Authors: Amartya Mukherjee, Melissa M. Stadt, Lena Podina, Mohammad Kohandel, Jun Liu</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have emerged as a promising class of generative models that
map noisy inputs to realistic images. More recently, they have been employed to
generate solutions to partial differential equations (PDEs). However, they
still struggle with inverse problems in the Laplacian operator, for instance,
the Poisson equation, because the eigenvalues that are large in magnitude
amplify the measurement noise. This paper presents a novel approach for the
inverse and forward solution of PDEs through the use of denoising diffusion
restoration models (DDRM). DDRMs were used in linear inverse problems to
restore original clean signals by exploiting the singular value decomposition
(SVD) of the linear operator. Equivalently, we present an approach to restore
the solution and the parameters in the Poisson equation by exploiting the
eigenvalues and the eigenfunctions of the Laplacian operator. Our results show
that using denoising diffusion restoration significantly improves the
estimation of the solution and parameters. Our research, as a result, pioneers
the integration of diffusion models with the principles of underlying physics
to solve PDEs.</div><div><a href='http://arxiv.org/abs/2402.08563v2'>2402.08563v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11407v1")'>Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors</div>
<div id='2403.11407v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T01:47:24Z</div><div>Authors: Yazid Janati, Alain Durmus, Eric Moulines, Jimmy Olsson</div><div style='padding-top: 10px; width: 80ex'>Interest in the use of Denoising Diffusion Models (DDM) as priors for solving
inverse Bayesian problems has recently increased significantly. However,
sampling from the resulting posterior distribution poses a challenge. To solve
this problem, previous works have proposed approximations to bias the drift
term of the diffusion. In this work, we take a different approach and utilize
the specific structure of the DDM prior to define a set of intermediate and
simpler posterior sampling problems, resulting in a lower approximation error
compared to previous methods. We empirically demonstrate the reconstruction
capability of our method for general linear inverse problems using synthetic
examples and various image restoration tasks.</div><div><a href='http://arxiv.org/abs/2403.11407v1'>2403.11407v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06054v2")'>Decoupled Data Consistency with Diffusion Purification for Image
  Restoration</div>
<div id='2403.06054v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T00:47:05Z</div><div>Authors: Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishanka, Qing Qu</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.</div><div><a href='http://arxiv.org/abs/2403.06054v2'>2403.06054v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16907v1")'>Diffusion Posterior Proximal Sampling for Image Restoration</div>
<div id='2402.16907v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T04:24:28Z</div><div>Authors: Hongjie Wu, Linchao He, Mingqin Zhang, Dongdong Chen, Kunming Luo, Mengting Luo, Ji-Zhe Zhou, Hu Chen, Jiancheng Lv</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have demonstrated remarkable efficacy in generating
high-quality samples. Existing diffusion-based image restoration algorithms
exploit pre-trained diffusion models to leverage data priors, yet they still
preserve elements inherited from the unconditional generation paradigm. These
strategies initiate the denoising process with pure white noise and incorporate
random noise at each generative step, leading to over-smoothed results. In this
paper, we introduce a refined paradigm for diffusion-based image restoration.
Specifically, we opt for a sample consistent with the measurement identity at
each generative step, exploiting the sampling selection as an avenue for output
stability and enhancement. Besides, we start the restoration process with an
initialization combined with the measurement signal, providing supplementary
information to better align the generative process. Extensive experimental
results and analyses validate the effectiveness of our proposed approach across
diverse image restoration tasks.</div><div><a href='http://arxiv.org/abs/2402.16907v1'>2402.16907v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02149v1")'>Improving Diffusion Models for Inverse Problems Using Optimal Posterior
  Covariance</div>
<div id='2402.02149v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T13:35:39Z</div><div>Authors: Xinyu Peng, Ziyang Zheng, Wenrui Dai, Nuoqian Xiao, Chenglin Li, Junni Zou, Hongkai Xiong</div><div style='padding-top: 10px; width: 80ex'>Recent diffusion models provide a promising zero-shot solution to noisy
linear inverse problems without retraining for specific inverse problems. In
this paper, we propose the first unified interpretation for existing zero-shot
methods from the perspective of approximating the conditional posterior mean
for the reverse diffusion process of conditional sampling. We reveal that
recent methods are equivalent to making isotropic Gaussian approximations to
intractable posterior distributions over clean images given diffused noisy
images, with the only difference in the handcrafted design of isotropic
posterior covariances. Inspired by this finding, we propose a general
plug-and-play posterior covariance optimization based on maximum likelihood
estimation to improve recent methods. To achieve optimal posterior covariance
without retraining, we provide general solutions based on two approaches
specifically designed to leverage pre-trained models with and without reverse
covariances. Experimental results demonstrate that the proposed methods
significantly enhance the overall performance or robustness to hyperparameters
of recent methods. Code is available at
https://github.com/xypeng9903/k-diffusion-inverse-problems</div><div><a href='http://arxiv.org/abs/2402.02149v1'>2402.02149v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15889v1")'>Sliced Wasserstein with Random-Path Projecting Directions</div>
<div id='2401.15889v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T04:59:30Z</div><div>Authors: Khai Nguyen, Shujian Zhang, Tam Le, Nhat Ho</div><div style='padding-top: 10px; width: 80ex'>Slicing distribution selection has been used as an effective technique to
improve the performance of parameter estimators based on minimizing sliced
Wasserstein distance in applications. Previous works either utilize expensive
optimization to select the slicing distribution or use slicing distributions
that require expensive sampling methods. In this work, we propose an
optimization-free slicing distribution that provides a fast sampling for the
Monte Carlo estimation of expectation. In particular, we introduce the
random-path projecting direction (RPD) which is constructed by leveraging the
normalized difference between two random vectors following the two input
measures. From the RPD, we derive the random-path slicing distribution (RPSD)
and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced
Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced
Wasserstein (IWRPSW). We then discuss the topological, statistical, and
computational properties of RPSW and IWRPSW. Finally, we showcase the favorable
performance of RPSW and IWRPSW in gradient flow and the training of denoising
diffusion generative models on images.</div><div><a href='http://arxiv.org/abs/2401.15889v1'>2401.15889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17629v1")'>Spatial-and-Frequency-aware Restoration method for Images based on
  Diffusion Models</div>
<div id='2401.17629v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T07:11:01Z</div><div>Authors: Kyungsung Lee, Donggyu Lee, Myungjoo Kang</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have recently emerged as a promising framework for Image
Restoration (IR), owing to their ability to produce high-quality
reconstructions and their compatibility with established methods. Existing
methods for solving noisy inverse problems in IR, considers the pixel-wise
data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware
diffusion model for IR with Gaussian noise. Our model encourages images to
preserve data-fidelity in both the spatial and frequency domains, resulting in
enhanced reconstruction quality. We comprehensively evaluate the performance of
our model on a variety of noisy inverse problems, including inpainting,
denoising, and super-resolution. Our thorough evaluation demonstrates that
SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and
FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS
and FID metrics.</div><div><a href='http://arxiv.org/abs/2401.17629v1'>2401.17629v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01779v1")'>Plug-and-Play image restoration with Stochastic deNOising REgularization</div>
<div id='2402.01779v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:05:47Z</div><div>Authors: Marien Renaud, Jean Prost, Arthur Leclaire, Nicolas Papadakis</div><div style='padding-top: 10px; width: 80ex'>Plug-and-Play (PnP) algorithms are a class of iterative algorithms that
address image inverse problems by combining a physical model and a deep neural
network for regularization. Even if they produce impressive image restoration
results, these algorithms rely on a non-standard use of a denoiser on images
that are less and less noisy along the iterations, which contrasts with recent
algorithms based on Diffusion Models (DM), where the denoiser is applied only
on re-noised images. We propose a new PnP framework, called Stochastic
deNOising REgularization (SNORE), which applies the denoiser only on images
with noise of the adequate level. It is based on an explicit stochastic
regularization, which leads to a stochastic gradient descent algorithm to solve
ill-posed inverse problems. A convergence analysis of this algorithm and its
annealing extension is provided. Experimentally, we prove that SNORE is
competitive with respect to state-of-the-art methods on deblurring and
inpainting tasks, both quantitatively and qualitatively.</div><div><a href='http://arxiv.org/abs/2402.01779v1'>2402.01779v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10585v1")'>Solving General Noisy Inverse Problem via Posterior Sampling: A Policy
  Gradient Viewpoint</div>
<div id='2403.10585v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T16:38:47Z</div><div>Authors: Haoyue Tang, Tian Xie, Aosong Feng, Hanyu Wang, Chenyang Zhang, Yang Bai</div><div style='padding-top: 10px; width: 80ex'>Solving image inverse problems (e.g., super-resolution and inpainting)
requires generating a high fidelity image that matches the given input (the
low-resolution image or the masked image). By using the input image as
guidance, we can leverage a pretrained diffusion generative model to solve a
wide range of image inverse tasks without task specific model fine-tuning. To
precisely estimate the guidance score function of the input image, we propose
Diffusion Policy Gradient (DPG), a tractable computation method by viewing the
intermediate noisy images as policies and the target image as the states
selected by the policy. Experiments show that our method is robust to both
Gaussian and Poisson noise degradation on multiple linear and non-linear
inverse tasks, resulting into a higher image restoration quality on FFHQ,
ImageNet and LSUN datasets.</div><div><a href='http://arxiv.org/abs/2403.10585v1'>2403.10585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02065v1")'>Training Implicit Networks for Image Deblurring using Jacobian-Free
  Backpropagation</div>
<div id='2402.02065v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T07:10:12Z</div><div>Authors: Linghai Liu, Shuaicheng Tong, Lisa Zhao</div><div style='padding-top: 10px; width: 80ex'>Recent efforts in applying implicit networks to solve inverse problems in
imaging have achieved competitive or even superior results when compared to
feedforward networks. These implicit networks only require constant memory
during backpropagation, regardless of the number of layers. However, they are
not necessarily easy to train. Gradient calculations are computationally
expensive because they require backpropagating through a fixed point. In
particular, this process requires solving a large linear system whose size is
determined by the number of features in the fixed point iteration. This paper
explores a recently proposed method, Jacobian-free Backpropagation (JFB), a
backpropagation scheme that circumvents such calculation, in the context of
image deblurring problems. Our results show that JFB is comparable against
fine-tuned optimization schemes, state-of-the-art (SOTA) feedforward networks,
and existing implicit networks at a reduced computational cost.</div><div><a href='http://arxiv.org/abs/2402.02065v1'>2402.02065v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06069v1")'>Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and
  Denoising</div>
<div id='2403.06069v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T03:22:57Z</div><div>Authors: Yuang Wang, Siyeop Yoon, Pengfei Jin, Matthew Tivnan, Zhennong Chen, Rui Hu, Li Zhang, Zhiqiang Chen, Quanzheng Li, Dufan Wu</div><div style='padding-top: 10px; width: 80ex'>Conditional diffusion models have gained recognition for their effectiveness
in image restoration tasks, yet their iterative denoising process, starting
from Gaussian noise, often leads to slow inference speeds. As a promising
alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the
generative process from corrupted images and integrates training techniques
from conditional diffusion models. In this study, we extended the I2SB method
by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB),
transitioning its generative process to a non-Markovian process by
incorporating corrupted images in each generative step. This enhancement
empowers I3SB to generate images with better texture restoration using a small
number of generative steps. The proposed method was validated on CT
super-resolution and denoising tasks and outperformed existing methods,
including the conditional denoising diffusion probabilistic model (cDDPM) and
I2SB, in both visual quality and quantitative metrics. These findings
underscore the potential of I3SB in improving medical image restoration by
providing fast and accurate generative modeling.</div><div><a href='http://arxiv.org/abs/2403.06069v1'>2403.06069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14440v1")'>Analysing Diffusion Segmentation for Medical Images</div>
<div id='2403.14440v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:45:54Z</div><div>Authors: Mathias Öttl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Katharina Breininger</div><div style='padding-top: 10px; width: 80ex'>Denoising Diffusion Probabilistic models have become increasingly popular due
to their ability to offer probabilistic modeling and generate diverse outputs.
This versatility inspired their adaptation for image segmentation, where
multiple predictions of the model can produce segmentation results that not
only achieve high quality but also capture the uncertainty inherent in the
model. Here, powerful architectures were proposed for improving diffusion
segmentation performance. However, there is a notable lack of analysis and
discussions on the differences between diffusion segmentation and image
generation, and thorough evaluations are missing that distinguish the
improvements these architectures provide for segmentation in general from their
benefit for diffusion segmentation specifically. In this work, we critically
analyse and discuss how diffusion segmentation for medical images differs from
diffusion image generation, with a particular focus on the training behavior.
Furthermore, we conduct an assessment how proposed diffusion segmentation
architectures perform when trained directly for segmentation. Lastly, we
explore how different medical segmentation tasks influence the diffusion
segmentation behavior and the diffusion process could be adapted accordingly.
With these analyses, we aim to provide in-depth insights into the behavior of
diffusion segmentation that allow for a better design and evaluation of
diffusion segmentation methods in the future.</div><div><a href='http://arxiv.org/abs/2403.14440v1'>2403.14440v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10786v1")'>ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion
  Models</div>
<div id='2403.10786v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T03:33:52Z</div><div>Authors: Yuwen Chen, Nicholas Konz, Hanxue Gu, Haoyu Dong, Yaqian Chen, Lin Li, Jisoo Lee, Maciej A. Mazurowski</div><div style='padding-top: 10px; width: 80ex'>Accurately translating medical images across different modalities (e.g., CT
to MRI) has numerous downstream clinical and machine learning applications.
While several methods have been proposed to achieve this, they often prioritize
perceptual quality with respect to output domain features over preserving
anatomical fidelity. However, maintaining anatomy during translation is
essential for many tasks, e.g., when leveraging masks from the input domain to
develop a segmentation model with images translated to the output domain. To
address these challenges, we propose ContourDiff, a novel framework that
leverages domain-invariant anatomical contour representations of images. These
representations are simple to extract from images, yet form precise spatial
constraints on their anatomical content. We introduce a diffusion model that
converts contour representations of images from arbitrary input domains into
images in the output domain of interest. By applying the contour as a
constraint at every diffusion sampling step, we ensure the preservation of
anatomical content. We evaluate our method by training a segmentation model on
images translated from CT to MRI with their original CT masks and testing its
performance on real MRIs. Our method outperforms other unpaired image
translation methods by a significant margin, furthermore without the need to
access any input domain information during training.</div><div><a href='http://arxiv.org/abs/2403.10786v1'>2403.10786v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07247v1")'>GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical
  structure Generation</div>
<div id='2403.07247v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T02:09:39Z</div><div>Authors: Linrui Dai, Rongzhao Zhang, Zhongzhen Huang, Xiaofan Zhang</div><div style='padding-top: 10px; width: 80ex'>The annotation burden and extensive labor for gathering a large medical
dataset with images and corresponding labels are rarely cost-effective and
highly intimidating. This results in a lack of abundant training data that
undermines downstream tasks and partially contributes to the challenge image
analysis faces in the medical field. As a workaround, given the recent success
of generative neural models, it is now possible to synthesize image datasets at
a high fidelity guided by external constraints. This paper explores this
possibility and presents \textbf{GuideGen}: a pipeline that jointly generates
CT images and tissue masks for abdominal organs and colorectal cancer
conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to
fit the discrete distribution of mask labels and generate low-resolution 3D
tissue masks. Secondly, our Conditional Image Generator autoregressively
generates CT slices conditioned on a corresponding mask slice to incorporate
both style information and anatomical guidance. This pipeline guarantees high
fidelity and variability as well as exact alignment between generated CT
volumes and tissue masks. Both qualitative and quantitative experiments on 3D
abdominal CTs demonstrate a high performance of our proposed pipeline, thereby
proving our method can serve as a dataset generator and provide potential
benefits to downstream tasks. It is hoped that our work will offer a promising
solution on the multimodality generation of CT and its anatomical mask. Our
source code is publicly available at
https://github.com/OvO1111/JointImageGeneration.</div><div><a href='http://arxiv.org/abs/2403.07247v1'>2403.07247v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14429v1")'>Style-Extracting Diffusion Models for Semi-Supervised Histopathology
  Segmentation</div>
<div id='2403.14429v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:36:59Z</div><div>Authors: Mathias Öttl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias Rübner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier, Ramona Erber, Bernhard Kainz, Katharina Breininger</div><div style='padding-top: 10px; width: 80ex'>Deep learning-based image generation has seen significant advancements with
diffusion models, notably improving the quality of generated images. Despite
these developments, generating images with unseen characteristics beneficial
for downstream tasks has received limited attention. To bridge this gap, we
propose Style-Extracting Diffusion Models, featuring two conditioning
mechanisms. Specifically, we utilize 1) a style conditioning mechanism which
allows to inject style information of previously unseen images during image
generation and 2) a content conditioning which can be targeted to a downstream
task, e.g., layout for segmentation. We introduce a trainable style encoder to
extract style information from images, and an aggregation block that merges
style information from multiple style inputs. This architecture enables the
generation of images with unseen styles in a zero-shot manner, by leveraging
styles from unseen images, resulting in more diverse generations. In this work,
we use the image layout as target condition and first show the capability of
our method on a natural image dataset as a proof-of-concept. We further
demonstrate its versatility in histopathology, where we combine prior knowledge
about tissue composition and unannotated data to create diverse synthetic
images with known layouts. This allows us to generate additional synthetic data
to train a segmentation network in a semi-supervised fashion. We verify the
added value of the generated images by showing improved segmentation results
and lower performance variability between patients when synthetic images are
included during segmentation training. Our code will be made publicly available
at [LINK].</div><div><a href='http://arxiv.org/abs/2403.14429v1'>2403.14429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15237v1")'>Unsupervised Domain Adaptation for Brain Vessel Segmentation through
  Transwarp Contrastive Learning</div>
<div id='2402.15237v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T10:01:22Z</div><div>Authors: Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Kun Wu, Nishant Ravikumar, Alejandro F. Frangi</div><div style='padding-top: 10px; width: 80ex'>Unsupervised domain adaptation (UDA) aims to align the labelled source
distribution with the unlabelled target distribution to obtain domain-invariant
predictive models. Since cross-modality medical data exhibit significant intra
and inter-domain shifts and most are unlabelled, UDA is more important while
challenging in medical image analysis. This paper proposes a simple yet potent
contrastive learning framework for UDA to narrow the inter-domain gap between
labelled source and unlabelled target distribution. Our method is validated on
cerebral vessel datasets. Experimental results show that our approach can learn
latent features from labelled 3DRA modality data and improve vessel
segmentation performance in unlabelled MRA modality data.</div><div><a href='http://arxiv.org/abs/2402.15237v1'>2402.15237v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15239v1")'>GS-EMA: Integrating Gradient Surgery Exponential Moving Average with
  Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in
  Aneurysm Segmentation</div>
<div id='2402.15239v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T10:02:15Z</div><div>Authors: Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Nina Cheng, Nishant Ravikumar, Alejandro F. Frangi</div><div style='padding-top: 10px; width: 80ex'>The automated segmentation of cerebral aneurysms is pivotal for accurate
diagnosis and treatment planning. Confronted with significant domain shifts and
class imbalance in 3D Rotational Angiography (3DRA) data from various medical
institutions, the task becomes challenging. These shifts include differences in
image appearance, intensity distribution, resolution, and aneurysm size, all of
which complicate the segmentation process. To tackle these issues, we propose a
novel domain generalization strategy that employs gradient surgery exponential
moving average (GS-EMA) optimization technique coupled with boundary-aware
contrastive learning (BACL). Our approach is distinct in its ability to adapt
to new, unseen domains by learning domain-invariant features, thereby improving
the robustness and accuracy of aneurysm segmentation across diverse clinical
datasets. The results demonstrate that our proposed approach can extract more
domain-invariant features, minimizing over-segmentation and capturing more
complete aneurysm structures.</div><div><a href='http://arxiv.org/abs/2402.15239v1'>2402.15239v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15103v1")'>Improving cross-domain brain tissue segmentation in fetal MRI with
  synthetic data</div>
<div id='2403.15103v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T10:42:25Z</div><div>Authors: Vladyslav Zalevskyi, Thomas Sanchez, Margaux Roulet, Jordina Aviles Verddera, Jana Hutter, Hamza Kebiri, Meritxell Bach Cuadra</div><div style='padding-top: 10px; width: 80ex'>Segmentation of fetal brain tissue from magnetic resonance imaging (MRI)
plays a crucial role in the study of in utero neurodevelopment. However,
automated tools face substantial domain shift challenges as they must be robust
to highly heterogeneous clinical data, often limited in numbers and lacking
annotations. Indeed, high variability of the fetal brain morphology, MRI
acquisition parameters, and superresolution reconstruction (SR) algorithms
adversely affect the model's performance when evaluated out-of-domain. In this
work, we introduce FetalSynthSeg, a domain randomization method to segment
fetal brain MRI, inspired by SynthSeg. Our results show that models trained
solely on synthetic data outperform models trained on real data in out-ofdomain
settings, validated on a 120-subject cross-domain dataset. Furthermore, we
extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and
reconstructed with novel SR models, showcasing robustness across different
magnetic field strengths and SR algorithms. Leveraging a generative synthetic
approach, we tackle the domain shift problem in fetal brain MRI and offer
compelling prospects for applications in fields with limited and highly
heterogeneous data.</div><div><a href='http://arxiv.org/abs/2403.15103v1'>2403.15103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13776v1")'>Cas-DiffCom: Cascaded diffusion model for infant longitudinal
  super-resolution 3D medical image completion</div>
<div id='2402.13776v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:54:40Z</div><div>Authors: Lianghu Guo, Tianli Tao, Xinyi Cai, Zihao Zhu, Jiawei Huang, Lixuan Zhu, Zhuoyang Gu, Haifeng Tang, Rui Zhou, Siyan Han, Yan Liang, Qing Yang, Dinggang Shen, Han Zhang</div><div style='padding-top: 10px; width: 80ex'>Early infancy is a rapid and dynamic neurodevelopmental period for behavior
and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an
effective tool to investigate such a crucial stage by capturing the
developmental trajectories of the brain structures. However, longitudinal MRI
acquisition always meets a serious data-missing problem due to participant
dropout and failed scans, making longitudinal infant brain atlas construction
and developmental trajectory delineation quite challenging. Thanks to the
development of an AI-based generative model, neuroimage completion has become a
powerful technique to retain as much available data as possible. However,
current image completion methods usually suffer from inconsistency within each
individual subject in the time dimension, compromising the overall quality. To
solve this problem, our paper proposed a two-stage cascaded diffusion model,
Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and
super-resolution. We applied our proposed method to the Baby Connectome Project
(BCP) dataset. The experiment results validate that Cas-DiffCom achieves both
individual consistency and high fidelity in longitudinal infant brain image
completion. We further applied the generated infant brain images to two
downstream tasks, brain tissue segmentation and developmental trajectory
delineation, to declare its task-oriented potential in the neuroscience field.</div><div><a href='http://arxiv.org/abs/2402.13776v1'>2402.13776v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02444v1")'>Anatomically Constrained Tractography of the Fetal Brain</div>
<div id='2403.02444v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:56:19Z</div><div>Authors: Camilo Calixto, Camilo Jaimes, Matheus D. Soldatelli, Simon K. Warfield, Ali Gholipour, Davood Karimi</div><div style='padding-top: 10px; width: 80ex'>Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to
study the fetal brain in utero. An important computation enabled by dMRI is
streamline tractography, which has unique applications such as tract-specific
analysis of the brain white matter and structural connectivity assessment.
However, due to the low fetal dMRI data quality and the challenging nature of
tractography, existing methods tend to produce highly inaccurate results. They
generate many false streamlines while failing to reconstruct streamlines that
constitute the major white matter tracts. In this paper, we advocate for
anatomically constrained tractography based on an accurate segmentation of the
fetal brain tissue directly in the dMRI space. We develop a deep learning
method to compute the segmentation automatically. Experiments on independent
test data show that this method can accurately segment the fetal brain tissue
and drastically improve tractography results. It enables the reconstruction of
highly curved tracts such as optic radiations. Importantly, our method infers
the tissue segmentation and streamline propagation direction from a diffusion
tensor fit to the dMRI data, making it applicable to routine fetal dMRI scans.
The proposed method can lead to significant improvements in the accuracy and
reproducibility of quantitative assessment of the fetal brain with dMRI.</div><div><a href='http://arxiv.org/abs/2403.02444v1'>2403.02444v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16782v1")'>A Literature Review on Fetus Brain Motion Correction in MRI</div>
<div id='2401.16782v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T06:43:40Z</div><div>Authors: Haoran Zhang, Yun Wang</div><div style='padding-top: 10px; width: 80ex'>This paper provides a comprehensive review of the latest advancements in
fetal motion correction in MRI. We delve into various contemporary
methodologies and technological advancements aimed at overcoming these
challenges. It includes traditional 3D fetal MRI correction methods like Slice
to Volume Registration (SVR), deep learning-based techniques such as
Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) Networks,
Transformers, Generative Adversarial Networks (GANs) and most recent
advancements of Diffusion Models. The insights derived from this literature
review reflect a thorough understanding of both the technical intricacies and
practical implications of fetal motion in MRI studies, offering a reasoned
perspective on potential solutions and future improvements in this field.</div><div><a href='http://arxiv.org/abs/2401.16782v1'>2401.16782v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.11789v1")'>Statistical Test for Generated Hypotheses by Diffusion Models</div>
<div id='2402.11789v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:32:45Z</div><div>Authors: Teruyuki Katsuoka, Tomohiro Shiraishi, Daiki Miwa, Vo Nguyen Le Duy, Ichiro Takeuchi</div><div style='padding-top: 10px; width: 80ex'>The enhanced performance of AI has accelerated its integration into
scientific research. In particular, the use of generative AI to create
scientific hypotheses is promising and is increasingly being applied across
various fields. However, when employing AI-generated hypotheses for critical
decisions, such as medical diagnoses, verifying their reliability is crucial.
In this study, we consider a medical diagnostic task using generated images by
diffusion models, and propose a statistical test to quantify its reliability.
The basic idea behind the proposed statistical test is to employ a selective
inference framework, where we consider a statistical test conditional on the
fact that the generated images are produced by a trained diffusion model. Using
the proposed method, the statistical reliability of medical image diagnostic
results can be quantified in the form of a p-value, allowing for
decision-making with a controlled error rate. We show the theoretical validity
of the proposed statistical test and its effectiveness through numerical
experiments on synthetic and brain image datasets.</div><div><a href='http://arxiv.org/abs/2402.11789v1'>2402.11789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14401v1")'>Diffusion Model Based Visual Compensation Guidance and Visual Difference
  Analysis for No-Reference Image Quality Assessment</div>
<div id='2402.14401v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:39:46Z</div><div>Authors: Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong, Xinbo Gao</div><div style='padding-top: 10px; width: 80ex'>Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA)
methods still suffer from finding a balance between learning feature
information at the pixel level of the image and capturing high-level feature
information and the efficient utilization of the obtained high-level feature
information remains a challenge. As a novel class of state-of-the-art (SOTA)
generative model, the diffusion model exhibits the capability to model
intricate relationships, enabling a comprehensive understanding of images and
possessing a better learning of both high-level and low-level visual features.
In view of these, we pioneer the exploration of the diffusion model into the
domain of NR-IQA. Firstly, we devise a new diffusion restoration network that
leverages the produced enhanced image and noise-containing images,
incorporating nonlinear features obtained during the denoising process of the
diffusion model, as high-level visual information. Secondly, two visual
evaluation branches are designed to comprehensively analyze the obtained
high-level feature information. These include the visual compensation guidance
branch, grounded in the transformer architecture and noise embedding strategy,
and the visual difference analysis branch, built on the ResNet architecture and
the residual transposed attention block. Extensive experiments are conducted on
seven public NR-IQA datasets, and the results demonstrate that the proposed
model outperforms SOTA methods for NR-IQA.</div><div><a href='http://arxiv.org/abs/2402.14401v1'>2402.14401v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08522v1")'>Video Quality Assessment Based on Swin TransformerV2 and Coarse to Fine
  Strategy</div>
<div id='2401.08522v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:33:54Z</div><div>Authors: Zihao Yu, Fengbin Guan, Yiting Lu, Xin Li, Zhibo Chen</div><div style='padding-top: 10px; width: 80ex'>The objective of non-reference video quality assessment is to evaluate the
quality of distorted video without access to reference high-definition
references. In this study, we introduce an enhanced spatial perception module,
pre-trained on multiple image quality assessment datasets, and a lightweight
temporal fusion module to address the no-reference visual quality assessment
(NR-VQA) task. This model implements Swin Transformer V2 as a local-level
spatial feature extractor and fuses these multi-stage representations through a
series of transformer layers. Furthermore, a temporal transformer is utilized
for spatiotemporal feature fusion across the video. To accommodate compressed
videos of varying bitrates, we incorporate a coarse-to-fine contrastive
strategy to enrich the model's capability to discriminate features from videos
of different bitrates. This is an expanded version of the one-page abstract.</div><div><a href='http://arxiv.org/abs/2401.08522v1'>2401.08522v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12292v1")'>Regularization by denoising: Bayesian model and Langevin-within-split
  Gibbs sampling</div>
<div id='2402.12292v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T17:12:16Z</div><div>Authors: Elhadji C. Faye, Mame Diarra Fall, Nicolas Dobigeon</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a Bayesian framework for image inversion by deriving a
probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It
additionally implements a Monte Carlo algorithm specifically tailored for
sampling from the resulting posterior distribution, based on an asymptotically
exact data augmentation (AXDA). The proposed algorithm is an approximate
instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo
step. The proposed method is applied to common imaging tasks such as
deblurring, inpainting and super-resolution, demonstrating its efficacy through
extensive numerical experiments. These contributions advance Bayesian inference
in imaging by leveraging data-driven regularization strategies within a
probabilistic framework.</div><div><a href='http://arxiv.org/abs/2402.12292v1'>2402.12292v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15316v1")'>Ultrasound Imaging based on the Variance of a Diffusion Restoration
  Model</div>
<div id='2403.15316v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T16:10:38Z</div><div>Authors: Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</div><div style='padding-top: 10px; width: 80ex'>Despite today's prevalence of ultrasound imaging in medicine, ultrasound
signal-to-noise ratio is still affected by several sources of noise and
artefacts. Moreover, enhancing ultrasound image quality involves balancing
concurrent factors like contrast, resolution, and speckle preservation.
Recently, there has been progress in both model-based and learning-based
approaches addressing the problem of ultrasound image reconstruction. Bringing
the best from both worlds, we propose a hybrid reconstruction method combining
an ultrasound linear direct model with a learning-based prior coming from a
generative Denoising Diffusion model. More specifically, we rely on the
unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model
(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this
paper proposes an empirical model to characterize the stochasticity of
diffusion reconstruction of ultrasound images, and shows the interest of its
variance as an echogenicity map estimator. We conduct experiments on synthetic,
in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging
approach in achieving high-quality image reconstructions from single plane-wave
acquisitions and in comparison to state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.15316v1'>2403.15316v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06275v1")'>UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation</div>
<div id='2403.06275v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T18:05:41Z</div><div>Authors: Kwanyoung Kim, Jaa-Yeon Lee, Jong Chul Ye</div><div style='padding-top: 10px; width: 80ex'>Nakagami imaging holds promise for visualizing and quantifying tissue
scattering in ultrasound waves, with potential applications in tumor diagnosis
and fat fraction estimation which are challenging to discern by conventional
ultrasound B-mode images. Existing methods struggle with optimal window size
selection and suffer from estimator instability, leading to degraded resolution
images. To address this, here we propose a novel method called UNICORN
(Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an
accurate, closed-form estimator for Nakagami parameter estimation in terms of
the score function of ultrasonic envelope. Extensive experiments using
simulation and real ultrasound RF data demonstrate UNICORN's superiority over
conventional approaches in accuracy and resolution quality.</div><div><a href='http://arxiv.org/abs/2403.06275v1'>2403.06275v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00289v1")'>Optimization of Array Encoding for Ultrasound Imaging</div>
<div id='2403.00289v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T05:19:59Z</div><div>Authors: Jacob Spainhour, Korben Smart, Stephen Becker, Nick Bottenus</div><div style='padding-top: 10px; width: 80ex'>Objective: The transmit encoding model for synthetic aperture imaging is a
robust and flexible framework for understanding the effect of acoustic
transmission on ultrasound image reconstruction. Our objective is to use
machine learning (ML) to construct scanning sequences, parameterized by time
delays and apodization weights, that produce high quality B-mode images.
Approach: We use an ML model in PyTorch and simulated RF data from Field II to
probe the space of possible encoding sequences for those that minimize a loss
function that describes image quality. This approach is made computationally
feasible by a novel formulation of the derivative for delay-and-sum
beamforming. We demonstrate these results experimentally on wire targets and a
tissue-mimicking phantom. Main Results: When trained according to a given set
of imaging parameters (imaging domain, hardware restrictions), our ML imaging
model produces optimized encoding sequences that improve a number of standard
quality metrics including resolution, field of view, and contrast, over
conventional sequences. Significance: This work demonstrates that the set of
encoding schemes that are commonly used represent only a narrow subset of those
available. Additionally, it demonstrates the value for ML tasks in synthetic
transmit aperture imaging to consider the beamformer within the model, instead
of as purely post-processing.</div><div><a href='http://arxiv.org/abs/2403.00289v1'>2403.00289v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.03228v1")'>Reflected Schrödinger Bridge for Constrained Generative Modeling</div>
<div id='2401.03228v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T14:39:58Z</div><div>Authors: Wei Deng, Yu Chen, Nicole Tianjiao Yang, Hengrong Du, Qi Feng, Ricky T. Q. Chen</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have become the go-to method for large-scale generative
models in real-world applications. These applications often involve data
distributions confined within bounded domains, typically requiring ad-hoc
thresholding techniques for boundary enforcement. Reflected diffusion models
(Lou23) aim to enhance generalizability by generating the data distribution
through a backward process governed by reflected Brownian motion. However,
reflected diffusion models may not easily adapt to diverse domains without the
derivation of proper diffeomorphic mappings and do not guarantee optimal
transport properties. To overcome these limitations, we introduce the Reflected
Schrodinger Bridge algorithm: an entropy-regularized optimal transport approach
tailored for generating data within diverse bounded domains. We derive elegant
reflected forward-backward stochastic differential equations with Neumann and
Robin boundary conditions, extend divergence-based likelihood training to
bounded domains, and explore natural connections to entropic optimal transport
for the study of approximate linear convergence - a valuable insight for
practical training. Our algorithm yields robust generative modeling in diverse
domains, and its scalability is demonstrated in real-world constrained
generative modeling through standard image benchmarks.</div><div><a href='http://arxiv.org/abs/2401.03228v1'>2401.03228v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05069v1")'>Improving Diffusion-Based Generative Models via Approximated Optimal
  Transport</div>
<div id='2403.05069v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T05:43:00Z</div><div>Authors: Daegyu Kim, Jooyoung Choi, Chaehun Shin, Uiwon Hwang, Sungroh Yoon</div><div style='padding-top: 10px; width: 80ex'>We introduce the Approximated Optimal Transport (AOT) technique, a novel
training scheme for diffusion-based generative models. Our approach aims to
approximate and integrate optimal transport into the training process,
significantly enhancing the ability of diffusion models to estimate the
denoiser outputs accurately. This improvement leads to ODE trajectories of
diffusion models with lower curvature and reduced truncation errors during
sampling. We achieve superior image quality and reduced sampling steps by
employing AOT in training. Specifically, we achieve FID scores of 1.88 with
just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional
generations, respectively. Furthermore, when applying AOT to train the
discriminator for guidance, we establish new state-of-the-art FID scores of
1.68 and 1.58 for unconditional and conditional generations, respectively, each
with 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing
the performance of diffusion models.</div><div><a href='http://arxiv.org/abs/2403.05069v1'>2403.05069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12510v1")'>Generalized Consistency Trajectory Models for Image Manipulation</div>
<div id='2403.12510v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T07:24:54Z</div><div>Authors: Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye</div><div style='padding-top: 10px; width: 80ex'>Diffusion-based generative models excel in unconditional generation, as well
as on applied tasks such as image editing and restoration. The success of
diffusion models lies in the iterative nature of diffusion: diffusion breaks
down the complex process of mapping noise to data into a sequence of simple
denoising tasks. Moreover, we are able to exert fine-grained control over the
generation process by injecting guidance terms into each denoising step.
However, the iterative process is also computationally intensive, often taking
from tens up to thousands of function evaluations. Although consistency
trajectory models (CTMs) enable traversal between any time points along the
probability flow ODE (PFODE) and score inference with a single function
evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this
work aims to unlock the full potential of CTMs by proposing generalized CTMs
(GCTMs), which translate between arbitrary distributions via ODEs. We discuss
the design space of GCTMs and demonstrate their efficacy in various image
manipulation tasks such as image-to-image translation, restoration, and
editing. Code: \url{https://github.com/1202kbs/GCTM}</div><div><a href='http://arxiv.org/abs/2403.12510v1'>2403.12510v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11415v1")'>DreamSampler: Unifying Diffusion Sampling and Score Distillation for
  Image Manipulation</div>
<div id='2403.11415v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T02:08:58Z</div><div>Authors: Jeongsol Kim, Geon Yeong Park, Jong Chul Ye</div><div style='padding-top: 10px; width: 80ex'>Reverse sampling and score-distillation have emerged as main workhorses in
recent years for image manipulation using latent diffusion models (LDMs). While
reverse diffusion sampling often requires adjustments of LDM architecture or
feature engineering, score distillation offers a simple yet powerful
model-agnostic approach, but it is often prone to mode-collapsing. To address
these limitations and leverage the strengths of both approaches, here we
introduce a novel framework called {\em DreamSampler}, which seamlessly
integrates these two distinct approaches through the lens of regularized latent
optimization. Similar to score-distillation, DreamSampler is a model-agnostic
approach applicable to any LDM architecture, but it allows both distillation
and reverse sampling with additional guidance for image editing and
reconstruction. Through experiments involving image editing, SVG reconstruction
and etc, we demonstrate the competitive performance of DreamSampler compared to
existing approaches, while providing new applications.</div><div><a href='http://arxiv.org/abs/2403.11415v1'>2403.11415v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00437v1")'>LoMOE: Localized Multi-Object Editing via Multi-Diffusion</div>
<div id='2403.00437v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T10:46:47Z</div><div>Authors: Goirik Chakrabarty, Aditya Chandrasekar, Ramya Hebbalaguppe, Prathosh AP</div><div style='padding-top: 10px; width: 80ex'>Recent developments in the field of diffusion models have demonstrated an
exceptional capacity to generate high-quality prompt-conditioned image edits.
Nevertheless, previous approaches have primarily relied on textual prompts for
image editing, which tend to be less effective when making precise edits to
specific objects or fine-grained regions within a scene containing
single/multiple objects. We introduce a novel framework for zero-shot localized
multi-object editing through a multi-diffusion process to overcome this
challenge. This framework empowers users to perform various operations on
objects within an image, such as adding, replacing, or editing $\textbf{many}$
objects in a complex scene $\textbf{in one pass}$. Our approach leverages
foreground masks and corresponding simple text prompts that exert localized
influences on the target regions resulting in high-fidelity image editing. A
combination of cross-attention and background preservation losses within the
latent space ensures that the characteristics of the object being edited are
preserved while simultaneously achieving a high-quality, seamless
reconstruction of the background with fewer artifacts compared to the current
methods. We also curate and release a dataset dedicated to multi-object
editing, named $\texttt{LoMOE}$-Bench. Our experiments against existing
state-of-the-art methods demonstrate the improved effectiveness of our approach
in terms of both image editing quality and inference speed.</div><div><a href='http://arxiv.org/abs/2403.00437v1'>2403.00437v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14617v2")'>Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion</div>
<div id='2403.14617v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:59:03Z</div><div>Authors: Xiang Fan, Anand Bhattad, Ranjay Krishna</div><div style='padding-top: 10px; width: 80ex'>We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.</div><div><a href='http://arxiv.org/abs/2403.14617v2'>2403.14617v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05735v1")'>Object-Centric Diffusion for Efficient Video Editing</div>
<div id='2401.05735v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T08:36:15Z</div><div>Authors: Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki M. Asano, Amirhossein Habibian</div><div style='padding-top: 10px; width: 80ex'>Diffusion-based video editing have reached impressive quality and can
transform either the global style, local structure, and attributes of given
video inputs, following textual edit prompts. However, such solutions typically
incur heavy memory and computational costs to generate temporally-coherent
frames, either in the form of diffusion inversion and/or cross-frame attention.
In this paper, we conduct an analysis of such inefficiencies, and suggest
simple yet effective modifications that allow significant speed-ups whilst
maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as
OCD, to further reduce latency by allocating computations more towards
foreground edited regions that are arguably more important for perceptual
quality. We achieve this by two novel proposals: i) Object-Centric Sampling,
decoupling the diffusion steps spent on salient regions or background,
allocating most of the model capacity to the former, and ii) Object-Centric 3D
Token Merging, which reduces cost of cross-frame attention by fusing redundant
tokens in unimportant background regions. Both techniques are readily
applicable to a given video editing model \textit{without} retraining, and can
drastically reduce its memory and computational cost. We evaluate our proposals
on inversion-based and control-signal-based editing pipelines, and show a
latency reduction up to 10x for a comparable synthesis quality.</div><div><a href='http://arxiv.org/abs/2401.05735v1'>2401.05735v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13551v1")'>Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute
  Editing</div>
<div id='2403.13551v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T12:40:32Z</div><div>Authors: Hangeol Chang, Jinho Chang, Jong Chul Ye</div><div style='padding-top: 10px; width: 80ex'>Despite recent advancements in text-to-image diffusion models facilitating
various image editing techniques, complex text prompts often lead to an
oversight of some requests due to a bottleneck in processing text information.
To tackle this challenge, we present Ground-A-Score, a simple yet powerful
model-agnostic image editing method by incorporating grounding during score
distillation. This approach ensures a precise reflection of intricate prompt
requirements in the editing outcomes, taking into account the prior knowledge
of the object locations within the image. Moreover, the selective application
with a new penalty coefficient and contrastive loss helps to precisely target
editing areas while preserving the integrity of the objects in the source
image. Both qualitative assessments and quantitative analyses confirm that
Ground-A-Score successfully adheres to the intricate details of extended and
multifaceted prompts, ensuring high-quality outcomes that respect the original
image attributes.</div><div><a href='http://arxiv.org/abs/2403.13551v1'>2403.13551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03082v1")'>Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual
  Text Processing</div>
<div id='2402.03082v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:13:20Z</div><div>Authors: Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou</div><div style='padding-top: 10px; width: 80ex'>Visual text, a pivotal element in both document and scene images, speaks
volumes and attracts significant attention in the computer vision domain.
Beyond visual text detection and recognition, the field of visual text
processing has experienced a surge in research, driven by the advent of
fundamental generative models. However, challenges persist due to the unique
properties and features that distinguish text from general objects. Effectively
leveraging these unique textual characteristics is crucial in visual text
processing, as observed in our study. In this survey, we present a
comprehensive, multi-perspective analysis of recent advancements in this field.
Initially, we introduce a hierarchical taxonomy encompassing areas ranging from
text image enhancement and restoration to text image manipulation, followed by
different learning paradigms. Subsequently, we conduct an in-depth discussion
of how specific textual features such as structure, stroke, semantics, style,
and spatial context are seamlessly integrated into various tasks. Furthermore,
we explore available public datasets and benchmark the reviewed methods on
several widely-used datasets. Finally, we identify principal challenges and
potential avenues for future research. Our aim is to establish this survey as a
fundamental resource, fostering continued exploration and innovation in the
dynamic area of visual text processing.</div><div><a href='http://arxiv.org/abs/2402.03082v1'>2402.03082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14792v1")'>Consolidating Attention Features for Multi-view Image Editing</div>
<div id='2402.14792v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:50:18Z</div><div>Authors: Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre</div><div style='padding-top: 10px; width: 80ex'>Large-scale text-to-image models enable a wide range of image editing
techniques, using text prompts or even spatial controls. However, applying
these editing methods to multi-view images depicting a single scene leads to
3D-inconsistent results. In this work, we focus on spatial control-based
geometric manipulations and introduce a method to consolidate the editing
process across various views. We build on two insights: (1) maintaining
consistent features throughout the generative process helps attain consistency
in multi-view editing, and (2) the queries in self-attention layers
significantly influence the image structure. Hence, we propose to improve the
geometric consistency of the edited images by enforcing the consistency of the
queries. To do so, we introduce QNeRF, a neural radiance field trained on the
internal query features of the edited images. Once trained, QNeRF can render
3D-consistent queries, which are then softly injected back into the
self-attention layers during generation, greatly improving multi-view
consistency. We refine the process through a progressive, iterative method that
better consolidates queries across the diffusion timesteps. We compare our
method to a range of existing techniques and demonstrate that it can achieve
better multi-view consistency and higher fidelity to the input scene. These
advantages allow us to train NeRFs with fewer visual artifacts, that are better
aligned with the target geometry.</div><div><a href='http://arxiv.org/abs/2402.14792v1'>2402.14792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14398v1")'>pix2gestalt: Amodal Segmentation by Synthesizing Wholes</div>
<div id='2401.14398v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:57:36Z</div><div>Authors: Ege Ozguroglu, Ruoshi Liu, Dídac Surís, Dian Chen, Achal Dave, Pavel Tokmakov, Carl Vondrick</div><div style='padding-top: 10px; width: 80ex'>We introduce pix2gestalt, a framework for zero-shot amodal segmentation,
which learns to estimate the shape and appearance of whole objects that are
only partially visible behind occlusions. By capitalizing on large-scale
diffusion models and transferring their representations to this task, we learn
a conditional diffusion model for reconstructing whole objects in challenging
zero-shot cases, including examples that break natural and physical priors,
such as art. As training data, we use a synthetically curated dataset
containing occluded objects paired with their whole counterparts. Experiments
show that our approach outperforms supervised baselines on established
benchmarks. Our model can furthermore be used to significantly improve the
performance of existing object recognition and 3D reconstruction methods in the
presence of occlusions.</div><div><a href='http://arxiv.org/abs/2401.14398v1'>2401.14398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03047v1")'>PFDM: Parser-Free Virtual Try-on via Diffusion Model</div>
<div id='2402.03047v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:32:57Z</div><div>Authors: Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang</div><div style='padding-top: 10px; width: 80ex'>Virtual try-on can significantly improve the garment shopping experiences in
both online and in-store scenarios, attracting broad interest in computer
vision. However, to achieve high-fidelity try-on performance, most
state-of-the-art methods still rely on accurate segmentation masks, which are
often produced by near-perfect parsers or manual labeling. To overcome the
bottleneck, we propose a parser-free virtual try-on method based on the
diffusion model (PFDM). Given two images, PFDM can "wear" garments on the
target person seamlessly by implicitly warping without any other information.
To learn the model effectively, we synthesize many pseudo-images and construct
sample pairs by wearing various garments on persons. Supervised by the
large-scale expanded dataset, we fuse the person and garment features using a
proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that
our proposed PFDM can successfully handle complex cases, synthesize
high-fidelity images, and outperform both state-of-the-art parser-free and
parser-based models.</div><div><a href='http://arxiv.org/abs/2402.03047v1'>2402.03047v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00994v1")'>A Cost-Efficient Approach for Creating Virtual Fitting Room using
  Generative Adversarial Networks (GANs)</div>
<div id='2402.00994v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T20:18:06Z</div><div>Authors: Kirolos Attallah, Girgis Zaky, Nourhan Abdelrhim, Kyrillos Botros, Amjad Dife, Nermin Negied</div><div style='padding-top: 10px; width: 80ex'>Customers all over the world want to see how the clothes fit them or not
before purchasing. Therefore, customers by nature prefer brick-and-mortar
clothes shopping so they can try on products before purchasing them. But after
the Pandemic of COVID19 many sellers either shifted to online shopping or
closed their fitting rooms which made the shopping process hesitant and
doubtful. The fact that the clothes may not be suitable for their buyers after
purchase led us to think about using new AI technologies to create an online
platform or a virtual fitting room (VFR) in the form of a mobile application
and a deployed model using a webpage that can be embedded later to any online
store where they can try on any number of cloth items without physically trying
them. Besides, it will save much searching time for their needs. Furthermore,
it will reduce the crowding and headache in the physical shops by applying the
same technology using a special type of mirror that will enable customers to
try on faster. On the other hand, from business owners' perspective, this
project will highly increase their online sales, besides, it will save the
quality of the products by avoiding physical trials issues. The main approach
used in this work is applying Generative Adversarial Networks (GANs) combined
with image processing techniques to generate one output image from two input
images which are the person image and the cloth image. This work achieved
results that outperformed the state-of-the-art approaches found in literature.</div><div><a href='http://arxiv.org/abs/2402.00994v1'>2402.00994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.10731v1")'>Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving
  Conditional Human Image Generation</div>
<div id='2403.10731v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T23:31:41Z</div><div>Authors: Anton Pelykh, Ozge Mercanoglu Sincan, Richard Bowden</div><div style='padding-top: 10px; width: 80ex'>Recent years have seen significant progress in human image generation,
particularly with the advancements in diffusion models. However, existing
diffusion methods encounter challenges when producing consistent hand anatomy
and the generated images often lack precise control over the hand pose. To
address this limitation, we introduce a novel approach to pose-conditioned
human image generation, dividing the process into two stages: hand generation
and subsequent body out-painting around the hands. We propose training the hand
generator in a multi-task setting to produce both hand images and their
corresponding segmentation masks, and employ the trained model in the first
stage of generation. An adapted ControlNet model is then used in the second
stage to outpaint the body around the generated hands, producing the final
result. A novel blending technique is introduced to preserve the hand details
during the second stage that combines the results of both stages in a coherent
way. This involves sequential expansion of the out-painted region while fusing
the latent representations, to ensure a seamless and cohesive synthesis of the
final image. Experimental evaluations demonstrate the superiority of our
proposed method over state-of-the-art techniques, in both pose accuracy and
image quality, as validated on the HaGRID dataset. Our approach not only
enhances the quality of the generated hands but also offers improved control
over hand pose, advancing the capabilities of pose-conditioned human image
generation. The source code of the proposed approach is available at
https://github.com/apelykh/hand-to-diffusion.</div><div><a href='http://arxiv.org/abs/2403.10731v1'>2403.10731v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19197v1")'>Fine Structure-Aware Sampling: A New Sampling Training Scheme for
  Pixel-Aligned Implicit Models in Single-View Human Reconstruction</div>
<div id='2402.19197v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T14:26:46Z</div><div>Authors: Kennard Yanting Chan, Fayao Liu, Guosheng Lin, Chuan Sheng Foo, Weisi Lin</div><div style='padding-top: 10px; width: 80ex'>Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for
single-view clothed human reconstruction. These models need to be trained using
a sampling training scheme. Existing sampling training schemes either fail to
capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in
reconstructed meshes. To address these problems, we introduce Fine
Structured-Aware Sampling (FSS), a new sampling training scheme to train
pixel-aligned implicit models for single-view human reconstruction. FSS
resolves the aforementioned problems by proactively adapting to the thickness
and complexity of surfaces. In addition, unlike existing sampling training
schemes, FSS shows how normals of sample points can be capitalized in the
training process to improve results. Lastly, to further improve the training
process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit
models. It becomes computationally feasible to introduce this loss once a
slight reworking of the pixel-aligned implicit function framework is carried
out. Our results show that our methods significantly outperform SOTA methods
qualitatively and quantitatively. Our code is publicly available at
https://github.com/kcyt/FSS.</div><div><a href='http://arxiv.org/abs/2402.19197v1'>2402.19197v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02936v1")'>Panoramic Image Inpainting With Gated Convolution And Contextual
  Reconstruction Loss</div>
<div id='2402.02936v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:58:08Z</div><div>Authors: Li Yu, Yanjun Gao, Farhad Pakdaman, Moncef Gabbouj</div><div style='padding-top: 10px; width: 80ex'>Deep learning-based methods have demonstrated encouraging results in tackling
the task of panoramic image inpainting. However, it is challenging for existing
methods to distinguish valid pixels from invalid pixels and find suitable
references for corrupted areas, thus leading to artifacts in the inpainted
results. In response to these challenges, we propose a panoramic image
inpainting framework that consists of a Face Generator, a Cube Generator, a
side branch, and two discriminators. We use the Cubemap Projection (CMP) format
as network input. The generator employs gated convolutions to distinguish valid
pixels from invalid ones, while a side branch is designed utilizing contextual
reconstruction (CR) loss to guide the generators to find the most suitable
reference patch for inpainting the missing region. The proposed method is
compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in
terms of PSNR and SSIM. Experimental results and ablation study demonstrate
that the proposed method outperforms SOTA both quantitatively and
qualitatively.</div><div><a href='http://arxiv.org/abs/2402.02936v1'>2402.02936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16468v3")'>InstructIR: High-Quality Image Restoration Following Human Instructions</div>
<div id='2401.16468v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:53:33Z</div><div>Authors: Marcos V. Conde, Gregor Geigle, Radu Timofte</div><div style='padding-top: 10px; width: 80ex'>Image restoration is a fundamental problem that involves recovering a
high-quality clean image from its degraded observation. All-In-One image
restoration models can effectively restore images from various types and levels
of degradation using degradation-specific information as prompts to guide the
restoration model. In this work, we present the first approach that uses
human-written instructions to guide the image restoration model. Given natural
language prompts, our model can recover high-quality images from their degraded
counterparts, considering multiple degradation types. Our method, InstructIR,
achieves state-of-the-art results on several restoration tasks including image
denoising, deraining, deblurring, dehazing, and (low-light) image enhancement.
InstructIR improves +1dB over previous all-in-one restoration methods.
Moreover, our dataset and results represent a novel benchmark for new research
on text-guided image restoration and enhancement. Our code, datasets and models
are available at: https://github.com/mv-lab/InstructIR</div><div><a href='http://arxiv.org/abs/2401.16468v3'>2401.16468v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08643v1")'>Learned Image Compression with Text Quality Enhancement</div>
<div id='2402.08643v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:20:04Z</div><div>Authors: Chih-Yu Lai, Dung Tran, Kazuhito Koishida</div><div style='padding-top: 10px; width: 80ex'>Learned image compression has gained widespread popularity for their
efficiency in achieving ultra-low bit-rates. Yet, images containing substantial
textual content, particularly screen-content images (SCI), often suffers from
text distortion at such compressed levels. To address this, we propose to
minimize a novel text logit loss designed to quantify the disparity in text
between the original and reconstructed images, thereby improving the perceptual
quality of the reconstructed text. Through rigorous experimentation across
diverse datasets and employing state-of-the-art algorithms, our findings reveal
significant enhancements in the quality of reconstructed text upon integration
of the proposed loss function with appropriate weighting. Notably, we achieve a
Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and
-28.03% for Word Error Rate (WER) on average by applying the text logit loss
for two screenshot datasets. Additionally, we present quantitative metrics
tailored for evaluating text quality in image compression tasks. Our findings
underscore the efficacy and potential applicability of our proposed text logit
loss function across various text-aware image compression contexts.</div><div><a href='http://arxiv.org/abs/2402.08643v1'>2402.08643v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02944v1")'>Neural Image Compression with Text-guided Encoding for both Pixel-level
  and Perceptual Fidelity</div>
<div id='2403.02944v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:15:01Z</div><div>Authors: Hagyeong Lee, Minkyu Kim, Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, Jaeho Lee</div><div style='padding-top: 10px; width: 80ex'>Recent advances in text-guided image compression have shown great potential
to enhance the perceptual quality of reconstructed images. These methods,
however, tend to have significantly degraded pixel-wise fidelity, limiting
their practicality. To fill this gap, we develop a new text-guided image
compression algorithm that achieves both high perceptual and pixel-wise
fidelity. In particular, we propose a compression framework that leverages text
information mainly by text-adaptive encoding and training with joint image-text
loss. By doing so, we avoid decoding based on text-guided generative models --
known for high generative diversity -- and effectively utilize the semantic
information of text at a global level. Experimental results on various datasets
show that our method can achieve high pixel-level and perceptual quality, with
either human- or machine-generated captions. In particular, our method
outperforms all baselines in terms of LPIPS, with some room for even more
improvements when we use more carefully generated captions.</div><div><a href='http://arxiv.org/abs/2403.02944v1'>2403.02944v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17544v1")'>Adapting Learned Image Codecs to Screen Content via Adjustable
  Transformations</div>
<div id='2402.17544v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T14:34:14Z</div><div>Authors: H. Burak Dogaroglu, A. Burakhan Koyuncu, Atanas Boev, Elena Alshina, Eckehard Steinbach</div><div style='padding-top: 10px; width: 80ex'>As learned image codecs (LICs) become more prevalent, their low coding
efficiency for out-of-distribution data becomes a bottleneck for some
applications. To improve the performance of LICs for screen content (SC) images
without breaking backwards compatibility, we propose to introduce parameterized
and invertible linear transformations into the coding pipeline without changing
the underlying baseline codec's operation flow. We design two neural networks
to act as prefilters and postfilters in our setup to increase the coding
efficiency and help with the recovery from coding artifacts. Our end-to-end
trained solution achieves up to 10% bitrate savings on SC compression compared
to the baseline LICs while introducing only 1% extra parameters.</div><div><a href='http://arxiv.org/abs/2402.17544v1'>2402.17544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15235v1")'>CascadedGaze: Efficiency in Global Context Extraction for Image
  Restoration</div>
<div id='2401.15235v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T22:59:51Z</div><div>Authors: Amirhosein Ghasemabadi, Mohammad Salameh, Muhammad Kamran Janjua, Chunhua Zhou, Fengyu Sun, Di Niu</div><div style='padding-top: 10px; width: 80ex'>Image restoration tasks traditionally rely on convolutional neural networks.
However, given the local nature of the convolutional operator, they struggle to
capture global information. The promise of attention mechanisms in Transformers
is to circumvent this problem, but it comes at the cost of intensive
computational overhead. Many recent studies in image restoration have focused
on solving the challenge of balancing performance and computational cost via
Transformer variants. In this paper, we present CascadedGaze Network (CGNet),
an encoder-decoder architecture that employs Global Context Extractor (GCE), a
novel and efficient way to capture global information for image restoration.
The GCE module leverages small kernels across convolutional layers to learn
global dependencies, without requiring self-attention. Extensive experimental
results show that our approach outperforms a range of state-of-the-art methods
on denoising benchmark datasets including both real image denoising and
synthetic image denoising, as well as on image deblurring task, while being
more computationally efficient.</div><div><a href='http://arxiv.org/abs/2401.15235v1'>2401.15235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09302v1")'>StainFuser: Controlling Diffusion for Faster Neural Style Transfer in
  Multi-Gigapixel Histology Images</div>
<div id='2403.09302v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T11:49:43Z</div><div>Authors: Robert Jewsbury, Ruoyu Wang, Abhir Bhalerao, Nasir Rajpoot, Quoc Dang Vu</div><div style='padding-top: 10px; width: 80ex'>Stain normalization algorithms aim to transform the color and intensity
characteristics of a source multi-gigapixel histology image to match those of a
target image, mitigating inconsistencies in the appearance of stains used to
highlight cellular components in the images. We propose a new approach,
StainFuser, which treats this problem as a style transfer task using a novel
Conditional Latent Diffusion architecture, eliminating the need for handcrafted
color components. With this method, we curate SPI-2M the largest stain
normalization dataset to date of over 2 million histology images with neural
style transfer for high-quality transformations. Trained on this data,
StainFuser outperforms current state-of-the-art GAN and handcrafted methods in
terms of the quality of normalized images. Additionally, compared to existing
approaches, it improves the performance of nuclei instance segmentation and
classification models when used as a test time augmentation method on the
challenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel
Whole Slide Images (WSIs) and demonstrate improved performance in terms of
computational efficiency, image quality and consistency across tiles over
current methods.</div><div><a href='http://arxiv.org/abs/2403.09302v1'>2403.09302v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14602v1")'>ReNoise: Real Image Inversion Through Iterative Noising</div>
<div id='2403.14602v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:52:08Z</div><div>Authors: Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, Daniel Cohen-Or</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in text-guided diffusion models have unlocked powerful
image manipulation capabilities. However, applying these methods to real images
necessitates the inversion of the images into the domain of the pretrained
diffusion model. Achieving faithful inversion remains a challenge, particularly
for more recent models trained to generate images with a small number of
denoising steps. In this work, we introduce an inversion method with a high
quality-to-operation ratio, enhancing reconstruction accuracy without
increasing the number of operations. Building on reversing the diffusion
sampling process, our method employs an iterative renoising mechanism at each
inversion sampling step. This mechanism refines the approximation of a
predicted point along the forward diffusion trajectory, by iteratively applying
the pretrained diffusion model, and averaging these predictions. We evaluate
the performance of our ReNoise technique using various sampling algorithms and
models, including recent accelerated diffusion models. Through comprehensive
evaluations and comparisons, we show its effectiveness in terms of both
accuracy and speed. Furthermore, we confirm that our method preserves
editability by demonstrating text-driven image editing on real images.</div><div><a href='http://arxiv.org/abs/2403.14602v1'>2403.14602v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08682v1")'>IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality
  3D Generation</div>
<div id='2402.08682v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:59:51Z</div><div>Authors: Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos</div><div style='padding-top: 10px; width: 80ex'>Most text-to-3D generators build upon off-the-shelf text-to-image models
trained on billions of images. They use variants of Score Distillation Sampling
(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation
is to fine-tune the 2D generator to be multi-view aware, which can help
distillation or can be combined with reconstruction networks to output 3D
objects directly. In this paper, we further explore the design space of
text-to-3D models. We significantly improve multi-view generation by
considering video instead of image generators. Combined with a 3D
reconstruction algorithm which, by using Gaussian splatting, can optimize a
robust image-based loss, we directly produce high-quality 3D outputs from the
generated views. Our new method, IM-3D, reduces the number of evaluations of
the 2D generator network 10-100x, resulting in a much more efficient pipeline,
better quality, fewer geometric inconsistencies, and higher yield of usable 3D
assets.</div><div><a href='http://arxiv.org/abs/2402.08682v1'>2402.08682v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02972v1")'>Retrieval-Augmented Score Distillation for Text-to-3D Generation</div>
<div id='2402.02972v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:50:30Z</div><div>Authors: Junyoung Seo, Susung Hong, Wooseok Jang, Inès Hyeonsu Kim, Minseop Kwak, Doyup Lee, Seungryong Kim</div><div style='padding-top: 10px; width: 80ex'>Text-to-3D generation has achieved significant success by incorporating
powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to
the inconsistency of 3D geometry. Recently, since large-scale multi-view
datasets have been released, fine-tuning the diffusion model on the multi-view
datasets becomes a mainstream to solve the 3D inconsistency problem. However,
it has confronted with fundamental difficulties regarding the limited quality
and diversity of 3D data, compared with 2D data. To sidestep these trade-offs,
we explore a retrieval-augmented approach tailored for score distillation,
dubbed RetDream. We postulate that both expressiveness of 2D diffusion models
and geometric consistency of 3D assets can be fully leveraged by employing the
semantically relevant assets directly within the optimization process. To this
end, we introduce novel framework for retrieval-based quality enhancement in
text-to-3D generation. We leverage the retrieved asset to incorporate its
geometric prior in the variational objective and adapt the diffusion model's 2D
prior toward view consistency, achieving drastic improvements in both geometry
and fidelity of generated scenes. We conduct extensive experiments to
demonstrate that RetDream exhibits superior quality with increased geometric
consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.</div><div><a href='http://arxiv.org/abs/2402.02972v1'>2402.02972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09050v1")'>Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation
  with Deterministic Sampling Prior</div>
<div id='2401.09050v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T08:32:07Z</div><div>Authors: Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, Hanwang Zhang</div><div style='padding-top: 10px; width: 80ex'>Score distillation sampling (SDS) and its variants have greatly boosted the
development of text-to-3D generation, but are vulnerable to geometry collapse
and poor textures yet. To solve this issue, we first deeply analyze the SDS and
find that its distillation sampling process indeed corresponds to the
trajectory sampling of a stochastic differential equation (SDE): SDS samples
along an SDE trajectory to yield a less noisy sample which then serves as a
guidance to optimize a 3D model. However, the randomness in SDE sampling often
leads to a diverse and unpredictable sample which is not always less noisy, and
thus is not a consistently correct guidance, explaining the vulnerability of
SDS. Since for any SDE, there always exists an ordinary differential equation
(ODE) whose trajectory sampling can deterministically and consistently converge
to the desired target point as the SDE, we propose a novel and effective
"Consistent3D" method that explores the ODE deterministic sampling prior for
text-to-3D generation. Specifically, at each training iteration, given a
rendered image by a 3D model, we first estimate its desired 3D score function
by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling.
Next, we design a consistency distillation sampling loss which samples along
the ODE trajectory to generate two adjacent samples and uses the less noisy
sample to guide another more noisy one for distilling the deterministic prior
into the 3D model. Experimental results show the efficacy of our Consistent3D
in generating high-fidelity and diverse 3D objects and large-scale scenes, as
shown in Fig. 1. The codes are available at
https://github.com/sail-sg/Consistent3D.</div><div><a href='http://arxiv.org/abs/2401.09050v1'>2401.09050v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16936v1")'>Disentangled 3D Scene Generation with Layout Learning</div>
<div id='2402.16936v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:54:15Z</div><div>Authors: Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A. Efros, Aleksander Holynski</div><div style='padding-top: 10px; width: 80ex'>We introduce a method to generate 3D scenes that are disentangled into their
component objects. This disentanglement is unsupervised, relying only on the
knowledge of a large pretrained text-to-image model. Our key insight is that
objects can be discovered by finding parts of a 3D scene that, when rearranged
spatially, still produce valid configurations of the same scene. Concretely,
our method jointly optimizes multiple NeRFs from scratch - each representing
its own object - along with a set of layouts that composite these objects into
scenes. We then encourage these composited scenes to be in-distribution
according to the image generator. We show that despite its simplicity, our
approach successfully generates 3D scenes decomposed into individual objects,
enabling new capabilities in text-to-3D content creation. For results and an
interactive demo, see our project page at https://dave.ml/layoutlearning/</div><div><a href='http://arxiv.org/abs/2402.16936v1'>2402.16936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05335v1")'>InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes</div>
<div id='2401.05335v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T18:59:53Z</div><div>Authors: Mohamad Shahbazi, Liesbeth Claessens, Michael Niemeyer, Edo Collins, Alessio Tonioni, Luc Van Gool, Federico Tombari</div><div style='padding-top: 10px; width: 80ex'>We introduce InseRF, a novel method for generative object insertion in the
NeRF reconstructions of 3D scenes. Based on a user-provided textual description
and a 2D bounding box in a reference viewpoint, InseRF generates new objects in
3D scenes. Recently, methods for 3D scene editing have been profoundly
transformed, owing to the use of strong priors of text-to-image diffusion
models in 3D generative modeling. Existing methods are mostly effective in
editing 3D scenes via style and appearance changes or removing existing
objects. Generating new objects, however, remains a challenge for such methods,
which we address in this study. Specifically, we propose grounding the 3D
object insertion to a 2D object insertion in a reference view of the scene. The
2D edit is then lifted to 3D using a single-view object reconstruction method.
The reconstructed object is then inserted into the scene, guided by the priors
of monocular depth estimation methods. We evaluate our method on various 3D
scenes and provide an in-depth analysis of the proposed components. Our
experiments with generative insertion of objects in several 3D scenes indicate
the effectiveness of our method compared to the existing methods. InseRF is
capable of controllable and 3D-consistent object insertion without requiring
explicit 3D information as input. Please visit our project page at
https://mohamad-shahbazi.github.io/inserf.</div><div><a href='http://arxiv.org/abs/2401.05335v1'>2401.05335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03286v1")'>Training-Free Consistent Text-to-Image Generation</div>
<div id='2402.03286v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:42:34Z</div><div>Authors: Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon</div><div style='padding-top: 10px; width: 80ex'>Text-to-image models offer a new level of creative flexibility by allowing
users to guide the image generation process through natural language. However,
using these models to consistently portray the same subject across diverse
prompts remains challenging. Existing approaches fine-tune the model to teach
it new words that describe specific user-provided subjects or add image
conditioning to the model. These methods require lengthy per-subject
optimization or large-scale pre-training. Moreover, they struggle to align
generated images with text prompts and face difficulties in portraying multiple
subjects. Here, we present ConsiStory, a training-free approach that enables
consistent subject generation by sharing the internal activations of the
pretrained model. We introduce a subject-driven shared attention block and
correspondence-based feature injection to promote subject consistency between
images. Additionally, we develop strategies to encourage layout diversity while
maintaining subject consistency. We compare ConsiStory to a range of baselines,
and demonstrate state-of-the-art performance on subject consistency and text
alignment, without requiring a single optimization step. Finally, ConsiStory
can naturally extend to multi-subject scenarios, and even enable training-free
personalization for common objects.</div><div><a href='http://arxiv.org/abs/2402.03286v1'>2402.03286v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05239v1")'>Towards Effective Usage of Human-Centric Priors in Diffusion Models for
  Text-based Human Image Generation</div>
<div id='2403.05239v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T11:59:32Z</div><div>Authors: Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen, Weihua Chen, Hao Li, Cheng Zhang, Yang Song</div><div style='padding-top: 10px; width: 80ex'>Vanilla text-to-image diffusion models struggle with generating accurate
human images, commonly resulting in imperfect anatomies such as unnatural
postures or disproportionate limbs.Existing methods address this issue mostly
by fine-tuning the model with extra images or adding additional controls --
human-centric priors such as pose or depth maps -- during the image generation
phase. This paper explores the integration of these human-centric priors
directly into the model fine-tuning stage, essentially eliminating the need for
extra conditions at the inference stage. We realize this idea by proposing a
human-centric alignment loss to strengthen human-related information from the
textual prompts within the cross-attention maps. To ensure semantic detail
richness and human structural accuracy during fine-tuning, we introduce
scale-aware and step-wise constraints within the diffusion process, according
to an in-depth analysis of the cross-attention layer. Extensive experiments
show that our method largely improves over state-of-the-art text-to-image
models to synthesize high-quality human images based on user-written prompts.
Project page: \url{https://hcplayercvpr2024.github.io}.</div><div><a href='http://arxiv.org/abs/2403.05239v1'>2403.05239v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16305v1")'>Referee Can Play: An Alternative Approach to Conditional Generation via
  Model Inversion</div>
<div id='2402.16305v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T05:08:40Z</div><div>Authors: Xuantong Liu, Tianyang Hu, Wenjia Wang, Kenji Kawaguchi, Yuan Yao</div><div style='padding-top: 10px; width: 80ex'>As a dominant force in text-to-image generation tasks, Diffusion
Probabilistic Models (DPMs) face a critical challenge in controllability,
struggling to adhere strictly to complex, multi-faceted instructions. In this
work, we aim to address this alignment challenge for conditional generation
tasks. First, we provide an alternative view of state-of-the-art DPMs as a way
of inverting advanced Vision-Language Models (VLMs). With this formulation, we
naturally propose a training-free approach that bypasses the conventional
sampling process associated with DPMs. By directly optimizing images with the
supervision of discriminative VLMs, the proposed method can potentially achieve
a better text-image alignment. As proof of concept, we demonstrate the pipeline
with the pre-trained BLIP-2 model and identify several key designs for improved
image generation. To further enhance the image fidelity, a Score Distillation
Sampling module of Stable Diffusion is incorporated. By carefully balancing the
two components during optimization, our method can produce high-quality images
with near state-of-the-art performance on T2I-Compbench.</div><div><a href='http://arxiv.org/abs/2402.16305v1'>2402.16305v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03290v1")'>InstanceDiffusion: Instance-level Control for Image Generation</div>
<div id='2402.03290v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T18:49:17Z</div><div>Authors: Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</div><div style='padding-top: 10px; width: 80ex'>Text-to-image diffusion models produce high quality images but do not offer
control over individual instances in the image. We introduce InstanceDiffusion
that adds precise instance-level control to text-to-image diffusion models.
InstanceDiffusion supports free-form language conditions per instance and
allows flexible ways to specify instance locations such as simple single
points, scribbles, bounding boxes or intricate instance segmentation masks, and
combinations thereof. We propose three major changes to text-to-image models
that enable precise instance-level control. Our UniFusion block enables
instance-level conditions for text-to-image models, the ScaleU block improves
image fidelity, and our Multi-instance Sampler improves generations for
multiple instances. InstanceDiffusion significantly surpasses specialized
state-of-the-art models for each location condition. Notably, on the COCO
dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$
for box inputs, and 25.4% IoU for mask inputs.</div><div><a href='http://arxiv.org/abs/2402.03290v1'>2402.03290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13929v3")'>SDXL-Lightning: Progressive Adversarial Diffusion Distillation</div>
<div id='2402.13929v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:51:05Z</div><div>Authors: Shanchuan Lin, Anran Wang, Xiao Yang</div><div style='padding-top: 10px; width: 80ex'>We propose a diffusion distillation method that achieves new state-of-the-art
in one-step/few-step 1024px text-to-image generation based on SDXL. Our method
combines progressive and adversarial distillation to achieve a balance between
quality and mode coverage. In this paper, we discuss the theoretical analysis,
discriminator design, model formulation, and training techniques. We
open-source our distilled SDXL-Lightning models both as LoRA and full UNet
weights.</div><div><a href='http://arxiv.org/abs/2402.13929v3'>2402.13929v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15429v1")'>ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion
  Models against Stochastic Perturbation</div>
<div id='2402.15429v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:48:56Z</div><div>Authors: Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir, Paul Jennings, Xingyu Zhao</div><div style='padding-top: 10px; width: 80ex'>Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in
generating high-quality images based on simple text descriptions. However, as
is common with many Deep Learning (DL) models, DMs are subject to a lack of
robustness. While there are attempts to evaluate the robustness of T2I DMs as a
binary or worst-case problem, they cannot answer how robust in general the
model is whenever an adversarial example (AE) can be found. In this study, we
first introduce a probabilistic notion of T2I DMs' robustness; and then
establish an efficient framework, ProTIP, to evaluate it with statistical
guarantees. The main challenges stem from: i) the high computational cost of
the generation process; and ii) determining if a perturbed input is an AE
involves comparing two output distributions, which is fundamentally harder
compared to other DL tasks like classification where an AE is identified upon
misprediction of labels. To tackle the challenges, we employ sequential
analysis with efficacy and futility early stopping rules in the statistical
testing for identifying AEs, and adaptive concentration inequalities to
dynamically determine the "just-right" number of stochastic perturbations
whenever the verification target is met. Empirical experiments validate the
effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we
demonstrate an application of ProTIP to rank commonly used defence methods.</div><div><a href='http://arxiv.org/abs/2402.15429v1'>2402.15429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11940v2")'>AICAttack: Adversarial Image Captioning Attack with Attention-Based
  Optimization</div>
<div id='2402.11940v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T08:27:23Z</div><div>Authors: Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu, Wei Liu</div><div style='padding-top: 10px; width: 80ex'>Recent advances in deep learning research have shown remarkable achievements
across many tasks in computer vision (CV) and natural language processing
(NLP). At the intersection of CV and NLP is the problem of image captioning,
where the related models' robustness against adversarial attacks has not been
well studied. In this paper, we present a novel adversarial attack strategy,
which we call AICAttack (Attention-based Image Captioning Attack), designed to
attack image captioning models through subtle perturbations on images.
Operating within a black-box attack scenario, our algorithm requires no access
to the target model's architecture, parameters, or gradient information. We
introduce an attention-based candidate selection mechanism that identifies the
optimal pixels to attack, followed by Differential Evolution (DE) for
perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through
extensive experiments on benchmark datasets with multiple victim models. The
experimental results demonstrate that our method surpasses current leading-edge
techniques by effectively distributing the alignment and semantics of words in
the output.</div><div><a href='http://arxiv.org/abs/2402.11940v2'>2402.11940v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01369v1")'>Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with
  Multi-Modal Priors</div>
<div id='2402.01369v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:39:49Z</div><div>Authors: Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian Yu</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have been widely deployed in various image generation tasks,
demonstrating an extraordinary connection between image and text modalities.
However, they face challenges of being maliciously exploited to generate
harmful or sensitive images by appending a specific suffix to the original
prompt. Existing works mainly focus on using single-modal information to
conduct attacks, which fails to utilize multi-modal features and results in
less than satisfactory performance. Integrating multi-modal priors (MMP), i.e.
both text and image features, we propose a targeted attack method named
MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a
target object into the image content while simultaneously removing the original
object. The MMP-Attack shows a notable advantage over existing works with
superior universality and transferability, which can effectively attack
commercial text-to-image (T2I) models such as DALL-E 3. To the best of our
knowledge, this marks the first successful attempt of transfer-based attack to
commercial T2I models. Our code is publicly available at
\url{https://github.com/ydc123/MMP-Attack}.</div><div><a href='http://arxiv.org/abs/2402.01369v1'>2402.01369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14073v1")'>Improving Language Understanding from Screenshots</div>
<div id='2402.14073v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:01:03Z</div><div>Authors: Tianyu Gao, Zirui Wang, Adithya Bhaskar, Danqi Chen</div><div style='padding-top: 10px; width: 80ex'>An emerging family of language models (LMs), capable of processing both text
and images within a single visual view, has the promise to unlock complex tasks
such as chart understanding and UI navigation. We refer to these models as
screenshot language models. Despite their appeal, existing screenshot LMs
substantially lag behind text-only models on language understanding tasks. To
close this gap, we adopt a simplified setting where the model inputs are
plain-text-rendered screenshots, and we focus on improving the text ability of
screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective,
which masks and recovers both image patches of screenshots and text within
screenshots. We also conduct extensive ablation studies on masking rates and
patch sizes, as well as designs for improving training stability. Our
pre-trained model, while solely taking visual inputs, achieves comparable
performance with BERT on 6 out of 8 GLUE tasks (within 2%) and improves up to
8% over prior work. Additionally, we extend PTP to train autoregressive
screenshot LMs and demonstrate its effectiveness--our models can significantly
reduce perplexity by utilizing the screenshot context. Together, we hope our
findings can inspire future research on developing powerful screenshot LMs and
extending their reach to broader applications.</div><div><a href='http://arxiv.org/abs/2402.14073v1'>2402.14073v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14613v1")'>DreamReward: Text-to-3D Generation with Human Preference</div>
<div id='2403.14613v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T17:58:04Z</div><div>Authors: Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>3D content creation from text prompts has shown remarkable success recently.
However, current text-to-3D methods often generate 3D results that do not align
well with human preferences. In this paper, we present a comprehensive
framework, coined DreamReward, to learn and improve text-to-3D models from
human preference feedback. To begin with, we collect 25k expert comparisons
based on a systematic annotation pipeline including rating and ranking. Then,
we build Reward3D -- the first general-purpose text-to-3D human preference
reward model to effectively encode human preferences. Building upon the 3D
reward model, we finally perform theoretical analysis and present the Reward3D
Feedback Learning (DreamFL), a direct tuning algorithm to optimize the
multi-view diffusion models with a redefined scorer. Grounded by theoretical
proof and extensive experiment comparisons, our DreamReward successfully
generates high-fidelity and 3D consistent results with significant boosts in
prompt alignment with human intention. Our results demonstrate the great
potential for learning from human feedback to improve text-to-3D models.</div><div><a href='http://arxiv.org/abs/2403.14613v1'>2403.14613v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07605v1")'>Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in
  Text-To-Image Generation</div>
<div id='2403.07605v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T12:44:34Z</div><div>Authors: Michael Ogezi, Ning Shi</div><div style='padding-top: 10px; width: 80ex'>In text-to-image generation, using negative prompts, which describe
undesirable image characteristics, can significantly boost image quality.
However, producing good negative prompts is manual and tedious. To address
this, we propose NegOpt, a novel method for optimizing negative prompt
generation toward enhanced image generation, using supervised fine-tuning and
reinforcement learning. Our combined approach results in a substantial increase
of 25% in Inception Score compared to other approaches and surpasses
ground-truth negative prompts from the test set. Furthermore, with NegOpt we
can preferentially optimize the metrics most important to us. Finally, we
construct Negative Prompts DB, a dataset of negative prompts.</div><div><a href='http://arxiv.org/abs/2403.07605v1'>2403.07605v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06952v1")'>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with
  Auto-Generated Data</div>
<div id='2403.06952v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:35:33Z</div><div>Authors: Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</div><div style='padding-top: 10px; width: 80ex'>Recent text-to-image (T2I) generation models have demonstrated impressive
capabilities in creating images from text descriptions. However, these T2I
generation models often fall short of generating images that precisely match
the details of the text inputs, such as incorrect spatial relationship or
missing objects. In this paper, we introduce SELMA: Skill-Specific Expert
Learning and Merging with Auto-Generated Data, a novel paradigm to improve the
faithfulness of T2I models by fine-tuning models on automatically generated,
multi-skill image-text datasets, with skill-specific expert learning and
merging. First, SELMA leverages an LLM's in-context learning capability to
generate multiple datasets of text prompts that can teach different skills, and
then generates the images with a T2I model based on the prompts. Next, SELMA
adapts the T2I model to the new skills by learning multiple single-skill LoRA
(low-rank adaptation) experts followed by expert merging. Our independent
expert fine-tuning specializes multiple models for different skills, and expert
merging helps build a joint multi-skill T2I model that can generate faithful
images given diverse text prompts, while mitigating the knowledge conflict from
different datasets. We empirically demonstrate that SELMA significantly
improves the semantic alignment and text faithfulness of state-of-the-art T2I
diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human
preference metrics (PickScore, ImageReward, and HPS), as well as human
evaluation. Moreover, fine-tuning with image-text pairs auto-collected via
SELMA shows comparable performance to fine-tuning with ground truth data.
Lastly, we show that fine-tuning with images from a weaker T2I model can help
improve the generation quality of a stronger T2I model, suggesting promising
weak-to-strong generalization in T2I models.</div><div><a href='http://arxiv.org/abs/2403.06952v1'>2403.06952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15120v1")'>Fine-tuning CLIP Text Encoders with Two-step Paraphrasing</div>
<div id='2402.15120v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:11:50Z</div><div>Authors: Hyunjae Kim, Seunghyun Yoon, Trung Bui, Handong Zhao, Quan Tran, Franck Dernoncourt, Jaewoo Kang</div><div style='padding-top: 10px; width: 80ex'>Contrastive language-image pre-training (CLIP) models have demonstrated
considerable success across various vision-language tasks, such as
text-to-image retrieval, where the model is required to effectively process
natural language input to produce an accurate visual output. However, current
models still face limitations in dealing with linguistic variations in input
queries, such as paraphrases, making it challenging to handle a broad range of
user queries in real-world applications. In this study, we introduce a
straightforward fine-tuning approach to enhance the representations of CLIP
models for paraphrases. Our approach involves a two-step paraphrase generation
process, where we automatically create two categories of paraphrases from
web-scale image captions by leveraging large language models. Subsequently, we
fine-tune the CLIP text encoder using these generated paraphrases while
freezing the image encoder. Our resulting model, which we call ParaCLIP,
exhibits significant improvements over baseline CLIP models across various
tasks, including paraphrased retrieval (with rank similarity scores improved by
up to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven
semantic textual similarity tasks.</div><div><a href='http://arxiv.org/abs/2402.15120v1'>2402.15120v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06105v1")'>PALP: Prompt Aligned Personalization of Text-to-Image Models</div>
<div id='2401.06105v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:35:33Z</div><div>Authors: Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, Ariel Shamir</div><div style='padding-top: 10px; width: 80ex'>Content creators often aim to create personalized images using personal
subjects that go beyond the capabilities of conventional text-to-image models.
Additionally, they may want the resulting image to encompass a specific
location, style, ambiance, and more. Existing personalization methods may
compromise personalization ability or the alignment to complex textual prompts.
This trade-off can impede the fulfillment of user prompts and subject fidelity.
We propose a new approach focusing on personalization methods for a
\emph{single} prompt to address this issue. We term our approach prompt-aligned
personalization. While this may seem restrictive, our method excels in
improving text alignment, enabling the creation of images with complex and
intricate prompts, which may pose a challenge for current techniques. In
particular, our method keeps the personalized model aligned with a target
prompt using an additional score distillation sampling term. We demonstrate the
versatility of our method in multi- and single-shot settings and further show
that it can compose multiple subjects or use inspiration from reference images,
such as artworks. We compare our approach quantitatively and qualitatively with
existing baselines and state-of-the-art techniques.</div><div><a href='http://arxiv.org/abs/2401.06105v1'>2401.06105v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05578v1")'>Chaining text-to-image and large language model: A novel approach for
  generating personalized e-commerce banners</div>
<div id='2403.05578v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:56:04Z</div><div>Authors: Shanu Vashishtha, Abhinav Prakash, Lalitesh Morishetti, Kaushiki Nag, Yokila Arora, Sushant Kumar, Kannan Achan</div><div style='padding-top: 10px; width: 80ex'>Text-to-image models such as stable diffusion have opened a plethora of
opportunities for generating art. Recent literature has surveyed the use of
text-to-image models for enhancing the work of many creative artists. Many
e-commerce platforms employ a manual process to generate the banners, which is
time-consuming and has limitations of scalability. In this work, we demonstrate
the use of text-to-image models for generating personalized web banners with
dynamic content for online shoppers based on their interactions. The novelty in
this approach lies in converting users' interaction data to meaningful prompts
without human intervention. To this end, we utilize a large language model
(LLM) to systematically extract a tuple of attributes from item
meta-information. The attributes are then passed to a text-to-image model via
prompt engineering to generate images for the banner. Our results show that the
proposed approach can create high-quality personalized banners for users.</div><div><a href='http://arxiv.org/abs/2403.05578v1'>2403.05578v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05941v1")'>Character-based Outfit Generation with Vision-augmented Style Extraction
  via LLMs</div>
<div id='2402.05941v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:11:31Z</div><div>Authors: Najmeh Forouzandehmehr, Yijie Cao, Nikhil Thakurdesai, Ramin Giahi, Luyi Ma, Nima Farrokhsiar, Jianpeng Xu, Evren Korpeoglu, Kannan Achan</div><div style='padding-top: 10px; width: 80ex'>The outfit generation problem involves recommending a complete outfit to a
user based on their interests. Existing approaches focus on recommending items
based on anchor items or specific query styles but do not consider customer
interests in famous characters from movie, social media, etc. In this paper, we
define a new Character-based Outfit Generation (COG) problem, designed to
accurately interpret character information and generate complete outfit sets
according to customer specifications such as age and gender. To tackle this
problem, we propose a novel framework LVA-COG that leverages Large Language
Models (LLMs) to extract insights from customer interests (e.g., character
information) and employ prompt engineering techniques for accurate
understanding of customer preferences. Additionally, we incorporate
text-to-image models to enhance the visual understanding and generation
(factual or counterfactual) of cohesive outfits. Our framework integrates LLMs
with text-to-image models and improves the customer's approach to fashion by
generating personalized recommendations. With experiments and case studies, we
demonstrate the effectiveness of our solution from multiple dimensions.</div><div><a href='http://arxiv.org/abs/2402.05941v1'>2402.05941v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01978v1")'>Tailor: Size Recommendations for High-End Fashion Marketplaces</div>
<div id='2401.01978v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T20:58:03Z</div><div>Authors: Alexandre Candeias, Ivo Silva, Vitor Sousa, José Marcelino</div><div style='padding-top: 10px; width: 80ex'>In the ever-changing and dynamic realm of high-end fashion marketplaces,
providing accurate and personalized size recommendations has become a critical
aspect. Meeting customer expectations in this regard is not only crucial for
ensuring their satisfaction but also plays a pivotal role in driving customer
retention, which is a key metric for the success of any fashion retailer. We
propose a novel sequence classification approach to address this problem,
integrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our
approach comprises two distinct models: one employs LSTMs to encode the user
signals, while the other leverages an Attention mechanism. Our best model
outperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions
we increase the user coverage by 24.5% when compared with only using Orders.
Moreover, we evaluate the models' usability in real-time recommendation
scenarios by conducting experiments to measure their latency performance.</div><div><a href='http://arxiv.org/abs/2401.01978v1'>2401.01978v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01877v1")'>Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models</div>
<div id='2402.01877v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:05:45Z</div><div>Authors: Justin Blalock, David Munechika, Harsha Karanth, Alec Helbling, Pratham Mehta, Seongmin Lee, Duen Horng Chau</div><div style='padding-top: 10px; width: 80ex'>The growing digital landscape of fashion e-commerce calls for interactive and
user-friendly interfaces for virtually trying on clothes. Traditional try-on
methods grapple with challenges in adapting to diverse backgrounds, poses, and
subjects. While newer methods, utilizing the recent advances of diffusion
models, have achieved higher-quality image generation, the human-centered
dimensions of mobile interface delivery and privacy concerns remain largely
unexplored. We present Mobile Fitting Room, the first on-device diffusion-based
virtual try-on system. To address multiple inter-related technical challenges
such as high-quality garment placement and model compression for mobile
devices, we present a novel technical pipeline and an interface design that
enables privacy preservation and user customization. A usage scenario
highlights how our tool can provide a seamless, interactive virtual try-on
experience for customers and provide a valuable service for fashion e-commerce
businesses.</div><div><a href='http://arxiv.org/abs/2402.01877v1'>2402.01877v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14301v1")'>GenSERP: Large Language Models for Whole Page Presentation</div>
<div id='2402.14301v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T05:41:24Z</div><div>Authors: Zhenning Zhang, Yunan Zhang, Suyu Ge, Guangwei Weng, Mridu Narang, Xia Song, Saurabh Tiwary</div><div style='padding-top: 10px; width: 80ex'>The advent of large language models (LLMs) brings an opportunity to minimize
the effort in search engine result page (SERP) organization. In this paper, we
propose GenSERP, a framework that leverages LLMs with vision in a few-shot
setting to dynamically organize intermediate search results, including
generated chat answers, website snippets, multimedia data, knowledge panels
into a coherent SERP layout based on a user's query. Our approach has three
main stages: (1) An information gathering phase where the LLM continuously
orchestrates API tools to retrieve different types of items, and proposes
candidate layouts based on the retrieved items, until it's confident enough to
generate the final result. (2) An answer generation phase where the LLM
populates the layouts with the retrieved content. In this phase, the LLM
adaptively optimize the ranking of items and UX configurations of the SERP.
Consequently, it assigns a location on the page to each item, along with the UX
display details. (3) A scoring phase where an LLM with vision scores all the
generated SERPs based on how likely it can satisfy the user. It then send the
one with highest score to rendering. GenSERP features two generation paradigms.
First, coarse-to-fine, which allow it to approach optimal layout in a more
manageable way, (2) beam search, which give it a better chance to hit the
optimal solution compared to greedy decoding. Offline experimental results on
real-world data demonstrate how LLMs can contextually organize heterogeneous
search results on-the-fly and provide a promising user experience.</div><div><a href='http://arxiv.org/abs/2402.14301v1'>2402.14301v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09625v1")'>Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation</div>
<div id='2403.09625v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:57:04Z</div><div>Authors: Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, Yueqi Duan</div><div style='padding-top: 10px; width: 80ex'>Recent years have witnessed the strong power of 3D generation models, which
offer a new level of creative flexibility by allowing users to guide the 3D
content generation process through a single image or natural language. However,
it remains challenging for existing 3D generation methods to create
subject-driven 3D content across diverse prompts. In this paper, we introduce a
novel 3D customization method, dubbed Make-Your-3D that can personalize
high-fidelity and consistent 3D content from only a single image of a subject
with text description within 5 minutes. Our key insight is to harmonize the
distributions of a multi-view diffusion model and an identity-specific 2D
generative model, aligning them with the distribution of the desired 3D
subject. Specifically, we design a co-evolution framework to reduce the
variance of distributions, where each model undergoes a process of learning
from the other through identity-aware optimization and subject-prior
optimization, respectively. Extensive experiments demonstrate that our method
can produce high-quality, consistent, and subject-specific 3D content with
text-driven modifications that are unseen in subject image.</div><div><a href='http://arxiv.org/abs/2403.09625v1'>2403.09625v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01248v1")'>SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code</div>
<div id='2403.01248v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T16:16:26Z</div><div>Authors: Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi</div><div style='padding-top: 10px; width: 80ex'>This paper introduces SceneCraft, a Large Language Model (LLM) Agent
converting text descriptions into Blender-executable Python scripts which
render complex scenes with up to a hundred 3D assets. This process requires
complex spatial planning and arrangement. We tackle these challenges through a
combination of advanced abstraction, strategic planning, and library learning.
SceneCraft first models a scene graph as a blueprint, detailing the spatial
relationships among assets in the scene. SceneCraft then writes Python scripts
based on this graph, translating relationships into numerical constraints for
asset layout. Next, SceneCraft leverages the perceptual strengths of
vision-language foundation models like GPT-V to analyze rendered images and
iteratively refine the scene. On top of this process, SceneCraft features a
library learning mechanism that compiles common script functions into a
reusable library, facilitating continuous self-improvement without expensive
LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses
existing LLM-based agents in rendering complex scenes, as shown by its
adherence to constraints and favorable human assessments. We also showcase the
broader application potential of SceneCraft by reconstructing detailed 3D
scenes from the Sintel movie and guiding a video generative model with
generated scenes as intermediary control signal.</div><div><a href='http://arxiv.org/abs/2403.01248v1'>2403.01248v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14379v1")'>UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation
  and Diffusion Models</div>
<div id='2401.14379v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:30:46Z</div><div>Authors: Timo Kapsalis</div><div style='padding-top: 10px; width: 80ex'>In contemporary design practices, the integration of computer vision and
generative artificial intelligence (genAI) represents a transformative shift
towards more interactive and inclusive processes. These technologies offer new
dimensions of image analysis and generation, which are particularly relevant in
the context of urban landscape reconstruction. This paper presents a novel
workflow encapsulated within a prototype application, designed to leverage the
synergies between advanced image segmentation and diffusion models for a
comprehensive approach to urban design. Our methodology encompasses the
OneFormer model for detailed image segmentation and the Stable Diffusion XL
(SDXL) diffusion model, implemented through ControlNet, for generating images
from textual descriptions. Validation results indicated a high degree of
performance by the prototype application, showcasing significant accuracy in
both object detection and text-to-image generation. This was evidenced by
superior Intersection over Union (IoU) and CLIP scores across iterative
evaluations for various categories of urban landscape features. Preliminary
testing included utilising UrbanGenAI as an educational tool enhancing the
learning experience in design pedagogy, and as a participatory instrument
facilitating community-driven urban planning. Early results suggested that
UrbanGenAI not only advances the technical frontiers of urban landscape
reconstruction but also provides significant pedagogical and participatory
planning benefits. The ongoing development of UrbanGenAI aims to further
validate its effectiveness across broader contexts and integrate additional
features such as real-time feedback mechanisms and 3D modelling capabilities.
Keywords: generative AI; panoptic image segmentation; diffusion models; urban
landscape design; design pedagogy; co-design</div><div><a href='http://arxiv.org/abs/2401.14379v1'>2401.14379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13251v1")'>FlashTex: Fast Relightable Mesh Texturing with LightControlNet</div>
<div id='2402.13251v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:59:00Z</div><div>Authors: Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala</div><div style='padding-top: 10px; width: 80ex'>Manually creating textures for 3D meshes is time-consuming, even for expert
visual content creators. We propose a fast approach for automatically texturing
an input 3D mesh based on a user-provided text prompt. Importantly, our
approach disentangles lighting from surface material/reflectance in the
resulting texture so that the mesh can be properly relit and rendered in any
lighting environment. We introduce LightControlNet, a new text-to-image model
based on the ControlNet architecture, which allows the specification of the
desired lighting as a conditioning image to the model. Our text-to-texture
pipeline then constructs the texture in two stages. The first stage produces a
sparse set of visually consistent reference views of the mesh using
LightControlNet. The second stage applies a texture optimization based on Score
Distillation Sampling (SDS) that works with LightControlNet to increase the
texture quality while disentangling surface material from lighting. Our
pipeline is significantly faster than previous text-to-texture methods, while
producing high-quality and relightable textures.</div><div><a href='http://arxiv.org/abs/2402.13251v1'>2402.13251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15385v1")'>LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis</div>
<div id='2403.15385v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:59:37Z</div><div>Authors: Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng</div><div style='padding-top: 10px; width: 80ex'>Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.</div><div><a href='http://arxiv.org/abs/2403.15385v1'>2403.15385v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05034v1")'>CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction
  Model</div>
<div id='2403.05034v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T04:25:29Z</div><div>Authors: Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>Feed-forward 3D generative models like the Large Reconstruction Model (LRM)
have demonstrated exceptional generation speed. However, the transformer-based
methods do not leverage the geometric priors of the triplane component in their
architecture, often leading to sub-optimal quality given the limited size of 3D
data and slow training. In this work, we present the Convolutional
Reconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D
generative model. Recognizing the limitations posed by sparse 3D data, we
highlight the necessity of integrating geometric priors into network design.
CRM builds on the key observation that the visualization of triplane exhibits
spatial correspondence of six orthographic images. First, it generates six
orthographic view images from a single input image, then feeds these images
into a convolutional U-Net, leveraging its strong pixel-level alignment
capabilities and significant bandwidth to create a high-resolution triplane.
CRM further employs Flexicubes as geometric representation, facilitating direct
end-to-end optimization on textured meshes. Overall, our model delivers a
high-fidelity textured mesh from an image in just 10 seconds, without any
test-time optimization.</div><div><a href='http://arxiv.org/abs/2403.05034v1'>2403.05034v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02411v1")'>What You See is What You GAN: Rendering Every Pixel for High-Fidelity
  Geometry in 3D GANs</div>
<div id='2401.02411v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T18:50:38Z</div><div>Authors: Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano</div><div style='padding-top: 10px; width: 80ex'>3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.</div><div><a href='http://arxiv.org/abs/2401.02411v1'>2401.02411v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03445v2")'>Denoising Diffusion via Image-Based Rendering</div>
<div id='2402.03445v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T19:00:45Z</div><div>Authors: Titas Anciukevičius, Fabian Manhardt, Federico Tombari, Paul Henderson</div><div style='padding-top: 10px; width: 80ex'>Generating 3D scenes is a challenging open problem, which requires
synthesizing plausible content that is fully consistent in 3D space. While
recent methods such as neural radiance fields excel at view synthesis and 3D
reconstruction, they cannot synthesize plausible details in unobserved regions
since they lack a generative capability. Conversely, existing generative
methods are typically not capable of reconstructing detailed, large-scale
scenes in the wild, as they use limited-capacity 3D scene representations,
require aligned camera poses, or rely on additional regularizers. In this work,
we introduce the first diffusion model able to perform fast, detailed
reconstruction and generation of real-world 3D scenes. To achieve this, we make
three contributions. First, we introduce a new neural scene representation,
IB-planes, that can efficiently and accurately represent large 3D scenes,
dynamically allocating more capacity as needed to capture details visible in
each image. Second, we propose a denoising-diffusion framework to learn a prior
over this novel 3D scene representation, using only 2D images without the need
for any additional supervision signal such as masks or depths. This supports 3D
reconstruction and generation in a unified architecture. Third, we develop a
principled approach to avoid trivial 3D solutions when integrating the
image-based rendering with the diffusion model, by dropping out representations
of some images. We evaluate the model on several challenging datasets of real
and synthetic images, and demonstrate superior results on generation, novel
view synthesis and 3D reconstruction.</div><div><a href='http://arxiv.org/abs/2402.03445v2'>2402.03445v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02906v1")'>ViewFusion: Learning Composable Diffusion Models for Novel View
  Synthesis</div>
<div id='2402.02906v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T11:22:14Z</div><div>Authors: Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin</div><div style='padding-top: 10px; width: 80ex'>Deep learning is providing a wealth of new approaches to the old problem of
novel view synthesis, from Neural Radiance Field (NeRF) based approaches to
end-to-end style architectures. Each approach offers specific strengths but
also comes with specific limitations in their applicability. This work
introduces ViewFusion, a state-of-the-art end-to-end generative approach to
novel view synthesis with unparalleled flexibility. ViewFusion consists in
simultaneously applying a diffusion denoising step to any number of input views
of a scene, then combining the noise gradients obtained for each view with an
(inferred) pixel-weighting mask, ensuring that for each region of the target
scene only the most informative input views are taken into account. Our
approach resolves several limitations of previous approaches by (1) being
trainable and generalizing across multiple scenes and object classes, (2)
adaptively taking in a variable number of pose-free views at both train and
test time, (3) generating plausible views even in severely undetermined
conditions (thanks to its generative nature) -- all while generating views of
quality on par or even better than state-of-the-art methods. Limitations
include not generating a 3D embedding of the scene, resulting in a relatively
slow inference speed, and our method only being tested on the relatively small
dataset NMR. Code is available.</div><div><a href='http://arxiv.org/abs/2402.02906v1'>2402.02906v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03608v1")'>GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D
  Scene Understanding</div>
<div id='2403.03608v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T10:55:50Z</div><div>Authors: Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang</div><div style='padding-top: 10px; width: 80ex'>Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.</div><div><a href='http://arxiv.org/abs/2403.03608v1'>2403.03608v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07310v1")'>BioNeRF: Biologically Plausible Neural Radiance Fields for View
  Synthesis</div>
<div id='2402.07310v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T21:16:42Z</div><div>Authors: Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, João Paulo Papa</div><div style='padding-top: 10px; width: 80ex'>This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.</div><div><a href='http://arxiv.org/abs/2402.07310v1'>2402.07310v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03896v1")'>DART: Implicit Doppler Tomography for Radar Novel View Synthesis</div>
<div id='2403.03896v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:54:50Z</div><div>Authors: Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia, Zico Kolter, Anthony Rowe</div><div style='padding-top: 10px; width: 80ex'>Simulation is an invaluable tool for radio-frequency system designers that
enables rapid prototyping of various algorithms for imaging, target detection,
classification, and tracking. However, simulating realistic radar scans is a
challenging task that requires an accurate model of the scene, radio frequency
material properties, and a corresponding radar synthesis function. Rather than
specifying these models explicitly, we propose DART - Doppler Aided Radar
Tomography, a Neural Radiance Field-inspired method which uses radar-specific
physics to create a reflectance and transmittance-based rendering pipeline for
range-Doppler images. We then evaluate DART by constructing a custom data
collection platform and collecting a novel radar dataset together with accurate
position and instantaneous velocity measurements from lidar-based localization.
In comparison to state-of-the-art baselines, DART synthesizes superior radar
range-Doppler images from novel views across all datasets and additionally can
be used to generate high quality tomographic images.</div><div><a href='http://arxiv.org/abs/2403.03896v1'>2403.03896v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01165v1")'>Reinforcement Learning for SAR View Angle Inversion with Differentiable
  SAR Renderer</div>
<div id='2401.01165v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T11:47:58Z</div><div>Authors: Yanni Wang, Hecheng Jia, Shilei Fu, Huiping Lin, Feng Xu</div><div style='padding-top: 10px; width: 80ex'>The electromagnetic inverse problem has long been a research hotspot. This
study aims to reverse radar view angles in synthetic aperture radar (SAR)
images given a target model. Nonetheless, the scarcity of SAR data, combined
with the intricate background interference and imaging mechanisms, limit the
applications of existing learning-based approaches. To address these
challenges, we propose an interactive deep reinforcement learning (DRL)
framework, where an electromagnetic simulator named differentiable SAR render
(DSR) is embedded to facilitate the interaction between the agent and the
environment, simulating a human-like process of angle prediction. Specifically,
DSR generates SAR images at arbitrary view angles in real-time. And the
differences in sequential and semantic aspects between the view
angle-corresponding images are leveraged to construct the state space in DRL,
which effectively suppress the complex background interference, enhance the
sensitivity to temporal variations, and improve the capability to capture
fine-grained information. Additionally, in order to maintain the stability and
convergence of our method, a series of reward mechanisms, such as memory
difference, smoothing and boundary penalty, are utilized to form the final
reward function. Extensive experiments performed on both simulated and real
datasets demonstrate the effectiveness and robustness of our proposed method.
When utilized in the cross-domain area, the proposed method greatly mitigates
inconsistency between simulated and real domains, outperforming reference
methods significantly.</div><div><a href='http://arxiv.org/abs/2401.01165v1'>2401.01165v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04114v1")'>Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs</div>
<div id='2403.04114v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T00:00:02Z</div><div>Authors: Nikhil Mishra, Maximilian Sieb, Pieter Abbeel, Xi Chen</div><div style='padding-top: 10px; width: 80ex'>Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.</div><div><a href='http://arxiv.org/abs/2403.04114v1'>2403.04114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10075v2")'>A survey of synthetic data augmentation methods in computer vision</div>
<div id='2403.10075v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T07:34:08Z</div><div>Authors: Alhassan Mumuni, Fuseini Mumuni, Nana Kobina Gerrar</div><div style='padding-top: 10px; width: 80ex'>The standard approach to tackling computer vision problems is to train deep
convolutional neural network (CNN) models using large-scale image datasets
which are representative of the target task. However, in many scenarios, it is
often challenging to obtain sufficient image data for the target task. Data
augmentation is a way to mitigate this challenge. A common practice is to
explicitly transform existing images in desired ways so as to create the
required volume and variability of training data necessary to achieve good
generalization performance. In situations where data for the target domain is
not accessible, a viable workaround is to synthesize training data from
scratch--i.e., synthetic data augmentation. This paper presents an extensive
review of synthetic data augmentation techniques. It covers data synthesis
approaches based on realistic 3D graphics modeling, neural style transfer
(NST), differential neural rendering, and generative artificial intelligence
(AI) techniques such as generative adversarial networks (GANs) and variational
autoencoders (VAEs). For each of these classes of methods, we focus on the
important data generation and augmentation techniques, general scope of
application and specific use-cases, as well as existing limitations and
possible workarounds. Additionally, we provide a summary of common synthetic
datasets for training computer vision models, highlighting the main features,
application domains and supported tasks. Finally, we discuss the effectiveness
of synthetic data augmentation methods. Since this is the first paper to
explore synthetic data augmentation methods in great detail, we are hoping to
equip readers with the necessary background information and in-depth knowledge
of existing methods and their attendant issues.</div><div><a href='http://arxiv.org/abs/2403.10075v2'>2403.10075v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00196v1")'>Learning to Find Missing Video Frames with Synthetic Data Augmentation:
  A General Framework and Application in Generating Thermal Images Using RGB
  Cameras</div>
<div id='2403.00196v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T23:52:15Z</div><div>Authors: Mathias Viborg Andersen, Ross Greer, Andreas Møgelmose, Mohan Trivedi</div><div style='padding-top: 10px; width: 80ex'>Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on
accurate driver perception within the vehicle cabin, often leveraging a
combination of sensing modalities. However, these modalities operate at varying
rates, posing challenges for real-time, comprehensive driver state monitoring.
This paper addresses the issue of missing data due to sensor frame rate
mismatches, introducing a generative model approach to create synthetic yet
realistic thermal imagery. We propose using conditional generative adversarial
networks (cGANs), specifically comparing the pix2pix and CycleGAN
architectures. Experimental results demonstrate that pix2pix outperforms
CycleGAN, and utilizing multi-view input styles, especially stacked views,
enhances the accuracy of thermal image generation. Moreover, the study
evaluates the model's generalizability across different subjects, revealing the
importance of individualized training for optimal performance. The findings
suggest the potential of generative models in addressing missing frames,
advancing driver state monitoring for intelligent vehicles, and underscoring
the need for continued research in model generalization and customization.</div><div><a href='http://arxiv.org/abs/2403.00196v1'>2403.00196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15370v1")'>Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks</div>
<div id='2403.15370v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T17:49:11Z</div><div>Authors: Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park</div><div style='padding-top: 10px; width: 80ex'>Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.</div><div><a href='http://arxiv.org/abs/2403.15370v1'>2403.15370v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01537v1")'>Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing
  Trimodal Data</div>
<div id='2402.01537v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T16:27:45Z</div><div>Authors: Christian Stippel, Thomas Heitzinger, Rafael Sterzinger, Martin Kampel</div><div style='padding-top: 10px; width: 80ex'>In pervasive machine learning, especially in Human Behavior Analysis (HBA),
RGB has been the primary modality due to its accessibility and richness of
information. However, linked with its benefits are challenges, including
sensitivity to lighting conditions and privacy concerns. One possibility to
overcome these vulnerabilities is to resort to different modalities. For
instance, thermal is particularly adept at accentuating human forms, while
depth adds crucial contextual layers. Despite their known benefits, only a few
HBA-specific datasets that integrate these modalities exist. To address this
shortage, our research introduces a novel generative technique for creating
trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique
capitalizes on human segmentation masks derived from RGB images, combined with
thermal and depth backgrounds that are sourced automatically. With these two
ingredients, we synthesize depth and thermal counterparts from existing RGB
data utilizing conditional image-to-image translation. By employing this
approach, we generate trimodal data that can be leveraged to train models for
settings with limited data, bad lightning conditions, or privacy-sensitive
areas.</div><div><a href='http://arxiv.org/abs/2402.01537v1'>2402.01537v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17417v1")'>Through-Wall Imaging based on WiFi Channel State Information</div>
<div id='2401.17417v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T20:17:51Z</div><div>Authors: Julian Strohmayer, Rafael Sterzinger, Christian Stippel, Martin Kampel</div><div style='padding-top: 10px; width: 80ex'>This work presents a seminal approach for synthesizing images from WiFi
Channel State Information (CSI) in through-wall scenarios. Leveraging the
strengths of WiFi, such as cost-effectiveness, illumination invariance, and
wall-penetrating capabilities, our approach enables visual monitoring of indoor
environments beyond room boundaries and without the need for cameras. More
generally, it improves the interpretability of WiFi CSI by unlocking the option
to perform image-based downstream tasks, e.g., visual activity recognition. In
order to achieve this crossmodal translation from WiFi CSI to images, we rely
on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics.
We extensively evaluate our proposed methodology through an ablation study on
architecture configuration and a quantitative/qualitative assessment of
reconstructed images. Our results demonstrate the viability of our method and
highlight its potential for practical applications.</div><div><a href='http://arxiv.org/abs/2401.17417v1'>2401.17417v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17781v1")'>Vision-Assisted Digital Twin Creation for mmWave Beam Management</div>
<div id='2401.17781v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T12:23:55Z</div><div>Authors: Maximilian Arnold, Bence Major, Fabio Valerio Massoli, Joseph B. Soriaga, Arash Behboodi</div><div style='padding-top: 10px; width: 80ex'>In the context of communication networks, digital twin technology provides a
means to replicate the radio frequency (RF) propagation environment as well as
the system behaviour, allowing for a way to optimize the performance of a
deployed system based on simulations. One of the key challenges in the
application of Digital Twin technology to mmWave systems is the prevalent
channel simulators' stringent requirements on the accuracy of the 3D Digital
Twin, reducing the feasibility of the technology in real applications. We
propose a practical Digital Twin creation pipeline and a channel simulator,
that relies only on a single mounted camera and position information. We
demonstrate the performance benefits compared to methods that do not explicitly
model the 3D environment, on downstream sub-tasks in beam acquisition, using
the real-world dataset of the DeepSense6G challenge</div><div><a href='http://arxiv.org/abs/2401.17781v1'>2401.17781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.10079v1")'>Review of the Learning-based Camera and Lidar Simulation Methods for
  Autonomous Driving Systems</div>
<div id='2402.10079v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T16:56:17Z</div><div>Authors: Hamed Haghighi, Xiaomeng Wang, Hao Jing, Mehrdad Dianati</div><div style='padding-top: 10px; width: 80ex'>Perception sensors, particularly camera and Lidar, are key elements of
Autonomous Driving Systems (ADS) that enable them to comprehend their
surroundings for informed driving and control decisions. Therefore, developing
realistic camera and Lidar simulation methods, also known as camera and Lidar
models, is of paramount importance to effectively conduct simulation-based
testing for ADS. Moreover, the rise of deep learning-based perception models
has propelled the prevalence of perception sensor models as valuable tools for
synthesising diverse training datasets. The traditional sensor simulation
methods rely on computationally expensive physics-based algorithms,
specifically in complex systems such as ADS. Hence, the current potential
resides in learning-based models, driven by the success of deep generative
models in synthesising high-dimensional data. This paper reviews the current
state-of-the-art in learning-based sensor simulation methods and validation
approaches, focusing on two main types of perception sensors: cameras and
Lidars. This review covers two categories of learning-based approaches, namely
raw-data-based and object-based models. Raw-data-based methods are explained
concerning the employed learning strategy, while object-based models are
categorised based on the type of error considered. Finally, the paper
illustrates commonly used validation techniques for evaluating perception
sensor models and highlights the existing research gaps in the area.</div><div><a href='http://arxiv.org/abs/2402.10079v1'>2402.10079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06088v1")'>Towards In-Vehicle Multi-Task Facial Attribute Recognition:
  Investigating Synthetic Data and Vision Foundation Models</div>
<div id='2403.06088v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T04:17:54Z</div><div>Authors: Esmaeil Seraj, Walter Talamonti</div><div style='padding-top: 10px; width: 80ex'>In the burgeoning field of intelligent transportation systems, enhancing
vehicle-driver interaction through facial attribute recognition, such as facial
expression, eye gaze, age, etc., is of paramount importance for safety,
personalization, and overall user experience. However, the scarcity of
comprehensive large-scale, real-world datasets poses a significant challenge
for training robust multi-task models. Existing literature often overlooks the
potential of synthetic datasets and the comparative efficacy of
state-of-the-art vision foundation models in such constrained settings. This
paper addresses these gaps by investigating the utility of synthetic datasets
for training complex multi-task models that recognize facial attributes of
passengers of a vehicle, such as gaze plane, age, and facial expression.
Utilizing transfer learning techniques with both pre-trained Vision Transformer
(ViT) and Residual Network (ResNet) models, we explore various training and
adaptation methods to optimize performance, particularly when data availability
is limited. We provide extensive post-evaluation analysis, investigating the
effects of synthetic data distributions on model performance in in-distribution
data and out-of-distribution inference. Our study unveils counter-intuitive
findings, notably the superior performance of ResNet over ViTs in our specific
multi-task context, which is attributed to the mismatch in model complexity
relative to task complexity. Our results highlight the challenges and
opportunities for enhancing the use of synthetic data and vision foundation
models in practical applications.</div><div><a href='http://arxiv.org/abs/2403.06088v1'>2403.06088v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07031v1")'>Instance-Level Safety-Aware Fidelity of Synthetic Data and Its
  Calibration</div>
<div id='2402.07031v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T19:45:40Z</div><div>Authors: Chih-Hong Cheng, Paul Stöckel, Xingyu Zhao</div><div style='padding-top: 10px; width: 80ex'>Modeling and calibrating the fidelity of synthetic data is paramount in
shaping the future of safe and reliable self-driving technology by offering a
cost-effective and scalable alternative to real-world data collection. We focus
on its role in safety-critical applications, introducing four types of
instance-level fidelity that go beyond mere visual input characteristics. The
aim is to align synthetic data with real-world safety issues. We suggest an
optimization method to refine the synthetic data generator, reducing fidelity
gaps identified by the DNN-based component. Our findings show this tuning
enhances the correlation between safety-critical errors in synthetic and real
images.</div><div><a href='http://arxiv.org/abs/2402.07031v1'>2402.07031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12198v1")'>FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo
  Endoscopic Videos</div>
<div id='2403.12198v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T19:13:02Z</div><div>Authors: Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir Navab, Benjamin Busam, Alexander Ladikos</div><div style='padding-top: 10px; width: 80ex'>Reconstruction of endoscopic scenes is an important asset for various medical
applications, from post-surgery analysis to educational training. Neural
rendering has recently shown promising results in endoscopic reconstruction
with deforming tissue. However, the setup has been restricted to a static
endoscope, limited deformation, or required an external tracking device to
retrieve camera pose information of the endoscopic camera. With FLex we adress
the challenging setup of a moving endoscope within a highly dynamic environment
of deforming tissue. We propose an implicit scene separation into multiple
overlapping 4D neural radiance fields (NeRFs) and a progressive optimization
scheme jointly optimizing for reconstruction and camera poses from scratch.
This improves the ease-of-use and allows to scale reconstruction capabilities
in time to process surgical videos of 5,000 frames and more; an improvement of
more than ten times compared to the state of the art while being agnostic to
external tracking information. Extensive evaluations on the StereoMIS dataset
show that FLex significantly improves the quality of novel view synthesis while
maintaining competitive pose accuracy.</div><div><a href='http://arxiv.org/abs/2403.12198v1'>2403.12198v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06901v1")'>LIBR+: Improving Intraoperative Liver Registration by Learning the
  Residual of Biomechanics-Based Deformable Registration</div>
<div id='2403.06901v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:54:44Z</div><div>Authors: Dingrong Wang, Soheil Azadvar, Jon Heiselman, Xiajun Jiang, Michael Miga, Linwei Wang</div><div style='padding-top: 10px; width: 80ex'>The surgical environment imposes unique challenges to the intraoperative
registration of organ shapes to their preoperatively-imaged geometry.
Biomechanical model-based registration remains popular, while deep learning
solutions remain limited due to the sparsity and variability of intraoperative
measurements and the limited ground-truth deformation of an organ that can be
obtained during the surgery. In this paper, we propose a novel \textit{hybrid}
registration approach that leverage a linearized iterative boundary
reconstruction (LIBR) method based on linear elastic biomechanics, and use deep
neural networks to learn its residual to the ground-truth deformation (LIBR+).
We further formulate a dual-branch spline-residual graph convolutional neural
network (SR-GCN) to assimilate information from sparse and variable
intraoperative measurements and effectively propagate it through the geometry
of the 3D organ. Experiments on a large intraoperative liver registration
dataset demonstrated the consistent improvements achieved by LIBR+ in
comparison to existing rigid, biomechnical model-based non-rigid, and
deep-learning based non-rigid approaches to intraoperative liver registration.</div><div><a href='http://arxiv.org/abs/2403.06901v1'>2403.06901v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15753v2")'>An objective comparison of methods for augmented reality in laparoscopic
  liver resection by preoperative-to-intraoperative image fusion</div>
<div id='2401.15753v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T20:30:14Z</div><div>Authors: Sharib Ali, Yamid Espinel, Yueming Jin, Peng Liu, Bianca Güttner, Xukun Zhang, Lihua Zhang, Tom Dowrick, Matthew J. Clarkson, Shiting Xiao, Yifan Wu, Yijun Yang, Lei Zhu, Dai Sun, Lan Li, Micha Pfeiffer, Shahid Farid, Lena Maier-Hein, Emmanuel Buc, Adrien Bartoli</div><div style='padding-top: 10px; width: 80ex'>Augmented reality for laparoscopic liver resection is a visualisation mode
that allows a surgeon to localise tumours and vessels embedded within the liver
by projecting them on top of a laparoscopic image. Preoperative 3D models
extracted from CT or MRI data are registered to the intraoperative laparoscopic
images during this process. In terms of 3D-2D fusion, most of the algorithms
make use of anatomical landmarks to guide registration. These landmarks include
the liver's inferior ridge, the falciform ligament, and the occluding contours.
They are usually marked by hand in both the laparoscopic image and the 3D
model, which is time-consuming and may contain errors if done by a
non-experienced user. Therefore, there is a need to automate this process so
that augmented reality can be used effectively in the operating room. We
present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge
(P2ILF), held during the Medical Imaging and Computer Assisted Interventions
(MICCAI 2022) conference, which investigates the possibilities of detecting
these landmarks automatically and using them in registration. The challenge was
divided into two tasks: 1) A 2D and 3D landmark detection task and 2) a 3D-2D
registration task. The teams were provided with training data consisting of 167
laparoscopic images and 9 preoperative 3D models from 9 patients, with the
corresponding 2D and 3D landmark annotations. A total of 6 teams from 4
countries participated, whose proposed methods were evaluated on 16 images and
two preoperative 3D models from two patients. All the teams proposed deep
learning-based methods for the 2D and 3D landmark segmentation tasks and
differentiable rendering-based methods for the registration task. Based on the
experimental outcomes, we propose three key hypotheses that determine current
limitations and future directions for research in this domain.</div><div><a href='http://arxiv.org/abs/2401.15753v2'>2401.15753v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16658v1")'>Multi-Objective Learning for Deformable Image Registration</div>
<div id='2402.16658v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T15:42:13Z</div><div>Authors: Monika Grewal, Henrike Westerveld, Peter A. N. Bosman, Tanja Alderliesten</div><div style='padding-top: 10px; width: 80ex'>Deformable image registration (DIR) involves optimization of multiple
conflicting objectives, however, not many existing DIR algorithms are
multi-objective (MO). Further, while there has been progress in the design of
deep learning algorithms for DIR, there is no work in the direction of MO DIR
using deep learning. In this paper, we fill this gap by combining a recently
proposed approach for MO training of neural networks with a well-known deep
neural network for DIR and create a deep learning based MO DIR approach. We
evaluate the proposed approach for DIR of pelvic magnetic resonance imaging
(MRI) scans. We experimentally demonstrate that the proposed MO DIR approach --
providing multiple registration outputs for each patient that each correspond
to a different trade-off between the objectives -- has additional desirable
properties from a clinical use point-of-view as compared to providing a single
DIR output. The experiments also show that the proposed MO DIR approach
provides a better spread of DIR outputs across the entire trade-off front than
simply training multiple neural networks with weights for each objective
sampled from a grid of possible values.</div><div><a href='http://arxiv.org/abs/2402.16658v1'>2402.16658v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08821v1")'>Surface-Enhanced Raman Spectroscopy and Transfer Learning Toward
  Accurate Reconstruction of the Surgical Zone</div>
<div id='2401.08821v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T20:47:19Z</div><div>Authors: Ashutosh Raman, Ren A. Odion, Kent K. Yamamoto, Weston Ross, Tuan Vo-Dinh, Patrick J. Codd</div><div style='padding-top: 10px; width: 80ex'>Raman spectroscopy, a photonic modality based on the inelastic backscattering
of coherent light, is a valuable asset to the intraoperative sensing space,
offering non-ionizing potential and highly-specific molecular fingerprint-like
spectroscopic signatures that can be used for diagnosis of pathological tissue
in the dynamic surgical field. Though Raman suffers from weakness in intensity,
Surface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to
amplify Raman signals, can achieve detection sensitivities that rival
traditional photonic modalities. In this study, we outline a robotic Raman
system that can reliably pinpoint the location and boundaries of a tumor
embedded in healthy tissue, modeled here as a tissue-mimicking phantom with
selectively infused Gold Nanostar regions. Further, due to the relative dearth
of collected biological SERS or Raman data, we implement transfer learning to
achieve 100% validation classification accuracy for Gold Nanostars compared to
Control Agarose, thus providing a proof-of-concept for Raman-based deep
learning training pipelines. We reconstruct a surgical field of 30x60mm in 10.2
minutes, and achieve 98.2% accuracy, preserving relative measurements between
features in the phantom. We also achieve an 84.3% Intersection-over-Union
score, which is the extent of overlap between the ground truth and predicted
reconstructions. Lastly, we also demonstrate that the Raman system and
classification algorithm do not discern based on sample color, but instead on
presence of SERS agents. This study provides a crucial step in the translation
of intelligent Raman systems in intraoperative oncological spaces.</div><div><a href='http://arxiv.org/abs/2401.08821v1'>2401.08821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03761v1")'>Deep Learning-Based Correction and Unmixing of Hyperspectral Images for
  Brain Tumor Surgery</div>
<div id='2402.03761v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T07:04:35Z</div><div>Authors: David Black, Jaidev Gill, Andrew Xie, Benoit Liquet, Antonio Di leva, Walter Stummer, Eric Suero Molina</div><div style='padding-top: 10px; width: 80ex'>Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection
enables visualization of differences between tissues that are not
distinguishable to humans. This augmentation can maximize brain tumor
resection, improving patient outcomes. However, much of the processing in HSI
uses simplified linear methods that are unable to capture the non-linear,
wavelength-dependent phenomena that must be modeled for accurate recovery of
fluorophore abundances. We therefore propose two deep learning models for
correction and unmixing, which can account for the nonlinear effects and
produce more accurate estimates of abundances. Both models use an
autoencoder-like architecture to process the captured spectra. One is trained
with protoporphyrin IX (PpIX) concentration labels. The other undergoes
semi-supervised training, first learning hyperspectral unmixing self-supervised
and then learning to correct fluorescence emission spectra for heterogeneous
optical and geometric properties using a reference white-light reflectance
spectrum in a few-shot manner. The models were evaluated against phantom and
pig brain data with known PpIX concentration; the supervised model achieved
Pearson correlation coefficients (R values) between the known and computed PpIX
concentrations of 0.997 and 0.990, respectively, whereas the classical approach
achieved only 0.93 and 0.82. The semi-supervised approach's R values were 0.98
and 0.91, respectively. On human data, the semi-supervised model gives
qualitatively more realistic results than the classical method, better removing
bright spots of specular reflectance and reducing the variance in PpIX
abundance over biopsies that should be relatively homogeneous. These results
show promise for using deep learning to improve HSI in fluorescence-guided
neurosurgery.</div><div><a href='http://arxiv.org/abs/2402.03761v1'>2402.03761v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05879v1")'>Deep Learning based acoustic measurement approach for robotic
  applications on orthopedics</div>
<div id='2403.05879v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T11:09:45Z</div><div>Authors: Bangyu Lan, Momen Abayazid, Nico Verdonschot, Stefano Stramigioli, Kenan Niu</div><div style='padding-top: 10px; width: 80ex'>In Total Knee Replacement Arthroplasty (TKA), surgical robotics can provide
image-guided navigation to fit implants with high precision. Its tracking
approach highly relies on inserting bone pins into the bones tracked by the
optical tracking system. This is normally done by invasive, radiative manners
(implantable markers and CT scans), which introduce unnecessary trauma and
prolong the preparation time for patients. To tackle this issue,
ultrasound-based bone tracking could offer an alternative. In this study, we
proposed a novel deep learning structure to improve the accuracy of bone
tracking by an A-mode ultrasound (US). We first obtained a set of ultrasound
dataset from the cadaver experiment, where the ground truth locations of bones
were calculated using bone pins. These data were used to train the proposed
CasAtt-UNet to predict bone location automatically and robustly. The ground
truth bone locations and those locations of US were recorded simultaneously.
Therefore, we could label bone peaks in the raw US signals. As a result, our
method achieved sub millimeter precision across all eight bone areas with the
only exception of one channel in the ankle. This method enables the robust
measurement of lower extremity bone positions from 1D raw ultrasound signals.
It shows great potential to apply A-mode ultrasound in orthopedic surgery from
safe, convenient, and efficient perspectives.</div><div><a href='http://arxiv.org/abs/2403.05879v1'>2403.05879v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16994v1")'>GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis</div>
<div id='2402.16994v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T20:00:57Z</div><div>Authors: Dmitry Petrov, Pradyumn Goyal, Vikas Thamizharasan, Vladimir G. Kim, Matheus Gadelha, Melinos Averkiou, Siddhartha Chaudhuri, Evangelos Kalogerakis</div><div style='padding-top: 10px; width: 80ex'>We introduce GEM3D -- a new deep, topology-aware generative model of 3D
shapes. The key ingredient of our method is a neural skeleton-based
representation encoding information on both shape topology and geometry.
Through a denoising diffusion probabilistic model, our method first generates
skeleton-based representations following the Medial Axis Transform (MAT), then
generates surfaces through a skeleton-driven neural implicit formulation. The
neural implicit takes into account the topological and geometric information
stored in the generated skeleton representations to yield surfaces that are
more topologically and geometrically accurate compared to previous neural field
formulations. We discuss applications of our method in shape synthesis and
point cloud reconstruction tasks, and evaluate our method both qualitatively
and quantitatively. We demonstrate significantly more faithful surface
reconstruction and diverse shape generation results compared to the
state-of-the-art, also involving challenging scenarios of reconstructing and
synthesizing structurally complex, high-genus shape surfaces from Thingi10K and
ShapeNet.</div><div><a href='http://arxiv.org/abs/2402.16994v1'>2402.16994v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01801v2")'>A quatum inspired neural network for geometric modeling</div>
<div id='2401.01801v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T15:59:35Z</div><div>Authors: Weitao Du, Shengchao Liu, Xuecang Zhang</div><div style='padding-top: 10px; width: 80ex'>By conceiving physical systems as 3D many-body point clouds, geometric graph
neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased
promising performance. In particular, their effective message-passing mechanics
make them adept at modeling molecules and crystalline materials. However,
current geometric GNNs only offer a mean-field approximation of the many-body
system, encapsulated within two-body message passing, thus falling short in
capturing intricate relationships within these geometric graphs. To address
this limitation, tensor networks, widely employed by computational physics to
handle manybody systems using high-order tensors, have been introduced.
Nevertheless, integrating these tensorized networks into the message-passing
framework of GNNs faces scalability and symmetry conservation (e.g.,
permutation and rotation) challenges. In response, we introduce an innovative
equivariant Matrix Product State (MPS)-based message-passing strategy, through
achieving an efficient implementation of the tensor contraction operation. Our
method effectively models complex many-body relationships, suppressing
mean-field approximations, and captures symmetries within geometric graphs.
Importantly, it seamlessly replaces the standard message-passing and
layer-aggregation modules intrinsic to geometric GNNs. We empirically validate
the superior accuracy of our approach on benchmark tasks, including predicting
classical Newton systems and quantum tensor Hamiltonian matrices. To our
knowledge, our approach represents the inaugural utilization of parameterized
geometric tensor networks.</div><div><a href='http://arxiv.org/abs/2401.01801v2'>2401.01801v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12820v1")'>A Physics-embedded Deep Learning Framework for Cloth Simulation</div>
<div id='2403.12820v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T15:21:00Z</div><div>Authors: Zhiwei Zhao</div><div style='padding-top: 10px; width: 80ex'>Delicate cloth simulations have long been desired in computer graphics.
Various methods were proposed to improve engaged force interactions, collision
handling, and numerical integrations. Deep learning has the potential to
achieve fast and real-time simulation, but common neural network structures
often demand many parameters to capture cloth dynamics. This paper proposes a
physics-embedded learning framework that directly encodes physical features of
cloth simulation. The convolutional neural network is used to represent spatial
correlations of the mass-spring system, after which three branches are designed
to learn linear, nonlinear, and time derivate features of cloth physics. The
framework can also integrate with other external forces and collision handling
through either traditional simulators or sub neural networks. The model is
tested across different cloth animation cases, without training with new data.
Agreement with baselines and predictive realism successfully validate its
generalization ability. Inference efficiency of the proposed model also defeats
traditional physics simulation. This framework is also designed to easily
integrate with other visual refinement techniques like wrinkle carving, which
leaves significant chances to incorporate prevailing macing learning techniques
in 3D cloth amination.</div><div><a href='http://arxiv.org/abs/2403.12820v1'>2403.12820v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14539v1")'>Object-Centric Domain Randomization for 3D Shape Reconstruction in the
  Wild</div>
<div id='2403.14539v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T16:40:10Z</div><div>Authors: Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh</div><div style='padding-top: 10px; width: 80ex'>One of the biggest challenges in single-view 3D shape reconstruction in the
wild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-world
environments. Inspired by remarkable achievements via domain randomization, we
propose ObjectDR which synthesizes such paired data via a random simulation of
visual variations in object appearances and backgrounds. Our data synthesis
framework exploits a conditional generative model (e.g., ControlNet) to
generate images conforming to spatial conditions such as 2.5D sketches, which
are obtainable through a rendering process of 3D shapes from object collections
(e.g., Objaverse-XL). To simulate diverse variations while preserving object
silhouettes embedded in spatial conditions, we also introduce a disentangled
framework which leverages an initial object guidance. After synthesizing a wide
range of data, we pre-train a model on them so that it learns to capture a
domain-invariant geometry prior which is consistent across various domains. We
validate its effectiveness by substantially improving 3D shape reconstruction
models on a real-world benchmark. In a scale-up evaluation, our pre-training
achieves 23.6% superior results compared with the pre-training on high-quality
computer graphics renderings.</div><div><a href='http://arxiv.org/abs/2403.14539v1'>2403.14539v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09384v1")'>Diverse Part Synthesis for 3D Shape Creation</div>
<div id='2401.09384v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T17:55:06Z</div><div>Authors: Yanran Guan, Oliver van Kaick</div><div style='padding-top: 10px; width: 80ex'>Methods that use neural networks for synthesizing 3D shapes in the form of a
part-based representation have been introduced over the last few years. These
methods represent shapes as a graph or hierarchy of parts and enable a variety
of applications such as shape sampling and reconstruction. However, current
methods do not allow easily regenerating individual shape parts according to
user preferences. In this paper, we investigate techniques that allow the user
to generate multiple, diverse suggestions for individual parts. Specifically,
we experiment with multimodal deep generative models that allow sampling
diverse suggestions for shape parts and focus on models which have not been
considered in previous work on shape synthesis. To provide a comparative study
of these techniques, we introduce a method for synthesizing 3D shapes in a
part-based representation and evaluate all the part suggestion techniques
within this synthesis method. In our method, which is inspired by previous
work, shapes are represented as a set of parts in the form of implicit
functions which are then positioned in space to form the final shape. Synthesis
in this representation is enabled by a neural network architecture based on an
implicit decoder and a spatial transformer. We compare the various multimodal
generative models by evaluating their performance in generating part
suggestions. Our contribution is to show with qualitative and quantitative
evaluations which of the new techniques for multimodal part generation perform
the best and that a synthesis method based on the top-performing techniques
allows the user to more finely control the parts that are generated in the 3D
shapes while maintaining high shape fidelity when reconstructing shapes.</div><div><a href='http://arxiv.org/abs/2401.09384v1'>2401.09384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06973v1")'>Bayesian Diffusion Models for 3D Shape Reconstruction</div>
<div id='2403.06973v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:55:53Z</div><div>Authors: Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu</div><div style='padding-top: 10px; width: 80ex'>We present Bayesian Diffusion Models (BDM), a prediction algorithm that
performs effective Bayesian inference by tightly coupling the top-down (prior)
information with the bottom-up (data-driven) procedure via joint diffusion
processes. We show the effectiveness of BDM on the 3D shape reconstruction
task. Compared to prototypical deep learning data-driven approaches trained on
paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM
brings in rich prior information from standalone labels (e.g. point clouds) to
improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian
frameworks where explicit prior and likelihood are required for the inference,
BDM performs seamless information fusion via coupled diffusion processes with
learned gradient computation networks. The specialty of our BDM lies in its
capability to engage the active and effective information exchange and fusion
of the top-down and bottom-up processes where each itself is a diffusion
process. We demonstrate state-of-the-art results on both synthetic and
real-world benchmarks for 3D shape reconstruction.</div><div><a href='http://arxiv.org/abs/2403.06973v1'>2403.06973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10395v1")'>Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding</div>
<div id='2403.10395v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:27:58Z</div><div>Authors: Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang Xue, Xinzhou Wang</div><div style='padding-top: 10px; width: 80ex'>Encouraged by the growing availability of pre-trained 2D diffusion models,
image-to-3D generation by leveraging Score Distillation Sampling (SDS) is
making remarkable progress. Most existing methods combine novel-view lifting
from 2D diffusion models which usually take the reference image as a condition
while applying hard L2 image supervision at the reference view. Yet heavily
adhering to the image is prone to corrupting the inductive knowledge of the 2D
diffusion model leading to flat or distorted 3D generation frequently. In this
work, we reexamine image-to-3D in a novel perspective and present Isotropic3D,
an image-to-3D generation pipeline that takes only an image CLIP embedding as
input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth
angle by solely resting on the SDS loss. The core of our framework lies in a
two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D
diffusion model by substituting its text encoder with an image encoder, by
which the model preliminarily acquires image-to-image capabilities. Secondly,
we perform fine-tuning using our Explicit Multi-view Attention (EMA) which
combines noisy multi-view images with the noise-free reference image as an
explicit condition. CLIP embedding is sent to the diffusion model throughout
the whole process while reference images are discarded once after fine-tuning.
As a result, with a single image CLIP embedding, Isotropic3D is capable of
generating multi-view mutually consistent images and also a 3D model with more
symmetrical and neat content, well-proportioned geometry, rich colored texture,
and less distortion compared with existing image-to-3D methods while still
preserving the similarity to the reference image to a large extent. The project
page is available at https://isotropic3d.github.io/. The code and models are
available at https://github.com/pkunliu/Isotropic3D.</div><div><a href='http://arxiv.org/abs/2403.10395v1'>2403.10395v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06127v1")'>E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image
  Translation</div>
<div id='2401.06127v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:59:14Z</div><div>Authors: Yifan Gong, Zheng Zhan, Qing Jin, Yanyu Li, Yerlan Idelbayev, Xian Liu, Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, Jian Ren</div><div style='padding-top: 10px; width: 80ex'>One highly promising direction for enabling flexible real-time on-device
image editing is utilizing data distillation by leveraging large-scale
text-to-image diffusion models, such as Stable Diffusion, to generate paired
datasets used for training generative adversarial networks (GANs). This
approach notably alleviates the stringent requirements typically imposed by
high-end commercial GPUs for performing image editing with diffusion models.
However, unlike text-to-image diffusion models, each distilled GAN is
specialized for a specific image editing task, necessitating costly training
efforts to obtain models for various concepts. In this work, we introduce and
address a novel research direction: can the process of distilling GANs from
diffusion models be made significantly more efficient? To achieve this goal, we
propose a series of innovative techniques. First, we construct a base GAN model
with generalized features, adaptable to different concepts through fine-tuning,
eliminating the need for training from scratch. Second, we identify crucial
layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a
simple yet effective rank search process, rather than fine-tuning the entire
base model. Third, we investigate the minimal amount of data necessary for
fine-tuning, further reducing the overall training time. Extensive experiments
show that we can efficiently empower GANs with the ability to perform real-time
high-quality image editing on mobile devices with remarkable reduced training
cost and storage for each concept.</div><div><a href='http://arxiv.org/abs/2401.06127v1'>2401.06127v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12036v1")'>One-Step Image Translation with Text-to-Image Models</div>
<div id='2403.12036v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:59:40Z</div><div>Authors: Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, Jun-Yan Zhu</div><div style='padding-top: 10px; width: 80ex'>In this work, we address two limitations of existing conditional diffusion
models: their slow inference speed due to the iterative denoising process and
their reliance on paired data for model fine-tuning. To tackle these issues, we
introduce a general method for adapting a single-step diffusion model to new
tasks and domains through adversarial learning objectives. Specifically, we
consolidate various modules of the vanilla latent diffusion model into a single
end-to-end generator network with small trainable weights, enhancing its
ability to preserve the input image structure while reducing overfitting. We
demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms
existing GAN-based and diffusion-based methods for various scene translation
tasks, such as day-to-night conversion and adding/removing weather effects like
fog, snow, and rain. We extend our method to paired settings, where our model
pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and
Edge2Image, but with a single-step inference. This work suggests that
single-step diffusion models can serve as strong backbones for a range of GAN
learning objectives. Our code and models are available at
https://github.com/GaParmar/img2img-turbo.</div><div><a href='http://arxiv.org/abs/2403.12036v1'>2403.12036v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12531v2")'>Improving Deep Generative Models on Many-To-One Image-to-Image
  Translation</div>
<div id='2402.12531v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:41:03Z</div><div>Authors: Sagar Saxena, Mohammad Nayeem Teli</div><div style='padding-top: 10px; width: 80ex'>Deep generative models have been applied to multiple applications in
image-to-image translation. Generative Adversarial Networks and Diffusion
Models have presented impressive results, setting new state-of-the-art results
on these tasks. Most methods have symmetric setups across the different domains
in a dataset. These methods assume that all domains have either multiple
modalities or only one modality. However, there are many datasets that have a
many-to-one relationship between two domains. In this work, we first introduce
a Colorized MNIST dataset and a Color-Recall score that can provide a simple
benchmark for evaluating models on many-to-one translation. We then introduce a
new asymmetric framework to improve existing deep generative models on
many-to-one image-to-image translation. We apply this framework to StarGAN V2
and show that in both unsupervised and semi-supervised settings, the
performance of this new model improves on many-to-one image-to-image
translation.</div><div><a href='http://arxiv.org/abs/2402.12531v2'>2402.12531v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07142v1")'>One Category One Prompt: Dataset Distillation using Diffusion Models</div>
<div id='2403.07142v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:23:59Z</div><div>Authors: Ali Abbasi, Ashkan Shahbazi, Hamed Pirsiavash, Soheil Kolouri</div><div style='padding-top: 10px; width: 80ex'>The extensive amounts of data required for training deep neural networks pose
significant challenges on storage and transmission fronts. Dataset distillation
has emerged as a promising technique to condense the information of massive
datasets into a much smaller yet representative set of synthetic samples.
However, traditional dataset distillation approaches often struggle to scale
effectively with high-resolution images and more complex architectures due to
the limitations in bi-level optimization. Recently, several works have proposed
exploiting knowledge distillation with decoupled optimization schemes to scale
up dataset distillation. Although these methods effectively address the
scalability issue, they rely on extensive image augmentations requiring the
storage of soft labels for augmented images. In this paper, we introduce
Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for
dataset distillation, leveraging recent advancements in generative
text-to-image foundation models. Our approach utilizes textual inversion, a
technique for fine-tuning text-to-image generative models, to create concise
and informative representations for large datasets. By employing these learned
text prompts, we can efficiently store and infer new samples for introducing
data variability within a fixed memory budget. We show the effectiveness of our
method through extensive experiments across various computer vision benchmark
datasets with different memory budgets.</div><div><a href='http://arxiv.org/abs/2403.07142v1'>2403.07142v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14421v1")'>DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning</div>
<div id='2403.14421v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T14:17:28Z</div><div>Authors: Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo</div><div style='padding-top: 10px; width: 80ex'>Text-to-image diffusion models have been shown to suffer from sample-level
memorization, possibly reproducing near-perfect replica of images that they are
trained on, which may be undesirable. To remedy this issue, we develop the
first differentially private (DP) retrieval-augmented generation algorithm that
is capable of generating high-quality image samples while providing provable
privacy guarantees. Specifically, we assume access to a text-to-image diffusion
model trained on a small amount of public data, and design a DP retrieval
mechanism to augment the text prompt with samples retrieved from a private
retrieval dataset. Our \emph{differentially private retrieval-augmented
diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to
adapt to another domain, and can use state-of-the-art generative models to
generate high-quality image samples while satisfying rigorous DP guarantees.
For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a
privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in
FID compared to public-only retrieval for up to $10,000$ queries.</div><div><a href='http://arxiv.org/abs/2403.14421v1'>2403.14421v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09450v1")'>Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative
  Privacy Risk</div>
<div id='2403.09450v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T14:48:37Z</div><div>Authors: Zhangheng Li, Junyuan Hong, Bo Li, Zhangyang Wang</div><div style='padding-top: 10px; width: 80ex'>While diffusion models have recently demonstrated remarkable progress in
generating realistic images, privacy risks also arise: published models or APIs
could generate training images and thus leak privacy-sensitive training
information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that
fine-tuning the pre-trained models with manipulated data can amplify the
existing privacy risks. We demonstrate that S2L could occur in various standard
fine-tuning strategies for diffusion models, including concept-injection
methods (DreamBooth and Textual Inversion) and parameter-efficient methods
(LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L
can amplify the state-of-the-art membership inference attack (MIA) on diffusion
models by $5.4\%$ (absolute difference) AUC and can increase extracted private
samples from almost $0$ samples to $16.3$ samples on average per target domain.
This discovery underscores that the privacy risk with diffusion models is even
more severe than previously recognized. Codes are available at
https://github.com/VITA-Group/Shake-to-Leak.</div><div><a href='http://arxiv.org/abs/2403.09450v1'>2403.09450v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18607v2")'>Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An
  Adversarial Perspective</div>
<div id='2402.18607v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T12:21:12Z</div><div>Authors: Xinjian Luo, Yangfan Jiang, Fei Wei, Yuncheng Wu, Xiaokui Xiao, Beng Chin Ooi</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have recently gained significant attention in both academia
and industry due to their impressive generative performance in terms of both
sampling quality and distribution coverage. Accordingly, proposals are made for
sharing pre-trained diffusion models across different organizations, as a way
of improving data utilization while enhancing privacy protection by avoiding
sharing private data directly. However, the potential risks associated with
such an approach have not been comprehensively examined.
  In this paper, we take an adversarial perspective to investigate the
potential privacy and fairness risks associated with the sharing of diffusion
models. Specifically, we investigate the circumstances in which one party (the
sharer) trains a diffusion model using private data and provides another party
(the receiver) black-box access to the pre-trained model for downstream tasks.
We demonstrate that the sharer can execute fairness poisoning attacks to
undermine the receiver's downstream models by manipulating the training data
distribution of the diffusion model. Meanwhile, the receiver can perform
property inference attacks to reveal the distribution of sensitive features in
the sharer's dataset. Our experiments conducted on real-world datasets
demonstrate remarkable attack performance on different types of diffusion
models, which highlights the critical importance of robust data auditing and
privacy protection protocols in pertinent applications.</div><div><a href='http://arxiv.org/abs/2402.18607v2'>2402.18607v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03140v3")'>Fair Sampling in Diffusion Models through Switching Mechanism</div>
<div id='2401.03140v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:55:26Z</div><div>Authors: Yujin Choi, Jinseong Park, Hoki Kim, Jaewook Lee, Saeroom Park</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have shown their effectiveness in generation tasks by
well-approximating the underlying probability distribution. However, diffusion
models are known to suffer from an amplified inherent bias from the training
data in terms of fairness. While the sampling process of diffusion models can
be controlled by conditional guidance, previous works have attempted to find
empirical guidance to achieve quantitative fairness. To address this
limitation, we propose a fairness-aware sampling method called
\textit{attribute switching} mechanism for diffusion models. Without additional
training, the proposed sampling can obfuscate sensitive attributes in generated
data without relying on classifiers. We mathematically prove and experimentally
demonstrate the effectiveness of the proposed method on two key aspects: (i)
the generation of fair data and (ii) the preservation of the utility of the
generated data.</div><div><a href='http://arxiv.org/abs/2401.03140v3'>2401.03140v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07588v1")'>Visual Privacy Auditing with Diffusion Models</div>
<div id='2403.07588v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T12:18:55Z</div><div>Authors: Kristian Schwethelm, Johannes Kaiser, Moritz Knolle, Daniel Rueckert, Georgios Kaissis, Alexander Ziller</div><div style='padding-top: 10px; width: 80ex'>Image reconstruction attacks on machine learning models pose a significant
risk to privacy by potentially leaking sensitive information. Although
defending against such attacks using differential privacy (DP) has proven
effective, determining appropriate DP parameters remains challenging. Current
formal guarantees on data reconstruction success suffer from overly theoretical
assumptions regarding adversary knowledge about the target data, particularly
in the image domain. In this work, we empirically investigate this discrepancy
and find that the practicality of these assumptions strongly depends on the
domain shift between the data prior and the reconstruction target. We propose a
reconstruction attack based on diffusion models (DMs) that assumes adversary
access to real-world image priors and assess its implications on privacy
leakage under DP-SGD. We show that (1) real-world data priors significantly
influence reconstruction success, (2) current reconstruction bounds do not
model the risk posed by data priors well, and (3) DMs can serve as effective
auditing tools for visualizing privacy leakage.</div><div><a href='http://arxiv.org/abs/2403.07588v1'>2403.07588v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11989v1")'>Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models</div>
<div id='2402.11989v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:32:48Z</div><div>Authors: Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang</div><div style='padding-top: 10px; width: 80ex'>Low-rank adaptation (LoRA) is an efficient strategy for adapting latent
diffusion models (LDMs) on a training dataset to generate specific objects by
minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable
to membership inference (MI) attacks that can judge whether a particular data
point belongs to private training datasets, thus facing severe risks of privacy
leakage. To defend against MI attacks, we make the first effort to propose a
straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is
formulated as a min-max optimization problem where a proxy attack model is
trained by maximizing its MI gain while the LDM is adapted by minimizing the
sum of the adaptation loss and the proxy attack model's MI gain. However, we
empirically disclose that PrivateLoRA has the issue of unstable optimization
due to the large fluctuation of the gradient scale which impedes adaptation. To
mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by
minimizing the ratio of the adaptation loss to the MI gain, which implicitly
rescales the gradient and thus stabilizes the optimization. Our comprehensive
empirical results corroborate that adapted LDMs via Stable PrivateLoRA can
effectively defend against MI attacks while generating high-quality images. Our
code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.</div><div><a href='http://arxiv.org/abs/2402.11989v1'>2402.11989v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11637v1")'>Poisoning Federated Recommender Systems with Fake Users</div>
<div id='2402.11637v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T16:34:12Z</div><div>Authors: Ming Yin, Yichang Xu, Minghong Fang, Neil Zhenqiang Gong</div><div style='padding-top: 10px; width: 80ex'>Federated recommendation is a prominent use case within federated learning,
yet it remains susceptible to various attacks, from user to server-side
vulnerabilities. Poisoning attacks are particularly notable among user-side
attacks, as participants upload malicious model updates to deceive the global
model, often intending to promote or demote specific targeted items. This study
investigates strategies for executing promotion attacks in federated
recommender systems.
  Current poisoning attacks on federated recommender systems often rely on
additional information, such as the local training data of genuine users or
item popularity. However, such information is challenging for the potential
attacker to obtain. Thus, there is a need to develop an attack that requires no
extra information apart from item embeddings obtained from the server. In this
paper, we introduce a novel fake user based poisoning attack named PoisonFRS to
promote the attacker-chosen targeted item in federated recommender systems
without requiring knowledge about user-item rating data, user attributes, or
the aggregation rule used by the server. Extensive experiments on multiple
real-world datasets demonstrate that PoisonFRS can effectively promote the
attacker-chosen targeted item to a large portion of genuine users and
outperform current benchmarks that rely on additional information about the
system. We further observe that the model updates from both genuine and fake
users are indistinguishable within the latent space.</div><div><a href='http://arxiv.org/abs/2402.11637v1'>2402.11637v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03559v1")'>Projected Generative Diffusion Models for Constraint Satisfaction</div>
<div id='2402.03559v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:18:16Z</div><div>Authors: Jacob K Christopher, Stephen Baek, Ferdinando Fioretto</div><div style='padding-top: 10px; width: 80ex'>Generative diffusion models excel at robustly synthesizing coherent content
from raw noise through a sequential process. However, their direct application
in scenarios requiring outputs to adhere to specific, stringent criteria faces
several severe challenges. This paper aims at overcome these challenges and
introduces Projected Generative Diffusion Models (PGDM), an approach that
recast traditional diffusion models sampling into a constrained-optimization
problem. This enables the application of an iterative projections method to
ensure that generated data faithfully adheres to specified constraints or
physical principles. This paper provides theoretical support for the ability of
PGDM to synthesize outputs from a feasible subdistribution under a restricted
class of constraints while also providing large empirical evidence in the case
of complex non-convex constraints and ordinary differential equations. These
capabilities are demonstrated by physics-informed motion in video generation,
trajectory optimization in path planning, and morphometric properties adherence
in material science.</div><div><a href='http://arxiv.org/abs/2402.03559v1'>2402.03559v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08077v1")'>Diffeomorphic Measure Matching with Kernels for Generative Modeling</div>
<div id='2402.08077v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T21:44:20Z</div><div>Authors: Biraj Pandey, Bamdad Hosseini, Pau Batlle, Houman Owhadi</div><div style='padding-top: 10px; width: 80ex'>This article presents a general framework for the transport of probability
measures towards minimum divergence generative modeling and sampling using
ordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces
(RKHSs), inspired by ideas from diffeomorphic matching and image registration.
A theoretical analysis of the proposed method is presented, giving a priori
error bounds in terms of the complexity of the model, the number of samples in
the training set, and model misspecification. An extensive suite of numerical
experiments further highlights the properties, strengths, and weaknesses of the
method and extends its applicability to other tasks, such as conditional
simulation and inference.</div><div><a href='http://arxiv.org/abs/2402.08077v1'>2402.08077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09970v1")'>Accelerating Parallel Sampling of Diffusion Models</div>
<div id='2402.09970v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:27:58Z</div><div>Authors: Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, Tsung-Hui Chang</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have emerged as state-of-the-art generative models for image
generation. However, sampling from diffusion models is usually time-consuming
due to the inherent autoregressive nature of their sampling process. In this
work, we propose a novel approach that accelerates the sampling of diffusion
models by parallelizing the autoregressive process. Specifically, we
reformulate the sampling process as solving a system of triangular nonlinear
equations through fixed-point iteration. With this innovative formulation, we
explore several systematic techniques to further reduce the iteration steps
required by the solving process. Applying these techniques, we introduce
ParaTAA, a universal and training-free parallel sampling algorithm that can
leverage extra computational and memory resources to increase the sampling
speed. Our experiments demonstrate that ParaTAA can decrease the inference
steps required by common sequential sampling algorithms such as DDIM and DDPM
by a factor of 4~14 times. Notably, when applying ParaTAA with 100 steps DDIM
for Stable Diffusion, a widely-used text-to-image diffusion model, it can
produce the same images as the sequential sampling in only 7 inference steps.</div><div><a href='http://arxiv.org/abs/2402.09970v1'>2402.09970v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00736v2")'>Diffusion Models, Image Super-Resolution And Everything: A Survey</div>
<div id='2401.00736v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T12:25:57Z</div><div>Authors: Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Sebastian Palacio, Andreas Dengel</div><div style='padding-top: 10px; width: 80ex'>Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field
and further closed the gap between image quality and human perceptual
preferences. They are easy to train and can produce very high-quality samples
that exceed the realism of those produced by previous generative methods.
Despite their promising results, they also come with new challenges that need
further research: high computational demands, comparability, lack of
explainability, color shifts, and more. Unfortunately, entry into this field is
overwhelming because of the abundance of publications. To address this, we
provide a unified recount of the theoretical foundations underlying DMs applied
to image SR and offer a detailed analysis that underscores the unique
characteristics and methodologies within this domain, distinct from broader
existing reviews in the field. This survey articulates a cohesive understanding
of DM principles and explores current research avenues, including alternative
input domains, conditioning techniques, guidance mechanisms, corruption spaces,
and zero-shot learning approaches. By offering a detailed examination of the
evolution and current trends in image SR through the lens of DMs, this survey
sheds light on the existing challenges and charts potential future directions,
aiming to inspire further innovation in this rapidly advancing area.</div><div><a href='http://arxiv.org/abs/2401.00736v2'>2401.00736v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05196v1")'>Denoising Autoregressive Representation Learning</div>
<div id='2403.05196v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T10:19:00Z</div><div>Authors: Yazhe Li, Jorg Bornschein, Ting Chen</div><div style='padding-top: 10px; width: 80ex'>In this paper, we explore a new generative approach for learning visual
representations. Our method, DARL, employs a decoder-only Transformer to
predict image patches autoregressively. We find that training with Mean Squared
Error (MSE) alone leads to strong representations. To enhance the image
generation ability, we replace the MSE loss with the diffusion objective by
using a denoising patch decoder. We show that the learned representation can be
improved by using tailored noise schedules and longer training in larger
models. Notably, the optimal schedule differs significantly from the typical
ones used in standard image diffusion models. Overall, despite its simple
architecture, DARL delivers performance remarkably close to state-of-the-art
masked prediction models under the fine-tuning protocol. This marks an
important step towards a unified model capable of both visual perception and
generation, effectively combining the strengths of autoregressive and denoising
diffusion models.</div><div><a href='http://arxiv.org/abs/2403.05196v1'>2403.05196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10520v1")'>Strong and Controllable Blind Image Decomposition</div>
<div id='2403.10520v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T17:59:44Z</div><div>Authors: Zeyu Zhang, Junlin Han, Chenhui Gou, Hongdong Li, Liang Zheng</div><div style='padding-top: 10px; width: 80ex'>Blind image decomposition aims to decompose all components present in an
image, typically used to restore a multi-degraded input image. While fully
recovering the clean image is appealing, in some scenarios, users might want to
retain certain degradations, such as watermarks, for copyright protection. To
address this need, we add controllability to the blind image decomposition
process, allowing users to enter which types of degradation to remove or
retain. We design an architecture named controllable blind image decomposition
network. Inserted in the middle of U-Net structure, our method first decomposes
the input feature maps and then recombines them according to user instructions.
Advantageously, this functionality is implemented at minimal computational
cost: decomposition and recombination are all parameter-free. Experimentally,
our system excels in blind image decomposition tasks and can outputs partially
or fully restored images that well reflect user intentions. Furthermore, we
evaluate and configure different options for the network structure and loss
functions. This, combined with the proposed decomposition-and-recombination
method, yields an efficient and competitive system for blind image
decomposition, compared with current state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2403.10520v1'>2403.10520v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15919v2")'>Learning to See Through Dazzle</div>
<div id='2402.15919v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T22:22:02Z</div><div>Authors: Xiaopeng Peng, Erin F. Fleet, Abbie T. Watnik, Grover A. Swartzlander</div><div style='padding-top: 10px; width: 80ex'>Machine vision is susceptible to laser dazzle, where intense laser light can
blind and distort its perception of the environment through oversaturation or
permanent damage to sensor pixels. Here we employ a wavefront-coded phase mask
to diffuse the energy of laser light and introduce a sandwich generative
adversarial network (SGAN) to restore images from complex image degradations,
such as varying laser-induced image saturation, mask-induced image blurring,
unknown lighting conditions, and various noise corruptions. The SGAN
architecture combines discriminative and generative methods by wrapping two
GANs around a learnable image deconvolution module. In addition, we make use of
Fourier feature representations to reduce the spectral bias of neural networks
and improve its learning of high-frequency image details. End-to-end training
includes the realistic physics-based synthesis of a large set of training data
from publicly available images. We trained the SGAN to suppress the peak laser
irradiance as high as $10^6$ times the sensor saturation threshold - the point
at which camera sensors may experience damage without the mask. The trained
model was evaluated on both a synthetic data set and data collected from the
laboratory. The proposed image restoration model quantitatively and
qualitatively outperforms state-of-the-art methods for a wide range of scene
contents, laser powers, incident laser angles, ambient illumination strengths,
and noise characteristics.</div><div><a href='http://arxiv.org/abs/2402.15919v2'>2402.15919v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06654v1")'>Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI
  Benchmarks</div>
<div id='2401.06654v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T16:01:17Z</div><div>Authors: Stefan Blücher, Johanna Vielhaben, Nils Strodthoff</div><div style='padding-top: 10px; width: 80ex'>Feature removal is a central building block for eXplainable AI (XAI), both
for occlusion-based explanations (Shapley values) as well as their evaluation
(pixel flipping, PF). However, occlusion strategies can vary significantly from
simple mean replacement up to inpainting with state-of-the-art diffusion
models. This ambiguity limits the usefulness of occlusion-based approaches. For
example, PF benchmarks lead to contradicting rankings. This is amplified by
competing PF measures: Features are either removed starting with most
influential first (MIF) or least influential first (LIF). This study proposes
two complementary perspectives to resolve this disagreement problem. Firstly,
we address the common criticism of occlusion-based XAI, that artificial samples
lead to unreliable model evaluations. We propose to measure the reliability by
the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a
systematic comparison of occlusion strategies and resolves the disagreement
problem by grouping consistent PF rankings. Secondly, we show that the
insightfulness of MIF and LIF is conversely dependent on the R-OMS score. To
leverage this, we combine the MIF and LIF measures into the symmetric relevance
gain (SRG) measure. This breaks the inherent connection to the underlying
occlusion strategy and leads to consistent rankings. This resolves the
disagreement problem, which we verify for a set of 40 different occlusion
strategies.</div><div><a href='http://arxiv.org/abs/2401.06654v1'>2401.06654v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.19186v1")'>Disentangling representations of retinal images with generative models</div>
<div id='2402.19186v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T14:11:08Z</div><div>Authors: Sarah Müller, Lisa M. Koch, Hendrik P. A. Lensch, Philipp Berens</div><div style='padding-top: 10px; width: 80ex'>Retinal fundus images play a crucial role in the early detection of eye
diseases and, using deep learning approaches, recent studies have even
demonstrated their potential for detecting cardiovascular risk factors and
neurological disorders. However, the impact of technical factors on these
images can pose challenges for reliable AI applications in ophthalmology. For
example, large fundus cohorts are often confounded by factors like camera type,
image quality or illumination level, bearing the risk of learning shortcuts
rather than the causal relationships behind the image generation process. Here,
we introduce a novel population model for retinal fundus images that
effectively disentangles patient attributes from camera effects, thus enabling
controllable and highly realistic image generation. To achieve this, we propose
a novel disentanglement loss based on distance correlation. Through qualitative
and quantitative analyses, we demonstrate the effectiveness of this novel loss
function in disentangling the learned subspaces. Our results show that our
model provides a new perspective on the complex relationship between patient
attributes and technical confounders in retinal fundus image generation.</div><div><a href='http://arxiv.org/abs/2402.19186v1'>2402.19186v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17563v2")'>Structure-Guided Adversarial Training of Diffusion Models</div>
<div id='2402.17563v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T15:05:13Z</div><div>Authors: Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have demonstrated exceptional efficacy in various generative
applications. While existing models focus on minimizing a weighted sum of
denoising score matching losses for data distribution modeling, their training
primarily emphasizes instance-level optimization, overlooking valuable
structural information within each mini-batch, indicative of pair-wise
relationships among samples. To address this limitation, we introduce
Structure-guided Adversarial training of Diffusion Models (SADM). In this
pioneering approach, we compel the model to learn manifold structures between
samples in each training batch. To ensure the model captures authentic manifold
structures in the data distribution, we advocate adversarial training of the
diffusion generator against a novel structure discriminator in a minimax game,
distinguishing real manifold structures from the generated ones. SADM
substantially improves existing diffusion transformers (DiT) and outperforms
existing methods in image generation and cross-domain fine-tuning tasks across
12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on
ImageNet for class-conditional image generation at resolutions of 256x256 and
512x512, respectively.</div><div><a href='http://arxiv.org/abs/2402.17563v2'>2402.17563v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11261v2")'>Diffusion Model Conditioning on Gaussian Mixture Model and Negative
  Gaussian Mixture Gradient</div>
<div id='2401.11261v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T16:01:18Z</div><div>Authors: Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan Yuan</div><div style='padding-top: 10px; width: 80ex'>Diffusion models (DMs) are a type of generative model that has a huge impact
on image synthesis and beyond. They achieve state-of-the-art generation results
in various generative tasks. A great diversity of conditioning inputs, such as
text or bounding boxes, are accessible to control the generation. In this work,
we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as
feature conditioning to guide the denoising process. Based on set theory, we
provide a comprehensive theoretical analysis that shows that conditional latent
distribution based on features and classes is significantly different, so that
conditional latent distribution on features produces fewer defect generations
than conditioning on classes. Two diffusion models conditioned on the Gaussian
mixture model are trained separately for comparison. Experiments support our
findings. A novel gradient function called the negative Gaussian mixture
gradient (NGMG) is proposed and applied in diffusion model training with an
additional classifier. Training stability has improved. We also theoretically
prove that NGMG shares the same benefit as the Earth Mover distance
(Wasserstein) as a more sensible cost function when learning distributions
supported by low-dimensional manifolds.</div><div><a href='http://arxiv.org/abs/2401.11261v2'>2401.11261v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03201v1")'>Guidance with Spherical Gaussian Constraint for Conditional Diffusion</div>
<div id='2402.03201v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:12:21Z</div><div>Authors: Lingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, Ye Shi</div><div style='padding-top: 10px; width: 80ex'>Recent advances in diffusion models attempt to handle conditional generative
tasks by utilizing a differentiable loss function for guidance without the need
for additional training. While these methods achieved certain success, they
often compromise on sample quality and require small guidance step sizes,
leading to longer sampling processes. This paper reveals that the fundamental
issue lies in the manifold deviation during the sampling process when loss
guidance is employed. We theoretically show the existence of manifold deviation
by establishing a certain lower bound for the estimation error of the loss
guidance. To mitigate this problem, we propose Diffusion with Spherical
Gaussian constraint (DSG), drawing inspiration from the concentration
phenomenon in high-dimensional Gaussian distributions. DSG effectively
constrains the guidance step within the intermediate data manifold through
optimization and enables the use of larger guidance steps. Furthermore, we
present a closed-form solution for DSG denoising with the Spherical Gaussian
constraint. Notably, DSG can seamlessly integrate as a plugin module within
existing training-free conditional diffusion methods. Implementing DSG merely
involves a few lines of additional code with almost no extra computational
overhead, yet it leads to significant performance improvements. Comprehensive
experimental results in various conditional generation tasks validate the
superiority and adaptability of DSG in terms of both sample quality and time
efficiency.</div><div><a href='http://arxiv.org/abs/2402.03201v1'>2402.03201v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03881v1")'>Latent Dataset Distillation with Diffusion Models</div>
<div id='2403.03881v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:41:41Z</div><div>Authors: Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov, Andreas Dengel</div><div style='padding-top: 10px; width: 80ex'>The efficacy of machine learning has traditionally relied on the availability
of increasingly larger datasets. However, large datasets pose storage
challenges and contain non-influential samples, which could be ignored during
training without impacting the final accuracy of the model. In response to
these limitations, the concept of distilling the information on a dataset into
a condensed set of (synthetic) samples, namely a distilled dataset, emerged.
One crucial aspect is the selected architecture (usually ConvNet) for linking
the original and synthetic datasets. However, the final accuracy is lower if
the employed model architecture differs from the model used during
distillation. Another challenge is the generation of high-resolution images,
e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation
with Diffusion Models (LD3M) that combine diffusion in latent space with
dataset distillation to tackle both challenges. LD3M incorporates a novel
diffusion process tailored for dataset distillation, which improves the
gradient norms for learning synthetic images. By adjusting the number of
diffusion steps, LD3M also offers a straightforward way of controlling the
trade-off between speed and accuracy. We evaluate our approach in several
ImageNet subsets and for high-resolution images (128x128 and 256x256). As a
result, LD3M consistently outperforms state-of-the-art distillation techniques
by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.</div><div><a href='http://arxiv.org/abs/2403.03881v1'>2403.03881v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12429v1")'>TransformMix: Learning Transformation and Mixing Strategies from Data</div>
<div id='2403.12429v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T04:36:41Z</div><div>Authors: Tsz-Him Cheung, Dit-Yan Yeung</div><div style='padding-top: 10px; width: 80ex'>Data augmentation improves the generalization power of deep learning models
by synthesizing more training samples. Sample-mixing is a popular data
augmentation approach that creates additional data by combining existing
samples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple
mixing operations to blend multiple inputs. Although such a heuristic approach
shows certain performance gains in some computer vision tasks, it mixes the
images blindly and does not adapt to different datasets automatically. A mixing
strategy that is effective for a particular dataset does not often generalize
well to other datasets. If not properly configured, the methods may create
misleading mixed images, which jeopardize the effectiveness of sample-mixing
augmentations. In this work, we propose an automated approach, TransformMix, to
learn better transformation and mixing augmentation strategies from data. In
particular, TransformMix applies learned transformations and mixing masks to
create compelling mixed images that contain correct and important information
for the target tasks. We demonstrate the effectiveness of TransformMix on
multiple datasets in transfer learning, classification, object detection, and
knowledge distillation settings. Experimental results show that our method
achieves better performance as well as efficiency when compared with strong
sample-mixing baselines.</div><div><a href='http://arxiv.org/abs/2403.12429v1'>2403.12429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17065v1")'>Taming the Tail in Class-Conditional GANs: Knowledge Sharing via
  Unconditional Training at Lower Resolutions</div>
<div id='2402.17065v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T23:03:00Z</div><div>Authors: Saeed Khorram, Mingqi Jiang, Mohamad Shahbazi, Mohamad H. Danesh, Li Fuxin</div><div style='padding-top: 10px; width: 80ex'>Despite the extensive research on training generative adversarial networks
(GANs) with limited training data, learning to generate images from long-tailed
training distributions remains fairly unexplored. In the presence of imbalanced
multi-class training data, GANs tend to favor classes with more samples,
leading to the generation of low-quality and less diverse samples in tail
classes. In this study, we aim to improve the training of class-conditional
GANs with long-tailed data. We propose a straightforward yet effective method
for knowledge sharing, allowing tail classes to borrow from the rich
information from classes with more abundant training data. More concretely, we
propose modifications to existing class-conditional GAN architectures to ensure
that the lower-resolution layers of the generator are trained entirely
unconditionally while reserving class-conditional generation for the
higher-resolution layers. Experiments on several long-tail benchmarks and GAN
architectures demonstrate a significant improvement over existing methods in
both the diversity and fidelity of the generated images. The code is available
at https://github.com/khorrams/utlo.</div><div><a href='http://arxiv.org/abs/2402.17065v1'>2402.17065v1</a></div>
</div></div>
    <div><a href="arxiv_4.html">Prev (4)</a></div>
    <div><a href="arxiv_6.html">Next (6)</a></div>
    