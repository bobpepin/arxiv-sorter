
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16346v1")'>Boosting Graph Pooling with Persistent Homology</div>
<div id='2402.16346v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:00:24Z</div><div>Authors: Chaolong Ying, Xinjian Zhao, Tianshu Yu</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been an emerging trend to integrate persistent homology
(PH) into graph neural networks (GNNs) to enrich expressive power. However,
naively plugging PH features into GNN layers always results in marginal
improvement with low interpretability. In this paper, we investigate a novel
mechanism for injecting global topological invariance into pooling layers using
PH, motivated by the observation that filtration operation in PH naturally
aligns graph pooling in a cut-off manner. In this fashion, message passing in
the coarsened graph acts along persistent pooled topology, leading to improved
performance. Experimentally, we apply our mechanism to a collection of graph
pooling methods and observe consistent and substantial performance gain over
several popular datasets, demonstrating its wide applicability and flexibility.</div><div><a href='http://arxiv.org/abs/2402.16346v1'>2402.16346v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14393v1")'>Graph Parsing Networks</div>
<div id='2402.14393v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:08:36Z</div><div>Authors: Yunchong Song, Siyuan Huang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin</div><div style='padding-top: 10px; width: 80ex'>Graph pooling compresses graph information into a compact representation.
State-of-the-art graph pooling methods follow a hierarchical approach, which
reduces the graph size step-by-step. These methods must balance memory
efficiency with preserving node information, depending on whether they use node
dropping or node clustering. Additionally, fixed pooling ratios or numbers of
pooling layers are predefined for all graphs, which prevents personalized
pooling structures from being captured for each individual graph. In this work,
inspired by bottom-up grammar induction, we propose an efficient graph parsing
algorithm to infer the pooling structure, which then drives graph pooling. The
resulting Graph Parsing Network (GPN) adaptively learns personalized pooling
structure for each individual graph. GPN benefits from the discrete assignments
generated by the graph parsing algorithm, allowing good memory efficiency while
preserving node information intact. Experimental results on standard benchmarks
demonstrate that GPN outperforms state-of-the-art graph pooling methods in
graph classification tasks while being able to achieve competitive performance
in node classification tasks. We also conduct a graph reconstruction task to
show GPN's ability to preserve node information and measure both memory and
time efficiency through relevant tests.</div><div><a href='http://arxiv.org/abs/2402.14393v1'>2402.14393v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09676v1")'>HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network</div>
<div id='2402.09676v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T03:05:45Z</div><div>Authors: Tatyana Benko, Martin Buck, Ilya Amburg, Stephen J. Young, Sinan G. Aksoy</div><div style='padding-top: 10px; width: 80ex'>In data science, hypergraphs are natural models for data exhibiting multi-way
relations, whereas graphs only capture pairwise. Nonetheless, many proposed
hypergraph neural networks effectively reduce hypergraphs to undirected graphs
via symmetrized matrix representations, potentially losing important
information. We propose an alternative approach to hypergraph neural networks
in which the hypergraph is represented as a non-reversible Markov chain. We use
this Markov chain to construct a complex Hermitian Laplacian matrix - the
magnetic Laplacian - which serves as the input to our proposed hypergraph
neural network. We study HyperMagNet for the task of node classification, and
demonstrate its effectiveness over graph-reduction based hypergraph neural
networks.</div><div><a href='http://arxiv.org/abs/2402.09676v1'>2402.09676v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11408v1")'>Layer-diverse Negative Sampling for Graph Neural Networks</div>
<div id='2403.11408v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T01:48:50Z</div><div>Authors: Wei Duan, Jie Lu, Yu Guang Wang, Junyu Xuan</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) are a powerful solution for various structure
learning applications due to their strong representation capabilities for graph
data. However, traditional GNNs, relying on message-passing mechanisms that
gather information exclusively from first-order neighbours (known as positive
samples), can lead to issues such as over-smoothing and over-squashing. To
mitigate these issues, we propose a layer-diverse negative sampling method for
message-passing propagation. This method employs a sampling matrix within a
determinantal point process, which transforms the candidate set into a space
and selectively samples from this space to generate negative samples. To
further enhance the diversity of the negative samples during each forward pass,
we develop a space-squeezing method to achieve layer-wise diversity in
multi-layer GNNs. Experiments on various real-world graph datasets demonstrate
the effectiveness of our approach in improving the diversity of negative
samples and overall learning performance. Moreover, adding negative samples
dynamically changes the graph's topology, thus with the strong potential to
improve the expressiveness of GNNs and reduce the risk of over-squashing.</div><div><a href='http://arxiv.org/abs/2403.11408v1'>2403.11408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09071v3")'>Rethinking Spectral Graph Neural Networks with Spatially Adaptive
  Filtering</div>
<div id='2401.09071v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T09:12:31Z</div><div>Authors: Jingwei Guo, Kaizhu Huang, Xinping Yi, Zixian Su, Rui Zhang</div><div style='padding-top: 10px; width: 80ex'>Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded
in the spectral domain, their practical reliance on polynomial approximation
implies a profound linkage to the spatial domain. As previous studies rarely
examine spectral GNNs from the spatial perspective, their spatial-domain
interpretability remains elusive, e.g., what information is essentially encoded
by spectral GNNs in the spatial domain? In this paper, to answer this question,
we establish a theoretical connection between spectral filtering and spatial
aggregation, unveiling an intrinsic interaction that spectral filtering
implicitly leads the original graph to an adapted new graph, explicitly
computed for spatial aggregation. Both theoretical and empirical investigations
reveal that the adapted new graph not only exhibits non-locality but also
accommodates signed edge weights to reflect label consistency among nodes.
These findings thus highlight the interpretable role of spectral GNNs in the
spatial domain and inspire us to rethink graph spectral filters beyond the
fixed-order polynomials, which neglect global information. Built upon the
theoretical findings, we revisit the state-of-the-art spectral GNNs and propose
a novel Spatially Adaptive Filtering (SAF) framework, which leverages the
adapted new graph by spectral filtering for an auxiliary non-local aggregation.
Notably, our proposed SAF comprehensively models both node similarity and
dissimilarity from a global perspective, therefore alleviating persistent
deficiencies of GNNs related to long-range dependencies and graph heterophily.
Extensive experiments over 13 node classification benchmarks demonstrate the
superiority of our proposed framework to the state-of-the-art models.</div><div><a href='http://arxiv.org/abs/2401.09071v3'>2401.09071v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04636v1")'>Entropy Aware Message Passing in Graph Neural Networks</div>
<div id='2403.04636v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T16:21:09Z</div><div>Authors: Philipp Nazari, Oliver Lemke, Davide Guidobene, Artiom Gesp</div><div style='padding-top: 10px; width: 80ex'>Deep Graph Neural Networks struggle with oversmoothing. This paper introduces
a novel, physics-inspired GNN model designed to mitigate this issue. Our
approach integrates with existing GNN architectures, introducing an
entropy-aware message passing term. This term performs gradient ascent on the
entropy during node aggregation, thereby preserving a certain degree of entropy
in the embeddings. We conduct a comparative analysis of our model against
state-of-the-art GNNs across various common datasets.</div><div><a href='http://arxiv.org/abs/2403.04636v1'>2403.04636v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03558v1")'>Path Signatures and Graph Neural Networks for Slow Earthquake Analysis:
  Better Together?</div>
<div id='2402.03558v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:16:05Z</div><div>Authors: Hans Riess, Manolis Veveakis, Michael M. Zavlanos</div><div style='padding-top: 10px; width: 80ex'>The path signature, having enjoyed recent success in the machine learning
community, is a theoretically-driven method for engineering features from
irregular paths. On the other hand, graph neural networks (GNN), neural
architectures for processing data on graphs, excel on tasks with irregular
domains, such as sensor networks. In this paper, we introduce a novel approach,
Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path
signatures into graph convolutional neural networks (GCNN), and leveraging the
strengths of both path signatures, for feature extraction, and GCNNs, for
handling spatial interactions. We apply our method to analyze slow earthquake
sequences, also called slow slip events (SSE), utilizing data from GPS
timeseries, with a case study on a GPS sensor network on the east coast of New
Zealand's north island. We also establish benchmarks for our method on
simulated stochastic differential equations, which model similar
reaction-diffusion phenomenon. Our methodology shows promise for future
advancement in earthquake prediction and sensor network analysis.</div><div><a href='http://arxiv.org/abs/2402.03558v1'>2402.03558v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07152v1")'>Explainable Global Wildfire Prediction Models using Graph Neural
  Networks</div>
<div id='2402.07152v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T10:44:41Z</div><div>Authors: Dayou Chen, Sibo Cheng, Jinwei Hu, Matthew Kasoar, Rossella Arcucci</div><div style='padding-top: 10px; width: 80ex'>Wildfire prediction has become increasingly crucial due to the escalating
impacts of climate change. Traditional CNN-based wildfire prediction models
struggle with handling missing oceanic data and addressing the long-range
dependencies across distant regions in meteorological data. In this paper, we
introduce an innovative Graph Neural Network (GNN)-based model for global
wildfire prediction. We propose a hybrid model that combines the spatial
prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long
Short-Term Memory (LSTM) networks. Our approach uniquely transforms global
climate and wildfire data into a graph representation, addressing challenges
such as null oceanic data locations and long-range dependencies inherent in
traditional models. Benchmarking against established architectures using an
unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior
predictive accuracy. Furthermore, we emphasise the model's explainability,
unveiling potential wildfire correlation clusters through community detection
and elucidating feature importance via Integrated Gradient analysis. Our
findings not only advance the methodological domain of wildfire prediction but
also underscore the importance of model transparency, offering valuable
insights for stakeholders in wildfire management.</div><div><a href='http://arxiv.org/abs/2402.07152v1'>2402.07152v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11199v1")'>Graph Unitary Message Passing</div>
<div id='2403.11199v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T12:55:23Z</div><div>Authors: Haiquan Qiu, Yatao Bian, Quanming Yao</div><div style='padding-top: 10px; width: 80ex'>Message passing mechanism contributes to the success of GNNs in various
applications, but also brings the oversquashing problem. Recent works combat
oversquashing by improving the graph spectrums with rewiring techniques,
disrupting the structural bias in graphs, and having limited improvement on
oversquashing in terms of oversquashing measure. Motivated by unitary RNN, we
propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs
by applying unitary adjacency matrix for message passing. To design GUMP, a
transformation is first proposed to make general graphs have unitary adjacency
matrix and keep its structural bias. Then, unitary adjacency matrix is obtained
with a unitary projection algorithm, which is implemented by utilizing the
intrinsic structure of unitary adjacency matrix and allows GUMP to be
permutation-equivariant. Experimental results show the effectiveness of GUMP in
improving the performance on various graph learning tasks.</div><div><a href='http://arxiv.org/abs/2403.11199v1'>2403.11199v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04874v1")'>Feature Network Methods in Machine Learning and Applications</div>
<div id='2401.04874v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T01:57:12Z</div><div>Authors: Xinying Mu, Mark Kon</div><div style='padding-top: 10px; width: 80ex'>A machine learning (ML) feature network is a graph that connects ML features
in learning tasks based on their similarity. This network representation allows
us to view feature vectors as functions on the network. By leveraging function
operations from Fourier analysis and from functional analysis, one can easily
generate new and novel features, making use of the graph structure imposed on
the feature vectors. Such network structures have previously been studied
implicitly in image processing and computational biology. We thus describe
feature networks as graph structures imposed on feature vectors, and provide
applications in machine learning. One application involves graph-based
generalizations of convolutional neural networks, involving structured deep
learning with hierarchical representations of features that have varying depth
or complexity. This extends also to learning algorithms that are able to
generate useful new multilevel features. Additionally, we discuss the use of
feature networks to engineer new features, which can enhance the expressiveness
of the model. We give a specific example of a deep tree-structured feature
network, where hierarchical connections are formed through feature clustering
and feed-forward learning. This results in low learning complexity and
computational efficiency. Unlike "standard" neural features which are limited
to modulated (thresholded) linear combinations of adjacent ones, feature
networks offer more general feedforward dependencies among features. For
example, radial basis functions or graph structure-based dependencies between
features can be utilized.</div><div><a href='http://arxiv.org/abs/2401.04874v1'>2401.04874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02441v4")'>TopoX: A Suite of Python Packages for Machine Learning on Topological
  Domains</div>
<div id='2402.02441v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:41:40Z</div><div>Authors: Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Agerberg, Ibrahem AlJabea, Ruben Ballester, Claudio Battiloro, Guillermo Bernárdez, Tolga Birdal, Aiden Brent, Peter Chin, Sergio Escalera, Simone Fiorellino, Odin Hoff Gardaa, Gurusankar Gopalakrishnan, Devendra Govil, Josef Hoppe, Maneel Reddy Karri, Jude Khouja, Manuel Lecha, Neal Livesay, Jan Meißner, Soham Mukherjee, Alexander Nikitin, Theodore Papamarkou, Jaro Prílepok, Karthikeyan Natesan Ramamurthy, Paul Rosen, Aldo Guzmán-Sáenz, Alessandro Salatiello, Shreyas N. Samaga, Simone Scardapane, Michael T. Schaub, Luca Scofano, Indro Spinelli, Lev Telyatnikov, Quang Truong, Robin Walters, Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali Zia, Nina Miolane</div><div style='padding-top: 10px; width: 80ex'>We introduce TopoX, a Python software suite that provides reliable and
user-friendly building blocks for computing and machine learning on topological
domains that extend graphs: hypergraphs, simplicial, cellular, path and
combinatorial complexes. TopoX consists of three packages: TopoNetX facilitates
constructing and computing on these domains, including working with nodes,
edges and higher-order cells; TopoEmbedX provides methods to embed topological
domains into vector spaces, akin to popular graph-based embedding algorithms
such as node2vec; TopoModelx is built on top of PyTorch and offers a
comprehensive toolbox of higher-order message passing functions for neural
networks on topological domains. The extensively documented and unit-tested
source code of TopoX is available under MIT license at
https://pyt-team.github.io/.</div><div><a href='http://arxiv.org/abs/2402.02441v4'>2402.02441v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02407v1")'>Defining Neural Network Architecture through Polytope Structures of
  Dataset</div>
<div id='2402.02407v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T08:57:42Z</div><div>Authors: Sangmin Lee, Abbas Mammadov, Jong Chul Ye</div><div style='padding-top: 10px; width: 80ex'>Current theoretical and empirical research in neural networks suggests that
complex datasets require large network architectures for thorough
classification, yet the precise nature of this relationship remains unclear.
This paper tackles this issue by defining upper and lower bounds for neural
network widths, which are informed by the polytope structure of the dataset in
question. We also delve into the application of these principles to simplicial
complexes and specific manifold shapes, explaining how the requirement for
network width varies in accordance with the geometric complexity of the
dataset. Moreover, we develop an algorithm to investigate a converse situation
where the polytope structure of a dataset can be inferred from its
corresponding trained neural networks. Through our algorithm, it is established
that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be
efficiently encapsulated using no more than two polytopes with a small number
of faces.</div><div><a href='http://arxiv.org/abs/2402.02407v1'>2402.02407v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15083v1")'>SIMAP: A simplicial-map layer for neural networks</div>
<div id='2403.15083v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T10:06:42Z</div><div>Authors: Rocio Gonzalez-Diaz, Miguel A. Gutiérrez-Naranjo, Eduardo Paluzo-Hidalgo</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present SIMAP, a novel layer integrated into deep learning
models, aimed at enhancing the interpretability of the output. The SIMAP layer
is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an
explainable neural network based on support sets and simplicial maps (functions
used in topology to transform shapes while preserving their structural
connectivity). The novelty of the methodology proposed in this paper is
two-fold: Firstly, SIMAP layers work in combination with other deep learning
architectures as an interpretable layer substituting classic dense final
layers. Secondly, unlike SMNNs, the support set is based on a fixed maximal
simplex, the barycentric subdivision being efficiently computed with a
matrix-based multiplication algorithm.</div><div><a href='http://arxiv.org/abs/2403.15083v1'>2403.15083v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11188v1")'>Fast and Exact Enumeration of Deep Networks Partitions Regions</div>
<div id='2401.11188v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T09:51:52Z</div><div>Authors: Randall Balestriero, Yann LeCun</div><div style='padding-top: 10px; width: 80ex'>One fruitful formulation of Deep Networks (DNs) enabling their theoretical
study and providing practical guidelines to practitioners relies on Piecewise
Affine Splines. In that realm, a DN's input-mapping is expressed as per-region
affine mapping where those regions are implicitly determined by the model's
architecture and form a partition of their input space. That partition -- which
is involved in all the results spanned from this line of research -- has so far
only been computed on $2/3$-dimensional slices of the DN's input space or
estimated by random sampling. In this paper, we provide the first parallel
algorithm that does exact enumeration of the DN's partition regions. The
proposed algorithm enables one to finally assess the closeness of the commonly
employed approximations methods, e.g. based on random sampling of the DN input
space. One of our key finding is that if one is only interested in regions with
``large'' volume, then uniform sampling of the space is highly efficient, but
that if one is also interested in discovering the ``small'' regions of the
partition, then uniform sampling is exponentially costly with the DN's input
space dimension. On the other hand, our proposed method has complexity scaling
linearly with input dimension and the number of regions.</div><div><a href='http://arxiv.org/abs/2401.11188v1'>2401.11188v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01813v1")'>Signal Processing in the Retina: Interpretable Graph Classifier to
  Predict Ganglion Cell Responses</div>
<div id='2401.01813v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T16:15:22Z</div><div>Authors: Yasaman Parhizkar, Gene Cheung, Andrew W. Eckford</div><div style='padding-top: 10px; width: 80ex'>It is a popular hypothesis in neuroscience that ganglion cells in the retina
are activated by selectively detecting visual features in an observed scene.
While ganglion cell firings can be predicted via data-trained deep neural nets,
the networks remain indecipherable, thus providing little understanding of the
cells' underlying operations. To extract knowledge from the cell firings, in
this paper we learn an interpretable graph-based classifier from data to
predict the firings of ganglion cells in response to visual stimuli.
Specifically, we learn a positive semi-definite (PSD) metric matrix $\mathbf{M}
\succeq 0$ that defines Mahalanobis distances between graph nodes (visual
events) endowed with pre-computed feature vectors; the computed inter-node
distances lead to edge weights and a combinatorial graph that is amenable to
binary classification. Mathematically, we define the objective of metric matrix
$\mathbf{M}$ optimization using a graph adaptation of large margin nearest
neighbor (LMNN), which is rewritten as a semi-definite programming (SDP)
problem. We solve it efficiently via a fast approximation called Gershgorin
disc perfect alignment (GDPA) linearization. The learned metric matrix
$\mathbf{M}$ provides interpretability: important features are identified along
$\mathbf{M}$'s diagonal, and their mutual relationships are inferred from
off-diagonal terms. Our fast metric learning framework can be applied to other
biological systems with pre-chosen features that require interpretation.</div><div><a href='http://arxiv.org/abs/2401.01813v1'>2401.01813v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08811v1")'>Deep and shallow data science for multi-scale optical neuroscience</div>
<div id='2402.08811v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T21:30:44Z</div><div>Authors: Gal Mishne, Adam Charles</div><div style='padding-top: 10px; width: 80ex'>Optical imaging of the brain has expanded dramatically in the past two
decades. New optics, indicators, and experimental paradigms are now enabling
in-vivo imaging from the synaptic to the cortex-wide scales. To match the
resulting flood of data across scales, computational methods are continuously
being developed to meet the need of extracting biologically relevant
information. In this pursuit, challenges arise in some domains (e.g., SNR and
resolution limits in micron-scale data) that require specialized algorithms.
These algorithms can, for example, make use of state-of-the-art machine
learning to maximally learn the details of a given scale to optimize the
processing pipeline. In contrast, other methods, however, such as graph signal
processing, seek to abstract away from some of the details that are
scale-specific to provide solutions to specific sub-problems common across
scales of neuroimaging. Here we discuss limitations and tradeoffs in
algorithmic design with the goal of identifying how data quality and
variability can hamper algorithm use and dissemination.</div><div><a href='http://arxiv.org/abs/2402.08811v1'>2402.08811v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12307v1")'>Molecular Classification Using Hyperdimensional Graph Classification</div>
<div id='2403.12307v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T23:16:17Z</div><div>Authors: Pere Verges, Igor Nunes, Mike Heddes, Tony Givargis, Alexandru Nicolau</div><div style='padding-top: 10px; width: 80ex'>Our work introduces an innovative approach to graph learning by leveraging
Hyperdimensional Computing. Graphs serve as a widely embraced method for
conveying information, and their utilization in learning has gained significant
attention. This is notable in the field of chemoinformatics, where learning
from graph representations plays a pivotal role. An important application
within this domain involves the identification of cancerous cells across
diverse molecular structures.
  We propose an HDC-based model that demonstrates comparable Area Under the
Curve results when compared to state-of-the-art models like Graph Neural
Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it
outperforms previously proposed hyperdimensional computing graph learning
methods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x
acceleration in the training phase and a 15x improvement in inference time
compared to GNN and WL models. This not only underscores the efficacy of the
HDC-based method, but also highlights its potential for expedited and
resource-efficient graph learning.</div><div><a href='http://arxiv.org/abs/2403.12307v1'>2403.12307v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18610v1")'>Why Attention Graphs Are All We Need: Pioneering Hierarchical
  Classification of Hematologic Cell Populations with LeukoGraph</div>
<div id='2402.18610v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:10:25Z</div><div>Authors: Fatemeh Nassajian Mojarrad, Lorenzo Bini, Thomas Matthes, Stéphane Marchand-Maillet</div><div style='padding-top: 10px; width: 80ex'>In the complex landscape of hematologic samples such as peripheral blood or
bone marrow, cell classification, delineating diverse populations into a
hierarchical structure, presents profound challenges. This study presents
LeukoGraph, a recently developed framework designed explicitly for this purpose
employing graph attention networks (GATs) to navigate hierarchical
classification (HC) complexities. Notably, LeukoGraph stands as a pioneering
effort, marking the application of graph neural networks (GNNs) for
hierarchical inference on graphs, accommodating up to one million nodes and
millions of edges, all derived from flow cytometry data. LeukoGraph intricately
addresses a classification paradigm where for example four different cell
populations undergo flat categorization, while a fifth diverges into two
distinct child branches, exemplifying the nuanced hierarchical structure
inherent in complex datasets. The technique is more general than this example.
A hallmark achievement of LeukoGraph is its F-score of 98%, significantly
outclassing prevailing state-of-the-art methodologies. Crucially, LeukoGraph's
prowess extends beyond theoretical innovation, showcasing remarkable precision
in predicting both flat and hierarchical cell types across flow cytometry
datasets from 30 distinct patients. This precision is further underscored by
LeukoGraph's ability to maintain a correct label ratio, despite the inherent
challenges posed by hierarchical classifications.</div><div><a href='http://arxiv.org/abs/2402.18610v1'>2402.18610v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18611v1")'>HemaGraph: Breaking Barriers in Hematologic Single Cell Classification
  with Graph Attention</div>
<div id='2402.18611v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:15:38Z</div><div>Authors: Lorenzo Bini, Fatemeh Nassajian Mojarrad, Thomas Matthes, Stéphane Marchand-Maillet</div><div style='padding-top: 10px; width: 80ex'>In the realm of hematologic cell populations classification, the intricate
patterns within flow cytometry data necessitate advanced analytical tools. This
paper presents 'HemaGraph', a novel framework based on Graph Attention Networks
(GATs) for single-cell multi-class classification of hematological cells from
flow cytometry data. Harnessing the power of GATs, our method captures subtle
cell relationships, offering highly accurate patient profiling. Based on
evaluation of data from 30 patients, HemaGraph demonstrates classification
performance across five different cell classes, outperforming traditional
methodologies and state-of-the-art methods. Moreover, the uniqueness of this
framework lies in the training and testing phase of HemaGraph, where it has
been applied for extremely large graphs, containing up to hundreds of thousands
of nodes and two million edges, to detect low frequency cell populations (e.g.
0.01% for one population), with accuracies reaching 98%. Our findings
underscore the potential of HemaGraph in improving hematoligic multi-class
classification, paving the way for patient-personalized interventions. To the
best of our knowledge, this is the first effort to use GATs, and Graph Neural
Networks (GNNs) in general, to classify cell populations from single-cell flow
cytometry data. We envision applying this method to single-cell data from
larger cohort of patients and on other hematologic diseases.</div><div><a href='http://arxiv.org/abs/2402.18611v1'>2402.18611v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00024v1")'>FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class
  Classification in Flow Cytometry Benchmarking</div>
<div id='2403.00024v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T15:01:59Z</div><div>Authors: Lorenzo Bini, Fatemeh Nassajian Mojarrad, Margarita Liarou, Thomas Matthes, Stéphane Marchand-Maillet</div><div style='padding-top: 10px; width: 80ex'>This paper presents FlowCyt, the first comprehensive benchmark for
multi-class single-cell classification in flow cytometry data. The dataset
comprises bone marrow samples from 30 patients, with each cell characterized by
twelve markers. Ground truth labels identify five hematological cell types: T
lymphocytes, B lymphocytes, Monocytes, Mast cells, and Hematopoietic
Stem/Progenitor Cells (HSPCs). Experiments utilize supervised inductive
learning and semi-supervised transductive learning on up to 1 million cells per
patient. Baseline methods include Gaussian Mixture Models, XGBoost, Random
Forests, Deep Neural Networks, and Graph Neural Networks (GNNs). GNNs
demonstrate superior performance by exploiting spatial relationships in
graph-encoded data. The benchmark allows standardized evaluation of clinically
relevant classification tasks, along with exploratory analyses to gain insights
into hematological cell phenotypes. This represents the first public flow
cytometry benchmark with a richly annotated, heterogeneous dataset. It will
empower the development and rigorous assessment of novel methodologies for
single-cell analysis.</div><div><a href='http://arxiv.org/abs/2403.00024v1'>2403.00024v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19296v1")'>An AI based Digital Score of Tumour-Immune Microenvironment Predicts
  Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric
  Adenocarcinoma</div>
<div id='2402.19296v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:59:42Z</div><div>Authors: Quoc Dang Vu, Caroline Fong, Anderley Gordon, Tom Lund, Tatiany L Silveira, Daniel Rodrigues, Katharina von Loga, Shan E Ahmed Raza, David Cunningham, Nasir Rajpoot</div><div style='padding-top: 10px; width: 80ex'>Gastric and oesophageal (OG) cancers are the leading causes of cancer
mortality worldwide. In OG cancers, recent studies have showed that PDL1 immune
checkpoint inhibitors (ICI) in combination with chemotherapy improves patient
survival. However, our understanding of the tumour immune microenvironment in
OG cancers remains limited. In this study, we interrogate multiplex
immunofluorescence (mIF) images taken from patients with advanced
Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine
and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict
the efficacy of the treatment and to explore the biological basis of patients
responding to maintenance durvalumab (PDL1 inhibitor). Our proposed Artificial
Intelligence (AI) based marker successfully identified responder from
non-responder (p &lt; 0.05) as well as those who could potentially benefit from
ICI with statistical significance (p &lt; 0.05) for both progression free and
overall survival. Our findings suggest that T cells that express FOXP3 seem to
heavily influence the patient treatment response and survival outcome. We also
observed that higher levels of CD8+PD1+ cells are consistently linked to poor
prognosis for both OS and PFS, regardless of ICI.</div><div><a href='http://arxiv.org/abs/2402.19296v1'>2402.19296v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08738v2")'>Machine Learning-Based Analysis of Ebola Virus' Impact on Gene
  Expression in Nonhuman Primates</div>
<div id='2401.08738v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:31:23Z</div><div>Authors: Mostafa Rezapour, Muhammad Khalid Khan Niazi, Hao Lu, Aarthi Narayanan, Metin Nafi Gurcan</div><div style='padding-top: 10px; width: 80ex'>This study introduces the Supervised Magnitude-Altitude Scoring (SMAS)
methodology, a machine learning-based approach, for analyzing gene expression
data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV).
We utilize a comprehensive dataset of NanoString gene expression profiles from
Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen
interaction analysis. SMAS effectively combines gene selection based on
statistical significance and expression changes, employing linear classifiers
such as logistic regression to accurately differentiate between RT-qPCR
positive and negative NHP samples. A key finding of our research is the
identification of IFI6 and IFI27 as critical biomarkers, demonstrating
exceptional predictive performance with 100% accuracy and Area Under the Curve
(AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6
and IFI27, genes, including MX1, OAS1, and ISG15, were significantly
upregulated, highlighting their essential roles in the immune response to EBOV.
Our results underscore the efficacy of the SMAS method in revealing complex
genetic interactions and response mechanisms during EBOV infection. This
research provides valuable insights into EBOV pathogenesis and aids in
developing more precise diagnostic tools and therapeutic strategies to address
EBOV infection in particular and viral infection in general.</div><div><a href='http://arxiv.org/abs/2401.08738v2'>2401.08738v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17178v1")'>GraphViz2Vec: A Structure-aware Feature Generation Model to Improve
  Classification in GNNs</div>
<div id='2401.17178v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:11:04Z</div><div>Authors: Shraban Kumar Chatterjee, Suman Kundu</div><div style='padding-top: 10px; width: 80ex'>GNNs are widely used to solve various tasks including node classification and
link prediction. Most of the GNN architectures assume the initial embedding to
be random or generated from popular distributions. These initial embeddings
require multiple layers of transformation to converge into a meaningful latent
representation. While number of layers allow accumulation of larger
neighbourhood of a node it also introduce the problem of over-smoothing. In
addition, GNNs are inept at representing structural information. For example,
the output embedding of a node does not capture its triangles participation. In
this paper, we presented a novel feature extraction methodology GraphViz2Vec
that can capture the structural information of a node's local neighbourhood to
create meaningful initial embeddings for a GNN model. These initial embeddings
helps existing models achieve state-of-the-art results in various
classification tasks. Further, these initial embeddings help the model to
produce desired results with only two layers which in turn reduce the problem
of over-smoothing. The initial encoding of a node is obtained from an image
classification model trained on multiple energy diagrams of its local
neighbourhood. These energy diagrams are generated with the induced sub-graph
of the nodes traversed by multiple random walks. The generated encodings
increase the performance of existing models on classification tasks (with a
mean increase of $4.65\%$ and $2.58\%$ for the node and link classification
tasks, respectively), with some models achieving state-of-the-art results.</div><div><a href='http://arxiv.org/abs/2401.17178v1'>2401.17178v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13781v1")'>Sparse Implementation of Versatile Graph-Informed Layers</div>
<div id='2403.13781v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T17:43:58Z</div><div>Authors: Francesco Della Santa</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have emerged as effective tools for learning
tasks on graph-structured data. Recently, Graph-Informed (GI) layers were
introduced to address regression tasks on graph nodes, extending their
applicability beyond classic GNNs. However, existing implementations of GI
layers lack efficiency due to dense memory allocation. This paper presents a
sparse implementation of GI layers, leveraging the sparsity of adjacency
matrices to reduce memory usage significantly. Additionally, a versatile
general form of GI layers is introduced, enabling their application to subsets
of graph nodes. The proposed sparse implementation improves the concrete
computational efficiency and scalability of the GI layers, permitting to build
deeper Graph-Informed Neural Networks (GINNs) and facilitating their
scalability to larger graphs.</div><div><a href='http://arxiv.org/abs/2403.13781v1'>2403.13781v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13268v1")'>Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural
  Network</div>
<div id='2403.13268v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T03:07:30Z</div><div>Authors: Ningyi Liao, Zihao Yu, Siqiang Luo</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have shown promising performance in various
graph learning tasks, but at the cost of resource-intensive computations. The
primary overhead of GNN update stems from graph propagation and weight
transformation, both involving operations on graph-scale matrices. Previous
studies attempt to reduce the computational budget by leveraging graph-level or
network-level sparsification techniques, resulting in downsized graph or
weights. In this work, we propose Unifews, which unifies the two operations in
an entry-wise manner considering individual matrix elements, and conducts joint
edge-weight sparsification to enhance learning efficiency. The entry-wise
design of Unifews enables adaptive compression across GNN layers with
progressively increased sparsity, and is applicable to a variety of
architectural designs with on-the-fly operation simplification. Theoretically,
we establish a novel framework to characterize sparsified GNN learning in view
of a graph optimization process, and prove that Unifews effectively
approximates the learning objective with bounded error and reduced
computational load. We conduct extensive experiments to evaluate the
performance of our method in diverse settings. Unifews is advantageous in
jointly removing more than 90% of edges and weight entries with comparable or
better accuracy than baseline models. The sparsification offers remarkable
efficiency improvements including 10-20x matrix operation reduction and up to
100x acceleration in graph propagation time for the largest graph at the
billion-edge scale.</div><div><a href='http://arxiv.org/abs/2403.13268v1'>2403.13268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14853v1")'>iSpLib: A Library for Accelerating Graph Neural Networks using
  Auto-tuned Sparse Operations</div>
<div id='2403.14853v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T21:56:44Z</div><div>Authors: Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, Ariful Azad</div><div style='padding-top: 10px; width: 80ex'>Core computations in Graph Neural Network (GNN) training and inference are
often mapped to sparse matrix operations such as sparse-dense matrix
multiplication (SpMM). These sparse operations are harder to optimize by manual
tuning because their performance depends significantly on the sparsity of input
graphs, GNN models, and computing platforms. To address this challenge, we
present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse
operations. iSpLib expedites GNN training with a cache-enabled backpropagation
that stores intermediate matrices in local caches. The library offers a
user-friendly Python plug-in that allows users to take advantage of our
optimized PyTorch operations out-of-the-box for any existing linear
algebra-based PyTorch implementation of popular GNNs (Graph Convolution
Network, GraphSAGE, Graph Inference Network, etc.) with only two lines of
additional code. We demonstrate that iSpLib obtains up to 27x overall training
speedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0
implementations on the CPU. Our library is publicly available at
https://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511).</div><div><a href='http://arxiv.org/abs/2403.14853v1'>2403.14853v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08296v1")'>Multi-Level GNN Preconditioner for Solving Large Scale Problems</div>
<div id='2402.08296v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T08:50:14Z</div><div>Authors: Matthieu Nastorg, Jean-Marc Gratien, Thibault Faney, Michele Alessandro Bucci, Guillaume Charpiat, Marc Schoenauer</div><div style='padding-top: 10px; width: 80ex'>Large-scale numerical simulations often come at the expense of daunting
computations. High-Performance Computing has enhanced the process, but adapting
legacy codes to leverage parallel GPU computations remains challenging.
Meanwhile, Machine Learning models can harness GPU computations effectively but
often struggle with generalization and accuracy. Graph Neural Networks (GNNs),
in particular, are great for learning from unstructured data like meshes but
are often limited to small-scale problems. Moreover, the capabilities of the
trained model usually restrict the accuracy of the data-driven solution. To
benefit from both worlds, this paper introduces a novel preconditioner
integrating a GNN model within a multi-level Domain Decomposition framework.
The proposed GNN-based preconditioner is used to enhance the efficiency of a
Krylov method, resulting in a hybrid solver that can converge with any desired
level of accuracy. The efficiency of the Krylov method greatly benefits from
the GNN preconditioner, which is adaptable to meshes of any size and shape, is
executed on GPUs, and features a multi-level approach to enforce the
scalability of the entire process. Several experiments are conducted to
validate the numerical behavior of the hybrid solver, and an in-depth analysis
of its performance is proposed to assess its competitiveness against a C++
legacy solver.</div><div><a href='http://arxiv.org/abs/2402.08296v1'>2402.08296v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06885v1")'>Accelerating Neural Networks for Large Language Models and Graph
  Processing with Silicon Photonics</div>
<div id='2401.06885v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T20:32:38Z</div><div>Authors: Salma Afifi, Febin Sunny, Mahdi Nikdast, Sudeep Pasricha</div><div style='padding-top: 10px; width: 80ex'>In the rapidly evolving landscape of artificial intelligence, large language
models (LLMs) and graph processing have emerged as transformative technologies
for natural language processing (NLP), computer vision, and graph-structured
data applications. However, the complex structures of these models pose
challenges for acceleration on conventional electronic platforms. In this
paper, we describe novel hardware accelerators based on silicon photonics to
accelerate transformer neural networks that are used in LLMs and graph neural
networks for graph data processing. Our analysis demonstrates that both
hardware accelerators achieve at least 10.2x throughput improvement and 3.8x
better energy efficiency over multiple state-of-the-art electronic hardware
accelerators designed for LLMs and graph processing.</div><div><a href='http://arxiv.org/abs/2401.06885v1'>2401.06885v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16731v1")'>Accelerating Graph Neural Networks on Real Processing-In-Memory Systems</div>
<div id='2402.16731v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T16:52:35Z</div><div>Authors: Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML framework that accelerates GNNs
on real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively, to match
their algorithmic nature. We extensively evaluate PyGim on a real-world PIM
system with 1992 PIM cores using emerging GNN models, and demonstrate that it
outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average
3.04x, and achieves higher resource utilization than CPU and GPU systems. Our
work provides useful recommendations for software, system and hardware
designers. PyGim will be open-sourced to enable the widespread use of PIM
systems in GNNs.</div><div><a href='http://arxiv.org/abs/2402.16731v1'>2402.16731v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05610v1")'>Graph Q-Learning for Combinatorial Optimization</div>
<div id='2401.05610v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T01:15:28Z</div><div>Authors: Victoria M. Dax, Jiachen Li, Kevin Leahy, Mykel J. Kochenderfer</div><div style='padding-top: 10px; width: 80ex'>Graph-structured data is ubiquitous throughout natural and social sciences,
and Graph Neural Networks (GNNs) have recently been shown to be effective at
solving prediction and inference problems on graph data. In this paper, we
propose and demonstrate that GNNs can be applied to solve Combinatorial
Optimization (CO) problems. CO concerns optimizing a function over a discrete
solution space that is often intractably large. To learn to solve CO problems,
we formulate the optimization process as a sequential decision making problem,
where the return is related to how close the candidate solution is to
optimality. We use a GNN to learn a policy to iteratively build increasingly
promising candidate solutions. We present preliminary evidence that GNNs
trained through Q-Learning can solve CO problems with performance approaching
state-of-the-art heuristic-based solvers, using only a fraction of the
parameters and training time.</div><div><a href='http://arxiv.org/abs/2401.05610v1'>2401.05610v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06026v2")'>Towards a Generic Representation of Combinatorial Problems for
  Learning-Based Approaches</div>
<div id='2403.06026v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T22:28:46Z</div><div>Authors: Léo Boisvert, Hélène Verhaeghe, Quentin Cappart</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a growing interest in using learning-based
approaches for solving combinatorial problems, either in an end-to-end manner
or in conjunction with traditional optimization algorithms. In both scenarios,
the challenge lies in encoding the targeted combinatorial problems into a
structure compatible with the learning algorithm. Many existing works have
proposed problem-specific representations, often in the form of a graph, to
leverage the advantages of \textit{graph neural networks}. However, these
approaches lack generality, as the representation cannot be easily transferred
from one combinatorial problem to another one. While some attempts have been
made to bridge this gap, they still offer a partial generality only. In
response to this challenge, this paper advocates for progress toward a fully
generic representation of combinatorial problems for learning-based approaches.
The approach we propose involves constructing a graph by breaking down any
constraint of a combinatorial problem into an abstract syntax tree and
expressing relationships (e.g., a variable involved in a constraint) through
the edges. Furthermore, we introduce a graph neural network architecture
capable of efficiently learning from this representation. The tool provided
operates on combinatorial problems expressed in the XCSP3 format, handling all
the constraints available in the 2023 mini-track competition. Experimental
results on four combinatorial problems demonstrate that our architecture
achieves performance comparable to dedicated architectures while maintaining
generality. Our code and trained models are publicly available at
\url{https://github.com/corail-research/learning-generic-csp}.</div><div><a href='http://arxiv.org/abs/2403.06026v2'>2403.06026v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13744v1")'>Reasoning Algorithmically in Graph Neural Networks</div>
<div id='2402.13744v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:16:51Z</div><div>Authors: Danilo Numeroso</div><div style='padding-top: 10px; width: 80ex'>The development of artificial intelligence systems with advanced reasoning
capabilities represents a persistent and long-standing research question.
Traditionally, the primary strategy to address this challenge involved the
adoption of symbolic approaches, where knowledge was explicitly represented by
means of symbols and explicitly programmed rules. However, with the advent of
machine learning, there has been a paradigm shift towards systems that can
autonomously learn from data, requiring minimal human guidance. In light of
this shift, in latest years, there has been increasing interest and efforts at
endowing neural networks with the ability to reason, bridging the gap between
data-driven learning and logical reasoning. Within this context, Neural
Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to
integrate the structured and rule-based reasoning of algorithms with the
adaptive learning capabilities of neural networks, typically by tasking neural
models to mimic classical algorithms. In this dissertation, we provide
theoretical and practical contributions to this area of research. We explore
the connections between neural networks and tropical algebra, deriving powerful
architectures that are aligned with algorithm execution. Furthermore, we
discuss and show the ability of such neural reasoners to learn and manipulate
complex algorithmic and combinatorial optimization concepts, such as the
principle of strong duality. Finally, in our empirical efforts, we validate the
real-world utility of NAR networks across different practical scenarios. This
includes tasks as diverse as planning problems, large-scale edge classification
tasks and the learning of polynomial-time approximate algorithms for NP-hard
combinatorial problems. Through this exploration, we aim to showcase the
potential integrating algorithmic reasoning in machine learning models.</div><div><a href='http://arxiv.org/abs/2402.13744v1'>2402.13744v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01107v1")'>Simulation of Graph Algorithms with Looped Transformers</div>
<div id='2402.01107v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:48:03Z</div><div>Authors: Artur Back de Luca, Kimon Fountoulakis</div><div style='padding-top: 10px; width: 80ex'>The execution of graph algorithms using neural networks has recently
attracted significant interest due to promising empirical progress. This
motivates further understanding of how neural networks can replicate reasoning
steps with relational data. In this work, we study the ability of transformer
networks to simulate algorithms on graphs from a theoretical perspective. The
architecture that we utilize is a looped transformer with extra attention heads
that interact with the graph. We prove by construction that this architecture
can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth-
and Depth-First Search, and Kosaraju's strongly connected components algorithm.
The width of the network does not increase with the size of the input graph,
which implies that the network can simulate the above algorithms for any graph.
Despite this property, we show that there is a limit to simulation in our
solution due to finite precision. Finally, we show a Turing Completeness result
with constant width when the extra attention heads are utilized.</div><div><a href='http://arxiv.org/abs/2402.01107v1'>2402.01107v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11734v1")'>Learning General Policies for Classical Planning Domains: Getting Beyond
  C$_2$</div>
<div id='2403.11734v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T12:42:53Z</div><div>Authors: Simon Ståhlberg, Blai Bonet, Hector Geffner</div><div style='padding-top: 10px; width: 80ex'>GNN-based approaches for learning general policies across planning domains
are limited by the expressive power of $C_2$, namely; first-order logic with
two variables and counting. This limitation can be overcomed by transitioning
to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet
embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$-
and $2$-GNNs that are confined to $C_2$, they require quartic time for message
exchange and cubic space for embeddings, rendering them impractical. In this
work, we introduce a parameterized version of relational GNNs. When $t$ is
infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for
embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$]
achieves a weaker approximation by exchanging fewer messages, yet
interestingly, often yield the $C_3$ features required in several planning
domains. Furthermore, the new R-GNN[$t$] architecture is the original R-GNN
architecture with a suitable transformation applied to the input states only.
Experimental results illustrate the clear performance gains of R-GNN[$1$] and
R-GNN[$2$] over plain R-GNNs, and also over edge transformers that also
approximate $3$-GNNs.</div><div><a href='http://arxiv.org/abs/2403.11734v1'>2403.11734v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04874v1")'>Choosing a Classical Planner with Graph Neural Networks</div>
<div id='2402.04874v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T13:04:27Z</div><div>Authors: Jana Vatter, Ruben Mayer, Hans-Arno Jacobsen, Horst Samulowitz, Michael Katz</div><div style='padding-top: 10px; width: 80ex'>Online planner selection is the task of choosing a solver out of a predefined
set for a given planning problem. As planning is computationally hard, the
performance of solvers varies greatly on planning problems. Thus, the ability
to predict their performance on a given problem is of great importance. While a
variety of learning methods have been employed, for classical cost-optimal
planning the prevailing approach uses Graph Neural Networks (GNNs). In this
work, we continue the line of work on using GNNs for online planner selection.
We perform a thorough investigation of the impact of the chosen GNN model,
graph representation and node features, as well as prediction task. Going
further, we propose using the graph representation obtained by a GNN as an
input to the Extreme Gradient Boosting (XGBoost) model, resulting in a more
resource-efficient yet accurate approach. We show the effectiveness of a
variety of GNN-based online planner selection methods, opening up new exciting
avenues for research on online planner selection.</div><div><a href='http://arxiv.org/abs/2402.04874v1'>2402.04874v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12470v1")'>Reinforcement Learning for Graph Coloring: Understanding the Power and
  Limits of Non-Label Invariant Representations</div>
<div id='2401.12470v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T03:43:34Z</div><div>Authors: Chase Cummins, Richard Veras</div><div style='padding-top: 10px; width: 80ex'>Register allocation is one of the most important problems for modern
compilers. With a practically unlimited number of user variables and a small
number of CPU registers, assigning variables to registers without conflicts is
a complex task. This work demonstrates the use of casting the register
allocation problem as a graph coloring problem. Using technologies such as
PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy
Optimization model can learn to solve the graph coloring problem. We will also
show that the labeling of a graph is critical to the performance of the model
by taking the matrix representation of a graph and permuting it. We then test
the model's effectiveness on each of these permutations and show that it is not
effective when given a relabeling of the same graph. Our main contribution lies
in showing the need for label reordering invariant representations of graphs
for machine learning models to achieve consistent performance.</div><div><a href='http://arxiv.org/abs/2401.12470v1'>2401.12470v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03310v1")'>Graph Learning for Parameter Prediction of Quantum Approximate
  Optimization Algorithm</div>
<div id='2403.03310v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T20:23:25Z</div><div>Authors: Zhiding Liang, Gang Liu, Zheyuan Liu, Jinglei Cheng, Tianyi Hao, Kecheng Liu, Hang Ren, Zhixin Song, Ji Liu, Fanny Ye, Yiyu Shi</div><div style='padding-top: 10px; width: 80ex'>In recent years, quantum computing has emerged as a transformative force in
the field of combinatorial optimization, offering novel approaches to tackling
complex problems that have long challenged classical computational methods.
Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out
for its potential to efficiently solve the Max-Cut problem, a quintessential
example of combinatorial optimization. However, practical application faces
challenges due to current limitations on quantum computational resource. Our
work optimizes QAOA initialization, using Graph Neural Networks (GNN) as a
warm-start technique. This sacrifices affordable computational resource on
classical computer to reduce quantum computational resource overhead, enhancing
QAOA's effectiveness. Experiments with various GNN architectures demonstrate
the adaptability and stability of our framework, highlighting the synergy
between quantum algorithms and machine learning. Our findings show GNN's
potential in improving QAOA performance, opening new avenues for hybrid
quantum-classical approaches in quantum computing and contributing to practical
applications.</div><div><a href='http://arxiv.org/abs/2403.03310v1'>2403.03310v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07099v1")'>Rethinking the Capacity of Graph Neural Networks for Branching Strategy</div>
<div id='2402.07099v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T04:09:50Z</div><div>Authors: Ziang Chen, Jialin Liu, Xiaohan Chen, Xinshang Wang, Wotao Yin</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have been widely used to predict properties and
heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP
solvers. This paper investigates the capacity of GNNs to represent strong
branching (SB) scores that provide an efficient strategy in the
branch-and-bound algorithm.
  Although message-passing GNN (MP-GNN), as the simplest GNN structure, is
frequently employed in the existing literature to learn SB scores, we prove a
fundamental limitation in its expressive power -- there exist two MILP
instances with different SB scores that cannot be distinguished by any MP-GNN,
regardless of the number of parameters. In addition, we establish a universal
approximation theorem for another GNN structure called the second-order
folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there
always exists a 2-FGNN that can approximate the SB score with arbitrarily high
accuracy and arbitrarily high probability. A small-scale numerical experiment
is conducted to directly validate our theoretical findings.</div><div><a href='http://arxiv.org/abs/2402.07099v1'>2402.07099v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10467v1")'>Learning Backdoors for Mixed Integer Programs with Contrastive Learning</div>
<div id='2401.10467v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T03:39:43Z</div><div>Authors: Junyang Cai, Taoan Huang, Bistra Dilkina</div><div style='padding-top: 10px; width: 80ex'>Many real-world problems can be efficiently modeled as Mixed Integer Programs
(MIPs) and solved with the Branch-and-Bound method. Prior work has shown the
existence of MIP backdoors, small sets of variables such that prioritizing
branching on them when possible leads to faster running times. However, finding
high-quality backdoors that improve running times remains an open question.
Previous work learns to estimate the relative solver speed of randomly sampled
backdoors through ranking and then decide whether to use it. In this paper, we
utilize the Monte-Carlo tree search method to collect backdoors for training,
rather than relying on random sampling, and adapt a contrastive learning
framework to train a Graph Attention Network model to predict backdoors. Our
method, evaluated on four common MIP problem domains, demonstrates performance
improvements over both Gurobi and previous models.</div><div><a href='http://arxiv.org/abs/2401.10467v1'>2401.10467v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03548v1")'>Single-GPU GNN Systems: Traps and Pitfalls</div>
<div id='2402.03548v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T22:07:58Z</div><div>Authors: Yidong Gong, Arnab Tarafder, Saima Afrin, Pradeep Kumar</div><div style='padding-top: 10px; width: 80ex'>The current graph neural network (GNN) systems have established a clear trend
of not showing training accuracy results, and directly or indirectly relying on
smaller datasets for evaluations majorly. Our in-depth analysis shows that it
leads to a chain of pitfalls in the system design and evaluation process,
questioning the practicality of many of the proposed system optimizations, and
affecting conclusions and lessons learned. We analyze many single-GPU systems
and show the fundamental impact of these pitfalls. We further develop
hypotheses, recommendations, and evaluation methodologies, and provide future
directions. Finally, a new reference system is developed to establish a new
line of optimizations rooted in solving the system-design pitfalls efficiently
and practically. The proposed design can productively be integrated into prior
works, thereby truly advancing the state-of-the-art.</div><div><a href='http://arxiv.org/abs/2402.03548v1'>2402.03548v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06048v1")'>On the Power of Graph Neural Networks and Feature Augmentation
  Strategies to Classify Social Networks</div>
<div id='2401.06048v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T17:09:40Z</div><div>Authors: Walid Guettala, László Gulyás</div><div style='padding-top: 10px; width: 80ex'>This paper studies four Graph Neural Network architectures (GNNs) for a graph
classification task on a synthetic dataset created using classic generative
models of Network Science. Since the synthetic networks do not contain (node or
edge) features, five different augmentation strategies (artificial feature
types) are applied to nodes. All combinations of the 4 GNNs (GCN with
Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types
(constant 1, noise, degree, normalized degree and ID -- a vector of the number
of cycles of various lengths) are studied and their performances compared as a
function of the hidden dimension of artificial neural networks used in the
GNNs. The generalisation ability of these models is also analysed using a
second synthetic network dataset (containing networks of different sizes).Our
results point towards the balanced importance of the computational power of the
GNN architecture and the the information level provided by the artificial
features. GNN architectures with higher computational power, like GIN and
GATv2, perform well for most augmentation strategies. On the other hand,
artificial features with higher information content, like ID or degree, not
only consistently outperform other augmentation strategies, but can also help
GNN architectures with lower computational power to achieve good performance.</div><div><a href='http://arxiv.org/abs/2401.06048v1'>2401.06048v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13865v1")'>Graph Neural Network for Crawling Target Nodes in Social Networks</div>
<div id='2403.13865v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T10:13:54Z</div><div>Authors: Kirill Lukyanov, Mikhail Drobyshevskiy, Danil Shaikhelislamov, Denis Turdakov</div><div style='padding-top: 10px; width: 80ex'>Social networks crawling is in the focus of active research the last years.
One of the challenging task is to collect target nodes in an initially unknown
graph given a budget of crawling steps. Predicting a node property based on its
partially known neighbourhood is at the heart of a successful crawler. In this
paper we adopt graph neural networks for this purpose and show they are
competitive to traditional classifiers and are better for individual cases.
Additionally we suggest a training sample boosting technique, which helps to
diversify the training set at early stages of crawling and thus improves the
predictor quality. The experimental study on three types of target set topology
indicates GNN based approach has a potential in crawling task, especially in
the case of distributed target nodes.</div><div><a href='http://arxiv.org/abs/2403.13865v1'>2403.13865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.00824v1")'>Graph-Convolutional Autoencoder Ensembles for the Humanities,
  Illustrated with a Study of the American Slave Trade</div>
<div id='2401.00824v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T17:48:25Z</div><div>Authors: Tom Lippincott</div><div style='padding-top: 10px; width: 80ex'>We introduce a graph-aware autoencoder ensemble framework, with associated
formalisms and tooling, designed to facilitate deep learning for scholarship in
the humanities. By composing sub-architectures to produce a model isomorphic to
a humanistic domain we maintain interpretability while providing function
signatures for each sub-architectural choice, allowing both traditional and
computational researchers to collaborate without disrupting established
practices. We illustrate a practical application of our approach to a
historical study of the American post-Atlantic slave trade, and make several
specific technical contributions: a novel hybrid graph-convolutional
autoencoder mechanism, batching policies for common graph topologies, and
masking techniques for particular use-cases. The effectiveness of the framework
for broadening participation of diverse domains is demonstrated by a growing
suite of two dozen studies, both collaborations with humanists and established
tasks from machine learning literature, spanning a variety of fields and data
modalities. We make performance comparisons of several different architectural
choices and conclude with an ambitious list of imminent next steps for this
research.</div><div><a href='http://arxiv.org/abs/2401.00824v1'>2401.00824v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03385v1")'>Adolescent relational behaviour and the obesity pandemic: A descriptive
  study applying social network analysis and machine learning techniques</div>
<div id='2402.03385v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:19:56Z</div><div>Authors: Pilar Marqués-Sánchez, María Cristina Martínez-Fernández, José Alberto Benítez-Andrades, Enedina Quiroga-Sánchez, María Teresa García-Ordás, Natalia Arias-Ramos</div><div style='padding-top: 10px; width: 80ex'>Aim: To study the existence of subgroups by exploring the similarities
between the attributes of the nodes of the groups, in relation to diet and
gender and, to analyse the connectivity between groups based on aspects of
similarities between them through SNA and artificial intelligence techniques.
  Methods: 235 students from 5 different educational centres participate in
this study between March and December 2015. Data analysis carried out is
divided into two blocks: social network analysis and unsupervised machine
learning techniques. As for the social network analysis, the Girvan-Newman
technique was applied to find the best number of cohesive groups within each of
the friendship networks of the different classes analysed.
  Results: After applying Girvan-Newman in the three classes, the best division
into clusters was respectively 2 for classroom A, 7 for classroom B and 6 for
classroom C. There are significant differences between the groups and the
gender and diet variables. After applying K-means using population diet as an
input variable, a K-means clustering of 2 clusters for class A, 3 clusters for
class B and 3 clusters for class C is obtained.
  Conclusion: Adolescents form subgroups within their classrooms. Subgroup
cohesion is defined by the fact that nodes share similarities in aspects that
influence obesity, they share attributes related to food quality and gender.
The concept of homophily, related to SNA, justifies our results. Artificial
intelligence techniques together with the application of the Girvan-Newman
provide robustness to the structural analysis of similarities and cohesion
between subgroups.</div><div><a href='http://arxiv.org/abs/2402.03385v1'>2402.03385v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02054v1")'>Neural Scaling Laws on Graphs</div>
<div id='2402.02054v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T06:17:21Z</div><div>Authors: Jingzhe Liu, Haitao Mao, Zhikai Chen, Tong Zhao, Neil Shah, Jiliang Tang</div><div style='padding-top: 10px; width: 80ex'>Deep graph models (e.g., graph neural networks and graph transformers) have
become important techniques for leveraging knowledge across various types of
graphs. Yet, the scaling properties of deep graph models have not been
systematically investigated, casting doubt on the feasibility of achieving
large graph models through enlarging the model and dataset sizes. In this work,
we delve into neural scaling laws on graphs from both model and data
perspectives. We first verify the validity of such laws on graphs, establishing
formulations to describe the scaling behaviors. For model scaling, we
investigate the phenomenon of scaling law collapse and identify overfitting as
the potential reason. Moreover, we reveal that the model depth of deep graph
models can impact the model scaling behaviors, which differ from observations
in other domains such as CV and NLP. For data scaling, we suggest that the
number of graphs can not effectively metric the graph data volume in scaling
law since the sizes of different graphs are highly irregular. Instead, we
reform the data scaling law with the number of edges as the metric to address
the irregular graph sizes. We further demonstrate the reformed law offers a
unified view of the data scaling behaviors for various fundamental graph tasks
including node classification, link prediction, and graph classification. This
work provides valuable insights into neural scaling laws on graphs, which can
serve as an essential step toward large graph models.</div><div><a href='http://arxiv.org/abs/2402.02054v1'>2402.02054v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08678v2")'>Graph Mamba: Towards Learning on Graphs with State Space Models</div>
<div id='2402.08678v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:58:17Z</div><div>Authors: Ali Behrouz, Farnoosh Hashemi</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have shown promising potential in graph
representation learning. The majority of GNNs define a local message-passing
mechanism, propagating information over the graph by stacking multiple layers.
These methods, however, are known to suffer from two major limitations:
over-squashing and poor capturing of long-range dependencies. Recently, Graph
Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural
Networks (MPNNs). GTs, however, have quadratic computational cost, lack
inductive biases on graph structures, and rely on complex Positional/Structural
Encodings (SE/PE). In this paper, we show that while Transformers, complex
message-passing, and SE/PE are sufficient for good performance in practice,
neither is necessary. Motivated by the recent success of State Space Models
(SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general
framework for a new class of GNNs based on selective SSMs. We discuss and
categorize the new challenges when adapting SSMs to graph-structured data, and
present four required and one optional steps to design GMNs, where we choose
(1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of
Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE
and SE. We further provide theoretical justification for the power of GMNs.
Experiments demonstrate that despite much less computational cost, GMNs attain
an outstanding performance in long-range, small-scale, large-scale, and
heterophilic benchmark datasets.</div><div><a href='http://arxiv.org/abs/2402.08678v2'>2402.08678v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17791v2")'>Graph Transformers without Positional Encodings</div>
<div id='2401.17791v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T12:33:31Z</div><div>Authors: Ayush Garg</div><div style='padding-top: 10px; width: 80ex'>Recently, Transformers for graph representation learning have become
increasingly popular, achieving state-of-the-art performance on a wide-variety
of datasets, either alone or in combination with message-passing graph neural
networks (MP-GNNs). Infusing graph inductive-biases in the innately
structure-agnostic transformer architecture in the form of structural or
positional encodings (PEs) is key to achieving these impressive results.
However, designing such encodings is tricky and disparate attempts have been
made to engineer such encodings including Laplacian eigenvectors, relative
random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge
encodings etc. In this work, we argue that such encodings may not be required
at all, provided the attention mechanism itself incorporates information about
the graph structure. We introduce Eigenformer, a Graph Transformer employing a
novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of
the graph, and empirically show that it achieves performance comparable to SOTA
Graph Transformers on a number of standard GNN benchmark datasets, even
surpassing the SOTA on some datasets. The simpler attention mechanism also
allows us to train wider and deeper models for a given parameter budget.</div><div><a href='http://arxiv.org/abs/2401.17791v2'>2401.17791v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10793v1")'>Masked Attention is All You Need for Graphs</div>
<div id='2402.10793v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:20:11Z</div><div>Authors: David Buterez, Jon Paul Janet, Dino Oglic, Pietro Lio</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) and variations of the message passing algorithm
are the predominant means for learning on graphs, largely due to their
flexibility, speed, and satisfactory performance. The design of powerful and
general purpose GNNs, however, requires significant research efforts and often
relies on handcrafted, carefully-chosen message passing operators. Motivated by
this, we propose a remarkably simple alternative for learning on graphs that
relies exclusively on attention. Graphs are represented as node or edge sets
and their connectivity is enforced by masking the attention weight matrix,
effectively creating custom attention patterns for each graph. Despite its
simplicity, masked attention for graphs (MAG) has state-of-the-art performance
on long-range tasks and outperforms strong message passing baselines and much
more involved attention-based methods on over 55 node and graph-level tasks. We
also show significantly better transfer learning capabilities compared to GNNs
and comparable or better time and memory scaling. MAG has sub-linear memory
scaling in the number of nodes or edges, enabling learning on dense graphs and
future-proofing the approach.</div><div><a href='http://arxiv.org/abs/2402.10793v1'>2402.10793v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12821v2")'>FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware
  Graph Transformer</div>
<div id='2403.12821v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T15:21:10Z</div><div>Authors: Dongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin</div><div style='padding-top: 10px; width: 80ex'>The success of a specific neural network architecture is closely tied to the
dataset and task it tackles; there is no one-size-fits-all solution. Thus,
considerable efforts have been made to quickly and accurately estimate the
performances of neural architectures, without full training or evaluation, for
given tasks and datasets. Neural architecture encoding has played a crucial
role in the estimation, and graphbased methods, which treat an architecture as
a graph, have shown prominent performance. For enhanced representation learning
of neural architectures, we introduce FlowerFormer, a powerful graph
transformer that incorporates the information flows within a neural
architecture. FlowerFormer consists of two key components: (a) bidirectional
asynchronous message passing, inspired by the flows; (b) global attention built
on flow-based masking. Our extensive experiments demonstrate the superiority of
FlowerFormer over existing neural encoding methods, and its effectiveness
extends beyond computer vision models to include graph neural networks and auto
speech recognition models. Our code is available at
http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.</div><div><a href='http://arxiv.org/abs/2403.12821v2'>2403.12821v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01232v2")'>Polynormer: Polynomial-Expressive Graph Transformer in Linear Time</div>
<div id='2403.01232v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T15:32:01Z</div><div>Authors: Chenhui Deng, Zichao Yue, Zhiru Zhang</div><div style='padding-top: 10px; width: 80ex'>Graph transformers (GTs) have emerged as a promising architecture that is
theoretically more expressive than message-passing graph neural networks
(GNNs). However, typical GT models have at least quadratic complexity and thus
cannot scale to large graphs. While there are several linear GTs recently
proposed, they still lag behind GNN counterparts on several popular graph
datasets, which poses a critical concern on their practical expressivity. To
balance the trade-off between expressivity and scalability of GTs, we propose
Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer
is built upon a novel base model that learns a high-degree polynomial on input
features. To enable the base model permutation equivariant, we integrate it
with graph topology and node features separately, resulting in local and global
equivariant attention models. Consequently, Polynormer adopts a linear
local-to-global attention scheme to learn high-degree equivariant polynomials
whose coefficients are controlled by attention scores. Polynormer has been
evaluated on $13$ homophilic and heterophilic datasets, including large graphs
with millions of nodes. Our extensive experiment results show that Polynormer
outperforms state-of-the-art GNN and GT baselines on most datasets, even
without the use of nonlinear activation functions.</div><div><a href='http://arxiv.org/abs/2403.01232v2'>2403.01232v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14580v2")'>Design Your Own Universe: A Physics-Informed Agnostic Method for
  Enhancing Graph Neural Networks</div>
<div id='2401.14580v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T00:47:43Z</div><div>Authors: Dai Shi, Andi Han, Lequan Lin, Yi Guo, Zhiyong Wang, Junbin Gao</div><div style='padding-top: 10px; width: 80ex'>Physics-informed Graph Neural Networks have achieved remarkable performance
in learning through graph-structured data by mitigating common GNN challenges
such as over-smoothing, over-squashing, and heterophily adaption. Despite these
advancements, the development of a simple yet effective paradigm that
appropriately integrates previous methods for handling all these challenges is
still underway. In this paper, we draw an analogy between the propagation of
GNNs and particle systems in physics, proposing a model-agnostic enhancement
framework. This framework enriches the graph structure by introducing
additional nodes and rewiring connections with both positive and negative
weights, guided by node labeling information. We theoretically verify that GNNs
enhanced through our approach can effectively circumvent the over-smoothing
issue and exhibit robustness against over-squashing. Moreover, we conduct a
spectral analysis on the rewired graph to demonstrate that the corresponding
GNNs can fit both homophilic and heterophilic graphs. Empirical validations on
benchmarks for homophilic, heterophilic graphs, and long-term graph datasets
show that GNNs enhanced by our method significantly outperform their original
counterparts.</div><div><a href='http://arxiv.org/abs/2401.14580v2'>2401.14580v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04747v1")'>GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural
  Networks</div>
<div id='2403.04747v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:52:27Z</div><div>Authors: Lisa Schneckenreiter, Richard Freinschlag, Florian Sestak, Johannes Brandstetter, Günter Klambauer, Andreas Mayr</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs), and especially message-passing neural networks,
excel in various domains such as physics, drug discovery, and molecular
modeling. The expressivity of GNNs with respect to their ability to
discriminate non-isomorphic graphs critically depends on the functions employed
for message aggregation and graph-level readout. By applying signal propagation
theory, we propose a variance-preserving aggregation function (VPA) that
maintains expressivity, but yields improved forward and backward dynamics.
Experiments demonstrate that VPA leads to increased predictive performance for
popular GNN architectures as well as improved learning dynamics. Our results
could pave the way towards normalizer-free or self-normalizing GNNs.</div><div><a href='http://arxiv.org/abs/2403.04747v1'>2403.04747v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00337v1")'>Nonlinear Sheaf Diffusion in Graph Neural Networks</div>
<div id='2403.00337v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T08:01:27Z</div><div>Authors: Olga Zaghen</div><div style='padding-top: 10px; width: 80ex'>This work focuses on exploring the potential benefits of introducing a
nonlinear Laplacian in Sheaf Neural Networks for graph-related tasks. The
primary aim is to understand the impact of such nonlinearity on diffusion
dynamics, signal propagation, and performance of neural network architectures
in discrete-time settings. The study primarily emphasizes experimental
analysis, using real-world and synthetic datasets to validate the practical
effectiveness of different versions of the model. This approach shifts the
focus from an initial theoretical exploration to demonstrating the practical
utility of the proposed model.</div><div><a href='http://arxiv.org/abs/2403.00337v1'>2403.00337v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16402v1")'>Graph Learning with Distributional Edge Layouts</div>
<div id='2402.16402v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T08:55:10Z</div><div>Authors: Xinjian Zhao, Chaolong Ying, Tianshu Yu</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) learn from graph-structured data by passing
local messages between neighboring nodes along edges on certain topological
layouts. Typically, these topological layouts in modern GNNs are
deterministically computed (e.g., attention-based GNNs) or locally sampled
(e.g., GraphSage) under heuristic assumptions. In this paper, we for the first
time pose that these layouts can be globally sampled via Langevin dynamics
following Boltzmann distribution equipped with explicit physical energy,
leading to higher feasibility in the physical world. We argue that such a
collection of sampled/optimized layouts can capture the wide energy
distribution and bring extra expressivity on top of WL-test, therefore easing
downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to
serve as a complement to a variety of GNNs. DEL is a pre-processing strategy
independent of subsequent GNN variants, thus being highly flexible.
Experimental results demonstrate that DELs consistently and substantially
improve a series of GNN baselines, achieving state-of-the-art performance on
multiple datasets.</div><div><a href='http://arxiv.org/abs/2402.16402v1'>2402.16402v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02005v1")'>Topology-Informed Graph Transformer</div>
<div id='2402.02005v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T03:17:44Z</div><div>Authors: Yun Young Choi, Sun Woo Park, Minho Lee, Youngho Woo</div><div style='padding-top: 10px; width: 80ex'>Transformers have revolutionized performance in Natural Language Processing
and Vision, paving the way for their integration with Graph Neural Networks
(GNNs). One key challenge in enhancing graph transformers is strengthening the
discriminative power of distinguishing isomorphisms of graphs, which plays a
crucial role in boosting their predictive performances. To address this
challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel
transformer enhancing both discriminative power in detecting graph isomorphisms
and the overall performance of Graph Transformers. TIGT consists of four
components: A topological positional embedding layer using non-isomorphic
universal covers based on cyclic subgraphs of graphs to ensure unique graph
representation: A dual-path message-passing layer to explicitly encode
topological characteristics throughout the encoder layers: A global attention
mechanism: And a graph information layer to recalibrate channel-wise graph
features for better feature representation. TIGT outperforms previous Graph
Transformers in classifying synthetic dataset aimed at distinguishing
isomorphism classes of graphs. Additionally, mathematical analysis and
empirical evaluations highlight our model's competitive edge over
state-of-the-art Graph Transformers across various benchmark datasets.</div><div><a href='http://arxiv.org/abs/2402.02005v1'>2402.02005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02216v2")'>Graph Foundation Models</div>
<div id='2402.02216v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T17:24:36Z</div><div>Authors: Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Mikhail Galkin, Jiliang Tang</div><div style='padding-top: 10px; width: 80ex'>Graph Foundation Model (GFM) is a new trending research topic in the graph
domain, aiming to develop a graph model capable of generalizing across
different graphs and tasks. However, a versatile GFM has not yet been achieved.
The key challenge in building GFM is how to enable positive transfer across
graphs with diverse structural patterns. Inspired by the existing foundation
models in the CV and NLP domains, we propose a novel perspective for the GFM
development by advocating for a "graph vocabulary", in which the basic
transferable units underlying graphs encode the invariance on graphs. We ground
the graph vocabulary construction from essential aspects including network
analysis, theoretical foundations, and stability. Such a vocabulary perspective
can potentially advance the future GFM design following the neural scaling
laws.</div><div><a href='http://arxiv.org/abs/2402.02216v2'>2402.02216v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07340v1")'>Random Geometric Graph Alignment with Graph Neural Networks</div>
<div id='2402.07340v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T00:18:25Z</div><div>Authors: Suqi Liu, Morgane Austern</div><div style='padding-top: 10px; width: 80ex'>We characterize the performance of graph neural networks for graph alignment
problems in the presence of vertex feature information. More specifically,
given two graphs that are independent perturbations of a single random
geometric graph with noisy sparse features, the task is to recover an unknown
one-to-one mapping between the vertices of the two graphs. We show under
certain conditions on the sparsity and noise level of the feature vectors, a
carefully designed one-layer graph neural network can with high probability
recover the correct alignment between the vertices with the help of the graph
structure. We also prove that our conditions on the noise level are tight up to
logarithmic factors. Finally we compare the performance of the graph neural
network to directly solving an assignment problem on the noisy vertex features.
We demonstrate that when the noise level is at least constant this direct
matching fails to have perfect recovery while the graph neural network can
tolerate noise level growing as fast as a power of the size of the graph.</div><div><a href='http://arxiv.org/abs/2402.07340v1'>2402.07340v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12269v2")'>End-to-end Supervised Prediction of Arbitrary-size Graphs with
  Partially-Masked Fused Gromov-Wasserstein Matching</div>
<div id='2402.12269v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T16:30:35Z</div><div>Authors: Paul Krzakala, Junjie Yang, Rémi Flamary, Florence d'Alché-Buc, Charlotte Laclau, Matthieu Labeau</div><div style='padding-top: 10px; width: 80ex'>We present a novel end-to-end deep learning-based approach for Supervised
Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based
loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows
to directly leverage graph representations such as adjacency and feature
matrices. PM-FGW exhibits all the desirable properties for SGP: it is node
permutation invariant, sub-differentiable and handles graphs of different sizes
by comparing their padded representations as well as their masking vectors.
Moreover, we present a flexible transformer-based architecture that easily
adapts to different types of input data. In the experimental section, three
different tasks, a novel and challenging synthetic dataset (image2graph) and
two real-world tasks, image2map and fingerprint2molecule - showcase the
efficiency and versatility of the approach compared to competitors.</div><div><a href='http://arxiv.org/abs/2402.12269v2'>2402.12269v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14381v1")'>Manifold GCN: Diffusion-based Convolutional Neural Network for
  Manifold-valued Graphs</div>
<div id='2401.14381v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:36:10Z</div><div>Authors: Martin Hanik, Gabriele Steidl, Christoph von Tycowicz</div><div style='padding-top: 10px; width: 80ex'>We propose two graph neural network layers for graphs with features in a
Riemannian manifold. First, based on a manifold-valued graph diffusion
equation, we construct a diffusion layer that can be applied to an arbitrary
number of nodes and graph connectivity patterns. Second, we model a tangent
multilayer perceptron by transferring ideas from the vector neuron framework to
our general setting. Both layers are equivariant with respect to node
permutations and isometries of the feature manifold. These properties have been
shown to lead to a beneficial inductive bias in many deep learning tasks.
Numerical examples on synthetic data as well as on triangle meshes of the right
hippocampus to classify Alzheimer's disease demonstrate the very good
performance of our layers.</div><div><a href='http://arxiv.org/abs/2401.14381v1'>2401.14381v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14202v1")'>Comparing Graph Transformers via Positional Encodings</div>
<div id='2402.14202v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T01:07:48Z</div><div>Authors: Mitchell Black, Zhengchao Wan, Gal Mishne, Amir Nayyeri, Yusu Wang</div><div style='padding-top: 10px; width: 80ex'>The distinguishing power of graph transformers is closely tied to the choice
of positional encoding: features used to augment the base transformer with
information about the graph. There are two primary types of positional
encoding: absolute positional encodings (APEs) and relative positional
encodings (RPEs). APEs assign features to each node and are given as input to
the transformer. RPEs instead assign a feature to each pair of nodes, e.g.,
graph distance, and are used to augment the attention block. A priori, it is
unclear which method is better for maximizing the power of the resulting graph
transformer. In this paper, we aim to understand the relationship between these
different types of positional encodings. Interestingly, we show that graph
transformers using APEs and RPEs are equivalent in terms of distinguishing
power. In particular, we demonstrate how to interchange APEs and RPEs while
maintaining their distinguishing power in terms of graph transformers. Based on
our theoretical results, we provide a study on several APEs and RPEs (including
the resistance distance and the recently introduced stable and expressive
positional encoding (SPE)) and compare their distinguishing power in terms of
transformers. We believe our work will help navigate the huge number of choices
of positional encoding and will provide guidance on the future design of
positional encodings for graph transformers.</div><div><a href='http://arxiv.org/abs/2402.14202v1'>2402.14202v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17752v1")'>PF-GNN: Differentiable particle filtering based approximation of
  universal graph representations</div>
<div id='2401.17752v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T11:26:03Z</div><div>Authors: Mohammed Haroon Dupty, Yanfei Dong, Wee Sun Lee</div><div style='padding-top: 10px; width: 80ex'>Message passing Graph Neural Networks (GNNs) are known to be limited in
expressive power by the 1-WL color-refinement test for graph isomorphism. Other
more expressive models either are computationally expensive or need
preprocessing to extract structural features from the graph. In this work, we
propose to make GNNs universal by guiding the learning process with exact
isomorphism solver techniques which operate on the paradigm of
Individualization and Refinement (IR), a method to artificially introduce
asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers
generate a search tree of colorings whose leaves uniquely identify the graph.
However, the tree grows exponentially large and needs hand-crafted pruning
techniques which are not desirable from a learning perspective. We take a
probabilistic view and approximate the search tree of colorings (i.e.
embeddings) by sampling multiple paths from root to leaves of the search tree.
To learn more discriminative representations, we guide the sampling process
with particle filter updates, a principled approach for sequential state
estimation. Our algorithm is end-to-end differentiable, can be applied with any
GNN as backbone and learns richer graph representations with only linear
increase in runtime. Experimental evaluation shows that our approach
consistently outperforms leading GNN models on both synthetic benchmarks for
isomorphism detection as well as real-world datasets.</div><div><a href='http://arxiv.org/abs/2401.17752v1'>2401.17752v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06662v1")'>Sign Rank Limitations for Attention-Based Graph Decoders</div>
<div id='2402.06662v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T02:17:20Z</div><div>Authors: Su Hyeong Lee, Qingqi Zhang, Risi Kondor</div><div style='padding-top: 10px; width: 80ex'>Inner product-based decoders are among the most influential frameworks used
to extract meaningful data from latent embeddings. However, such decoders have
shown limitations in representation capacity in numerous works within the
literature, which have been particularly notable in graph reconstruction
problems. In this paper, we provide the first theoretical elucidation of this
pervasive phenomenon in graph data, and suggest straightforward modifications
to circumvent this issue without deviating from the inner product framework.</div><div><a href='http://arxiv.org/abs/2402.06662v1'>2402.06662v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08085v1")'>Message Detouring: A Simple Yet Effective Cycle Representation for
  Expressive Graph Learning</div>
<div id='2402.08085v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:06:37Z</div><div>Authors: Ziquan Wei, Tingting Dan, Guorong Wu</div><div style='padding-top: 10px; width: 80ex'>Graph learning is crucial in the fields of bioinformatics, social networks,
and chemicals. Although high-order graphlets, such as cycles, are critical to
achieving an informative graph representation for node classification, edge
prediction, and graph recognition, modeling high-order topological
characteristics poses significant computational challenges, restricting its
widespread applications in machine learning. To address this limitation, we
introduce the concept of \textit{message detouring} to hierarchically
characterize cycle representation throughout the entire graph, which
capitalizes on the contrast between the shortest and longest pathways within a
range of local topologies associated with each graph node. The topological
feature representations derived from our message detouring landscape
demonstrate comparable expressive power to high-order
\textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In
addition to the integration with graph kernel and message passing neural
networks, we present a novel message detouring neural network, which uses
Transformer backbone to integrate cycle representations across nodes and edges.
Aside from theoretical results, experimental results on expressiveness, graph
classification, and node classification show message detouring can
significantly outperform current counterpart approaches on various benchmark
datasets.</div><div><a href='http://arxiv.org/abs/2402.08085v1'>2402.08085v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08450v1")'>Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph
  Products</div>
<div id='2402.08450v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T13:37:13Z</div><div>Authors: Guy Bar-Shalom, Beatrice Bevilacqua, Haggai Maron</div><div style='padding-top: 10px; width: 80ex'>In the realm of Graph Neural Networks (GNNs), two exciting research
directions have recently emerged: Subgraph GNNs and Graph Transformers. In this
paper, we propose an architecture that integrates both approaches, dubbed
Subgraphormer, which combines the enhanced expressive power, message-passing
mechanisms, and aggregation schemes from Subgraph GNNs with attention and
positional encodings, arguably the most important components in Graph
Transformers. Our method is based on an intriguing new connection we reveal
between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be
formulated as Message Passing Neural Networks (MPNNs) operating on a product of
the graph with itself. We use this formulation to design our architecture:
first, we devise an attention mechanism based on the connectivity of the
product graph. Following this, we propose a novel and efficient positional
encoding scheme for Subgraph GNNs, which we derive as a positional encoding for
the product graph. Our experimental results demonstrate significant performance
improvements over both Subgraph GNNs and Graph Transformers on a wide range of
datasets.</div><div><a href='http://arxiv.org/abs/2402.08450v1'>2402.08450v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06080v1")'>Local Vertex Colouring Graph Neural Networks</div>
<div id='2403.06080v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T03:59:24Z</div><div>Authors: Shouheng Li, Dongwoo Kim, Qing Wang</div><div style='padding-top: 10px; width: 80ex'>In recent years, there has been a significant amount of research focused on
expanding the expressivity of Graph Neural Networks (GNNs) beyond the
Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded
advancements in expressivity, they have frequently come at the expense of
decreased efficiency or have been restricted to specific types of graphs. In
this study, we investigate the expressivity of GNNs from the perspective of
graph search. Specifically, we propose a new vertex colouring scheme and
demonstrate that classical search algorithms can efficiently compute graph
representations that extend beyond the 1-WL. We show the colouring scheme
inherits useful properties from graph search that can help solve problems like
graph biconnectivity. Furthermore, we show that under certain conditions, the
expressivity of GNNs increases hierarchically with the radius of the search
neighbourhood. To further investigate the proposed scheme, we develop a new
type of GNN based on two search strategies, breadth-first search and
depth-first search, highlighting the graph properties they can capture on top
of 1-WL. Our code is available at https://github.com/seanli3/lvc.</div><div><a href='http://arxiv.org/abs/2403.06080v1'>2403.06080v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07294v1")'>Graph Data Condensation via Self-expressive Graph Structure
  Reconstruction</div>
<div id='2403.07294v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T03:54:25Z</div><div>Authors: Zhanyu Liu, Chaolv Zeng, Guanjie Zheng</div><div style='padding-top: 10px; width: 80ex'>With the increasing demands of training graph neural networks (GNNs) on
large-scale graphs, graph data condensation has emerged as a critical technique
to relieve the storage and time costs during the training phase. It aims to
condense the original large-scale graph to a much smaller synthetic graph while
preserving the essential information necessary for efficiently training a
downstream GNN. However, existing methods concentrate either on optimizing node
features exclusively or endeavor to independently learn node features and the
graph structure generator. They could not explicitly leverage the information
of the original graph structure and failed to construct an interpretable graph
structure for the synthetic dataset. To address these issues, we introduce a
novel framework named \textbf{G}raph Data \textbf{C}ondensation via
\textbf{S}elf-expressive Graph Structure \textbf{R}econstruction
(\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the
original graph structure into the condensing process and (2) capturing the
nuanced interdependencies between the condensed nodes by reconstructing an
interpretable self-expressive graph structure. Extensive experiments and
comprehensive analysis validate the efficacy of the proposed method across
diverse GNN models and datasets. Our code is available at
https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&amp;dl=0</div><div><a href='http://arxiv.org/abs/2403.07294v1'>2403.07294v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14951v1")'>Simple Graph Condensation</div>
<div id='2403.14951v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T05:04:48Z</div><div>Authors: Zhenbang Xiao, Yu Wang, Shunyu Liu, Huiqiong Wang, Mingli Song, Tongya Zheng</div><div style='padding-top: 10px; width: 80ex'>The burdensome training costs on large-scale graphs have aroused significant
interest in graph condensation, which involves tuning Graph Neural Networks
(GNNs) on a small condensed graph for use on the large-scale original graph.
Existing methods primarily focus on aligning key metrics between the condensed
and original graphs, such as gradients, distribution and trajectory of GNNs,
yielding satisfactory performance on downstream tasks. However, these complex
metrics necessitate intricate computations and can potentially disrupt the
optimization process of the condensation graph, making the condensation process
highly demanding and unstable. Motivated by the recent success of simplified
models in various fields, we propose a simplified approach to metric alignment
in graph condensation, aiming to reduce unnecessary complexity inherited from
GNNs. In our approach, we eliminate external parameters and exclusively retain
the target condensed graph during the condensation process. Following the
hierarchical aggregation principles of GNNs, we introduce the Simple Graph
Condensation (SimGC) framework, which aligns the condensed graph with the
original graph from the input layer to the prediction layer, guided by a
pre-trained Simple Graph Convolution (SGC) model on the original graph. As a
result, both graphs possess the similar capability to train GNNs. This
straightforward yet effective strategy achieves a significant speedup of up to
10 times compared to existing graph condensation methods while performing on
par with state-of-the-art baselines. Comprehensive experiments conducted on
seven benchmark datasets demonstrate the effectiveness of SimGC in prediction
accuracy, condensation time, and generalization capability. Our code will be
made publicly available.</div><div><a href='http://arxiv.org/abs/2403.14951v1'>2403.14951v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07353v2")'>Graph Unlearning with Efficient Partial Retraining</div>
<div id='2403.07353v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T06:22:10Z</div><div>Authors: Jiahao Zhang, Lin Wang, Shijie Wang, Wenqi Fan</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have achieved remarkable success in various
real-world applications. However, GNNs may be trained on undesirable graph
data, which can degrade their performance and reliability. To enable trained
GNNs to efficiently unlearn unwanted data, a desirable solution is
retraining-based graph unlearning, which partitions the training graph into
subgraphs and trains sub-models on them, allowing fast unlearning through
partial retraining. However, the graph partition process causes information
loss in the training graph, resulting in the low model utility of sub-GNN
models. In this paper, we propose GraphRevoker, a novel graph unlearning
framework that better maintains the model utility of unlearnable GNNs.
Specifically, we preserve the graph property with graph property-aware sharding
and effectively aggregate the sub-GNN models for prediction with graph
contrastive sub-model aggregation. We conduct extensive experiments to
demonstrate the superiority of our proposed approach.</div><div><a href='http://arxiv.org/abs/2403.07353v2'>2403.07353v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01400v1")'>Decoupling Weighing and Selecting for Integrating Multiple Graph
  Pre-training Tasks</div>
<div id='2403.01400v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T05:29:49Z</div><div>Authors: Tianyu Fan, Lirong Wu, Yufei Huang, Haitao Lin, Cheng Tan, Zhangyang Gao, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Recent years have witnessed the great success of graph pre-training for graph
representation learning. With hundreds of graph pre-training tasks proposed,
integrating knowledge acquired from multiple pre-training tasks has become a
popular research topic. In this paper, we identify two important collaborative
processes for this topic: (1) select: how to select an optimal task combination
from a given task pool based on their compatibility, and (2) weigh: how to
weigh the selected tasks based on their importance. While there currently has
been a lot of work focused on weighing, comparatively little effort has been
devoted to selecting. This paper proposes a novel instance-level framework for
integrating multiple graph pre-training tasks, Weigh And Select (WAS), where
the two collaborative processes, weighing and selecting, are combined by
decoupled siamese networks. Specifically, it first adaptively learns an optimal
combination of tasks for each instance from a given task pool, based on which a
customized instance-level task weighing strategy is learned. Extensive
experiments on 16 graph datasets across node-level and graph-level downstream
tasks have demonstrated that by combining a few simple but classical tasks, WAS
can achieve comparable performance to other leading counterparts. The code is
available at https://github.com/TianyuFan0504/WAS.</div><div><a href='http://arxiv.org/abs/2403.01400v1'>2403.01400v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03483v1")'>A Teacher-Free Graph Knowledge Distillation Framework with Dual
  Self-Distillation</div>
<div id='2403.03483v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T05:52:13Z</div><div>Authors: Lirong Wu, Haitao Lin, Zhangyang Gao, Guojiang Zhao, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Recent years have witnessed great success in handling graph-related tasks
with Graph Neural Networks (GNNs). Despite their great academic success,
Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical
industrial applications. One reason for such an academic-industry gap is the
neighborhood-fetching latency incurred by data dependency in GNNs. To reduce
their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a
standard teacher-student architecture, to distill knowledge from a large
teacher GNN into a lightweight student GNN or MLP. However, we found in this
paper that neither teachers nor GNNs are necessary for graph knowledge
distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework
that does not require any teacher model or GNNs during both training and
inference. More importantly, the proposed TGS framework is purely based on
MLPs, where structural information is only implicitly used to guide dual
knowledge self-distillation between the target node and its neighborhood. As a
result, TGS enjoys the benefits of graph topology awareness in training but is
free from data dependency in inference. Extensive experiments have shown that
the performance of vanilla MLPs can be greatly improved with dual
self-distillation, e.g., TGS improves over vanilla MLPs by 15.54% on average
and outperforms state-of-the-art GKD algorithms on six real-world datasets. In
terms of inference speed, TGS infers 75X-89X faster than existing GNNs and
16X-25X faster than classical inference acceleration methods.</div><div><a href='http://arxiv.org/abs/2403.03483v1'>2403.03483v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01079v1")'>Teaching MLP More Graph Information: A Three-stage Multitask Knowledge
  Distillation Framework</div>
<div id='2403.01079v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T03:29:11Z</div><div>Authors: Junxian Li, Bin Shi, Erfei Cui, Hua Wei, Qinghua Zheng</div><div style='padding-top: 10px; width: 80ex'>We study the challenging problem for inference tasks on large-scale graph
datasets of Graph Neural Networks: huge time and memory consumption, and try to
overcome it by reducing reliance on graph structure. Even though distilling
graph knowledge to student MLP is an excellent idea, it faces two major
problems of positional information loss and low generalization. To solve the
problems, we propose a new three-stage multitask distillation framework. In
detail, we use Positional Encoding to capture positional information. Also, we
introduce Neural Heat Kernels responsible for graph data processing in GNN and
utilize hidden layer outputs matching for better performance of student MLP's
hidden layers. To the best of our knowledge, it is the first work to include
hidden layer distillation for student MLP on graphs and to combine graph
Positional Encoding with MLP. We test its performance and robustness with
several settings and draw the conclusion that our work can outperform well with
good stability.</div><div><a href='http://arxiv.org/abs/2403.01079v1'>2403.01079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10807v1")'>FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning</div>
<div id='2403.10807v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T04:43:46Z</div><div>Authors: Eugene Ku</div><div style='padding-top: 10px; width: 80ex'>Knowledge Distillation (KD) aims to transfer a more capable teacher model's
knowledge to a lighter student model in order to improve the efficiency of the
model, making it faster and more deployable. However, the student model's
optimization process over the noisy pseudo labels (generated by the teacher
model) is tricky and the amount of pseudo labels one can generate is limited
due to Out of Memory (OOM) error. In this paper, we propose FlyKD (Knowledge
Distillation on the Fly) which enables the generation of virtually unlimited
number of pseudo labels, coupled with Curriculum Learning that greatly
alleviates the optimization process over the noisy pseudo labels. Empirically,
we observe that FlyKD outperforms vanilla KD and the renown Local Structure
Preserving Graph Convolutional Network (LSPGCN). Lastly, with the success of
Curriculum Learning, we shed light on a new research direction of improving
optimization over noisy pseudo labels.</div><div><a href='http://arxiv.org/abs/2403.10807v1'>2403.10807v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14101v1")'>Text-Enhanced Data-free Approach for Federated Class-Incremental
  Learning</div>
<div id='2403.14101v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T03:24:01Z</div><div>Authors: Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Dinh Phung</div><div style='padding-top: 10px; width: 80ex'>Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal
issue, involving the dynamic addition of new classes in the context of
federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a
crucial role in addressing catastrophic forgetting and data privacy problems.
However, prior approaches lack the crucial synergy between DFKT and the model
training phases, causing DFKT to encounter difficulties in generating
high-quality data from a non-anchored latent space of the old task model. In
this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge
Transfer) to address this issue by utilizing label text embeddings (LTE)
produced by pretrained language models. Specifically, during the model training
phase, our approach treats LTE as anchor points and constrains the feature
embeddings of corresponding training samples around them, enriching the
surrounding area with more meaningful information. In the DFKT phase, by using
these LTE anchors, LANDER can synthesize more meaningful samples, thereby
effectively addressing the forgetting problem. Additionally, instead of tightly
constraining embeddings toward the anchor, the Bounding Loss is introduced to
encourage sample embeddings to remain flexible within a defined radius. This
approach preserves the natural differences in sample embeddings and mitigates
the embedding overlap caused by heterogeneous federated settings. Extensive
experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that
LANDER significantly outperforms previous methods and achieves state-of-the-art
performance in FCIL. The code is available at
https://github.com/tmtuan1307/lander.</div><div><a href='http://arxiv.org/abs/2403.14101v1'>2403.14101v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05752v2")'>Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and
  Efficient Modeling</div>
<div id='2403.05752v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T01:17:26Z</div><div>Authors: Hussein Abdallah, Waleed Afandi, Panos Kalnis, Essam Mansour</div><div style='padding-top: 10px; width: 80ex'>A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range
of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular
for training machine learning tasks like node classification and link
prediction on KGs. However, HGNN methods exhibit excessive complexity
influenced by the KG's size, density, and the number of node and edge types. AI
practitioners handcraft a subgraph of a KG G relevant to a specific task. We
refer to this subgraph as a task-oriented subgraph (TOSG), which contains a
subset of task-related node and edge types in G. Training the task using TOSG
instead of G alleviates the excessive computation required for a large KG.
Crafting the TOSG demands a deep understanding of the KG's structure and the
task's objectives. Hence, it is challenging and time-consuming. This paper
proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented
HGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that
captures the KG's local and global structure relevant to a specific task. We
explore different techniques to extract subgraphs matching our graph pattern:
namely (i) two techniques sampling around targeted nodes using biased random
walk or influence scores, and (ii) a SPARQL-based extraction method leveraging
RDF engines' built-in indices. Hence, it achieves negligible preprocessing
overhead compared to the sampling techniques. We develop a benchmark of real
KGs of large sizes and various tasks for node classification and link
prediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN
methods reduce training time and memory usage by up to 70% while improving the
model performance, e.g., accuracy and inference time.</div><div><a href='http://arxiv.org/abs/2403.05752v2'>2403.05752v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14733v1")'>Open Knowledge Base Canonicalization with Multi-task Learning</div>
<div id='2403.14733v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T08:03:46Z</div><div>Authors: Bingchen Liu, Huang Peng, Weixin Zeng, Xiang Zhao, Shijun Liu, Li Pan</div><div style='padding-top: 10px; width: 80ex'>The construction of large open knowledge bases (OKBs) is integral to many
knowledge-driven applications on the world wide web such as web search.
However, noun phrases and relational phrases in OKBs often suffer from
redundancy and ambiguity, which calls for the investigation on OKB
canonicalization. Current solutions address OKB canonicalization by devising
advanced clustering algorithms and using knowledge graph embedding (KGE) to
further facilitate the canonicalization process. Nevertheless, these works fail
to fully exploit the synergy between clustering and KGE learning, and the
methods designed for these subtasks are sub-optimal. To this end, we put
forward a multi-task learning framework, namely MulCanon, to tackle OKB
canonicalization. In addition, diffusion model is used in the soft clustering
process to improve the noun phrase representations with neighboring
information, which can lead to more accurate representations. MulCanon unifies
the learning objectives of these sub-tasks, and adopts a two-stage multi-task
learning paradigm for training. A thorough experimental study on popular OKB
canonicalization benchmarks validates that MulCanon can achieve competitive
canonicalization results.</div><div><a href='http://arxiv.org/abs/2403.14733v1'>2403.14733v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16278v3")'>A Self-matching Training Method with Annotation Embedding Models for
  Ontology Subsumption Prediction</div>
<div id='2402.16278v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T03:46:01Z</div><div>Authors: Yukihiro Shiraishi, Ken Kaneiwa</div><div style='padding-top: 10px; width: 80ex'>Recently, ontology embeddings representing entities in a low-dimensional
space have been proposed for ontology completion. However, the ontology
embeddings for concept subsumption prediction do not address the difficulties
of similar and isolated entities and fail to extract the global information of
annotation axioms from an ontology. In this paper, we propose a self-matching
training method for the two ontology embedding models: Inverted-index Matrix
Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings
capture the global and local information in annotation axioms by means of the
occurring locations of each word in a set of axioms and the co-occurrences of
words in each axiom. The self-matching training method increases the robustness
of the concept subsumption prediction when predicted superclasses are similar
to subclasses and are isolated to other entities in an ontology. Our evaluation
experiments show that the self-matching training method with InME outperforms
the existing ontology embeddings for the GO and FoodOn ontologies and that the
method with the concatenation of CoME and OWL2Vec* outperforms them for the
HeLiS ontology.</div><div><a href='http://arxiv.org/abs/2402.16278v3'>2402.16278v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.03114v1")'>GLISP: A Scalable GNN Learning System by Exploiting Inherent Structural
  Properties of Graphs</div>
<div id='2401.03114v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T02:59:24Z</div><div>Authors: Zhongshu Zhu, Bin Jing, Xiaopei Wan, Zhizhen Liu, Lei Liang, Jun zhou</div><div style='padding-top: 10px; width: 80ex'>As a powerful tool for modeling graph data, Graph Neural Networks (GNNs) have
received increasing attention in both academia and industry. Nevertheless, it
is notoriously difficult to deploy GNNs on industrial scale graphs, due to
their huge data size and complex topological structures. In this paper, we
propose GLISP, a sampling based GNN learning system for industrial scale
graphs. By exploiting the inherent structural properties of graphs, such as
power law distribution and data locality, GLISP addresses the scalability and
performance issues that arise at different stages of the graph learning
process. GLISP consists of three core components: graph partitioner, graph
sampling service and graph inference engine. The graph partitioner adopts the
proposed vertex-cut graph partitioning algorithm AdaDNE to produce balanced
partitioning for power law graphs, which is essential for sampling based GNN
systems. The graph sampling service employs a load balancing design that allows
the one hop sampling request of high degree vertices to be handled by multiple
servers. In conjunction with the memory efficient data structure, the
efficiency and scalability are effectively improved. The graph inference engine
splits the $K$-layer GNN into $K$ slices and caches the vertex embeddings
produced by each slice in the data locality aware hybrid caching system for
reuse, thus completely eliminating redundant computation caused by the data
dependency of graph. Extensive experiments show that GLISP achieves up to
$6.53\times$ and $70.77\times$ speedups over existing GNN systems for training
and inference tasks, respectively, and can scale to the graph with over 10
billion vertices and 40 billion edges with limited resources.</div><div><a href='http://arxiv.org/abs/2401.03114v1'>2401.03114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11139v1")'>LiGNN: Graph Neural Networks at LinkedIn</div>
<div id='2402.11139v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:10:33Z</div><div>Authors: Fedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou, Chengming Jiang, Nitin Pasumarthy, Priya Bannur, Birjodh Tiwana, Ping Liu, Siddharth Dangi, Daqi Sun, Zhoutao Pei, Xiao Shi, Sirou Zhu, Qianqi Shen, Kuang-Hsuan Lee, David Stein, Baolei Li, Haichao Wei, Amol Ghoting, Souvik Ghosh</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks
(GNNs) Framework. We share our insight on developing and deployment of GNNs at
large scale at LinkedIn. We present a set of algorithmic improvements to the
quality of GNN representation learning including temporal graph architectures
with long term losses, effective cold start solutions via graph densification,
ID embeddings and multi-hop neighbor sampling. We explain how we built and sped
up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of
neighbors, grouping and slicing of training data batches, specialized
shared-memory queue and local gradient optimization. We summarize our
deployment lessons and learnings gathered from A/B test experiments. The
techniques presented in this work have contributed to an approximate relative
improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5%
of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active
user lift from people recommendation. We believe that this work can provide
practical solutions and insights for engineers who are interested in applying
Graph neural networks at large scale.</div><div><a href='http://arxiv.org/abs/2402.11139v1'>2402.11139v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13430v1")'>LinkSAGE: Optimizing Job Matching Using Graph Neural Networks</div>
<div id='2402.13430v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T23:49:25Z</div><div>Authors: Ping Liu, Haichao Wei, Xiaochen Hou, Jianqiang Shen, Shihai He, Kay Qianqi Shen, Zhujun Chen, Fedor Borisyuk, Daniel Hewlett, Liang Wu, Srikant Veeraraghavan, Alex Tsun, Chengming Jiang, Wenjing Zhang</div><div style='padding-top: 10px; width: 80ex'>We present LinkSAGE, an innovative framework that integrates Graph Neural
Networks (GNNs) into large-scale personalized job matching systems, designed to
address the complex dynamics of LinkedIns extensive professional network. Our
approach capitalizes on a novel job marketplace graph, the largest and most
intricate of its kind in industry, with billions of nodes and edges. This graph
is not merely extensive but also richly detailed, encompassing member and job
nodes along with key attributes, thus creating an expansive and interwoven
network. A key innovation in LinkSAGE is its training and serving methodology,
which effectively combines inductive graph learning on a heterogeneous,
evolving graph with an encoder-decoder GNN model. This methodology decouples
the training of the GNN model from that of existing Deep Neural Nets (DNN)
models, eliminating the need for frequent GNN retraining while maintaining
up-to-date graph signals in near realtime, allowing for the effective
integration of GNN insights through transfer learning. The subsequent nearline
inference system serves the GNN encoder within a real-world setting,
significantly reducing online latency and obviating the need for costly
real-time GNN infrastructure. Validated across multiple online A/B tests in
diverse product scenarios, LinkSAGE demonstrates marked improvements in member
engagement, relevance matching, and member retention, confirming its
generalizability and practical impact.</div><div><a href='http://arxiv.org/abs/2402.13430v1'>2402.13430v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13435v1")'>Learning to Retrieve for Job Matching</div>
<div id='2402.13435v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T00:05:25Z</div><div>Authors: Jianqiang Shen, Yuchin Juan, Shaobo Zhang, Ping Liu, Wen Pu, Sriram Vasudevan, Qingquan Song, Fedor Borisyuk, Kay Qianqi Shen, Haichao Wei, Yunxiang Ren, Yeou S. Chiou, Sicong Kuang, Yuan Yin, Ben Zheng, Muchen Wu, Shaghayegh Gharghabi, Xiaoqing Wang, Huichao Xue, Qi Guo, Daniel Hewlett, Luke Simon, Liangjie Hong, Wenjing Zhang</div><div style='padding-top: 10px; width: 80ex'>Web-scale search systems typically tackle the scalability challenge with a
two-step paradigm: retrieval and ranking. The retrieval step, also known as
candidate selection, often involves extracting standardized entities, creating
an inverted index, and performing term matching for retrieval. Such traditional
methods require manual and time-consuming development of query models. In this
paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns
job search and recommendation systems. In the realm of promoted jobs, the key
objective is to improve the quality of applicants, thereby delivering value to
recruiter customers. To achieve this, we leverage confirmed hire data to
construct a graph that evaluates a seeker's qualification for a job, and
utilize learned links for retrieval. Our learned model is easy to explain,
debug, and adjust. On the other hand, the focus for organic jobs is to optimize
seeker engagement. We accomplished this by training embeddings for personalized
retrieval, fortified by a set of rules derived from the categorization of
member feedback. In addition to a solution based on a conventional inverted
index, we developed an on-GPU solution capable of supporting both KNN and term
matching efficiently.</div><div><a href='http://arxiv.org/abs/2402.13435v1'>2402.13435v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15665v1")'>Teacher-Student Learning on Complexity in Intelligent Routing</div>
<div id='2402.15665v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T00:40:40Z</div><div>Authors: Shu-Ting Pi, Michael Yang, Yuying Zhu, Qun Liu</div><div style='padding-top: 10px; width: 80ex'>Customer service is often the most time-consuming aspect for e-commerce
websites, with each contact typically taking 10-15 minutes. Effectively routing
customers to appropriate agents without transfers is therefore crucial for
e-commerce success. To this end, we have developed a machine learning framework
that predicts the complexity of customer contacts and routes them to
appropriate agents accordingly. The framework consists of two parts. First, we
train a teacher model to score the complexity of a contact based on the
post-contact transcripts. Then, we use the teacher model as a data annotator to
provide labels to train a student model that predicts the complexity based on
pre-contact data only. Our experiments show that such a framework is successful
and can significantly improve customer experience. We also propose a useful
metric called complexity AUC that evaluates the effectiveness of customer
service at a statistical level.</div><div><a href='http://arxiv.org/abs/2402.15665v1'>2402.15665v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15655v1")'>Contact Complexity in Customer Service</div>
<div id='2402.15655v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T00:09:27Z</div><div>Authors: Shu-Ting Pi, Michael Yang, Qun Liu</div><div style='padding-top: 10px; width: 80ex'>Customers who reach out for customer service support may face a range of
issues that vary in complexity. Routing high-complexity contacts to junior
agents can lead to multiple transfers or repeated contacts, while directing
low-complexity contacts to senior agents can strain their capacity to assist
customers who need professional help. To tackle this, a machine learning model
that accurately predicts the complexity of customer issues is highly desirable.
However, defining the complexity of a contact is a difficult task as it is a
highly abstract concept. While consensus-based data annotation by experienced
agents is a possible solution, it is time-consuming and costly. To overcome
these challenges, we have developed a novel machine learning approach to define
contact complexity. Instead of relying on human annotation, we trained an AI
expert model to mimic the behavior of agents and evaluate each contact's
complexity based on how the AI expert responds. If the AI expert is uncertain
or lacks the skills to comprehend the contact transcript, it is considered a
high-complexity contact. Our method has proven to be reliable, scalable, and
cost-effective based on the collected data.</div><div><a href='http://arxiv.org/abs/2402.15655v1'>2402.15655v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13973v1")'>Linear-Time Graph Neural Networks for Scalable Recommendations</div>
<div id='2402.13973v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T17:58:10Z</div><div>Authors: Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, Xiaorui Liu</div><div style='padding-top: 10px; width: 80ex'>In an era of information explosion, recommender systems are vital tools to
deliver personalized recommendations for users. The key of recommender systems
is to forecast users' future behaviors based on previous user-item
interactions. Due to their strong expressive power of capturing high-order
connectivities in user-item interaction data, recent years have witnessed a
rising interest in leveraging Graph Neural Networks (GNNs) to boost the
prediction performance of recommender systems. Nonetheless, classic Matrix
Factorization (MF) and Deep Neural Network (DNN) approaches still play an
important role in real-world large-scale recommender systems due to their
scalability advantages. Despite the existence of GNN-acceleration solutions, it
remains an open question whether GNN-based recommender systems can scale as
efficiently as classic MF and DNN methods. In this paper, we propose a
Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender
systems to achieve comparable scalability as classic MF approaches while
maintaining GNNs' powerful expressiveness for superior prediction accuracy.
Extensive experiments and ablation studies are presented to validate the
effectiveness and scalability of the proposed algorithm. Our implementation
based on PyTorch is available.</div><div><a href='http://arxiv.org/abs/2402.13973v1'>2402.13973v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05185v1")'>Personalized Audiobook Recommendations at Spotify Through Graph Neural
  Networks</div>
<div id='2403.05185v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T09:53:07Z</div><div>Authors: Marco De Nadai, Francesco Fabbri, Paul Gigioli, Alice Wang, Ang Li, Fabrizio Silvestri, Laura Kim, Shawn Lin, Vladan Radosavljevic, Sandeep Ghael, David Nyhan, Hugues Bouchard, Mounia Lalmas-Roelleke, Andreas Damianou</div><div style='padding-top: 10px; width: 80ex'>In the ever-evolving digital audio landscape, Spotify, well-known for its
music and talk content, has recently introduced audiobooks to its vast user
base. While promising, this move presents significant challenges for
personalized recommendations. Unlike music and podcasts, audiobooks, initially
available for a fee, cannot be easily skimmed before purchase, posing higher
stakes for the relevance of recommendations. Furthermore, introducing a new
content type into an existing platform confronts extreme data sparsity, as most
users are unfamiliar with this new content type. Lastly, recommending content
to millions of users requires the model to react fast and be scalable. To
address these challenges, we leverage podcast and music user preferences and
introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous
Graph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach
uncovers nuanced item relationships while ensuring low latency and complexity.
We decouple users from the HGNN graph and propose an innovative multi-link
neighbor sampler. These choices, together with the 2T component, significantly
reduce the complexity of the HGNN model. Empirical evaluations involving
millions of users show significant improvement in the quality of personalized
recommendations, resulting in a +46% increase in new audiobooks start rate and
a +23% boost in streaming rates. Intriguingly, our model's impact extends
beyond audiobooks, benefiting established products like podcasts.</div><div><a href='http://arxiv.org/abs/2403.05185v1'>2403.05185v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07478v1")'>Towards Graph Foundation Models for Personalization</div>
<div id='2403.07478v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:12:59Z</div><div>Authors: Andreas Damianou, Francesco Fabbri, Paul Gigioli, Marco De Nadai, Alice Wang, Enrico Palumbo, Mounia Lalmas</div><div style='padding-top: 10px; width: 80ex'>In the realm of personalization, integrating diverse information sources such
as consumption signals and content-based representations is becoming
increasingly critical to build state-of-the-art solutions. In this regard, two
of the biggest trends in research around this subject are Graph Neural Networks
(GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in
industry for powering personalization at scale, FMs have only recently caught
attention for their promising performance in personalization tasks like ranking
and retrieval. In this paper, we present a graph-based foundation modeling
approach tailored to personalization. Central to this approach is a
Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption
relationships across a range of recommendable item types. To ensure the
generality required from a Foundation Model, we employ a Large Language Model
(LLM) text-based featurization of nodes that accommodates all item types, and
construct the graph using co-interaction signals, which inherently transcend
content specificity. To facilitate practical generalization, we further couple
the HGNN with an adaptation mechanism based on a two-tower (2T) architecture,
which also operates agnostically to content type. This multi-stage approach
ensures high scalability; while the HGNN produces general purpose embeddings,
the 2T component models in a continuous space the sheer size of user-item
interaction data. Our comprehensive approach has been rigorously tested and
proven effective in delivering recommendations across a diverse array of
products within a real-world, industrial audio streaming platform.</div><div><a href='http://arxiv.org/abs/2403.07478v1'>2403.07478v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16299v1")'>Against Filter Bubbles: Diversified Music Recommendation via Weighted
  Hypergraph Embedding Learning</div>
<div id='2402.16299v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T04:43:44Z</div><div>Authors: Chaoguang Luo, Liuying Wen, Yong Qin, Liangwei Yang, Zhineng Hu, Philip S. Yu</div><div style='padding-top: 10px; width: 80ex'>Recommender systems serve a dual purpose for users: sifting out inappropriate
or mismatched information while accurately identifying items that align with
their preferences. Numerous recommendation algorithms are designed to provide
users with a personalized array of information tailored to their preferences.
Nevertheless, excessive personalization can confine users within a "filter
bubble". Consequently, achieving the right balance between accuracy and
diversity in recommendations is a pressing concern. To address this challenge,
exemplified by music recommendation, we introduce the Diversified Weighted
Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm,
the initial connections between users and listened tracks are represented by a
weighted hypergraph. Simultaneously, associations between artists, albums and
tags with tracks are also appended to the hypergraph. To explore users' latent
preferences, a hypergraph-based random walk embedding method is applied to the
constructed hypergraph. In our investigation, accuracy is gauged by the
alignment between the user and the track, whereas the array of recommended
track types measures diversity. We rigorously compared DWHRec against seven
state-of-the-art recommendation algorithms using two real-world music datasets.
The experimental results validate DWHRec as a solution that adeptly harmonizes
accuracy and diversity, delivering a more enriched musical experience. Beyond
music recommendation, DWHRec can be extended to cater to other scenarios with
similar data structures.</div><div><a href='http://arxiv.org/abs/2402.16299v1'>2402.16299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02630v2")'>FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal
  Decoupling</div>
<div id='2403.02630v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T03:40:39Z</div><div>Authors: Hongyu Zhang, Dongyi Zheng, Lin Zhong, Xu Yang, Jiyuan Feng, Yunqing Feng, Qing Liao</div><div style='padding-top: 10px; width: 80ex'>In recent years, Cross-Domain Recommendation (CDR) has drawn significant
attention, which utilizes user data from multiple domains to enhance the
recommendation performance. However, current CDR methods require sharing user
data across domains, thereby violating the General Data Protection Regulation
(GDPR). Consequently, numerous approaches have been proposed for Federated
Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity
across different domains inevitably influences the overall performance of
federated learning. In this study, we propose FedHCDR, a novel Federated
Cross-Domain Recommendation framework with Hypergraph signal decoupling.
Specifically, to address the data heterogeneity across domains, we introduce an
approach called hypergraph signal decoupling (HSD) to decouple the user
features into domain-exclusive and domain-shared features. The approach employs
high-pass and low-pass hypergraph filters to decouple domain-exclusive and
domain-shared user representations, which are trained by the local-global
bi-directional transfer algorithm. In addition, a hypergraph contrastive
learning (HCL) module is devised to enhance the learning of domain-shared user
relationship information by perturbing the user hypergraph. Extensive
experiments conducted on three real-world scenarios demonstrate that FedHCDR
outperforms existing baselines significantly.</div><div><a href='http://arxiv.org/abs/2403.02630v2'>2403.02630v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19101v1")'>Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain
  Recommendation</div>
<div id='2402.19101v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T12:29:58Z</div><div>Authors: Jianyu Guan, Zongming Yin, Tianyi Zhang, Leihui Chen, Yin Zhang, Fei Huang, Jufeng Chen, Shuguang Han</div><div style='padding-top: 10px; width: 80ex'>In recent years, the recommendation content on e-commerce platforms has
become increasingly rich -- a single user feed may contain multiple entities,
such as selling products, short videos, and content posts. To deal with the
multi-entity recommendation problem, an intuitive solution is to adopt the
shared-network-based architecture for joint training. The idea is to transfer
the extracted knowledge from one type of entity (source entity) to another
(target entity). However, different from the conventional same-entity
cross-domain recommendation, multi-entity knowledge transfer encounters several
important issues: (1) data distributions of the source entity and target entity
are naturally different, making the shared-network-based joint training
susceptible to the negative transfer issue, (2) more importantly, the
corresponding feature schema of each entity is not exactly aligned (e.g., price
is an essential feature for selling product while missing for content posts),
making the existing methods no longer appropriate. Recent researchers have also
experimented with the pre-training and fine-tuning paradigm. Again, they only
consider the scenarios with the same entity type and feature systems, which is
inappropriate in our case. To this end, we design a pre-training &amp; fine-tuning
based Multi-entity Knowledge Transfer framework called MKT. MKT utilizes a
multi-entity pre-training module to extract transferable knowledge across
different entities. In particular, a feature alignment module is first applied
to scale and align different feature schemas. Afterward, a couple of knowledge
extractors are employed to extract the common and entity-specific knowledge. In
the end, the extracted common knowledge is adopted for target entity model
training. Through extensive offline and online experiments, we demonstrated the
superiority of MKT over multiple State-Of-The-Art methods.</div><div><a href='http://arxiv.org/abs/2402.19101v1'>2402.19101v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.08426v1")'>Frequency-aware Graph Signal Processing for Collaborative Filtering</div>
<div id='2402.08426v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T12:53:18Z</div><div>Authors: Jiafeng Xia, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu</div><div style='padding-top: 10px; width: 80ex'>Graph Signal Processing (GSP) based recommendation algorithms have recently
attracted lots of attention due to its high efficiency. However, these methods
failed to consider the importance of various interactions that reflect unique
user/item characteristics and failed to utilize user and item high-order
neighborhood information to model user preference, thus leading to sub-optimal
performance. To address the above issues, we propose a frequency-aware graph
signal processing method (FaGSP) for collaborative filtering. Firstly, we
design a Cascaded Filter Module, consisting of an ideal high-pass filter and an
ideal low-pass filter that work in a successive manner, to capture both unique
and common user/item characteristics to more accurately model user preference.
Then, we devise a Parallel Filter Module, consisting of two low-pass filters
that can easily capture the hierarchy of neighborhood, to fully utilize
high-order neighborhood information of users/items for more accurate user
preference modeling. Finally, we combine these two modules via a linear model
to further improve recommendation accuracy. Extensive experiments on six public
datasets demonstrate the superiority of our method from the perspectives of
prediction accuracy and training efficiency compared with state-of-the-art
GCN-based recommendation methods and GSP-based recommendation methods.</div><div><a href='http://arxiv.org/abs/2402.08426v1'>2402.08426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14296v1")'>"All of Me": Mining Users' Attributes from their Public Spotify
  Playlists</div>
<div id='2401.14296v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T16:38:06Z</div><div>Authors: Pier Paolo Tricomi, Luca Pajola, Luca Pasa, Mauro Conti</div><div style='padding-top: 10px; width: 80ex'>In the age of digital music streaming, playlists on platforms like Spotify
have become an integral part of individuals' musical experiences. People create
and publicly share their own playlists to express their musical tastes, promote
the discovery of their favorite artists, and foster social connections. These
publicly accessible playlists transcend the boundaries of mere musical
preferences: they serve as sources of rich insights into users' attributes and
identities. For example, the musical preferences of elderly individuals may
lean more towards Frank Sinatra, while Billie Eilish remains a favored choice
among teenagers. These playlists thus become windows into the diverse and
evolving facets of one's musical identity.
  In this work, we investigate the relationship between Spotify users'
attributes and their public playlists. In particular, we focus on identifying
recurring musical characteristics associated with users' individual attributes,
such as demographics, habits, or personality traits. To this end, we conducted
an online survey involving 739 Spotify users, yielding a dataset of 10,286
publicly shared playlists encompassing over 200,000 unique songs and 55,000
artists. Through extensive statistical analyses, we first assess a deep
connection between a user's Spotify playlists and their real-life attributes.
For instance, we found individuals high in openness often create playlists
featuring a diverse array of artists, while female users prefer Pop and K-pop
music genres. Building upon these observed associations, we create accurate
predictive models for users' attributes, presenting a novel DeepSet application
that outperforms baselines in most of these users' attributes.</div><div><a href='http://arxiv.org/abs/2401.14296v1'>2401.14296v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08818v1")'>Link Me Baby One More Time: Social Music Discovery on Spotify</div>
<div id='2401.08818v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T20:41:11Z</div><div>Authors: Shazia'Ayn Babul, Desislava Hristova, Antonio Lima, Renaud Lambiotte, Mariano Beguerisse-Díaz</div><div style='padding-top: 10px; width: 80ex'>We explore the social and contextual factors that influence the outcome of
person-to-person music recommendations and discovery. Specifically, we use data
from Spotify to investigate how a link sent from one user to another results in
the receiver engaging with the music of the shared artist. We consider several
factors that may influence this process, such as the strength of the
sender-receiver relationship, the user's role in the Spotify social network,
their music social cohesion, and how similar the new artist is to the
receiver's taste. We find that the receiver of a link is more likely to engage
with a new artist when (1) they have similar music taste to the sender and the
shared track is a good fit for their taste, (2) they have a stronger and more
intimate tie with the sender, and (3) the shared artist is popular with the
receiver's connections. Finally, we use these findings to build a Random Forest
classifier to predict whether a shared music track will result in the
receiver's engagement with the shared artist. This model elucidates which type
of social and contextual features are most predictive, although peak
performance is achieved when a diverse set of features are included. These
findings provide new insights into the multifaceted mechanisms underpinning the
interplay between music discovery and social processes.</div><div><a href='http://arxiv.org/abs/2401.08818v1'>2401.08818v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12079v1")'>Beyond Beats: A Recipe to Song Popularity? A machine learning approach</div>
<div id='2403.12079v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T17:14:41Z</div><div>Authors: Niklas Sebastian, Jung, Florian Mayer</div><div style='padding-top: 10px; width: 80ex'>Music popularity prediction has garnered significant attention in both
industry and academia, fuelled by the rise of data-driven algorithms and
streaming platforms like Spotify. This study aims to explore the predictive
power of various machine learning models in forecasting song popularity using a
dataset comprising 30,000 songs spanning different genres from 1957 to 2020.
Methods: We employ Ordinary Least Squares (OLS), Multivariate Adaptive
Regression Splines (MARS), Random Forest, and XGBoost algorithms to analyse
song characteristics and their impact on popularity. Results: Ordinary Least
Squares (OLS) regression analysis reveals genre as the primary influencer of
popularity, with notable trends over time. MARS modelling highlights the
complex relationship between variables, particularly with features like
instrumentalness and duration. Random Forest and XGBoost models underscore the
importance of genre, especially EDM, in predicting popularity. Despite
variations in performance, Random Forest emerges as the most effective model,
improving prediction accuracy by 7.1% compared to average scores. Despite the
importance of genre, predicting song popularity remains challenging, as
observed variations in music-related features suggest complex interactions
between genre and other factors. Consequently, while certain characteristics
like loudness and song duration may impact popularity scores, accurately
predicting song success remains elusive.</div><div><a href='http://arxiv.org/abs/2403.12079v1'>2403.12079v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02827v1")'>Let's Get It Started: Fostering the Discoverability of New Releases on
  Deezer</div>
<div id='2401.02827v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T14:21:10Z</div><div>Authors: Léa Briand, Théo Bontempelli, Walid Bendada, Mathieu Morlon, François Rigaud, Benjamin Chapus, Thomas Bouabça, Guillaume Salha-Galvan</div><div style='padding-top: 10px; width: 80ex'>This paper presents our recent initiatives to foster the discoverability of
new releases on the music streaming service Deezer. After introducing our
search and recommendation features dedicated to new releases, we outline our
shift from editorial to personalized release suggestions using cold start
embeddings and contextual bandits. Backed by online experiments, we discuss the
advantages of this shift in terms of recommendation quality and exposure of new
releases on the service.</div><div><a href='http://arxiv.org/abs/2401.02827v1'>2401.02827v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.03162v1")'>QoS-Aware Graph Contrastive Learning for Web Service Recommendation</div>
<div id='2401.03162v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:36:04Z</div><div>Authors: Jeongwhan Choi, Duksan Ryu</div><div style='padding-top: 10px; width: 80ex'>With the rapid growth of cloud services driven by advancements in web service
technology, selecting a high-quality service from a wide range of options has
become a complex task. This study aims to address the challenges of data
sparsity and the cold-start problem in web service recommendation using Quality
of Service (QoS). We propose a novel approach called QoS-aware graph
contrastive learning (QAGCL) for web service recommendation. Our model
harnesses the power of graph contrastive learning to handle cold-start problems
and improve recommendation accuracy effectively. By constructing contextually
augmented graphs with geolocation information and randomness, our model
provides diverse views. Through the use of graph convolutional networks and
graph contrastive learning techniques, we learn user and service embeddings
from these augmented graphs. The learned embeddings are then utilized to
seamlessly integrate QoS considerations into the recommendation process.
Experimental results demonstrate the superiority of our QAGCL model over
several existing models, highlighting its effectiveness in addressing data
sparsity and the cold-start problem in QoS-aware service recommendations. Our
research contributes to the potential for more accurate recommendations in
real-world scenarios, even with limited user-service interaction data.</div><div><a href='http://arxiv.org/abs/2401.03162v1'>2401.03162v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11624v1")'>Dual-Channel Multiplex Graph Neural Networks for Recommendation</div>
<div id='2403.11624v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T09:56:00Z</div><div>Authors: Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu Dong, Yanwei Yu</div><div style='padding-top: 10px; width: 80ex'>Efficient recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interaction relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant shortcomings: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations in the behavior patterns on the target relation in recommender system
scenarios. In this study, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interaction relations, and includes a relation chain representation
learning and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our \model
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06\% and 12.15\% on average across all datasets in terms
of R@10 and N@10 respectively.</div><div><a href='http://arxiv.org/abs/2403.11624v1'>2403.11624v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00895v3")'>End-to-End Graph-Sequential Representation Learning for Accurate
  Recommendations</div>
<div id='2403.00895v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T15:32:44Z</div><div>Authors: Vladimir Baikalov, Evgeny Frolov</div><div style='padding-top: 10px; width: 80ex'>Recent recommender system advancements have focused on developing
sequence-based and graph-based approaches. Both approaches proved useful in
modeling intricate relationships within behavioral data, leading to promising
outcomes in personalized ranking and next-item recommendation tasks while
maintaining good scalability. However, they capture very different signals from
data. While the former approach represents users directly through ordered
interactions with recent items, the latter aims to capture indirect
dependencies across the interactions graph. This paper presents a novel
multi-representational learning framework exploiting these two paradigms'
synergies. Our empirical evaluation on several datasets demonstrates that
mutual training of sequential and graph components with the proposed framework
significantly improves recommendations performance.</div><div><a href='http://arxiv.org/abs/2403.00895v3'>2403.00895v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01243v1")'>Contrastive Sequential Interaction Network Learning on Co-Evolving
  Riemannian Spaces</div>
<div id='2401.01243v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T15:19:01Z</div><div>Authors: Li Sun, Junda Ye, Jiawei Zhang, Yong Yang, Mingsheng Liu, Feiyang Wang, Philip S. Yu</div><div style='padding-top: 10px; width: 80ex'>The sequential interaction network usually find itself in a variety of
applications, e.g., recommender system. Herein, inferring future interaction is
of fundamental importance, and previous efforts are mainly focused on the
dynamics in the classic zero-curvature Euclidean space. Despite the promising
results achieved by previous methods, a range of significant issues still
largely remains open: On the bipartite nature, is it appropriate to place user
and item nodes in one identical space regardless of their inherent difference?
On the network dynamics, instead of a fixed curvature space, will the
representation spaces evolve when new interactions arrive continuously? On the
learning paradigm, can we get rid of the label information costly to acquire?
To address the aforementioned issues, we propose a novel Contrastive model for
Sequential Interaction Network learning on Co-Evolving RiEmannian spaces,
CSINCERE. To the best of our knowledge, we are the first to introduce a couple
of co-evolving representation spaces, rather than a single or static space, and
propose a co-contrastive learning for the sequential interaction network. In
CSINCERE, we formulate a Cross-Space Aggregation for message-passing across
representation spaces of different Riemannian geometries, and design a Neural
Curvature Estimator based on Ricci curvatures for modeling the space evolvement
over time. Thereafter, we present a Reweighed Co-Contrast between the temporal
views of the sequential network, so that the couple of Riemannian spaces
interact with each other for the interaction prediction without labels.
Empirical results on 5 public datasets show the superiority of CSINCERE over
the state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2401.01243v1'>2401.01243v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07769v2")'>Deep Evolutional Instant Interest Network for CTR Prediction in
  Trigger-Induced Recommendation</div>
<div id='2401.07769v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T15:27:24Z</div><div>Authors: Zhibo Xiao, Luwei Yang, Tao Zhang, Wen Jiang, Wei Ning, Yujiu Yang</div><div style='padding-top: 10px; width: 80ex'>The recommendation has been playing a key role in many industries, e.g.,
e-commerce, streaming media, social media, etc. Recently, a new recommendation
scenario, called Trigger-Induced Recommendation (TIR), where users are able to
explicitly express their instant interests via trigger items, is emerging as an
essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.
Without explicitly modeling the user's instant interest, traditional
recommendation methods usually obtain sub-optimal results in TIR. Even though
there are a few methods considering the trigger and target items simultaneously
to solve this problem, they still haven't taken into account temporal
information of user behaviors, the dynamic change of user instant interest when
the user scrolls down and the interactions between the trigger and target
items. To tackle these problems, we propose a novel method -- Deep Evolutional
Instant Interest Network (DEI2N), for click-through rate prediction in TIR
scenarios. Specifically, we design a User Instant Interest Modeling Layer to
predict the dynamic change of the intensity of instant interest when the user
scrolls down. Temporal information is utilized in user behavior modeling.
Moreover, an Interaction Layer is introduced to learn better interactions
between the trigger and target items. We evaluate our method on several offline
and real-world industrial datasets. Experimental results show that our proposed
DEI2N outperforms state-of-the-art baselines. In addition, online A/B testing
demonstrates the superiority over the existing baseline in real-world
production environments.</div><div><a href='http://arxiv.org/abs/2401.07769v2'>2401.07769v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16433v2")'>Within-basket Recommendation via Neural Pattern Associator</div>
<div id='2401.16433v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T19:40:55Z</div><div>Authors: Kai Luo, Tianshu Shen, Lan Yao, Ga Wu, Aaron Liblong, Istvan Fehervari, Ruijian An, Jawad Ahmed, Harshit Mishra, Charu Pujari</div><div style='padding-top: 10px; width: 80ex'>Within-basket recommendation (WBR) refers to the task of recommending items
to the end of completing a non-empty shopping basket during a shopping session.
While the latest innovations in this space demonstrate remarkable performance
improvement on benchmark datasets, they often overlook the complexity of user
behaviors in practice, such as 1) co-existence of multiple shopping intentions,
2) multi-granularity of such intentions, and 3) interleaving behavior
(switching intentions) in a shopping session. This paper presents Neural
Pattern Associator (NPA), a deep item-association-mining model that explicitly
models the aforementioned factors. Specifically, inspired by vector
quantization, the NPA model learns to encode common user intentions (or
item-combination patterns) as quantized representations (a.k.a. codebook),
which permits identification of users's shopping intentions via
attention-driven lookup during the reasoning phase. This yields coherent and
self-interpretable recommendations. We evaluated the proposed NPA model across
multiple extensive datasets, encompassing the domains of grocery e-commerce
(shopping basket completion) and music (playlist extension), where our
quantitative evaluations show that the NPA model significantly outperforms a
wide range of existing WBR solutions, reflecting the benefit of explicitly
modeling complex user intentions.</div><div><a href='http://arxiv.org/abs/2401.16433v2'>2401.16433v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14377v1")'>Knowledge-Enhanced Recommendation with User-Centric Subgraph Network</div>
<div id='2403.14377v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T13:09:23Z</div><div>Authors: Guangyi Liu, Quanming Yao, Yongqi Zhang, Lei Chen</div><div style='padding-top: 10px; width: 80ex'>Recommendation systems, as widely implemented nowadays on various platforms,
recommend relevant items to users based on their preferences. The classical
methods which rely on user-item interaction matrices has limitations,
especially in scenarios where there is a lack of interaction data for new
items. Knowledge graph (KG)-based recommendation systems have emerged as a
promising solution. However, most KG-based methods adopt node embeddings, which
do not provide personalized recommendations for different users and cannot
generalize well to the new items. To address these limitations, we propose
Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning
approach with graph neural network (GNN) for effective recommendation. KUCNet
constructs a U-I subgraph for each user-item pair that captures both the
historical information of user-item interactions and the side information
provided in KG. An attention-based GNN is designed to encode the U-I subgraphs
for recommendation. Considering efficiency, the pruned user-centric computation
graph is further introduced such that multiple U-I subgraphs can be
simultaneously computed and that the size can be pruned by Personalized
PageRank. Our proposed method achieves accurate, efficient, and interpretable
recommendations especially for new items. Experimental results demonstrate the
superiority of KUCNet over state-of-the-art KG-based and collaborative
filtering (CF)-based methods.</div><div><a href='http://arxiv.org/abs/2403.14377v1'>2403.14377v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06436v1")'>Improving Graph Convolutional Networks with Transformer Layer in
  social-based items recommendation</div>
<div id='2401.06436v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T08:07:09Z</div><div>Authors: Thi Linh Hoang, Tuan Dung Pham, Viet Cuong Ta</div><div style='padding-top: 10px; width: 80ex'>In this work, we have proposed an approach for improving the GCN for
predicting ratings in social networks. Our model is expanded from the standard
model with several layers of transformer architecture. The main focus of the
paper is on the encoder architecture for node embedding in the network. Using
the embedding layer from the graph-based convolution layer, the attention
mechanism could rearrange the feature space to get a more efficient embedding
for the downstream task. The experiments showed that our proposed architecture
achieves better performance than GCN on the traditional link prediction task.</div><div><a href='http://arxiv.org/abs/2401.06436v1'>2401.06436v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03365v1")'>Heterophily-Aware Fair Recommendation using Graph Convolutional Networks</div>
<div id='2402.03365v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T11:03:58Z</div><div>Authors: Nemat Gholinejad, Mostafa Haghir Chehreghani</div><div style='padding-top: 10px; width: 80ex'>In recent years, graph neural networks (GNNs) have become a popular tool to
improve the accuracy and performance of recommender systems. Modern recommender
systems are not only designed to serve the end users, but also to benefit other
participants, such as items and items providers. These participants may have
different or conflicting goals and interests, which raise the need for fairness
and popularity bias considerations. GNN-based recommendation methods also face
the challenges of unfairness and popularity bias and their normalization and
aggregation processes suffer from these challenges. In this paper, we propose a
fair GNN-based recommender system, called HetroFair, to improve items' side
fairness. HetroFair uses two separate components to generate fairness-aware
embeddings: i) fairness-aware attention which incorporates dot product in the
normalization process of GNNs, to decrease the effect of nodes' degrees, and
ii) heterophily feature weighting to assign distinct weights to different
features during the aggregation process. In order to evaluate the effectiveness
of HetroFair, we conduct extensive experiments over six real-world datasets.
Our experimental results reveal that HetroFair not only alleviates the
unfairness and popularity bias on the items' side, but also achieves superior
accuracy on the users' side. Our implementation is publicly available at
https://github.com/NematGH/HetroFair</div><div><a href='http://arxiv.org/abs/2402.03365v1'>2402.03365v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10799v1")'>Novel Representation Learning Technique using Graphs for Performance
  Analytics</div>
<div id='2401.10799v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:34:37Z</div><div>Authors: Tarek Ramadan, Ankur Lahiry, Tanzima Z. Islam</div><div style='padding-top: 10px; width: 80ex'>The performance analytics domain in High Performance Computing (HPC) uses
tabular data to solve regression problems, such as predicting the execution
time. Existing Machine Learning (ML) techniques leverage the correlations among
features given tabular datasets, not leveraging the relationships between
samples directly. Moreover, since high-quality embeddings from raw features
improve the fidelity of the downstream predictive models, existing methods rely
on extensive feature engineering and pre-processing steps, costing time and
manual effort. To fill these two gaps, we propose a novel idea of transforming
tabular performance data into graphs to leverage the advancement of Graph
Neural Network-based (GNN) techniques in capturing complex relationships
between features and samples. In contrast to other ML application domains, such
as social networks, the graph is not given; instead, we need to build it. To
address this gap, we propose graph-building methods where nodes represent
samples, and the edges are automatically inferred iteratively based on the
similarity between the features in the samples. We evaluate the effectiveness
of the generated embeddings from GNNs based on how well they make even a simple
feed-forward neural network perform for regression tasks compared to other
state-of-the-art representation learning techniques. Our evaluation
demonstrates that even with up to 25% random missing values for each dataset,
our method outperforms commonly used graph and Deep Neural Network (DNN)-based
approaches and achieves up to 61.67% &amp; 78.56% improvement in MSE loss over the
DNN baseline respectively for HPC dataset and Machine Learning Datasets.</div><div><a href='http://arxiv.org/abs/2401.10799v1'>2401.10799v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06176v1")'>GOODAT: Towards Test-time Graph Out-of-Distribution Detection</div>
<div id='2401.06176v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T08:37:39Z</div><div>Authors: Luzhi Wang, Dongxiao He, He Zhang, Yixin Liu, Wenjie Wang, Shirui Pan, Di Jin, Tat-Seng Chua</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have found widespread application in modeling
graph data across diverse domains. While GNNs excel in scenarios where the
testing data shares the distribution of their training counterparts (in
distribution, ID), they often exhibit incorrect predictions when confronted
with samples from an unfamiliar distribution (out-of-distribution, OOD). To
identify and reject OOD samples with GNNs, recent studies have explored graph
OOD detection, often focusing on training a specific model or modifying the
data on top of a well-trained GNN. Despite their effectiveness, these methods
come with heavy training resources and costs, as they need to optimize the
GNN-based models on training data. Moreover, their reliance on modifying the
original GNNs and accessing training data further restricts their universality.
To this end, this paper introduces a method to detect Graph Out-of-Distribution
At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play
solution that operates independently of training data and modifications of GNN
architecture. With a lightweight graph masker, GOODAT can learn informative
subgraphs from test samples, enabling the capture of distinct graph patterns
between OOD and ID samples. To optimize the graph masker, we meticulously
design three unsupervised objective functions based on the graph information
bottleneck principle, motivating the masker to capture compact yet informative
subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT
method outperforms state-of-the-art benchmarks across a variety of real-world
datasets. The code is available at Github: https://github.com/Ee1s/GOODAT</div><div><a href='http://arxiv.org/abs/2401.06176v1'>2401.06176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11153v1")'>Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on
  Graphs</div>
<div id='2402.11153v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T00:40:12Z</div><div>Authors: Shuhan Liu, Kaize Ding</div><div style='padding-top: 10px; width: 80ex'>Distribution shifts on graphs -- the data distribution discrepancies between
training and testing a graph machine learning model, are often ubiquitous and
unavoidable in real-world scenarios. Such shifts may severely deteriorate the
performance of the model, posing significant challenges for reliable graph
machine learning. Consequently, there has been a surge in research on graph
Out-Of-Distribution (OOD) adaptation methods that aim to mitigate the
distribution shifts and adapt the knowledge from one distribution to another.
In our survey, we provide an up-to-date and forward-looking review of graph OOD
adaptation methods, covering two main problem scenarios including training-time
as well as test-time graph OOD adaptation. We start by formally formulating the
two problems and then discuss different types of distribution shifts on graphs.
Based on our proposed taxonomy for graph OOD adaptation, we systematically
categorize the existing methods according to their learning paradigm and
investigate the techniques behind them. Finally, we point out promising
research directions and the corresponding challenges. We also provide a
continuously updated reading list at
https://github.com/kaize0409/Awesome-Graph-OOD-Adaptation.git</div><div><a href='http://arxiv.org/abs/2402.11153v1'>2402.11153v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16374v2")'>Graph Learning under Distribution Shifts: A Comprehensive Survey on
  Domain Adaptation, Out-of-distribution, and Continual Learning</div>
<div id='2402.16374v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:52:40Z</div><div>Authors: Man Wu, Xin Zheng, Qin Zhang, Xiao Shen, Xiong Luo, Xingquan Zhu, Shirui Pan</div><div style='padding-top: 10px; width: 80ex'>Graph learning plays a pivotal role and has gained significant attention in
various application scenarios, from social network analysis to recommendation
systems, for its effectiveness in modeling complex data relations represented
by graph structural data. In reality, the real-world graph data typically show
dynamics over time, with changing node attributes and edge structure, leading
to the severe graph data distribution shift issue. This issue is compounded by
the diverse and complex nature of distribution shifts, which can significantly
impact the performance of graph learning methods in degraded generalization and
adaptation capabilities, posing a substantial challenge to their effectiveness.
In this survey, we provide a comprehensive review and summary of the latest
approaches, strategies, and insights that address distribution shifts within
the context of graph learning. Concretely, according to the observability of
distributions in the inference stage and the availability of sufficient
supervision information in the training stage, we categorize existing graph
learning methods into several essential scenarios, including graph domain
adaptation learning, graph out-of-distribution learning, and graph continual
learning. For each scenario, a detailed taxonomy is proposed, with specific
descriptions and discussions of existing progress made in distribution-shifted
graph learning. Additionally, we discuss the potential applications and future
directions for graph learning under distribution shifts with a systematic
analysis of the current state in this field. The survey is positioned to
provide general guidance for the development of effective graph learning
algorithms in handling graph distribution shifts, and to stimulate future
research and advancements in this area.</div><div><a href='http://arxiv.org/abs/2402.16374v2'>2402.16374v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11565v1")'>Continual Learning on Graphs: Challenges, Solutions, and Opportunities</div>
<div id='2402.11565v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T12:24:45Z</div><div>Authors: Xikun Zhang, Dongjin Song, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Continual learning on graph data has recently attracted paramount attention
for its aim to resolve the catastrophic forgetting problem on existing tasks
while adapting the sequentially updated model to newly emerged graph tasks.
While there have been efforts to summarize progress on continual learning
research over Euclidean data, e.g., images and texts, a systematic review of
progress in continual learning on graphs, a.k.a, continual graph learning (CGL)
or lifelong graph learning, is still demanding. Graph data are far more complex
in terms of data structures and application scenarios, making CGL task
settings, model designs, and applications extremely challenging. To bridge the
gap, we provide a comprehensive review of existing continual graph learning
(CGL) algorithms by elucidating the different task settings and categorizing
the existing methods based on their characteristics. We compare the CGL methods
with traditional continual learning techniques and analyze the applicability of
the traditional continual learning techniques to CGL tasks. Additionally, we
review the benchmark works that are crucial to CGL research. Finally, we
discuss the remaining challenges and propose several future directions. We will
maintain an up-to-date GitHub repository featuring a comprehensive list of CGL
algorithms, accessible at
https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs.</div><div><a href='http://arxiv.org/abs/2402.11565v1'>2402.11565v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13711v4")'>DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based
  Graph Continual Learning</div>
<div id='2402.13711v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T11:25:54Z</div><div>Authors: Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim, Chanyoung Park</div><div style='padding-top: 10px; width: 80ex'>We investigate the replay buffer in rehearsal-based approaches for graph
continual learning (GCL) methods. Existing rehearsal-based GCL methods select
the most representative nodes for each class and store them in a replay buffer
for later use in training subsequent tasks. However, we discovered that
considering only the class representativeness of each replayed node makes the
replayed nodes to be concentrated around the center of each class, incurring a
potential risk of overfitting to nodes residing in those regions, which
aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach
heavily relies on a few replayed nodes to retain knowledge obtained from
previous tasks, involving the replayed nodes that have irrelevant neighbors in
the model training may have a significant detrimental impact on model
performance. In this paper, we propose a GCL model named DSLR, specifically, we
devise a coverage-based diversity (CD) approach to consider both the class
representativeness and the diversity within each class of the replayed nodes.
Moreover, we adopt graph structure learning (GSL) to ensure that the replayed
nodes are connected to truly informative neighbors. Extensive experimental
results demonstrate the effectiveness and efficiency of DSLR. Our source code
is available at https://github.com/seungyoon-Choi/DSLR_official.</div><div><a href='http://arxiv.org/abs/2402.13711v4'>2402.13711v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12987v1")'>Towards Robust Graph Incremental Learning on Evolving Graphs</div>
<div id='2402.12987v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T13:17:37Z</div><div>Authors: Junwei Su, Difan Zou, Zijun Zhang, Chuan Wu</div><div style='padding-top: 10px; width: 80ex'>Incremental learning is a machine learning approach that involves training a
model on a sequence of tasks, rather than all tasks at once. This ability to
learn incrementally from a stream of tasks is crucial for many real-world
applications. However, incremental learning is a challenging problem on
graph-structured data, as many graph-related problems involve prediction tasks
for each individual node, known as Node-wise Graph Incremental Learning (NGIL).
This introduces non-independent and non-identically distributed characteristics
in the sample data generation process, making it difficult to maintain the
performance of the model as new tasks are added. In this paper, we focus on the
inductive NGIL problem, which accounts for the evolution of graph structure
(structural shift) induced by emerging tasks. We provide a formal formulation
and analysis of the problem, and propose a novel regularization-based technique
called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the
structural shift on catastrophic forgetting of the inductive NGIL problem. We
show that the structural shift can lead to a shift in the input distribution
for the existing tasks, and further lead to an increased risk of catastrophic
forgetting. Through comprehensive empirical studies with several benchmark
datasets, we demonstrate that our proposed method,
Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to
improve the performance of state-of-the-art GNN incremental learning frameworks
in the inductive setting.</div><div><a href='http://arxiv.org/abs/2402.12987v1'>2402.12987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07738v2")'>Universal Link Predictor By In-Context Learning on Graphs</div>
<div id='2402.07738v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:52:27Z</div><div>Authors: Kaiwen Dong, Haitao Mao, Zhichun Guo, Nitesh V. Chawla</div><div style='padding-top: 10px; width: 80ex'>Link prediction is a crucial task in graph machine learning, where the goal
is to infer missing or future links within a graph. Traditional approaches
leverage heuristic methods based on widely observed connectivity patterns,
offering broad applicability and generalizability without the need for model
training. Despite their utility, these methods are limited by their reliance on
human-derived heuristics and lack the adaptability of data-driven approaches.
Conversely, parametric link predictors excel in automatically learning the
connectivity patterns from data and achieving state-of-the-art but fail short
to directly transfer across different graphs. Instead, it requires the cost of
extensive training and hyperparameter optimization to adapt to the target
graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel
model that combines the generalizability of heuristic approaches with the
pattern learning capabilities of parametric models. UniLP is designed to
autonomously identify connectivity patterns across diverse graphs, ready for
immediate application to any unseen graph dataset without targeted training. We
address the challenge of conflicting connectivity patterns-arising from the
unique distributions of different graphs-through the implementation of
In-context Learning (ICL). This approach allows UniLP to dynamically adjust to
various target graphs based on contextual demonstrations, thereby avoiding
negative transfer. Through rigorous experimentation, we demonstrate UniLP's
effectiveness in adapting to new, unseen graphs at test time, showcasing its
ability to perform comparably or even outperform parametric models that have
been finetuned for specific datasets. Our findings highlight UniLP's potential
to set a new standard in link prediction, combining the strengths of heuristic
and parametric methods in a single, versatile framework.</div><div><a href='http://arxiv.org/abs/2402.07738v2'>2402.07738v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08174v1")'>Hierarchical Position Embedding of Graphs with Landmarks and Clustering
  for Link Prediction</div>
<div id='2402.08174v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T02:13:12Z</div><div>Authors: Minsang Kim, Seungjun Baek</div><div style='padding-top: 10px; width: 80ex'>Learning positional information of nodes in a graph is important for link
prediction tasks. We propose a representation of positional information using
representative nodes called landmarks. A small number of nodes with high degree
centrality are selected as landmarks, which serve as reference points for the
nodes' positions. We justify this selection strategy for well-known random
graph models and derive closed-form bounds on the average path lengths
involving landmarks. In a model for power-law graphs, we prove that landmarks
provide asymptotically exact information on inter-node distances. We apply
theoretical insights to practical networks and propose Hierarchical Position
embedding with Landmarks and Clustering (HPLC). HPLC combines landmark
selection and graph clustering, where the graph is partitioned into densely
connected clusters in which nodes with the highest degree are selected as
landmarks. HPLC leverages the positional information of nodes based on
landmarks at various levels of hierarchy such as nodes' distances to landmarks,
inter-landmark distances and hierarchical grouping of clusters. Experiments
show that HPLC achieves state-of-the-art performances of link prediction on
various datasets in terms of HIT@K, MRR, and AUC. The code is available at
\url{https://github.com/kmswin1/HPLC}.</div><div><a href='http://arxiv.org/abs/2402.08174v1'>2402.08174v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11354v1")'>Probabilistic Routing for Graph-Based Approximate Nearest Neighbor
  Search</div>
<div id='2402.11354v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T18:08:37Z</div><div>Authors: Kejing Lu, Chuan Xiao, Yoshiharu Ishikawa</div><div style='padding-top: 10px; width: 80ex'>Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a
pivotal challenge in the field of machine learning. In recent years,
graph-based methods have emerged as the superior approach to ANNS, establishing
a new state of the art. Although various optimizations for graph-based ANNS
have been introduced, they predominantly rely on heuristic methods that lack
formal theoretical backing. This paper aims to enhance routing within
graph-based ANNS by introducing a method that offers a probabilistic guarantee
when exploring a node's neighbors in the graph. We formulate the problem as
probabilistic routing and develop two baseline strategies by incorporating
locality-sensitive techniques. Subsequently, we introduce PEOs, a novel
approach that efficiently identifies which neighbors in the graph should be
considered for exact distance computation, thus significantly improving
efficiency in practice. Our experiments demonstrate that equipping PEOs can
increase throughput on a commonly utilized graph index (HNSW) by a factor of
1.6 to 2.5, and its efficiency consistently outperforms the leading-edge
routing technique by 1.1 to 1.4 times.</div><div><a href='http://arxiv.org/abs/2402.11354v1'>2402.11354v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.01943v2")'>Precedence-Constrained Winter Value for Effective Graph Data Valuation</div>
<div id='2402.01943v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T22:39:50Z</div><div>Authors: Hongliang Chi, Wei Jin, Charu Aggarwal, Yao Ma</div><div style='padding-top: 10px; width: 80ex'>Data valuation is essential for quantifying data's worth, aiding in assessing
data quality and determining fair compensation. While existing data valuation
methods have proven effective in evaluating the value of Euclidean data, they
face limitations when applied to the increasingly popular graph-structured
data. Particularly, graph data valuation introduces unique challenges,
primarily stemming from the intricate dependencies among nodes and the
exponential growth in value estimation costs. To address the challenging
problem of graph data valuation, we put forth an innovative solution,
Precedence-Constrained Winter (PC-Winter) Value, to account for the complex
graph structure. Furthermore, we develop a variety of strategies to address the
computational challenges and enable efficient approximation of PC-Winter.
Extensive experiments demonstrate the effectiveness of PC-Winter across diverse
datasets and tasks.</div><div><a href='http://arxiv.org/abs/2402.01943v2'>2402.01943v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09165v1")'>Unifying Invariance and Spuriousity for Graph Out-of-Distribution via
  Probability of Necessity and Sufficiency</div>
<div id='2402.09165v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T13:31:53Z</div><div>Authors: Xuexin Chen, Ruichu Cai, Kaitao Zheng, Zhifan Jiang, Zhengting Huang, Zhifeng Hao, Zijian Li</div><div style='padding-top: 10px; width: 80ex'>Graph Out-of-Distribution (OOD), requiring that models trained on biased data
generalize to the unseen test data, has a massive of real-world applications.
One of the most mainstream methods is to extract the invariant subgraph by
aligning the original and augmented data with the help of environment
augmentation. However, these solutions might lead to the loss or redundancy of
semantic subgraph and further result in suboptimal generalization. To address
this challenge, we propose a unified framework to exploit the Probability of
Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond
that, this framework further leverages the spurious subgraph to boost the
generalization performance in an ensemble manner to enhance the robustness on
the noise data. Specificially, we first consider the data generation process
for graph data. Under mild conditions, we show that the invariant subgraph can
be extracted by minimizing an upper bound, which is built on the theoretical
advance of probability of necessity and sufficiency. To further bridge the
theory and algorithm, we devise the PNSIS model, which involves an invariant
subgraph extractor for invariant graph learning as well invariant and spurious
subgraph classifiers for generalization enhancement. Experimental results
demonstrate that our \textbf{PNSIS} model outperforms the state-of-the-art
techniques on graph OOD on several benchmarks, highlighting the effectiveness
in real-world scenarios.</div><div><a href='http://arxiv.org/abs/2402.09165v1'>2402.09165v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11641v2")'>Towards Versatile Graph Learning Approach: from the Perspective of Large
  Language Models</div>
<div id='2402.11641v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T16:43:21Z</div><div>Authors: Lanning Wei, Jun Gao, Huan Zhao, Quanming Yao</div><div style='padding-top: 10px; width: 80ex'>Graph-structured data are the commonly used and have wide application
scenarios in the real world. For these diverse applications, the vast variety
of learning tasks, graph domains, and complex graph learning procedures present
challenges for human experts when designing versatile graph learning
approaches. Facing these challenges, large language models (LLMs) offer a
potential solution due to the extensive knowledge and the human-like
intelligence. This paper proposes a novel conceptual prototype for designing
versatile graph learning methods with LLMs, with a particular focus on the
"where" and "how" perspectives. From the "where" perspective, we summarize four
key graph learning procedures, including task definition, graph data feature
engineering, model selection and optimization, deployment and serving. We then
explore the application scenarios of LLMs in these procedures across a wider
spectrum. In the "how" perspective, we align the abilities of LLMs with the
requirements of each procedure. Finally, we point out the promising directions
that could better leverage the strength of LLMs towards versatile graph
learning methods.</div><div><a href='http://arxiv.org/abs/2402.11641v2'>2402.11641v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05952v1")'>Advancing Graph Representation Learning with Large Language Models: A
  Comprehensive Survey of Techniques</div>
<div id='2402.05952v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:51:14Z</div><div>Authors: Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, Jianling Sun</div><div style='padding-top: 10px; width: 80ex'>The integration of Large Language Models (LLMs) with Graph Representation
Learning (GRL) marks a significant evolution in analyzing complex data
structures. This collaboration harnesses the sophisticated linguistic
capabilities of LLMs to improve the contextual understanding and adaptability
of graph models, thereby broadening the scope and potential of GRL. Despite a
growing body of research dedicated to integrating LLMs into the graph domain, a
comprehensive review that deeply analyzes the core components and operations
within these models is notably lacking. Our survey fills this gap by proposing
a novel taxonomy that breaks down these models into primary components and
operation techniques from a novel technical perspective. We further dissect
recent literature into two primary components including knowledge extractors
and organizers, and two operation techniques including integration and training
stratigies, shedding light on effective model design and training strategies.
Additionally, we identify and explore potential future research avenues in this
nascent yet underexplored field, proposing paths for continued progress.</div><div><a href='http://arxiv.org/abs/2402.05952v1'>2402.05952v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13227v3")'>LPNL: Scalable Link Prediction with Large Language Models</div>
<div id='2401.13227v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T04:50:16Z</div><div>Authors: Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Cheng</div><div style='padding-top: 10px; width: 80ex'>Exploring the application of large language models (LLMs) to graph learning
is a emerging endeavor. However, the vast amount of information inherent in
large graphs poses significant challenges to this process. This work focuses on
the link prediction task and introduces $\textbf{LPNL}$ (Link Prediction via
Natural Language), a framework based on large language models designed for
scalable link prediction on large-scale heterogeneous graphs. We design novel
prompts for link prediction that articulate graph details in natural language.
We propose a two-stage sampling pipeline to extract crucial information from
the graphs, and a divide-and-conquer strategy to control the input tokens
within predefined limits, addressing the challenge of overwhelming information.
We fine-tune a T5 model based on our self-supervised learning designed for link
prediction. Extensive experimental results demonstrate that LPNL outperforms
multiple advanced baselines in link prediction tasks on large-scale graphs.</div><div><a href='http://arxiv.org/abs/2401.13227v3'>2401.13227v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08583v1")'>Mixture of Link Predictors</div>
<div id='2402.08583v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T16:36:50Z</div><div>Authors: Li Ma, Haoyu Han, Juanhui Li, Harry Shomer, Hui Liu, Xiaofeng Gao, Jiliang Tang</div><div style='padding-top: 10px; width: 80ex'>Link prediction, which aims to forecast unseen connections in graphs, is a
fundamental task in graph machine learning. Heuristic methods, leveraging a
range of different pairwise measures such as common neighbors and shortest
paths, often rival the performance of vanilla Graph Neural Networks (GNNs).
Therefore, recent advancements in GNNs for link prediction (GNN4LP) have
primarily focused on integrating one or a few types of pairwise information. In
this work, we reveal that different node pairs within the same dataset
necessitate varied pairwise information for accurate prediction and models that
only apply the same pairwise information uniformly could achieve suboptimal
performance. As a result, we propose a simple mixture of experts model Link-MoE
for link prediction. Link-MoE utilizes various GNNs as experts and
strategically selects the appropriate expert for each node pair based on
various types of pairwise information. Experimental results across diverse
real-world datasets demonstrate substantial performance improvement from
Link-MoE. Notably, Link-MoE achieves a relative improvement of 18.82\% on the
MRR metric for the Pubmed dataset and 10.8\% on the Hits@100 metric for the
ogbl-ppa dataset, compared to the best baselines.</div><div><a href='http://arxiv.org/abs/2402.08583v1'>2402.08583v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07999v3")'>NetInfoF Framework: Measuring and Exploiting Network Usable Information</div>
<div id='2402.07999v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:04:32Z</div><div>Authors: Meng-Chieh Lee, Haiyang Yu, Jian Zhang, Vassilis N. Ioannidis, Xiang Song, Soji Adeshina, Da Zheng, Christos Faloutsos</div><div style='padding-top: 10px; width: 80ex'>Given a node-attributed graph, and a graph task (link prediction or node
classification), can we tell if a graph neural network (GNN) will perform well?
More specifically, do the graph structure and the node features carry enough
usable information for the task? Our goals are (1) to develop a fast tool to
measure how much information is in the graph structure and in the node
features, and (2) to exploit the information to solve the task, if there is
enough. We propose NetInfoF, a framework including NetInfoF_Probe and
NetInfoF_Act, for the measurement and the exploitation of network usable
information (NUI), respectively. Given a graph data, NetInfoF_Probe measures
NUI without any model training, and NetInfoF_Act solves link prediction and
node classification, while two modules share the same backbone. In summary,
NetInfoF has following notable advantages: (a) General, handling both link
prediction and node classification; (b) Principled, with theoretical guarantee
and closed-form solution; (c) Effective, thanks to the proposed adjustment to
node similarity; (d) Scalable, scaling linearly with the input size. In our
carefully designed synthetic datasets, NetInfoF correctly identifies the ground
truth of NUI and is the only method being robust to all graph scenarios.
Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link
prediction compared to general GNN baselines.</div><div><a href='http://arxiv.org/abs/2402.07999v3'>2402.07999v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05468v1")'>Introducing New Node Prediction in Graph Mining: Predicting All Links
  from Isolated Nodes with Graph Neural Networks</div>
<div id='2401.05468v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T15:05:03Z</div><div>Authors: Damiano Zanardini, Emilio Serrano</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a new problem in the field of graph mining and social
network analysis called new node prediction. More technically, the task can be
categorized as zero-shot out-of-graph all-links prediction. This challenging
problem aims to predict all links from a new, isolated, and unobserved node
that was previously disconnected from the graph. Unlike classic approaches to
link prediction (including few-shot out-of-graph link prediction), this problem
presents two key differences: (1) the new node has no existing links from which
to extract patterns for new predictions; and (2) the goal is to predict not
just one, but all the links of this new node, or at least a significant part of
them. Experiments demonstrate that an architecture based on Deep Graph Neural
Networks can learn to solve this challenging problem in a bibliographic
citation network.</div><div><a href='http://arxiv.org/abs/2401.05468v1'>2401.05468v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07516v1")'>Temporal Link Prediction Using Graph Embedding Dynamics</div>
<div id='2401.07516v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T07:35:29Z</div><div>Authors: Sanaz Hasanzadeh Fard, Mohammad Ghassemi</div><div style='padding-top: 10px; width: 80ex'>Graphs are a powerful representation tool in machine learning applications,
with link prediction being a key task in graph learning. Temporal link
prediction in dynamic networks is of particular interest due to its potential
for solving complex scientific and real-world problems. Traditional approaches
to temporal link prediction have focused on finding the aggregation of dynamics
of the network as a unified output. In this study, we propose a novel
perspective on temporal link prediction by defining nodes as Newtonian objects
and incorporating the concept of velocity to predict network dynamics. By
computing more specific dynamics of each node, rather than overall dynamics, we
improve both accuracy and explainability in predicting future connections. We
demonstrate the effectiveness of our approach using two datasets, including 17
years of co-authorship data from PubMed. Experimental results show that our
temporal graph embedding dynamics approach improves downstream classification
models' ability to predict future collaboration efficacy in co-authorship
networks by 17.34% (AUROC improvement relative to the baseline model).
Furthermore, our approach offers an interpretable layer over traditional
approaches to address the temporal link prediction problem.</div><div><a href='http://arxiv.org/abs/2401.07516v1'>2401.07516v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04605v2")'>In-n-Out: Calibrating Graph Neural Networks for Link Prediction</div>
<div id='2403.04605v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T15:54:46Z</div><div>Authors: Erik Nascimento, Diego Mesquita, Samuel Kaski, Amauri H Souza</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks are notoriously miscalibrated, i.e., their outputs do
not reflect the true probability of the event we aim to predict. While networks
for tabular or image data are usually overconfident, recent works have shown
that graph neural networks (GNNs) show the opposite behavior for node-level
classification. But what happens when we are predicting links? We show that, in
this case, GNNs often exhibit a mixed behavior. More specifically, they may be
overconfident in negative predictions while being underconfident in positive
ones. Based on this observation, we propose IN-N-OUT, the first-ever method to
calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions:
i) attributing true/false labels to an edge while respecting a GNNs prediction
should cause but small fluctuations in that edge's embedding; and, conversely,
ii) if we label that same edge contradicting our GNN, embeddings should change
more substantially. An extensive experimental campaign shows that IN-N-OUT
significantly improves the calibration of GNNs in link prediction, consistently
outperforming the baselines available -- which are not designed for this
specific task.</div><div><a href='http://arxiv.org/abs/2403.04605v2'>2403.04605v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12411v1")'>Deep Structural Knowledge Exploitation and Synergy for Estimating Node
  Importance Value on Heterogeneous Information Networks</div>
<div id='2402.12411v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T02:34:23Z</div><div>Authors: Yankai Chen, Yixiang Fang, Qiongyan Wang, Xin Cao, Irwin King</div><div style='padding-top: 10px; width: 80ex'>Node importance estimation problem has been studied conventionally with
homogeneous network topology analysis. To deal with network heterogeneity, a
few recent methods employ graph neural models to automatically learn diverse
sources of information. However, the major concern revolves around that their
full adaptive learning process may lead to insufficient information
exploration, thereby formulating the problem as the isolated node value
prediction with underperformance and less interpretability. In this work, we
propose a novel learning framework: SKES. Different from previous automatic
learning designs, SKES exploits heterogeneous structural knowledge to enrich
the informativeness of node representations. Based on a sufficiently
uninformative reference, SKES estimates the importance value for any input
node, by quantifying its disparity against the reference. This establishes an
interpretable node importance computation paradigm. Furthermore, SKES dives
deep into the understanding that "nodes with similar characteristics are prone
to have similar importance values" whilst guaranteeing that such
informativeness disparity between any different nodes is orderly reflected by
the embedding distance of their associated latent features. Extensive
experiments on three widely-evaluated benchmarks demonstrate the performance
superiority of SKES over several recent competing methods.</div><div><a href='http://arxiv.org/abs/2402.12411v1'>2402.12411v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09711v1")'>Node Duplication Improves Cold-start Link Prediction</div>
<div id='2402.09711v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T05:07:39Z</div><div>Authors: Zhichun Guo, Tong Zhao, Yozen Liu, Kaiwen Dong, William Shiao, Neil Shah, Nitesh V. Chawla</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) are prominent in graph machine learning and have
shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless,
recent studies show that GNNs struggle to produce good results on low-degree
nodes despite their overall strong performance. In practical applications of
LP, like recommendation systems, improving performance on low-degree nodes is
critical, as it amounts to tackling the cold-start problem of improving the
experiences of users with few observed interactions. In this paper, we
investigate improving GNNs' LP performance on low-degree nodes while preserving
their performance on high-degree nodes and propose a simple yet surprisingly
effective augmentation technique called NodeDup. Specifically, NodeDup
duplicates low-degree nodes and creates links between nodes and their own
duplicates before following the standard supervised LP training scheme. By
leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows
significant LP performance improvements on low-degree nodes without
compromising any performance on high-degree nodes. Additionally, as a
plug-and-play augmentation module, NodeDup can be easily applied to existing
GNNs with very light computational cost. Extensive experiments show that
NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated,
low-degree, and warm nodes, respectively, on average across all datasets
compared to GNNs and state-of-the-art cold-start methods.</div><div><a href='http://arxiv.org/abs/2402.09711v1'>2402.09711v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11518v1")'>Large Language Model-driven Meta-structure Discovery in Heterogeneous
  Information Network</div>
<div id='2402.11518v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T09:21:12Z</div><div>Authors: Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui</div><div style='padding-top: 10px; width: 80ex'>Heterogeneous information networks (HIN) have gained increasing popularity
for being able to capture complex relations between nodes of diverse types.
Meta-structure was proposed to identify important patterns of relations on HIN,
which has been proven effective for extracting rich semantic information and
facilitating graph neural networks to learn expressive representations.
However, hand-crafted meta-structures pose challenges for scaling up, which
draws wide research attention for developing automatic meta-structure search
algorithms. Previous efforts concentrate on searching for meta-structures with
good empirical prediction performance, overlooking explainability. Thus, they
often produce meta-structures prone to overfitting and incomprehensible to
humans. To address this, we draw inspiration from the emergent reasoning
abilities of large language models (LLMs). We propose a novel REasoning
meta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into
the evolutionary procedure. ReStruct uses a grammar translator to encode
meta-structures into natural language sentences, and leverages the reasoning
power of LLMs to evaluate semantically feasible meta-structures. ReStruct also
employs performance-oriented evolutionary operations. These two competing
forces jointly optimize for semantic explainability and empirical performance
of meta-structures. We also design a differential LLM explainer that can
produce natural language explanations for the discovered meta-structures, and
refine the explanation by reasoning through the search history. Experiments on
five datasets demonstrate ReStruct achieve SOTA performance in node
classification and link recommendation tasks. Additionally, a survey study
involving 73 graduate students shows that the meta-structures and natural
language explanations generated by ReStruct are substantially more
comprehensible.</div><div><a href='http://arxiv.org/abs/2402.11518v1'>2402.11518v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16215v1")'>Learning big logical rules by joining small rules</div>
<div id='2401.16215v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T15:09:40Z</div><div>Authors: Céline Hocquette, Andreas Niskanen, Rolf Morel, Matti Järvisalo, Andrew Cropper</div><div style='padding-top: 10px; width: 80ex'>A major challenge in inductive logic programming is learning big rules. To
address this challenge, we introduce an approach where we join small rules to
learn big rules. We implement our approach in a constraint-driven system and
use constraint solvers to efficiently join rules. Our experiments on many
domains, including game playing and drug design, show that our approach can (i)
learn rules with more than 100 literals, and (ii) drastically outperform
existing approaches in terms of predictive accuracies.</div><div><a href='http://arxiv.org/abs/2401.16215v1'>2401.16215v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03720v1")'>Similarity-based Neighbor Selection for Graph LLMs</div>
<div id='2402.03720v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T05:29:05Z</div><div>Authors: Rui Li, Jiwei Li, Jiawei Han, Guoyin Wang</div><div style='padding-top: 10px; width: 80ex'>Text-attributed graphs (TAGs) present unique challenges for direct processing
by Language Learning Models (LLMs), yet their extensive commonsense knowledge
and robust reasoning capabilities offer great promise for node classification
in TAGs. Prior research in this field has grappled with issues such as
over-squashing, heterophily, and ineffective graph information integration,
further compounded by inconsistencies in dataset partitioning and
underutilization of advanced LLMs. To address these challenges, we introduce
Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor
selection techniques, SNS effectively improves the quality of selected
neighbors, thereby improving graph representation and alleviating issues like
over-squashing and heterophily. Besides, as an inductive and training-free
approach, SNS demonstrates superior generalization and scalability over
traditional GNN methods. Our comprehensive experiments, adhering to standard
dataset partitioning practices, demonstrate that SNS, through simple prompt
interactions with LLMs, consistently outperforms vanilla GNNs and achieves
state-of-the-art results on datasets like PubMed in node classification,
showcasing LLMs' potential in graph structure understanding. Our research
further underscores the significance of graph structure integration in LLM
applications and identifies key factors for their success in node
classification. Code is available at https://github.com/ruili33/SNS.</div><div><a href='http://arxiv.org/abs/2402.03720v1'>2402.03720v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00053v1")'>Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework
  for Knowledge Graph Link Predictors</div>
<div id='2402.00053v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T15:44:46Z</div><div>Authors: Filip Cornell, Yifei Jin, Jussi Karlgren, Sarunas Girdzijauskas</div><div style='padding-top: 10px; width: 80ex'>The standard evaluation protocol for measuring the quality of Knowledge Graph
Completion methods - the task of inferring new links to be added to a graph -
typically involves a step which ranks every entity of a Knowledge Graph to
assess their fit as a head or tail of a candidate link to be added. In
Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively
heavy. Previous approaches mitigate this problem by using random sampling of
entities to assess the quality of links predicted or suggested by a method.
However, we show that this approach has serious limitations since the ranking
metrics produced do not properly reflect true outcomes. In this paper, we
present a thorough analysis of these effects along with the following findings.
First, we empirically find and theoretically motivate why sampling uniformly at
random vastly overestimates the ranking performance of a method. We show that
this can be attributed to the effect of easy versus hard negative candidates.
Second, we propose a framework that uses relational recommenders to guide the
selection of candidates for evaluation. We provide both theoretical and
empirical justification of our methodology, and find that simple and fast
methods can work extremely well, and that they match advanced neural
approaches. Even when a large portion of true candidates for a property are
missed, the estimation barely deteriorates. With our proposed framework, we can
reduce the time and computation needed similar to random sampling strategies
while vastly improving the estimation; on ogbl-wikikg2, we show that accurate
estimations of the full, filtered ranking can be obtained in 20 seconds instead
of 30 minutes. We conclude that considerable computational effort can be saved
by effective preprocessing and sampling methods and still reliably predict
performance accurately of the true performance for the entire ranking
procedure.</div><div><a href='http://arxiv.org/abs/2402.00053v1'>2402.00053v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15810v1")'>OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining</div>
<div id='2402.15810v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T13:15:54Z</div><div>Authors: Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, Jie Tang</div><div style='padding-top: 10px; width: 80ex'>With the rapid proliferation of scientific literature, versatile academic
knowledge services increasingly rely on comprehensive academic graph mining.
Despite the availability of public academic graphs, benchmarks, and datasets,
these resources often fall short in multi-aspect and fine-grained annotations,
are constrained to specific task types and domains, or lack underlying real
academic graphs. In this paper, we present OAG-Bench, a comprehensive,
multi-aspect, and fine-grained human-curated benchmark based on the Open
Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines,
and 120+ experimental results to date. We propose new data annotation
strategies for certain tasks and offer a suite of data pre-processing codes,
algorithm implementations, and standardized evaluation protocols to facilitate
academic graph mining. Extensive experiments reveal that even advanced
algorithms like large language models (LLMs) encounter difficulties in
addressing key challenges in certain tasks, such as paper source tracing and
scholar profiling. We also introduce the Open Academic Graph Challenge
(OAG-Challenge) to encourage community input and sharing. We envisage that
OAG-Bench can serve as a common ground for the community to evaluate and
compare algorithms in academic graph mining, thereby accelerating algorithm
development and advancement in this field. OAG-Bench is accessible at
https://www.aminer.cn/data/.</div><div><a href='http://arxiv.org/abs/2402.15810v1'>2402.15810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02576v1")'>AceMap: Knowledge Discovery through Academic Graph</div>
<div id='2403.02576v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T01:17:56Z</div><div>Authors: Xinbing Wang, Luoyi Fu, Xiaoying Gan, Ying Wen, Guanjie Zheng, Jiaxin Ding, Liyao Xiang, Nanyang Ye, Meng Jin, Shiyu Liang, Bin Lu, Haiwen Wang, Yi Xu, Cheng Deng, Shao Zhang, Huquan Kang, Xingli Wang, Qi Li, Zhixin Guo, Jiexing Qi, Pan Liu, Yuyang Ren, Lyuwen Wu, Jungang Yang, Jianping Zhou, Chenghu Zhou</div><div style='padding-top: 10px; width: 80ex'>The exponential growth of scientific literature requires effective management
and extraction of valuable insights. While existing scientific search engines
excel at delivering search results based on relational databases, they often
neglect the analysis of collaborations between scientific entities and the
evolution of ideas, as well as the in-depth analysis of content within
scientific publications. The representation of heterogeneous graphs and the
effective measurement, analysis, and mining of such graphs pose significant
challenges. To address these challenges, we present AceMap, an academic system
designed for knowledge discovery through academic graph. We present advanced
database construction techniques to build the comprehensive AceMap database
with large-scale academic publications that contain rich visual, textual, and
numerical information. AceMap also employs innovative visualization,
quantification, and analysis methods to explore associations and logical
relationships among academic entities. AceMap introduces large-scale academic
network visualization techniques centered on nebular graphs, providing a
comprehensive view of academic networks from multiple perspectives. In
addition, AceMap proposes a unified metric based on structural entropy to
quantitatively measure the knowledge content of different academic entities.
Moreover, AceMap provides advanced analysis capabilities, including tracing the
evolution of academic ideas through citation relationships and concept
co-occurrence, and generating concise summaries informed by this evolutionary
process. In addition, AceMap uses machine reading methods to generate potential
new ideas at the intersection of different fields. Exploring the integration of
large language models and knowledge graphs is a promising direction for future
research in idea evolution. Please visit \url{https://www.acemap.info} for
further exploration.</div><div><a href='http://arxiv.org/abs/2403.02576v1'>2403.02576v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08640v2")'>Forecasting high-impact research topics via machine learning on evolving
  knowledge graphs</div>
<div id='2402.08640v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:09:38Z</div><div>Authors: Xuemei Gu, Mario Krenn</div><div style='padding-top: 10px; width: 80ex'>The exponential growth in scientific publications poses a severe challenge
for human researchers. It forces attention to more narrow sub-fields, which
makes it challenging to discover new impactful research ideas and
collaborations outside one's own field. While there are ways to predict a
scientific paper's future citation counts, they need the research to be
finished and the paper written, usually assessing impact long after the idea
was conceived. Here we show how to predict the impact of onsets of ideas that
have never been published by researchers. For that, we developed a large
evolving knowledge graph built from more than 21 million scientific papers. It
combines a semantic network created from the content of the papers and an
impact network created from the historic citations of papers. Using machine
learning, we can predict the dynamic of the evolving network into the future
with high accuracy, and thereby the impact of new research directions. We
envision that the ability to predict the impact of new ideas will be a crucial
component of future artificial muses that can inspire new impactful and
interesting scientific ideas.</div><div><a href='http://arxiv.org/abs/2402.08640v2'>2402.08640v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11996v1")'>Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning</div>
<div id='2403.11996v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:30:27Z</div><div>Authors: Markus J. Buehler</div><div style='padding-top: 10px; width: 80ex'>Using generative Artificial Intelligence (AI), we transformed a set of 1,000
scientific papers in the area of biological materials into detailed ontological
knowledge graphs, revealing their inherently scale-free nature. Using graph
traversal path detection between dissimilar concepts based on combinatorial
ranking of node similarity and betweenness centrality, we reveal deep insights
into unprecedented interdisciplinary relationships that can be used to answer
queries, identify gaps in knowledge, and propose never-before-seen material
designs and their behaviors. One comparison revealed detailed structural
parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. The
algorithm further created an innovative hierarchical mycelium-based composite
that incorporates joint synthesis of graph sampling with principles extracted
from Kandinsky's Composition VII painting, where the resulting composite
reflects a balance of chaos and order, with features like adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across physical, biological, and artistic spheres,
revealing a nuanced ontology of immanence and material flux that resonates with
postmodern philosophy, and positions these interconnections within a
heterarchical framework. Our findings reveal the dynamic, context-dependent
interplay of entities beyond traditional hierarchical paradigms, emphasizing
the significant role of individual components and their fluctuative
relationships within the system. Our predictions achieve a far higher degree of
novelty, technical detail and explorative capacity than conventional generative
AI methods. The approach establishes a widely useful framework for innovation
by revealing hidden connections that facilitate discovery.</div><div><a href='http://arxiv.org/abs/2403.11996v1'>2403.11996v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11821v2")'>Microstructures and Accuracy of Graph Recall by Large Language Models</div>
<div id='2402.11821v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T04:29:45Z</div><div>Authors: Yanbang Wang, Hejie Cui, Jon Kleinberg</div><div style='padding-top: 10px; width: 80ex'>Graphs data is crucial for many applications, and much of it exists in the
relations described in textual format. As a result, being able to accurately
recall and encode a graph described in earlier text is a basic yet pivotal
ability that LLMs need to demonstrate if they are to perform reasoning tasks
that involve graph-structured information. Human performance at graph recall
has been studied by cognitive scientists for decades, and has been found to
often exhibit certain structural patterns of bias that align with human
handling of social relationships. To date, however, we know little about how
LLMs behave in analogous graph recall tasks: do their recalled graphs also
exhibit certain biased patterns, and if so, how do they compare with humans and
affect other graph reasoning tasks? In this work, we perform the first
systematical study of graph recall by LLMs, investigating the accuracy and
biased microstructures (local structural patterns) in their recall. We find
that LLMs not only underperform often in graph recall, but also tend to favor
more triangles and alternating 2-paths. Moreover, we find that more advanced
LLMs have a striking dependence on the domain that a real-world graph comes
from -- by yielding the best recall accuracy when the graph is narrated in a
language style consistent with its original domain.</div><div><a href='http://arxiv.org/abs/2402.11821v2'>2402.11821v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13551v1")'>Graph Representation of Narrative Context: Coherence Dependency via
  Retrospective Questions</div>
<div id='2402.13551v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T06:14:04Z</div><div>Authors: Liyan Xu, Jiangnan Li, Mo Yu, Jie Zhou</div><div style='padding-top: 10px; width: 80ex'>This work introduces a novel and practical paradigm for narrative
comprehension, stemming from the observation that individual passages within
narratives are often cohesively related than being isolated. We therefore
propose to formulate a graph upon narratives dubbed NARCO that depicts a
task-agnostic coherence dependency of the entire context. Especially, edges in
NARCO encompass retrospective free-form questions between two context snippets
reflecting high-level coherent relations, inspired by the cognitive perception
of humans who constantly reinstate relevant events from prior context.
Importantly, our graph is instantiated through our designed two-stage LLM
prompting, thereby without reliance on human annotations. We present three
unique studies on its practical utility, examining the edge efficacy via recap
identification, local context augmentation via plot retrieval, and broader
applications exemplified by long document QA. Experiments suggest that our
approaches leveraging NARCO yield performance boost across all three tasks.</div><div><a href='http://arxiv.org/abs/2402.13551v1'>2402.13551v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.14749v1")'>Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric
  QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes</div>
<div id='2401.14749v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T10:14:10Z</div><div>Authors: Vasiliy Usatyuk, Denis Sapozhnikov, Sergey Egorov</div><div style='padding-top: 10px; width: 80ex'>This paper presents a method for achieving equilibrium in the ISING
Hamiltonian when confronted with unevenly distributed charges on an irregular
grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our
approach involves dimensionally expanding the system, substituting charges with
circulants, and representing distances through circulant shifts. This results
in a systematic mapping of the charge system onto a space, transforming the
irregular grid into a uniform configuration, applicable to Torical and Circular
Hyperboloid Topologies. The paper covers fundamental definitions and notations
related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine.
It explores the marginalization problem in code on the graph probabilistic
models for evaluating the partition function, encompassing exact and
approximate estimation techniques. Rigorous proof is provided for the
attainability of equilibrium states for the Boltzmann machine under Torical and
Circular Hyperboloid, paving the way for the application of our methodology.
Practical applications of our approach are investigated in Finite Geometry
QC-LDPC Codes, specifically in Material Science. The paper further explores its
effectiveness in the realm of Natural Language Processing Transformer Deep
Neural Networks, examining Generalized Repeat Accumulate Codes,
Spatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful
nature of our topology-aware hardware-efficient quasi-cycle codes equilibrium
method is showcased across diverse scientific domains without the use of
specific section delineations.</div><div><a href='http://arxiv.org/abs/2401.14749v1'>2401.14749v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16176v1")'>A Survey on Structure-Preserving Graph Transformers</div>
<div id='2401.16176v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T14:18:09Z</div><div>Authors: Van Thuy Hoang, O-Joun Lee</div><div style='padding-top: 10px; width: 80ex'>The transformer architecture has shown remarkable success in various domains,
such as natural language processing and computer vision. When it comes to graph
learning, transformers are required not only to capture the interactions
between pairs of nodes but also to preserve graph structures connoting the
underlying relations and proximity between them, showing the expressive power
to capture different graph structures. Accordingly, various
structure-preserving graph transformers have been proposed and widely used for
various tasks, such as graph-level tasks in bioinformatics and
chemoinformatics. However, strategies related to graph structure preservation
have not been well organized and systematized in the literature. In this paper,
we provide a comprehensive overview of structure-preserving graph transformers
and generalize these methods from the perspective of their design objective.
First, we divide strategies into four main groups: node feature modulation,
context node sampling, graph rewriting, and transformer architecture
improvements. We then further divide the strategies according to the coverage
and goals of graph structure preservation. Furthermore, we also discuss
challenges and future directions for graph transformer models to preserve the
graph structure and understand the nature of graphs.</div><div><a href='http://arxiv.org/abs/2401.16176v1'>2401.16176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03358v3")'>A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening,
  and Condensation</div>
<div id='2402.03358v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T01:19:09Z</div><div>Authors: Mohammad Hashemi, Shengbo Gong, Juntong Ni, Wenqi Fan, B. Aditya Prakash, Wei Jin</div><div style='padding-top: 10px; width: 80ex'>Many real-world datasets can be naturally represented as graphs, spanning a
wide range of domains. However, the increasing complexity and size of graph
datasets present significant challenges for analysis and computation. In
response, graph reduction techniques have gained prominence for simplifying
large graphs while preserving essential properties. In this survey, we aim to
provide a comprehensive understanding of graph reduction methods, including
graph sparsification, graph coarsening, and graph condensation. Specifically,
we establish a unified definition for these methods and introduce a
hierarchical taxonomy to categorize the challenges they address. Our survey
then systematically reviews the technical details of these methods and
emphasizes their practical applications across diverse scenarios. Furthermore,
we outline critical research directions to ensure the continued effectiveness
of graph reduction techniques, as well as provide a comprehensive paper list at
https://github.com/ChandlerBang/awesome-graph-reduction. We hope this survey
will bridge literature gaps and propel the advancement of this promising field.</div><div><a href='http://arxiv.org/abs/2402.03358v3'>2402.03358v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02000v1")'>A Survey on Graph Condensation</div>
<div id='2402.02000v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T02:50:51Z</div><div>Authors: Hongjia Xu, Liangliang Zhang, Yao Ma, Sheng Zhou, Zhuonan Zheng, Bu Jiajun</div><div style='padding-top: 10px; width: 80ex'>Analytics on large-scale graphs have posed significant challenges to
computational efficiency and resource requirements. Recently, Graph
condensation (GC) has emerged as a solution to address challenges arising from
the escalating volume of graph data. The motivation of GC is to reduce the
scale of large graphs to smaller ones while preserving essential information
for downstream tasks. For a better understanding of GC and to distinguish it
from other related topics, we present a formal definition of GC and establish a
taxonomy that systematically categorizes existing methods into three types
based on its objective, and classify the formulations to generate the condensed
graphs into two categories as modifying the original graphs or synthetic
completely new ones. Moreover, our survey includes a comprehensive analysis of
datasets and evaluation metrics in this field. Finally, we conclude by
addressing challenges and limitations, outlining future directions, and
offering concise guidelines to inspire future research in this field.</div><div><a href='http://arxiv.org/abs/2402.02000v1'>2402.02000v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13286v1")'>A Sampling-based Framework for Hypothesis Testing on Large Attributed
  Graphs</div>
<div id='2403.13286v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T03:56:22Z</div><div>Authors: Yun Wang, Chrysanthi Kosyfaki, Sihem Amer-Yahia, Reynold Cheng</div><div style='padding-top: 10px; width: 80ex'>Hypothesis testing is a statistical method used to draw conclusions about
populations from sample data, typically represented in tables. With the
prevalence of graph representations in real-life applications, hypothesis
testing in graphs is gaining importance. In this work, we formalize node, edge,
and path hypotheses in attributed graphs. We develop a sampling-based
hypothesis testing framework, which can accommodate existing
hypothesis-agnostic graph sampling methods. To achieve accurate and efficient
sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m-
dimensional random walk that accounts for the paths specified in a hypothesis.
We further optimize its time efficiency and propose PHASEopt. Experiments on
real datasets demonstrate the ability of our framework to leverage common graph
sampling methods for hypothesis testing, and the superiority of
hypothesis-aware sampling in terms of accuracy and time efficiency.</div><div><a href='http://arxiv.org/abs/2403.13286v1'>2403.13286v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16990v1")'>inGRASS: Incremental Graph Spectral Sparsification via
  Low-Resistance-Diameter Decomposition</div>
<div id='2402.16990v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T19:49:54Z</div><div>Authors: Ali Aghdaei, Zhuo Feng</div><div style='padding-top: 10px; width: 80ex'>This work presents inGRASS, a novel algorithm designed for incremental
spectral sparsification of large undirected graphs. The proposed inGRASS
algorithm is highly scalable and parallel-friendly, having a nearly-linear time
complexity for the setup phase and the ability to update the spectral
sparsifier in $O(\log N)$ time for each incremental change made to the original
graph with $N$ nodes. A key component in the setup phase of inGRASS is a
multilevel resistance embedding framework introduced for efficiently
identifying spectrally-critical edges and effectively detecting redundant ones,
which is achieved by decomposing the initial sparsifier into many node clusters
with bounded effective-resistance diameters leveraging a
low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS
exploits low-dimensional node embedding vectors for efficiently estimating the
importance and uniqueness of each newly added edge. As demonstrated through
extensive experiments, inGRASS achieves up to over $200 \times$ speedups while
retaining comparable solution quality in incremental spectral sparsification of
graphs obtained from various datasets, such as circuit simulations, finite
element analysis, and social networks.</div><div><a href='http://arxiv.org/abs/2402.16990v1'>2402.16990v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13846v1")'>A Clustering Method with Graph Maximum Decoding Information</div>
<div id='2403.13846v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T05:18:19Z</div><div>Authors: Xinrun Xu, Manying Lv, Yurong Wu, Zhanbiao Lian, Zhiming Ding, Jin Yan, Shan Jiang</div><div style='padding-top: 10px; width: 80ex'>The clustering method based on graph models has garnered increased attention
for its widespread applicability across various knowledge domains. Its
adaptability to integrate seamlessly with other relevant applications endows
the graph model-based clustering analysis with the ability to robustly extract
"natural associations" or "graph structures" within datasets, facilitating the
modelling of relationships between data points. Despite its efficacy, the
current clustering method utilizing the graph-based model overlooks the
uncertainty associated with random walk access between nodes and the embedded
structural information in the data. To address this gap, we present a novel
Clustering method for Maximizing Decoding Information within graph-based
models, named CMDI. CMDI innovatively incorporates two-dimensional structural
information theory into the clustering process, consisting of two phases: graph
structure extraction and graph vertex partitioning. Within CMDI, graph
partitioning is reformulated as an abstract clustering problem, leveraging
maximum decoding information to minimize uncertainty associated with random
visits to vertices. Empirical evaluations on three real-world datasets
demonstrate that CMDI outperforms classical baseline methods, exhibiting a
superior decoding information ratio (DI-R). Furthermore, CMDI showcases
heightened efficiency, particularly when considering prior knowledge (PK).
These findings underscore the effectiveness of CMDI in enhancing decoding
information quality and computational efficiency, positioning it as a valuable
tool in graph-based clustering analyses.</div><div><a href='http://arxiv.org/abs/2403.13846v1'>2403.13846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03670v1")'>CDC: A Simple Framework for Complex Data Clustering</div>
<div id='2403.03670v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T12:47:14Z</div><div>Authors: Zhao Kang, Xuanting Xie, Bingheng Li, Erlin Pan</div><div style='padding-top: 10px; width: 80ex'>In today's data-driven digital era, the amount as well as complexity, such as
multi-view, non-Euclidean, and multi-relational, of the collected data are
growing exponentially or even faster. Clustering, which unsupervisely extracts
valid knowledge from data, is extremely useful in practice. However, existing
methods are independently developed to handle one particular challenge at the
expense of the others. In this work, we propose a simple but effective
framework for complex data clustering (CDC) that can efficiently process
different types of data with linear complexity. We first utilize graph
filtering to fuse geometry structure and attribute information. We then reduce
the complexity with high-quality anchors that are adaptively learned via a
novel similarity-preserving regularizer. We illustrate the cluster-ability of
our proposed method theoretically and experimentally. In particular, we deploy
CDC to graph data of size 111M.</div><div><a href='http://arxiv.org/abs/2403.03670v1'>2403.03670v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12854v1")'>Differentiable Mapper For Topological Optimization Of Data
  Representation</div>
<div id='2402.12854v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T09:33:22Z</div><div>Authors: Ziyad Oulhaj, Mathieu Carrière, Bertrand Michel</div><div style='padding-top: 10px; width: 80ex'>Unsupervised data representation and visualization using tools from topology
is an active and growing field of Topological Data Analysis (TDA) and data
science. Its most prominent line of work is based on the so-called Mapper
graph, which is a combinatorial graph whose topological structures (connected
components, branches, loops) are in correspondence with those of the data
itself. While highly generic and applicable, its use has been hampered so far
by the manual tuning of its many parameters-among these, a crucial one is the
so-called filter: it is a continuous function whose variations on the data set
are the main ingredient for both building the Mapper representation and
assessing the presence and sizes of its topological structures. However, while
a few parameter tuning methods have already been investigated for the other
Mapper parameters (i.e., resolution, gain, clustering), there is currently no
method for tuning the filter itself. In this work, we build on a recently
proposed optimization framework incorporating topology to provide the first
filter optimization scheme for Mapper graphs. In order to achieve this, we
propose a relaxed and more general version of the Mapper graph, whose
convergence properties are investigated. Finally, we demonstrate the usefulness
of our approach by optimizing Mapper graph representations on several datasets,
and showcasing the superiority of the optimized representation over arbitrary
ones.</div><div><a href='http://arxiv.org/abs/2402.12854v1'>2402.12854v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13713v1")'>EMP: Effective Multidimensional Persistence for Graph Representation
  Learning</div>
<div id='2401.13713v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T00:41:51Z</div><div>Authors: Ignacio Segovia-Dominguez, Yuzhou Chen, Cuneyt G. Akcora, Zhiwei Zhen, Murat Kantarcioglu, Yulia R. Gel, Baris Coskunuzer</div><div style='padding-top: 10px; width: 80ex'>Topological data analysis (TDA) is gaining prominence across a wide spectrum
of machine learning tasks that spans from manifold learning to graph
classification. A pivotal technique within TDA is persistent homology (PH),
which furnishes an exclusive topological imprint of data by tracing the
evolution of latent structures as a scale parameter changes. Present PH tools
are confined to analyzing data through a single filter parameter. However, many
scenarios necessitate the consideration of multiple relevant parameters to
attain finer insights into the data. We address this issue by introducing the
Effective Multidimensional Persistence (EMP) framework. This framework empowers
the exploration of data by simultaneously varying multiple scale parameters.
The framework integrates descriptor functions into the analysis process,
yielding a highly expressive data summary. It seamlessly integrates established
single PH summaries into multidimensional counterparts like EMP Landscapes,
Silhouettes, Images, and Surfaces. These summaries represent data's
multidimensional aspects as matrices and arrays, aligning effectively with
diverse ML models. We provide theoretical guarantees and stability proofs for
EMP summaries. We demonstrate EMP's utility in graph classification tasks,
showing its effectiveness. Results reveal that EMP enhances various single PH
descriptors, outperforming cutting-edge methods on multiple benchmark datasets.</div><div><a href='http://arxiv.org/abs/2401.13713v1'>2401.13713v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14489v1")'>A Class of Topological Pseudodistances for Fast Comparison of
  Persistence Diagrams</div>
<div id='2402.14489v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T12:27:35Z</div><div>Authors: Rolando Kindelan Nuñez, Mircea Petrache, Mauricio Cerda, Nancy Hitschfeld</div><div style='padding-top: 10px; width: 80ex'>Persistence diagrams (PD)s play a central role in topological data analysis,
and are used in an ever increasing variety of applications. The comparison of
PD data requires computing comparison metrics among large sets of PDs, with
metrics which are accurate, theoretically sound, and fast to compute.
Especially for denser multi-dimensional PDs, such comparison metrics are
lacking. While on the one hand, Wasserstein-type distances have high accuracy
and theoretical guarantees, they incur high computational cost. On the other
hand, distances between vectorizations such as Persistence Statistics (PS)s
have lower computational cost, but lack the accuracy guarantees and in general
they are not guaranteed to distinguish PDs (i.e. the two PS vectors of
different PDs may be equal). In this work we introduce a class of
pseudodistances called Extended Topological Pseudodistances (ETD)s, which have
tunable complexity, and can approximate Sliced and classical Wasserstein
distances at the high-complexity extreme, while being computationally lighter
and close to Persistence Statistics at the lower complexity extreme, and thus
allow users to interpolate between the two metrics. We build theoretical
comparisons to show how to fit our new distances at an intermediate level
between persistence vectorizations and Wasserstein distances. We also
experimentally verify that ETDs outperform PSs in terms of accuracy and
outperform Wasserstein and Sliced Wasserstein distances in terms of
computational complexity.</div><div><a href='http://arxiv.org/abs/2402.14489v1'>2402.14489v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17295v1")'>Quantum Distance Approximation for Persistence Diagrams</div>
<div id='2402.17295v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T08:16:17Z</div><div>Authors: Bernardo Ameneyro, Rebekah Herrman, George Siopsis, Vasileios Maroulas</div><div style='padding-top: 10px; width: 80ex'>Topological Data Analysis methods can be useful for classification and
clustering tasks in many different fields as they can provide two dimensional
persistence diagrams that summarize important information about the shape of
potentially complex and high dimensional data sets. The space of persistence
diagrams can be endowed with various metrics such as the Wasserstein distance
which admit a statistical structure and allow to use these summaries for
machine learning algorithms. However, computing the distance between two
persistence diagrams involves finding an optimal way to match the points of the
two diagrams and may not always be an easy task for classical computers. In
this work we explore the potential of quantum computers to estimate the
distance between persistence diagrams, in particular we propose variational
quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$
distance. Our implementation is a weighted version of the Quantum Approximate
Optimization Algorithm that relies on control clauses to encode the constraints
of the optimization problem.</div><div><a href='http://arxiv.org/abs/2402.17295v1'>2402.17295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12237v1")'>A distribution-guided Mapper algorithm</div>
<div id='2401.12237v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T17:07:05Z</div><div>Authors: Yuyang Tao, Shufei Ge</div><div style='padding-top: 10px; width: 80ex'>Motivation: The Mapper algorithm is an essential tool to explore shape of
data in topology data analysis. With a dataset as an input, the Mapper
algorithm outputs a graph representing the topological features of the whole
dataset. This graph is often regarded as an approximation of a reeb graph of
data. The classic Mapper algorithm uses fixed interval lengths and overlapping
ratios, which might fail to reveal subtle features of data, especially when the
underlying structure is complex.
  Results: In this work, we introduce a distribution guided Mapper algorithm
named D-Mapper, that utilizes the property of the probability model and data
intrinsic characteristics to generate density guided covers and provides
enhanced topological features. Our proposed algorithm is a probabilistic
model-based approach, which could serve as an alternative to non-prababilistic
ones. Moreover, we introduce a metric accounting for both the quality of
overlap clustering and extended persistence homology to measure the performance
of Mapper type algorithm. Our numerical experiments indicate that the D-Mapper
outperforms the classical Mapper algorithm in various scenarios. We also apply
the D-Mapper to a SARS-COV-2 coronavirus RNA sequences dataset to explore the
topological structure of different virus variants. The results indicate that
the D-Mapper algorithm can reveal both vertical and horizontal evolution
processes of the viruses.
  Availability: Our package is available at
https://github.com/ShufeiGe/D-Mapper.</div><div><a href='http://arxiv.org/abs/2401.12237v1'>2401.12237v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10972v1")'>Clustering Molecular Energy Landscapes by Adaptive Network Embedding</div>
<div id='2401.10972v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T17:12:07Z</div><div>Authors: Paula Mercurio, Di Liu</div><div style='padding-top: 10px; width: 80ex'>In order to efficiently explore the chemical space of all possible small
molecules, a common approach is to compress the dimension of the system to
facilitate downstream machine learning tasks. Towards this end, we present a
data driven approach for clustering potential energy landscapes of molecular
structures by applying recently developed Network Embedding techniques, to
obtain latent variables defined through the embedding function. To scale up the
method, we also incorporate an entropy sensitive adaptive scheme for
hierarchical sampling of the energy landscape, based on Metadynamics and
Transition Path Theory. By taking into account the kinetic information implied
by a system's energy landscape, we are able to interpret dynamical node-node
relationships in reduced dimensions. We demonstrate the framework through
Lennard-Jones (LJ) clusters and a human DNA sequence.</div><div><a href='http://arxiv.org/abs/2401.10972v1'>2401.10972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07970v1")'>Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical
  Similarity Search</div>
<div id='2402.07970v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:24:32Z</div><div>Authors: Kathryn E. Kirchoff, James Wellnitz, Joshua E. Hochuli, Travis Maxfield, Konstantin I. Popov, Shawn Gomez, Alexander Tropsha</div><div style='padding-top: 10px; width: 80ex'>Nearest neighbor-based similarity searching is a common task in chemistry,
with notable use cases in drug discovery. Yet, some of the most commonly used
approaches for this task still leverage a brute-force approach. In practice
this can be computationally costly and overly time-consuming, due in part to
the sheer size of modern chemical databases. Previous computational
advancements for this task have generally relied on improvements to hardware or
dataset-specific tricks that lack generalizability. Approaches that leverage
lower-complexity searching algorithms remain relatively underexplored. However,
many of these algorithms are approximate solutions and/or struggle with typical
high-dimensional chemical embeddings. Here we evaluate whether a combination of
low-dimensional chemical embeddings and a k-d tree data structure can achieve
fast nearest neighbor queries while maintaining performance on standard
chemical similarity search benchmarks. We examine different dimensionality
reductions of standard chemical embeddings as well as a learned,
structurally-aware embedding -- SmallSA -- for this task. With this framework,
searches on over one billion chemicals execute in less than a second on a
single CPU core, five orders of magnitude faster than the brute-force approach.
We also demonstrate that SmallSA achieves competitive performance on chemical
similarity benchmarks.</div><div><a href='http://arxiv.org/abs/2402.07970v1'>2402.07970v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01234v1")'>Active Deep Kernel Learning of Molecular Functionalities: Realizing
  Dynamic Structural Embeddings</div>
<div id='2403.01234v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T15:34:31Z</div><div>Authors: Ayana Ghosh, Maxim Ziatdinov and, Sergei V. Kalinin</div><div style='padding-top: 10px; width: 80ex'>Exploring molecular spaces is crucial for advancing our understanding of
chemical properties and reactions, leading to groundbreaking innovations in
materials science, medicine, and energy. This paper explores an approach for
active learning in molecular discovery using Deep Kernel Learning (DKL), a
novel approach surpassing the limits of classical Variational Autoencoders
(VAEs). Employing the QM9 dataset, we contrast DKL with traditional VAEs, which
analyze molecular structures based on similarity, revealing limitations due to
sparse regularities in latent spaces. DKL, however, offers a more holistic
perspective by correlating structure with properties, creating latent spaces
that prioritize molecular functionality. This is achieved by recalculating
embedding vectors iteratively, aligning with the experimental availability of
target properties. The resulting latent spaces are not only better organized
but also exhibit unique characteristics such as concentrated maxima
representing molecular functionalities and a correlation between predictive
uncertainty and error. Additionally, the formation of exclusion regions around
certain compounds indicates unexplored areas with potential for groundbreaking
functionalities. This study underscores DKL's potential in molecular research,
offering new avenues for understanding and discovering molecular
functionalities beyond classical VAE limitations.</div><div><a href='http://arxiv.org/abs/2403.01234v1'>2403.01234v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07249v2")'>The Impact of Domain Knowledge and Multi-Modality on Intelligent
  Molecular Property Prediction: A Systematic Survey</div>
<div id='2402.07249v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T17:29:58Z</div><div>Authors: Taojie Kuang, Pengfei Liu, Zhixiang Ren</div><div style='padding-top: 10px; width: 80ex'>The precise prediction of molecular properties is essential for advancements
in drug development, particularly in virtual screening and compound
optimization. The recent introduction of numerous deep learning-based methods
has shown remarkable potential in enhancing molecular property prediction
(MPP), especially improving accuracy and insights into molecular structures.
Yet, two critical questions arise: does the integration of domain knowledge
augment the accuracy of molecular property prediction and does employing
multi-modal data fusion yield more precise results than unique data source
methods? To explore these matters, we comprehensively review and quantitatively
analyze recent deep learning methods based on various benchmarks. We discover
that integrating molecular information will improve both MPP regression and
classification tasks by upto 3.98% and 1.72%, respectively. We also discover
that the utilizing 3-dimensional information with 1-dimensional and
2-dimensional information simultaneously can substantially enhance MPP upto
4.2%. The two consolidated insights offer crucial guidance for future
advancements in drug discovery.</div><div><a href='http://arxiv.org/abs/2402.07249v2'>2402.07249v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17174v1")'>A large dataset curation and benchmark for drug target interaction</div>
<div id='2401.17174v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:06:25Z</div><div>Authors: Alex Golts, Vadim Ratner, Yoel Shoshan, Moshe Raboh, Sagi Polaczek, Michal Ozery-Flato, Daniel Shats, Liam Hazan, Sivan Ravid, Efrat Hexter</div><div style='padding-top: 10px; width: 80ex'>Bioactivity data plays a key role in drug discovery and repurposing. The
resource-demanding nature of \textit{in vitro} and \textit{in vivo}
experiments, as well as the recent advances in data-driven computational
biochemistry research, highlight the importance of \textit{in silico} drug
target interaction (DTI) prediction approaches. While numerous large public
bioactivity data sources exist, research in the field could benefit from better
standardization of existing data resources. At present, different research
works that share similar goals are often difficult to compare properly because
of different choices of data sources and train/validation/test split
strategies. Additionally, many works are based on small data subsets, leading
to results and insights of possible limited validity. In this paper we propose
a way to standardize and represent efficiently a very large dataset curated
from multiple public sources, split the data into train, validation and test
sets based on different meaningful strategies, and provide a concrete
evaluation protocol to accomplish a benchmark. We analyze the proposed data
curation, prove its usefulness and validate the proposed benchmark through
experimental studies based on an existing neural network model.</div><div><a href='http://arxiv.org/abs/2401.17174v1'>2401.17174v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07632v1")'>CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs
  for Reduced hERG Liability</div>
<div id='2403.07632v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T13:12:24Z</div><div>Authors: Gregory W. Kyro, Matthew T. Martin, Eric D. Watt, Victor S. Batista</div><div style='padding-top: 10px; width: 80ex'>Drug-induced cardiotoxicity is a major health concern which can lead to
serious adverse effects including life-threatening cardiac arrhythmias via the
blockade of the voltage-gated hERG potassium ion channel. It is therefore of
tremendous interest to develop advanced methods to identify hERG-active
compounds in early stages of drug development, as well as to optimize
commercially available drugs for reduced hERG activity. In this work, we
present CardioGenAI, a machine learning-based framework for re-engineering both
developmental and marketed drugs for reduced hERG activity while preserving
their pharmacological activity. The framework incorporates novel
state-of-the-art discriminative models for predicting hERG channel activity, as
well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to
their potential implications in modulating the arrhythmogenic potential induced
by hERG channel blockade. These models can also serve independently as
effective components of a virtual screening pipeline. We applied the complete
framework to pimozide, an FDA-approved antipsychotic agent that demonstrates
high affinity to the hERG channel, and generated 100 refined candidates.
Remarkably, among the candidates is fluspirilene, a compound which is of the
same class of drugs (diphenylmethanes) as pimozide and therefore has similar
pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We
have made all of our software open-source to facilitate integration of the
CardioGenAI framework for molecular hypothesis generation into drug discovery
workflows.</div><div><a href='http://arxiv.org/abs/2403.07632v1'>2403.07632v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03089v1")'>VQSynery: Robust Drug Synergy Prediction With Vector Quantization
  Mechanism</div>
<div id='2403.03089v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T16:21:53Z</div><div>Authors: Jiawei Wu, Mingyuan Yan, Dianbo Liu</div><div style='padding-top: 10px; width: 80ex'>The pursuit of optimizing cancer therapies is significantly advanced by the
accurate prediction of drug synergy. Traditional methods, such as clinical
trials, are reliable yet encumbered by extensive time and financial demands.
The emergence of high-throughput screening and computational innovations has
heralded a shift towards more efficient methodologies for exploring drug
interactions. In this study, we present VQSynergy, a novel framework that
employs the Vector Quantization (VQ) mechanism, integrated with gated residuals
and a tailored attention mechanism, to enhance the precision and
generalizability of drug synergy predictions. Our findings demonstrate that
VQSynergy surpasses existing models in terms of robustness, particularly under
Gaussian noise conditions, highlighting its superior performance and utility in
the complex and often noisy domain of drug synergy research. This study
underscores the potential of VQSynergy in revolutionizing the field through its
advanced predictive capabilities, thereby contributing to the optimization of
cancer treatment strategies.</div><div><a href='http://arxiv.org/abs/2403.03089v1'>2403.03089v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03768v3")'>DeepCRE: Transforming Drug R&amp;D via AI-Driven Cross-drug Response
  Evaluation</div>
<div id='2403.03768v3' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T15:03:09Z</div><div>Authors: Yushuai Wu, Ting Zhang, Hao Zhou, Hainan Wu, Hanwen Sunchu, Lei Hu, Xiaofang Chen, Suyuan Zhao, Gaochao Liu, Chao Sun, Jiahuan Zhang, Yizhen Luo, Peng Liu, Zaiqing Nie, Yushuai Wu</div><div style='padding-top: 10px; width: 80ex'>The fields of therapeutic application and drug research and development (R&amp;D)
both face substantial challenges, i.e., the therapeutic domain calls for more
treatment alternatives, while numerous promising pre-clinical drugs have failed
in clinical trials. One of the reasons is the inadequacy of Cross-drug Response
Evaluation (CRE) during the late stages of drug R&amp;D. Although in-silico CRE
models bring a promising solution, existing methodologies are restricted to
early stages of drug R&amp;D, such as target and cell-line levels, offering limited
improvement to clinical success rates. Herein, we introduce DeepCRE, a
pioneering AI model designed to predict CRE effectively in the late stages of
drug R&amp;D. DeepCRE outperforms the existing best models by achieving an average
performance improvement of 17.7% in patient-level CRE, and a 5-fold increase in
indication-level CRE, facilitating more accurate personalized treatment
predictions and better pharmaceutical value assessment for indications,
respectively. Furthermore, DeepCRE has identified a set of six drug candidates
that show significantly greater effectiveness than a comparator set of two
approved drugs in 5/8 colorectal cancer organoids. This demonstrates the
capability of DeepCRE to systematically uncover a spectrum of drug candidates
with enhanced therapeutic effects, highlighting its potential to transform drug
R&amp;D.</div><div><a href='http://arxiv.org/abs/2403.03768v3'>2403.03768v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09176v1")'>ADCNet: a unified framework for predicting the activity of antibody-drug
  conjugates</div>
<div id='2401.09176v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T12:34:17Z</div><div>Authors: Liye Chen, Biaoshun Li, Yihao Chen, Mujie Lin, Shipeng Zhang, Chenxin Li, Yu Pang, Ling Wang</div><div style='padding-top: 10px; width: 80ex'>Antibody-drug conjugate (ADC) has revolutionized the field of cancer
treatment in the era of precision medicine due to their ability to precisely
target cancer cells and release highly effective drug. Nevertheless, the
realization of rational design of ADC is very difficult because the
relationship between their structures and activities is difficult to
understand. In the present study, we introduce a unified deep learning
framework called ADCNet to help design potential ADCs. The ADCNet highly
integrates the protein representation learning language model ESM-2 and
small-molecule representation learning language model FG-BERT models to achieve
activity prediction through learning meaningful features from antigen and
antibody protein sequences of ADC, SMILES strings of linker and payload, and
drug-antibody ratio (DAR) value. Based on a carefully designed and manually
tailored ADC data set, extensive evaluation results reveal that ADCNet performs
best on the test set compared to baseline machine learning models across all
evaluation metrics. For example, it achieves an average prediction accuracy of
87.12%, a balanced accuracy of 0.8689, and an area under receiver operating
characteristic curve of 0.9293 on the test set. In addition, cross-validation,
ablation experiments, and external independent testing results further prove
the stability, advancement, and robustness of the ADCNet architecture. For the
convenience of the community, we develop the first online platform
(https://ADCNet.idruglab.cn) for the prediction of ADCs activity based on the
optimal ADCNet model, and the source code is publicly available at
https://github.com/idrugLab/ADCNet.</div><div><a href='http://arxiv.org/abs/2401.09176v1'>2401.09176v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14442v2")'>Improving Antibody Humanness Prediction using Patent Data</div>
<div id='2401.14442v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T16:04:17Z</div><div>Authors: Talip Ucar, Aubin Ramon, Dino Oglic, Rebecca Croasdale-Wood, Tom Diethe, Pietro Sormanni</div><div style='padding-top: 10px; width: 80ex'>We investigate the potential of patent data for improving the antibody
humanness prediction using a multi-stage, multi-loss training process.
Humanness serves as a proxy for the immunogenic response to antibody
therapeutics, one of the major causes of attrition in drug discovery and a
challenging obstacle for their use in clinical settings. We pose the initial
learning stage as a weakly-supervised contrastive-learning problem, where each
antibody sequence is associated with possibly multiple identifiers of function
and the objective is to learn an encoder that groups them according to their
patented properties. We then freeze a part of the contrastive encoder and
continue training it on the patent data using the cross-entropy loss to predict
the humanness score of a given antibody sequence. We illustrate the utility of
the patent data and our approach by performing inference on three different
immunogenicity datasets, unseen during training. Our empirical results
demonstrate that the learned model consistently outperforms the alternative
baselines and establishes new state-of-the-art on five out of six inference
tasks, irrespective of the used metric.</div><div><a href='http://arxiv.org/abs/2401.14442v2'>2401.14442v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14080v1")'>Efficient Normalized Conformal Prediction and Uncertainty Quantification
  for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests</div>
<div id='2402.14080v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:09:53Z</div><div>Authors: Daniel Nolte, Souparno Ghosh, Ranadip Pal</div><div style='padding-top: 10px; width: 80ex'>Deep learning models are being adopted and applied on various critical
decision-making tasks, yet they are trained to provide point predictions
without providing degrees of confidence. The trustworthiness of deep learning
models can be increased if paired with uncertainty estimations. Conformal
Prediction has emerged as a promising method to pair machine learning models
with prediction intervals, allowing for a view of the model's uncertainty.
However, popular uncertainty estimation methods for conformal prediction fail
to provide heteroskedastic intervals that are equally accurate for all samples.
In this paper, we propose a method to estimate the uncertainty of each sample
by calculating the variance obtained from a Deep Regression Forest. We show
that the deep regression forest variance improves the efficiency and coverage
of normalized inductive conformal prediction on a drug response prediction
task.</div><div><a href='http://arxiv.org/abs/2402.14080v1'>2402.14080v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.05145v1")'>Machine Learning to Promote Translational Research: Predicting Patent
  and Clinical Trial Inclusion in Dementia Research</div>
<div id='2401.05145v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T13:25:49Z</div><div>Authors: Matilda Beinat, Julian Beinat, Mohammed Shoaib, Jorge Gomez Magenti</div><div style='padding-top: 10px; width: 80ex'>Projected to impact 1.6 million people in the UK by 2040 and costing
{\pounds}25 billion annually, dementia presents a growing challenge to society.
This study, a pioneering effort to predict the translational potential of
dementia research using machine learning, hopes to address the slow translation
of fundamental discoveries into practical applications despite dementia's
significant societal and economic impact. We used the Dimensions database to
extract data from 43,091 UK dementia research publications between the years
1990-2023, specifically metadata (authors, publication year etc.), concepts
mentioned in the paper, and the paper abstract. To prepare the data for machine
learning we applied methods such as one hot encoding and/or word embeddings. We
trained a CatBoost Classifier to predict if a publication will be cited in a
future patent or clinical trial. We trained several model variations. The model
combining metadata, concept, and abstract embeddings yielded the highest
performance: for patent predictions, an Area Under the Receiver Operating
Characteristic Curve (AUROC) of 0.84 and 77.17% accuracy; for clinical trial
predictions, an AUROC of 0.81 and 75.11% accuracy. The results demonstrate that
integrating machine learning within current research methodologies can uncover
overlooked publications, expediting the identification of promising research
and potentially transforming dementia research by predicting real-world impact
and guiding translational strategies.</div><div><a href='http://arxiv.org/abs/2401.05145v1'>2401.05145v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13214v1")'>Nellie: Automated organelle segmentation, tracking, and hierarchical
  feature extraction in 2D/3D live-cell microscopy</div>
<div id='2403.13214v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T00:23:42Z</div><div>Authors: Austin E. Y. T. Lefebvre, Gabriel Sturm, Ting-Yu Lin, Emily Stoops, Magdalena Preciado Lopez, Benjamin Kaufmann-Malaga, Kayley Hake</div><div style='padding-top: 10px; width: 80ex'>The analysis of dynamic organelles remains a formidable challenge, though key
to understanding biological processes. We introduce Nellie, an automated and
unbiased pipeline for segmentation, tracking, and feature extraction of diverse
intracellular structures. Nellie adapts to image metadata, eliminating user
input. Nellie's preprocessing pipeline enhances structural contrast on multiple
intracellular scales allowing for robust hierarchical segmentation of
sub-organellar regions. Internal motion capture markers are generated and
tracked via a radius-adaptive pattern matching scheme, and used as guides for
sub-voxel flow interpolation. Nellie extracts a plethora of features at
multiple hierarchical levels for deep and customizable analysis. Nellie
features a Napari-based GUI that allows for code-free operation and
visualization, while its modular open-source codebase invites customization by
experienced users. We demonstrate Nellie's wide variety of use cases with two
examples: unmixing multiple organelles from a single channel using
feature-based classification and training an unsupervised graph autoencoder on
mitochondrial multi-mesh graphs to quantify latent space embedding changes
following ionomycin treatment.</div><div><a href='http://arxiv.org/abs/2403.13214v1'>2403.13214v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02501v2")'>The cell signaling structure function</div>
<div id='2401.02501v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T19:25:00Z</div><div>Authors: Layton Aho, Mark Winter, Marc DeCarlo, Agne Frismantiene, Yannick Blum, Paolo Armando Gagliardi, Olivier Pertz, Andrew R. Cohen</div><div style='padding-top: 10px; width: 80ex'>Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here an approach
to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell
microscopy movies unique in requiring no a priori knowledge of expected pattern
dynamics, and no training data. The proposed cell signaling structure function
(SSF) is a Kolmogorov structure function that optimally measures cell signaling
state as nuclear intensity w.r.t. surrounding cytoplasm, a significant
improvement compared to the current state-of-the-art cytonuclear ratio. SSF
kymographs store at each spatiotemporal cell centroid the SSF value, or a
functional output such as velocity. Patterns of similarity are identified via
the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input SSF kymographs as points
in a low dimensional embedding that optimally captures the pattern similarity
identified by the NCD throughout the space. The only parameter is the expected
cell radii ($\mu m$). A new formulation of the cluster structure function
optimally estimates how meaningful an embedding from the RKHS representation.
Results are presented quantifying the impact of ERK and AKT signaling between
different oncogenic mutations, and by the relation between ERK signaling and
cellular velocity patterns for movies of 2-D monolayers of human breast
epithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation
of ERK, and human induced pluripotent stem cells .</div><div><a href='http://arxiv.org/abs/2401.02501v2'>2401.02501v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17695v1")'>Datacube segmentation via Deep Spectral Clustering</div>
<div id='2401.17695v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T09:31:28Z</div><div>Authors: Alessandro Bombini, Fernando García-Avello Bofías, Caterina Bracci, Michele Ginolfi, Chiara Ruberto</div><div style='padding-top: 10px; width: 80ex'>Extended Vision techniques are ubiquitous in physics. However, the data cubes
steaming from such analysis often pose a challenge in their interpretation, due
to the intrinsic difficulty in discerning the relevant information from the
spectra composing the data cube.
  Furthermore, the huge dimensionality of data cube spectra poses a complex
task in its statistical interpretation; nevertheless, this complexity contains
a massive amount of statistical information that can be exploited in an
unsupervised manner to outline some essential properties of the case study at
hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering
of data-cube's spectra, performed in a suitably defined low-dimensional
embedding space.
  To tackle this topic, we explore the possibility of applying unsupervised
clustering methods in encoded space, i.e. perform deep clustering on the
spectral properties of datacube pixels. A statistical dimensional reduction is
performed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping
spectra into lower dimensional metric spaces, while the clustering process is
performed by a (learnable) iterative K-Means clustering algorithm.
  We apply this technique to two different use cases, of different physical
origins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on
pictorial artworks, and a dataset of simulated astrophysical observations.</div><div><a href='http://arxiv.org/abs/2401.17695v1'>2401.17695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11996v2")'>ISCUTE: Instance Segmentation of Cables Using Text Embedding</div>
<div id='2402.11996v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:41:57Z</div><div>Authors: Shir Kozlovsky, Omkar Joglekar, Dotan Di Castro</div><div style='padding-top: 10px; width: 80ex'>In the field of robotics and automation, conventional object recognition and
instance segmentation methods face a formidable challenge when it comes to
perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible
tubes. This challenge arises primarily from the lack of distinct attributes
such as shape, color, and texture, which calls for tailored solutions to
achieve precise identification. In this work, we propose a foundation
model-based DLO instance segmentation technique that is text-promptable and
user-friendly. Specifically, our approach combines the text-conditioned
semantic segmentation capabilities of CLIPSeg model with the zero-shot
generalization capabilities of Segment Anything Model (SAM). We show that our
method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU
of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for
instance segmentation.</div><div><a href='http://arxiv.org/abs/2402.11996v2'>2402.11996v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13014v1")'>General Line Coordinates in 3D</div>
<div id='2403.13014v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T17:42:20Z</div><div>Authors: Joshua Martinez, Boris Kovalerchuk</div><div style='padding-top: 10px; width: 80ex'>Interpretable interactive visual pattern discovery in lossless 3D
visualization is a promising way to advance machine learning. It enables end
users who are not data scientists to take control of the model development
process as a self-service. It is conducted in 3D General Line Coordinates (GLC)
visualization space, which preserves all n-D information in 3D. This paper
presents a system which combines three types of GLC: Shifted Paired Coordinates
(SPC), Shifted Tripled Coordinates (STC), and General Line Coordinates-Linear
(GLC-L) for interactive visual pattern discovery. A transition from 2-D
visualization to 3-D visualization allows for a more distinct visual pattern
than in 2-D and it also allows for finding the best data viewing positions,
which are not available in 2-D. It enables in-depth visual analysis of various
class-specific data subsets comprehensible for end users in the original
interpretable attributes. Controlling model overgeneralization by end users is
an additional benefit of this approach.</div><div><a href='http://arxiv.org/abs/2403.13014v1'>2403.13014v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12533v1")'>Efficient Constrained $k$-Center Clustering with Background Knowledge</div>
<div id='2401.12533v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T07:16:32Z</div><div>Authors: Longkun Guo, Chaoqi Jia, Kewen Liao, Zhigang Lu, Minhui Xue</div><div style='padding-top: 10px; width: 80ex'>Center-based clustering has attracted significant research interest from both
theory and practice. In many practical applications, input data often contain
background knowledge that can be used to improve clustering results. In this
work, we build on widely adopted $k$-center clustering and model its input
background knowledge as must-link (ML) and cannot-link (CL) constraint sets.
However, most clustering problems including $k$-center are inherently
$\mathcal{NP}$-hard, while the more complex constrained variants are known to
suffer severer approximation and computation barriers that significantly limit
their applicability. By employing a suite of techniques including reverse
dominating sets, linear programming (LP) integral polyhedron, and LP duality,
we arrive at the first efficient approximation algorithm for constrained
$k$-center with the best possible ratio of 2. We also construct competitive
baseline algorithms and empirically evaluate our approximation algorithm
against them on a variety of real datasets. The results validate our
theoretical findings and demonstrate the great advantages of our algorithm in
terms of clustering cost, clustering quality, and running time.</div><div><a href='http://arxiv.org/abs/2401.12533v1'>2401.12533v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05502v1")'>Diversity-aware clustering: Computational Complexity and Approximation
  Algorithms</div>
<div id='2401.05502v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T19:01:05Z</div><div>Authors: Suhas Thejaswi, Ameet Gadekar, Bruno Ordozgoiti, Aristides Gionis</div><div style='padding-top: 10px; width: 80ex'>In this work, we study diversity-aware clustering problems where the data
points are associated with multiple attributes resulting in intersecting
groups. A clustering solution need to ensure that a minimum number of cluster
centers are chosen from each group while simultaneously minimizing the
clustering objective, which can be either $k$-median, $k$-means or
$k$-supplier. We present parameterized approximation algorithms with
approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for
diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware
$k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH
and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint
faicility groups, we present parameterized approximation algorithm with
approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For
fair $k$-supplier with disjoint facility groups, we present a polynomial-time
approximation algorithm with factor $3$, improving the previous best known
approximation ratio of factor $5$.</div><div><a href='http://arxiv.org/abs/2401.05502v1'>2401.05502v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11351v1")'>An SDP-based Branch-and-Cut Algorithm for Biclustering</div>
<div id='2403.11351v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T21:43:19Z</div><div>Authors: Antonio M. Sudoso</div><div style='padding-top: 10px; width: 80ex'>Biclustering, also called co-clustering, block clustering, or two-way
clustering, involves the simultaneous clustering of both the rows and columns
of a data matrix into distinct groups, such that the rows and columns within a
group display similar patterns. As a model problem for biclustering, we
consider the $k$-densest-disjoint biclique problem, whose goal is to identify
$k$ disjoint complete bipartite subgraphs (called bicliques) of a given
weighted complete bipartite graph such that the sum of their densities is
maximized. To address this problem, we present a tailored branch-and-cut
algorithm. For the upper bound routine, we consider a semidefinite programming
relaxation and propose valid inequalities to strengthen the bound. We solve
this relaxation in a cutting-plane fashion using a first-order method. For the
lower bound, we design a maximum weight matching rounding procedure that
exploits the solution of the relaxation solved at each node. Computational
results on both synthetic and real-world instances show that the proposed
algorithm can solve instances approximately 20 times larger than those handled
by general-purpose solvers.</div><div><a href='http://arxiv.org/abs/2403.11351v1'>2403.11351v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03736v1")'>An Effective Branch-and-Bound Algorithm with New Bounding Methods for
  the Maximum $s$-Bundle Problem</div>
<div id='2402.03736v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:05:11Z</div><div>Authors: Jinghui Xue, Jiongzhi Zheng, Mingming Jin, Kun He</div><div style='padding-top: 10px; width: 80ex'>The Maximum s-Bundle Problem (MBP) addresses the task of identifying a
maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if
its vertex connectivity is at least |V|-s, where the vertex connectivity equals
the minimum number of vertices whose deletion yields a disconnected or trivial
graph. MBP is NP-hard and holds relevance in numerous realworld scenarios
emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the
branch-and-bound (BnB) framework, whose performance heavily depends on the
quality of the upper bound on the cardinality of a maximum s-bundle and the
initial lower bound with graph reduction. In this work, we introduce a novel
Partition-based Upper Bound (PUB) that leverages the graph partitioning
technique to achieve a tighter upper bound compared to existing ones. To
increase the lower bound, we propose to do short random walks on a clique to
generate larger initial solutions. Then, we propose a new BnB algorithm that
uses the initial lower bound and PUB in preprocessing for graph reduction, and
uses PUB in the BnB search process for branch pruning. Extensive experiments
with diverse s values demonstrate the significant progress of our algorithm
over state-of-the-art BnB MBP algorithms. Moreover, our initial lower bound can
also be generalized to other relaxation clique problems.</div><div><a href='http://arxiv.org/abs/2402.03736v1'>2402.03736v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14013v1")'>Towards a connection between the capacitated vehicle routing problem and
  the constrained centroid-based clustering</div>
<div id='2403.14013v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T22:24:36Z</div><div>Authors: Abdelhakim Abdellaoui, Loubna Benabbou, Issmail El Hallaoui</div><div style='padding-top: 10px; width: 80ex'>Efficiently solving a vehicle routing problem (VRP) in a practical runtime is
a critical challenge for delivery management companies. This paper explores
both a theoretical and experimental connection between the Capacitated Vehicle
Routing Problem (CVRP) and the Constrained Centroid-Based Clustering (CCBC).
Reducing a CVRP to a CCBC is a synonym for a transition from an exponential to
a polynomial complexity using commonly known algorithms for clustering, i.e
K-means. At the beginning, we conduct an exploratory analysis to highlight the
existence of such a relationship between the two problems through illustrative
small-size examples and simultaneously deduce some mathematically-related
formulations and properties. On a second level, the paper proposes a CCBC based
approach endowed with some enhancements. The proposed framework consists of
three stages. At the first step, a constrained centroid-based clustering
algorithm generates feasible clusters of customers. This methodology
incorporates three enhancement tools to achieve near-optimal clusters, namely:
a multi-start procedure for initial centroids, a customer assignment metric,
and a self-adjustment mechanism for choosing the number of clusters. At the
second step, a traveling salesman problem (T SP) solver is used to optimize the
order of customers within each cluster. Finally, we introduce a process relying
on routes cutting and relinking procedure, which calls upon solving a linear
and integer programming model to further improve the obtained routes. This step
is inspired by the ruin &amp; recreate algorithm. This approach is an extension of
the classical cluster-first, route-second method and provides near-optimal
solutions on well-known benchmark instances in terms of solution quality and
computational runtime, offering a milestone in solving VRP.</div><div><a href='http://arxiv.org/abs/2403.14013v1'>2403.14013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00041v1")'>Spatial-temporal-demand clustering for solving large-scale vehicle
  routing problems with time windows</div>
<div id='2402.00041v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T06:06:01Z</div><div>Authors: Christoph Kerscher, Stefan Minner</div><div style='padding-top: 10px; width: 80ex'>Several metaheuristics use decomposition and pruning strategies to solve
large-scale instances of the vehicle routing problem (VRP). Those complexity
reduction techniques often rely on simple, problem-specific rules. However, the
growth in available data and advances in computer hardware enable data-based
approaches that use machine learning (ML) to improve scalability of solution
algorithms. We propose a decompose-route-improve (DRI) framework that groups
customers using clustering. Its similarity metric incorporates customers'
spatial, temporal, and demand data and is formulated to reflect the problem's
objective function and constraints. The resulting sub-routing problems can
independently be solved using any suitable algorithm. We apply pruned local
search (LS) between solved subproblems to improve the overall solution. Pruning
is based on customers' similarity information obtained in the decomposition
phase. In a computational study, we parameterize and compare existing
clustering algorithms and benchmark the DRI against the Hybrid Genetic Search
(HGS) of Vidal et al. (2013). Results show that our data-based approach
outperforms classic cluster-first, route-second approaches solely based on
customers' spatial information. The newly introduced similarity metric forms
separate sub-VRPs and improves the selection of LS moves in the improvement
phase. Thus, the DRI scales existing metaheuristics to achieve high-quality
solutions faster for large-scale VRPs by efficiently reducing complexity.
Further, the DRI can be easily adapted to various solution methods and VRP
characteristics, such as distribution of customer locations and demands, depot
location, and different time window scenarios, making it a generalizable
approach to solving routing problems.</div><div><a href='http://arxiv.org/abs/2402.00041v1'>2402.00041v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06968v1")'>Contextual Stochastic Vehicle Routing with Time Windows</div>
<div id='2402.06968v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T14:56:36Z</div><div>Authors: Breno Serrano, Alexandre M. Florio, Stefan Minner, Maximilian Schiffer, Thibaut Vidal</div><div style='padding-top: 10px; width: 80ex'>We study the vehicle routing problem with time windows (VRPTW) and stochastic
travel times, in which the decision-maker observes related contextual
information, represented as feature variables, before making routing decisions.
Despite the extensive literature on stochastic VRPs, the integration of feature
variables has received limited attention in this context. We introduce the
contextual stochastic VRPTW, which minimizes the total transportation cost and
expected late arrival penalties conditioned on the observed features. Since the
joint distribution of travel times and features is unknown, we present novel
data-driven prescriptive models that use historical data to provide an
approximate solution to the problem. We distinguish the prescriptive models
between point-based approximation, sample average approximation, and
penalty-based approximation, each taking a different perspective on dealing
with stochastic travel times and features. We develop specialized
branch-price-and-cut algorithms to solve these data-driven prescriptive models.
In our computational experiments, we compare the out-of-sample cost performance
of different methods on instances with up to one hundred customers. Our results
show that, surprisingly, a feature-dependent sample average approximation
outperforms existing and novel methods in most settings.</div><div><a href='http://arxiv.org/abs/2402.06968v1'>2402.06968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13224v3")'>Controlling Large Electric Vehicle Charging Stations via User Behavior
  Modeling and Stochastic Programming</div>
<div id='2402.13224v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T18:37:11Z</div><div>Authors: Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud</div><div style='padding-top: 10px; width: 80ex'>This paper introduces an Electric Vehicle Charging Station (EVCS) model that
incorporates real-world constraints, such as slot power limitations, contract
threshold overruns penalties, or early disconnections of electric vehicles
(EVs). We propose a formulation of the problem of EVCS control under
uncertainty, and implement two Multi-Stage Stochastic Programming approaches
that leverage user-provided information, namely, Model Predictive Control and
Two-Stage Stochastic Programming. The model addresses uncertainties in charging
session start and end times, as well as in energy demand. A user's behavior
model based on a sojourn-time-dependent stochastic process enhances cost
reduction while maintaining customer satisfaction. The benefits of the two
proposed methods are showcased against two baselines over a 22-day simulation
using a real-world dataset. The two-stage approach demonstrates robustness
against early disconnections by considering a wider range of uncertainty
scenarios for optimization. The algorithm prioritizing user satisfaction over
electricity cost achieves a 20% and 36% improvement in two user satisfaction
metrics compared to an industry-standard baseline. Additionally, the algorithm
striking the best balance between cost and user satisfaction exhibits a mere 3%
relative cost increase compared to the theoretically optimal baseline - for
which the nonanticipativity constraint is relaxed - while attaining 94% and 84%
of the user satisfaction performance in the two used satisfaction metrics.</div><div><a href='http://arxiv.org/abs/2402.13224v3'>2402.13224v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01608v1")'>Contingency Analysis of a Grid of Connected EVs for Primary Frequency
  Control of an Industrial Microgrid Using Efficient Control Scheme</div>
<div id='2402.01608v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:14:16Z</div><div>Authors: J. N. Sabhahit, S. S. Solanke, V. K. Jadoun, H. Malik, F. P. García Márquez, J. M. Pinar-Pérez</div><div style='padding-top: 10px; width: 80ex'>After over a century of internal combustion engines ruling the transport
sector, electric vehicles appear to be on the verge of gaining traction due to
a slew of advantages, including lower operating costs and lower CO2 emissions.
By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are
utilized as load) approach, EVs can operate as both a load and a source.
Primary frequency regulation and congestion management are two essential
characteristics of this technology that are added to an industrial microgrid.
Industrial Microgrids are made up of different energy sources such as wind
farms and PV farms, storage systems, and loads. EVs have gained a lot of
interest as a technique for frequency management because of their ability to
regulate quickly. Grid reliability depends on this quick reaction. Different
contingency, state of charge of the electric vehicles, and a varying number of
EVs in an EV fleet are considered in this work, and a proposed control scheme
for frequency management is presented. This control scheme enables
bidirectional power flow, allowing for primary frequency regulation during the
various scenarios that an industrial microgrid may encounter over the course of
a 24-h period. The presented controller will provide dependable frequency
regulation support to the industrial microgrid during contingencies, as will be
demonstrated by simulation results, achieving a more reliable system. However,
simulation results will show that by increasing a number of the EVs in a fleet
for the Vehicle-to-Grid approach, an industrial microgrid\'s frequency can be
enhanced even further.</div><div><a href='http://arxiv.org/abs/2402.01608v1'>2402.01608v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.11103v1")'>Efficient Data Shapley for Weighted Nearest Neighbor Algorithms</div>
<div id='2401.11103v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T03:34:18Z</div><div>Authors: Jiachen T. Wang, Prateek Mittal, Ruoxi Jia</div><div style='padding-top: 10px; width: 80ex'>This work aims to address an open problem in data valuation literature
concerning the efficient computation of Data Shapley for weighted $K$ nearest
neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label
KNN with discretized weights as the utility function, we reframe the
computation of WKNN-Shapley into a counting problem and introduce a
quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the
best result from existing literature. We develop a deterministic approximation
algorithm that further improves computational efficiency while maintaining the
key fairness properties of the Shapley value. Through extensive experiments, we
demonstrate WKNN-Shapley's computational efficiency and its superior
performance in discerning data quality compared to its unweighted counterpart.</div><div><a href='http://arxiv.org/abs/2401.11103v1'>2401.11103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03664v1")'>Efficient Solvers for Partial Gromov-Wasserstein</div>
<div id='2402.03664v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T03:36:05Z</div><div>Authors: Yikun Bai, Rocio Diaz Martin, Hengrong Du, Ashkan Shahbazi, Soheil Kolouri</div><div style='padding-top: 10px; width: 80ex'>The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of
measures with unequal masses residing in potentially distinct metric spaces,
thereby enabling unbalanced and partial matching across these spaces. In this
paper, we demonstrate that the PGW problem can be transformed into a variant of
the Gromov-Wasserstein problem, akin to the conversion of the partial optimal
transport problem into an optimal transport problem. This transformation leads
to two new solvers, mathematically and computationally equivalent, based on the
Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We
further establish that the PGW problem constitutes a metric for metric measure
spaces. Finally, we validate the effectiveness of our proposed solvers in terms
of computation time and performance on shape-matching and positive-unlabeled
learning problems, comparing them against existing baselines.</div><div><a href='http://arxiv.org/abs/2402.03664v1'>2402.03664v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06021v1")'>Hierarchical Query Classification in E-commerce Search</div>
<div id='2403.06021v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T21:55:55Z</div><div>Authors: Bing He, Sreyashi Nag, Limeng Cui, Suhang Wang, Zheng Li, Rahul Goutam, Zhen Li, Haiyang Zhang</div><div style='padding-top: 10px; width: 80ex'>E-commerce platforms typically store and structure product information and
search data in a hierarchy. Efficiently categorizing user search queries into a
similar hierarchical structure is paramount in enhancing user experience on
e-commerce platforms as well as news curation and academic research. The
significance of this task is amplified when dealing with sensitive query
categorization or critical information dissemination, where inaccuracies can
lead to considerable negative impacts. The inherent complexity of hierarchical
query classification is compounded by two primary challenges: (1) the
pronounced class imbalance that skews towards dominant categories, and (2) the
inherent brevity and ambiguity of search queries that hinder accurate
classification.
  To address these challenges, we introduce a novel framework that leverages
hierarchical information through (i) enhanced representation learning that
utilizes the contrastive loss to discern fine-grained instance relationships
within the hierarchy, called ''instance hierarchy'', and (ii) a nuanced
hierarchical classification loss that attends to the intrinsic label taxonomy,
named ''label hierarchy''. Additionally, based on our observation that certain
unlabeled queries share typographical similarities with labeled queries, we
propose a neighborhood-aware sampling technique to intelligently select these
unlabeled queries to boost the classification performance. Extensive
experiments demonstrate that our proposed method is better than
state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to
SOTA on the public datasets of Web of Science and RCV1-V2. These results
underscore the efficacy of our proposed solution, and pave the path toward the
next generation of hierarchy-aware query classification systems.</div><div><a href='http://arxiv.org/abs/2403.06021v1'>2403.06021v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10091v1")'>Text-Based Product Matching -- Semi-Supervised Clustering Approach</div>
<div id='2402.10091v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:52:26Z</div><div>Authors: Alicja Martinek, Szymon Łukasik, Amir H. Gandomi</div><div style='padding-top: 10px; width: 80ex'>Matching identical products present in multiple product feeds constitutes a
crucial element of many tasks of e-commerce, such as comparing product
offerings, dynamic price optimization, and selecting the assortment
personalized for the client. It corresponds to the well-known machine learning
task of entity matching, with its own specificity, like omnipresent
unstructured data or inaccurate and inconsistent product descriptions. This
paper aims to present a new philosophy to product matching utilizing a
semi-supervised clustering approach. We study the properties of this method by
experimenting with the IDEC algorithm on the real-world dataset using
predominantly textual features and fuzzy string matching, with more standard
approaches as a point of reference. Encouraging results show that unsupervised
matching, enriched with a small annotated sample of product links, could be a
possible alternative to the dominant supervised strategy, requiring extensive
manual data labeling.</div><div><a href='http://arxiv.org/abs/2402.10091v1'>2402.10091v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.05953v1")'>idMotif: An Interactive Motif Identification in Protein Sequences</div>
<div id='2402.05953v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T06:51:03Z</div><div>Authors: Ji Hwan Park, Vikash Prasad, Sydney Newsom, Fares Najar, Rakhi Rajan</div><div style='padding-top: 10px; width: 80ex'>This article introduces idMotif, a visual analytics framework designed to aid
domain experts in the identification of motifs within protein sequences.
Motifs, short sequences of amino acids, are critical for understanding the
distinct functions of proteins. Identifying these motifs is pivotal for
predicting diseases or infections. idMotif employs a deep learning-based method
for the categorization of protein sequences, enabling the discovery of
potential motif candidates within protein groups through local explanations of
deep learning model decisions. It offers multiple interactive views for the
analysis of protein clusters or groups and their sequences. A case study,
complemented by expert feedback, illustrates idMotif's utility in facilitating
the analysis and identification of protein sequences and motifs.</div><div><a href='http://arxiv.org/abs/2402.05953v1'>2402.05953v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02321v1")'>Active Learning for Graphs with Noisy Structures</div>
<div id='2402.02321v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T02:23:45Z</div><div>Authors: Hongliang Chi, Cong Qi, Suhang Wang, Yao Ma</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have seen significant success in tasks such as
node classification, largely contingent upon the availability of sufficient
labeled nodes. Yet, the excessive cost of labeling large-scale graphs led to a
focus on active learning on graphs, which aims for effective data selection to
maximize downstream model performance. Notably, most existing methods assume
reliable graph topology, while real-world scenarios often present noisy graphs.
Given this, designing a successful active learning framework for noisy graphs
is highly needed but challenging, as selecting data for labeling and obtaining
a clean graph are two tasks naturally interdependent: selecting high-quality
data requires clean graph structure while cleaning noisy graph structure
requires sufficient labeled data. Considering the complexity mentioned above,
we propose an active learning framework, GALClean, which has been specifically
designed to adopt an iterative approach for conducting both data selection and
graph purification simultaneously with best information learned from the prior
iteration. Importantly, we summarize GALClean as an instance of the
Expectation-Maximization algorithm, which provides a theoretical understanding
of its design and mechanisms. This theory naturally leads to an enhanced
version, GALClean+. Extensive experiments have demonstrated the effectiveness
and robustness of our proposed method across various types and levels of noisy
graphs.</div><div><a href='http://arxiv.org/abs/2402.02321v1'>2402.02321v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00447v1")'>A Survey of Data-Efficient Graph Learning</div>
<div id='2402.00447v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T09:28:48Z</div><div>Authors: Wei Ju, Siyu Yi, Yifan Wang, Qingqing Long, Junyu Luo, Zhiping Xiao, Ming Zhang</div><div style='padding-top: 10px; width: 80ex'>Graph-structured data, prevalent in domains ranging from social networks to
biochemical analysis, serve as the foundation for diverse real-world systems.
While graph neural networks demonstrate proficiency in modeling this type of
data, their success is often reliant on significant amounts of labeled data,
posing a challenge in practical scenarios with limited annotation resources. To
tackle this problem, tremendous efforts have been devoted to enhancing graph
machine learning performance under low-resource settings by exploring various
approaches to minimal supervision. In this paper, we introduce a novel concept
of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the
first survey that summarizes the current progress of DEGL. We initiate by
highlighting the challenges inherent in training models with large labeled
data, paving the way for our exploration into DEGL. Next, we systematically
review recent advances on this topic from several key aspects, including
self-supervised graph learning, semi-supervised graph learning, and few-shot
graph learning. Also, we state promising directions for future research,
contributing to the evolution of graph machine learning.</div><div><a href='http://arxiv.org/abs/2402.00447v1'>2402.00447v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01121v1")'>OpenGraph: Towards Open Graph Foundation Models</div>
<div id='2403.01121v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T08:05:03Z</div><div>Authors: Lianghao Xia, Ben Kao, Chao Huang</div><div style='padding-top: 10px; width: 80ex'>Graph learning has become indispensable for interpreting and harnessing
relational data in diverse fields, ranging from recommendation systems to
social network analysis. In this context, a variety of GNNs have emerged as
promising methodologies for encoding the structural information of graphs. By
effectively capturing the graph's underlying structure, these GNNs have shown
great potential in enhancing performance in graph learning tasks, such as link
prediction and node classification. However, despite their successes, a
significant challenge persists: these advanced methods often face difficulties
in generalizing to unseen graph data that significantly differs from the
training instances. In this work, our aim is to advance the graph learning
paradigm by developing a general graph foundation model. This model is designed
to understand the complex topological patterns present in diverse graph data,
enabling it to excel in zero-shot graph learning tasks across different
downstream datasets. To achieve this goal, we address several key technical
challenges in our OpenGraph model. Firstly, we propose a unified graph
tokenizer to adapt our graph model to generalize well on unseen graph data,
even when the underlying graph properties differ significantly from those
encountered during training. Secondly, we develop a scalable graph transformer
as the foundational encoder, which effectively captures node-wise dependencies
within the global topological context. Thirdly, we introduce a data
augmentation mechanism enhanced by a LLM to alleviate the limitations of data
scarcity in real-world scenarios. Extensive experiments validate the
effectiveness of our framework. By adapting our OpenGraph to new graph
characteristics and comprehending the nuances of diverse graphs, our approach
achieves remarkable zero-shot graph learning performance across various
settings and domains.</div><div><a href='http://arxiv.org/abs/2403.01121v1'>2403.01121v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11235v1")'>ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs</div>
<div id='2402.11235v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T09:52:43Z</div><div>Authors: Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li</div><div style='padding-top: 10px; width: 80ex'>With the development of foundation models such as large language models,
zero-shot transfer learning has become increasingly significant. This is
highlighted by the generative capabilities of NLP models like GPT-4, and the
retrieval-based approaches of CV models like CLIP, both of which effectively
bridge the gap between seen and unseen data. In the realm of graph learning,
the continuous emergence of new graphs and the challenges of human labeling
also amplify the necessity for zero-shot transfer learning, driving the
exploration of approaches that can generalize across diverse graph data without
necessitating dataset-specific and label-specific fine-tuning. In this study,
we extend such paradigms to zero-shot transferability in graphs by introducing
ZeroG, a new framework tailored to enable cross-dataset generalization.
Addressing the inherent challenges such as feature misalignment, mismatched
label spaces, and negative transfer, we leverage a language model to encode
both node attributes and class semantics, ensuring consistent feature
dimensions across datasets. We also propose a prompt-based subgraph sampling
module that enriches the semantic information and structure information of
extracted subgraphs using prompting nodes and neighborhood aggregation,
respectively. We further adopt a lightweight fine-tuning strategy that reduces
the risk of overfitting and maintains the zero-shot learning efficacy of the
language model. The results underscore the effectiveness of our model in
achieving significant cross-dataset zero-shot transferability, opening pathways
for the development of graph foundation models. Especially, ZeroG, as a
zero-shot method, can even achieve results comparable to those of
semi-supervised learning on Pubmed.</div><div><a href='http://arxiv.org/abs/2402.11235v1'>2402.11235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13834v1")'>Few-shot Learning on Heterogeneous Graphs: Challenges, Progress, and
  Prospects</div>
<div id='2403.13834v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T00:43:36Z</div><div>Authors: Pengfei Ding, Yan Wang, Guanfeng Liu</div><div style='padding-top: 10px; width: 80ex'>Few-shot learning on heterogeneous graphs (FLHG) is attracting more attention
from both academia and industry because prevailing studies on heterogeneous
graphs often suffer from label sparsity. FLHG aims to tackle the performance
degradation in the face of limited annotated data and there have been numerous
recent studies proposing various methods and applications. In this paper, we
provide a comprehensive review of existing FLHG methods, covering challenges,
research progress, and future prospects. Specifically, we first formalize FLHG
and categorize its methods into three types: single-heterogeneity FLHG,
dual-heterogeneity FLHG, and multi-heterogeneity FLHG. Then, we analyze the
research progress within each category, highlighting the most recent and
representative developments. Finally, we identify and discuss promising
directions for future research in FLHG. To the best of our knowledge, this
paper is the first systematic and comprehensive review of FLHG.</div><div><a href='http://arxiv.org/abs/2403.13834v1'>2403.13834v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01440v3")'>Few-Shot Learning on Graphs: from Meta-learning to Pre-training and
  Prompting</div>
<div id='2402.01440v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:32:42Z</div><div>Authors: Xingtong Yu, Yuan Fang, Zemin Liu, Yuxia Wu, Zhihao Wen, Jianyuan Bo, Xinming Zhang, Steven C. H. Hoi</div><div style='padding-top: 10px; width: 80ex'>Graph representation learning, a critical step in graph-centric tasks, has
seen significant advancements. Earlier techniques often operate in an
end-to-end setting, where performance heavily relies on the availability of
ample labeled data. This constraint has spurred the emergence of few-shot
learning on graphs, where only a few task-specific labels are available for
each task. Given the extensive literature in this field, this survey endeavors
to synthesize recent developments, provide comparative insights, and identify
future directions. We systematically categorize existing studies into three
major families: meta-learning approaches, pre-training approaches, and hybrid
approaches, with a finer-grained classification in each family to aid readers
in their method selection process. Within each category, we analyze the
relationships among these methods and compare their strengths and limitations.
Finally, we outline prospective future directions for few-shot learning on
graphs to catalyze continued innovation in this field.</div><div><a href='http://arxiv.org/abs/2402.01440v3'>2402.01440v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08743v1")'>ADS: Approximate Densest Subgraph for Novel Image Discovery</div>
<div id='2402.08743v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:27:34Z</div><div>Authors: Shanfeng Hu</div><div style='padding-top: 10px; width: 80ex'>The volume of image repositories continues to grow. Despite the availability
of content-based addressing, we still lack a lightweight tool that allows us to
discover images of distinct characteristics from a large collection. In this
paper, we propose a fast and training-free algorithm for novel image discovery.
The key of our algorithm is formulating a collection of images as a perceptual
distance-weighted graph, within which our task is to locate the K-densest
subgraph that corresponds to a subset of the most unique images. While solving
this problem is not just NP-hard but also requires a full computation of the
potentially huge distance matrix, we propose to relax it into a K-sparse
eigenvector problem that we can efficiently solve using stochastic gradient
descent (SGD) without explicitly computing the distance matrix. We compare our
algorithm against state-of-the-arts on both synthetic and real datasets,
showing that it is considerably faster to run with a smaller memory footprint
while able to mine novel images more accurately.</div><div><a href='http://arxiv.org/abs/2402.08743v1'>2402.08743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00904v1")'>Graph Domain Adaptation: Challenges, Progress and Prospects</div>
<div id='2402.00904v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T02:44:32Z</div><div>Authors: Boshen Shi, Yongqing Wang, Fangda Guo, Bingbing Xu, Huawei Shen, Xueqi Cheng</div><div style='padding-top: 10px; width: 80ex'>As graph representation learning often suffers from label scarcity problems
in real-world applications, researchers have proposed graph domain adaptation
(GDA) as an effective knowledge-transfer paradigm across graphs. In particular,
to enhance model performance on target graphs with specific tasks, GDA
introduces a bunch of task-related graphs as source graphs and adapts the
knowledge learnt from source graphs to the target graphs. Since GDA combines
the advantages of graph representation learning and domain adaptation, it has
become a promising direction of transfer learning on graphs and has attracted
an increasing amount of research interest in recent years. In this paper, we
comprehensively overview the studies of GDA and present a detailed survey of
recent advances. Specifically, we outline the research status and challenges,
propose a taxonomy, introduce the details of representative works, and discuss
the prospects. To the best of our knowledge, this paper is the first survey for
graph domain adaptation. A detailed paper list is available at
https://github.com/Skyorca/Awesome-Graph-Domain-Adaptation-Papers.</div><div><a href='http://arxiv.org/abs/2402.00904v1'>2402.00904v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10231v1")'>Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge
  Graphs</div>
<div id='2403.10231v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T12:00:12Z</div><div>Authors: Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Quanming Yao, Bo Han</div><div style='padding-top: 10px; width: 80ex'>To deduce new facts on a knowledge graph (KG), a link predictor learns from
the graph structure and collects local evidence to find the answer to a given
query. However, existing methods suffer from a severe scalability problem due
to the utilization of the whole KG for prediction, which hinders their promise
on large scale KGs and cannot be directly addressed by vanilla sampling
methods. In this work, we propose the one-shot-subgraph link prediction to
achieve efficient and adaptive prediction. The design principle is that,
instead of directly acting on the whole KG, the prediction procedure is
decoupled into two steps, i.e., (i) extracting only one subgraph according to
the query and (ii) predicting on this single, query dependent subgraph. We
reveal that the non-parametric and computation-efficient heuristics
Personalized PageRank (PPR) can effectively identify the potential answers and
supporting evidence. With efficient subgraph-based prediction, we further
introduce the automated searching of the optimal configurations in both data
and model spaces. Empirically, we achieve promoted efficiency and leading
performances on five large-scale benchmarks. The code is publicly available at:
https://github.com/tmlr-group/one-shot-subgraph.</div><div><a href='http://arxiv.org/abs/2403.10231v1'>2403.10231v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17791v1")'>Label Informed Contrastive Pretraining for Node Importance Estimation on
  Knowledge Graphs</div>
<div id='2402.17791v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T12:28:51Z</div><div>Authors: Tianyu Zhang, Chengbin Hou, Rui Jiang, Xuegong Zhang, Chenghu Zhou, Ke Tang, Hairong Lv</div><div style='padding-top: 10px; width: 80ex'>Node Importance Estimation (NIE) is a task of inferring importance scores of
the nodes in a graph. Due to the availability of richer data and knowledge,
recent research interests of NIE have been dedicating to knowledge graphs for
predicting future or missing node importance scores. Existing state-of-the-art
NIE methods train the model by available labels, and they consider every
interested node equally before training. However, the nodes with higher
importance often require or receive more attention in real-world scenarios,
e.g., people may care more about the movies or webpages with higher importance.
To this end, we introduce Label Informed ContrAstive Pretraining (LICAP) to the
NIE problem for being better aware of the nodes with high importance scores.
Specifically, LICAP is a novel type of contrastive learning framework that aims
to fully utilize the continuous labels to generate contrastive samples for
pretraining embeddings. Considering the NIE problem, LICAP adopts a novel
sampling strategy called top nodes preferred hierarchical sampling to first
group all interested nodes into a top bin and a non-top bin based on node
importance scores, and then divide the nodes within top bin into several finer
bins also based on the scores. The contrastive samples are generated from those
bins, and are then used to pretrain node embeddings of knowledge graphs via a
newly proposed Predicate-aware Graph Attention Networks (PreGAT), so as to
better separate the top nodes from non-top nodes, and distinguish the top nodes
within top bin by keeping the relative order among finer bins. Extensive
experiments demonstrate that the LICAP pretrained embeddings can further boost
the performance of existing NIE methods and achieve the new state-of-the-art
performance regarding both regression and ranking metrics. The source code for
reproducibility is available at https://github.com/zhangtia16/LICAP</div><div><a href='http://arxiv.org/abs/2402.17791v1'>2402.17791v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03583v2")'>MQuinE: a cure for "Z-paradox" in knowledge graph embedding models</div>
<div id='2402.03583v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T23:20:05Z</div><div>Authors: Yang Liu, Huang Fang, Yunfeng Cai, Mingming Sun</div><div style='padding-top: 10px; width: 80ex'>Knowledge graph embedding (KGE) models achieved state-of-the-art results on
many knowledge graph tasks including link prediction and information retrieval.
Despite the superior performance of KGE models in practice, we discover a
deficiency in the expressiveness of some popular existing KGE models called
\emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE
model called \emph{MQuinE} that does not suffer from Z-paradox while preserves
strong expressiveness to model various relation patterns including
symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with
theoretical justification. Experiments on real-world knowledge bases indicate
that Z-paradox indeed degrades the performance of existing KGE models, and can
cause more than 20\% accuracy drop on some challenging test samples. Our
experiments further demonstrate that MQuinE can mitigate the negative impact of
Z-paradox and outperform existing KGE models by a visible margin on link
prediction tasks.</div><div><a href='http://arxiv.org/abs/2402.03583v2'>2402.03583v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13749v1")'>Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph
  Representational Learning</div>
<div id='2403.13749v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T16:58:28Z</div><div>Authors: Raffaele Paolino, Sohir Maskey, Pascal Welke, Gitta Kutyniok</div><div style='padding-top: 10px; width: 80ex'>We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchy
of graph isomorphism tests and a corresponding GNN framework, $r$-$\ell{}$MPNN,
that can count cycles up to length $r + 2$. Most notably, we show that
$r$-$\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends
classical 1-WL, which can only count homomorphisms of trees and, in fact, is
incomparable to $k$-WL for any fixed $k$. We empirically validate the
expressive and counting power of the proposed $r$-$\ell{}$MPNN on several
synthetic datasets and present state-of-the-art predictive performance on
various real-world datasets. The code is available at
https://github.com/RPaolino/loopy</div><div><a href='http://arxiv.org/abs/2403.13749v1'>2403.13749v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04923v1")'>Control-based Graph Embeddings with Data Augmentation for Contrastive
  Learning</div>
<div id='2403.04923v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T22:14:04Z</div><div>Authors: Obaid Ullah Ahmad, Anwar Said, Mudassir Shabbir, Waseem Abbas, Xenofon Koutsoukos</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the problem of unsupervised graph representation
learning by harnessing the control properties of dynamical networks defined on
graphs. Our approach introduces a novel framework for contrastive learning, a
widely prevalent technique for unsupervised representation learning. A crucial
step in contrastive learning is the creation of 'augmented' graphs from the
input graphs. Though different from the original graphs, these augmented graphs
retain the original graph's structural characteristics. Here, we propose a
unique method for generating these augmented graphs by leveraging the control
properties of networks. The core concept revolves around perturbing the
original graph to create a new one while preserving the controllability
properties specific to networks and graphs. Compared to the existing methods,
we demonstrate that this innovative approach enhances the effectiveness of
contrastive learning frameworks, leading to superior results regarding the
accuracy of the classification tasks. The key innovation lies in our ability to
decode the network structure using these control properties, opening new
avenues for unsupervised graph representation learning.</div><div><a href='http://arxiv.org/abs/2403.04923v1'>2403.04923v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01614v1")'>L2G2G: a Scalable Local-to-Global Network Embedding with Graph
  Autoencoders</div>
<div id='2402.01614v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:24:37Z</div><div>Authors: Ruikang Ouyang, Andrew Elliott, Stratis Limnios, Mihai Cucuringu, Gesine Reinert</div><div style='padding-top: 10px; width: 80ex'>For analysing real-world networks, graph representation learning is a popular
tool. These methods, such as a graph autoencoder (GAE), typically rely on
low-dimensional representations, also called embeddings, which are obtained
through minimising a loss function; these embeddings are used with a decoder
for downstream tasks such as node classification and edge prediction. While
GAEs tend to be fairly accurate, they suffer from scalability issues. For
improved speed, a Local2Global approach, which combines graph patch embeddings
based on eigenvector synchronisation, was shown to be fast and achieve good
accuracy. Here we propose L2G2G, a Local2Global method which improves GAE
accuracy without sacrificing scalability. This improvement is achieved by
dynamically synchronising the latent node representations, while training the
GAEs. It also benefits from the decoder computing an only local patch loss.
Hence, aligning the local embeddings in each epoch utilises more information
from the graph than a single post-training alignment does, while maintaining
scalability. We illustrate on synthetic benchmarks, as well as real-world
examples, that L2G2G achieves higher accuracy than the standard Local2Global
approach and scales efficiently on the larger data sets. We find that for large
and dense networks, it even outperforms the slow, but assumed more accurate,
GAEs.</div><div><a href='http://arxiv.org/abs/2402.01614v1'>2402.01614v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11339v1")'>Expressive Higher-Order Link Prediction through Hypergraph Symmetry
  Breaking</div>
<div id='2402.11339v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T17:13:41Z</div><div>Authors: Simon Zhang, Cheng Xin, Tamal K. Dey</div><div style='padding-top: 10px; width: 80ex'>A hypergraph consists of a set of nodes along with a collection of subsets of
the nodes called hyperedges. Higher-order link prediction is the task of
predicting the existence of a missing hyperedge in a hypergraph. A hyperedge
representation learned for higher order link prediction is fully expressive
when it does not lose distinguishing power up to an isomorphism. Many existing
hypergraph representation learners, are bounded in expressive power by the
Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the
Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In
fact, induced subhypergraphs with identical GWL-1 valued nodes are
indistinguishable. Furthermore, message passing on hypergraphs can already be
computationally expensive, especially on GPU memory. To address these
limitations, we devise a preprocessing algorithm that can identify certain
regular subhypergraphs exhibiting symmetry. Our preprocessing algorithm runs
once with complexity the size of the input hypergraph. During training, we
randomly replace subhypergraphs identified by the algorithm with covering
hyperedges to break symmetry. We show that our method improves the expressivity
of GWL-1. Our extensive experiments also demonstrate the effectiveness of our
approach for higher-order link prediction on both graph and hypergraph datasets
with negligible change in computation.</div><div><a href='http://arxiv.org/abs/2402.11339v1'>2402.11339v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08519v1")'>From Graphs to Hypergraphs: Hypergraph Projection and its Remediation</div>
<div id='2401.08519v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:31:54Z</div><div>Authors: Yanbang Wang, Jon Kleinberg</div><div style='padding-top: 10px; width: 80ex'>We study the implications of the modeling choice to use a graph, instead of a
hypergraph, to represent real-world interconnected systems whose constituent
relationships are of higher order by nature. Such a modeling choice typically
involves an underlying projection process that maps the original hypergraph
onto a graph, and is common in graph-based analysis. While hypergraph
projection can potentially lead to loss of higher-order relations, there exists
very limited studies on the consequences of doing so, as well as its
remediation. This work fills this gap by doing two things: (1) we develop
analysis based on graph and set theory, showing two ubiquitous patterns of
hyperedges that are root to structural information loss in all hypergraph
projections; we also quantify the combinatorial impossibility of recovering the
lost higher-order structures if no extra help is provided; (2) we still seek to
recover the lost higher-order structures in hypergraph projection, and in light
of (1)'s findings we propose to relax the problem into a learning-based
setting. Under this setting, we develop a learning-based hypergraph
reconstruction method based on an important statistic of hyperedge
distributions that we find. Our reconstruction method is evaluated on 8
real-world datasets under different settings, and exhibits consistently good
performance. We also demonstrate benefits of the reconstructed hypergraphs via
use cases of protein rankings and link predictions.</div><div><a href='http://arxiv.org/abs/2401.08519v1'>2401.08519v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13054v1")'>Frustrated Random Walks: A Fast Method to Compute Node Distances on
  Hypergraphs</div>
<div id='2401.13054v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T19:26:24Z</div><div>Authors: Enzhi Li, Bilal Fadlallah</div><div style='padding-top: 10px; width: 80ex'>A hypergraph is a generalization of a graph that arises naturally when
attribute-sharing among entities is considered. Although a hypergraph can be
converted into a graph by expanding its hyperedges into fully connected
subgraphs, going the reverse way is computationally complex and NP-complete. We
therefore hypothesize that a hypergraph contains more information than a graph.
In addition, it is more convenient to manipulate a hypergraph directly, rather
than expand it into a graph. An open problem in hypergraphs is how to
accurately and efficiently calculate their node distances. Estimating node
distances enables us to find a node's nearest neighbors, and perform label
propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this
paper, we propose a novel approach based on random walks to achieve label
propagation on hypergraphs. We estimate node distances as the expected hitting
times of random walks. We note that simple random walks (SRW) cannot accurately
describe highly complex real-world hypergraphs, which motivates us to introduce
frustrated random walks (FRW) to better describe them. We further benchmark our
method against DeepWalk, and show that while the latter can achieve comparable
results, FRW has a distinct computational advantage in cases where the number
of targets is fairly small. For such cases, we show that FRW runs in
significantly shorter time than DeepWalk. Finally, we analyze the time
complexity of our method, and show that for large and sparse hypergraphs, the
complexity is approximately linear, rendering it superior to the DeepWalk
alternative.</div><div><a href='http://arxiv.org/abs/2401.13054v1'>2401.13054v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.00688v1")'>Inferring community structure in attributed hypergraphs using stochastic
  block models</div>
<div id='2401.00688v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T07:31:32Z</div><div>Authors: Kazuki Nakajima, Takeaki Uno</div><div style='padding-top: 10px; width: 80ex'>Hypergraphs are a representation of complex systems involving interactions
among more than two entities and allow to investigation of higher-order
structure and dynamics in real-world complex systems. Community structure is a
common property observed in empirical networks in various domains. Stochastic
block models have been employed to investigate community structure in networks.
Node attribute data, often accompanying network data, has been found to
potentially enhance the learning of community structure in dyadic networks. In
this study, we develop a statistical framework that incorporates node attribute
data into the learning of community structure in a hypergraph, employing a
stochastic block model. We demonstrate that our model, which we refer to as
HyperNEO, enhances the learning of community structure in synthetic and
empirical hypergraphs when node attributes are sufficiently associated with the
communities. Furthermore, we found that applying a dimensionality reduction
method, UMAP, to the learned representations obtained using stochastic block
models, including our model, maps nodes into a two-dimensional vector space
while largely preserving community structure in empirical hypergraphs. We
expect that our framework will broaden the investigation and understanding of
higher-order community structure in real-world complex systems.</div><div><a href='http://arxiv.org/abs/2401.00688v1'>2401.00688v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12719v1")'>Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis</div>
<div id='2403.12719v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T13:28:03Z</div><div>Authors: Angelica I. Aviles-Rivero, Chun-Wun Cheng, Zhongying Deng, Zoe Kourtzi, Carola-Bibiane Schönlieb</div><div style='padding-top: 10px; width: 80ex'>Early detection of Alzheimer's disease's precursor stages is imperative for
significantly enhancing patient outcomes and quality of life. This challenge is
tackled through a semi-supervised multi-modal diagnosis framework. In
particular, we introduce a new hypergraph framework that enables higher-order
relations between multi-modal data, while utilising minimal labels. We first
introduce a bilevel hypergraph optimisation framework that jointly learns a
graph augmentation policy and a semi-supervised classifier. This dual learning
strategy is hypothesised to enhance the robustness and generalisation
capabilities of the model by fostering new pathways for information
propagation. Secondly, we introduce a novel strategy for generating
pseudo-labels more effectively via a gradient-driven flow. Our experimental
results demonstrate the superior performance of our framework over current
techniques in diagnosing Alzheimer's disease.</div><div><a href='http://arxiv.org/abs/2403.12719v1'>2403.12719v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09603v1")'>Scalable Graph Self-Supervised Learning</div>
<div id='2402.09603v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T22:23:35Z</div><div>Authors: Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Raika Karimi, Ali Ghodsi</div><div style='padding-top: 10px; width: 80ex'>In regularization Self-Supervised Learning (SSL) methods for graphs,
computational complexity increases with the number of nodes in graphs and
embedding dimensions. To mitigate the scalability of non-contrastive graph SSL,
we propose a novel approach to reduce the cost of computing the covariance
matrix for the pre-training loss function with volume-maximization terms. Our
work focuses on reducing the cost associated with the loss computation via
graph node or dimension sampling. We provide theoretical insight into why
dimension sampling would result in accurate loss computations and support it
with mathematical derivation of the novel approach. We develop our experimental
setup on the node-level graph prediction tasks, where SSL pre-training has
shown to be difficult due to the large size of real world graphs. Our
experiments demonstrate that the cost associated with the loss computation can
be reduced via node or dimension sampling without lowering the downstream
performance. Our results demonstrate that sampling mostly results in improved
downstream performance. Ablation studies and experimental analysis are provided
to untangle the role of the different factors in the experimental setup.</div><div><a href='http://arxiv.org/abs/2402.09603v1'>2402.09603v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17073v1")'>One-Shot Graph Representation Learning Using Hyperdimensional Computing</div>
<div id='2402.17073v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T23:15:01Z</div><div>Authors: Abhishek Dalvi, Vasant Honavar</div><div style='padding-top: 10px; width: 80ex'>We present a novel, simple, fast, and efficient approach for semi-supervised
learning on graphs. The proposed approach takes advantage of hyper-dimensional
computing which encodes data samples using random projections into a high
dimensional space (HD space for short). Specifically, we propose a
Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the
injectivity property of the node representations of a family of graph neural
networks. HDGL maps node features to the HD space and then uses HD operators
such as bundling and binding to aggregate information from the local
neighborhood of each node. Results of experiments with widely used benchmark
data sets show that HDGL achieves predictive performance that is competitive
with the state-of-the-art deep learning methods, without the need for
computationally expensive training.</div><div><a href='http://arxiv.org/abs/2402.17073v1'>2402.17073v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15587v1")'>Hyperedge Interaction-aware Hypergraph Neural Network</div>
<div id='2401.15587v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T07:05:30Z</div><div>Authors: Xiaobing Pei, Rongping Ye, Haoran Yang, Ruiqi Wang</div><div style='padding-top: 10px; width: 80ex'>Hypergraphs provide an effective modeling approach for modeling high-order
relationships in many real-world datasets. To capture such complex
relationships, several hypergraph neural networks have been proposed for
learning hypergraph structure, which propagate information from nodes to
hyperedges and then from hyperedges back to nodes. However, most existing
methods focus on information propagation between hyperedges and nodes,
neglecting the interactions among hyperedges themselves. In this paper, we
propose HeIHNN, a hyperedge interaction-aware hypergraph neural network, which
captures the interactions among hyperedges during the convolution process and
introduce a novel mechanism to enhance information flow between hyperedges and
nodes. Specifically, HeIHNN integrates the interactions between hyperedges into
the hypergraph convolution by constructing a three-stage information
propagation process. After propagating information from nodes to hyperedges, we
introduce a hyperedge-level convolution to update the hyperedge embeddings.
Finally, the embeddings that capture rich information from the interaction
among hyperedges will be utilized to update the node embeddings. Additionally,
we introduce a hyperedge outlier removal mechanism in the information
propagation stages between nodes and hyperedges, which dynamically adjusts the
hypergraph structure using the learned embeddings, effectively removing
outliers. Extensive experiments conducted on real-world datasets show the
competitive performance of HeIHNN compared with state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2401.15587v1'>2401.15587v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07191v1")'>GSINA: Improving Subgraph Extraction for Graph Invariant Learning via
  Graph Sinkhorn Attention</div>
<div id='2402.07191v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T12:57:16Z</div><div>Authors: Fangyu Ding, Haiyang Wang, Zhixuan Chu, Tianming Li, Zhaoping Hu, Junchi Yan</div><div style='padding-top: 10px; width: 80ex'>Graph invariant learning (GIL) has been an effective approach to discovering
the invariant relationships between graph data and its labels for different
graph learning tasks under various distribution shifts. Many recent endeavors
of GIL focus on extracting the invariant subgraph from the input graph for
prediction as a regularization strategy to improve the generalization
performance of graph learning. Despite their success, such methods also have
various limitations in obtaining their invariant subgraphs. In this paper, we
provide in-depth analyses of the drawbacks of existing works and propose
corresponding principles of our invariant subgraph extraction: 1) the sparsity,
to filter out the variant features, 2) the softness, for a broader solution
space, and 3) the differentiability, for a soundly end-to-end optimization. To
meet these principles in one shot, we leverage the Optimal Transport (OT)
theory and propose a novel graph attention mechanism called Graph Sinkhorn
Attention (GSINA). This novel approach serves as a powerful regularization
method for GIL tasks. By GSINA, we are able to obtain meaningful,
differentiable invariant subgraphs with controllable sparsity and softness.
Moreover, GSINA is a general graph learning framework that could handle GIL
tasks of multiple data grain levels. Extensive experiments on both synthetic
and real-world datasets validate the superiority of our GSINA, which
outperforms the state-of-the-art GIL methods by large margins on both
graph-level tasks and node-level tasks. Our code is publicly available at
\url{https://github.com/dingfangyu/GSINA}.</div><div><a href='http://arxiv.org/abs/2402.07191v1'>2402.07191v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01261v3")'>TEDDY: Trimming Edges with Degree-based Discrimination strategY</div>
<div id='2402.01261v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:32:03Z</div><div>Authors: Hyunjin Seo, Jihun Yun, Eunho Yang</div><div style='padding-top: 10px; width: 80ex'>Since the pioneering work on the lottery ticket hypothesis for graph neural
networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph
lottery tickets (GLT) has become one of the pivotal focus in the GNN community,
inspiring researchers to discover sparser GLT while achieving comparable
performance to original dense networks. In parallel, the graph structure has
gained substantial attention as a crucial factor in GNN training dynamics, also
elucidated by several recent studies. Despite this, contemporary studies on
GLT, in general, have not fully exploited inherent pathways in the graph
structure and identified tickets in an iterative manner, which is
time-consuming and inefficient. To address these limitations, we introduce
TEDDY, a one-shot edge sparsification framework that leverages structural
information by incorporating edge-degree information. Following edge
sparsification, we encourage the parameter sparsity during training via simple
projected gradient descent on the $\ell_0$ ball. Given the target sparsity
levels for both the graph structure and the model parameters, our TEDDY
facilitates efficient and rapid realization of GLT within a single training.
Remarkably, our experimental results demonstrate that TEDDY significantly
surpasses conventional iterative approaches in generalization, even when
conducting one-shot sparsification that solely utilizes graph structures,
without taking feature information into account.</div><div><a href='http://arxiv.org/abs/2402.01261v3'>2402.01261v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09171v1")'>ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks</div>
<div id='2403.09171v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T08:31:39Z</div><div>Authors: Zhaoliang Chen, Zhihao Wu, Ylli Sadikaj, Claudia Plant, Hong-Ning Dai, Shiping Wang, Wenzhong Guo</div><div style='padding-top: 10px; width: 80ex'>Although Graph Neural Networks (GNNs) have exhibited the powerful ability to
gather graph-structured information from neighborhood nodes via various
message-passing mechanisms, the performance of GNNs is limited by poor
generalization and fragile robustness caused by noisy and redundant graph data.
As a prominent solution, Graph Augmentation Learning (GAL) has recently
received increasing attention. Among prior GAL approaches, edge-dropping
methods that randomly remove edges from a graph during training are effective
techniques to improve the robustness of GNNs. However, randomly dropping edges
often results in bypassing critical edges, consequently weakening the
effectiveness of message passing. In this paper, we propose a novel adversarial
edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor
guiding the removal of edges, which can be flexibly incorporated into diverse
GNN backbones. Employing an adversarial training framework, the edge predictor
utilizes the line graph transformed from the original graph to estimate the
edges to be dropped, which improves the interpretability of the edge-dropping
method. The proposed ADEdgeDrop is optimized alternately by stochastic gradient
descent and projected gradient descent. Comprehensive experiments on six graph
benchmark datasets demonstrate that the proposed ADEdgeDrop outperforms
state-of-the-art baselines across various GNN backbones, demonstrating improved
generalization and robustness.</div><div><a href='http://arxiv.org/abs/2403.09171v1'>2403.09171v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13987v1")'>A Simple and Yet Fairly Effective Defense for Graph Neural Networks</div>
<div id='2402.13987v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:16:48Z</div><div>Authors: Sofiane Ennadir, Yassine Abbahaddou, Johannes F. Lutzeyer, Michalis Vazirgiannis, Henrik Boström</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have emerged as the dominant approach for
machine learning on graph-structured data. However, concerns have arisen
regarding the vulnerability of GNNs to small adversarial perturbations.
Existing defense methods against such perturbations suffer from high time
complexity and can negatively impact the model's performance on clean graphs.
To address these challenges, this paper introduces NoisyGNNs, a novel defense
method that incorporates noise into the underlying model's architecture. We
establish a theoretical connection between noise injection and the enhancement
of GNN robustness, highlighting the effectiveness of our approach. We further
conduct extensive empirical evaluations on the node classification task to
validate our theoretical findings, focusing on two popular GNNs: the GCN and
GIN. The results demonstrate that NoisyGNN achieves superior or comparable
defense performance to existing methods while minimizing added time complexity.
The NoisyGNN approach is model-agnostic, allowing it to be integrated with
different GNN architectures. Successful combinations of our NoisyGNN approach
with existing defense techniques demonstrate even further improved adversarial
defense results. Our code is publicly available at:
https://github.com/Sennadir/NoisyGNN.</div><div><a href='http://arxiv.org/abs/2402.13987v1'>2402.13987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12426v2")'>Attacks on Node Attributes in Graph Neural Networks</div>
<div id='2402.12426v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T17:52:29Z</div><div>Authors: Ying Xu, Michael Lanier, Anindya Sarkar, Yevgeniy Vorobeychik</div><div style='padding-top: 10px; width: 80ex'>Graphs are commonly used to model complex networks prevalent in modern social
media and literacy applications. Our research investigates the vulnerability of
these graphs through the application of feature based adversarial attacks,
focusing on both decision time attacks and poisoning attacks. In contrast to
state of the art models like Net Attack and Meta Attack, which target node
attributes and graph structure, our study specifically targets node attributes.
For our analysis, we utilized the text dataset Hellaswag and graph datasets
Cora and CiteSeer, providing a diverse basis for evaluation. Our findings
indicate that decision time attacks using Projected Gradient Descent (PGD) are
more potent compared to poisoning attacks that employ Mean Node Embeddings and
Graph Contrastive Learning strategies. This provides insights for graph data
security, pinpointing where graph-based models are most vulnerable and thereby
informing the development of stronger defense mechanisms against such attacks.</div><div><a href='http://arxiv.org/abs/2402.12426v2'>2402.12426v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05680v1")'>Use of Graph Neural Networks in Aiding Defensive Cyber Operations</div>
<div id='2401.05680v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T05:56:29Z</div><div>Authors: Shaswata Mitra, Trisha Chakraborty, Subash Neupane, Aritran Piplai, Sudip Mittal</div><div style='padding-top: 10px; width: 80ex'>In an increasingly interconnected world, where information is the lifeblood
of modern society, regular cyber-attacks sabotage the confidentiality,
integrity, and availability of digital systems and information. Additionally,
cyber-attacks differ depending on the objective and evolve rapidly to disguise
defensive systems. However, a typical cyber-attack demonstrates a series of
stages from attack initiation to final resolution, called an attack life cycle.
These diverse characteristics and the relentless evolution of cyber attacks
have led cyber defense to adopt modern approaches like Machine Learning to
bolster defensive measures and break the attack life cycle. Among the adopted
ML approaches, Graph Neural Networks have emerged as a promising approach for
enhancing the effectiveness of defensive measures due to their ability to
process and learn from heterogeneous cyber threat data. In this paper, we look
into the application of GNNs in aiding to break each stage of one of the most
renowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address
each phase of CKC and discuss how GNNs contribute to preparing and preventing
an attack from a defensive standpoint. Furthermore, We also discuss open
research areas and further improvement scopes.</div><div><a href='http://arxiv.org/abs/2401.05680v1'>2401.05680v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07878v1")'>Using Graph Theory for Improving Machine Learning-based Detection of
  Cyber Attacks</div>
<div id='2402.07878v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T18:44:02Z</div><div>Authors: Giacomo Zonneveld, Lorenzo Principi, Marco Baldi</div><div style='padding-top: 10px; width: 80ex'>Early detection of network intrusions and cyber threats is one of the main
pillars of cybersecurity. One of the most effective approaches for this purpose
is to analyze network traffic with the help of artificial intelligence
algorithms, with the aim of detecting the possible presence of an attacker by
distinguishing it from a legitimate user. This is commonly done by collecting
the traffic exchanged between terminals in a network and analyzing it on a
per-packet or per-connection basis. In this paper, we propose instead to
perform pre-processing of network traffic under analysis with the aim of
extracting some new metrics on which we can perform more efficient detection
and overcome some limitations of classical approaches. These new metrics are
based on graph theory, and consider the network as a whole, rather than
focusing on individual packets or connections. Our approach is validated
through experiments performed on publicly available data sets, from which it
results that it can not only overcome some of the limitations of classical
approaches, but also achieve a better detection capability of cyber threats.</div><div><a href='http://arxiv.org/abs/2402.07878v1'>2402.07878v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07480v2")'>Topological safeguard for evasion attack interpreting the neural
  networks' behavior</div>
<div id='2402.07480v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T08:39:40Z</div><div>Authors: Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Iñigo Mendialdua, Raul Orduna-Urrutia</div><div style='padding-top: 10px; width: 80ex'>In the last years, Deep Learning technology has been proposed in different
fields, bringing many advances in each of them, but identifying new threats in
these solutions regarding cybersecurity. Those implemented models have brought
several vulnerabilities associated with Deep Learning technology. Moreover,
those allow taking advantage of the implemented model, obtaining private
information, and even modifying the model's decision-making. Therefore,
interest in studying those vulnerabilities/attacks and designing defenses to
avoid or fight them is gaining prominence among researchers. In particular, the
widely known evasion attack is being analyzed by researchers; thus, several
defenses to avoid such a threat can be found in the literature. Since the
presentation of the L-BFG algorithm, this threat concerns the research
community. However, it continues developing new and ingenious countermeasures
since there is no perfect defense for all the known evasion algorithms. In this
work, a novel detector of evasion attacks is developed. It focuses on the
information of the activations of the neurons given by the model when an input
sample is injected. Moreover, it puts attention to the topology of the targeted
deep learning model to analyze the activations according to which neurons are
connecting. This approach has been decided because the literature shows that
the targeted model's topology contains essential information about if the
evasion attack occurs. For this purpose, a huge data preprocessing is required
to introduce all this information in the detector, which uses the Graph
Convolutional Neural Network (GCN) technology. Thus, it understands the
topology of the target model, obtaining promising results and improving the
outcomes presented in the literature related to similar defenses.</div><div><a href='http://arxiv.org/abs/2402.07480v2'>2402.07480v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00839v1")'>X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection
  System</div>
<div id='2402.00839v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:29:16Z</div><div>Authors: Kiymet Kaya, Elif Ak, Sumeyye Bas, Berk Canberk, Sule Gunduz Oguducu</div><div style='padding-top: 10px; width: 80ex'>The effectiveness of Intrusion Detection Systems (IDS) is critical in an era
where cyber threats are becoming increasingly complex. Machine learning (ML)
and deep learning (DL) models provide an efficient and accurate solution for
identifying attacks and anomalies in computer networks. However, using ML and
DL models in IDS has led to a trust deficit due to their non-transparent
decision-making. This transparency gap in IDS research is significant,
affecting confidence and accountability. To address, this paper introduces a
novel Explainable IDS approach, called X-CBA, that leverages the structural
advantages of Graph Neural Networks (GNNs) to effectively process network
traffic data, while also adapting a new Explainable AI (XAI) methodology.
Unlike most GNN-based IDS that depend on labeled network traffic and node
features, thereby overlooking critical packet-level information, our approach
leverages a broader range of traffic data through network flows, including edge
attributes, to improve detection capabilities and adapt to novel threats.
Through empirical testing, we establish that our approach not only achieves
high accuracy with 99.47% in threat detection but also advances the field by
providing clear, actionable explanations of its analytical outcomes. This
research also aims to bridge the current gap and facilitate the broader
integration of ML/DL technologies in cybersecurity defenses by offering a local
and global explainability solution that is both precise and interpretable.</div><div><a href='http://arxiv.org/abs/2402.00839v1'>2402.00839v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07496v1")'>Understanding Deep Learning defenses Against Adversarial Examples
  Through Visualizations for Dynamic Risk Assessment</div>
<div id='2402.07496v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T09:05:01Z</div><div>Authors: Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Jon Egana-Zubia, Raul Orduna-Urrutia</div><div style='padding-top: 10px; width: 80ex'>In recent years, Deep Neural Network models have been developed in different
fields, where they have brought many advances. However, they have also started
to be used in tasks where risk is critical. A misdiagnosis of these models can
lead to serious accidents or even death. This concern has led to an interest
among researchers to study possible attacks on these models, discovering a long
list of vulnerabilities, from which every model should be defended. The
adversarial example attack is a widely known attack among researchers, who have
developed several defenses to avoid such a threat. However, these defenses are
as opaque as a deep neural network model, how they work is still unknown. This
is why visualizing how they change the behavior of the target model is
interesting in order to understand more precisely how the performance of the
defended model is being modified. For this work, some defenses, against
adversarial example attack, have been selected in order to visualize the
behavior modification of each of them in the defended model. Adversarial
training, dimensionality reduction and prediction similarity were the selected
defenses, which have been developed using a model composed by convolution
neural network layers and dense neural network layers. In each defense, the
behavior of the original model has been compared with the behavior of the
defended model, representing the target model by a graph in a visualization.</div><div><a href='http://arxiv.org/abs/2402.07496v1'>2402.07496v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09820v2")'>Utilizing Deep Learning for Enhancing Network Resilience in Finance</div>
<div id='2402.09820v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T09:35:57Z</div><div>Authors: Yulu Gong, Mengran Zhu, Shuning Huo, Yafei Xiang, Hanyi Yu</div><div style='padding-top: 10px; width: 80ex'>In the age of the Internet, people's lives are increasingly dependent on
today's network technology. Maintaining network integrity and protecting the
legitimate interests of users is at the heart of network construction. Threat
detection is an important part of a complete and effective defense system. How
to effectively detect unknown threats is one of the concerns of network
protection. Currently, network threat detection is usually based on rules and
traditional machine learning methods, which create artificial rules or extract
common spatiotemporal features, which cannot be applied to large-scale data
applications, and the emergence of unknown risks causes the detection accuracy
of the original model to decline. With this in mind, this paper uses deep
learning for advanced threat detection to improve protective measures in the
financial industry. Many network researchers have shifted their focus to
exception-based intrusion detection techniques. The detection technology mainly
uses statistical machine learning methods - collecting normal program and
network behavior data, extracting multidimensional features, and training
decision machine learning models on this basis (commonly used include naive
Bayes, decision trees, support vector machines, random forests, etc.).</div><div><a href='http://arxiv.org/abs/2402.09820v2'>2402.09820v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15147v1")'>TREC: APT Tactic / Technique Recognition via Few-Shot Provenance
  Subgraph Learning</div>
<div id='2402.15147v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T07:05:32Z</div><div>Authors: Mingqi Lv, HongZhe Gao, Xuebo Qiu, Tieming Chen, Tiantian Zhu</div><div style='padding-top: 10px; width: 80ex'>APT (Advanced Persistent Threat) with the characteristics of persistence,
stealth, and diversity is one of the greatest threats against
cyber-infrastructure. As a countermeasure, existing studies leverage provenance
graphs to capture the complex relations between system entities in a host for
effective APT detection. In addition to detecting single attack events as most
existing work does, understanding the tactics / techniques (e.g., Kill-Chain,
ATT&amp;CK) applied to organize and accomplish the APT attack campaign is more
important for security operations. Existing studies try to manually design a
set of rules to map low-level system events to high-level APT tactics /
techniques. However, the rule based methods are coarse-grained and lack
generalization ability, thus they can only recognize APT tactics and cannot
identify fine-grained APT techniques and mutant APT attacks. In this paper, we
propose TREC, the first attempt to recognize APT tactics / techniques from
provenance graphs by exploiting deep learning techniques. To address the
"needle in a haystack" problem, TREC segments small and compact subgraphs
covering individual APT technique instances from a large provenance graph based
on a malicious node detection model and a subgraph sampling algorithm. To
address the "training sample scarcity" problem, TREC trains the APT tactic /
technique recognition model in a few-shot learning manner by adopting a Siamese
neural network. We evaluate TREC based on a customized dataset collected and
made public by our team. The experiment results show that TREC significantly
outperforms state-of-the-art systems in APT tactic recognition and TREC can
also effectively identify APT techniques.</div><div><a href='http://arxiv.org/abs/2402.15147v1'>2402.15147v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12743v1")'>APT-MMF: An advanced persistent threat actor attribution method based on
  multimodal and multilevel feature fusion</div>
<div id='2402.12743v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T06:19:55Z</div><div>Authors: Nan Xiao, Bo Lang, Ting Wang, Yikai Chen</div><div style='padding-top: 10px; width: 80ex'>Threat actor attribution is a crucial defense strategy for combating advanced
persistent threats (APTs). Cyber threat intelligence (CTI), which involves
analyzing multisource heterogeneous data from APTs, plays an important role in
APT actor attribution. The current attribution methods extract features from
different CTI perspectives and employ machine learning models to classify CTI
reports according to their threat actors. However, these methods usually
extract only one kind of feature and ignore heterogeneous information,
especially the attributes and relations of indicators of compromise (IOCs),
which form the core of CTI. To address these problems, we propose an APT actor
attribution method based on multimodal and multilevel feature fusion (APT-MMF).
First, we leverage a heterogeneous attributed graph to characterize APT reports
and their IOC information. Then, we extract and fuse multimodal features,
including attribute type features, natural language text features and
topological relationship features, to construct comprehensive node
representations. Furthermore, we design multilevel heterogeneous graph
attention networks to learn the deep hidden features of APT report nodes; these
networks integrate IOC type-level, metapath-based neighbor node-level, and
metapath semantic-level attention. Utilizing multisource threat intelligence,
we construct a heterogeneous attributed graph dataset for verification
purposes. The experimental results show that our method not only outperforms
the existing methods but also demonstrates its good interpretability for
attribution analysis tasks.</div><div><a href='http://arxiv.org/abs/2402.12743v1'>2402.12743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07943v1")'>Revisiting Edge Perturbation for Graph Neural Network in Graph Data
  Augmentation and Attack</div>
<div id='2403.07943v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T15:50:04Z</div><div>Authors: Xin Liu, Yuxiang Zhang, Meng Wu, Mingyu Yan, Kun He, Wei Yan, Shirui Pan, Xiaochun Ye, Dongrui Fan</div><div style='padding-top: 10px; width: 80ex'>Edge perturbation is a basic method to modify graph structures. It can be
categorized into two veins based on their effects on the performance of graph
neural networks (GNNs), i.e., graph data augmentation and attack. Surprisingly,
both veins of edge perturbation methods employ the same operations, yet yield
opposite effects on GNNs' accuracy. A distinct boundary between these methods
in using edge perturbation has never been clearly defined. Consequently,
inappropriate perturbations may lead to undesirable outcomes, necessitating
precise adjustments to achieve desired effects. Therefore, questions of ``why
edge perturbation has a two-faced effect?'' and ``what makes edge perturbation
flexible and effective?'' still remain unanswered.
  In this paper, we will answer these questions by proposing a unified
formulation and establishing a clear boundary between two categories of edge
perturbation methods. Specifically, we conduct experiments to elucidate the
differences and similarities between these methods and theoretically unify the
workflow of these methods by casting it to one optimization problem. Then, we
devise Edge Priority Detector (EPD) to generate a novel priority metric,
bridging these methods up in the workflow. Experiments show that EPD can make
augmentation or attack flexibly and achieve comparable or superior performance
to other counterparts with less time overhead.</div><div><a href='http://arxiv.org/abs/2403.07943v1'>2403.07943v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09901v1")'>Robust Subgraph Learning by Monitoring Early Training Representations</div>
<div id='2403.09901v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T22:25:37Z</div><div>Authors: Sepideh Neshatfar, Salimeh Yasaei Sekeh</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have attracted significant attention for their
outstanding performance in graph learning and node classification tasks.
However, their vulnerability to adversarial attacks, particularly through
susceptible nodes, poses a challenge in decision-making. The need for robust
graph summarization is evident in adversarial challenges resulting from the
propagation of attacks throughout the entire graph. In this paper, we address
both performance and adversarial robustness in graph input by introducing the
novel technique SHERD (Subgraph Learning Hale through Early Training
Representation Distances). SHERD leverages information from layers of a
partially trained graph convolutional network (GCN) to detect susceptible nodes
during adversarial attacks using standard distance metrics. The method
identifies "vulnerable (bad)" nodes and removes such nodes to form a robust
subgraph while maintaining node classification performance. Through our
experiments, we demonstrate the increased performance of SHERD in enhancing
robustness by comparing the network's performance on original and subgraph
inputs against various baselines alongside existing adversarial attacks. Our
experiments across multiple datasets, including citation datasets such as Cora,
Citeseer, and Pubmed, as well as microanatomical tissue structures of cell
graphs in the placenta, highlight that SHERD not only achieves substantial
improvement in robust performance but also outperforms several baselines in
terms of node classification accuracy and computational complexity.</div><div><a href='http://arxiv.org/abs/2403.09901v1'>2403.09901v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02663v1")'>A backdoor attack against link prediction tasks with graph neural
  networks</div>
<div id='2401.02663v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T06:45:48Z</div><div>Authors: Jiazhu Dai, Haoyu Sun</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) are a class of deep learning models capable of
processing graph-structured data, and they have demonstrated significant
performance in a variety of real-world applications. Recent studies have found
that GNN models are vulnerable to backdoor attacks. When specific patterns
(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input
data, the backdoor embedded in the GNN models is activated, which misclassifies
the input data into the target class label specified by the attacker, whereas
when there are no backdoor triggers in the input, the backdoor embedded in the
GNN models is not activated, and the models work normally. Backdoor attacks are
highly stealthy and expose GNN models to serious security risks. Currently,
research on backdoor attacks against GNNs mainly focus on tasks such as graph
classification and node classification, and backdoor attacks against link
prediction tasks are rarely studied. In this paper, we propose a backdoor
attack against the link prediction tasks based on GNNs and reveal the existence
of such security vulnerability in GNN models, which make the backdoored GNN
models to incorrectly predict unlinked two nodes as having a link relationship
when a trigger appear. The method uses a single node as the trigger and poison
selected node pairs in the training graph, and then the backdoor will be
embedded in the GNN models through the training process. In the inference
stage, the backdoor in the GNN models can be activated by simply linking the
trigger node to the two end nodes of the unlinked node pairs in the input data,
causing the GNN models to produce incorrect link prediction results for the
target node pairs.</div><div><a href='http://arxiv.org/abs/2401.02663v1'>2401.02663v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10695v2")'>Unlink to Unlearn: Simplifying Edge Unlearning in GNNs</div>
<div id='2402.10695v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T13:58:23Z</div><div>Authors: Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen</div><div style='padding-top: 10px; width: 80ex'>As concerns over data privacy intensify, unlearning in Graph Neural Networks
(GNNs) has emerged as a prominent research frontier in academia. This concept
is pivotal in enforcing the \textit{right to be forgotten}, which entails the
selective removal of specific data from trained GNNs upon user request. Our
research focuses on edge unlearning, a process of particular relevance to
real-world applications. Current state-of-the-art approaches like GNNDelete can
eliminate the influence of specific edges yet suffer from
\textit{over-forgetting}, which means the unlearning process inadvertently
removes excessive information beyond needed, leading to a significant
performance decline for remaining edges. Our analysis identifies the loss
functions of GNNDelete as the primary source of over-forgetting and also
suggests that loss functions may be redundant for effective edge unlearning.
Building on these insights, we simplify GNNDelete to develop \textbf{Unlink to
Unlearn} (UtU), a novel method that facilitates unlearning exclusively through
unlinking the forget edges from graph structure. Our extensive experiments
demonstrate that UtU delivers privacy protection on par with that of a
retrained model while preserving high accuracy in downstream tasks, by
upholding over 97.3\% of the retrained model's privacy protection capabilities
and 99.8\% of its link prediction accuracy. Meanwhile, UtU requires only
constant computational demands, underscoring its advantage as a highly
lightweight and practical edge unlearning solution.</div><div><a href='http://arxiv.org/abs/2402.10695v2'>2402.10695v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01242v1")'>Two Heads Are Better Than One: Boosting Graph Sparse Training via
  Semantic and Topological Awareness</div>
<div id='2402.01242v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:10:35Z</div><div>Authors: Guibin Zhang, Yanwei Yue, Kun Wang, Junfeng Fang, Yongduo Sui, Kai Wang, Yuxuan Liang, Dawei Cheng, Shirui Pan, Tianlong Chen</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) excel in various graph learning tasks but face
computational challenges when applied to large-scale graphs. A promising
solution is to remove non-essential edges to reduce the computational overheads
in GNN. Previous literature generally falls into two categories:
topology-guided and semantic-guided. The former maintains certain graph
topological properties yet often underperforms on GNNs due to low integration
with neural network training. The latter performs well at lower sparsity on
GNNs but faces performance collapse at higher sparsity levels. With this in
mind, we take the first step to propose a new research line and concept termed
Graph Sparse Training (GST), which dynamically manipulates sparsity at the data
level. Specifically, GST initially constructs a topology &amp; semantic anchor at a
low training cost, followed by performing dynamic sparse training to align the
sparse graph with the anchor. We introduce the Equilibria Sparsification
Principle to guide this process, effectively balancing the preservation of both
topological and semantic information. Ultimately, GST produces a sparse graph
with maximum topological integrity and no performance degradation. Extensive
experiments on 6 datasets and 5 backbones showcase that GST (I) identifies
subgraphs at higher graph sparsity levels (1.67%~15.85% $\uparrow$) than
state-of-the-art sparsification methods, (II) preserves more key spectral
properties, (III) achieves 1.27-3.42$\times$ speedup in GNN inference and (IV)
successfully helps graph adversarial defense and graph lottery tickets.</div><div><a href='http://arxiv.org/abs/2402.01242v1'>2402.01242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00450v2")'>CPT: Competence-progressive Training Strategy for Few-shot Node
  Classification</div>
<div id='2402.00450v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T09:36:56Z</div><div>Authors: Qilong Yan, Yufeng Zhang, Jinghao Zhang, Jingpu Duan, Jian Yin</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have made significant advancements in node
classification, but their success relies on sufficient labeled nodes per class
in the training data. Real-world graph data often exhibits a long-tail
distribution with sparse labels, emphasizing the importance of GNNs' ability in
few-shot node classification, which entails categorizing nodes with limited
data. Traditional episodic meta-learning approaches have shown promise in this
domain, but they face an inherent limitation: it might lead the model to
converge to suboptimal solutions because of random and uniform task assignment,
ignoring task difficulty levels. This could lead the meta-learner to face
complex tasks too soon, hindering proper learning. Ideally, the meta-learner
should start with simple concepts and advance to more complex ones, like human
learning. So, we introduce CPT, a novel two-stage curriculum learning method
that aligns task difficulty with the meta-learner's progressive competence,
enhancing overall performance. Specifically, in CPT's initial stage, the focus
is on simpler tasks, fostering foundational skills for engaging with complex
tasks later. Importantly, the second stage dynamically adjusts task difficulty
based on the meta-learner's growing competence, aiming for optimal knowledge
acquisition. Extensive experiments on popular node classification datasets
demonstrate significant improvements of our strategy over existing methods.</div><div><a href='http://arxiv.org/abs/2402.00450v2'>2402.00450v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09586v1")'>WERank: Towards Rank Degradation Prevention for Self-Supervised Learning
  Using Weight Regularization</div>
<div id='2402.09586v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T21:29:28Z</div><div>Authors: Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Ali Ghodsi</div><div style='padding-top: 10px; width: 80ex'>A common phenomena confining the representation quality in Self-Supervised
Learning (SSL) is dimensional collapse (also known as rank degeneration), where
the learned representations are mapped to a low dimensional subspace of the
representation space. The State-of-the-Art SSL methods have shown to suffer
from dimensional collapse and fall behind maintaining full rank. Recent
approaches to prevent this problem have proposed using contrastive losses,
regularization techniques, or architectural tricks. We propose WERank, a new
regularizer on the weight parameters of the network to prevent rank
degeneration at different layers of the network. We provide empirical evidence
and mathematical justification to demonstrate the effectiveness of the proposed
regularization method in preventing dimensional collapse. We verify the impact
of WERank on graph SSL where dimensional collapse is more pronounced due to the
lack of proper data augmentation. We empirically demonstrate that WERank is
effective in helping BYOL to achieve higher rank during SSL pre-training and
consequently downstream accuracy during evaluation probing. Ablation studies
and experimental analysis shed lights on the underlying factors behind the
performance gains of the proposed approach.</div><div><a href='http://arxiv.org/abs/2402.09586v1'>2402.09586v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.06932v1")'>Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with
  Trainable Attribute</div>
<div id='2402.06932v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T12:10:13Z</div><div>Authors: Tajima Shinji, Ren Sugihara, Ryota Kitahara, Masayuki Karasuyama</div><div style='padding-top: 10px; width: 80ex'>The graph classification problem has been widely studied; however, achieving
an interpretable model with high predictive performance remains a challenging
issue. This paper proposes an interpretable classification algorithm for
attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA
learns importance weights for small attributed subgraphs, called attributed
graphlets (AGs), while simultaneously optimizing their attribute vectors. This
enables us to obtain a combination of subgraph structures and their attribute
vectors that strongly contribute to discriminating different classes. A
significant characteristics of LAGRA is that all the subgraph structures in the
training dataset can be considered as a candidate structures of AGs. This
approach can explore all the potentially important subgraphs exhaustively, but
obviously, a naive implementation can require a large amount of computations.
To mitigate this issue, we propose an efficient pruning strategy by combining
the proximal gradient descent and a graph mining tree search. Our pruning
strategy can ensure that the quality of the solution is maintained compared to
the result without pruning. We empirically demonstrate that LAGRA has superior
or comparable prediction performance to the standard existing algorithms
including graph neural networks, while using only a small number of AGs in an
interpretable manner.</div><div><a href='http://arxiv.org/abs/2402.06932v1'>2402.06932v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07849v1")'>Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining
  of Explanations</div>
<div id='2403.07849v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T17:41:27Z</div><div>Authors: Harish G. Naik, Jan Polster, Raj Shekhar, Tamás Horváth, György Turán</div><div style='padding-top: 10px; width: 80ex'>We formulate an XAI-based model improvement approach for Graph Neural
Networks (GNNs) for node classification, called Explanation Enhanced Graph
Learning (EEGL). The goal is to improve predictive performance of GNN using
explanations. EEGL is an iterative self-improving algorithm, which starts with
a learned "vanilla" GNN, and repeatedly uses frequent subgraph mining to find
relevant patterns in explanation subgraphs. These patterns are then filtered
further to obtain application-dependent features corresponding to the presence
of certain subgraphs in the node neighborhoods. Giving an application-dependent
algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL)
algorithm has previously been posed as an open problem. We present experimental
evidence, with synthetic and real-world data, which show that EEGL outperforms
related approaches in predictive performance and that it has a
node-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's
training dynamics.</div><div><a href='http://arxiv.org/abs/2403.07849v1'>2403.07849v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02086v2")'>View-based Explanations for Graph Neural Networks</div>
<div id='2401.02086v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T06:20:24Z</div><div>Authors: Tingyang Chen, Dazhuo Qiu, Yinghui Wu, Arijit Khan, Xiangyu Ke, Yunjun Gao</div><div style='padding-top: 10px; width: 80ex'>Generating explanations for graph neural networks (GNNs) has been studied to
understand their behavior in analytical tasks such as graph classification.
Existing approaches aim to understand the overall results of GNNs rather than
providing explanations for specific class labels of interest, and may return
explanation structures that are hard to access, nor directly queryable.We
propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1)
We design a two-tier explanation structure called explanation views. An
explanation view consists of a set of graph patterns and a set of induced
explanation subgraphs. Given a database G of multiple graphs and a specific
class label l assigned by a GNN-based classifier M, it concisely describes the
fraction of G that best explains why l is assigned by M. (2) We propose quality
measures and formulate an optimization problem to compute optimal explanation
views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3)
We present two algorithms. The first one follows an explain-and-summarize
strategy that first generates high-quality explanation subgraphs which best
explain GNNs in terms of feature influence maximization, and then performs a
summarization step to generate patterns. We show that this strategy provides an
approximation ratio of 1/2. Our second algorithm performs a single-pass to an
input node stream in batches to incrementally maintain explanation views,
having an anytime quality guarantee of 1/4 approximation. Using real-world
benchmark data, we experimentally demonstrate the effectiveness, efficiency,
and scalability of GVEX. Through case studies, we showcase the practical
applications of GVEX.</div><div><a href='http://arxiv.org/abs/2401.02086v2'>2401.02086v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05763v1")'>HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge
  Graph Reasoning</div>
<div id='2403.05763v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T02:17:43Z</div><div>Authors: Hanning Chen, Yang Ni, Ali Zakeri, Zhuowen Zou, Sanggeon Yun, Fei Wen, Behnam Khaleghi, Narayan Srinivasa, Hugo Latapie, Mohsen Imani</div><div style='padding-top: 10px; width: 80ex'>In recent times, a plethora of hardware accelerators have been put forth for
graph learning applications such as vertex classification and graph
classification. However, previous works have paid little attention to Knowledge
Graph Completion (KGC), a task that is well-known for its significantly higher
algorithm complexity. The state-of-the-art KGC solutions based on graph
convolution neural network (GCN) involve extensive vertex/relation embedding
updates and complicated score functions, which are inherently cumbersome for
acceleration. As a result, existing accelerator designs are no longer optimal,
and a novel algorithm-hardware co-design for KG reasoning is needed.
  Recently, brain-inspired HyperDimensional Computing (HDC) has been introduced
as a promising solution for lightweight machine learning, particularly for
graph learning applications. In this paper, we leverage HDC for an
intrinsically more efficient and acceleration-friendly KGC algorithm. We also
co-design an acceleration framework named HDReason targeting FPGA platforms. On
the algorithm level, HDReason achieves a balance between high reasoning
accuracy, strong model interpretability, and less computation complexity. In
terms of architecture, HDReason offers reconfigurability, high training
throughput, and low energy consumption. When compared with NVIDIA RTX 4090 GPU,
the proposed accelerator achieves an average 10.6x speedup and 65x energy
efficiency improvement. When conducting cross-models and cross-platforms
comparison, HDReason yields an average 4.2x higher performance and 3.4x better
energy efficiency with similar accuracy versus the state-of-the-art FPGA-based
GCN training platform.</div><div><a href='http://arxiv.org/abs/2403.05763v1'>2403.05763v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14367v1")'>Representation Learning for Frequent Subgraph Mining</div>
<div id='2402.14367v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T08:11:22Z</div><div>Authors: Rex Ying, Tianyu Fu, Andrew Wang, Jiaxuan You, Yu Wang, Jure Leskovec</div><div style='padding-top: 10px; width: 80ex'>Identifying frequent subgraphs, also called network motifs, is crucial in
analyzing and predicting properties of real-world networks. However, finding
large commonly-occurring motifs remains a challenging problem not only due to
its NP-hard subroutine of subgraph counting, but also the exponential growth of
the number of possible subgraphs patterns. Here we present Subgraph Pattern
Miner (SPMiner), a novel neural approach for approximately finding frequent
subgraphs in a large target graph. SPMiner combines graph neural networks,
order embedding space, and an efficient search strategy to identify network
subgraph patterns that appear most frequently in the target graph. SPMiner
first decomposes the target graph into many overlapping subgraphs and then
encodes each subgraph into an order embedding space. SPMiner then uses a
monotonic walk in the order embedding space to identify frequent motifs.
Compared to existing approaches and possible neural alternatives, SPMiner is
more accurate, faster, and more scalable. For 5- and 6-node motifs, we show
that SPMiner can almost perfectly identify the most frequent motifs while being
100x faster than exact enumeration methods. In addition, SPMiner can also
reliably identify frequent 10-node motifs, which is well beyond the size limit
of exact enumeration approaches. And last, we show that SPMiner can find large
up to 20 node motifs with 10-100x higher frequency than those found by current
approximate methods.</div><div><a href='http://arxiv.org/abs/2402.14367v1'>2402.14367v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11449v1")'>Graph Partial Label Learning with Potential Cause Discovering</div>
<div id='2403.11449v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T03:56:34Z</div><div>Authors: Hang Gao, Jiaguo Yuan, Jiangmeng Li, Chengyu Yao, Fengge Wu, Junsuo Zhao, Changwen Zheng</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have gained considerable attention for their
potential in addressing challenges posed by complex graph-structured data in
diverse domains. However, accurately annotating graph data for training is
difficult due to the inherent complexity and interconnectedness of graphs. To
tackle this issue, we propose a novel graph representation learning method that
enables GNN models to effectively learn discriminative information even in the
presence of noisy labels within the context of Partially Labeled Learning
(PLL). PLL is a critical weakly supervised learning problem, where each
training instance is associated with a set of candidate labels, including both
the true label and additional noisy labels. Our approach leverages potential
cause extraction to obtain graph data that exhibit a higher likelihood of
possessing a causal relationship with the labels. By incorporating auxiliary
training based on the extracted graph data, our model can effectively filter
out the noise contained in the labels. We support the rationale behind our
approach with a series of theoretical analyses. Moreover, we conduct extensive
evaluations and ablation studies on multiple datasets, demonstrating the
superiority of our proposed method.</div><div><a href='http://arxiv.org/abs/2403.11449v1'>2403.11449v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05064v1")'>Unsupervised Graph Neural Architecture Search with Disentangled
  Self-supervision</div>
<div id='2403.05064v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T05:23:55Z</div><div>Authors: Zeyang Zhang, Xin Wang, Ziwei Zhang, Guangyao Shen, Shiqi Shen, Wenwu Zhu</div><div style='padding-top: 10px; width: 80ex'>The existing graph neural architecture search (GNAS) methods heavily rely on
supervised labels during the search process, failing to handle ubiquitous
scenarios where supervisions are not available. In this paper, we study the
problem of unsupervised graph neural architecture search, which remains
unexplored in the literature. The key problem is to discover the latent graph
factors that drive the formation of graph data as well as the underlying
relations between the factors and the optimal neural architectures. Handling
this problem is challenging given that the latent graph factors together with
architectures are highly entangled due to the nature of the graph and the
complexity of the neural architecture search process. To address the challenge,
we propose a novel Disentangled Self-supervised Graph Neural Architecture
Search (DSGAS) model, which is able to discover the optimal architectures
capturing various latent graph factors in a self-supervised fashion based on
unlabeled graph data. Specifically, we first design a disentangled graph
super-network capable of incorporating multiple architectures with factor-wise
disentanglement, which are optimized simultaneously. Then, we estimate the
performance of architectures under different factors by our proposed
self-supervised training with joint architecture-graph disentanglement.
Finally, we propose a contrastive search with architecture augmentations to
discover architectures with factor-specific expertise. Extensive experiments on
11 real-world datasets demonstrate that the proposed model is able to achieve
state-of-the-art performance against several baseline methods in an
unsupervised manner.</div><div><a href='http://arxiv.org/abs/2403.05064v1'>2403.05064v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12780v1")'>DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for
  Alleviating Over-squashing</div>
<div id='2401.12780v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:06:08Z</div><div>Authors: Li Sun, Zhenhao Huang, Hua Wu, Junda Ye, Hao Peng, Zhengtao Yu, Philip S. Yu</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have shown great power for learning and mining
on graphs, and Graph Structure Learning (GSL) plays an important role in
boosting GNNs with a refined graph. In the literature, most GSL solutions
either primarily focus on structure refinement with task-specific supervision
(i.e., node classification), or overlook the inherent weakness of GNNs
themselves (e.g., over-squashing), resulting in suboptimal performance despite
sophisticated designs. In light of these limitations, we propose to study
self-supervised graph structure-feature co-refinement for effectively
alleviating the issue of over-squashing in typical GNNs. In this paper, we take
a fundamentally different perspective of the Ricci curvature in Riemannian
geometry, in which we encounter the challenges of modeling, utilizing and
computing Ricci curvature. To tackle these challenges, we present a
self-supervised Riemannian model, DeepRicci. Specifically, we introduce a
latent Riemannian space of heterogeneous curvatures to model various Ricci
curvatures, and propose a gyrovector feature mapping to utilize Ricci curvature
for typical GNNs. Thereafter, we refine node features by geometric contrastive
learning among different geometric views, and simultaneously refine graph
structure by backward Ricci flow based on a novel formulation of differentiable
Ricci curvature. Finally, extensive experiments on public datasets show the
superiority of DeepRicci, and the connection between backward Ricci flow and
over-squashing. Codes of our work are given in https://github.com/RiemanGraph/.</div><div><a href='http://arxiv.org/abs/2401.12780v1'>2401.12780v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01232v1")'>Motif-aware Riemannian Graph Neural Network with Generative-Contrastive
  Learning</div>
<div id='2401.01232v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T14:58:26Z</div><div>Authors: Li Sun, Zhenhao Huang, Zixi Wang, Feiyang Wang, Hao Peng, Philip Yu</div><div style='padding-top: 10px; width: 80ex'>Graphs are typical non-Euclidean data of complex structures. In recent years,
Riemannian graph representation learning has emerged as an exciting alternative
to Euclidean ones. However, Riemannian methods are still in an early stage:
most of them present a single curvature (radius) regardless of structural
complexity, suffer from numerical instability due to the
exponential/logarithmic map, and lack the ability to capture motif regularity.
In light of the issues above, we propose the problem of \emph{Motif-aware
Riemannian Graph Representation Learning}, seeking a numerically stable encoder
to capture motif regularity in a diverse-curvature manifold without labels. To
this end, we present a novel Motif-aware Riemannian model with
Generative-Contrastive learning (MotifRGC), which conducts a minmax game in
Riemannian manifold in a self-supervised manner. First, we propose a new type
of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold
by a product layer with the diversified factor, and replace the
exponential/logarithmic map by a stable kernel layer. Second, we introduce a
motif-aware Riemannian generative-contrastive learning to capture motif
regularity in the constructed manifold and learn motif-aware node
representation without external labels. Empirical results show the superiority
of MofitRGC.</div><div><a href='http://arxiv.org/abs/2401.01232v1'>2401.01232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11087v2")'>Incorporating Higher-order Structural Information for Graph Clustering</div>
<div id='2403.11087v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T04:42:41Z</div><div>Authors: Qiankun Li, Haobing Liu, Ruobing Jiang, Tingting Wang</div><div style='padding-top: 10px; width: 80ex'>Clustering holds profound significance in data mining. In recent years, graph
convolutional network (GCN) has emerged as a powerful tool for deep clustering,
integrating both graph structural information and node attributes. However,
most existing methods ignore the higher-order structural information of the
graph. Evidently, nodes within the same cluster can establish distant
connections. Besides, recent deep clustering methods usually apply a
self-supervised module to monitor the training process of their model, focusing
solely on node attributes without paying attention to graph structure. In this
paper, we propose a novel graph clustering network to make full use of graph
structural information. To capture the higher-order structural information, we
design a graph mutual infomax module, effectively maximizing mutual information
between graph-level and node-level representations, and employ a trinary
self-supervised module that includes modularity as a structural constraint. Our
proposed model outperforms many state-of-the-art methods on various datasets,
demonstrating its superiority.</div><div><a href='http://arxiv.org/abs/2403.11087v2'>2403.11087v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16012v1")'>Deep Contrastive Graph Learning with Clustering-Oriented Guidance</div>
<div id='2402.16012v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T07:03:37Z</div><div>Authors: Mulin Chen, Bocheng Wang, Xuelong Li</div><div style='padding-top: 10px; width: 80ex'>Graph Convolutional Network (GCN) has exhibited remarkable potential in
improving graph-based clustering. To handle the general clustering scenario
without a prior graph, these models estimate an initial graph beforehand to
apply GCN. Throughout the literature, we have witnessed that 1) most models
focus on the initial graph while neglecting the original features. Therefore,
the discriminability of the learned representation may be corrupted by a
low-quality initial graph; 2) the training procedure lacks effective clustering
guidance, which may lead to the incorporation of clustering-irrelevant
information into the learned graph. To tackle these problems, the Deep
Contrastive Graph Learning (DCGL) model is proposed for general data
clustering. Specifically, we establish a pseudo-siamese network, which
incorporates auto-encoder with GCN to emphasize both the graph structure and
the original features. On this basis, feature-level contrastive learning is
introduced to enhance the discriminative capacity, and the relationship between
samples and centroids is employed as the clustering-oriented guidance.
Afterward, a two-branch graph learning mechanism is designed to extract the
local and global structural relationships, which are further embedded into a
unified graph under the cluster-level contrastive guidance. Experimental
results on several benchmark datasets demonstrate the superiority of DCGL
against state-of-the-art algorithms.</div><div><a href='http://arxiv.org/abs/2402.16012v1'>2402.16012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06595v1")'>Every Node is Different: Dynamically Fusing Self-Supervised Tasks for
  Attributed Graph Clustering</div>
<div id='2401.06595v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T14:24:10Z</div><div>Authors: Pengfei Zhu, Qian Wang, Yu Wang, Jialu Li, Qinghua Hu</div><div style='padding-top: 10px; width: 80ex'>Attributed graph clustering is an unsupervised task that partitions nodes
into different groups. Self-supervised learning (SSL) shows great potential in
handling this task, and some recent studies simultaneously learn multiple SSL
tasks to further boost performance. Currently, different SSL tasks are assigned
the same set of weights for all graph nodes. However, we observe that some
graph nodes whose neighbors are in different groups require significantly
different emphases on SSL tasks. In this paper, we propose to dynamically learn
the weights of SSL tasks for different nodes and fuse the embeddings learned
from different SSL tasks to boost performance. We design an innovative graph
clustering approach, namely Dynamically Fusing Self-Supervised Learning
(DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks
using distinct weights derived from a gating network. To effectively learn the
gating network, we design a dual-level self-supervised strategy that
incorporates pseudo labels and the graph structure. Extensive experiments on
five datasets show that DyFSS outperforms the state-of-the-art multi-task SSL
methods by up to 8.66% on the accuracy metric. The code of DyFSS is available
at: https://github.com/q086/DyFSS.</div><div><a href='http://arxiv.org/abs/2401.06595v1'>2401.06595v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17345v1")'>LocalGCL: Local-aware Contrastive Learning for Graphs</div>
<div id='2402.17345v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T09:23:54Z</div><div>Authors: Haojun Jiang, Jiawei Sun, Jie Li, Chentao Wu</div><div style='padding-top: 10px; width: 80ex'>Graph representation learning (GRL) makes considerable progress recently,
which encodes graphs with topological structures into low-dimensional
embeddings. Meanwhile, the time-consuming and costly process of annotating
graph labels manually prompts the growth of self-supervised learning (SSL)
techniques. As a dominant approach of SSL, Contrastive learning (CL) learns
discriminative representations by differentiating between positive and negative
samples. However, when applied to graph data, it overemphasizes global patterns
while neglecting local structures. To tackle the above issue, we propose
\underline{Local}-aware \underline{G}raph \underline{C}ontrastive
\underline{L}earning (\textbf{\methnametrim}), a self-supervised learning
framework that supplementarily captures local graph information with
masking-based modeling compared with vanilla contrastive learning. Extensive
experiments validate the superiority of \methname against state-of-the-art
methods, demonstrating its promise as a comprehensive graph representation
learner.</div><div><a href='http://arxiv.org/abs/2402.17345v1'>2402.17345v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15270v1")'>Smoothed Graph Contrastive Learning via Seamless Proximity Integration</div>
<div id='2402.15270v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T11:32:46Z</div><div>Authors: Maysam Behmanesh, Maks Ovsjanikov</div><div style='padding-top: 10px; width: 80ex'>Graph contrastive learning (GCL) aligns node representations by classifying
node pairs into positives and negatives using a selection process that
typically relies on establishing correspondences within two augmented graphs.
The conventional GCL approaches incorporate negative samples uniformly in the
contrastive loss, resulting in the equal treatment negative nodes, regardless
of their proximity to the true positive. In this paper, we present a Smoothed
Graph Contrastive Learning model (SGCL), which leverages the geometric
structure of augmented graphs to inject proximity information associated with
positive/negative pairs in the contrastive loss, thus significantly
regularizing the learning process. The proposed SGCL adjusts the penalties
associated with node pairs in the contrastive loss by incorporating three
distinct smoothing techniques that result in proximity aware positives and
negatives. To enhance scalability for large-scale graphs, the proposed
framework incorporates a graph batch-generating strategy that partitions the
given graphs into multiple subgraphs, facilitating efficient training in
separate batches. Through extensive experimentation in the unsupervised setting
on various benchmarks, particularly those of large scale, we demonstrate the
superiority of our proposed framework against recent baselines.</div><div><a href='http://arxiv.org/abs/2402.15270v1'>2402.15270v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01092v1")'>Pairwise Alignment Improves Graph Domain Adaptation</div>
<div id='2403.01092v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T04:31:28Z</div><div>Authors: Shikun Liu, Deyu Zou, Han Zhao, Pan Li</div><div style='padding-top: 10px; width: 80ex'>Graph-based methods, pivotal for label inference over interconnected objects
in many real-world applications, often encounter generalization challenges, if
the graph used for model training differs significantly from the graph used for
testing. This work delves into Graph Domain Adaptation (GDA) to address the
unique complexities of distribution shifts over graph data, where
interconnected data points experience shifts in features, labels, and in
particular, connecting patterns. We propose a novel, theoretically principled
method, Pairwise Alignment (Pair-Align) to counter graph structure shift by
mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align
uses edge weights to recalibrate the influence among neighboring nodes to
handle CSS and adjusts the classification loss with label weights to handle LS.
Our method demonstrates superior performance in real-world applications,
including node classification with region shift in social networks, and the
pileup mitigation task in particle colliding experiments. For the first
application, we also curate the largest dataset by far for GDA studies. Our
method shows strong performance in synthetic and other existing benchmark
datasets.</div><div><a href='http://arxiv.org/abs/2403.01092v1'>2403.01092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12564v2")'>Graph Contrastive Invariant Learning from the Causal Perspective</div>
<div id='2401.12564v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T08:47:28Z</div><div>Authors: Yanhu Mo, Xiao Wang, Shaohua Fan, Chuan Shi</div><div style='padding-top: 10px; width: 80ex'>Graph contrastive learning (GCL), learning the node representation by
contrasting two augmented graphs in a self-supervised way, has attracted
considerable attention. GCL is usually believed to learn the invariant
representation. However, does this understanding always hold in practice? In
this paper, we first study GCL from the perspective of causality. By analyzing
GCL with the structural causal model (SCM), we discover that traditional GCL
may not well learn the invariant representations due to the non-causal
information contained in the graph. How can we fix it and encourage the current
GCL to learn better invariant representations? The SCM offers two requirements
and motives us to propose a novel GCL method. Particularly, we introduce the
spectral graph augmentation to simulate the intervention upon non-causal
factors. Then we design the invariance objective and independence objective to
better capture the causal factors. Specifically, (i) the invariance objective
encourages the encoder to capture the invariant information contained in causal
variables, and (ii) the independence objective aims to reduce the influence of
confounders on the causal variables. Experimental results demonstrate the
effectiveness of our approach on node classification tasks.</div><div><a href='http://arxiv.org/abs/2401.12564v2'>2401.12564v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09600v1")'>Low-Rank Graph Contrastive Learning for Node Classification</div>
<div id='2402.09600v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T22:15:37Z</div><div>Authors: Yancheng Wang, Yingzhen Yang</div><div style='padding-top: 10px; width: 80ex'>Graph Neural Networks (GNNs) have been widely used to learn node
representations and with outstanding performance on various tasks such as node
classification. However, noise, which inevitably exists in real-world graph
data, would considerably degrade the performance of GNNs revealed by recent
studies. In this work, we propose a novel and robust GNN encoder, Low-Rank
Graph Contrastive Learning (LR-GCL). Our method performs transductive node
classification in two steps. First, a low-rank GCL encoder named LR-GCL is
trained by prototypical contrastive learning with low-rank regularization.
Next, using the features produced by LR-GCL, a linear transductive
classification algorithm is used to classify the unlabeled nodes in the graph.
Our LR-GCL is inspired by the low frequency property of the graph data and its
labels, and it is also theoretically motivated by our sharp generalization
bound for transductive learning. To the best of our knowledge, our theoretical
result is among the first to theoretically demonstrate the advantage of
low-rank learning in graph contrastive learning supported by strong empirical
performance. Extensive experiments on public benchmarks demonstrate the
superior performance of LR-GCL and the robustness of the learned node
representations. The code of LR-GCL is available at
\url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}.</div><div><a href='http://arxiv.org/abs/2402.09600v1'>2402.09600v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02713v1")'>Graph-level Protein Representation Learning by Structure Knowledge
  Refinement</div>
<div id='2401.02713v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T09:05:33Z</div><div>Authors: Ge Wang, Zelin Zang, Jiangbin Zheng, Jun Xia, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>This paper focuses on learning representation on the whole graph level in an
unsupervised manner. Learning graph-level representation plays an important
role in a variety of real-world issues such as molecule property prediction,
protein structure feature extraction, and social network analysis. The
mainstream method is utilizing contrastive learning to facilitate graph feature
extraction, known as Graph Contrastive Learning (GCL). GCL, although effective,
suffers from some complications in contrastive learning, such as the effect of
false negative pairs. Moreover, augmentation strategies in GCL are weakly
adaptive to diverse graph datasets. Motivated by these problems, we propose a
novel framework called Structure Knowledge Refinement (SKR) which uses data
structure to determine the probability of whether a pair is positive or
negative. Meanwhile, we propose an augmentation strategy that naturally
preserves the semantic meaning of the original data and is compatible with our
SKR framework. Furthermore, we illustrate the effectiveness of our SKR
framework through intuition and experiments. The experimental results on the
tasks of graph-level classification demonstrate that our SKR framework is
superior to most state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2401.02713v1'>2401.02713v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16011v1")'>GPS: Graph Contrastive Learning via Multi-scale Augmented Views from
  Adversarial Pooling</div>
<div id='2401.16011v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T10:00:53Z</div><div>Authors: Wei Ju, Yiyang Gu, Zhengyang Mao, Ziyue Qiao, Yifang Qin, Xiao Luo, Hui Xiong, Ming Zhang</div><div style='padding-top: 10px; width: 80ex'>Self-supervised graph representation learning has recently shown considerable
promise in a range of fields, including bioinformatics and social networks. A
large number of graph contrastive learning approaches have shown promising
performance for representation learning on graphs, which train models by
maximizing agreement between original graphs and their augmented views (i.e.,
positive views). Unfortunately, these methods usually involve pre-defined
augmentation strategies based on the knowledge of human experts. Moreover,
these strategies may fail to generate challenging positive views to provide
sufficient supervision signals. In this paper, we present a novel approach
named Graph Pooling ContraSt (GPS) to address these issues. Motivated by the
fact that graph pooling can adaptively coarsen the graph with the removal of
redundancy, we rethink graph pooling and leverage it to automatically generate
multi-scale positive views with varying emphasis on providing challenging
positives and preserving semantics, i.e., strongly-augmented view and
weakly-augmented view. Then, we incorporate both views into a joint contrastive
learning framework with similarity learning and consistency learning, where our
pooling module is adversarially trained with respect to the encoder for
adversarial robustness. Experiments on twelve datasets on both graph
classification and transfer learning tasks verify the superiority of the
proposed method over its counterparts.</div><div><a href='http://arxiv.org/abs/2401.16011v1'>2401.16011v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08907v1")'>Tackling Negative Transfer on Graphs</div>
<div id='2402.08907v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T02:46:47Z</div><div>Authors: Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye</div><div style='padding-top: 10px; width: 80ex'>Transfer learning aims to boost the learning on the target task leveraging
knowledge learned from other relevant tasks. However, when the source and
target are not closely related, the learning performance may be adversely
affected, a phenomenon known as negative transfer. In this paper, we
investigate the negative transfer in graph transfer learning, which is
important yet underexplored. We reveal that, unlike image or text, negative
transfer commonly occurs in graph-structured data, even when source and target
graphs share semantic similarities. Specifically, we identify that structural
differences significantly amplify the dissimilarities in the node embeddings
across graphs. To mitigate this, we bring a new insight: for semantically
similar graphs, although structural differences lead to significant
distribution shift in node embeddings, their impact on subgraph embeddings
could be marginal. Building on this insight, we introduce two effective yet
elegant methods, Subgraph Pooling (SP) and Subgraph Pooling++ (SP++), that
transfer subgraph-level knowledge across graphs. We theoretically analyze the
role of SP in reducing graph discrepancy and conduct extensive experiments to
evaluate its superiority under various settings. Our code and datasets are
available at: https://github.com/Zehong-Wang/Subgraph-Pooling.</div><div><a href='http://arxiv.org/abs/2402.08907v1'>2402.08907v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15680v1")'>Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward
  Comprehensive Benchmarks</div>
<div id='2402.15680v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T01:47:56Z</div><div>Authors: Qian Ma, Hongliang Chi, Hengrui Zhang, Kay Liu, Zhiwei Zhang, Lu Cheng, Suhang Wang, Philip S. Yu, Yao Ma</div><div style='padding-top: 10px; width: 80ex'>The rise of self-supervised learning, which operates without the need for
labeled data, has garnered significant interest within the graph learning
community. This enthusiasm has led to the development of numerous Graph
Contrastive Learning (GCL) techniques, all aiming to create a versatile graph
encoder that leverages the wealth of unlabeled data for various downstream
tasks. However, the current evaluation standards for GCL approaches are flawed
due to the need for extensive hyper-parameter tuning during pre-training and
the reliance on a single downstream task for assessment. These flaws can skew
the evaluation away from the intended goals, potentially leading to misleading
conclusions. In our paper, we thoroughly examine these shortcomings and offer
fresh perspectives on how GCL methods are affected by hyper-parameter choices
and the choice of downstream tasks for their evaluation. Additionally, we
introduce an enhanced evaluation framework designed to more accurately gauge
the effectiveness, consistency, and overall capability of GCL methods.</div><div><a href='http://arxiv.org/abs/2402.15680v1'>2402.15680v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11004v1")'>Forward Learning of Graph Neural Networks</div>
<div id='2403.11004v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T19:40:35Z</div><div>Authors: Namyong Park, Xing Wang, Antoine Simoulin, Shuai Yang, Grey Yang, Ryan Rossi, Puja Trivedi, Nesreen Ahmed</div><div style='padding-top: 10px; width: 80ex'>Graph neural networks (GNNs) have achieved remarkable success across a wide
range of applications, such as recommendation, drug discovery, and question
answering. Behind the success of GNNs lies the backpropagation (BP) algorithm,
which is the de facto standard for training deep neural networks (NNs).
However, despite its effectiveness, BP imposes several constraints, which are
not only biologically implausible, but also limit the scalability, parallelism,
and flexibility in learning NNs. Examples of such constraints include storage
of neural activities computed in the forward pass for use in the subsequent
backward pass, and the dependence of parameter updates on non-local signals. To
address these limitations, the forward-forward algorithm (FF) was recently
proposed as an alternative to BP in the image classification domain, which
trains NNs by performing two forward passes over positive and negative data.
Inspired by this advance, we propose ForwardGNN in this work, a new forward
learning procedure for GNNs, which avoids the constraints imposed by BP via an
effective layer-wise local forward training. ForwardGNN extends the original FF
to deal with graph data and GNNs, and makes it possible to operate without
generating negative inputs (hence no longer forward-forward). Further,
ForwardGNN enables each layer to learn from both the bottom-up and top-down
signals without relying on the backpropagation of errors. Extensive experiments
on real-world datasets show the effectiveness and generality of the proposed
forward graph learning framework. We release our code at
https://github.com/facebookresearch/forwardgnn.</div><div><a href='http://arxiv.org/abs/2403.11004v1'>2403.11004v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18434v1")'>Graph Regularized Encoder Training for Extreme Classification</div>
<div id='2402.18434v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T16:00:25Z</div><div>Authors: Anshul Mittal, Shikhar Mohan, Deepak Saini, Suchith C. Prabhu, Jain jiao, Sumeet Agarwal, Soumen Chakrabarti, Purushottam Kar, Manik Varma</div><div style='padding-top: 10px; width: 80ex'>Deep extreme classification (XC) aims to train an encoder architecture and an
accompanying classifier architecture to tag a data point with the most relevant
subset of labels from a very large universe of labels. XC applications in
ranking, recommendation and tagging routinely encounter tail labels for which
the amount of training data is exceedingly small. Graph convolutional networks
(GCN) present a convenient but computationally expensive way to leverage task
metadata and enhance model accuracies in these settings. This paper formally
establishes that in several use cases, the steep computational cost of GCNs is
entirely avoidable by replacing GCNs with non-GCN architectures. The paper
notices that in these settings, it is much more effective to use graph data to
regularize encoder training than to implement a GCN. Based on these insights,
an alternative paradigm RAMEN is presented to utilize graph metadata in XC
settings that offers significant performance boosts with zero increase in
inference computational costs. RAMEN scales to datasets with up to 1M labels
and offers prediction accuracy up to 15% higher on benchmark datasets than
state of the art methods, including those that use graph metadata to train
GCNs. RAMEN also offers 10% higher accuracy over the best baseline on a
proprietary recommendation dataset sourced from click logs of a popular search
engine. Code for RAMEN will be released publicly.</div><div><a href='http://arxiv.org/abs/2402.18434v1'>2402.18434v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10590v1")'>Adversarially Robust Signed Graph Contrastive Learning from Balance
  Augmentation</div>
<div id='2401.10590v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T10:02:20Z</div><div>Authors: Jialong Zhou, Xing Ai, Yuni Lai, Kai Zhou</div><div style='padding-top: 10px; width: 80ex'>Signed graphs consist of edges and signs, which can be separated into
structural information and balance-related information, respectively. Existing
signed graph neural networks (SGNNs) typically rely on balance-related
information to generate embeddings. Nevertheless, the emergence of recent
adversarial attacks has had a detrimental impact on the balance-related
information. Similar to how structure learning can restore unsigned graphs,
balance learning can be applied to signed graphs by improving the balance
degree of the poisoned graph. However, this approach encounters the challenge
"Irreversibility of Balance-related Information" - while the balance degree
improves, the restored edges may not be the ones originally affected by
attacks, resulting in poor defense effectiveness. To address this challenge, we
propose a robust SGNN framework called Balance Augmented-Signed Graph
Contrastive Learning (BA-SGCL), which combines Graph Contrastive Learning
principles with balance augmentation techniques. Experimental results
demonstrate that BA-SGCL not only enhances robustness against existing
adversarial attacks but also achieves superior performance on link sign
prediction task across various datasets.</div><div><a href='http://arxiv.org/abs/2401.10590v1'>2401.10590v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17580v2")'>Graph Contrastive Learning with Cohesive Subgraph Awareness</div>
<div id='2401.17580v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T03:51:30Z</div><div>Authors: Yucheng Wu, Leye Wang, Xiao Han, Han-Jia Ye</div><div style='padding-top: 10px; width: 80ex'>Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy
for learning representations of diverse graphs including social and biomedical
networks. GCL widely uses stochastic graph topology augmentation, such as
uniform node dropping, to generate augmented graphs. However, such stochastic
augmentations may severely damage the intrinsic properties of a graph and
deteriorate the following representation learning process. We argue that
incorporating an awareness of cohesive subgraphs during the graph augmentation
and learning processes has the potential to enhance GCL performance. To this
end, we propose a novel unified framework called CTAug, to seamlessly integrate
cohesion awareness into various existing GCL mechanisms. In particular, CTAug
comprises two specialized modules: topology augmentation enhancement and graph
learning enhancement. The former module generates augmented graphs that
carefully preserve cohesion properties, while the latter module bolsters the
graph encoder's ability to discern subgraph patterns. Theoretical analysis
shows that CTAug can strictly improve existing GCL mechanisms. Empirical
experiments verify that CTAug can achieve state-of-the-art performance for
graph representation learning, especially for graphs with high degrees. The
code is available at https://doi.org/10.5281/zenodo.10594093, or
https://github.com/wuyucheng2002/CTAug.</div><div><a href='http://arxiv.org/abs/2401.17580v2'>2401.17580v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11483v1")'>Open-World Semi-Supervised Learning for Node Classification</div>
<div id='2403.11483v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T05:12:54Z</div><div>Authors: Yanling Wang, Jing Zhang, Lingxi Zhang, Lixin Liu, Yuxiao Dong, Cuiping Li, Hong Chen, Hongzhi Yin</div><div style='padding-top: 10px; width: 80ex'>Open-world semi-supervised learning (Open-world SSL) for node classification,
that classifies unlabeled nodes into seen classes or multiple novel classes, is
a practical but under-explored problem in the graph community. As only seen
classes have human labels, they are usually better learned than novel classes,
and thus exhibit smaller intra-class variances within the embedding space
(named as imbalance of intra-class variances between seen and novel classes).
Based on empirical and theoretical analysis, we find the variance imbalance can
negatively impact the model performance. Pre-trained feature encoders can
alleviate this issue via producing compact representations for novel classes.
However, creating general pre-trained encoders for various types of graph data
has been proven to be challenging. As such, there is a demand for an effective
method that does not rely on pre-trained graph encoders. In this paper, we
propose an IMbalance-Aware method named OpenIMA for Open-world semi-supervised
node classification, which trains the node classification model from scratch
via contrastive learning with bias-reduced pseudo labels. Extensive experiments
on seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and
the source code has been available on GitHub.</div><div><a href='http://arxiv.org/abs/2403.11483v1'>2403.11483v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13114v1")'>BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer
  Nodes</div>
<div id='2402.13114v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T16:11:59Z</div><div>Authors: Qian Wang, Zemin Liu, Zhen Zhang, Bingsheng He</div><div style='padding-top: 10px; width: 80ex'>Class imbalance in graph-structured data, where minor classes are
significantly underrepresented, poses a critical challenge for Graph Neural
Networks (GNNs). To address this challenge, existing studies generally generate
new minority nodes and edges connecting new nodes to the original graph to make
classes balanced. However, they do not solve the problem that majority classes
still propagate information to minority nodes by edges in the original graph
which introduces bias towards majority classes. To address this, we introduce
BuffGraph, which inserts buffer nodes into the graph, modulating the impact of
majority classes to improve minor class representation. Our extensive
experiments across diverse real-world datasets empirically demonstrate that
BuffGraph outperforms existing baseline methods in class-imbalanced node
classification in both natural settings and imbalanced settings. Code is
available at https://anonymous.4open.science/r/BuffGraph-730A.</div><div><a href='http://arxiv.org/abs/2402.13114v1'>2402.13114v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09953v1")'>Online GNN Evaluation Under Test-time Graph Distribution Shifts</div>
<div id='2403.09953v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T01:28:08Z</div><div>Authors: Xin Zheng, Dongjin Song, Qingsong Wen, Bo Du, Shirui Pan</div><div style='padding-top: 10px; width: 80ex'>Evaluating the performance of a well-trained GNN model on real-world graphs
is a pivotal step for reliable GNN online deployment and serving. Due to a lack
of test node labels and unknown potential training-test graph data distribution
shifts, conventional model evaluation encounters limitations in calculating
performance metrics (e.g., test error) and measuring graph data-level
discrepancies, particularly when the training graph used for developing GNNs
remains unobserved during test time. In this paper, we study a new research
problem, online GNN evaluation, which aims to provide valuable insights into
the well-trained GNNs's ability to effectively generalize to real-world
unlabeled graphs under the test-time graph distribution shifts. Concretely, we
develop an effective learning behavior discrepancy score, dubbed LeBeD, to
estimate the test-time generalization errors of well-trained GNN models.
Through a novel GNN re-training strategy with a parameter-free optimality
criterion, the proposed LeBeD comprehensively integrates learning behavior
discrepancies from both node prediction and structure reconstruction
perspectives. This enables the effective evaluation of the well-trained GNNs'
ability to capture test node semantics and structural representations, making
it an expressive metric for estimating the generalization error in online GNN
evaluation. Extensive experiments on real-world test graphs under diverse graph
distribution shifts could verify the effectiveness of the proposed method,
revealing its strong correlation with ground-truth test errors on various
well-trained GNN models.</div><div><a href='http://arxiv.org/abs/2403.09953v1'>2403.09953v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00789v1")'>Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective
  State Spaces</div>
<div id='2402.00789v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:21:53Z</div><div>Authors: Chloe Wang, Oleksii Tsepa, Jun Ma, Bo Wang</div><div style='padding-top: 10px; width: 80ex'>Attention mechanisms have been widely used to capture long-range dependencies
among nodes in Graph Transformers. Bottlenecked by the quadratic computational
cost, attention mechanisms fail to scale in large graphs. Recent improvements
in computational efficiency are mainly achieved by attention sparsification
with random or heuristic-based graph subsampling, which falls short in
data-dependent context reasoning. State space models (SSMs), such as Mamba,
have gained prominence for their effectiveness and efficiency in modeling
long-range dependencies in sequential data. However, adapting SSMs to
non-sequential graph data presents a notable challenge. In this work, we
introduce Graph-Mamba, the first attempt to enhance long-range context modeling
in graph networks by integrating a Mamba block with the input-dependent node
selection mechanism. Specifically, we formulate graph-centric node
prioritization and permutation strategies to enhance context-aware reasoning,
leading to a substantial improvement in predictive performance. Extensive
experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms
state-of-the-art methods in long-range graph prediction tasks, with a fraction
of the computational cost in both FLOPs and GPU memory consumption. The code
and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.</div><div><a href='http://arxiv.org/abs/2402.00789v1'>2402.00789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15203v1")'>FedGT: Federated Node Classification with Scalable Graph Transformer</div>
<div id='2401.15203v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T21:02:36Z</div><div>Authors: Zaixi Zhang, Qingyong Hu, Yang Yu, Weibo Gao, Qi Liu</div><div style='padding-top: 10px; width: 80ex'>Graphs are widely used to model relational data. As graphs are getting larger
and larger in real-world scenarios, there is a trend to store and compute
subgraphs in multiple local systems. For example, recently proposed
\emph{subgraph federated learning} methods train Graph Neural Networks (GNNs)
distributively on local subgraphs and aggregate GNN parameters with a central
server. However, existing methods have the following limitations: (1) The links
between local subgraphs are missing in subgraph federated learning. This could
severely damage the performance of GNNs that follow message-passing paradigms
to update node/edge features. (2) Most existing methods overlook the subgraph
heterogeneity issue, brought by subgraphs being from different parts of the
whole graph. To address the aforementioned challenges, we propose a scalable
\textbf{Fed}erated \textbf{G}raph \textbf{T}ransformer (\textbf{FedGT}) in the
paper. Firstly, we design a hybrid attention scheme to reduce the complexity of
the Graph Transformer to linear while ensuring a global receptive field with
theoretical bounds. Specifically, each node attends to the sampled local
neighbors and a set of curated global nodes to learn both local and global
information and be robust to missing links. The global nodes are dynamically
updated during training with an online clustering algorithm to capture the data
distribution of the corresponding local subgraph. Secondly, FedGT computes
clients' similarity based on the aligned global nodes with optimal transport.
The similarity is then used to perform weighted averaging for personalized
aggregation, which well addresses the data heterogeneity problem. Moreover,
local differential privacy is applied to further protect the privacy of
clients. Finally, extensive experimental results on 6 datasets and 2 subgraph
settings demonstrate the superiority of FedGT.</div><div><a href='http://arxiv.org/abs/2401.15203v1'>2401.15203v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19163v1")'>FedStruct: Federated Decoupled Learning over Interconnected Graphs</div>
<div id='2402.19163v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T13:47:23Z</div><div>Authors: Javad Aliakbari, Johan Östman, Alexandre Graell i Amat</div><div style='padding-top: 10px; width: 80ex'>We address the challenge of federated learning on graph-structured data
distributed across multiple clients. Specifically, we focus on the prevalent
scenario of interconnected subgraphs, where inter-connections between different
clients play a critical role. We present a novel framework for this scenario,
named FedStruct, that harnesses deep structural dependencies. To uphold
privacy, unlike existing methods, FedStruct eliminates the necessity of sharing
or generating sensitive node features or embeddings among clients. Instead, it
leverages explicit global graph structure information to capture inter-node
dependencies. We validate the effectiveness of FedStruct through experimental
results conducted on six datasets for semi-supervised node classification,
showcasing performance close to the centralized approach across various
scenarios, including different data partitioning methods, varying levels of
label availability, and number of clients.</div><div><a href='http://arxiv.org/abs/2402.19163v1'>2402.19163v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14340v1")'>Exploring Task Unification in Graph Representation Learning via
  Generative Approach</div>
<div id='2403.14340v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T12:14:02Z</div><div>Authors: Yulan Hu, Sheng Ouyang, Zhirui Yang, Ge Chen, Junchen Wan, Xiao Wang, Yong Liu</div><div style='padding-top: 10px; width: 80ex'>Graphs are ubiquitous in real-world scenarios and encompass a diverse range
of tasks, from node-, edge-, and graph-level tasks to transfer learning.
However, designing specific tasks for each type of graph data is often costly
and lacks generalizability. Recent endeavors under the "Pre-training +
Fine-tuning" or "Pre-training + Prompt" paradigms aim to design a unified
framework capable of generalizing across multiple graph tasks. Among these,
graph autoencoders (GAEs), generative self-supervised models, have demonstrated
their potential in effectively addressing various graph tasks. Nevertheless,
these methods typically employ multi-stage training and require adaptive
designs, which on one hand make it difficult to be seamlessly applied to
diverse graph tasks and on the other hand overlook the negative impact caused
by discrepancies in task objectives between the different stages. To address
these challenges, we propose GA^2E, a unified adversarially masked autoencoder
capable of addressing the above challenges seamlessly. Specifically, GA^2E
proposes to use the subgraph as the meta-structure, which remains consistent
across all graph tasks (ranging from node-, edge-, and graph-level to transfer
learning) and all stages (both during training and inference). Further, GA^2E
operates in a \textbf{"Generate then Discriminate"} manner. It leverages the
masked GAE to reconstruct the input subgraph whilst treating it as a generator
to compel the reconstructed graphs resemble the input subgraph. Furthermore,
GA^2E introduces an auxiliary discriminator to discern the authenticity between
the reconstructed (generated) subgraph and the input subgraph, thus ensuring
the robustness of the graph representation through adversarial training
mechanisms. We validate GA^2E's capabilities through extensive experiments on
21 datasets across four types of graph tasks.</div><div><a href='http://arxiv.org/abs/2403.14340v1'>2403.14340v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07225v1")'>Rethinking Graph Masked Autoencoders through Alignment and Uniformity</div>
<div id='2402.07225v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T15:21:08Z</div><div>Authors: Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, Liang Wang</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning on graphs can be bifurcated into contrastive and
generative methods. Contrastive methods, also known as graph contrastive
learning (GCL), have dominated graph self-supervised learning in the past few
years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles
the momentum behind generative methods. Despite the empirical success of
GraphMAE, there is still a dearth of theoretical understanding regarding its
efficacy. Moreover, while both generative and contrastive methods have been
shown to be effective, their connections and differences have yet to be
thoroughly investigated. Therefore, we theoretically build a bridge between
GraphMAE and GCL, and prove that the node-level reconstruction objective in
GraphMAE implicitly performs context-level GCL. Based on our theoretical
analysis, we further identify the limitations of the GraphMAE from the
perspectives of alignment and uniformity, which have been considered as two key
properties of high-quality representations in GCL. We point out that GraphMAE's
alignment performance is restricted by the masking strategy, and the uniformity
is not strictly guaranteed. To remedy the aforementioned limitations, we
propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named
AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy
to provide hard-to-align samples, which improves the alignment performance.
Meanwhile, we introduce an explicit uniformity regularizer to ensure the
uniformity of the learned representations. Experimental results on benchmark
datasets demonstrate the superiority of our model over existing
state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2402.07225v1'>2402.07225v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01143v1")'>Learning Network Representations with Disentangled Graph Auto-Encoder</div>
<div id='2402.01143v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T04:52:52Z</div><div>Authors: Di Fan, Chuanhou Gao</div><div style='padding-top: 10px; width: 80ex'>The (variational) graph auto-encoder is extensively employed for learning
representations of graph-structured data. However, the formation of real-world
graphs is a complex and heterogeneous process influenced by latent factors.
Existing encoders are fundamentally holistic, neglecting the entanglement of
latent factors. This not only makes graph analysis tasks less effective but
also makes it harder to understand and explain the representations. Learning
disentangled graph representations with (variational) graph auto-encoder poses
significant challenges, and remains largely unexplored in the existing
literature. In this article, we introduce the Disentangled Graph Auto-Encoder
(DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that
leverage generative models to learn disentangled representations. Specifically,
we first design a disentangled graph convolutional network with multi-channel
message-passing layers, as the encoder aggregating information related to each
disentangled latent factor. Subsequently, a component-wise flow is applied to
each channel to enhance the expressive capabilities of disentangled variational
graph auto-encoder. Additionally, we design a factor-wise decoder, considering
the characteristics of disentangled representations. In order to further
enhance the independence among representations, we introduce independence
constraints on mapping channels for different latent factors. Empirical
experiments on both synthetic and real-world datasets show the superiority of
our proposed method compared to several state-of-the-art baselines.</div><div><a href='http://arxiv.org/abs/2402.01143v1'>2402.01143v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06727v1")'>Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding</div>
<div id='2401.06727v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T17:57:07Z</div><div>Authors: Bozhen Hu, Zelin Zang, Jun Xia, Lirong Wu, Cheng Tan, Stan Z. Li</div><div style='padding-top: 10px; width: 80ex'>Representing graph data in a low-dimensional space for subsequent tasks is
the purpose of attributed graph embedding. Most existing neural network
approaches learn latent representations by minimizing reconstruction errors.
Rare work considers the data distribution and the topological structure of
latent codes simultaneously, which often results in inferior embeddings in
real-world graph data. This paper proposes a novel Deep Manifold (Variational)
Graph Auto-Encoder (DMVGAE/DMGAE) method for attributed graph data to improve
the stability and quality of learned representations to tackle the crowding
problem. The node-to-node geodesic similarity is preserved between the original
and latent space under a pre-defined distribution. The proposed method
surpasses state-of-the-art baseline algorithms by a significant margin on
different downstream tasks across popular datasets, which validates our
solutions. We promise to release the code after acceptance.</div><div><a href='http://arxiv.org/abs/2401.06727v1'>2401.06727v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08023v1")'>UGMAE: A Unified Framework for Graph Masked Autoencoders</div>
<div id='2402.08023v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:39:26Z</div><div>Authors: Yijun Tian, Chuxu Zhang, Ziyi Kou, Zheyuan Liu, Xiangliang Zhang, Nitesh V. Chawla</div><div style='padding-top: 10px; width: 80ex'>Generative self-supervised learning on graphs, particularly graph masked
autoencoders, has emerged as a popular learning paradigm and demonstrated its
efficacy in handling non-Euclidean data. However, several remaining issues
limit the capability of existing methods: 1) the disregard of uneven node
significance in masking, 2) the underutilization of holistic graph information,
3) the ignorance of semantic knowledge in the representation space due to the
exclusive use of reconstruction loss in the output space, and 4) the unstable
reconstructions caused by the large volume of masked contents. In light of
this, we propose UGMAE, a unified framework for graph masked autoencoders to
address these issues from the perspectives of adaptivity, integrity,
complementarity, and consistency. Specifically, we first develop an adaptive
feature mask generator to account for the unique significance of nodes and
sample informative masks (adaptivity). We then design a ranking-based structure
reconstruction objective joint with feature reconstruction to capture holistic
graph information and emphasize the topological proximity between neighbors
(integrity). After that, we present a bootstrapping-based similarity module to
encode the high-level semantic knowledge in the representation space,
complementary to the low-level reconstruction in the output space
(complementarity). Finally, we build a consistency assurance module to provide
reconstruction objectives with extra stabilized consistency targets
(consistency). Extensive experiments demonstrate that UGMAE outperforms both
contrastive and generative state-of-the-art baselines on several tasks across
multiple datasets.</div><div><a href='http://arxiv.org/abs/2402.08023v1'>2402.08023v1</a></div>
</div></div>
    <div><a href="arxiv_21.html">Prev (21)</a></div>
    <div><a href="arxiv_23.html">Next (23)</a></div>
    