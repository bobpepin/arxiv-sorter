
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.09347v1")'>BurstAttention: An Efficient Distributed Attention Framework for
  Extremely Long Sequences</div>
<div id='2403.09347v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T12:51:58Z</div><div>Authors: Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun, Shengnan Wang, Teng Su</div><div style='padding-top: 10px; width: 80ex'>Effective attention modules have played a crucial role in the success of
Transformer-based large language models (LLMs), but the quadratic time and
memory complexities of these attention modules also pose a challenge when
processing long sequences. One potential solution for the long sequence problem
is to utilize distributed clusters to parallelize the computation of attention
modules across multiple devices (e.g., GPUs). However, adopting a distributed
approach inevitably introduces extra memory overheads to store local attention
results and incurs additional communication costs to aggregate local results
into global ones. In this paper, we propose a distributed attention framework
named ``BurstAttention'' to optimize memory access and communication operations
at both the global cluster and local device levels. In our experiments, we
compare BurstAttention with other competitive distributed attention solutions
for long sequence processing. The experimental results under different length
settings demonstrate that BurstAttention offers significant advantages for
processing long sequences compared with these competitive baselines, reducing
40% communication overheads and achieving 2 X speedup during training 32K
sequence length on 8 X A100.</div><div><a href='http://arxiv.org/abs/2403.09347v1'>2403.09347v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15220v2")'>ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and
  Two-Phase Partition</div>
<div id='2402.15220v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T09:29:19Z</div><div>Authors: Lu Ye, Ze Tao, Yong Huang, Yang Li</div><div style='padding-top: 10px; width: 80ex'>Self-attention is an essential component of large language models(LLMs) but a
significant source of inference latency for long sequences. In multi-tenant
LLMs serving scenarios, the compute and memory operation cost of self-attention
can be optimized by using the probability that multiple LLM requests have
shared system prompts in prefixes. In this paper, we introduce ChunkAttention,
a prefix-aware self-attention module that can detect matching prompt prefixes
across multiple requests and share their key/value tensors in memory at runtime
to improve the memory utilization of KV cache. This is achieved by breaking
monolithic key/value tensors into smaller chunks and structuring them into the
auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,
we design an efficient self-attention kernel, where a two-phase partition
algorithm is implemented to improve the data locality during self-attention
computation in the presence of shared system prompts. Experiments show that
ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$
compared to the start-of-the-art implementation, with the length of the system
prompt ranging from 1024 to 4096.</div><div><a href='http://arxiv.org/abs/2402.15220v2'>2402.15220v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19427v1")'>Griffin: Mixing Gated Linear Recurrences with Local Attention for
  Efficient Language Models</div>
<div id='2402.19427v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T18:24:46Z</div><div>Authors: Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, Caglar Gulcehre</div><div style='padding-top: 10px; width: 80ex'>Recurrent neural networks (RNNs) have fast inference and scale efficiently on
long sequences, but they are difficult to train and hard to scale. We propose
Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that
mixes gated linear recurrences with local attention. Hawk exceeds the reported
performance of Mamba on downstream tasks, while Griffin matches the performance
of Llama-2 despite being trained on over 6 times fewer tokens. We also show
that Griffin can extrapolate on sequences significantly longer than those seen
during training. Our models match the hardware efficiency of Transformers
during training, and during inference they have lower latency and significantly
higher throughput. We scale Griffin up to 14B parameters, and explain how to
shard our models for efficient distributed training.</div><div><a href='http://arxiv.org/abs/2402.19427v1'>2402.19427v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02920v1")'>TaylorShift: Shifting the Complexity of Self-Attention from Squared to
  Linear (and Back) using Taylor-Softmax</div>
<div id='2403.02920v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T12:38:14Z</div><div>Authors: Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel</div><div style='padding-top: 10px; width: 80ex'>The quadratic complexity of the attention mechanism represents one of the
biggest hurdles for processing long sequences using Transformers. Current
methods, relying on sparse representations or stateful recurrence, sacrifice
token-to-token interactions, which ultimately leads to compromises in
performance. This paper introduces TaylorShift, a novel reformulation of the
Taylor softmax that enables computing full token-to-token interactions in
linear time and space. We analytically determine the crossover points where
employing TaylorShift becomes more efficient than traditional attention,
aligning closely with empirical measurements. Specifically, our findings
demonstrate that TaylorShift enhances memory efficiency for sequences as short
as 800 tokens and accelerates inference for inputs of approximately 1700 tokens
and beyond. For shorter sequences, TaylorShift scales comparably with the
vanilla attention. Furthermore, a classification benchmark across five tasks
involving long sequences reveals no degradation in accuracy when employing
Transformers equipped with TaylorShift. For reproducibility, we provide access
to our code under https://github.com/tobna/TaylorShift.</div><div><a href='http://arxiv.org/abs/2403.02920v1'>2403.02920v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08081v1")'>Mechanics of Next Token Prediction with Self-Attention</div>
<div id='2403.08081v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T21:15:38Z</div><div>Authors: Yingcong Li, Yixiao Huang, M. Emrullah Ildiz, Ankit Singh Rawat, Samet Oymak</div><div style='padding-top: 10px; width: 80ex'>Transformer-based language models are trained on large datasets to predict
the next token given an input sequence. Despite this simple training objective,
they have led to revolutionary advances in natural language processing.
Underlying this success is the self-attention mechanism. In this work, we ask:
$\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$
$\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$
$\textit{next-token}$ $\textit{prediction?}$ We show that training
self-attention with gradient descent learns an automaton which generates the
next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$
$\textbf{retrieval:}$ Given input sequence, self-attention precisely selects
the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with
the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It
then creates a convex combination of the high-priority tokens from which the
next token can be sampled. Under suitable conditions, we rigorously
characterize these mechanics through a directed graph over tokens extracted
from the training data. We prove that gradient descent implicitly discovers the
strongly-connected components (SCC) of this graph and self-attention learns to
retrieve the tokens that belong to the highest-priority SCC available in the
context window. Our theory relies on decomposing the model weights into a
directional component and a finite component that correspond to hard retrieval
and soft composition steps respectively. This also formalizes a related
implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that
these findings shed light on how self-attention processes sequential data and
pave the path toward demystifying more complex architectures.</div><div><a href='http://arxiv.org/abs/2403.08081v1'>2403.08081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14735v1")'>How Transformers Learn Causal Structure with Gradient Descent</div>
<div id='2402.14735v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:47:03Z</div><div>Authors: Eshaan Nichani, Alex Damian, Jason D. Lee</div><div style='padding-top: 10px; width: 80ex'>The incredible success of transformers on sequence modeling tasks can be
largely attributed to the self-attention mechanism, which allows information to
be transferred between different parts of a sequence. Self-attention allows
transformers to encode causal structure which makes them particularly suitable
for sequence modeling. However, the process by which transformers learn such
causal structure via gradient-based training algorithms remains poorly
understood. To better understand this process, we introduce an in-context
learning task that requires learning latent causal structure. We prove that
gradient descent on a simplified two-layer transformer learns to solve this
task by encoding the latent causal graph in the first attention layer. The key
insight of our proof is that the gradient of the attention matrix encodes the
mutual information between tokens. As a consequence of the data processing
inequality, the largest entries of this gradient correspond to edges in the
latent causal graph. As a special case, when the sequences are generated from
in-context Markov chains, we prove that transformers learn an induction head
(Olsson et al., 2022). We confirm our theoretical findings by showing that
transformers trained on our in-context learning task are able to recover a wide
variety of causal structures.</div><div><a href='http://arxiv.org/abs/2402.14735v1'>2402.14735v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14027v1")'>Learning causation event conjunction sequences</div>
<div id='2402.14027v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T22:26:56Z</div><div>Authors: Thomas E. Portegys</div><div style='padding-top: 10px; width: 80ex'>This is an examination of some methods that learn causations in event
sequences. A causation is defined as a conjunction of one or more cause events
occurring in an arbitrary order, with possible intervening non-causal events,
that lead to an effect. The methods include recurrent and non-recurrent
artificial neural networks (ANNs), as well as a histogram-based algorithm. An
attention recurrent ANN performed the best of the ANNs, while the histogram
algorithm was significantly superior to all the ANNs.</div><div><a href='http://arxiv.org/abs/2402.14027v1'>2402.14027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02602v2")'>Neural Causal Abstractions</div>
<div id='2401.02602v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T02:00:27Z</div><div>Authors: Kevin Xia, Elias Bareinboim</div><div style='padding-top: 10px; width: 80ex'>The abilities of humans to understand the world in terms of cause and effect
relationships, as well as to compress information into abstract concepts, are
two hallmark features of human intelligence. These two topics have been studied
in tandem in the literature under the rubric of causal abstractions theory. In
practice, it remains an open problem how to best leverage abstraction theory in
real-world causal inference tasks, where the true mechanisms are unknown and
only limited data is available. In this paper, we develop a new family of
causal abstractions by clustering variables and their domains. This approach
refines and generalizes previous notions of abstractions to better accommodate
individual causal distributions that are spawned by Pearl's causal hierarchy.
We show that such abstractions are learnable in practical settings through
Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning
toolkit to solve various challenging causal inference tasks -- identification,
estimation, sampling -- at different levels of granularity. Finally, we
integrate these results with representation learning to create more flexible
abstractions, moving these results closer to practical applications. Our
experiments support the theory and illustrate how to scale causal inferences to
high-dimensional settings involving image data.</div><div><a href='http://arxiv.org/abs/2401.02602v2'>2401.02602v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03726v2")'>Learning Granger Causality from Instance-wise Self-attentive Hawkes
  Processes</div>
<div id='2402.03726v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T05:46:51Z</div><div>Authors: Dongxia Wu, Tsuyoshi Idé, Aurélie Lozano, Georgios Kollias, Jiří Navrátil, Naoki Abe, Yi-An Ma, Rose Yu</div><div style='padding-top: 10px; width: 80ex'>We address the problem of learning Granger causality from asynchronous,
interdependent, multi-type event sequences. In particular, we are interested in
discovering instance-level causal structures in an unsupervised manner.
Instance-level causality identifies causal relationships among individual
events, providing more fine-grained information for decision-making. Existing
work in the literature either requires strong assumptions, such as linearity in
the intensity function, or heuristically defined model parameters that do not
necessarily meet the requirements of Granger causality. We propose
Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning
framework that can directly infer the Granger causality at the event instance
level. ISAHP is the first neural point process model that meets the
requirements of Granger causality. It leverages the self-attention mechanism of
the transformer to align with the principles of Granger causality. We
empirically demonstrate that ISAHP is capable of discovering complex
instance-level causal structures that cannot be handled by classical models. We
also show that ISAHP achieves state-of-the-art performance in proxy tasks
involving type-level causal discovery and instance-level event type prediction.</div><div><a href='http://arxiv.org/abs/2402.03726v2'>2402.03726v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.05969v1")'>Breaking Symmetry When Training Transformers</div>
<div id='2402.05969v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T00:32:28Z</div><div>Authors: Chunsheng Zuo, Michael Guerzhoy</div><div style='padding-top: 10px; width: 80ex'>As we show in this paper, the prediction for output token $n+1$ of
Transformer architectures without one of the mechanisms of positional encodings
and causal attention is invariant to permutations of input tokens $1, 2, ...,
n-1$. Usually, both mechanisms are employed and the symmetry with respect to
the input tokens is broken. Recently, it has been shown that one can train
Transformers without positional encodings. This must be enabled by the causal
attention mechanism. In this paper, we elaborate on the argument that the
causal connection mechanism must be responsible for the fact that Transformers
are able to model input sequences where the order is important. Vertical
"slices" of Transformers are all encouraged to represent the same location $k$
in the input sequence. We hypothesize that residual connections contribute to
this phenomenon, and demonstrate evidence for this.</div><div><a href='http://arxiv.org/abs/2402.05969v1'>2402.05969v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06963v1")'>The pitfalls of next-token prediction</div>
<div id='2403.06963v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T17:47:30Z</div><div>Authors: Gregor Bachmann, Vaishnavh Nagarajan</div><div style='padding-top: 10px; width: 80ex'>Can a mere next-token predictor faithfully model human intelligence? We
crystallize this intuitive concern, which is fragmented in the literature. As a
starting point, we argue that the two often-conflated phases of next-token
prediction -- autoregressive inference and teacher-forced training -- must be
treated distinctly. The popular criticism that errors can compound during
autoregressive inference, crucially assumes that teacher-forcing has learned an
accurate next-token predictor. This assumption sidesteps a more deep-rooted
problem we expose: in certain classes of tasks, teacher-forcing can simply fail
to learn an accurate next-token predictor in the first place. We describe a
general mechanism of how teacher-forcing can fail, and design a minimal
planning task where both the Transformer and the Mamba architecture empirically
fail in that manner -- remarkably, despite the task being straightforward to
learn. We provide preliminary evidence that this failure can be resolved when
training to predict multiple tokens in advance. We hope this finding can ground
future debates and inspire explorations beyond the next-token prediction
paradigm. We make our code available under
https://github.com/gregorbachmann/Next-Token-Failures</div><div><a href='http://arxiv.org/abs/2403.06963v1'>2403.06963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13838v1")'>Circuit Transformer: End-to-end Circuit Design by Predicting the Next
  Gate</div>
<div id='2403.13838v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T03:24:14Z</div><div>Authors: Xihan Li, Xing Li, Lei Chen, Xing Zhang, Mingxuan Yuan, Jun Wang</div><div style='padding-top: 10px; width: 80ex'>Language, a prominent human ability to express through sequential symbols,
has been computationally mastered by recent advances of large language models
(LLMs). By predicting the next word recurrently with huge neural models, LLMs
have shown unprecedented capabilities in understanding and reasoning. Circuit,
as the "language" of electronic design, specifies the functionality of an
electronic device by cascade connections of logic gates. Then, can circuits
also be mastered by a a sufficiently large "circuit model", which can conquer
electronic design tasks by simply predicting the next logic gate? In this work,
we take the first step to explore such possibilities. Two primary barriers
impede the straightforward application of LLMs to circuits: their complex,
non-sequential structure, and the intolerance of hallucination due to strict
constraints (e.g., equivalence). For the first barrier, we encode a circuit as
a memory-less, depth-first traversal trajectory, which allows Transformer-based
neural models to better leverage its structural information, and predict the
next gate on the trajectory as a circuit model. For the second barrier, we
introduce an equivalence-preserving decoding process, which ensures that every
token in the generated trajectory adheres to the specified equivalence
constraints. Moreover, the circuit model can also be regarded as a stochastic
policy to tackle optimization-oriented circuit design tasks. Experimentally, we
trained a Transformer-based model of 88M parameters, named "Circuit
Transformer", which demonstrates impressive performance in end-to-end logic
synthesis. With Monte-Carlo tree search, Circuit Transformer significantly
improves over resyn2 while retaining strict equivalence, showcasing the
potential of generative AI in conquering electronic design challenges.</div><div><a href='http://arxiv.org/abs/2403.13838v1'>2403.13838v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02352v1")'>ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys</div>
<div id='2403.02352v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T19:24:37Z</div><div>Authors: Yue Niu, Saurav Prakash, Salman Avestimehr</div><div style='padding-top: 10px; width: 80ex'>We propose a new attention mechanism with linear complexity, ATP, that
fixates \textbf{A}ttention on \textbf{T}op \textbf{P}rincipal keys, rather than
on each individual token. Particularly, ATP is driven by an important
observation that input sequences are typically low-rank, i.e., input sequences
can be represented by a few principal bases. Therefore, instead of directly
iterating over all the input tokens, ATP transforms inputs into an orthogonal
space and computes attention only on the top principal bases (keys). Owing to
the observed low-rank structure in input sequences, ATP is able to capture
semantic relationships in input sequences with a few principal keys.
Furthermore, the attention complexity is reduced from \emph{quadratic} to
\emph{linear} without incurring a noticeable performance drop. ATP further
reduces complexity for other linear layers with low-rank inputs, leading to
more speedup compared to prior works that solely target the attention module.
Our evaluations on various models (e.g., BERT and Llama) demonstrate that ATP
achieves comparable accuracy with much lower computation and memory complexity
than the standard attention mechanism. In particular, ATP barely loses accuracy
with only $1/2$ principal keys, and only incurs around $2\%$ accuracy drops
with $1/4$ principal keys.</div><div><a href='http://arxiv.org/abs/2403.02352v1'>2403.02352v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01325v2")'>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning</div>
<div id='2401.01325v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T18:30:51Z</div><div>Authors: Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu</div><div style='padding-top: 10px; width: 80ex'>It is well known that LLMs cannot generalize well to long contexts whose
lengths are larger than the training sequence length. This poses challenges
when employing LLMs for processing long input sequences during inference. In
this work, we argue that LLMs themselves have inherent capabilities to handle
long contexts without fine-tuning. To achieve this goal, we propose SelfExtend
to extend the context window of LLMs by constructing bi-level attention
information: the grouped attention and the neighbor attention. The grouped
attention captures the dependencies among tokens that are far apart, while
neighbor attention captures dependencies among adjacent tokens within a
specified range. The two-level attentions are computed based on the original
model's self-attention mechanism during inference. With minor code
modification, our SelfExtend can effortlessly extend existing LLMs' context
window without any fine-tuning. We conduct comprehensive experiments on
multiple benchmarks and the results show that our SelfExtend can effectively
extend existing LLMs' context window length. The code can be found at
\url{https://github.com/datamllab/LongLM}.</div><div><a href='http://arxiv.org/abs/2401.01325v2'>2401.01325v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00522v3")'>Understanding the Expressive Power and Mechanisms of Transformer for
  Sequence Modeling</div>
<div id='2402.00522v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T11:43:13Z</div><div>Authors: Mingze Wang, Weinan E</div><div style='padding-top: 10px; width: 80ex'>We conduct a systematic study of the approximation properties of Transformer
for sequence modeling with long, sparse and complicated memory. We investigate
the mechanisms through which different components of Transformer, such as the
dot-product self-attention, positional encoding and feed-forward layer, affect
its expressive power, and we study their combined effects through establishing
explicit approximation rates. Our study reveals the roles of critical
parameters in the Transformer, such as the number of layers and the number of
attention heads, and these insights also provide natural suggestions for
alternative architectures.</div><div><a href='http://arxiv.org/abs/2402.00522v3'>2402.00522v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17574v1")'>Scavenging Hyena: Distilling Transformers into Long Convolution Models</div>
<div id='2401.17574v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T03:39:07Z</div><div>Authors: Tokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, Laurence Liang</div><div style='padding-top: 10px; width: 80ex'>The rapid evolution of Large Language Models (LLMs), epitomized by
architectures like GPT-4, has reshaped the landscape of natural language
processing. This paper introduces a pioneering approach to address the
efficiency concerns associated with LLM pre-training, proposing the use of
knowledge distillation for cross-architecture transfer. Leveraging insights
from the efficient Hyena mechanism, our method replaces attention heads in
transformer models by Hyena, offering a cost-effective alternative to
traditional pre-training while confronting the challenge of processing long
contextual information, inherent in quadratic attention mechanisms. Unlike
conventional compression-focused methods, our technique not only enhances
inference speed but also surpasses pre-training in terms of both accuracy and
efficiency. In the era of evolving LLMs, our work contributes to the pursuit of
sustainable AI solutions, striking a balance between computational power and
environmental impact.</div><div><a href='http://arxiv.org/abs/2401.17574v1'>2401.17574v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18508v1")'>Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling</div>
<div id='2402.18508v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:36:45Z</div><div>Authors: Mahdi Karami, Ali Ghodsi</div><div style='padding-top: 10px; width: 80ex'>In the rapidly evolving landscape of deep learning, the quest for models that
balance expressivity with computational efficiency has never been more
critical. This paper introduces Orchid, a novel architecture that reimagines
sequence modeling by incorporating a new data-dependent convolution mechanism.
Orchid is designed to address the inherent limitations of traditional attention
mechanisms, particularly their quadratic complexity, without compromising the
ability to capture long-range dependencies and in-context learning. At the core
of Orchid lies the data-dependent convolution layer, which dynamically adjusts
its kernel conditioned on input data using a dedicated conditioning neural
network. We design two simple conditioning networks that maintain shift
equivariance in the adaptive convolution operation. The dynamic nature of
data-dependent convolution kernel, coupled with gating operations, grants
Orchid high expressivity while maintaining efficiency and quasilinear
scalability for long sequences. We rigorously evaluate Orchid across multiple
domains, including language modeling and image classification, to showcase its
performance and generality. Our experiments demonstrate that Orchid
architecture not only outperforms traditional attention-based architectures
such as BERT and Vision Transformers with smaller model sizes, but also extends
the feasible sequence length beyond the limitations of the dense attention
layers. This achievement represents a significant step towards more efficient
and scalable deep learning models for sequence modeling.</div><div><a href='http://arxiv.org/abs/2402.18508v1'>2402.18508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00818v2")'>DenseMamba: State Space Models with Dense Hidden Connection for
  Efficient Large Language Models</div>
<div id='2403.00818v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T09:21:59Z</div><div>Authors: Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) face a daunting challenge due to the excessive
computational and memory requirements of the commonly used Transformer
architecture. While state space model (SSM) is a new type of foundational
network architecture offering lower computational complexity, their performance
has yet to fully rival that of Transformers. This paper introduces DenseSSM, a
novel approach to enhance the flow of hidden information between layers in
SSMs. By selectively integrating shallowlayer hidden states into deeper layers,
DenseSSM retains fine-grained information crucial for the final output. Dense
connections enhanced DenseSSM still maintains the training parallelizability
and inference efficiency. The proposed method can be widely applicable to
various SSM types like RetNet and Mamba. With similar model size, DenseSSM
achieves significant improvements, exemplified by DenseRetNet outperforming the
original RetNet with up to 5% accuracy improvement on public benchmarks. code
is avalaible at https://github.com/WailordHe/DenseSSM</div><div><a href='http://arxiv.org/abs/2403.00818v2'>2403.00818v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12688v1")'>SEVEN: Pruning Transformer Model by Reserving Sentinels</div>
<div id='2403.12688v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T12:47:43Z</div><div>Authors: Jinying Xiao, Ping Li, Jie Nie, Zhe Tang</div><div style='padding-top: 10px; width: 80ex'>Large-scale Transformer models (TM) have demonstrated outstanding performance
across various tasks. However, their considerable parameter size restricts
their applicability, particularly on mobile devices. Due to the dynamic and
intricate nature of gradients on TM compared to Convolutional Neural Networks,
commonly used pruning methods tend to retain weights with larger gradient
noise. This results in pruned models that are sensitive to sparsity and
datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general
approach for training and fine-tuning TM. In this paper, we attempt to describe
the noisy batch gradient sequences on TM through the cumulative process of SD.
We utilize this design to dynamically assess the importance scores of
weights.SEVEN is introduced by us, which particularly favors weights with
consistently high sensitivity, i.e., weights with small gradient noise. These
weights are tended to be preserved by SEVEN. Extensive experiments on various
TM in natural language, question-answering, and image classification domains
are conducted to validate the effectiveness of SEVEN. The results demonstrate
significant improvements of SEVEN in multiple pruning scenarios and across
different sparsity levels. Additionally, SEVEN exhibits robust performance
under various fine-tuning strategies. The code is publicly available at
https://github.com/xiaojinying/SEVEN.</div><div><a href='http://arxiv.org/abs/2403.12688v1'>2403.12688v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09054v1")'>Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient
  Generative Inference</div>
<div id='2403.09054v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T02:42:42Z</div><div>Authors: Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath</div><div style='padding-top: 10px; width: 80ex'>Transformers have emerged as the underpinning architecture for Large Language
Models (LLMs). In generative language models, the inference process involves
two primary phases: prompt processing and token generation. Token generation,
which constitutes the majority of the computational workload, primarily entails
vector-matrix multiplications and interactions with the Key-Value (KV) Cache.
This phase is constrained by memory bandwidth due to the overhead of
transferring weights and KV cache values from the memory system to the
computing units. This memory bottleneck becomes particularly pronounced in
applications that require long-context and extensive text generation, both of
which are increasingly crucial for LLMs.
  This paper introduces "Keyformer", an innovative inference-time approach, to
mitigate the challenges associated with KV cache size and memory bandwidth
utilization. Keyformer leverages the observation that approximately 90% of the
attention weight in generative inference focuses on a specific subset of
tokens, referred to as "key" tokens. Keyformer retains only the key tokens in
the KV cache by identifying these crucial tokens using a novel score function.
This approach effectively reduces both the KV cache size and memory bandwidth
usage without compromising model accuracy. We evaluate Keyformer's performance
across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ
various positional embedding algorithms. Our assessment encompasses a variety
of tasks, with a particular emphasis on summarization and conversation tasks
involving extended contexts. Keyformer's reduction of KV cache reduces
inference latency by 2.1x and improves token generation throughput by 2.4x,
while preserving the model's accuracy.</div><div><a href='http://arxiv.org/abs/2403.09054v1'>2403.09054v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03512v3")'>CharPoet: A Chinese Classical Poetry Generation System Based on
  Token-free LLM</div>
<div id='2401.03512v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T15:00:36Z</div><div>Authors: Chengyue Yu, Lei Zang, Jiaotuan Wang, Chenyi Zhuang, Jinjie Gu</div><div style='padding-top: 10px; width: 80ex'>Automatic Chinese classical poetry generation has attracted much research
interest, but achieving effective control over format and content
simultaneously remains challenging. Traditional systems usually accept keywords
as user inputs, resulting in limited control over content. Large language
models (LLMs) improve content control by allowing unrestricted user
instructions, but the token-by-token generation process frequently makes format
errors. Motivated by this, we propose CharPoet, a Chinese classical poetry
generation system based on token-free LLM, which provides effective control
over both format and content. Our token-free architecture generates in a
character-by-character manner, enabling precise control over the number of
characters. Pruned from existing token-based LLMs, CharPoet inherits their
pretrained capabilities and can generate poetry following instructions like
"Write me a poem for my mother's birthday." CharPoet achieves format accuracy
above 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of
content quality, CharPoet surpasses traditional systems including Jiuge, and is
comparable to other LLMs. Our system is open source and available at
https://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of
CharPoet is available at https://youtu.be/voZ25qEp3Dc.</div><div><a href='http://arxiv.org/abs/2401.03512v3'>2401.03512v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10787v1")'>EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for
  the Acceleration of Lightweight LLMs on the Edge</div>
<div id='2402.10787v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T16:10:38Z</div><div>Authors: Xuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam Leeser, Pu Zhao, Yanzhi Wang</div><div style='padding-top: 10px; width: 80ex'>Despite the remarkable strides of Large Language Models (LLMs) in various
fields, the wide applications of LLMs on edge devices are limited due to their
massive parameters and computations. To address this, quantization is commonly
adopted to generate lightweight LLMs with efficient computations and fast
inference. However, Post-Training Quantization (PTQ) methods dramatically
degrade in quality when quantizing weights, activations, and KV cache together
to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize
model weights, leaving the activations untouched, which do not fully exploit
the potential of quantization for inference acceleration on the edge. In this
paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the
optimization of lightweight LLMs to achieve inference acceleration on Edge
devices. We first identify that the performance drop of quantization primarily
stems from the information distortion in quantized attention maps, demonstrated
by the different distributions in quantized query and key of the self-attention
mechanism. Then, the entropy and distribution guided QAT is proposed to
mitigate the information distortion. Moreover, we design a token
importance-aware adaptive method to dynamically quantize the tokens with
different bit widths for further optimization and acceleration. Our extensive
experiments verify the substantial improvements with our framework across
various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x
compared with its FP16 counterparts across multiple edge devices, signaling a
groundbreaking advancement.</div><div><a href='http://arxiv.org/abs/2402.10787v1'>2402.10787v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01911v1")'>From PEFT to DEFT: Parameter Efficient Finetuning for Reducing
  Activation Density in Transformers</div>
<div id='2402.01911v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T21:25:46Z</div><div>Authors: Bharat Runwal, Tejaswini Pedapati, Pin-Yu Chen</div><div style='padding-top: 10px; width: 80ex'>Pretrained Language Models (PLMs) have become the de facto starting point for
fine-tuning on downstream tasks. However, as model sizes continue to increase,
traditional fine-tuning of all parameters becomes challenging. To address this,
parameter-efficient fine-tuning (PEFT) methods have gained popularity as a
means to adapt PLMs effectively. In parallel, recent studies have revealed the
presence of activation sparsity within the intermediate outputs of the
multilayer perception (MLP) blocks in transformers. Low activation density
enables efficient model inference on sparsity-aware hardware. Building upon
this insight, in this work, we propose a novel density loss that encourages
higher activation sparsity (equivalently, lower activation density) in the
pre-trained models. We demonstrate the effectiveness of our approach by
utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter,
Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse
downstream tasks. Experiments show that our proposed method DEFT,
Density-Efficient Fine-Tuning, can reduce the activation density consistently
and up to $\boldsymbol{50.72\%}$ on RoBERTa$_\mathrm{Large}$, and $\boldsymbol
{53.19\%}$ (encoder density) and $\boldsymbol{90.60\%}$ (decoder density) on
Flan-T5$_\mathrm{XXL}$ ($\boldsymbol{11B}$) compared to PEFT using GLUE and QA
(SQuAD) benchmarks respectively while maintaining competitive performance on
downstream tasks. We also showcase that DEFT works complementary with quantized
and pruned models</div><div><a href='http://arxiv.org/abs/2402.01911v1'>2402.01911v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08958v1")'>Towards Next-Level Post-Training Quantization of Hyper-Scale
  Transformers</div>
<div id='2402.08958v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T05:58:43Z</div><div>Authors: Junhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon</div><div style='padding-top: 10px; width: 80ex'>With the increasing complexity of generative AI models, post-training
quantization (PTQ) has emerged as a promising solution for deploying
hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ
schemes, however, consume considerable time and resources, which could be a
bottleneck in real situations where frequent model updates and multiple
hyper-parameter tunings are required. As a cost-effective alternative, one-shot
PTQ schemes have been proposed. Still, the performance is somewhat limited
because they cannot consider the inter-layer dependency within the attention
module, which is a very important feature of Transformers. In this paper, we
thus propose a novel PTQ algorithm that balances accuracy and efficiency. The
key idea of the proposed algorithm called aespa is to perform quantization
layer-wise for efficiency while considering cross-layer dependency to preserve
the attention score. Through extensive experiments on various language models
and complexity analysis, we demonstrate that aespa is accurate and efficient in
quantizing Transformer models.</div><div><a href='http://arxiv.org/abs/2402.08958v1'>2402.08958v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04797v1")'>Found in the Middle: How Language Models Use Long Contexts Better via
  Plug-and-Play Positional Encoding</div>
<div id='2403.04797v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T04:58:37Z</div><div>Authors: Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, Zhangyang Wang</div><div style='padding-top: 10px; width: 80ex'>This paper aims to overcome the "lost-in-the-middle" challenge of large
language models (LLMs). While recent advancements have successfully enabled
LLMs to perform stable language modeling with up to 4 million tokens, the
persistent difficulty faced by most LLMs in identifying relevant information
situated in the middle of the context has not been adequately tackled. To
address this problem, this paper introduces Multi-scale Positional Encoding
(Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the
capacity of LLMs to handle the relevant information located in the middle of
the context, without fine-tuning or introducing any additional overhead. Ms-PoE
leverages the position indice rescaling to relieve the long-term decay effect
introduced by RoPE, while meticulously assigning distinct scaling ratios to
different attention heads to preserve essential knowledge learned during the
pre-training step, forming a multi-scale context fusion from short to long
distance. Extensive experiments with a wide range of LLMs demonstrate the
efficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of
up to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are
available at https://github.com/VITA-Group/Ms-PoE.</div><div><a href='http://arxiv.org/abs/2403.04797v1'>2403.04797v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15319v1")'>GPTVQ: The Blessing of Dimensionality for LLM Quantization</div>
<div id='2402.15319v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T13:39:16Z</div><div>Authors: Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul Whatmough</div><div style='padding-top: 10px; width: 80ex'>In this work we show that the size versus accuracy trade-off of neural
network quantization can be significantly improved by increasing the
quantization dimensionality. We propose the GPTVQ method, a new fast method for
post-training vector quantization (VQ) that scales well to Large Language
Models (LLMs). Our method interleaves quantization of one or more columns with
updates to the remaining unquantized weights, using information from the
Hessian of the per-layer output reconstruction MSE. Quantization codebooks are
initialized using an efficient data-aware version of the EM algorithm. The
codebooks are then updated, and further compressed by using integer
quantization and SVD-based compression. GPTVQ establishes a new state-of-the
art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2
and Mistral. Furthermore, our method is efficient: on a single H100 it takes
between 3 and 11 hours to process a Llamav2-70B model, depending on
quantization setting. Lastly, with on-device timings for VQ decompression on a
mobile CPU we show that VQ leads to improved latency compared to using a 4-bit
integer format.</div><div><a href='http://arxiv.org/abs/2402.15319v1'>2402.15319v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14999v1")'>Magic for the Age of Quantized DNNs</div>
<div id='2403.14999v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T07:21:09Z</div><div>Authors: Yoshihide Sawada, Ryuji Saiin, Kazuma Suetake</div><div style='padding-top: 10px; width: 80ex'>Recently, the number of parameters in DNNs has explosively increased, as
exemplified by LLMs (Large Language Models), making inference on small-scale
computers more difficult. Model compression technology is, therefore, essential
for integration into products. In this paper, we propose a method of
quantization-aware training. We introduce a novel normalization (Layer-Batch
Normalization) that is independent of the mini-batch size and does not require
any additional computation cost during inference. Then, we quantize the weights
by the scaled round-clip function with the weight standardization. We also
quantize activation functions using the same function and apply surrogate
gradients to train the model with both quantized weights and the quantized
activation functions. We call this method Magic for the age of Quantised DNNs
(MaQD). Experimental results show that our quantization method can be achieved
with minimal accuracy degradation.</div><div><a href='http://arxiv.org/abs/2403.14999v1'>2403.14999v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14732v1")'>Residual Quantization with Implicit Neural Codebooks</div>
<div id='2401.14732v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T09:42:51Z</div><div>Authors: Iris Huijben, Matthijs Douze, Matthew Muckley, Ruud van Sloun, Jakob Verbeek</div><div style='padding-top: 10px; width: 80ex'>Vector quantization is a fundamental operation for data compression and
vector search. To obtain high accuracy, multi-codebook methods increase the
rate by representing each vector using codewords across multiple codebooks.
Residual quantization (RQ) is one such method, which increases accuracy by
iteratively quantizing the error of the previous step. The error distribution
is dependent on previously selected codewords. This dependency is, however, not
accounted for in conventional RQ as it uses a generic codebook per quantization
step. In this paper, we propose QINCo, a neural RQ variant which predicts
specialized codebooks per vector using a neural network that is conditioned on
the approximation of the vector from previous steps. Experiments show that
QINCo outperforms state-of-the-art methods by a large margin on several
datasets and code sizes. For example, QINCo achieves better nearest-neighbor
search accuracy using 12 bytes codes than other methods using 16 bytes on the
BigANN and Deep1B dataset.</div><div><a href='http://arxiv.org/abs/2401.14732v1'>2401.14732v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07134v1")'>COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization</div>
<div id='2403.07134v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:04:03Z</div><div>Authors: Aozhong Zhang, Zi Yang, Naigang Wang, Yingyong Qin, Jack Xin, Xin Li, Penghang Yin</div><div style='padding-top: 10px; width: 80ex'>Post-training quantization (PTQ) has emerged as a practical approach to
compress large neural networks, making them highly efficient for deployment.
However, effectively reducing these models to their low-bit counterparts
without compromising the original accuracy remains a key challenge. In this
paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially
conducts coordinate-wise minimization of the layer-wise reconstruction errors.
We consider the widely used integer quantization, where every quantized weight
can be decomposed into a shared floating-point scalar and an integer bit-code.
Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as
the variables of the reconstruction error. Every iteration improves this error
along a single coordinate while keeping all other variables constant. COMQ is
easy to use and requires no hyper-parameter tuning. It instead involves only
dot products and rounding operations. We update these variables in a carefully
designed greedy order, significantly enhancing the accuracy. COMQ achieves
remarkable results in quantizing 4-bit Vision Transformers, with a negligible
loss of less than 1% in Top-1 accuracy. In 4-bit INT quantization of
convolutional neural networks, COMQ maintains near-lossless accuracy with a
minimal drop of merely 0.3% in Top-1 accuracy.</div><div><a href='http://arxiv.org/abs/2403.07134v1'>2403.07134v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06082v1")'>FrameQuant: Flexible Low-Bit Quantization for Transformers</div>
<div id='2403.06082v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T04:01:49Z</div><div>Authors: Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, Vikas Singh</div><div style='padding-top: 10px; width: 80ex'>Transformers are the backbone of powerful foundation models for many Vision
and Natural Language Processing tasks. But their compute and memory/storage
footprint is large, and so, serving such models is expensive often requiring
high-end hardware. To mitigate this difficulty, Post-Training Quantization
seeks to modify a pre-trained model and quantize it to eight bits or lower,
significantly boosting compute/memory/latency efficiency. Such models have been
successfully quantized to four bits with some performance loss. In this work,
we outline a simple scheme to quantize Transformer-based models to just two
bits (plus some overhead) with only a small drop in accuracy. Key to our
formulation is a concept borrowed from Harmonic analysis called Fusion Frames.
Our main finding is that the quantization must take place not in the original
weight space, but instead in the Fusion Frame representations. If quantization
is interpreted as the addition of noise, our casting of the problem allows
invoking an extensive body of known consistent recovery and noise robustness
guarantees. Further, if desired, de-noising filters are known in closed form.
We show empirically, via a variety of experiments, that (almost) two-bit
quantization for Transformer models promises sizable efficiency gains.</div><div><a href='http://arxiv.org/abs/2403.06082v1'>2403.06082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07200v1")'>Outlier-Aware Training for Low-Bit Quantization of Structural
  Re-Parameterized Networks</div>
<div id='2402.07200v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T13:26:40Z</div><div>Authors: Muqun Niu, Yuan Ren, Boyu Li, Chenchen Ding</div><div style='padding-top: 10px; width: 80ex'>Lightweight design of Convolutional Neural Networks (CNNs) requires co-design
efforts in the model architectures and compression techniques. As a novel
design paradigm that separates training and inference, a structural
re-parameterized (SR) network such as the representative RepVGG revitalizes the
simple VGG-like network with a high accuracy comparable to advanced and often
more complicated networks. However, the merging process in SR networks
introduces outliers into weights, making their distribution distinct from
conventional networks and thus heightening difficulties in quantization. To
address this, we propose an operator-level improvement for training called
Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of
limited bitwidths while upkeeping the inference accuracy, we develop a
clustering-based non-uniform quantization framework for Quantization-Aware
Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the
quantized performance of RepVGG is largely enhanced, particularly when the
bitwidth falls below 8.</div><div><a href='http://arxiv.org/abs/2402.07200v1'>2402.07200v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17710v1")'>Understanding Neural Network Binarization with Forward and Backward
  Proximal Quantizers</div>
<div id='2402.17710v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:43:51Z</div><div>Authors: Yiwei Lu, Yaoliang Yu, Xinlin Li, Vahid Partovi Nia</div><div style='padding-top: 10px; width: 80ex'>In neural network binarization, BinaryConnect (BC) and its variants are
considered the standard. These methods apply the sign function in their forward
pass and their respective gradients are backpropagated to update the weights.
However, the derivative of the sign function is zero whenever defined, which
consequently freezes training. Therefore, implementations of BC (e.g., BNN)
usually replace the derivative of sign in the backward computation with
identity or other approximate gradient alternatives. Although such practice
works well empirically, it is largely a heuristic or ''training trick.'' We aim
at shedding some light on these training tricks from the optimization
perspective. Building from existing theory on ProxConnect (PC, a generalization
of BC), we (1) equip PC with different forward-backward quantizers and obtain
ProxConnect++ (PC++) that includes existing binarization techniques as special
cases; (2) derive a principled way to synthesize forward-backward quantizers
with automatic theoretical guarantees; (3) illustrate our theory by proposing
an enhanced binarization algorithm BNN++; (4) conduct image classification
experiments on CNNs and vision transformers, and empirically verify that BNN++
generally achieves competitive results on binarizing these models.</div><div><a href='http://arxiv.org/abs/2402.17710v1'>2402.17710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04884v1")'>Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural
  Networks</div>
<div id='2403.04884v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T20:16:42Z</div><div>Authors: Yuli Wu, Julian Wittmann, Peter Walter, Johannes Stegmaier</div><div style='padding-top: 10px; width: 80ex'>Implantable retinal prostheses offer a promising solution to restore partial
vision by circumventing damaged photoreceptor cells in the retina and directly
stimulating the remaining functional retinal cells. However, the information
transmission between the camera and retinal cells is often limited by the low
resolution of the electrode array and the lack of specificity for different
ganglion cell types, resulting in suboptimal stimulations. In this work, we
propose to utilize normalizing flow-based conditional invertible neural
networks to optimize retinal implant stimulation in an unsupervised manner. The
invertibility of these networks allows us to use them as a surrogate for the
computational model of the visual system, while also encoding input camera
signals into optimized electrical stimuli on the electrode array. Compared to
other methods, such as trivial downsampling, linear models, and feed-forward
convolutional neural networks, the flow-based invertible neural network and its
conditional extension yield better visual reconstruction qualities w.r.t.
various metrics using a physiologically validated simulation tool.</div><div><a href='http://arxiv.org/abs/2403.04884v1'>2403.04884v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14324v1")'>Neural Network-Based Processing and Reconstruction of Compromised
  Biophotonic Image Data</div>
<div id='2403.14324v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T11:44:25Z</div><div>Authors: Michael John Fanous, Paloma Casteleiro Costa, Cagatay Isil, Luzhe Huang, Aydogan Ozcan</div><div style='padding-top: 10px; width: 80ex'>The integration of deep learning techniques with biophotonic setups has
opened new horizons in bioimaging. A compelling trend in this field involves
deliberately compromising certain measurement metrics to engineer better
bioimaging tools in terms of cost, speed, and form-factor, followed by
compensating for the resulting defects through the utilization of deep learning
models trained on a large amount of ideal, superior or alternative data. This
strategic approach has found increasing popularity due to its potential to
enhance various aspects of biophotonic imaging. One of the primary motivations
for employing this strategy is the pursuit of higher temporal resolution or
increased imaging speed, critical for capturing fine dynamic biological
processes. This approach also offers the prospect of simplifying hardware
requirements/complexities, thereby making advanced imaging standards more
accessible in terms of cost and/or size. This article provides an in-depth
review of the diverse measurement aspects that researchers intentionally impair
in their biophotonic setups, including the point spread function,
signal-to-noise ratio, sampling density, and pixel resolution. By deliberately
compromising these metrics, researchers aim to not only recuperate them through
the application of deep learning networks, but also bolster in return other
crucial parameters, such as the field-of-view, depth-of-field, and
space-bandwidth product. Here, we discuss various biophotonic methods that have
successfully employed this strategic approach. These techniques span broad
applications and showcase the versatility and effectiveness of deep learning in
the context of compromised biophotonic data. Finally, by offering our
perspectives on the future possibilities of this rapidly evolving concept, we
hope to motivate our readers to explore novel ways of balancing hardware
compromises with compensation via AI.</div><div><a href='http://arxiv.org/abs/2403.14324v1'>2403.14324v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03739v1")'>A&amp;B BNN: Add&amp;Bit-Operation-Only Hardware-Friendly Binary Neural Network</div>
<div id='2403.03739v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T14:28:49Z</div><div>Authors: Ruichen Ma, Guanchao Qiao, Yian Liu, Liwei Meng, Ning Ning, Yang Liu, Shaogang Hu</div><div style='padding-top: 10px; width: 80ex'>Binary neural networks utilize 1-bit quantized weights and activations to
reduce both the model's storage demands and computational burden. However,
advanced binary architectures still incorporate millions of inefficient and
nonhardware-friendly full-precision multiplication operations. A&amp;B BNN is
proposed to directly remove part of the multiplication operations in a
traditional BNN and replace the rest with an equal number of bit operations,
introducing the mask layer and the quantized RPReLU structure based on the
normalizer-free network architecture. The mask layer can be removed during
inference by leveraging the intrinsic characteristics of BNN with
straightforward mathematical transformations to avoid the associated
multiplication operations. The quantized RPReLU structure enables more
efficient bit operations by constraining its slope to be integer powers of 2.
Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10,
CIFAR-100, and ImageNet datasets, respectively, which are competitive with the
state-of-the-art. Ablation studies have verified the efficacy of the quantized
RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to
using a fixed slope RLeakyReLU. The proposed add&amp;bit-operation-only BNN offers
an innovative approach for hardware-friendly network architecture.</div><div><a href='http://arxiv.org/abs/2403.03739v1'>2403.03739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18595v1")'>EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural
  Network Acceleration</div>
<div id='2402.18595v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T09:35:30Z</div><div>Authors: Bo Liu, Grace Li Zhang, Xunzhao Yin, Ulf Schlichtmann, Bing Li</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks (DNNs) have achieved great breakthroughs in many fields
such as image classification and natural language processing. However, the
execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC)
operations on hardware and thus incurs a large power consumption. To address
this challenge, we propose a novel digital MAC design based on encoding. In
this new design, the multipliers are replaced by simple logic gates to project
the results onto a wide bit representation. These bits carry individual
position weights, which can be trained for specific neural networks to enhance
inference accuracy. The outputs of the new multipliers are added by bit-wise
weighted accumulation and the accumulation results are compatible with existing
computing platforms accelerating neural networks with either uniform or
non-uniform quantization. Since the multiplication function is replaced by
simple logic projection, the critical paths in the resulting circuits become
much shorter. Correspondingly, pipelining stages in the MAC array can be
reduced, leading to a significantly smaller area as well as a better power
efficiency. The proposed design has been synthesized and verified by
ResNet18-Cifar10, ResNet20-Cifar100 and ResNet50-ImageNet. The experimental
results confirmed the reduction of circuit area by up to 79.63% and the
reduction of power consumption of executing DNNs by up to 70.18%, while the
accuracy of the neural networks can still be well maintained.</div><div><a href='http://arxiv.org/abs/2402.18595v1'>2402.18595v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.15024v2")'>SliceGPT: Compress Large Language Models by Deleting Rows and Columns</div>
<div id='2401.15024v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T17:35:45Z</div><div>Authors: Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James Hensman</div><div style='padding-top: 10px; width: 80ex'>Large language models have become the cornerstone of natural language
processing, but their use comes with substantial costs in terms of compute and
memory resources. Sparsification provides a solution to alleviate these
resource constraints, and recent works have shown that trained models can be
sparsified post-hoc. Existing sparsification techniques face challenges as they
need additional data structures and offer constrained speedup with current
hardware. In this paper we present SliceGPT, a new post-training sparsification
scheme which replaces each weight matrix with a smaller (dense) matrix,
reducing the embedding dimension of the network. Through extensive
experimentation, we show that SliceGPT can remove up to 25% of the model
parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models
while maintaining 99%, 99% and 90% zero-shot task performance of the dense
model respectively. Our sliced models run on fewer GPUs and run faster without
any additional code optimization: on 24GB consumer GPUs we reduce the total
compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB
A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance
in transformer networks, which enables SliceGPT and we hope it will inspire and
enable future avenues to reduce memory and computation demands for pre-trained
models. Code is available at:
https://github.com/microsoft/TransformerCompression</div><div><a href='http://arxiv.org/abs/2401.15024v2'>2401.15024v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05964v1")'>A Survey on Transformer Compression</div>
<div id='2402.05964v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:16:28Z</div><div>Authors: Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Large models based on the Transformer architecture play increasingly vital
roles in artificial intelligence, particularly within the realms of natural
language processing (NLP) and computer vision (CV). Model compression methods
reduce their memory and computational cost, which is a necessary step to
implement the transformer models on practical devices. Given the unique
architecture of transformer, featuring alternative attention and Feedforward
Neural Network (FFN) modules, specific compression techniques are required. The
efficiency of these compression methods is also paramount, as it is usually
impractical to retrain large models on the entire training dataset.This survey
provides a comprehensive review of recent compression methods, with a specific
focus on their application to transformer models. The compression methods are
primarily categorized into pruning, quantization, knowledge distillation, and
efficient architecture design. In each category, we discuss compression methods
for both CV and NLP tasks, highlighting common underlying principles. At last,
we delve into the relation between various compression methods, and discuss the
further directions in this domain.</div><div><a href='http://arxiv.org/abs/2402.05964v1'>2402.05964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05772v1")'>Knowledge Translation: A New Pathway for Model Compression</div>
<div id='2401.05772v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T09:25:42Z</div><div>Authors: Wujie Sun, Defang Chen, Jiawei Chen, Yan Feng, Chun Chen, Can Wang</div><div style='padding-top: 10px; width: 80ex'>Deep learning has witnessed significant advancements in recent years at the
cost of increasing training, inference, and model storage overhead. While
existing model compression methods strive to reduce the number of model
parameters while maintaining high accuracy, they inevitably necessitate the
re-training of the compressed model or impose architectural constraints. To
overcome these limitations, this paper presents a novel framework, termed
\textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model
is trained to receive the parameters of a larger model and generate compressed
parameters. The concept of KT draws inspiration from language translation,
which effectively employs neural networks to convert different languages,
maintaining identical meaning. Accordingly, we explore the potential of neural
networks to convert models of disparate sizes, while preserving their
functionality. We propose a comprehensive framework for KT, introduce data
augmentation strategies to enhance model performance despite restricted
training data, and successfully demonstrate the feasibility of KT on the MNIST
dataset. Code is available at \url{https://github.com/zju-SWJ/KT}.</div><div><a href='http://arxiv.org/abs/2401.05772v1'>2401.05772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14112v2")'>FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric
  Algorithm-System Co-Design</div>
<div id='2401.14112v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:46:38Z</div><div>Authors: Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song</div><div style='padding-top: 10px; width: 80ex'>Six-bit quantization (FP6) can effectively reduce the size of large language
models (LLMs) and preserve the model quality consistently across varied
applications. However, existing systems do not provide Tensor Core support for
FP6 quantization and struggle to achieve practical performance improvements
during LLM inference. It is challenging to support FP6 quantization on GPUs due
to (1) unfriendly memory access of model weights with irregular bit-width and
(2) high runtime overhead of weight de-quantization. To address these problems,
we propose TC-FPx, the first full-stack GPU kernel design scheme with unified
Tensor Core support of float-point weights for various quantization bit-width.
We integrate TC-FPx kernel into an existing inference system, providing new
end-to-end support (called FP6-LLM) for quantized LLM inference, where better
trade-offs between inference cost and model quality are achieved. Experiments
show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,
achieving 1.69x-2.65x higher normalized inference throughput than the FP16
baseline. The source code is publicly available at
https://github.com/usyd-fsalab/fp6_llm.</div><div><a href='http://arxiv.org/abs/2401.14112v2'>2401.14112v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17985v1")'>FlattenQuant: Breaking Through the Inference Compute-bound for Large
  Language Models with Per-tensor Quantization</div>
<div id='2402.17985v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T02:00:34Z</div><div>Authors: Yi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin Pan</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have demonstrated state-of-the-art performance
across various tasks. However, the latency of inference and the large GPU
memory consumption of LLMs restrict their deployment performance. Recently,
there have been some efficient attempts to quantize LLMs, yet inference with
large batch size or long sequence still has the issue of being compute-bound.
Fine-grained quantization methods have showcased their proficiency in achieving
low-bit quantization for LLMs, while requiring FP16 data type for linear layer
computations, which is time-consuming when dealing with large batch size or
long sequence. In this paper, we introduce a method called FlattenQuant, which
significantly reduces the maximum value of the tensor by flattening the large
channels in the tensor, to achieve low bit per-tensor quantization with minimal
accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits
to achieve 48.29% of the linear layer calculation in LLMs, with the remaining
layers using 8 bits. The 4-bit matrix multiplication introduced in the
FlattenQuant method can effectively address the compute-bound caused by large
matrix calculation. Our work achieves up to 2$\times$ speedup and 2.3$\times$
memory reduction for LLMs with negligible loss in accuracy.</div><div><a href='http://arxiv.org/abs/2402.17985v1'>2402.17985v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04925v1")'>TP-Aware Dequantization</div>
<div id='2402.04925v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T08:01:40Z</div><div>Authors: Adnan Hoque, Mudhakar Srivatsa, Chih-Chieh Yang, Raghu Ganti</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present a novel method that reduces model inference latency
during distributed deployment of Large Language Models (LLMs). Our contribution
is an optimized inference deployment scheme that address the current
limitations of state-of-the-art quantization kernels when used in conjunction
with Tensor Parallel (TP). Our method preserves data locality in GPU memory
access patterns and exploits a priori knowledge of TP to reduce global
communication. We demonstrate an up to 1.81x speedup over existing methods for
Llama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer
problem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.</div><div><a href='http://arxiv.org/abs/2402.04925v1'>2402.04925v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16677v1")'>T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of
  Compute &amp; Collectives</div>
<div id='2401.16677v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T01:55:34Z</div><div>Authors: Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, Matthew D. Sinclair</div><div style='padding-top: 10px; width: 80ex'>Large Language Models increasingly rely on distributed techniques for their
training and inference. These techniques require communication across devices
which can reduce scaling efficiency as the number of devices increases. While
some distributed techniques can overlap, and thus, hide this communication with
independent computations, techniques such as Tensor Parallelism (TP) inherently
serialize communication with model execution. One approach to hide this
serialized communication is to interleave it with the producer operation (of
the communicated data) in a fine-grained manner. However, this fine-grained
interleaving of communication and computation in software can be difficult.
Furthermore, as with any concurrent execution, it requires compute and memory
resources to be shared between computation and communication, causing resource
contention that reduces overlapping efficacy.
  To overcome these challenges, we propose T3 which applies hardware-software
co-design to transparently overlap serialized communication while minimizing
resource contention with compute. T3 transparently fuses producer operations
with the subsequent communication via a simple configuration of the producer's
output address space and requires minor software changes. At the hardware
level, T3 adds a lightweight track and trigger mechanism to orchestrate the
producer's compute, and communication. It further uses compute-enhanced
memories for communication's attendant compute. As a result, T3 reduces
resource contention, and efficiently overlaps serialized communication with
computation. For important Transformer models like T-NLG, T3 speeds up
communication-heavy sublayers by 30% geomean (max 47%) and reduces data
movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models
scale: geomean 29% for sublayers in $\sim$500-billion parameter models, PALM
and MT-NLG.</div><div><a href='http://arxiv.org/abs/2401.16677v1'>2401.16677v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07128v1")'>FAX: Scalable and Differentiable Federated Primitives in JAX</div>
<div id='2403.07128v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T19:51:01Z</div><div>Authors: Keith Rush, Zachary Charles, Zachary Garrett</div><div style='padding-top: 10px; width: 80ex'>We present FAX, a JAX-based library designed to support large-scale
distributed and federated computations in both data center and cross-device
applications. FAX leverages JAX's sharding mechanisms to enable native
targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX
embeds building blocks for federated computations as primitives in JAX. This
enables three key benefits. First, FAX computations can be translated to XLA
HLO. Second, FAX provides a full implementation of federated automatic
differentiation, greatly simplifying the expression of federated computations.
Last, FAX computations can be interpreted out to existing production
cross-device federated compute systems. We show that FAX provides an easily
programmable, performant, and scalable framework for federated computations in
the data center. FAX is available at
https://github.com/google-research/google-research/tree/master/fax .</div><div><a href='http://arxiv.org/abs/2403.07128v1'>2403.07128v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13533v1")'>FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models
  for Financial Applications with High-Performance Computing</div>
<div id='2402.13533v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T05:03:17Z</div><div>Authors: Xiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, Anwar Walid</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) are computationally intensive. The computation
workload and the memory footprint grow quadratically with the dimension (layer
width). Most of LLMs' parameters come from the linear layers of the transformer
structure and are highly redundant. These linear layers contribute more than
80% of the computation workload and 99% of the model size. To pretrain and
finetune LLMs efficiently, there are three major challenges to address: 1)
reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3)
improving GPU utilization when using distributed training. Prior methods, such
as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the
number of trainable parameters and model size, respectively. However, the
resulting model still consumes a large amount of GPU memory. In this paper, we
present high-performance GPU-based methods that exploit low-rank structures to
pretrain and finetune LLMs for financial applications. We replace one
conventional linear layer of the transformer structure with two narrower linear
layers, which allows us to reduce the number of parameters by several orders of
magnitude. By quantizing the parameters into low precision (8-bit and 4-bit),
the memory consumption of the resulting model is further reduced. Compared with
existing LLMs, our methods achieve a speedup of 1.3X and a model compression
ratio of 2.64X for pretaining without accuracy drop. For finetuning, our
methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks
and financial tasks, respectively, and GPU memory consumption ratio of 6.3X.
The sizes of our models are smaller than 0.59 GB, allowing inference on a
smartphone.</div><div><a href='http://arxiv.org/abs/2402.13533v1'>2402.13533v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13160v1")'>SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced
  Token Detection</div>
<div id='2401.13160v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T00:36:13Z</div><div>Authors: Ke Ye, Heinrich Jiang, Afshin Rostamizadeh, Ayan Chakrabarti, Giulia DeSalvo, Jean-François Kagy, Lazaros Karydas, Gui Citovsky, Sanjiv Kumar</div><div style='padding-top: 10px; width: 80ex'>Pre-training large language models is known to be extremely resource
intensive and often times inefficient, under-utilizing the information
encapsulated in the training text sequences. In this paper, we present SpacTor,
a new training procedure consisting of (1) a hybrid objective combining span
corruption (SC) and token replacement detection (RTD), and (2) a two-stage
curriculum that optimizes the hybrid objective over the initial $\tau$
iterations, then transitions to standard SC loss. We show empirically that the
effectiveness of the hybrid objective is tied to the two-stage pre-training
schedule, and provide extensive analysis on why this is the case. In our
experiments with encoder-decoder architectures (T5) on a variety of NLP tasks,
SpacTor-T5 yields the same downstream performance as standard SC pre-training,
while enabling a 50% reduction in pre-training iterations and 40% reduction in
total FLOPs. Alternatively, given the same amount of computing budget, we find
that SpacTor results in significantly improved downstream benchmark
performance.</div><div><a href='http://arxiv.org/abs/2401.13160v1'>2401.13160v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11960v1")'>DB-LLM: Accurate Dual-Binarization for Efficient LLMs</div>
<div id='2402.11960v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T09:04:30Z</div><div>Authors: Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, Dacheng Tao</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have significantly advanced the field of natural
language processing, while the expensive memory and computation consumption
impede their practical deployment. Quantization emerges as one of the most
effective methods for improving the computational efficiency of LLMs. However,
existing ultra-low-bit quantization always causes severe accuracy drops. In
this paper, we empirically relieve the micro and macro characteristics of
ultra-low bit quantization and present a novel Dual-Binarization method for
LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage
of 2-bit-width and the efficiency advantage of binarization into account,
introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized
weights into two independent sets of binaries, FDB ensures the accuracy of
representations and introduces flexibility, utilizing the efficient bitwise
operations of binarization while retaining the inherent high sparsity of
ultra-low bit quantization. For the macro-level, we find the distortion that
exists in the prediction of LLM after quantization, which is specified as the
deviations related to the ambiguity of samples. We propose the Deviation-Aware
Distillation (DAD) method, enabling the model to focus differently on various
samples. Comprehensive experiments show that our DB-LLM not only significantly
surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization
(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional
20\% reduction in computational consumption compared to the SOTA method under
the same bit-width. Our code will be released soon.</div><div><a href='http://arxiv.org/abs/2402.11960v1'>2402.11960v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03507v1")'>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</div>
<div id='2403.03507v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T07:29:57Z</div><div>Authors: Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian</div><div style='padding-top: 10px; width: 80ex'>Training Large Language Models (LLMs) presents significant memory challenges,
predominantly due to the growing size of weights and optimizer states. Common
memory-reduction approaches, such as low-rank adaptation (LoRA), add a
trainable low-rank matrix to the frozen pre-trained weight in each layer,
reducing trainable parameters and optimizer states. However, such approaches
typically underperform training with full-rank weights in both pre-training and
fine-tuning stages since they limit the parameter search to a low-rank subspace
and alter the training dynamics, and further, may require full-rank warm start.
In this work, we propose Gradient Low-Rank Projection (GaLore), a training
strategy that allows full-parameter learning but is more memory-efficient than
common low-rank adaptation methods such as LoRA. Our approach reduces memory
usage by up to 65.5% in optimizer states while maintaining both efficiency and
performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset
with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit
GaLore further reduces optimizer memory by up to 82.5% and total training
memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the
first time, the feasibility of pre-training a 7B model on consumer GPUs with
24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or
offloading strategies.</div><div><a href='http://arxiv.org/abs/2403.03507v1'>2403.03507v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06664v1")'>Smart-Infinity: Fast Large Language Model Training using Near-Storage
  Processing on a Real System</div>
<div id='2403.06664v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T12:32:14Z</div><div>Authors: Hongsun Jang, Jaeyong Song, Jaewon Jung, Jaeyoung Park, Youngsok Kim, Jinho Lee</div><div style='padding-top: 10px; width: 80ex'>The recent huge advance of Large Language Models (LLMs) is mainly driven by
the increase in the number of parameters. This has led to substantial memory
capacity requirements, necessitating the use of dozens of GPUs just to meet the
capacity. One popular solution to this is storage-offloaded training, which
uses host memory and storage as an extended memory hierarchy. However, this
obviously comes at the cost of storage bandwidth bottleneck because storage
devices have orders of magnitude lower bandwidth compared to that of GPU device
memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck
of storage-offloaded LLM training using near-storage processing devices on a
real system. The main component of Smart-Infinity is SmartUpdate, which
performs parameter updates on custom near-storage accelerators. We identify
that moving parameter updates to the storage side removes most of the storage
traffic. In addition, we propose an efficient data transfer handler structure
to address the system integration issues for Smart-Infinity. The handler allows
overlapping data transfers with fixed memory consumption by reusing the device
buffer. Lastly, we propose accelerator-assisted gradient
compression/decompression to enhance the scalability of Smart-Infinity. When
scaling to multiple near-storage processing devices, the write traffic on the
shared channel becomes the bottleneck. To alleviate this, we compress the
gradients on the GPU and decompress them on the accelerators. It provides
further acceleration from reduced traffic. As a result, Smart-Infinity achieves
a significant speedup compared to the baseline. Notably, Smart-Infinity is a
ready-to-use approach that is fully integrated into PyTorch on a real system.
We will open-source Smart-Infinity to facilitate its use.</div><div><a href='http://arxiv.org/abs/2403.06664v1'>2403.06664v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07221v1")'>LookupFFN: Making Transformers Compute-lite for CPU inference</div>
<div id='2403.07221v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T00:26:16Z</div><div>Authors: Zhanpeng Zeng, Michael Davies, Pranav Pulijala, Karthikeyan Sankaralingam, Vikas Singh</div><div style='padding-top: 10px; width: 80ex'>While GPU clusters are the de facto choice for training large deep neural
network (DNN) models today, several reasons including ease of workflow,
security and cost have led to efforts investigating whether CPUs may be viable
for inference in routine use in many sectors of the industry. But the imbalance
between the compute capabilities of GPUs and CPUs is huge. Motivated by these
considerations, we study a module which is a workhorse within modern DNN
architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent
to which it can be made compute- (or FLOP-) lite. Specifically, we propose an
alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by
the recent studies of using Locality Sensitive Hashing (LSH) to approximate
FFNs. Our formulation recasts most essential operations as a memory look-up,
leveraging the trade-off between the two resources on any platform: compute and
memory (since CPUs offer it in abundance). For RoBERTa language model
pretraining, our formulation achieves similar performance compared to GEMM
based FFNs, while dramatically reducing the required FLOP. Our development is
complemented with a detailed hardware profiling of strategies that will
maximize efficiency -- not just on contemporary hardware but on products that
will be offered in the near/medium term future. Code is avaiable at
\url{https://github.com/mlpen/LookupFFN}.</div><div><a href='http://arxiv.org/abs/2403.07221v1'>2403.07221v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01621v1")'>Stochastic Two Points Method for Deep Model Zeroth-order Optimization</div>
<div id='2402.01621v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T18:39:40Z</div><div>Authors: Yijiang Pang, Jiayu Zhou</div><div style='padding-top: 10px; width: 80ex'>Large foundation models, such as large language models, have performed
exceptionally well in various application scenarios. Building or fully
fine-tuning such large models is usually prohibitive due to either hardware
budget or lack of access to backpropagation. The zeroth-order methods offer a
promising direction for tackling this challenge, where only forward passes are
needed to update the model. This paper introduces an efficient Stochastic
Two-Point (S2P) approach within the gradient-free regime. We present the
theoretical convergence properties of S2P under the general and relaxed
smoothness assumptions. The theoretical properties also shed light on a faster
and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new
convergence properties that better represent the dynamics of deep models in
training. Our comprehensive empirical results show that AS2P is highly
effective in optimizing objectives for large deep models, including language
models, and outperforms standard methods across various model types and scales,
with 2 $\times$ speed-up in training over most conducted tasks.</div><div><a href='http://arxiv.org/abs/2402.01621v1'>2402.01621v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00574v1")'>Beyond Single-Model Views for Deep Learning: Optimization versus
  Generalizability of Stochastic Optimization Algorithms</div>
<div id='2403.00574v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T14:55:22Z</div><div>Authors: Toki Tahmid Inan, Mingrui Liu, Amarda Shehu</div><div style='padding-top: 10px; width: 80ex'>Despite an extensive body of literature on deep learning optimization, our
current understanding of what makes an optimization algorithm effective is
fragmented. In particular, we do not understand well whether enhanced
optimization translates to improved generalizability. Current research
overlooks the inherent stochastic nature of stochastic gradient descent (SGD)
and its variants, resulting in a lack of comprehensive benchmarking and insight
into their statistical performance. This paper aims to address this gap by
adopting a novel approach. Rather than solely evaluating the endpoint of
individual optimization trajectories, we draw from an ensemble of trajectories
to estimate the stationary distribution of stochastic optimizers. Our
investigation encompasses a wide array of techniques, including SGD and its
variants, flat-minima optimizers, and new algorithms we propose under the Basin
Hopping framework. Through our evaluation, which encompasses synthetic
functions with known minima and real-world problems in computer vision and
natural language processing, we emphasize fair benchmarking under a statistical
framework, comparing stationary distributions and establishing statistical
significance. Our study uncovers several key findings regarding the
relationship between training loss and hold-out accuracy, as well as the
comparable performance of SGD, noise-enabled variants, and novel optimizers
utilizing the BH framework. Notably, these algorithms demonstrate performance
on par with flat-minima optimizers like SAM, albeit with half the gradient
evaluations. We anticipate that our work will catalyze further exploration in
deep learning optimization, encouraging a shift away from single-model
approaches towards methodologies that acknowledge and leverage the stochastic
nature of optimizers.</div><div><a href='http://arxiv.org/abs/2403.00574v1'>2403.00574v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07379v1")'>Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The
  Lengths, Bends, and Dead Ends</div>
<div id='2403.07379v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T07:32:47Z</div><div>Authors: Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Schölkopf</div><div style='padding-top: 10px; width: 80ex'>We propose a fresh take on understanding the mechanisms of neural networks by
analyzing the rich structure of parameters contained within their optimization
trajectories. Towards this end, we introduce some natural notions of the
complexity of optimization trajectories, both qualitative and quantitative,
which reveal the inherent nuance and interplay involved between various
optimization choices, such as momentum, weight decay, and batch size. We use
them to provide key hallmarks about the nature of optimization in deep neural
networks: when it goes right, and when it finds itself in a dead end. Further,
thanks to our trajectory perspective, we uncover an intertwined behaviour of
momentum and weight decay that promotes directional exploration, as well as a
directional regularization behaviour of some others. We perform experiments
over large-scale vision and language settings, including large language models
(LLMs) with up to 12 billion parameters, to demonstrate the value of our
approach.</div><div><a href='http://arxiv.org/abs/2403.07379v1'>2403.07379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17457v1")'>Why do Learning Rates Transfer? Reconciling Optimization and Scaling
  Limits for Deep Learning</div>
<div id='2402.17457v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T12:28:01Z</div><div>Authors: Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, Antonio Orvieto</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been growing evidence that if the width and depth of a
neural network are scaled toward the so-called rich feature learning limit
($\mu$P and its depth extension), then some hyperparameters - such as the
learning rate - exhibit transfer from small to very large models, thus reducing
the cost of hyperparameter tuning. From an optimization perspective, this
phenomenon is puzzling, as it implies that the loss landscape is remarkably
consistent across very different model sizes. In this work, we find empirical
evidence that learning rate transfer can be attributed to the fact that under
$\mu$P and its depth extension, the largest eigenvalue of the training loss
Hessian (i.e. the sharpness) is largely independent of the width and depth of
the network for a sustained period of training time. On the other hand, we show
that under the neural tangent kernel (NTK) regime, the sharpness exhibits very
different dynamics at different scales, thus preventing learning rate transfer.
But what causes these differences in the sharpness dynamics? Through a
connection between the spectra of the Hessian and the NTK matrix, we argue that
the cause lies in the presence (for $\mu$P) or progressive absence (for the NTK
regime) of feature learning, which results in a different evolution of the NTK,
and thus of the sharpness. We corroborate our claims with a substantial suite
of experiments, covering a wide range of datasets and architectures: from
ResNets and Vision Transformers trained on benchmark vision datasets to
Transformers-based language models trained on WikiText</div><div><a href='http://arxiv.org/abs/2402.17457v1'>2402.17457v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07085v1")'>When Does Feature Learning Happen? Perspective from an Analytically
  Solvable Model</div>
<div id='2401.07085v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T14:21:46Z</div><div>Authors: Yizhou Xu, Liu Ziyin</div><div style='padding-top: 10px; width: 80ex'>We identify and solve a hidden-layer model that is analytically tractable at
any finite width and whose limits exhibit both the kernel phase and the feature
learning phase. We analyze the phase diagram of this model in all possible
limits of common hyperparameters including width, layer-wise learning rates,
scale of output, and scale of initialization. We apply our result to analyze
how and when feature learning happens in both infinite and finite-width models.
Three prototype mechanisms of feature learning are identified: (1) learning by
alignment, (2) learning by disalignment, and (3) learning by rescaling. In
sharp contrast, neither of these mechanisms is present when the model is in the
kernel regime. This discovery explains why large initialization often leads to
worse performance. Lastly, we empirically demonstrate that discoveries we made
for this analytical model also appear in nonlinear networks in real tasks.</div><div><a href='http://arxiv.org/abs/2401.07085v1'>2401.07085v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13558v1")'>Task structure and nonlinearity jointly determine learned
  representational geometry</div>
<div id='2401.13558v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T16:14:38Z</div><div>Authors: Matteo Alleman, Jack W Lindsey, Stefano Fusi</div><div style='padding-top: 10px; width: 80ex'>The utility of a learned neural representation depends on how well its
geometry supports performance in downstream tasks. This geometry depends on the
structure of the inputs, the structure of the target outputs, and the
architecture of the network. By studying the learning dynamics of networks with
one hidden layer, we discovered that the network's activation function has an
unexpectedly strong impact on the representational geometry: Tanh networks tend
to learn representations that reflect the structure of the target outputs,
while ReLU networks retain more information about the structure of the raw
inputs. This difference is consistently observed across a broad class of
parameterized tasks in which we modulated the degree of alignment between the
geometry of the task inputs and that of the task labels. We analyzed the
learning dynamics in weight space and show how the differences between the
networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic
behavior of ReLU, which leads feature neurons to specialize for different
regions of input space. By contrast, feature neurons in Tanh networks tend to
inherit the task label structure. Consequently, when the target outputs are low
dimensional, Tanh networks generate neural representations that are more
disentangled than those obtained with a ReLU nonlinearity. Our findings shed
light on the interplay between input-output geometry, nonlinearity, and learned
representations in neural networks.</div><div><a href='http://arxiv.org/abs/2401.13558v1'>2401.13558v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08010v1")'>Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature
  Learning</div>
<div id='2402.08010v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T19:18:50Z</div><div>Authors: Yuxiao Wen, Arthur Jacot</div><div style='padding-top: 10px; width: 80ex'>We describe the emergence of a Convolution Bottleneck (CBN) structure in
CNNs, where the network uses its first few layers to transform the input
representation into a representation that is supported only along a few
frequencies and channels, before using the last few layers to map back to the
outputs. We define the CBN rank, which describes the number and type of
frequencies that are kept inside the bottleneck, and partially prove that the
parameter norm required to represent a function $f$ scales as depth times the
CBN rank $f$. We also show that the parameter norm depends at next order on the
regularity of $f$. We show that any network with almost optimal parameter norm
will exhibit a CBN structure in both the weights and - under the assumption
that the network is stable under large learning rate - the activations, which
motivates the common practice of down-sampling; and we verify that the CBN
results still hold with down-sampling. Finally we use the CBN structure to
interpret the functions learned by CNNs on a number of tasks.</div><div><a href='http://arxiv.org/abs/2402.08010v1'>2402.08010v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09092v1")'>Three Decades of Activations: A Comprehensive Survey of 400 Activation
  Functions for Neural Networks</div>
<div id='2402.09092v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T11:13:33Z</div><div>Authors: Vladimír Kunc, Jiří Kléma</div><div style='padding-top: 10px; width: 80ex'>Neural networks have proven to be a highly effective tool for solving complex
problems in many areas of life. Recently, their importance and practical
usability have further been reinforced with the advent of deep learning. One of
the important conditions for the success of neural networks is the choice of an
appropriate activation function introducing non-linearity into the model. Many
types of these functions have been proposed in the literature in the past, but
there is no single comprehensive source containing their exhaustive overview.
The absence of this overview, even in our experience, leads to redundancy and
the unintentional rediscovery of already existing activation functions. To
bridge this gap, our paper presents an extensive survey involving 400
activation functions, which is several times larger in scale than previous
surveys. Our comprehensive compilation also references these surveys; however,
its main goal is to provide the most comprehensive overview and systematization
of previously published activation functions with links to their original
sources. The secondary aim is to update the current understanding of this
family of functions.</div><div><a href='http://arxiv.org/abs/2402.09092v1'>2402.09092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09249v1")'>Exploring the Relationship: Transformative Adaptive Activation Functions
  in Comparison to Other Activation Functions</div>
<div id='2402.09249v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:37:58Z</div><div>Authors: Vladimír Kunc</div><div style='padding-top: 10px; width: 80ex'>Neural networks are the state-of-the-art approach for many tasks and the
activation function is one of the main building blocks that allow such
performance. Recently, a novel transformative adaptive activation function
(TAAF) allowing for any vertical and horizontal translation and scaling was
proposed. This work sets the TAAF into the context of other activation
functions. It shows that the TAAFs generalize over 50 existing activation
functions and utilize similar concepts as over 70 other activation functions,
underscoring the versatility of TAAFs. This comprehensive exploration positions
TAAFs as a promising and adaptable addition to neural networks.</div><div><a href='http://arxiv.org/abs/2402.09249v1'>2402.09249v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13833v1")'>Linearly Constrained Weights: Reducing Activation Shift for Faster
  Training of Neural Networks</div>
<div id='2403.13833v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T01:01:24Z</div><div>Authors: Takuro Kutsuna</div><div style='padding-top: 10px; width: 80ex'>In this paper, we first identify activation shift, a simple but remarkable
phenomenon in a neural network in which the preactivation value of a neuron has
non-zero mean that depends on the angle between the weight vector of the neuron
and the mean of the activation vector in the previous layer. We then propose
linearly constrained weights (LCW) to reduce the activation shift in both fully
connected and convolutional layers. The impact of reducing the activation shift
in a neural network is studied from the perspective of how the variance of
variables in the network changes through layer operations in both forward and
backward chains. We also discuss its relationship to the vanishing gradient
problem. Experimental results show that LCW enables a deep feedforward network
with sigmoid activation functions to be trained efficiently by resolving the
vanishing gradient problem. Moreover, combined with batch normalization, LCW
improves generalization performance of both feedforward and convolutional
networks.</div><div><a href='http://arxiv.org/abs/2403.13833v1'>2403.13833v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08244v1")'>APALU: A Trainable, Adaptive Activation Function for Deep Learning
  Networks</div>
<div id='2402.08244v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T06:18:42Z</div><div>Authors: Barathi Subramanian, Rathinaraja Jeyaraj, Rakhmonov Akhrorjon Akhmadjon Ugli, Jeonghong Kim</div><div style='padding-top: 10px; width: 80ex'>Activation function is a pivotal component of deep learning, facilitating the
extraction of intricate data patterns. While classical activation functions
like ReLU and its variants are extensively utilized, their static nature and
simplicity, despite being advantageous, often limit their effectiveness in
specialized tasks. The trainable activation functions also struggle sometimes
to adapt to the unique characteristics of the data. Addressing these
limitations, we introduce a novel trainable activation function, adaptive
piecewise approximated activation linear unit (APALU), to enhance the learning
performance of deep learning across a broad range of tasks. It presents a
unique set of features that enable it to maintain stability and efficiency in
the learning process while adapting to complex data representations.
Experiments reveal significant improvements over widely used activation
functions for different tasks. In image classification, APALU increases
MobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the
CIFAR10 dataset. In anomaly detection, it improves the average area under the
curve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11%
improvements with DifferNet, and knowledge distillation, respectively, on the
MVTech dataset. Notably, APALU achieves 100% accuracy on a sign language
recognition task with a limited dataset. For regression tasks, APALU enhances
the performance of deep neural networks and recurrent neural networks on
different datasets. These improvements highlight the robustness and
adaptability of APALU across diverse deep-learning applications.</div><div><a href='http://arxiv.org/abs/2402.08244v1'>2402.08244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14469v1")'>Unveiling the Unseen: Identifiable Clusters in Trained Depthwise
  Convolutional Kernels</div>
<div id='2401.14469v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T19:05:53Z</div><div>Authors: Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu</div><div style='padding-top: 10px; width: 80ex'>Recent advances in depthwise-separable convolutional neural networks
(DS-CNNs) have led to novel architectures, that surpass the performance of
classical CNNs, by a considerable scalability and accuracy margin. This paper
reveals another striking property of DS-CNN architectures: discernible and
explainable patterns emerge in their trained depthwise convolutional kernels in
all layers. Through an extensive analysis of millions of trained filters, with
different sizes and from various models, we employed unsupervised clustering
with autoencoders, to categorize these filters. Astonishingly, the patterns
converged into a few main clusters, each resembling the difference of Gaussian
(DoG) functions, and their first and second-order derivatives. Notably, we were
able to classify over 95\% and 90\% of the filters from state-of-the-art
ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a
technological curiosity; it echoes the foundational models neuroscientists have
long proposed for the vision systems of mammals. Our results thus deepen our
understanding of the emergent properties of trained DS-CNNs and provide a
bridge between artificial and biological visual processing systems. More
broadly, they pave the way for more interpretable and biologically-inspired
neural network designs in the future.</div><div><a href='http://arxiv.org/abs/2401.14469v1'>2401.14469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06005v1")'>How does the primate brain combine generative and discriminative
  computations in vision?</div>
<div id='2401.06005v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T16:07:58Z</div><div>Authors: Benjamin Peters, James J. DiCarlo, Todd Gureckis, Ralf Haefner, Leyla Isik, Joshua Tenenbaum, Talia Konkle, Thomas Naselaris, Kimberly Stachenfeld, Zenna Tavares, Doris Tsao, Ilker Yildirim, Nikolaus Kriegeskorte</div><div style='padding-top: 10px; width: 80ex'>Vision is widely understood as an inference problem. However, two contrasting
conceptions of the inference process have each been influential in research on
biological vision as well as the engineering of machine vision. The first
emphasizes bottom-up signal flow, describing vision as a largely feedforward,
discriminative inference process that filters and transforms the visual
information to remove irrelevant variation and represent behaviorally relevant
information in a format suitable for downstream functions of cognition and
behavioral control. In this conception, vision is driven by the sensory data,
and perception is direct because the processing proceeds from the data to the
latent variables of interest. The notion of "inference" in this conception is
that of the engineering literature on neural networks, where feedforward
convolutional neural networks processing images are said to perform inference.
The alternative conception is that of vision as an inference process in
Helmholtz's sense, where the sensory evidence is evaluated in the context of a
generative model of the causal processes giving rise to it. In this conception,
vision inverts a generative model through an interrogation of the evidence in a
process often thought to involve top-down predictions of sensory data to
evaluate the likelihood of alternative hypotheses. The authors include
scientists rooted in roughly equal numbers in each of the conceptions and
motivated to overcome what might be a false dichotomy between them and engage
the other perspective in the realm of theory and experiment. The primate brain
employs an unknown algorithm that may combine the advantages of both
conceptions. We explain and clarify the terminology, review the key empirical
evidence, and propose an empirical research program that transcends the
dichotomy and sets the stage for revealing the mysterious hybrid algorithm of
primate vision.</div><div><a href='http://arxiv.org/abs/2401.06005v1'>2401.06005v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14434v1")'>Transforming gradient-based techniques into interpretable methods</div>
<div id='2401.14434v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T09:24:19Z</div><div>Authors: Caroline Mazini Rodrigues, Nicolas Boutry, Laurent Najman</div><div style='padding-top: 10px; width: 80ex'>The explication of Convolutional Neural Networks (CNN) through xAI techniques
often poses challenges in interpretation. The inherent complexity of input
features, notably pixels extracted from images, engenders complex correlations.
Gradient-based methodologies, exemplified by Integrated Gradients (IG),
effectively demonstrate the significance of these features. Nevertheless, the
conversion of these explanations into images frequently yields considerable
noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a
supportive framework for gradient-based techniques. Its primary objective is to
accentuate influential regions by establishing distinctions between classes.
The essence of GAD is to limit the scope of analysis during visualization and,
consequently reduce image noise. Empirical investigations involving occluded
images have demonstrated that the identified regions through this methodology
indeed play a pivotal role in facilitating class differentiation.</div><div><a href='http://arxiv.org/abs/2401.14434v1'>2401.14434v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08919v1")'>Interpretable Measures of Conceptual Similarity by
  Complexity-Constrained Descriptive Auto-Encoding</div>
<div id='2402.08919v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T03:31:17Z</div><div>Authors: Alessandro Achille, Greg Ver Steeg, Tian Yu Liu, Matthew Trager, Carson Klingenberg, Stefano Soatto</div><div style='padding-top: 10px; width: 80ex'>Quantifying the degree of similarity between images is a key copyright issue
for image-based machine learning. In legal doctrine however, determining the
degree of similarity between works requires subjective analysis, and
fact-finders (judges and juries) can demonstrate considerable variability in
these subjective judgement calls. Images that are structurally similar can be
deemed dissimilar, whereas images of completely different scenes can be deemed
similar enough to support a claim of copying. We seek to define and compute a
notion of "conceptual similarity" among images that captures high-level
relations even among images that do not share repeated elements or visually
similar components. The idea is to use a base multi-modal model to generate
"explanations" (captions) of visual data at increasing levels of complexity.
Then, similarity can be measured by the length of the caption needed to
discriminate between the two images: Two highly dissimilar images can be
discriminated early in their description, whereas conceptually dissimilar ones
will need more detail to be distinguished. We operationalize this definition
and show that it correlates with subjective (averaged human evaluation)
assessment, and beats existing baselines on both image-to-image and
text-to-text similarity benchmarks. Beyond just providing a number, our method
also offers interpretability by pointing to the specific level of granularity
of the description where the source data are differentiated.</div><div><a href='http://arxiv.org/abs/2402.08919v1'>2402.08919v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14469v1")'>Reimagining Anomalies: What If Anomalies Were Normal?</div>
<div id='2402.14469v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T11:56:44Z</div><div>Authors: Philipp Liznerski, Saurabh Varshneya, Ece Calikus, Sophie Fellenz, Marius Kloft</div><div style='padding-top: 10px; width: 80ex'>Deep learning-based methods have achieved a breakthrough in image anomaly
detection, but their complexity introduces a considerable challenge to
understanding why an instance is predicted to be anomalous. We introduce a
novel explanation method that generates multiple counterfactual examples for
each anomaly, capturing diverse concepts of anomalousness. A counterfactual
example is a modification of the anomaly that is perceived as normal by the
anomaly detector. The method provides a high-level semantic explanation of the
mechanism that triggered the anomaly detector, allowing users to explore
"what-if scenarios." Qualitative and quantitative analyses across various image
datasets show that the method applied to state-of-the-art anomaly detectors can
achieve high-quality semantic explanations of detectors.</div><div><a href='http://arxiv.org/abs/2402.14469v1'>2402.14469v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15352v1")'>On normalization-equivariance properties of supervised and unsupervised
  denoising methods: a survey</div>
<div id='2402.15352v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:39:12Z</div><div>Authors: Sébastien Herbreteau, Charles Kervrann</div><div style='padding-top: 10px; width: 80ex'>Image denoising is probably the oldest and still one of the most active
research topic in image processing. Many methodological concepts have been
introduced in the past decades and have improved performances significantly in
recent years, especially with the emergence of convolutional neural networks
and supervised deep learning. In this paper, we propose a survey of guided tour
of supervised and unsupervised learning methods for image denoising,
classifying the main principles elaborated during this evolution, with a
particular concern given to recent developments in supervised learning. It is
conceived as a tutorial organizing in a comprehensive framework current
approaches. We give insights on the rationales and limitations of the most
performant methods in the literature, and we highlight the common features
between many of them. Finally, we focus on on the normalization equivariance
properties that is surprisingly not guaranteed with most of supervised methods.
It is of paramount importance that intensity shifting or scaling applied to the
input image results in a corresponding change in the denoiser output.</div><div><a href='http://arxiv.org/abs/2402.15352v1'>2402.15352v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00261v1")'>Understanding Neural Network Systems for Image Analysis using Vector
  Spaces and Inverse Maps</div>
<div id='2402.00261v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T01:11:15Z</div><div>Authors: Rebecca Pattichis, Marios S. Pattichis</div><div style='padding-top: 10px; width: 80ex'>There is strong interest in developing mathematical methods that can be used
to understand complex neural networks used in image analysis. In this paper, we
introduce techniques from Linear Algebra to model neural network layers as maps
between signal spaces. First, we demonstrate how signal spaces can be used to
visualize weight spaces and convolutional layer kernels. We also demonstrate
how residual vector spaces can be used to further visualize information lost at
each layer. Second, we introduce the concept of invertible networks and an
algorithm for computing input images that yield specific outputs. We
demonstrate our approach on two invertible networks and ResNet18.</div><div><a href='http://arxiv.org/abs/2402.00261v1'>2402.00261v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14212v1")'>Moonwalk: Inverse-Forward Differentiation</div>
<div id='2402.14212v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T01:33:31Z</div><div>Authors: Dmitrii Krylov, Armin Karamzade, Roy Fox</div><div style='padding-top: 10px; width: 80ex'>Backpropagation, while effective for gradient computation, falls short in
addressing memory consumption, limiting scalability. This work explores
forward-mode gradient computation as an alternative in invertible networks,
showing its potential to reduce the memory footprint without substantial
drawbacks. We introduce a novel technique based on a vector-inverse-Jacobian
product that accelerates the computation of forward gradients while retaining
the advantages of memory reduction and preserving the fidelity of true
gradients. Our method, Moonwalk, has a time complexity linear in the depth of
the network, unlike the quadratic time complexity of na\"ive forward, and
empirically reduces computation time by several orders of magnitude without
allocating more memory. We further accelerate Moonwalk by combining it with
reverse-mode differentiation to achieve time complexity comparable with
backpropagation while maintaining a much smaller memory footprint. Finally, we
showcase the robustness of our method across several architecture choices.
Moonwalk is the first forward-based method to compute true gradients in
invertible networks in computation time comparable to backpropagation and using
significantly less memory.</div><div><a href='http://arxiv.org/abs/2402.14212v1'>2402.14212v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16020v1")'>A Step-by-step Introduction to the Implementation of Automatic
  Differentiation</div>
<div id='2402.16020v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T07:41:08Z</div><div>Authors: Yu-Hsueh Fang, He-Zhe Lin, Jie-Jyun Liu, Chih-Jen Lin</div><div style='padding-top: 10px; width: 80ex'>Automatic differentiation is a key component in deep learning. This topic is
well studied and excellent surveys such as Baydin et al. (2018) have been
available to clearly describe the basic concepts. Further, sophisticated
implementations of automatic differentiation are now an important part of
popular deep learning frameworks. However, it is difficult, if not impossible,
to directly teach students the implementation of existing systems due to the
complexity. On the other hand, if the teaching stops at the basic concept,
students fail to sense the realization of an implementation. For example, we
often mention the computational graph in teaching automatic differentiation,
but students wonder how to implement and use it. In this document, we partially
fill the gap by giving a step by step introduction of implementing a simple
automatic differentiation system. We streamline the mathematical concepts and
the implementation. Further, we give the motivation behind each implementation
detail, so the whole setting becomes very natural.</div><div><a href='http://arxiv.org/abs/2402.16020v1'>2402.16020v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10967v1")'>HOSC: A Periodic Activation Function for Preserving Sharp Features in
  Implicit Neural Representations</div>
<div id='2401.10967v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T09:56:34Z</div><div>Authors: Danzel Serrano, Jakub Szymkowiak, Przemyslaw Musialski</div><div style='padding-top: 10px; width: 80ex'>Recently proposed methods for implicitly representing signals such as images,
scenes, or geometries using coordinate-based neural network architectures often
do not leverage the choice of activation functions, or do so only to a limited
extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC),
a novel activation function with a controllable sharpness parameter. Unlike any
previous activations, HOSC has been specifically designed to better capture
sudden changes in the input signal, and hence sharp or acute features of the
underlying data, as well as smooth low-frequency transitions. Due to its
simplicity and modularity, HOSC offers a plug-and-play functionality that can
be easily incorporated into any existing method employing a neural network as a
way of implicitly representing a signal. We benchmark HOSC against other
popular activations in an array of general tasks, empirically showing an
improvement in the quality of obtained representations, provide the
mathematical motivation behind the efficacy of HOSC, and discuss its
limitations.</div><div><a href='http://arxiv.org/abs/2401.10967v1'>2401.10967v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02208v1")'>Implicit Neural Representation of Tileable Material Textures</div>
<div id='2402.02208v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T16:44:25Z</div><div>Authors: Hallison Paz, Tiago Novello, Luiz Velho</div><div style='padding-top: 10px; width: 80ex'>We explore sinusoidal neural networks to represent periodic tileable
textures. Our approach leverages the Fourier series by initializing the first
layer of a sinusoidal neural network with integer frequencies with a period
$P$. We prove that the compositions of sinusoidal layers generate only integer
frequencies with period $P$. As a result, our network learns a continuous
representation of a periodic pattern, enabling direct evaluation at any spatial
coordinate without the need for interpolation. To enforce the resulting pattern
to be tileable, we add a regularization term, based on the Poisson equation, to
the loss function. Our proposed neural implicit representation is compact and
enables efficient reconstruction of high-resolution textures with high visual
fidelity and sharpness across multiple levels of detail. We present
applications of our approach in the domain of anti-aliased surface.</div><div><a href='http://arxiv.org/abs/2402.02208v1'>2402.02208v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14427v1")'>Text me the data: Generating Ground Pressure Sequence from Textual
  Descriptions for HAR</div>
<div id='2402.14427v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T10:14:59Z</div><div>Authors: Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Lars Krupp, Vitor Fortes Rey, Paul Lukowicz</div><div style='padding-top: 10px; width: 80ex'>In human activity recognition (HAR), the availability of substantial ground
truth is necessary for training efficient models. However, acquiring ground
pressure data through physical sensors itself can be cost-prohibitive,
time-consuming. To address this critical need, we introduce Text-to-Pressure
(T2P), a framework designed to generate extensive ground pressure sequences
from textual descriptions of human activities using deep learning techniques.
We show that the combination of vector quantization of sensor data along with
simple text conditioned auto regressive strategy allows us to obtain
high-quality generated pressure sequences from textual descriptions with the
help of discrete latent correlation between text and pressure maps. We achieved
comparable performance on the consistency between text and generated motion
with an R squared value of 0.722, Masked R squared value of 0.892, and FID
score of 1.83. Additionally, we trained a HAR model with the the synthesized
data and evaluated it on pressure dynamics collected by a real pressure sensor
which is on par with a model trained on only real data. Combining both real and
synthesized training data increases the overall macro F1 score by 5.9 percent.</div><div><a href='http://arxiv.org/abs/2402.14427v1'>2402.14427v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16729v2")'>Widely Linear Matched Filter: A Lynchpin towards the Interpretability of
  Complex-valued CNNs</div>
<div id='2401.16729v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T04:01:18Z</div><div>Authors: Qingchen Wang, Zhe Li, Zdenka Babic, Wei Deng, Ljubiša Stanković, Danilo P. Mandic</div><div style='padding-top: 10px; width: 80ex'>A recent study on the interpretability of real-valued convolutional neural
networks (CNNs) {Stankovic_Mandic_2023CNN} has revealed a direct and physically
meaningful link with the task of finding features in data through matched
filters. However, applying this paradigm to illuminate the interpretability of
complex-valued CNNs meets a formidable obstacle: the extension of matched
filtering to a general class of noncircular complex-valued data, referred to
here as the widely linear matched filter (WLMF), has been only implicit in the
literature. To this end, to establish the interpretability of the operation of
complex-valued CNNs, we introduce a general WLMF paradigm, provide its solution
and undertake analysis of its performance. For rigor, our WLMF solution is
derived without imposing any assumption on the probability density of noise.
The theoretical advantages of the WLMF over its standard strictly linear
counterpart (SLMF) are provided in terms of their output signal-to-noise-ratios
(SNRs), with WLMF consistently exhibiting enhanced SNR. Moreover, the lower
bound on the SNR gain of WLMF is derived, together with condition to attain
this bound. This serves to revisit the convolution-activation-pooling chain in
complex-valued CNNs through the lens of matched filtering, which reveals the
potential of WLMFs to provide physical interpretability and enhance
explainability of general complex-valued CNNs. Simulations demonstrate the
agreement between the theoretical and numerical results.</div><div><a href='http://arxiv.org/abs/2401.16729v2'>2401.16729v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.10262v1")'>Null Space Properties of Neural Networks with Applications to Image
  Steganography</div>
<div id='2401.10262v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T03:32:28Z</div><div>Authors: Xiang Li, Kevin M. Short</div><div style='padding-top: 10px; width: 80ex'>This paper explores the null space properties of neural networks. We extend
the null space definition from linear to nonlinear maps and discuss the
presence of a null space in neural networks. The null space of a given neural
network can tell us the part of the input data that makes no contribution to
the final prediction so that we can use it to trick the neural network. This
reveals an inherent weakness in neural networks that can be exploited. One
application described here leads to a method of image steganography. Through
experiments on image datasets such as MNIST, we show that we can use null space
components to force the neural network to choose a selected hidden image class,
even though the overall image can be made to look like a completely different
image. We conclude by showing comparisons between what a human viewer would
see, and the part of the image that the neural network is actually using to
make predictions and, hence, show that what the neural network ``sees'' is
completely different than what we would expect.</div><div><a href='http://arxiv.org/abs/2401.10262v1'>2401.10262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.04141v1")'>On The Potential of The Fractal Geometry and The CNNs Ability to Encode
  it</div>
<div id='2401.04141v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T15:22:56Z</div><div>Authors: Julia El Zini, Bassel Musharrafieh, Mariette Awad</div><div style='padding-top: 10px; width: 80ex'>The fractal dimension provides a statistical index of object complexity by
studying how the pattern changes with the measuring scale. Although useful in
several classification tasks, the fractal dimension is under-explored in deep
learning applications. In this work, we investigate the features that are
learned by deep models and we study whether these deep networks are able to
encode features as complex and high-level as the fractal dimensions.
Specifically, we conduct a correlation analysis experiment to show that deep
networks are not able to extract such a feature in none of their layers. We
combine our analytical study with a human evaluation to investigate the
differences between deep learning networks and models that operate on the
fractal feature solely. Moreover, we show the effectiveness of fractal features
in applications where the object structure is crucial for the classification
task. We empirically show that training a shallow network on fractal features
achieves performance comparable, even superior in specific cases, to that of
deep networks trained on raw data while requiring less computational resources.
Fractals improved the accuracy of the classification by 30% on average while
requiring up to 84% less time to train. We couple our empirical study with a
complexity analysis of the computational cost of extracting the proposed
fractal features, and we study its limitation.</div><div><a href='http://arxiv.org/abs/2401.04141v1'>2401.04141v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09050v1")'>End-to-End Training Induces Information Bottleneck through Layer-Role
  Differentiation: A Comparative Analysis with Layer-wise Training</div>
<div id='2402.09050v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T09:46:53Z</div><div>Authors: Keitaro Sakamoto, Issei Sato</div><div style='padding-top: 10px; width: 80ex'>End-to-end (E2E) training, optimizing the entire model through error
backpropagation, fundamentally supports the advancements of deep learning.
Despite its high performance, E2E training faces the problems of memory
consumption, parallel computing, and discrepancy with the functionalities of
the actual brain. Various alternative methods have been proposed to overcome
these difficulties; however, no one can yet match the performance of E2E
training, thereby falling short in practicality. Furthermore, there is no deep
understanding regarding differences in the trained model properties beyond the
performance gap. In this paper, we reconsider why E2E training demonstrates a
superior performance through a comparison with layer-wise training, a non-E2E
method that locally sets errors. On the basis of the observation that E2E
training has an advantage in propagating input information, we analyze the
information plane dynamics of intermediate representations based on the
Hilbert-Schmidt independence criterion (HSIC). The results of our normalized
HSIC value analysis reveal the E2E training ability to exhibit different
information dynamics across layers, in addition to efficient information
propagation. Furthermore, we show that this layer-role differentiation leads to
the final representation following the information bottleneck principle. It
suggests the need to consider the cooperative interactions between layers, not
just the final layer when analyzing the information bottleneck of deep
learning.</div><div><a href='http://arxiv.org/abs/2402.09050v1'>2402.09050v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00332v1")'>Comparing Spectral Bias and Robustness For Two-Layer Neural Networks:
  SGD vs Adaptive Random Fourier Features</div>
<div id='2402.00332v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T04:35:37Z</div><div>Authors: Aku Kammonen, Lisi Liang, Anamika Pandey, Raúl Tempone</div><div style='padding-top: 10px; width: 80ex'>We present experimental results highlighting two key differences resulting
from the choice of training algorithm for two-layer neural networks. The
spectral bias of neural networks is well known, while the spectral bias
dependence on the choice of training algorithm is less studied. Our experiments
demonstrate that an adaptive random Fourier features algorithm (ARFF) can yield
a spectral bias closer to zero compared to the stochastic gradient descent
optimizer (SGD). Additionally, we train two identically structured classifiers,
employing SGD and ARFF, to the same accuracy levels and empirically assess
their robustness against adversarial noise attacks.</div><div><a href='http://arxiv.org/abs/2402.00332v1'>2402.00332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08947v2")'>AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized
  Phishing URL Detection</div>
<div id='2401.08947v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T03:44:27Z</div><div>Authors: Saba Aslam, Hafsa Aslam, Arslan Manzoor, Chen Hui, Abdur Rasool</div><div style='padding-top: 10px; width: 80ex'>The escalating reliance on revolutionary online web services has introduced
heightened security risks, with persistent challenges posed by phishing despite
extensive security measures. Traditional phishing systems, reliant on machine
learning and manual features, struggle with evolving tactics. Recent advances
in deep learning offer promising avenues for tackling novel phishing challenges
and malicious URLs. This paper introduces a two-phase stack generalized model
named AntiPhishStack, designed to detect phishing sites. The model leverages
the learning of URLs and character-level TF-IDF features symmetrically,
enhancing its ability to combat emerging phishing threats. In Phase I, features
are trained on a base machine learning classifier, employing K-fold
cross-validation for robust mean prediction. Phase II employs a two-layered
stacked-based LSTM network with five adaptive optimizers for dynamic
compilation, ensuring premier prediction on these features. Additionally, the
symmetrical predictions from both phases are optimized and integrated to train
a meta-XGBoost classifier, contributing to a final robust prediction. The
significance of this work lies in advancing phishing detection with
AntiPhishStack, operating without prior phishing-specific feature knowledge.
Experimental validation on two benchmark datasets, comprising benign and
phishing or malicious URLs, demonstrates the model's exceptional performance,
achieving a notable 96.04% accuracy compared to existing studies. This research
adds value to the ongoing discourse on symmetry and asymmetry in information
security and provides a forward-thinking solution for enhancing network
security in the face of evolving cyber threats.</div><div><a href='http://arxiv.org/abs/2401.08947v2'>2401.08947v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11342v1")'>Ransomware detection using stacked autoencoder for feature selection</div>
<div id='2402.11342v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T17:31:48Z</div><div>Authors: Mike Nkongolo, Mahmut Tokmak</div><div style='padding-top: 10px; width: 80ex'>The aim of this study is to propose and evaluate an advanced ransomware
detection and classification method that combines a Stacked Autoencoder (SAE)
for precise feature selection with a Long Short Term Memory (LSTM) classifier
to enhance ransomware stratification accuracy. The proposed approach involves
thorough pre processing of the UGRansome dataset and training an unsupervised
SAE for optimal feature selection or fine tuning via supervised learning to
elevate the LSTM model's classification capabilities. The study meticulously
analyzes the autoencoder's learned weights and activations to identify
essential features for distinguishing ransomware families from other malware
and creates a streamlined feature set for precise classification. Extensive
experiments, including up to 400 epochs and varying learning rates, are
conducted to optimize the model's performance. The results demonstrate the
outstanding performance of the SAE-LSTM model across all ransomware families,
boasting high precision, recall, and F1 score values that underscore its robust
classification capabilities. Furthermore, balanced average scores affirm the
proposed model's ability to generalize effectively across various malware
types. The proposed model achieves an exceptional 99% accuracy in ransomware
classification, surpassing the Extreme Gradient Boosting (XGBoost) algorithm
primarily due to its effective SAE feature selection mechanism. The model also
demonstrates outstanding performance in identifying signature attacks,
achieving a 98% accuracy rate.</div><div><a href='http://arxiv.org/abs/2402.11342v1'>2402.11342v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01359v1")'>TESSERACT: Eliminating Experimental Bias in Malware Classification
  across Space and Time (Extended Version)</div>
<div id='2402.01359v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:27:32Z</div><div>Authors: Zeliang Kan, Shae McFadden, Daniel Arp, Feargus Pendlebury, Roberto Jordaney, Johannes Kinder, Fabio Pierazzi, Lorenzo Cavallaro</div><div style='padding-top: 10px; width: 80ex'>Machine learning (ML) plays a pivotal role in detecting malicious software.
Despite the high F1-scores reported in numerous studies reaching upwards of
0.99, the issue is not completely solved. Malware detectors often experience
performance decay due to constantly evolving operating systems and attack
methods, which can render previously learned knowledge insufficient for
accurate decision-making on new inputs. This paper argues that commonly
reported results are inflated due to two pervasive sources of experimental bias
in the detection task: spatial bias caused by data distributions that are not
representative of a real-world deployment; and temporal bias caused by
incorrect time splits of data, leading to unrealistic configurations. To
address these biases, we introduce a set of constraints for fair experiment
design, and propose a new metric, AUT, for classifier robustness in real-world
settings. We additionally propose an algorithm designed to tune training data
to enhance classifier performance. Finally, we present TESSERACT, an
open-source framework for realistic classifier comparison. Our evaluation
encompasses both traditional ML and deep learning methods, examining published
works on an extensive Android dataset with 259,230 samples over a five-year
span. Additionally, we conduct case studies in the Windows PE and PDF domains.
Our findings identify the existence of biases in previous studies and reveal
that significant performance enhancements are possible through appropriate,
periodic tuning. We explore how mitigation strategies may support in achieving
a more stable and better performance over time by employing multiple strategies
to delay performance decay.</div><div><a href='http://arxiv.org/abs/2402.01359v1'>2402.01359v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02953v1")'>Unraveling the Key of Machine Learning Solutions for Android Malware
  Detection</div>
<div id='2402.02953v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:31:19Z</div><div>Authors: Jiahao Liu, Jun Zeng, Fabio Pierazzi, Lorenzo Cavallaro, Zhenkai Liang</div><div style='padding-top: 10px; width: 80ex'>Android malware detection serves as the front line against malicious apps.
With the rapid advancement of machine learning (ML), ML-based Android malware
detection has attracted increasing attention due to its capability of
automatically capturing malicious patterns from Android APKs. These
learning-driven methods have reported promising results in detecting malware.
However, the absence of an in-depth analysis of current research progress makes
it difficult to gain a holistic picture of the state of the art in this area.
  This paper presents a comprehensive investigation to date into ML-based
Android malware detection with empirical and quantitative analysis. We first
survey the literature, categorizing contributions into a taxonomy based on the
Android feature engineering and ML modeling pipeline. Then, we design a
general-propose framework for ML-based Android malware detection, re-implement
12 representative approaches from different research communities, and evaluate
them from three primary dimensions, i.e., effectiveness, robustness, and
efficiency. The evaluation reveals that ML-based approaches still face open
challenges and provides insightful findings like more powerful ML models are
not the silver bullet for designing better malware detectors. We further
summarize our findings and put forth recommendations to guide future research.</div><div><a href='http://arxiv.org/abs/2402.02953v1'>2402.02953v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.14603v1")'>Balanced Resonate-and-Fire Neurons</div>
<div id='2402.14603v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:57:21Z</div><div>Authors: Saya Higuchi, Sebastian Kairat, Sander M. Bohte, Sebastian Otte</div><div style='padding-top: 10px; width: 80ex'>The resonate-and-fire (RF) neuron, introduced over two decades ago, is a
simple, efficient, yet biologically plausible spiking neuron model, which can
extract frequency patterns within the time domain due to its resonating
membrane dynamics. However, previous RF formulations suffer from intrinsic
shortcomings that limit effective learning and prevent exploiting the
principled advantage of RF neurons. Here, we introduce the balanced RF (BRF)
neuron, which alleviates some of the intrinsic limitations of vanilla RF
neurons and demonstrates its effectiveness within recurrent spiking neural
networks (RSNNs) on various sequence learning tasks. We show that networks of
BRF neurons achieve overall higher task performance, produce only a fraction of
the spikes, and require significantly fewer parameters as compared to modern
RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable
training convergence, even when bridging many hundreds of time steps during
backpropagation through time (BPTT). These results underscore that our BRF-RSNN
is a strong candidate for future large-scale RSNN architectures, further lines
of research in SNN methodology, and more efficient hardware implementations.</div><div><a href='http://arxiv.org/abs/2402.14603v1'>2402.14603v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01782v1")'>Benchmarking Spiking Neural Network Learning Methods with Varying
  Locality</div>
<div id='2402.01782v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T19:57:08Z</div><div>Authors: Jiaqi Lin, Sen Lu, Malyaban Bal, Abhronil Sengupta</div><div style='padding-top: 10px; width: 80ex'>Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics,
have shown to achieve performance comparable to Artificial Neural Networks
(ANNs) in several machine learning tasks. Information is processed as spikes
within SNNs in an event-based mechanism that significantly reduces energy
consumption. However, training SNNs is challenging due to the
non-differentiable nature of the spiking mechanism. Traditional approaches,
such as Backpropagation Through Time (BPTT), have shown effectiveness but comes
with additional computational and memory costs and are biologically
implausible. In contrast, recent works propose alternative learning methods
with varying degrees of locality, demonstrating success in classification
tasks. In this work, we show that these methods share similarities during the
training process, while they present a trade-off between biological
plausibility and performance. Further, this research examines the implicitly
recurrent nature of SNNs and investigates the influence of addition of explicit
recurrence to SNNs. We experimentally prove that the addition of explicit
recurrent weights enhances the robustness of SNNs. We also investigate the
performance of local learning methods under gradient and non-gradient based
adversarial attacks.</div><div><a href='http://arxiv.org/abs/2402.01782v1'>2402.01782v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14557v1")'>Extension of Recurrent Kernels to different Reservoir Computing
  topologies</div>
<div id='2401.14557v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T22:54:39Z</div><div>Authors: Giuseppe Alessio D'Inverno, Jonathan Dong</div><div style='padding-top: 10px; width: 80ex'>Reservoir Computing (RC) has become popular in recent years due to its fast
and efficient computational capabilities. Standard RC has been shown to be
equivalent in the asymptotic limit to Recurrent Kernels, which helps in
analyzing its expressive power. However, many well-established RC paradigms,
such as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way.
This study aims to fill this gap by providing an empirical analysis of the
equivalence of specific RC architectures with their corresponding Recurrent
Kernel formulation. We conduct a convergence study by varying the activation
function implemented in each architecture. Our study also sheds light on the
role of sparse connections in RC architectures and propose an optimal sparsity
level that depends on the reservoir size. Furthermore, our systematic analysis
shows that in Deep RC models, convergence is better achieved with successive
reservoirs of decreasing sizes.</div><div><a href='http://arxiv.org/abs/2401.14557v1'>2401.14557v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.10710v1")'>Classification with neural networks with quadratic decision functions</div>
<div id='2401.10710v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T14:18:32Z</div><div>Authors: Leon Frischauf, Otmar Scherzer, Cong Shi</div><div style='padding-top: 10px; width: 80ex'>Neural network with quadratic decision functions have been introduced as
alternatives to standard neural networks with affine linear one. They are
advantageous when the objects to be identified are of compact basic geometries
like circles, ellipsis etc. In this paper we investigate the use of such ansatz
functions for classification. In particular we test and compare the algorithm
on the MNIST dataset for classification of handwritten digits and for
classification of subspecies. We also show, that the implementation can be
based on the neural network structure in the software Tensorflow and Keras,
respectively.</div><div><a href='http://arxiv.org/abs/2401.10710v1'>2401.10710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16017v1")'>Spectrum Extraction and Clipping for Implicitly Linear Layers</div>
<div id='2402.16017v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T07:28:28Z</div><div>Authors: Ali Ebrahimpour Boroojeny, Matus Telgarsky, Hari Sundaram</div><div style='padding-top: 10px; width: 80ex'>We show the effectiveness of automatic differentiation in efficiently and
correctly computing and controlling the spectrum of implicitly linear
operators, a rich family of layer types including all standard convolutional
and dense layers. We provide the first clipping method which is correct for
general convolution layers, and illuminate the representational limitation that
caused correctness issues in prior work. We study the effect of the batch
normalization layers when concatenated with convolutional layers and show how
our clipping method can be applied to their composition. By comparing the
accuracy and performance of our algorithms to the state-of-the-art methods,
using various experiments, we show they are more precise and efficient and lead
to better generalization and adversarial robustness. We provide the code for
using our methods at https://github.com/Ali-E/FastClip.</div><div><a href='http://arxiv.org/abs/2402.16017v1'>2402.16017v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00240v1")'>Spectral Norm of Convolutional Layers with Circular and Zero Paddings</div>
<div id='2402.00240v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T23:48:48Z</div><div>Authors: Blaise Delattre, Quentin Barthélemy, Alexandre Allauzen</div><div style='padding-top: 10px; width: 80ex'>This paper leverages the use of \emph{Gram iteration} an efficient,
deterministic, and differentiable method for computing spectral norm with an
upper bound guarantee. Designed for circular convolutional layers, we
generalize the use of the Gram iteration to zero padding convolutional layers
and prove its quadratic convergence. We also provide theorems for bridging the
gap between circular and zero padding convolution's spectral norm. We design a
\emph{spectral rescaling} that can be used as a competitive $1$-Lipschitz layer
that enhances network robustness. Demonstrated through experiments, our method
outperforms state-of-the-art techniques in precision, computational cost, and
scalability. The code of experiments is available at
https://github.com/blaisedelattre/lip4conv.</div><div><a href='http://arxiv.org/abs/2402.00240v1'>2402.00240v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14917v1")'>Mean-field Analysis on Two-layer Neural Networks from a Kernel
  Perspective</div>
<div id='2403.14917v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T02:41:57Z</div><div>Authors: Shokichi Takakura, Taiji Suzuki</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study the feature learning ability of two-layer neural
networks in the mean-field regime through the lens of kernel methods. To focus
on the dynamics of the kernel induced by the first layer, we utilize a
two-timescale limit, where the second layer moves much faster than the first
layer. In this limit, the learning problem is reduced to the minimization
problem over the intrinsic kernel. Then, we show the global convergence of the
mean-field Langevin dynamics and derive time and particle discretization error.
We also demonstrate that two-layer neural networks can learn a union of
multiple reproducing kernel Hilbert spaces more efficiently than any kernel
methods, and neural networks acquire data-dependent kernel which aligns with
the target function. In addition, we develop a label noise procedure, which
converges to the global optimum and show that the degrees of freedom appears as
an implicit regularization.</div><div><a href='http://arxiv.org/abs/2403.14917v1'>2403.14917v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02890v1")'>Nonlinear functional regression by functional deep neural network with
  kernel embedding</div>
<div id='2401.02890v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T16:43:39Z</div><div>Authors: Zhongjie Shi, Jun Fan, Linhao Song, Ding-Xuan Zhou, Johan A. K. Suykens</div><div style='padding-top: 10px; width: 80ex'>With the rapid development of deep learning in various fields of science and
technology, such as speech recognition, image classification, and natural
language processing, recently it is also widely applied in the functional data
analysis (FDA) with some empirical success. However, due to the infinite
dimensional input, we need a powerful dimension reduction method for functional
learning tasks, especially for the nonlinear functional regression. In this
paper, based on the idea of smooth kernel integral transformation, we propose a
functional deep neural network with an efficient and fully data-dependent
dimension reduction method. The architecture of our functional net consists of
a kernel embedding step: an integral transformation with a data-dependent
smooth kernel; a projection step: a dimension reduction by projection with
eigenfunction basis based on the embedding kernel; and finally an expressive
deep ReLU neural network for the prediction. The utilization of smooth kernel
embedding enables our functional net to be discretization invariant, efficient,
and robust to noisy observations, capable of utilizing information in both
input functions and responses data, and have a low requirement on the number of
discrete points for an unimpaired generalization performance. We conduct
theoretical analysis including approximation error and generalization error
analysis, and numerical simulations to verify these advantages of our
functional net.</div><div><a href='http://arxiv.org/abs/2401.02890v1'>2401.02890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03698v1")'>Estimating the Local Learning Coefficient at Scale</div>
<div id='2402.03698v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T04:37:09Z</div><div>Authors: Zach Furman, Edmund Lau</div><div style='padding-top: 10px; width: 80ex'>The \textit{local learning coefficient} (LLC) is a principled way of
quantifying model complexity, originally derived in the context of Bayesian
statistics using singular learning theory (SLT). Several methods are known for
numerically estimating the local learning coefficient, but so far these methods
have not been extended to the scale of modern deep learning architectures or
data sets. Using a method developed in {\tt arXiv:2308.12108 [stat.ML]} we
empirically show how the LLC may be measured accurately and self-consistently
for deep linear networks (DLNs) up to 100M parameters. We also show that the
estimated LLC has the rescaling invariance that holds for the theoretical
quantity.</div><div><a href='http://arxiv.org/abs/2402.03698v1'>2402.03698v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04810v1")'>Restricted Bayesian Neural Network</div>
<div id='2403.04810v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T19:09:11Z</div><div>Authors: Sourav Ganguly</div><div style='padding-top: 10px; width: 80ex'>Modern deep learning tools are remarkably effective in addressing intricate
problems. However, their operation as black-box models introduces increased
uncertainty in predictions. Additionally, they contend with various challenges,
including the need for substantial storage space in large networks, issues of
overfitting, underfitting, vanishing gradients, and more. This study explores
the concept of Bayesian Neural Networks, presenting a novel architecture
designed to significantly alleviate the storage space complexity of a network.
Furthermore, we introduce an algorithm adept at efficiently handling
uncertainties, ensuring robust convergence values without becoming trapped in
local optima, particularly when the objective function lacks perfect convexity.</div><div><a href='http://arxiv.org/abs/2403.04810v1'>2403.04810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13410v1")'>Bayesian Neural Networks with Domain Knowledge Priors</div>
<div id='2402.13410v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T22:34:53Z</div><div>Authors: Dylan Sam, Rattana Pukdee, Daniel P. Jeong, Yewon Byun, J. Zico Kolter</div><div style='padding-top: 10px; width: 80ex'>Bayesian neural networks (BNNs) have recently gained popularity due to their
ability to quantify model uncertainty. However, specifying a prior for BNNs
that captures relevant domain knowledge is often extremely challenging. In this
work, we propose a framework for integrating general forms of domain knowledge
(i.e., any knowledge that can be represented by a loss function) into a BNN
prior through variational inference, while enabling computationally efficient
posterior inference and sampling. Specifically, our approach results in a prior
over neural network weights that assigns high probability mass to models that
better align with our domain knowledge, leading to posterior samples that also
exhibit this behavior. We show that BNNs using our proposed domain knowledge
priors outperform those with standard priors (e.g., isotropic Gaussian,
Gaussian process), successfully incorporating diverse types of prior
information such as fairness, physics rules, and healthcare knowledge and
achieving better predictive performance. We also present techniques for
transferring the learned priors across different model architectures,
demonstrating their broad utility across various settings.</div><div><a href='http://arxiv.org/abs/2402.13410v1'>2402.13410v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14532v1")'>A Framework for Variational Inference of Lightweight Bayesian Neural
  Networks with Heteroscedastic Uncertainties</div>
<div id='2402.14532v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T13:24:43Z</div><div>Authors: David J. Schodt, Ryan Brown, Michael Merritt, Samuel Park, Delsin Menolascino, Mark A. Peot</div><div style='padding-top: 10px; width: 80ex'>Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural
Network (BNN) is vital to many applications. Often, heteroscedastic aleatoric
uncertainties are learned as outputs of the BNN in addition to the predictive
means, however doing so may necessitate adding more learnable parameters to the
network. In this work, we demonstrate that both the heteroscedastic aleatoric
and epistemic variance can be embedded into the variances of learned BNN
parameters, improving predictive performance for lightweight networks. By
complementing this approach with a moment propagation approach to inference, we
introduce a relatively simple framework for sampling-free variational inference
suitable for lightweight BNNs.</div><div><a href='http://arxiv.org/abs/2402.14532v1'>2402.14532v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12418v1")'>Towards Improved Variational Inference for Deep Bayesian Models</div>
<div id='2401.12418v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T00:40:20Z</div><div>Authors: Sebastian W. Ober</div><div style='padding-top: 10px; width: 80ex'>Deep learning has revolutionized the last decade, being at the forefront of
extraordinary advances in a wide range of tasks including computer vision,
natural language processing, and reinforcement learning, to name but a few.
However, it is well-known that deep models trained via maximum likelihood
estimation tend to be overconfident and give poorly-calibrated predictions.
Bayesian deep learning attempts to address this by placing priors on the model
parameters, which are then combined with a likelihood to perform posterior
inference. Unfortunately, for deep models, the true posterior is intractable,
forcing the user to resort to approximations. In this thesis, we explore the
use of variational inference (VI) as an approximation, as it is unique in
simultaneously approximating the posterior and providing a lower bound to the
marginal likelihood. If tight enough, this lower bound can be used to optimize
hyperparameters and to facilitate model selection. However, this capacity has
rarely been used to its full extent for Bayesian neural networks, likely
because the approximate posteriors typically used in practice can lack the
flexibility to effectively bound the marginal likelihood. We therefore explore
three aspects of Bayesian learning for deep models: 1) we ask whether it is
necessary to perform inference over as many parameters as possible, or whether
it is reasonable to treat many of them as optimizable hyperparameters; 2) we
propose a variational posterior that provides a unified view of inference in
Bayesian neural networks and deep Gaussian processes; 3) we demonstrate how VI
can be improved in certain deep Gaussian process models by analytically
removing symmetries from the posterior, and performing inference on Gram
matrices instead of features. We hope that our contributions will provide a
stepping stone to fully realize the promises of VI in the future.</div><div><a href='http://arxiv.org/abs/2401.12418v1'>2401.12418v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10671v1")'>Hessian-Free Laplace in Bayesian Deep Learning</div>
<div id='2403.10671v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T20:47:39Z</div><div>Authors: James McInerney, Nathan Kallus</div><div style='padding-top: 10px; width: 80ex'>The Laplace approximation (LA) of the Bayesian posterior is a Gaussian
distribution centered at the maximum a posteriori estimate. Its appeal in
Bayesian deep learning stems from the ability to quantify uncertainty post-hoc
(i.e., after standard network parameter optimization), the ease of sampling
from the approximate posterior, and the analytic form of model evidence.
However, an important computational bottleneck of LA is the necessary step of
calculating and inverting the Hessian matrix of the log posterior. The Hessian
may be approximated in a variety of ways, with quality varying with a number of
factors including the network, dataset, and inference task. In this paper, we
propose an alternative framework that sidesteps Hessian calculation and
inversion. The Hessian-free Laplace (HFL) approximation uses curvature of both
the log posterior and network prediction to estimate its variance. Only two
point estimates are needed: the standard maximum a posteriori parameter and the
optimal parameter under a loss regularized by the network prediction. We show
that, under standard assumptions of LA in Bayesian deep learning, HFL targets
the same variance as LA, and can be efficiently amortized in a pre-trained
network. Experiments demonstrate comparable performance to that of exact and
approximate Hessians, with excellent coverage for in-between uncertainty.</div><div><a href='http://arxiv.org/abs/2403.10671v1'>2403.10671v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14758v1")'>Batch and match: black-box variational inference with a score-based
  divergence</div>
<div id='2402.14758v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:20:22Z</div><div>Authors: Diana Cai, Chirag Modi, Loucas Pillaud-Vivien, Charles C. Margossian, Robert M. Gower, David M. Blei, Lawrence K. Saul</div><div style='padding-top: 10px; width: 80ex'>Most leading implementations of black-box variational inference (BBVI) are
based on optimizing a stochastic evidence lower bound (ELBO). But such
approaches to BBVI often converge slowly due to the high variance of their
gradient estimates. In this work, we propose batch and match (BaM), an
alternative approach to BBVI based on a score-based divergence. Notably, this
score-based divergence can be optimized by a closed-form proximal update for
Gaussian variational families with full covariance matrices. We analyze the
convergence of BaM when the target distribution is Gaussian, and we prove that
in the limit of infinite batch size the variational parameter updates converge
exponentially quickly to the target mean and covariance. We also evaluate the
performance of BaM on Gaussian and non-Gaussian target distributions that arise
from posterior inference in hierarchical and deep generative models. In these
experiments, we find that BaM typically converges in fewer (and sometimes
significantly fewer) gradient evaluations than leading implementations of BBVI
based on ELBO maximization.</div><div><a href='http://arxiv.org/abs/2402.14758v1'>2402.14758v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13748v1")'>An Ordering of Divergences for Variational Inference with Factorized
  Gaussian Approximations</div>
<div id='2403.13748v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T16:56:08Z</div><div>Authors: Charles C. Margossian, Loucas Pillaud-Vivien, Lawrence K. Saul</div><div style='padding-top: 10px; width: 80ex'>Given an intractable distribution $p$, the problem of variational inference
(VI) is to compute the best approximation $q$ from some more tractable family
$\mathcal{Q}$. Most commonly the approximation is found by minimizing a
Kullback-Leibler (KL) divergence. However, there exist other valid choices of
divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence
champions a different solution. We analyze how the choice of divergence affects
the outcome of VI when a Gaussian with a dense covariance matrix is
approximated by a Gaussian with a diagonal covariance matrix. In this setting
we show that different divergences can be \textit{ordered} by the amount that
their variational approximations misestimate various measures of uncertainty,
such as the variance, precision, and entropy. We also derive an impossibility
theorem showing that no two of these measures can be simultaneously matched by
a factorized approximation; hence, the choice of divergence informs which
measure, if any, is correctly estimated. Our analysis covers the KL divergence,
the R\'enyi divergences, and a score-based divergence that compares $\nabla\log
p$ and $\nabla\log q$. We empirically evaluate whether these orderings hold
when VI is used to approximate non-Gaussian distributions.</div><div><a href='http://arxiv.org/abs/2403.13748v1'>2403.13748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01272v1")'>Can a Confident Prior Replace a Cold Posterior?</div>
<div id='2403.01272v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T17:28:55Z</div><div>Authors: Martin Marek, Brooks Paige, Pavel Izmailov</div><div style='padding-top: 10px; width: 80ex'>Benchmark datasets used for image classification tend to have very low levels
of label noise. When Bayesian neural networks are trained on these datasets,
they often underfit, misrepresenting the aleatoric uncertainty of the data. A
common solution is to cool the posterior, which improves fit to the training
data but is challenging to interpret from a Bayesian perspective. We explore
whether posterior tempering can be replaced by a confidence-inducing prior
distribution. First, we introduce a "DirClip" prior that is practical to sample
and nearly matches the performance of a cold posterior. Second, we introduce a
"confidence prior" that directly approximates a cold likelihood in the limit of
decreasing temperature but cannot be easily sampled. Lastly, we provide several
general insights into confidence-inducing priors, such as when they might
diverge and how fine-tuning can mitigate numerical instability.</div><div><a href='http://arxiv.org/abs/2403.01272v1'>2403.01272v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04933v1")'>Rethinking Test-time Likelihood: The Likelihood Path Principle and Its
  Application to OOD Detection</div>
<div id='2401.04933v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T05:07:14Z</div><div>Authors: Sicong Huang, Jiawei He, Kry Yik Chau Lui</div><div style='padding-top: 10px; width: 80ex'>While likelihood is attractive in theory, its estimates by deep generative
models (DGMs) are often broken in practice, and perform poorly for out of
distribution (OOD) Detection. Various recent works started to consider
alternative scores and achieved better performances. However, such recipes do
not come with provable guarantees, nor is it clear that their choices extract
sufficient information.
  We attempt to change this by conducting a case study on variational
autoencoders (VAEs). First, we introduce the likelihood path (LPath) principle,
generalizing the likelihood principle. This narrows the search for informative
summary statistics down to the minimal sufficient statistics of VAEs'
conditional likelihoods. Second, introducing new theoretic tools such as nearly
essential support, essential distance and co-Lipschitzness, we obtain
non-asymptotic provable OOD detection guarantees for certain distillation of
the minimal sufficient statistics. The corresponding LPath algorithm
demonstrates SOTA performances, even using simple and small VAEs with poor
likelihood estimates. To our best knowledge, this is the first provable
unsupervised OOD method that delivers excellent empirical results, better than
any other VAEs based techniques. We use the same model as
\cite{xiao2020likelihood}, open sourced from:
https://github.com/XavierXiao/Likelihood-Regret</div><div><a href='http://arxiv.org/abs/2401.04933v1'>2401.04933v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15710v1")'>A Statistical Analysis of Wasserstein Autoencoders for Intrinsically
  Low-dimensional Data</div>
<div id='2402.15710v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T04:13:40Z</div><div>Authors: Saptarshi Chakraborty, Peter L. Bartlett</div><div style='padding-top: 10px; width: 80ex'>Variational Autoencoders (VAEs) have gained significant popularity among
researchers as a powerful tool for understanding unknown distributions based on
limited samples. This popularity stems partly from their impressive performance
and partly from their ability to provide meaningful feature representations in
the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to
not only improve model efficiency but also interpretability. However, there has
been limited focus on analyzing their statistical guarantees. The matter is
further complicated by the fact that the data distributions to which WAEs are
applied - such as natural images - are often presumed to possess an underlying
low-dimensional structure within a high-dimensional feature space, which
current theory does not adequately account for, rendering known bounds
inefficient. To bridge the gap between the theory and practice of WAEs, in this
paper, we show that WAEs can learn the data distributions when the network
architectures are properly chosen. We show that the convergence rates of the
expected excess risk in the number of samples for WAEs are independent of the
high feature dimension, instead relying only on the intrinsic dimension of the
data distribution.</div><div><a href='http://arxiv.org/abs/2402.15710v1'>2402.15710v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03069v1")'>Improving Variational Autoencoder Estimation from Incomplete Data with
  Mixture Variational Families</div>
<div id='2403.03069v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T15:57:52Z</div><div>Authors: Vaidotas Simkus, Michael U. Gutmann</div><div style='padding-top: 10px; width: 80ex'>We consider the task of estimating variational autoencoders (VAEs) when the
training data is incomplete. We show that missing data increases the complexity
of the model's posterior distribution over the latent variables compared to the
fully-observed case. The increased complexity may adversely affect the fit of
the model due to a mismatch between the variational and model posterior
distributions. We introduce two strategies based on (i) finite
variational-mixture and (ii) imputation-based variational-mixture distributions
to address the increased posterior complexity. Through a comprehensive
evaluation of the proposed approaches, we show that variational mixtures are
effective at improving the accuracy of VAE estimation from incomplete data.</div><div><a href='http://arxiv.org/abs/2403.03069v1'>2403.03069v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05300v1")'>Unity by Diversity: Improved Representation Learning in Multimodal VAEs</div>
<div id='2403.05300v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T13:29:46Z</div><div>Authors: Thomas M. Sutter, Yang Meng, Norbert Fortin, Julia E. Vogt, Stephan Mandt</div><div style='padding-top: 10px; width: 80ex'>Variational Autoencoders for multimodal data hold promise for many tasks in
data analysis, such as representation learning, conditional generation, and
imputation. Current architectures either share the encoder output, decoder
input, or both across modalities to learn a shared representation. Such
architectures impose hard constraints on the model. In this work, we show that
a better latent representation can be obtained by replacing these hard
constraints with a soft constraint. We propose a new mixture-of-experts prior,
softly guiding each modality's latent representation towards a shared aggregate
posterior. This approach results in a superior latent representation and allows
each encoding to preserve information from its uncompressed original features
better. In extensive experiments on multiple benchmark datasets and a
challenging real-world neuroscience data set, we show improved learned latent
representations and imputation of missing data modalities compared to existing
methods.</div><div><a href='http://arxiv.org/abs/2403.05300v1'>2403.05300v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06338v1")'>Disentangling shared and private latent factors in multimodal
  Variational Autoencoders</div>
<div id='2403.06338v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T23:11:05Z</div><div>Authors: Kaspar Märtens, Christopher Yau</div><div style='padding-top: 10px; width: 80ex'>Generative models for multimodal data permit the identification of latent
factors that may be associated with important determinants of observed data
heterogeneity. Common or shared factors could be important for explaining
variation across modalities whereas other factors may be private and important
only for the explanation of a single modality. Multimodal Variational
Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those
underlying latent factors and separating shared variation from private. In this
work, we investigate their capability to reliably perform this disentanglement.
In particular, we highlight a challenging problem setting where
modality-specific variation dominates the shared signal. Taking a cross-modal
prediction perspective, we demonstrate limitations of existing models, and
propose a modification how to make them more robust to modality-specific
variation. Our findings are supported by experiments on synthetic as well as
various real-world multi-omics data sets.</div><div><a href='http://arxiv.org/abs/2403.06338v1'>2403.06338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07456v1")'>A tutorial on multi-view autoencoders using the multi-view-AE library</div>
<div id='2403.07456v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T09:51:05Z</div><div>Authors: Ana Lawry Aguila, Andre Altmann</div><div style='padding-top: 10px; width: 80ex'>There has been a growing interest in recent years in modelling multiple
modalities (or views) of data to for example, understand the relationship
between modalities or to generate missing data. Multi-view autoencoders have
gained significant traction for their adaptability and versatility in modelling
multi-modal data, demonstrating an ability to tailor their approach to suit the
characteristics of the data at hand. However, most multi-view autoencoders have
inconsistent notation and are often implemented using different coding
frameworks. To address this, we present a unified mathematical framework for
multi-view autoencoders, consolidating their formulations. Moreover, we offer
insights into the motivation and theoretical advantages of each model. To
facilitate accessibility and practical use, we extend the documentation and
functionality of the previously introduced \texttt{multi-view-AE} library. This
library offers Python implementations of numerous multi-view autoencoder
models, presented within a user-friendly framework. Through benchmarking
experiments, we evaluate our implementations against previous ones,
demonstrating comparable or superior performance. This work aims to establish a
cohesive foundation for multi-modal modelling, serving as a valuable
educational resource in the field.</div><div><a href='http://arxiv.org/abs/2403.07456v1'>2403.07456v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16497v2")'>A Bayesian Gaussian Process-Based Latent Discriminative Generative
  Decoder (LDGD) Model for High-Dimensional Data</div>
<div id='2401.16497v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T19:11:03Z</div><div>Authors: Navid Ziaei, Behzad Nazari, Uri T. Eden, Alik Widge, Ali Yousefi</div><div style='padding-top: 10px; width: 80ex'>Extracting meaningful information from high-dimensional data poses a
formidable modeling challenge, particularly when the data is obscured by noise
or represented through different modalities. This research proposes a novel
non-parametric modeling approach, leveraging the Gaussian process (GP), to
characterize high-dimensional data by mapping it to a latent low-dimensional
manifold. This model, named the latent discriminative generative decoder
(LDGD), employs both the data and associated labels in the manifold discovery
process. We derive a Bayesian solution to infer the latent variables, allowing
LDGD to effectively capture inherent stochasticity in the data. We demonstrate
applications of LDGD on both synthetic and benchmark datasets. Not only does
LDGD infer the manifold accurately, but its accuracy in predicting data points'
labels surpasses state-of-the-art approaches. In the development of LDGD, we
have incorporated inducing points to reduce the computational complexity of
Gaussian processes for large datasets, enabling batch training for enhanced
efficient processing and scalability. Additionally, we show that LDGD can
robustly infer manifold and precisely predict labels for scenarios in that data
size is limited, demonstrating its capability to efficiently characterize
high-dimensional data with limited samples. These collective attributes
highlight the importance of developing non-parametric modeling approaches to
analyze high-dimensional data.</div><div><a href='http://arxiv.org/abs/2401.16497v2'>2401.16497v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14031v1")'>Autoencoder with Ordered Variance for Nonlinear Model Identification</div>
<div id='2402.14031v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T11:34:19Z</div><div>Authors: Midhun T. Augustine, Parag Patil, Mani Bhushan, Sharad Bhartiya</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel autoencoder with ordered variance (AEO) in which
the loss function is modified with a variance regularization term to enforce
order in the latent space. Further, the autoencoder is modified using ResNets,
which results in a ResNet AEO (RAEO). The paper also illustrates the
effectiveness of AEO and RAEO in extracting nonlinear relationships among input
variables in an unsupervised setting.</div><div><a href='http://arxiv.org/abs/2402.14031v1'>2402.14031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01855v1")'>Transformer Neural Autoregressive Flows</div>
<div id='2401.01855v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T17:51:16Z</div><div>Authors: Massimiliano Patacchiola, Aliaksandra Shysheya, Katja Hofmann, Richard E. Turner</div><div style='padding-top: 10px; width: 80ex'>Density estimation, a central problem in machine learning, can be performed
using Normalizing Flows (NFs). NFs comprise a sequence of invertible
transformations, that turn a complex target distribution into a simple one, by
exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs)
and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant
members of the NF family. However, they suffer scalability issues and training
instability due to the constraints imposed on the network structure. In this
paper, we propose a novel solution to these challenges by exploiting
transformers to define a new class of neural flows called Transformer Neural
Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable
as a separate input token, using attention masking to enforce an autoregressive
constraint. We take an amortization-inspired approach where the transformer
outputs the parameters of an invertible transformation. The experimental
results demonstrate that T-NAFs consistently match or outperform NAFs and
B-NAFs across multiple datasets from the UCI benchmark. Remarkably, T-NAFs
achieve these results using an order of magnitude fewer parameters than
previous approaches, without composing multiple flows.</div><div><a href='http://arxiv.org/abs/2401.01855v1'>2401.01855v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10754v2")'>Data Augmentation for Traffic Classification</div>
<div id='2401.10754v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T15:25:09Z</div><div>Authors: Chao Wang, Alessandro Finamore, Pietro Michiardi, Massimo Gallo, Dario Rossi</div><div style='padding-top: 10px; width: 80ex'>Data Augmentation (DA) -- enriching training data by adding synthetic samples
-- is a technique widely adopted in Computer Vision (CV) and Natural Language
Processing (NLP) tasks to improve models performance. Yet, DA has struggled to
gain traction in networking contexts, particularly in Traffic Classification
(TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation
functions applied to 3 TC datasets using packet time series as input
representation and considering a variety of training conditions. Our results
show that (i) DA can reap benefits previously unexplored, (ii) augmentations
acting on time series sequence order and masking are better suited for TC than
amplitude augmentations and (iii) basic models latent space analysis can help
understanding the positive/negative effects of augmentations on classification
performance.</div><div><a href='http://arxiv.org/abs/2401.10754v2'>2401.10754v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02526v1")'>Branched Variational Autoencoder Classifiers</div>
<div id='2401.02526v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T20:29:44Z</div><div>Authors: Ahmed Salah, David Yevick</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a modified variational autoencoder (VAEs) that contains
an additional neural network branch. The resulting branched VAE (BVAE)
contributes a classification component based on the class labels to the total
loss and therefore imparts categorical information to the latent
representation. As a result, the latent space distributions of the input
classes are separated and ordered, thereby enhancing the classification
accuracy. The degree of improvement is quantified by numerical calculations
employing the benchmark MNIST dataset for both unrotated and rotated digits.
The proposed technique is then compared to and then incorporated into a VAE
with fixed output distributions. This procedure is found to yield improved
performance for a wide range of output distributions.</div><div><a href='http://arxiv.org/abs/2401.02526v1'>2401.02526v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03724v1")'>Statistical Test for Anomaly Detections by Variational Auto-Encoders</div>
<div id='2402.03724v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T05:42:27Z</div><div>Authors: Daiki Miwa, Tomohiro Shiraishi, Vo Nguyen Le Duy, Teruyuki Katsuoka, Ichiro Takeuchi</div><div style='padding-top: 10px; width: 80ex'>In this study, we consider the reliability assessment of anomaly detection
(AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD
has been actively studied in various perspective, from method development to
applied research. However, when the results of ADs are used in high-stakes
decision-making, such as in medical diagnosis, it is necessary to ensure the
reliability of the detected anomalies. In this study, we propose the VAE-AD
Test as a method for quantifying the statistical reliability of VAE-based AD
within the framework of statistical testing. Using the VAE-AD Test, the
reliability of the anomaly regions detected by a VAE can be quantified in the
form of p-values. This means that if an anomaly is declared when the p-value is
below a certain threshold, it is possible to control the probability of false
detection to a desired level. Since the VAE-AD Test is constructed based on a
new statistical inference framework called selective inference, its validity is
theoretically guaranteed in finite samples. To demonstrate the validity and
effectiveness of the proposed VAE-AD Test, numerical experiments on artificial
data and applications to brain image analysis are conducted.</div><div><a href='http://arxiv.org/abs/2402.03724v1'>2402.03724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09303v2")'>Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical
  Perspective</div>
<div id='2403.09303v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T11:51:01Z</div><div>Authors: Yu Cai, Hao Chen, Kwang-Ting Cheng</div><div style='padding-top: 10px; width: 80ex'>Medical anomaly detection aims to identify abnormal findings using only
normal training data, playing a crucial role in health screening and
recognizing rare diseases. Reconstruction-based methods, particularly those
utilizing autoencoders (AEs), are dominant in this field. They work under the
assumption that AEs trained on only normal data cannot reconstruct unseen
abnormal regions well, thereby enabling the anomaly detection based on
reconstruction errors. However, this assumption does not always hold due to the
mismatch between the reconstruction training objective and the anomaly
detection task objective, rendering these methods theoretically unsound. This
study focuses on providing a theoretical foundation for AE-based reconstruction
methods in anomaly detection. By leveraging information theory, we elucidate
the principles of these methods and reveal that the key to improving AE in
anomaly detection lies in minimizing the information entropy of latent vectors.
Experiments on four datasets with two image modalities validate the
effectiveness of our theory. To the best of our knowledge, this is the first
effort to theoretically clarify the principles and design philosophy of AE for
anomaly detection. Code will be available upon acceptance.</div><div><a href='http://arxiv.org/abs/2403.09303v2'>2403.09303v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10637v1")'>Towards Universal Unsupervised Anomaly Detection in Medical Imaging</div>
<div id='2401.10637v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T11:35:07Z</div><div>Authors: Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, Julia A. Schnabel</div><div style='padding-top: 10px; width: 80ex'>The increasing complexity of medical imaging data underscores the need for
advanced anomaly detection methods to automatically identify diverse
pathologies. Current methods face challenges in capturing the broad spectrum of
anomalies, often limiting their use to specific lesion types in brain scans. To
address this challenge, we introduce a novel unsupervised approach, termed
\textit{Reversed Auto-Encoders (RA)}, designed to create realistic
pseudo-healthy reconstructions that enable the detection of a wider range of
pathologies. We evaluate the proposed method across various imaging modalities,
including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray,
and chest X-ray, and demonstrate superior performance in detecting anomalies
compared to existing state-of-the-art methods. Our unsupervised anomaly
detection approach may enhance diagnostic accuracy in medical imaging by
identifying a broader range of unknown pathologies. Our code is publicly
available at: \url{https://github.com/ci-ber/RA}.</div><div><a href='http://arxiv.org/abs/2401.10637v1'>2401.10637v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14262v1")'>Diffusion Models with Ensembled Structure-Based Anomaly Scoring for
  Unsupervised Anomaly Detection</div>
<div id='2403.14262v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T09:50:39Z</div><div>Authors: Finn Behrendt, Debayan Bhattacharya, Lennart Maack, Julia Krüger, Roland Opfer, Robin Mieling, Alexander Schlaefer</div><div style='padding-top: 10px; width: 80ex'>Supervised deep learning techniques show promise in medical image analysis.
However, they require comprehensive annotated data sets, which poses
challenges, particularly for rare diseases. Consequently, unsupervised anomaly
detection (UAD) emerges as a viable alternative for pathology segmentation, as
only healthy data is required for training. However, recent UAD anomaly scoring
functions often focus on intensity only and neglect structural differences,
which impedes the segmentation performance. This work investigates the
potential of Structural Similarity (SSIM) to bridge this gap. SSIM captures
both intensity and structural disparities and can be advantageous over the
classical $l1$ error. However, we show that there is more than one optimal
kernel size for the SSIM calculation for different pathologies. Therefore, we
investigate an adaptive ensembling strategy for various kernel sizes to offer a
more pathology-agnostic scoring mechanism. We demonstrate that this ensembling
strategy can enhance the performance of DMs and mitigate the sensitivity to
different kernel sizes across varying pathologies, highlighting its promise for
brain MRI anomaly detection.</div><div><a href='http://arxiv.org/abs/2403.14262v1'>2403.14262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01013v1")'>Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact
  Detection with Self-Supervised Learning</div>
<div id='2401.01013v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T04:00:48Z</div><div>Authors: Thanh-Dung Le</div><div style='padding-top: 10px; width: 80ex'>Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU)
has revealed that traditional machine learning methods, such as semi-supervised
label propagation and K-nearest neighbors, outperform Transformer-based models
in artifact detection from PPG signals, mainly when data is limited. This study
addresses the underutilization of abundant unlabeled data by employing
self-supervised learning (SSL) to extract latent features from these data,
followed by fine-tuning on labeled data. Our experiments demonstrate that SSL
significantly enhances the Transformer model's ability to learn
representations, improving its robustness in artifact classification tasks.
Among various SSL techniques, including masking, contrastive learning, and DINO
(self-distillation with no labels)-contrastive learning exhibited the most
stable and superior performance in small PPG datasets. Further, we delve into
optimizing contrastive loss functions, which are crucial for contrastive SSL.
Inspired by InfoNCE, we introduce a novel contrastive loss function that
facilitates smoother training and better convergence, thereby enhancing
performance in artifact classification. In summary, this study establishes the
efficacy of SSL in leveraging unlabeled data, particularly in enhancing the
capabilities of the Transformer model. This approach holds promise for broader
applications in PICU environments, where annotated data is often limited.</div><div><a href='http://arxiv.org/abs/2401.01013v1'>2401.01013v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02188v1")'>Diabetes detection using deep learning techniques with oversampling and
  feature augmentation</div>
<div id='2402.02188v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T15:30:20Z</div><div>Authors: María Teresa García-Ordás, Carmen Benavides, José Alberto Benítez-Andrades, Héctor Alaiz-Moretón, Isaías García-Rodríguez</div><div style='padding-top: 10px; width: 80ex'>Background and objective: Diabetes is a chronic pathology which is affecting
more and more people over the years. It gives rise to a large number of deaths
each year. Furthermore, many people living with the disease do not realize the
seriousness of their health status early enough. Late diagnosis brings about
numerous health problems and a large number of deaths each year so the
development of methods for the early diagnosis of this pathology is essential.
  Methods: In this paper, a pipeline based on deep learning techniques is
proposed to predict diabetic people. It includes data augmentation using a
variational autoencoder (VAE), feature augmentation using an sparse autoencoder
(SAE) and a convolutional neural network for classification. Pima Indians
Diabetes Database, which takes into account information on the patients such as
the number of pregnancies, glucose or insulin level, blood pressure or age, has
been evaluated.
  Results: A 92.31% of accuracy was obtained when CNN classifier is trained
jointly the SAE for featuring augmentation over a well balanced dataset. This
means an increment of 3.17% of accuracy with respect the state-of-the-art.
  Conclusions: Using a full deep learning pipeline for data preprocessing and
classification has demonstrate to be very promising in the diabetes detection
field outperforming the state-of-the-art proposals.</div><div><a href='http://arxiv.org/abs/2402.02188v1'>2402.02188v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07483v1")'>A Deep Learning Approach to Diabetes Diagnosis</div>
<div id='2403.07483v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:18:59Z</div><div>Authors: Zeyu Zhang, Khandaker Asif Ahmed, Md Rakibul Hasan, Tom Gedeon, Md Zakir Hossain</div><div style='padding-top: 10px; width: 80ex'>Diabetes, resulting from inadequate insulin production or utilization, causes
extensive harm to the body. Existing diagnostic methods are often invasive and
come with drawbacks, such as cost constraints. Although there are machine
learning models like Classwise k Nearest Neighbor (CkNN) and General Regression
Neural Network (GRNN), they struggle with imbalanced data and result in
under-performance. Leveraging advancements in sensor technology and machine
learning, we propose a non-invasive diabetes diagnosis using a Back Propagation
Neural Network (BPNN) with batch normalization, incorporating data re-sampling
and normalization for class balancing. Our method addresses existing challenges
such as limited performance associated with traditional machine learning.
Experimental results on three datasets show significant improvements in overall
accuracy, sensitivity, and specificity compared to traditional methods.
Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in
CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores
the potential of deep learning models for robust diabetes diagnosis. See
project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/</div><div><a href='http://arxiv.org/abs/2403.07483v1'>2403.07483v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05756v1")'>Model-Free Local Recalibration of Neural Networks</div>
<div id='2403.05756v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T01:58:45Z</div><div>Authors: R. Torres, D. J. Nott, S. A. Sisson, T. Rodrigues, J. G. Reis, G. S. Rodrigues</div><div style='padding-top: 10px; width: 80ex'>Artificial neural networks (ANNs) are highly flexible predictive models.
However, reliably quantifying uncertainty for their predictions is a continuing
challenge. There has been much recent work on "recalibration" of predictive
distributions for ANNs, so that forecast probabilities for events of interest
are consistent with certain frequency evaluations of them. Uncalibrated
probabilistic forecasts are of limited use for many important decision-making
tasks. To address this issue, we propose a localized recalibration of ANN
predictive distributions using the dimension-reduced representation of the
input provided by the ANN hidden layers. Our novel method draws inspiration
from recalibration techniques used in the literature on approximate Bayesian
computation and likelihood-free inference methods. Most existing calibration
methods for ANNs can be thought of as calibrating either on the input layer,
which is difficult when the input is high-dimensional, or the output layer,
which may not be sufficiently flexible. Through a simulation study, we
demonstrate that our method has good performance compared to alternative
approaches, and explore the benefits that can be achieved by localizing the
calibration based on different layers of the network. Finally, we apply our
proposed method to a diamond price prediction problem, demonstrating the
potential of our approach to improve prediction and uncertainty quantification
in real-world applications.</div><div><a href='http://arxiv.org/abs/2403.05756v1'>2403.05756v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11964v1")'>Probabilistic Calibration by Design for Neural Network Regression</div>
<div id='2403.11964v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:04:33Z</div><div>Authors: Victor Dheur, Souhaib Ben Taieb</div><div style='padding-top: 10px; width: 80ex'>Generating calibrated and sharp neural network predictive distributions for
regression problems is essential for optimal decision-making in many real-world
applications. To address the miscalibration issue of neural networks, various
methods have been proposed to improve calibration, including post-hoc methods
that adjust predictions after training and regularization methods that act
during training. While post-hoc methods have shown better improvement in
calibration compared to regularization methods, the post-hoc step is completely
independent of model training. We introduce a novel end-to-end model training
procedure called Quantile Recalibration Training, integrating post-hoc
calibration directly into the training process without additional parameters.
We also present a unified algorithm that includes our method and other post-hoc
and regularization methods, as particular cases. We demonstrate the performance
of our method in a large-scale experiment involving 57 tabular regression
datasets, showcasing improved predictive accuracy while maintaining
calibration. We also conduct an ablation study to evaluate the significance of
different components within our proposed method, as well as an in-depth
analysis of the impact of the base model and different hyperparameters on
predictive accuracy.</div><div><a href='http://arxiv.org/abs/2403.11964v1'>2403.11964v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13655v1")'>Stable Update of Regression Trees</div>
<div id='2402.13655v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:41:56Z</div><div>Authors: Morten Blørstad, Berent Å. S. Lunde, Nello Blaser</div><div style='padding-top: 10px; width: 80ex'>Updating machine learning models with new information usually improves their
predictive performance, yet, in many applications, it is also desirable to
avoid changing the model predictions too much. This property is called
stability. In most cases when stability matters, so does explainability. We
therefore focus on the stability of an inherently explainable machine learning
method, namely regression trees. We aim to use the notion of empirical
stability and design algorithms for updating regression trees that provide a
way to balance between predictability and empirical stability. To achieve this,
we propose a regularization method, where data points are weighted based on the
uncertainty in the initial model. The balance between predictability and
empirical stability can be adjusted through hyperparameters. This
regularization method is evaluated in terms of loss and stability and assessed
on a broad range of data characteristics. The results show that the proposed
update method improves stability while achieving similar or better predictive
performance. This shows that it is possible to achieve both predictive and
stable results when updating regression trees.</div><div><a href='http://arxiv.org/abs/2402.13655v1'>2402.13655v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01128v2")'>Sensitivity Analysis On Loss Landscape</div>
<div id='2403.01128v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T08:18:32Z</div><div>Authors: Salman Faroz</div><div style='padding-top: 10px; width: 80ex'>Gradients can be employed for sensitivity analysis. Here, we leverage the
advantages of the Loss Landscape to comprehend which independent variables
impact the dependent variable. We seek to grasp the loss landscape by utilizing
first, second, and third derivatives through automatic differentiation. we know
that Spearman's rank correlation coefficient can detect the monotonic
relationship between two variables. However, I have found that second-order
gradients, with certain configurations and parameters, provide information that
can be visualized similarly to Spearman results, In this approach, we
incorporate a loss function with an activation function, resulting in a
non-linear pattern. Each exploration of the loss landscape through retraining
yields new valuable information. Furthermore, the first and third derivatives
are also beneficial, as they indicate the extent to which independent variables
influence the dependent variable.</div><div><a href='http://arxiv.org/abs/2403.01128v2'>2403.01128v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13425v1")'>Investigating the Histogram Loss in Regression</div>
<div id='2402.13425v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T23:29:41Z</div><div>Authors: Ehsan Imani, Kai Luedemann, Sam Scholnick-Hughes, Esraa Elelimy, Martha White</div><div style='padding-top: 10px; width: 80ex'>It is becoming increasingly common in regression to train neural networks
that model the entire distribution even if only the mean is required for
prediction. This additional modeling often comes with performance gain and the
reasons behind the improvement are not fully known. This paper investigates a
recent approach to regression, the Histogram Loss, which involves learning the
conditional distribution of the target variable by minimizing the cross-entropy
between a target distribution and a flexible histogram prediction. We design
theoretical and empirical analyses to determine why and when this performance
gain appears, and how different components of the loss contribute to it. Our
results suggest that the benefits of learning distributions in this setup come
from improvements in optimization rather than learning a better representation.
We then demonstrate the viability of the Histogram Loss in common deep learning
applications without a need for costly hyperparameter tuning.</div><div><a href='http://arxiv.org/abs/2402.13425v1'>2402.13425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14695v1")'>Chain-structured neural architecture search for financial time series
  forecasting</div>
<div id='2403.14695v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T15:05:59Z</div><div>Authors: Denis Levchenko, Efstratios Rappos, Shabnam Ataee, Biagio Nigro, Stephan Robert</div><div style='padding-top: 10px; width: 80ex'>We compare three popular neural architecture search strategies on
chain-structured search spaces: Bayesian optimization, the hyperband method,
and reinforcement learning in the context of financial time series forecasting.</div><div><a href='http://arxiv.org/abs/2403.14695v1'>2403.14695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05600v1")'>Density-Regression: Efficient and Distance-Aware Deep Regressor for
  Uncertainty Estimation under Distribution Shifts</div>
<div id='2403.05600v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T23:20:34Z</div><div>Authors: Ha Manh Bui, Anqi Liu</div><div style='padding-top: 10px; width: 80ex'>Morden deep ensembles technique achieves strong uncertainty estimation
performance by going through multiple forward passes with different models.
This is at the price of a high storage space and a slow speed in the inference
(test) time. To address this issue, we propose Density-Regression, a method
that leverages the density function in uncertainty estimation and achieves fast
inference by a single forward pass. We prove it is distance aware on the
feature space, which is a necessary condition for a neural network to produce
high-quality uncertainty estimation under distribution shifts. Empirically, we
conduct experiments on regression tasks with the cubic toy dataset, benchmark
UCI, weather forecast with time series, and depth estimation under real-world
shifted applications. We show that Density-Regression has competitive
uncertainty estimation performance under distribution shifts with modern deep
regressors while using a lower model size and a faster inference speed.</div><div><a href='http://arxiv.org/abs/2403.05600v1'>2403.05600v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01484v1")'>Connecting the Dots: Is Mode-Connectedness the Key to Feasible
  Sample-Based Inference in Bayesian Neural Networks?</div>
<div id='2402.01484v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T15:12:16Z</div><div>Authors: Emanuel Sommer, Lisa Wimmer, Theodore Papamarkou, Ludwig Bothmann, Bernd Bischl, David Rügamer</div><div style='padding-top: 10px; width: 80ex'>A major challenge in sample-based inference (SBI) for Bayesian neural
networks is the size and structure of the networks' parameter space. Our work
shows that successful SBI is possible by embracing the characteristic
relationship between weight and function space, uncovering a systematic link
between overparameterization and the difficulty of the sampling problem.
Through extensive experiments, we establish practical guidelines for sampling
and convergence diagnosis. As a result, we present a Bayesian deep ensemble
approach as an effective solution with competitive performance and uncertainty
quantification.</div><div><a href='http://arxiv.org/abs/2402.01484v1'>2402.01484v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13335v1")'>Full Bayesian Significance Testing for Neural Networks</div>
<div id='2401.13335v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T09:59:48Z</div><div>Authors: Zehua Liu, Zimeng Li, Jingyuan Wang, Yue He</div><div style='padding-top: 10px; width: 80ex'>Significance testing aims to determine whether a proposition about the
population distribution is the truth or not given observations. However,
traditional significance testing often needs to derive the distribution of the
testing statistic, failing to deal with complex nonlinear relationships. In
this paper, we propose to conduct Full Bayesian Significance Testing for neural
networks, called \textit{n}FBST, to overcome the limitation in relationship
characterization of traditional approaches. A Bayesian neural network is
utilized to fit the nonlinear and multi-dimensional relationships with small
errors and avoid hard theoretical derivation by computing the evidence value.
Besides, \textit{n}FBST can test not only global significance but also local
and instance-wise significance, which previous testing methods don't focus on.
Moreover, \textit{n}FBST is a general framework that can be extended based on
the measures selected, such as Grad-\textit{n}FBST, LRP-\textit{n}FBST,
DeepLIFT-\textit{n}FBST, LIME-\textit{n}FBST. A range of experiments on both
simulated and real data are conducted to show the advantages of our method.</div><div><a href='http://arxiv.org/abs/2401.13335v1'>2401.13335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08324v1")'>Uncertainty Quantification via Stable Distribution Propagation</div>
<div id='2402.08324v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T09:40:19Z</div><div>Authors: Felix Petersen, Aashwin Mishra, Hilde Kuehne, Christian Borgelt, Oliver Deussen, Mikhail Yurochkin</div><div style='padding-top: 10px; width: 80ex'>We propose a new approach for propagating stable probability distributions
through neural networks. Our method is based on local linearization, which we
show to be an optimal approximation in terms of total variation distance for
the ReLU non-linearity. This allows propagating Gaussian and Cauchy input
uncertainties through neural networks to quantify their output uncertainties.
To demonstrate the utility of propagating distributions, we apply the proposed
method to predicting calibrated confidence intervals and selective prediction
on out-of-distribution data. The results demonstrate a broad applicability of
propagating distributions and show the advantages of our method over other
approaches such as moment matching.</div><div><a href='http://arxiv.org/abs/2402.08324v1'>2402.08324v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03495v1")'>Partially Stochastic Infinitely Deep Bayesian Neural Networks</div>
<div id='2402.03495v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T20:15:19Z</div><div>Authors: Sergio Calvo-Ordonez, Matthieu Meunier, Francesco Piatti, Yuantao Shi</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present Partially Stochastic Infinitely Deep Bayesian
Neural Networks, a novel family of architectures that integrates partial
stochasticity into the framework of infinitely deep neural networks. Our new
class of architectures is designed to improve the limitations of existing
architectures around computational efficiency at training and inference time.
To do this, we leverage the advantages of partial stochasticity in the
infinite-depth limit which include the benefits of full stochasticity e.g.
robustness, uncertainty quantification, and memory efficiency, whilst improving
their limitations around computational efficiency at training and inference
time. We present a variety of architectural configurations, offering
flexibility in network design including different methods for weight partition.
We also provide mathematical guarantees on the expressivity of our models by
establishing that our network family qualifies as Universal Conditional
Distribution Approximators. Lastly, empirical evaluations across multiple tasks
show that our proposed architectures achieve better downstream task performance
and uncertainty quantification than their counterparts while being
significantly more efficient.</div><div><a href='http://arxiv.org/abs/2402.03495v1'>2402.03495v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10459v1")'>Understanding the Double Descent Phenomenon in Deep Learning</div>
<div id='2403.10459v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T16:51:24Z</div><div>Authors: Marc Lafon, Alexandre Thomas</div><div style='padding-top: 10px; width: 80ex'>Combining empirical risk minimization with capacity control is a classical
strategy in machine learning when trying to control the generalization gap and
avoid overfitting, as the model class capacity gets larger. Yet, in modern deep
learning practice, very large over-parameterized models (e.g. neural networks)
are optimized to fit perfectly the training data and still obtain great
generalization performance. Past the interpolation point, increasing model
complexity seems to actually lower the test error.
  In this tutorial, we explain the concept of double descent and its
mechanisms. The first section sets the classical statistical learning framework
and introduces the double descent phenomenon. By looking at a number of
examples, section 2 introduces inductive biases that appear to have a key role
in double descent by selecting, among the multiple interpolating solutions, a
smooth empirical risk minimizer. Finally, section 3 explores the double descent
with two linear models, and gives other points of view from recent related
works.</div><div><a href='http://arxiv.org/abs/2403.10459v1'>2403.10459v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03353v2")'>Hypothesis Spaces for Deep Learning</div>
<div id='2403.03353v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T22:42:29Z</div><div>Authors: Rui Wang, Yuesheng Xu, Mingsong Yan</div><div style='padding-top: 10px; width: 80ex'>This paper introduces a hypothesis space for deep learning that employs deep
neural networks (DNNs). By treating a DNN as a function of two variables, the
physical variable and parameter variable, we consider the primitive set of the
DNNs for the parameter variable located in a set of the weight matrices and
biases determined by a prescribed depth and widths of the DNNs. We then
complete the linear span of the primitive DNN set in a weak* topology to
construct a Banach space of functions of the physical variable. We prove that
the Banach space so constructed is a reproducing kernel Banach space (RKBS) and
construct its reproducing kernel. We investigate two learning models,
regularized learning and minimum interpolation problem in the resulting RKBS,
by establishing representer theorems for solutions of the learning models. The
representer theorems unfold that solutions of these learning models can be
expressed as linear combination of a finite number of kernel sessions
determined by given data and the reproducing kernel.</div><div><a href='http://arxiv.org/abs/2403.03353v2'>2403.03353v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04978v1")'>Closed-Form Interpretation of Neural Network Classifiers with Symbolic
  Regression Gradients</div>
<div id='2401.04978v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T07:47:42Z</div><div>Authors: Sebastian Johann Wetzel</div><div style='padding-top: 10px; width: 80ex'>I introduce a unified framework for interpreting neural network classifiers
tailored toward automated scientific discovery. In contrast to neural
network-based regression, for classification, it is in general impossible to
find a one-to-one mapping from the neural network to a symbolic equation even
if the neural network itself bases its classification on a quantity that can be
written as a closed-form equation. In this paper, I embed a trained neural
network into an equivalence class of classifying functions that base their
decisions on the same quantity. I interpret neural networks by finding an
intersection between this equivalence class and human-readable equations
defined by the search space of symbolic regression. The approach is not limited
to classifiers or full neural networks and can be applied to arbitrary neurons
in hidden layers or latent spaces or to simplify the process of interpreting
neural network regressors.</div><div><a href='http://arxiv.org/abs/2401.04978v1'>2401.04978v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13904v1")'>Empowering Machines to Think Like Chemists: Unveiling Molecular
  Structure-Polarity Relationships with Hierarchical Symbolic Regression</div>
<div id='2401.13904v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T02:48:44Z</div><div>Authors: Siyu Lou, Chengchun Liu, Yuntian Chen, Fanyang Mo</div><div style='padding-top: 10px; width: 80ex'>Thin-layer chromatography (TLC) is a crucial technique in molecular polarity
analysis. Despite its importance, the interpretability of predictive models for
TLC, especially those driven by artificial intelligence, remains a challenge.
Current approaches, utilizing either high-dimensional molecular fingerprints or
domain-knowledge-driven feature engineering, often face a dilemma between
expressiveness and interpretability. To bridge this gap, we introduce
Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical
neural networks and symbolic regression. UHiSR automatically distills
chemical-intuitive polarity indices, and discovers interpretable equations that
link molecular structure to chromatographic behavior.</div><div><a href='http://arxiv.org/abs/2401.13904v1'>2401.13904v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01506v2")'>AIRI: Predicting Retention Indices and their Uncertainties using
  Artificial Intelligence</div>
<div id='2401.01506v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T02:22:39Z</div><div>Authors: Lewis Y. Geer, Stephen E. Stein, William Gary Mallard, Douglas J. Slotta</div><div style='padding-top: 10px; width: 80ex'>The Kov\'ats Retention index (RI) is a quantity measured using gas
chromatography and commonly used in the identification of chemical structures.
Creating libraries of observed RI values is a laborious task, so we explore the
use of a deep neural network for predicting RI values from structure for
standard semipolar columns. This network generated predictions with a mean
absolute error of 15.1 and, in a quantification of the tail of the error
distribution, a 95th percentile absolute error of 46.5. Because of the
Artificial Intelligence Retention Indices (AIRI) network's accuracy, it was
used to predict RI values for the NIST EI-MS spectral libraries. These RI
values are used to improve chemical identification methods and the quality of
the library. Estimating uncertainty is an important practical need when using
prediction models. To quantify the uncertainty of our network for each
individual prediction, we used the outputs of an ensemble of 8 networks to
calculate a predicted standard deviation for each RI value prediction. This
predicted standard deviation was corrected to follow the error between observed
and predicted RI values. The Z scores using these predicted standard deviations
had a standard deviation of 1.52 and a 95th percentile absolute Z score
corresponding to a mean RI value of 42.6.</div><div><a href='http://arxiv.org/abs/2401.01506v2'>2401.01506v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04978v1")'>Stacking as Accelerated Gradient Descent</div>
<div id='2403.04978v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T01:23:25Z</div><div>Authors: Naman Agarwal, Pranjal Awasthi, Satyen Kale, Eric Zhao</div><div style='padding-top: 10px; width: 80ex'>Stacking, a heuristic technique for training deep residual networks by
progressively increasing the number of layers and initializing new layers by
copying parameters from older layers, has proven quite successful in improving
the efficiency of training deep neural networks. In this paper, we propose a
theoretical explanation for the efficacy of stacking: viz., stacking implements
a form of Nesterov's accelerated gradient descent. The theory also covers
simpler models such as the additive ensembles constructed in boosting methods,
and provides an explanation for a similar widely-used practical heuristic for
initializing the new classifier in each round of boosting. We also prove that
for certain deep linear residual networks, stacking does provide accelerated
training, via a new potential function analysis of the Nesterov's accelerated
gradient method which allows errors in updates. We conduct proof-of-concept
experiments to validate our theory as well.</div><div><a href='http://arxiv.org/abs/2403.04978v1'>2403.04978v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12975v1")'>Training morphological neural networks with gradient descent: some
  theoretical insights</div>
<div id='2403.12975v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:11:15Z</div><div>Authors: Samy Blusseau</div><div style='padding-top: 10px; width: 80ex'>Morphological neural networks, or layers, can be a powerful tool to boost the
progress in mathematical morphology, either on theoretical aspects such as the
representation of complete lattice operators, or in the development of image
processing pipelines. However, these architectures turn out to be difficult to
train when they count more than a few morphological layers, at least within
popular machine learning frameworks which use gradient descent based
optimization algorithms. In this paper we investigate the potential and
limitations of differentiation based approaches and back-propagation applied to
morphological networks, in light of the non-smooth optimization concept of
Bouligand derivative. We provide insights and first theoretical guidelines, in
particular regarding initialization and learning rates.</div><div><a href='http://arxiv.org/abs/2403.12975v1'>2403.12975v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02296v1")'>Training Single-Layer Morphological Perceptron Using Convex-Concave
  Programming</div>
<div id='2401.02296v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T14:34:58Z</div><div>Authors: Iara Cunha, Marcos Eduardo Valle</div><div style='padding-top: 10px; width: 80ex'>This paper concerns the training of a single-layer morphological perceptron
using disciplined convex-concave programming (DCCP). We introduce an algorithm
referred to as K-DDCCP, which combines the existing single-layer morphological
perceptron (SLMP) model proposed by Ritter and Urcid with the weighted
disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and
Maragos. The proposed training algorithm leverages the disciplined
convex-concave procedure (DCCP) and formulates a non-convex optimization
problem for binary classification. To tackle this problem, the constraints are
expressed as differences of convex functions, enabling the application of the
DCCP package. The experimental results confirm the effectiveness of the K-DDCCP
algorithm in solving binary classification problems. Overall, this work
contributes to the field of morphological neural networks by proposing an
algorithm that extends the capabilities of the SLMP model.</div><div><a href='http://arxiv.org/abs/2401.02296v1'>2401.02296v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09142v1")'>When Representations Align: Universality in Representation Learning
  Dynamics</div>
<div id='2402.09142v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T12:48:17Z</div><div>Authors: Loek van Rossem, Andrew M. Saxe</div><div style='padding-top: 10px; width: 80ex'>Deep neural networks come in many sizes and architectures. The choice of
architecture, in conjunction with the dataset and learning algorithm, is
commonly understood to affect the learned neural representations. Yet, recent
results have shown that different architectures learn representations with
striking qualitative similarities. Here we derive an effective theory of
representation learning under the assumption that the encoding map from input
to hidden representation and the decoding map from representation to output are
arbitrary smooth functions. This theory schematizes representation learning
dynamics in the regime of complex, large architectures, where hidden
representations are not strongly constrained by the parametrization. We show
through experiments that the effective theory describes aspects of
representation learning dynamics across a range of deep networks with different
activation functions and architectures, and exhibits phenomena similar to the
"rich" and "lazy" regime. While many network behaviors depend quantitatively on
architecture, our findings point to certain behaviors that are widely conserved
once models are sufficiently flexible.</div><div><a href='http://arxiv.org/abs/2402.09142v1'>2402.09142v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16770v1")'>Neural Population Geometry and Optimal Coding of Tasks with Shared
  Latent Structure</div>
<div id='2402.16770v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T17:39:23Z</div><div>Authors: Albert J. Wakhloo, Will Slatton, SueYeon Chung</div><div style='padding-top: 10px; width: 80ex'>Humans and animals can recognize latent structures in their environment and
apply this information to efficiently navigate the world. Several recent works
argue that the brain supports these abilities by forming neural representations
that encode such latent structures in flexible, generalizable ways. However, it
remains unclear what aspects of neural population activity are contributing to
these computational capabilities. Here, we develop an analytical theory linking
the mesoscopic statistics of a neural population's activity to generalization
performance on a multi-task learning problem. To do this, we rely on a
generative model in which different tasks depend on a common, unobserved latent
structure and predictions are formed from a linear readout of a neural
population's activity. We show that three geometric measures of the population
activity determine generalization performance in these settings. Using this
theory, we find that experimentally observed factorized (or disentangled)
representations naturally emerge as an optimal solution to the multi-task
learning problem. We go on to show that when data is scarce, optimal codes
compress less informative latent variables, and when data is abundant, optimal
codes expand this information in the state space. We validate predictions from
our theory using biological and artificial neural network data. Our results
therefore tie neural population geometry to the multi-task learning problem and
make normative predictions of the structure of population activity in these
settings.</div><div><a href='http://arxiv.org/abs/2402.16770v1'>2402.16770v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17975v1")'>Understanding polysemanticity in neural networks through coding theory</div>
<div id='2401.17975v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T16:31:54Z</div><div>Authors: Simon C. Marshall, Jan H. Kirchner</div><div style='padding-top: 10px; width: 80ex'>Despite substantial efforts, neural network interpretability remains an
elusive goal, with previous research failing to provide succinct explanations
of most single neurons' impact on the network output. This limitation is due to
the polysemantic nature of most neurons, whereby a given neuron is involved in
multiple unrelated network states, complicating the interpretation of that
neuron. In this paper, we apply tools developed in neuroscience and information
theory to propose both a novel practical approach to network interpretability
and theoretical insights into polysemanticity and the density of codes. We
infer levels of redundancy in the network's code by inspecting the
eigenspectrum of the activation's covariance matrix. Furthermore, we show how
random projections can reveal whether a network exhibits a smooth or
non-differentiable code and hence how interpretable the code is. This same
framework explains the advantages of polysemantic neurons to learning
performance and explains trends found in recent results by Elhage et
al.~(2022). Our approach advances the pursuit of interpretability in neural
networks, providing insights into their underlying structure and suggesting new
avenues for circuit-level interpretability.</div><div><a href='http://arxiv.org/abs/2401.17975v1'>2401.17975v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09735v1")'>DFORM: Diffeomorphic vector field alignment for assessing dynamics
  across learned models</div>
<div id='2402.09735v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T06:22:50Z</div><div>Authors: Ruiqi Chen, Giacomo Vedovati, Todd Braver, ShiNung Ching</div><div style='padding-top: 10px; width: 80ex'>Dynamical system models such as Recurrent Neural Networks (RNNs) have become
increasingly popular as hypothesis-generating tools in scientific research.
Evaluating the dynamics in such networks is key to understanding their learned
generative mechanisms. However, comparison of learned dynamics across models is
challenging due to their inherent nonlinearity and because a priori there is no
enforced equivalence of their coordinate systems. Here, we propose the DFORM
(Diffeomorphic vector field alignment for comparing dynamics across learned
models) framework. DFORM learns a nonlinear coordinate transformation which
provides a continuous, maximally one-to-one mapping between the trajectories of
learned models, thus approximating a diffeomorphism between them. The mismatch
between DFORM-transformed vector fields defines the orbital similarity between
two models, thus providing a generalization of the concepts of smooth orbital
and topological equivalence. As an example, we apply DFORM to models trained on
a canonical neuroscience task, showing that learned dynamics may be
functionally similar, despite overt differences in attractor landscapes.</div><div><a href='http://arxiv.org/abs/2402.09735v1'>2402.09735v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14102v2")'>Learning dynamic representations of the functional connectome in
  neurobiological networks</div>
<div id='2402.14102v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T19:54:25Z</div><div>Authors: Luciano Dyballa, Samuel Lang, Alexandra Haslund-Gourley, Eviatar Yemini, Steven W. Zucker</div><div style='padding-top: 10px; width: 80ex'>The static synaptic connectivity of neuronal circuits stands in direct
contrast to the dynamics of their function. As in changing community
interactions, different neurons can participate actively in various
combinations to effect behaviors at different times. We introduce an
unsupervised approach to learn the dynamic affinities between neurons in live,
behaving animals, and to reveal which communities form among neurons at
different times. The inference occurs in two major steps. First, pairwise
non-linear affinities between neuronal traces from brain-wide calcium activity
are organized by non-negative tensor factorization (NTF). Each factor specifies
which groups of neurons are most likely interacting for an inferred interval in
time, and for which animals. Finally, a generative model that allows for
weighted community detection is applied to the functional motifs produced by
NTF to reveal a dynamic functional connectome. Since time codes the different
experimental variables (e.g., application of chemical stimuli), this provides
an atlas of neural motifs active during separate stages of an experiment (e.g.,
stimulus application or spontaneous behaviors). Results from our analysis are
experimentally validated, confirming that our method is able to robustly
predict causal interactions between neurons to generate behavior. Code is
available at https://github.com/dyballa/dynamic-connectomes.</div><div><a href='http://arxiv.org/abs/2402.14102v2'>2402.14102v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17500v1")'>Predicting Instability in Complex Oscillator Networks: Limitations and
  Potentials of Network Measures and Machine Learning</div>
<div id='2402.17500v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T13:34:08Z</div><div>Authors: Christian Nauck, Michael Lindner, Nora Molkenthin, Jürgen Kurths, Eckehard Schöll, Jörg Raisch, Frank Hellmann</div><div style='padding-top: 10px; width: 80ex'>A central question of network science is how functional properties of systems
arise from their structure. For networked dynamical systems, structure is
typically quantified with network measures. A functional property that is of
theoretical and practical interest for oscillatory systems is the stability of
synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs)
have been shown to predict this stability successfully; at the same time,
network measures have struggled to paint a clear picture. Here we collect 46
relevant network measures and find that no small subset can reliably predict
stability. The performance of GNNs can only be matched by combining all network
measures and nodewise machine learning. However, unlike GNNs, this approach
fails to extrapolate from network ensembles to several real power grid
topologies. This suggests that correlations of network measures and function
may be misleading, and that GNNs capture the causal relationship between
structure and stability substantially better.</div><div><a href='http://arxiv.org/abs/2402.17500v1'>2402.17500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16887v1")'>Artificial Intelligence for Complex Network: Potential, Methodology and
  Application</div>
<div id='2402.16887v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T09:06:36Z</div><div>Authors: Jingtao Ding, Chang Liu, Yu Zheng, Yunke Zhang, Zihan Yu, Ruikun Li, Hongyi Chen, Jinghua Piao, Huandong Wang, Jiazhen Liu, Yong Li</div><div style='padding-top: 10px; width: 80ex'>Complex networks pervade various real-world systems, from the natural
environment to human societies. The essence of these networks is in their
ability to transition and evolve from microscopic disorder-where network
topology and node dynamics intertwine-to a macroscopic order characterized by
certain collective behaviors. Over the past two decades, complex network
science has significantly enhanced our understanding of the statistical
mechanics, structures, and dynamics underlying real-world networks. Despite
these advancements, there remain considerable challenges in exploring more
realistic systems and enhancing practical applications. The emergence of
artificial intelligence (AI) technologies, coupled with the abundance of
diverse real-world network data, has heralded a new era in complex network
science research. This survey aims to systematically address the potential
advantages of AI in overcoming the lingering challenges of complex network
research. It endeavors to summarize the pivotal research problems and provide
an exhaustive review of the corresponding methodologies and applications.
Through this comprehensive survey-the first of its kind on AI for complex
networks-we expect to provide valuable insights that will drive further
research and advancement in this interdisciplinary field.</div><div><a href='http://arxiv.org/abs/2402.16887v1'>2402.16887v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00974v1")'>Motif distribution and function of sparse deep neural networks</div>
<div id='2403.00974v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T20:51:10Z</div><div>Authors: Olivia T. Zahn, Thomas L. Daniel, J. Nathan Kutz</div><div style='padding-top: 10px; width: 80ex'>We characterize the connectivity structure of feed-forward, deep neural
networks (DNNs) using network motif theory. To address whether a particular
motif distribution is characteristic of the training task, or function of the
DNN, we compare the connectivity structure of 350 DNNs trained to simulate a
bio-mechanical flight control system with different randomly initialized
parameters. We develop and implement algorithms for counting second- and
third-order motifs and calculate their significance using their Z-score. The
DNNs are trained to solve the inverse problem of the flight dynamics model in
Bustamante, et al. (2022) (i.e., predict the controls necessary for controlled
flight from the initial and final state-space inputs) and are sparsified
through an iterative pruning and retraining algorithm Zahn, et al. (2022). We
show that, despite random initialization of network parameters, enforced
sparsity causes DNNs to converge to similar connectivity patterns as
characterized by their motif distributions. The results suggest how neural
network function can be encoded in motif distributions, suggesting a variety of
experiments for informing function and control.</div><div><a href='http://arxiv.org/abs/2403.00974v1'>2403.00974v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03103v1")'>Emergent Equivariance in Deep Ensembles</div>
<div id='2403.03103v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T16:43:25Z</div><div>Authors: Jan E. Gerken, Pan Kessel</div><div style='padding-top: 10px; width: 80ex'>We demonstrate that deep ensembles are secretly equivariant models. More
precisely, we show that deep ensembles become equivariant for all inputs and at
all training times by simply using data augmentation. Crucially, equivariance
holds off-manifold and for any architecture in the infinite width limit. The
equivariance is emergent in the sense that predictions of individual ensemble
members are not equivariant but their collective prediction is. Neural tangent
kernel theory is used to derive this result and we verify our theoretical
insights using detailed numerical experiments.</div><div><a href='http://arxiv.org/abs/2403.03103v1'>2403.03103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15958v2")'>On the dynamics of three-layer neural networks: initial condensation</div>
<div id='2402.15958v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T02:36:14Z</div><div>Authors: Zheng-An Chen, Tao Luo</div><div style='padding-top: 10px; width: 80ex'>Empirical and theoretical works show that the input weights of two-layer
neural networks, when initialized with small values, converge towards isolated
orientations. This phenomenon, referred to as condensation, indicates that the
gradient descent methods tend to spontaneously reduce the complexity of neural
networks during the training process. In this work, we elucidate the mechanisms
behind the condensation phenomena occurring in the training of three-layer
neural networks and distinguish it from the training of two-layer neural
networks. Through rigorous theoretical analysis, we establish the blow-up
property of effective dynamics and present a sufficient condition for the
occurrence of condensation, findings that are substantiated by experimental
results. Additionally, we explore the association between condensation and the
low-rank bias observed in deep matrix factorization.</div><div><a href='http://arxiv.org/abs/2402.15958v2'>2402.15958v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18724v1")'>Learning Associative Memories with Gradient Descent</div>
<div id='2402.18724v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T21:47:30Z</div><div>Authors: Vivien Cabannes, Berfin Simsek, Alberto Bietti</div><div style='padding-top: 10px; width: 80ex'>This work focuses on the training dynamics of one associative memory module
storing outer products of token embeddings. We reduce this problem to the study
of a system of particles, which interact according to properties of the data
distribution and correlations between embeddings. Through theory and
experiments, we provide several insights. In overparameterized regimes, we
obtain logarithmic growth of the ``classification margins.'' Yet, we show that
imbalance in token frequencies and memory interferences due to correlated
embeddings lead to oscillatory transitory regimes. The oscillations are more
pronounced with large step sizes, which can create benign loss spikes, although
these learning rates speed up the dynamics and accelerate the asymptotic
convergence. In underparameterized regimes, we illustrate how the cross-entropy
loss can lead to suboptimal memorization schemes. Finally, we assess the
validity of our findings on small Transformer models.</div><div><a href='http://arxiv.org/abs/2402.18724v1'>2402.18724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13999v1")'>Asymptotics of Learning with Deep Structured (Random) Features</div>
<div id='2402.13999v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:35:27Z</div><div>Authors: Dominik Schröder, Daniil Dmitriev, Hugo Cui, Bruno Loureiro</div><div style='padding-top: 10px; width: 80ex'>For a large class of feature maps we provide a tight asymptotic
characterisation of the test error associated with learning the readout layer,
in the high-dimensional limit where the input dimension, hidden layer widths,
and number of training samples are proportionally large. This characterization
is formulated in terms of the population covariance of the features. Our work
is partially motivated by the problem of learning with Gaussian rainbow neural
networks, namely deep non-linear fully-connected networks with random but
structured weights, whose row-wise covariances are further allowed to depend on
the weights of previous layers. For such networks we also derive a closed-form
formula for the feature covariance in terms of the weight matrices. We further
find that in some cases our results can capture feature maps learned by deep,
finite-width neural networks trained under gradient descent.</div><div><a href='http://arxiv.org/abs/2402.13999v1'>2402.13999v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08726v1")'>Trained quantum neural networks are Gaussian processes</div>
<div id='2402.08726v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T19:00:08Z</div><div>Authors: Filippo Girardi, Giacomo De Palma</div><div style='padding-top: 10px; width: 80ex'>We study quantum neural networks made by parametric one-qubit gates and fixed
two-qubit gates in the limit of infinite width, where the generated function is
the expectation value of the sum of single-qubit observables over all the
qubits. First, we prove that the probability distribution of the function
generated by the untrained network with randomly initialized parameters
converges in distribution to a Gaussian process whenever each measured qubit is
correlated only with few other measured qubits. Then, we analytically
characterize the training of the network via gradient descent with square loss
on supervised learning problems. We prove that, as long as the network is not
affected by barren plateaus, the trained network can perfectly fit the training
set and that the probability distribution of the function generated after
training still converges in distribution to a Gaussian process. Finally, we
consider the statistical noise of the measurement at the output of the network
and prove that a polynomial number of measurements is sufficient for all the
previous results to hold and that the network can always be trained in
polynomial time.</div><div><a href='http://arxiv.org/abs/2402.08726v1'>2402.08726v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14694v1")'>A Quick Introduction to Quantum Machine Learning for Non-Practitioners</div>
<div id='2402.14694v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T16:48:17Z</div><div>Authors: Ethan N. Evans, Dominic Byrne, Matthew G. Cook</div><div style='padding-top: 10px; width: 80ex'>This paper provides an introduction to quantum machine learning, exploring
the potential benefits of using quantum computing principles and algorithms
that may improve upon classical machine learning approaches. Quantum computing
utilizes particles governed by quantum mechanics for computational purposes,
leveraging properties like superposition and entanglement for information
representation and manipulation. Quantum machine learning applies these
principles to enhance classical machine learning models, potentially reducing
network size and training time on quantum hardware. The paper covers basic
quantum mechanics principles, including superposition, phase space, and
entanglement, and introduces the concept of quantum gates that exploit these
properties. It also reviews classical deep learning concepts, such as
artificial neural networks, gradient descent, and backpropagation, before
delving into trainable quantum circuits as neural networks. An example problem
demonstrates the potential advantages of quantum neural networks, and the
appendices provide detailed derivations. The paper aims to help researchers new
to quantum mechanics and machine learning develop their expertise more
efficiently.</div><div><a href='http://arxiv.org/abs/2402.14694v1'>2402.14694v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07059v2")'>Better than classical? The subtle art of benchmarking quantum machine
  learning models</div>
<div id='2403.07059v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T18:00:06Z</div><div>Authors: Joseph Bowles, Shahnawaz Ahmed, Maria Schuld</div><div style='padding-top: 10px; width: 80ex'>Benchmarking models via classical simulations is one of the main ways to
judge ideas in quantum machine learning before noise-free hardware is
available. However, the huge impact of the experimental design on the results,
the small scales within reach today, as well as narratives influenced by the
commercialisation of quantum technologies make it difficult to gain robust
insights. To facilitate better decision-making we develop an open-source
package based on the PennyLane software framework and use it to conduct a
large-scale study that systematically tests 12 popular quantum machine learning
models on 6 binary classification tasks used to create 160 individual datasets.
We find that overall, out-of-the-box classical machine learning models
outperform the quantum classifiers. Moreover, removing entanglement from a
quantum model often results in as good or better performance, suggesting that
"quantumness" may not be the crucial ingredient for the small learning tasks
considered here. Our benchmarks also unlock investigations beyond simplistic
leaderboard comparisons, and we identify five important questions for quantum
model design that follow from our results.</div><div><a href='http://arxiv.org/abs/2403.07059v2'>2403.07059v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02405v1")'>Classification of the Fashion-MNIST Dataset on a Quantum Computer</div>
<div id='2403.02405v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:01:14Z</div><div>Authors: Kevin Shen, Bernhard Jobst, Elvira Shishenina, Frank Pollmann</div><div style='padding-top: 10px; width: 80ex'>The potential impact of quantum machine learning algorithms on industrial
applications remains an exciting open question. Conventional methods for
encoding classical data into quantum computers are not only too costly for a
potential quantum advantage in the algorithms but also severely limit the scale
of feasible experiments on current hardware. Therefore, recent works, despite
claiming the near-term suitability of their algorithms, do not provide
experimental benchmarking on standard machine learning datasets. We attempt to
solve the data encoding problem by improving a recently proposed variational
algorithm [1] that approximately prepares the encoded data, using
asymptotically shallow circuits that fit the native gate set and topology of
currently available quantum computers. We apply the improved algorithm to
encode the Fashion-MNIST dataset [2], which can be directly used in future
empirical studies of quantum machine learning algorithms. We deploy simple
quantum variational classifiers trained on the encoded dataset on a current
quantum computer ibmq-kolkata [3] and achieve moderate accuracies, providing a
proof of concept for the near-term usability of our data encoding method.</div><div><a href='http://arxiv.org/abs/2403.02405v1'>2403.02405v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13352v3")'>KetGPT - Dataset Augmentation of Quantum Circuits using Transformers</div>
<div id='2402.13352v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T20:02:21Z</div><div>Authors: Boran Apak, Medina Bandic, Aritra Sarkar, Sebastian Feld</div><div style='padding-top: 10px; width: 80ex'>Quantum algorithms, represented as quantum circuits, can be used as
benchmarks for assessing the performance of quantum systems. Existing datasets,
widely utilized in the field, suffer from limitations in size and versatility,
leading researchers to employ randomly generated circuits. Random circuits are,
however, not representative benchmarks as they lack the inherent properties of
real quantum algorithms for which the quantum systems are manufactured. This
shortage of `useful' quantum benchmarks poses a challenge to advancing the
development and comparison of quantum compilers and hardware.
  This research aims to enhance the existing quantum circuit datasets by
generating what we refer to as `realistic-looking' circuits by employing the
Transformer machine learning architecture. For this purpose, we introduce
KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose
structure is based on quantum circuits derived from existing quantum algorithms
and follows the typical patterns of human-written algorithm-based code (e.g.,
order of gates and qubits). Our three-fold verification process, involving
manual inspection and Qiskit framework execution, transformer-based
classification, and structural analysis, demonstrates the efficacy of KetGPT in
producing large amounts of additional circuits that closely align with
algorithm-based structures. Beyond benchmarking, we envision KetGPT
contributing substantially to AI-driven quantum compilers and systems.</div><div><a href='http://arxiv.org/abs/2402.13352v3'>2402.13352v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.11098v1")'>Neural auto-designer for enhanced quantum kernels</div>
<div id='2401.11098v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T03:11:59Z</div><div>Authors: Cong Lei, Yuxuan Du, Peng Mi, Jun Yu, Tongliang Liu</div><div style='padding-top: 10px; width: 80ex'>Quantum kernels hold great promise for offering computational advantages over
classical learners, with the effectiveness of these kernels closely tied to the
design of the quantum feature map. However, the challenge of designing
effective quantum feature maps for real-world datasets, particularly in the
absence of sufficient prior information, remains a significant obstacle. In
this study, we present a data-driven approach that automates the design of
problem-specific quantum feature maps. Our approach leverages feature-selection
techniques to handle high-dimensional data on near-term quantum machines with
limited qubits, and incorporates a deep neural predictor to efficiently
evaluate the performance of various candidate quantum kernels. Through
extensive numerical simulations on different datasets, we demonstrate the
superiority of our proposal over prior methods, especially for the capability
of eliminating the kernel concentration issue and identifying the feature map
with prediction advantages. Our work not only unlocks the potential of quantum
kernels for enhancing real-world tasks but also highlights the substantial role
of deep learning in advancing quantum machine learning.</div><div><a href='http://arxiv.org/abs/2401.11098v1'>2401.11098v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08606v1")'>Arbitrary Polynomial Separations in Trainable Quantum Machine Learning</div>
<div id='2402.08606v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T17:12:01Z</div><div>Authors: Eric R. Anschuetz, Xun Gao</div><div style='padding-top: 10px; width: 80ex'>Recent theoretical results in quantum machine learning have demonstrated a
general trade-off between the expressive power of quantum neural networks
(QNNs) and their trainability; as a corollary of these results, practical
exponential separations in expressive power over classical machine learning
models are believed to be infeasible as such QNNs take a time to train that is
exponential in the model size. We here circumvent these negative results by
constructing a hierarchy of efficiently trainable QNNs that exhibit
unconditionally provable, polynomial memory separations of arbitrary constant
degree over classical neural networks in performing a classical sequence
modeling task. Furthermore, each unit cell of the introduced class of QNNs is
computationally efficient, implementable in constant time on a quantum device.
The classical networks we prove a separation over include well-known examples
such as recurrent neural networks and Transformers. We show that quantum
contextuality is the source of the expressivity separation, suggesting that
other classical sequence learning problems with long-time correlations may be a
regime where practical advantages in quantum machine learning may exist.</div><div><a href='http://arxiv.org/abs/2402.08606v1'>2402.08606v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02880v1")'>Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks</div>
<div id='2402.02880v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:47:46Z</div><div>Authors: Han-Xiao Tao, Jiaqi Hu, Re-Bing Wu</div><div style='padding-top: 10px; width: 80ex'>Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum
(NISQ) devices requires the optimal utilization of limited quantum resources.
The commonly used gate-based QML models are convenient for software engineers,
but their expressivity is restricted by the permissible circuit depth within a
finite coherence time. In contrast, pulse-based models enable the construction
of "infinitely" deep quantum neural networks within the same coherence time,
which may unleash greater expressive power for complex learning tasks. In this
paper, we investigate this potential from the perspective of quantum control
theory. We first indicate that the nonlinearity of pulse-based models comes
from the encoding process that can be viewed as the continuous limit of
data-reuploading in gate-based models. Subsequently, we prove that the
pulse-based model can approximate arbitrary nonlinear functions when the
underlying physical system is ensemble controllable. Under this condition,
numerical simulations show that the expressivity can be enhanced by either
increasing the pulse length or the number of qubits. As anticipated, we
demonstrate through numerical examples that the pulse-based model can unleash
more expressive power compared to the gate-based model. These findings
establish a theoretical foundation for understanding and designing expressive
QML models using NISQ devices.</div><div><a href='http://arxiv.org/abs/2402.02880v1'>2402.02880v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00653v1")'>Coherent Feed Forward Quantum Neural Network</div>
<div id='2402.00653v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T15:13:26Z</div><div>Authors: Utkarsh Singh, Aaron Z. Goldberg, Khabat Heshami</div><div style='padding-top: 10px; width: 80ex'>Quantum machine learning, focusing on quantum neural networks (QNNs), remains
a vastly uncharted field of study. Current QNN models primarily employ
variational circuits on an ansatz or a quantum feature map, often requiring
multiple entanglement layers. This methodology not only increases the
computational cost of the circuit beyond what is practical on near-term quantum
devices but also misleadingly labels these models as neural networks, given
their divergence from the structure of a typical feed-forward neural network
(FFNN). Moreover, the circuit depth and qubit needs of these models scale
poorly with the number of data features, resulting in an efficiency challenge
for real-world machine-learning tasks. We introduce a bona fide QNN model,
which seamlessly aligns with the versatility of a traditional FFNN in terms of
its adaptable intermediate layers and nodes, absent from intermediate
measurements such that our entire model is coherent. This model stands out with
its reduced circuit depth and number of requisite C-NOT gates to outperform
prevailing QNN models. Furthermore, the qubit count in our model remains
unaffected by the data's feature quantity. We test our proposed model on
various benchmarking datasets such as the diagnostic breast cancer (Wisconsin)
and credit card fraud detection datasets. We compare the outcomes of our model
with the existing QNN methods to showcase the advantageous efficacy of our
approach, even with a reduced requirement on quantum resources. Our model paves
the way for application of quantum neural networks to real relevant machine
learning problems.</div><div><a href='http://arxiv.org/abs/2402.00653v1'>2402.00653v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09146v1")'>ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural
  Networks</div>
<div id='2402.09146v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T12:55:28Z</div><div>Authors: Muhammad Kashif, Muhammad Shafique</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.</div><div><a href='http://arxiv.org/abs/2402.09146v1'>2402.09146v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09393v1")'>Élivágar: Efficient Quantum Circuit Search for Classification</div>
<div id='2401.09393v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T18:09:26Z</div><div>Authors: Sashwat Anagolum, Narges Alavisamani, Poulami Das, Moinuddin Qureshi, Eric Kessler, Yunong Shi</div><div style='padding-top: 10px; width: 80ex'>Designing performant and noise-robust circuits for Quantum Machine Learning
(QML) is challenging -- the design space scales exponentially with circuit
size, and there are few well-supported guiding principles for QML circuit
design. Although recent Quantum Circuit Search (QCS) methods attempt to search
for performant QML circuits that are also robust to hardware noise, they
directly adopt designs from classical Neural Architecture Search (NAS) that are
misaligned with the unique constraints of quantum hardware, resulting in high
search overheads and severe performance bottlenecks.
  We present \'Eliv\'agar, a novel resource-efficient, noise-guided QCS
framework. \'Eliv\'agar innovates in all three major aspects of QCS -- search
space, search algorithm and candidate evaluation strategy -- to address the
design flaws in current classically-inspired QCS methods. \'Eliv\'agar achieves
hardware-efficiency and avoids an expensive circuit-mapping co-search via
noise- and device topology-aware candidate generation. By introducing two
cheap-to-compute predictors, Clifford noise resilience and Representational
capacity, \'Eliv\'agar decouples the evaluation of noise robustness and
performance, enabling early rejection of low-fidelity circuits and reducing
circuit evaluation costs. Due to its resource-efficiency, \'Eliv\'agar can
further search for data embeddings, significantly improving performance.
  Based on a comprehensive evaluation of \'Eliv\'agar on 12 real quantum
devices and 9 QML applications, \'Eliv\'agar achieves 5.3% higher accuracy and
a 271$\times$ speedup compared to state-of-the-art QCS methods.</div><div><a href='http://arxiv.org/abs/2401.09393v1'>2401.09393v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05571v1")'>QuantumSEA: In-Time Sparse Exploration for Noise Adaptive Quantum
  Circuits</div>
<div id='2401.05571v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T22:33:00Z</div><div>Authors: Tianlong Chen, Zhenyu Zhang, Hanrui Wang, Jiaqi Gu, Zirui Li, David Z. Pan, Frederic T. Chong, Song Han, Zhangyang Wang</div><div style='padding-top: 10px; width: 80ex'>Parameterized Quantum Circuits (PQC) have obtained increasing popularity
thanks to their great potential for near-term Noisy Intermediate-Scale Quantum
(NISQ) computers. Achieving quantum advantages usually requires a large number
of qubits and quantum circuits with enough capacity. However, limited coherence
time and massive quantum noises severely constrain the size of quantum circuits
that can be executed reliably on real machines. To address these two pain
points, we propose QuantumSEA, an in-time sparse exploration for noise-adaptive
quantum circuits, aiming to achieve two key objectives: (1) implicit circuits
capacity during training - by dynamically exploring the circuit's sparse
connectivity and sticking a fixed small number of quantum gates throughout the
training which satisfies the coherence time and enjoy light noises, enabling
feasible executions on real quantum devices; (2) noise robustness - by jointly
optimizing the topology and parameters of quantum circuits under real device
noise models. In each update step of sparsity, we leverage the moving average
of historical gradients to grow necessary gates and utilize salience-based
pruning to eliminate insignificant gates. Extensive experiments are conducted
with 7 Quantum Machine Learning (QML) and Variational Quantum Eigensolver (VQE)
benchmarks on 6 simulated or real quantum computers, where QuantumSEA
consistently surpasses noise-aware search, human-designed, and randomly
generated quantum circuit baselines by a clear performance margin. For example,
even in the most challenging on-chip training regime, our method establishes
state-of-the-art results with only half the number of quantum gates and ~2x
time saving of circuit executions. Codes are available at
https://github.com/VITA-Group/QuantumSEA.</div><div><a href='http://arxiv.org/abs/2401.05571v1'>2401.05571v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03500v1")'>Curriculum reinforcement learning for quantum architecture search under
  hardware errors</div>
<div id='2402.03500v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T20:33:00Z</div><div>Authors: Yash J. Patel, Akash Kundu, Mateusz Ostaszewski, Xavier Bonet-Monroig, Vedran Dunjko, Onur Danaci</div><div style='padding-top: 10px; width: 80ex'>The key challenge in the noisy intermediate-scale quantum era is finding
useful circuits compatible with current device limitations. Variational quantum
algorithms (VQAs) offer a potential solution by fixing the circuit architecture
and optimizing individual gate parameters in an external loop. However,
parameter optimization can become intractable, and the overall performance of
the algorithm depends heavily on the initially chosen circuit architecture.
Several quantum architecture search (QAS) algorithms have been developed to
design useful circuit architectures automatically. In the case of parameter
optimization alone, noise effects have been observed to dramatically influence
the performance of the optimizer and final outcomes, which is a key line of
study. However, the effects of noise on the architecture search, which could be
just as critical, are poorly understood. This work addresses this gap by
introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm
designed to tackle challenges in realistic VQA deployment. The algorithm
incorporates (i) a 3D architecture encoding and restrictions on environment
dynamics to explore the search space of possible circuits efficiently, (ii) an
episode halting scheme to steer the agent to find shorter circuits, and (iii) a
novel variant of simultaneous perturbation stochastic approximation as an
optimizer for faster convergence. To facilitate studies, we developed an
optimized simulator for our algorithm, significantly improving computational
efficiency in simulating noisy quantum circuits by employing the Pauli-transfer
matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing
on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS
algorithms across several metrics in both noiseless and noisy environments.</div><div><a href='http://arxiv.org/abs/2402.03500v1'>2402.03500v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13754v3")'>Reinforcement learning-assisted quantum architecture search for
  variational quantum algorithms</div>
<div id='2402.13754v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T12:30:39Z</div><div>Authors: Akash Kundu</div><div style='padding-top: 10px; width: 80ex'>A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is
identifying functional quantum circuits. These circuits must also adhere to the
constraints imposed by current quantum hardware limitations. Variational
quantum algorithms (VQAs), a class of quantum-classical optimization
algorithms, were developed to address these challenges in the currently
available quantum devices. However, the overall performance of VQAs depends on
the initialization strategy of the variational circuit, the structure of the
circuit (also known as ansatz), and the configuration of the cost function.
Focusing on the structure of the circuit, in this thesis, we improve the
performance of VQAs by automating the search for an optimal structure for the
variational circuits using reinforcement learning (RL). Within the thesis, the
optimality of a circuit is determined by evaluating its depth, the overall
count of gates and parameters, and its accuracy in solving the given problem.
The task of automating the search for optimal quantum circuits is known as
quantum architecture search (QAS). The majority of research in QAS is primarily
focused on a noiseless scenario. Yet, the impact of noise on the QAS remains
inadequately explored. In this thesis, we tackle the issue by introducing a
tensor-based quantum circuit encoding, restrictions on environment dynamics to
explore the search space of possible circuits efficiently, an episode halting
scheme to steer the agent to find shorter circuits, a double deep Q-network
(DDQN) with an $\epsilon$-greedy policy for better stability. The numerical
experiments on noiseless and noisy quantum hardware show that in dealing with
various VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods
we propose in the thesis can be readily adapted to address a wide range of
other VQAs.</div><div><a href='http://arxiv.org/abs/2402.13754v3'>2402.13754v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14396v2")'>Quantum Circuit Optimization with AlphaTensor</div>
<div id='2402.14396v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T09:20:54Z</div><div>Authors: Francisco J. R. Ruiz, Tuomas Laakkonen, Johannes Bausch, Matej Balog, Mohammadamin Barekatain, Francisco J. H. Heras, Alexander Novikov, Nathan Fitzpatrick, Bernardino Romera-Paredes, John van de Wetering, Alhussein Fawzi, Konstantinos Meichanetzidis, Pushmeet Kohli</div><div style='padding-top: 10px; width: 80ex'>A key challenge in realizing fault-tolerant quantum computers is circuit
optimization. Focusing on the most expensive gates in fault-tolerant quantum
computation (namely, the T gates), we address the problem of T-count
optimization, i.e., minimizing the number of T gates that are needed to
implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a
method based on deep reinforcement learning that exploits the relationship
between optimizing T-count and tensor decomposition. Unlike existing methods
for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific
knowledge about quantum computation and leverage gadgets, which significantly
reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms
the existing methods for T-count optimization on a set of arithmetic benchmarks
(even when compared without making use of gadgets). Remarkably, it discovers an
efficient algorithm akin to Karatsuba's method for multiplication in finite
fields. AlphaTensor-Quantum also finds the best human-designed solutions for
relevant arithmetic computations used in Shor's algorithm and for quantum
chemistry simulation, thus demonstrating it can save hundreds of hours of
research by optimizing relevant quantum circuits in a fully automated way.</div><div><a href='http://arxiv.org/abs/2402.14396v2'>2402.14396v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12979v2")'>AltGraph: Redesigning Quantum Circuits Using Generative Graph Models for
  Efficient Optimization</div>
<div id='2403.12979v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T19:01:47Z</div><div>Authors: Collin Beaudoin, Koustubh Phalak, Swaroop Ghosh</div><div style='padding-top: 10px; width: 80ex'>Quantum circuit transformation aims to produce equivalent circuits while
optimizing for various aspects such as circuit depth, gate count, and
compatibility with modern Noisy Intermediate Scale Quantum (NISQ) devices.
There are two techniques for circuit transformation. The first is a rule-based
approach that greedily cancels out pairs of gates that equate to the identity
unitary operation. Rule-based approaches are used in quantum compilers such as
Qiskit, tket, and Quilc. The second is a search-based approach that tries to
find an equivalent quantum circuit by exploring the quantum circuits search
space. Search-based approaches typically rely on machine learning techniques
such as generative models and Reinforcement Learning (RL). In this work, we
propose AltGraph, a novel search-based circuit transformation approach that
generates equivalent quantum circuits using existing generative graph models.
We use three main graph models: DAG Variational Autoencoder (D-VAE) with two
variants: Gated Recurrent Unit (GRU) and Graph Convolutional Network (GCN), and
Deep Generative Model for Graphs (DeepGMG) that take a Direct Acyclic Graph
(DAG) of the quantum circuit as input and output a new DAG from which we
reconstruct the equivalent quantum circuit. Next, we perturb the latent space
to generate equivalent quantum circuits some of which may be more compatible
with the hardware coupling map and/or enable better optimization leading to
reduced gate count and circuit depth. AltGraph achieves on average a 37.55%
reduction in the number of gates and a 37.75% reduction in the circuit depth
post-transpiling compared to the original transpiled circuit with only 0.0074
Mean Squared Error (MSE) in the density matrix.</div><div><a href='http://arxiv.org/abs/2403.12979v2'>2403.12979v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10753v1")'>BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation</div>
<div id='2401.10753v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T15:22:28Z</div><div>Authors: Yingjie Li, Anthony Agnesina, Yanqing Zhang, Haoxing Ren, Cunxi Yu</div><div style='padding-top: 10px; width: 80ex'>Boolean algebraic manipulation is at the core of logic synthesis in
Electronic Design Automation (EDA) design flow. Existing methods struggle to
fully exploit optimization opportunities, and often suffer from an explosive
search space and limited scalability efficiency. This work presents BoolGebra,
a novel attributed graph-learning approach for Boolean algebraic manipulation
that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph
Neural Networks (GNNs) and takes initial feature embeddings from both
structural and functional information as inputs. A fully connected neural
network is employed as the predictor for direct optimization result
predictions, significantly reducing the search space and efficiently locating
the optimization space. The experiments involve training the BoolGebra model
w.r.t design-specific and cross-design inferences using the trained model,
where BoolGebra demonstrates generalizability for cross-design inference and
its potential to scale from small, simple training datasets to large, complex
inference datasets. Finally, BoolGebra is integrated with existing synthesis
tool ABC to perform end-to-end logic minimization evaluation w.r.t SOTA
baselines.</div><div><a href='http://arxiv.org/abs/2401.10753v1'>2401.10753v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01317v2")'>Less is More: Hop-Wise Graph Attention for Scalable and Generalizable
  Learning on Circuits</div>
<div id='2403.01317v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T21:33:23Z</div><div>Authors: Chenhui Deng, Zichao Yue, Cunxi Yu, Gokce Sarar, Ryan Carey, Rajeev Jain, Zhiru Zhang</div><div style='padding-top: 10px; width: 80ex'>While graph neural networks (GNNs) have gained popularity for learning
circuit representations in various electronic design automation (EDA) tasks,
they face challenges in scalability when applied to large graphs and exhibit
limited generalizability to new designs. These limitations make them less
practical for addressing large-scale, complex circuit problems. In this work we
propose HOGA, a novel attention-based model for learning circuit
representations in a scalable and generalizable manner. HOGA first computes
hop-wise features per node prior to model training. Subsequently, the hop-wise
features are solely used to produce node representations through a gated
self-attention module, which adaptively learns important features among
different hops without involving the graph topology. As a result, HOGA is
adaptive to various structures across different circuits and can be efficiently
trained in a distributed manner. To demonstrate the efficacy of HOGA, we
consider two representative EDA tasks: quality of results (QoR) prediction and
functional reasoning. Our experimental results indicate that (1) HOGA reduces
estimation error over conventional GNNs by 46.76% for predicting QoR after
logic synthesis; (2) HOGA improves 10.0% reasoning accuracy over GNNs for
identifying functional blocks on unseen gate-level netlists after complex
technology mapping; (3) The training time for HOGA almost linearly decreases
with an increase in computing resources.</div><div><a href='http://arxiv.org/abs/2403.01317v2'>2403.01317v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00012v2")'>PreRoutGNN for Timing Prediction with Order Preserving Partition: Global
  Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling</div>
<div id='2403.00012v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T02:23:07Z</div><div>Authors: Ruizhe Zhong, Junjie Ye, Zhentao Tang, Shixiong Kai, Mingxuan Yuan, Jianye Hao, Junchi Yan</div><div style='padding-top: 10px; width: 80ex'>Pre-routing timing prediction has been recently studied for evaluating the
quality of a candidate cell placement in chip design. It involves directly
estimating the timing metrics for both pin-level (slack, slew) and edge-level
(net delay, cell delay), without time-consuming routing. However, it often
suffers from signal decay and error accumulation due to the long timing paths
in large-scale industrial circuits. To address these challenges, we propose a
two-stage approach. First, we propose global circuit training to pre-train a
graph auto-encoder that learns the global graph embedding from circuit netlist.
Second, we use a novel node updating scheme for message passing on GCN,
following the topological sorting sequence of the learned graph embedding and
circuit graph. This scheme residually models the local time delay between two
adjacent pins in the updating sequence, and extracts the lookup table
information inside each cell via a new attention mechanism. To handle
large-scale circuits efficiently, we introduce an order preserving partition
scheme that reduces memory consumption while maintaining the topological
dependencies. Experiments on 21 real world circuits achieve a new SOTA R2 of
0.93 for slack prediction, which is significantly surpasses 0.59 by previous
SOTA method. Code will be available at:
https://github.com/Thinklab-SJTU/EDA-AI.</div><div><a href='http://arxiv.org/abs/2403.00012v2'>2403.00012v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04268v1")'>Qubit-Wise Architecture Search Method for Variational Quantum Circuits</div>
<div id='2403.04268v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T07:08:57Z</div><div>Authors: Jialin Chen, Zhiqiang Cai, Ke Xu, Di Wu, Wei Cao</div><div style='padding-top: 10px; width: 80ex'>Considering the noise level limit, one crucial aspect for quantum machine
learning is to design a high-performing variational quantum circuit
architecture with small number of quantum gates. As the classical neural
architecture search (NAS), quantum architecture search methods (QAS) employ
methods like reinforcement learning, evolutionary algorithms and supernet
optimiza-tion to improve the search efficiency. In this paper, we propose a
novel qubit-wise architec-ture search (QWAS) method, which progres-sively
search one-qubit configuration per stage, and combine with Monte Carlo Tree
Search al-gorithm to find good quantum architectures by partitioning the search
space into several good and bad subregions. The numerical experimental results
indicate that our proposed method can balance the exploration and exploitation
of cir-cuit performance and size in some real-world tasks, such as MNIST,
Fashion and MOSI. As far as we know, QWAS achieves the state-of-art re-sults of
all tasks in the terms of accuracy and circuit size.</div><div><a href='http://arxiv.org/abs/2403.04268v1'>2403.04268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07043v1")'>Quantum Advantage Actor-Critic for Reinforcement Learning</div>
<div id='2401.07043v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T11:08:45Z</div><div>Authors: Michael Kölle, Mohamad Hgog, Fabian Ritz, Philipp Altmann, Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien</div><div style='padding-top: 10px; width: 80ex'>Quantum computing offers efficient encapsulation of high-dimensional states.
In this work, we propose a novel quantum reinforcement learning approach that
combines the Advantage Actor-Critic algorithm with variational quantum circuits
by substituting parts of the classical components. This approach addresses
reinforcement learning's scalability concerns while maintaining high
performance. We empirically test multiple quantum Advantage Actor-Critic
configurations with the well known Cart Pole environment to evaluate our
approach in control tasks with continuous state spaces. Our results indicate
that the hybrid strategy of using either a quantum actor or quantum critic with
classical post-processing yields a substantial performance increase compared to
pure classical and pure quantum variants with similar parameter counts. They
further reveal the limits of current quantum approaches due to the hardware
constraints of noisy intermediate-scale quantum computers, suggesting further
research to scale hybrid approaches for larger and more complex control tasks.</div><div><a href='http://arxiv.org/abs/2401.07043v1'>2401.07043v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08307v1")'>On Quantum Natural Policy Gradients</div>
<div id='2401.08307v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T12:08:31Z</div><div>Authors: André Sequeira, Luis Paulo Santos, Luis Soares Barbosa</div><div style='padding-top: 10px; width: 80ex'>This research delves into the role of the quantum Fisher Information Matrix
(FIM) in enhancing the performance of Parameterized Quantum Circuit (PQC)-based
reinforcement learning agents. While previous studies have highlighted the
effectiveness of PQC-based policies preconditioned with the quantum FIM in
contextual bandits, its impact in broader reinforcement learning contexts, such
as Markov Decision Processes, is less clear. Through a detailed analysis of
L\"owner inequalities between quantum and classical FIMs, this study uncovers
the nuanced distinctions and implications of using each type of FIM. Our
results indicate that a PQC-based agent using the quantum FIM without
additional insights typically incurs a larger approximation error and does not
guarantee improved performance compared to the classical FIM. Empirical
evaluations in classic control benchmarks suggest even though quantum FIM
preconditioning outperforms standard gradient ascent, in general it is not
superior to classical FIM preconditioning.</div><div><a href='http://arxiv.org/abs/2401.08307v1'>2401.08307v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11048v1")'>JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks</div>
<div id='2403.11048v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T00:29:42Z</div><div>Authors: Ruhan Wang, Fahiz Baba-Yara, Fan Chen</div><div style='padding-top: 10px; width: 80ex'>Despite the success of Quantum Neural Networks (QNNs) in decision-making
systems, their fairness remains unexplored, as the focus primarily lies on
accuracy. This work conducts a design space exploration, unveiling QNN
unfairness, and highlighting the significant influence of QNN deployment and
quantum noise on accuracy and fairness. To effectively navigate the vast QNN
deployment design space, we propose JustQ, a framework for deploying fair and
accurate QNNs on NISQ computers. It includes a complete NISQ error model,
reinforcement learning-based deployment, and a flexible optimization objective
incorporating both fairness and accuracy. Experimental results show JustQ
outperforms previous methods, achieving superior accuracy and fairness. This
work pioneers fair QNN design on NISQ computers, paving the way for future
investigations.</div><div><a href='http://arxiv.org/abs/2403.11048v1'>2403.11048v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08491v2")'>Deep Reinforcement Learning for Controlled Traversing of the Attractor
  Landscape of Boolean Models in the Context of Cellular Reprogramming</div>
<div id='2402.08491v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T14:36:46Z</div><div>Authors: Andrzej Mizera, Jakub Zarzycki</div><div style='padding-top: 10px; width: 80ex'>Cellular reprogramming can be used for both the prevention and cure of
different diseases. However, the efficiency of discovering reprogramming
strategies with classical wet-lab experiments is hindered by lengthy time
commitments and high costs. In this study, we develop a novel computational
framework based on deep reinforcement learning that facilitates the
identification of reprogramming strategies. For this aim, we formulate a
control problem in the context of cellular reprogramming for the frameworks of
BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the
notion of a pseudo-attractor and a procedure for identification of
pseudo-attractor state during training. Finally, we devise a computational
framework for solving the control problem, which we test on a number of
different models.</div><div><a href='http://arxiv.org/abs/2402.08491v2'>2402.08491v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.11687v1")'>Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum
  Neural Networks</div>
<div id='2402.11687v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T19:35:30Z</div><div>Authors: Satwik Kundu, Debarshi Kundu, Swaroop Ghosh</div><div style='padding-top: 10px; width: 80ex'>Cloud hosting of quantum machine learning (QML) models exposes them to a
range of vulnerabilities, the most significant of which is the model stealing
attack. In this study, we assess the efficacy of such attacks in the realm of
quantum computing. We conducted comprehensive experiments on various datasets
with multiple QML model architectures. Our findings revealed that model
stealing attacks can produce clone models achieving up to $0.9\times$ and
$0.99\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels,
respectively ($k:$ num\_classes). To defend against these attacks, we leverage
the unique properties of current noisy hardware and perturb the victim model
outputs and hinder the attacker's training process. In particular, we propose:
1) hardware variation-induced perturbation (HVIP) and 2) hardware and
architecture variation-induced perturbation (HAVIP). Although noise and
architectural variability can provide up to $\sim16\%$ output obfuscation, our
comprehensive analysis revealed that models cloned under noisy conditions tend
to be resilient, suffering little to no performance degradation due to such
obfuscations. Despite limited success with our defense techniques, this outcome
has led to an important discovery: QML models trained on noisy hardwares are
naturally resistant to perturbation or obfuscation-based defenses or attacks.</div><div><a href='http://arxiv.org/abs/2402.11687v1'>2402.11687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10790v1")'>QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ
  Machines</div>
<div id='2403.10790v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T03:42:29Z</div><div>Authors: Zhenxiao Fu, Min Yang, Cheng Chu, Yilun Xu, Gang Huang, Fan Chen</div><div style='padding-top: 10px; width: 80ex'>Variational quantum circuits (VQCs) have become a powerful tool for
implementing Quantum Neural Networks (QNNs), addressing a wide range of complex
problems. Well-trained VQCs serve as valuable intellectual assets hosted on
cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them
susceptible to malicious VQC stealing attacks. However, traditional model
extraction techniques designed for classical machine learning models encounter
challenges when applied to NISQ computers due to significant noise in current
devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN
model extraction technique from cloud-based NISQ machines. Compared to existing
classical model stealing techniques, QuantumLeak improves local VQC accuracy by
4.99\%$\sim$7.35\% across diverse datasets and VQC architectures.</div><div><a href='http://arxiv.org/abs/2403.10790v1'>2403.10790v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15542v1")'>Streaming IoT Data and the Quantum Edge: A Classic/Quantum Machine
  Learning Use Case</div>
<div id='2402.15542v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T10:36:22Z</div><div>Authors: Sabrina Herbst, Vincenzo De Maio, Ivona Brandic</div><div style='padding-top: 10px; width: 80ex'>With the advent of the Post-Moore era, the scientific community is faced with
the challenge of addressing the demands of current data-intensive machine
learning applications, which are the cornerstone of urgent analytics in
distributed computing. Quantum machine learning could be a solution for the
increasing demand of urgent analytics, providing potential theoretical speedups
and increased space efficiency. However, challenges such as (1) the encoding of
data from the classical to the quantum domain, (2) hyperparameter tuning, and
(3) the integration of quantum hardware into a distributed computing continuum
limit the adoption of quantum machine learning for urgent analytics. In this
work, we investigate the use of Edge computing for the integration of quantum
machine learning into a distributed computing continuum, identifying the main
challenges and possible solutions. Furthermore, exploring the data encoding and
hyperparameter tuning challenges, we present preliminary results for quantum
machine learning analytics on an IoT scenario.</div><div><a href='http://arxiv.org/abs/2402.15542v1'>2402.15542v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10202v1")'>Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes</div>
<div id='2403.10202v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T11:07:38Z</div><div>Authors: Ahcen Aliouat, Elsa Dupraz</div><div style='padding-top: 10px; width: 80ex'>In goal-oriented communications, the objective of the receiver is often to
apply a Deep-Learning model, rather than reconstructing the original data. In
this context, direct learning over compressed data, without any prior decoding,
holds promise for enhancing the time-efficient execution of inference models at
the receiver. However, conventional entropic-coding methods like Huffman and
Arithmetic break data structure, rendering them unsuitable for learning without
decoding. In this paper, we propose an alternative approach in which entropic
coding is realized with Low-Density Parity Check (LDPC) codes. We hypothesize
that Deep Learning models can more effectively exploit the internal code
structure of LDPC codes. At the receiver, we leverage a specific class of
Recurrent Neural Networks (RNNs), specifically Gated Recurrent Unit (GRU),
trained for image classification. Our numerical results indicate that
classification based on LDPC-coded bit-planes surpasses Huffman and Arithmetic
coding, while necessitating a significantly smaller learning model. This
demonstrates the efficiency of classification directly from LDPC-coded data,
eliminating the need for any form of decompression, even partial, prior to
applying the learning model.</div><div><a href='http://arxiv.org/abs/2403.10202v1'>2403.10202v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08864v1")'>DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep
  Learning</div>
<div id='2402.08864v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T00:18:10Z</div><div>Authors: S Ashwin Hebbar, Sravan Kumar Ankireddy, Hyeji Kim, Sewoong Oh, Pramod Viswanath</div><div style='padding-top: 10px; width: 80ex'>Polar codes, developed on the foundation of Arikan's polarization kernel,
represent a breakthrough in coding theory and have emerged as the
state-of-the-art error-correction-code in short-to-medium block length regimes.
Importantly, recent research has indicated that the reliability of polar codes
can be further enhanced by substituting Arikan's kernel with a larger one,
leading to a faster polarization. However, for short-to-medium block length
regimes, the development of polar codes that effectively employ large kernel
sizes has not yet been realized. In this paper, we explore a novel, non-linear
generalization of polar codes with an expanded kernel size, which we call
DeepPolar codes. Our results show that DeepPolar codes effectively utilize the
benefits of larger kernel size, resulting in enhanced reliability compared to
both the existing neural codes and conventional polar codes.</div><div><a href='http://arxiv.org/abs/2402.08864v1'>2402.08864v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.01155v1")'>Deep Learning-Based Detection for Marker Codes over Insertion and
  Deletion Channels</div>
<div id='2401.01155v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T11:13:01Z</div><div>Authors: Guochen Ma, Xiaopeng Jiao, Jianjun Mu, Hui Han, Yaming Yang</div><div style='padding-top: 10px; width: 80ex'>Marker code is an effective coding scheme to protect data from insertions and
deletions. It has potential applications in future storage systems, such as DNA
storage and racetrack memory. When decoding marker codes, perfect channel state
information (CSI), i.e., insertion and deletion probabilities, are required to
detect insertion and deletion errors. Sometimes, the perfect CSI is not easy to
obtain or the accurate channel model is unknown. Therefore, it is deserved to
develop detecting algorithms for marker code without the knowledge of perfect
CSI. In this paper, we propose two CSI-agnostic detecting algorithms for marker
code based on deep learning. The first one is a model-driven deep learning
method, which deep unfolds the original iterative detecting algorithm of marker
code. In this method, CSI become weights in neural networks and these weights
can be learned from training data. The second one is a data-driven method which
is an end-to-end system based on the deep bidirectional gated recurrent unit
network. Simulation results show that error performances of the proposed
methods are significantly better than that of the original detection algorithm
with CSI uncertainty. Furthermore, the proposed data-driven method exhibits
better error performances than other methods for unknown channel models.</div><div><a href='http://arxiv.org/abs/2401.01155v1'>2401.01155v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.13575v1")'>CNN architecture extraction on edge GPU</div>
<div id='2401.13575v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T16:40:30Z</div><div>Authors: Peter Horvath, Lukasz Chmielewski, Leo Weissbart, Lejla Batina, Yuval Yarom</div><div style='padding-top: 10px; width: 80ex'>Neural networks have become popular due to their versatility and
state-of-the-art results in many applications, such as image classification,
natural language processing, speech recognition, forecasting, etc. These
applications are also used in resource-constrained environments such as
embedded devices. In this work, the susceptibility of neural network
implementations to reverse engineering is explored on the NVIDIA Jetson Nano
microcomputer via side-channel analysis. To this end, an architecture
extraction attack is presented. In the attack, 15 popular convolutional neural
network architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented
on the GPU of Jetson Nano and the electromagnetic radiation of the GPU is
analyzed during the inference operation of the neural networks. The results of
the analysis show that neural network architectures are easily distinguishable
using deep learning-based side-channel analysis.</div><div><a href='http://arxiv.org/abs/2401.13575v1'>2401.13575v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03761v1")'>Parameterized quantum comb and simpler circuits for reversing unknown
  qubit-unitary operations</div>
<div id='2403.03761v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T14:53:24Z</div><div>Authors: Yin Mo, Lei Zhang, Yu-Ao Chen, Yingjian Liu, Tengxiang Lin, Xin Wang</div><div style='padding-top: 10px; width: 80ex'>Quantum comb is an essential tool for characterizing complex quantum
protocols in quantum information processing. In this work, we introduce PQComb,
a framework leveraging parameterized quantum circuits to explore the
capabilities of quantum combs for general quantum process transformation tasks
and beyond. By optimizing PQComb for time-reversal simulations of unknown
unitary evolutions, we develop a simpler protocol for unknown qubit unitary
inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the
existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This
demonstrates the utility of quantum comb structures and showcases PQComb's
potential for solving complex quantum tasks. Our results pave the way for
broader PQComb applications in quantum computing and quantum information,
emphasizing its versatility for tackling diverse problems in quantum machine
learning.</div><div><a href='http://arxiv.org/abs/2403.03761v1'>2403.03761v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16353v1")'>An optimal tradeoff between entanglement and copy complexity for state
  tomography</div>
<div id='2402.16353v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T07:18:57Z</div><div>Authors: Sitan Chen, Jerry Li, Allen Liu</div><div style='padding-top: 10px; width: 80ex'>There has been significant interest in understanding how practical
constraints on contemporary quantum devices impact the complexity of quantum
learning. For the classic question of tomography, recent work tightly
characterized the copy complexity for any protocol that can only measure one
copy of the unknown state at a time, showing it is polynomially worse than if
one can make fully-entangled measurements. While we now have a fairly complete
picture of the rates for such tasks in the near-term and fault-tolerant
regimes, it remains poorly understood what the landscape in between looks like.
  In this work, we study tomography in the natural setting where one can make
measurements of $t$ copies at a time. For sufficiently small $\epsilon$, we
show that for any $t \le d^2$,
$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$ copies are necessary and
sufficient to learn an unknown $d$-dimensional state $\rho$ to trace distance
$\epsilon$. This gives a smooth and optimal interpolation between the known
rates for single-copy and fully-entangled measurements.
  To our knowledge, this is the first smooth entanglement-copy tradeoff known
for any quantum learning task, and for tomography, no intermediate point on
this curve was known, even at $t = 2$. An important obstacle is that unlike the
optimal single-copy protocol, the optimal fully-entangled protocol is
inherently biased and thus precludes naive batching approaches. Instead, we
devise a novel two-stage procedure that uses Keyl's algorithm to refine a crude
estimate for $\rho$ based on single-copy measurements. A key insight is to use
Schur-Weyl sampling not to estimate the spectrum of $\rho$, but to estimate the
deviation of $\rho$ from the maximally mixed state. When $\rho$ is far from the
maximally mixed state, we devise a novel quantum splitting procedure that
reduces to the case where $\rho$ is close to maximally mixed.</div><div><a href='http://arxiv.org/abs/2402.16353v1'>2402.16353v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17911v1")'>Demonstration of Robust and Efficient Quantum Property Learning with
  Shallow Shadows</div>
<div id='2402.17911v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:53:32Z</div><div>Authors: Hong-Ye Hu, Andi Gu, Swarnadeep Majumder, Hang Ren, Yipei Zhang, Derek S. Wang, Yi-Zhuang You, Zlatko Minev, Susanne F. Yelin, Alireza Seif</div><div style='padding-top: 10px; width: 80ex'>Extracting information efficiently from quantum systems is a major component
of quantum information processing tasks. Randomized measurements, or classical
shadows, enable predicting many properties of arbitrary quantum states using
few measurements. While random single qubit measurements are experimentally
friendly and suitable for learning low-weight Pauli observables, they perform
poorly for nonlocal observables. Prepending a shallow random quantum circuit
before measurements maintains this experimental friendliness, but also has
favorable sample complexities for observables beyond low-weight Paulis,
including high-weight Paulis and global low-rank properties such as fidelity.
However, in realistic scenarios, quantum noise accumulated with each additional
layer of the shallow circuit biases the results. To address these challenges,
we propose the robust shallow shadows protocol. Our protocol uses Bayesian
inference to learn the experimentally relevant noise model and mitigate it in
postprocessing. This mitigation introduces a bias-variance trade-off:
correcting for noise-induced bias comes at the cost of a larger estimator
variance. Despite this increased variance, as we demonstrate on a
superconducting quantum processor, our protocol correctly recovers state
properties such as expectation values, fidelity, and entanglement entropy,
while maintaining a lower sample complexity compared to the random single qubit
measurement scheme. We also theoretically analyze the effects of noise on
sample complexity and show how the optimal choice of the shallow shadow depth
varies with noise strength. This combined theoretical and experimental analysis
positions the robust shallow shadow protocol as a scalable, robust, and
sample-efficient protocol for characterizing quantum states on current quantum
computing platforms.</div><div><a href='http://arxiv.org/abs/2402.17911v1'>2402.17911v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02968v1")'>Hamiltonian Property Testing</div>
<div id='2403.02968v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T13:44:28Z</div><div>Authors: Andreas Bluhm, Matthias C. Caro, Aadil Oufkir</div><div style='padding-top: 10px; width: 80ex'>Locality is a fundamental feature of many physical time evolutions.
Assumptions on locality and related structural properties also underlie
recently proposed procedures for learning an unknown Hamiltonian from access to
the induced time evolution. However, no protocols to rigorously test whether an
unknown Hamiltonian is local were known. We investigate Hamiltonian locality
testing as a property testing problem, where the task is to determine whether
an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all
$k$-local Hamiltonians, given access to the time evolution along $H$. First, we
emphasize the importance of the chosen distance measure: With respect to the
operator norm, a worst-case distance measure, incoherent quantum locality
testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an
expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even
coherent testers need $\Omega(2^{n/2})$ many queries and
$\Omega(2^{n/2}/\varepsilon)$ total evolution time. In contrast, when distances
are measured according to the normalized Frobenius norm, corresponding to an
average-case distance, we give a sample-, time-, and computationally efficient
incoherent Hamiltonian locality testing algorithm based on randomized
measurements. In fact, our procedure can be used to simultaneously test a wide
class of Hamiltonian properties beyond locality. Finally, we prove that
learning a general Hamiltonian remains exponentially hard with this
average-case distance, thereby establishing an exponential separation between
Hamiltonian testing and learning. Our work initiates the study of property
testing for quantum Hamiltonians, demonstrating that a broad class of
Hamiltonian properties is efficiently testable even with limited quantum
capabilities, and positioning Hamiltonian testing as an independent area of
research alongside Hamiltonian learning.</div><div><a href='http://arxiv.org/abs/2403.02968v1'>2403.02968v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00176v2")'>Adversarial Quantum Machine Learning: An Information-Theoretic
  Generalization Analysis</div>
<div id='2402.00176v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T21:07:43Z</div><div>Authors: Petros Georgiou, Sharu Theresa Jose, Osvaldo Simeone</div><div style='padding-top: 10px; width: 80ex'>In a manner analogous to their classical counterparts, quantum classifiers
are vulnerable to adversarial attacks that perturb their inputs. A promising
countermeasure is to train the quantum classifier by adopting an attack-aware,
or adversarial, loss function. This paper studies the generalization properties
of quantum classifiers that are adversarially trained against bounded-norm
white-box attacks. Specifically, a quantum adversary maximizes the classifier's
loss by transforming an input state $\rho(x)$ into a state $\lambda$ that is
$\epsilon$-close to the original state $\rho(x)$ in $p$-Schatten distance.
Under suitable assumptions on the quantum embedding $\rho(x)$, we derive novel
information-theoretic upper bounds on the generalization error of adversarially
trained quantum classifiers for $p = 1$ and $p = \infty$. The derived upper
bounds consist of two terms: the first is an exponential function of the
2-R\'enyi mutual information between classical data and quantum embedding,
while the second term scales linearly with the adversarial perturbation size
$\epsilon$. Both terms are shown to decrease as $1/\sqrt{T}$ over the training
set size $T$ . An extension is also considered in which the adversary assumed
during training has different parameters $p$ and $\epsilon$ as compared to the
adversary affecting the test inputs. Finally, we validate our theoretical
findings with numerical experiments for a synthetic setting.</div><div><a href='http://arxiv.org/abs/2402.00176v2'>2402.00176v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08648v1")'>Generating Universal Adversarial Perturbations for Quantum Classifiers</div>
<div id='2402.08648v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:27:53Z</div><div>Authors: Gautham Anil, Vishnu Vinod, Apurva Narayan</div><div style='padding-top: 10px; width: 80ex'>Quantum Machine Learning (QML) has emerged as a promising field of research,
aiming to leverage the capabilities of quantum computing to enhance existing
machine learning methodologies. Recent studies have revealed that, like their
classical counterparts, QML models based on Parametrized Quantum Circuits
(PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of
Universal Adversarial Perturbations (UAPs) in the quantum domain has been
demonstrated theoretically in the context of quantum classifiers. In this work,
we introduce QuGAP: a novel framework for generating UAPs for quantum
classifiers. We conceptualize the notion of additive UAPs for PQC-based
classifiers and theoretically demonstrate their existence. We then utilize
generative models (QuGAP-A) to craft additive UAPs and experimentally show that
quantum classifiers are susceptible to such attacks. Moreover, we formulate a
new method for generating unitary UAPs (QuGAP-U) using quantum generative
models and a novel loss function based on fidelity constraints. We evaluate the
performance of the proposed framework and show that our method achieves
state-of-the-art misclassification rates, while maintaining high fidelity
between legitimate and adversarial samples.</div><div><a href='http://arxiv.org/abs/2402.08648v1'>2402.08648v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12704v1")'>Quantum Embedding with Transformer for High-dimensional Data</div>
<div id='2402.12704v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T04:06:28Z</div><div>Authors: Hao-Yuan Chen, Yen-Jui Chang, Shih-Wei Liao, Ching-Ray Chang</div><div style='padding-top: 10px; width: 80ex'>Quantum embedding with transformers is a novel and promising architecture for
quantum machine learning to deliver exceptional capability on near-term devices
or simulators. The research incorporated a vision transformer (ViT) to advance
quantum significantly embedding ability and results for a single qubit
classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a
challenging high-dimensional dataset. The study showcases and analyzes
empirical evidence that our transformer-based architecture is a highly
versatile and practical approach to modern quantum machine learning problems.</div><div><a href='http://arxiv.org/abs/2402.12704v1'>2402.12704v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14753v1")'>Learning with SASQuaTCh: a Novel Variational Quantum Transformer
  Architecture with Kernel-Based Self-Attention</div>
<div id='2403.14753v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T18:00:04Z</div><div>Authors: Ethan N. Evans, Matthew Cook, Zachary P. Bradshaw, Margarite L. LaBorde</div><div style='padding-top: 10px; width: 80ex'>The widely popular transformer network popularized by the generative
pre-trained transformer (GPT) has a large field of applicability, including
predicting text and images, classification, and even predicting solutions to
the dynamics of physical systems. In the latter context, the continuous analog
of the self-attention mechanism at the heart of transformer networks has been
applied to learning the solutions of partial differential equations and reveals
a convolution kernel nature that can be exploited by the Fourier transform. It
is well known that many quantum algorithms that have provably demonstrated a
speedup over classical algorithms utilize the quantum Fourier transform. In
this work, we explore quantum circuits that can efficiently express a
self-attention mechanism through the perspective of kernel-based operator
learning. In this perspective, we are able to represent deep layers of a vision
transformer network using simple gate operations and a set of multi-dimensional
quantum Fourier transforms. We analyze the computational and parameter
complexity of our novel variational quantum circuit, which we call
Self-Attention Sequential Quantum Transformer Channel (SASQuaTCh), and
demonstrate its utility on simplified classification problems.</div><div><a href='http://arxiv.org/abs/2403.14753v1'>2403.14753v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15031v1")'>Image Classification with Rotation-Invariant Variational Quantum
  Circuits</div>
<div id='2403.15031v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T08:26:31Z</div><div>Authors: Paul San Sebastian, Mikel Cañizo, Román Orús</div><div style='padding-top: 10px; width: 80ex'>Variational quantum algorithms are gaining attention as an early application
of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of
variational methods lies in the phenomenon of Barren Plateaus, present in the
optimization of variational parameters. Adding geometric inductive bias to the
quantum models has been proposed as a potential solution to mitigate this
problem, leading to a new field called Geometric Quantum Machine Learning. In
this work, an equivariant architecture for variational quantum classifiers is
introduced to create a label-invariant model for image classification with
$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against
two different architectures, and it is experimentally observed that the
geometric approach boosts the model's performance. Finally, a classical
equivariant convolution operation is proposed to extend the quantum model for
the processing of larger images, employing the resources available in NISQ
devices.</div><div><a href='http://arxiv.org/abs/2403.15031v1'>2403.15031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09318v1")'>A Hierarchical Fused Quantum Fuzzy Neural Network for Image
  Classification</div>
<div id='2403.09318v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T12:09:36Z</div><div>Authors: Sheng-Yao Wu, Run-Ze Li, Yan-Qi Song, Su-Juan Qin, Qiao-Yan Wen, Fei Gao</div><div style='padding-top: 10px; width: 80ex'>Neural network is a powerful learning paradigm for data feature learning in
the era of big data. However, most neural network models are deterministic
models that ignore the uncertainty of data. Fuzzy neural networks are proposed
to address this problem. FDNN is a hierarchical deep neural network that
derives information from both fuzzy and neural representations, the
representations are then fused to form representation to be classified. FDNN
perform well on uncertain data classification tasks. In this paper, we proposed
a novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from
classical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership
functions in fuzzy neural network. We conducted simulated experiment on two
types of datasets (Dirty-MNIST and 15-Scene), the results show that the
proposed model can outperform several existing methods. In addition, we
demonstrate the robustness of the proposed quantum circuit.</div><div><a href='http://arxiv.org/abs/2403.09318v1'>2403.09318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05754v1")'>Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with
  Completeness Analysis</div>
<div id='2403.05754v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T01:34:26Z</div><div>Authors: Andi Chen, Hua-Lei Yin, Zeng-Bing Chen, Shengjun Wu</div><div style='padding-top: 10px; width: 80ex'>With the contemporary digital technology approaching, deep neural networks
are emerging as the foundational algorithm of the artificial intelligence boom.
Whereas, the evolving social demands have been emphasizing the necessity of
novel methodologies to substitute traditional neural networks. Concurrently,
the advent of the post-Moore era has spurred the development of
quantum-inspired neural networks with outstanding potentials at certain
circumstances. Nonetheless, a definitive evaluating system with detailed
metrics is tremendously vital and indispensable owing to the vague indicators
in comparison between the novel and traditional deep learning models at
present. Hence, to improve and evaluate the performances of the novel neural
networks more comprehensively in complex and unpredictable environments, we
propose two hybrid quantum-inspired neural networks which are rooted in
residual and dense connections respectively for pattern recognitions with
completeness representation theory for model assessment. Comparative analyses
against pure classical models with detailed frameworks reveal that our hybrid
models with lower parameter complexity not only match the generalization power
of pure classical models, but also outperform them notably in resistance to
parameter attacks with various asymmetric noises. Moreover, our hybrid models
indicate unique superiority to prevent gradient explosion problems through
theoretical argumentation. Eventually, We elaborate on the application
scenarios where our hybrid models are applicable and efficient, which paves the
way for their industrialization and commercialization.</div><div><a href='http://arxiv.org/abs/2403.05754v1'>2403.05754v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02866v1")'>Quantum Normalizing Flows for Anomaly Detection</div>
<div id='2402.02866v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T10:28:20Z</div><div>Authors: Bodo Rosenhahn, Christoph Hirche</div><div style='padding-top: 10px; width: 80ex'>A Normalizing Flow computes a bijective mapping from an arbitrary
distribution to a predefined (e.g. normal) distribution. Such a flow can be
used to address different tasks, e.g. anomaly detection, once such a mapping
has been learned. In this work we introduce Normalizing Flows for Quantum
architectures, describe how to model and optimize such a flow and evaluate our
method on example datasets. Our proposed models show competitive performance
for anomaly detection compared to classical methods, e.g. based on isolation
forests, the local outlier factor (LOF) or single-class SVMs, while being fully
executable on a quantum computer.</div><div><a href='http://arxiv.org/abs/2402.02866v1'>2402.02866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09978v1")'>Deep learning for the design of non-Hermitian topolectrical circuits</div>
<div id='2402.09978v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:41:55Z</div><div>Authors: Xi Chen, Jinyang Sun, Xiumei Wang, Hengxuan Jiang, Dandan Zhu, Xingping Zhou</div><div style='padding-top: 10px; width: 80ex'>Non-Hermitian topological phases can produce some remarkable properties,
compared with their Hermitian counterpart, such as the breakdown of
conventional bulk-boundary correspondence and the non-Hermitian topological
edge mode. Here, we introduce several algorithms with multi-layer perceptron
(MLP), and convolutional neural network (CNN) in the field of deep learning, to
predict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we
use the smallest module of the periodic circuit as one unit to construct
high-dimensional circuit data features. Further, we use the Dense Convolutional
Network (DenseNet), a type of convolutional neural network that utilizes dense
connections between layers to design a non-Hermitian topolectrical Chern
circuit, as the DenseNet algorithm is more suitable for processing
high-dimensional data. Our results demonstrate the effectiveness of the deep
learning network in capturing the global topological characteristics of a
non-Hermitian system based on training data.</div><div><a href='http://arxiv.org/abs/2402.09978v1'>2402.09978v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07232v1")'>Polariton lattices as binarized neuromorphic networks</div>
<div id='2401.07232v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T08:32:41Z</div><div>Authors: Evgeny Sedov, Alexey Kavokin</div><div style='padding-top: 10px; width: 80ex'>We introduce a novel neuromorphic network architecture based on a lattice of
exciton-polariton condensates, intricately interconnected and energized through
non-resonant optical pumping. The network employs a binary framework, where
each neuron, facilitated by the spatial coherence of pairwise coupled
condensates, performs binary operations. This coherence, emerging from the
ballistic propagation of polaritons, ensures efficient, network-wide
communication. The binary neuron switching mechanism, driven by the nonlinear
repulsion through the excitonic component of polaritons, offers computational
efficiency and scalability advantages over continuous weight neural networks.
Our network enables parallel processing, enhancing computational speed compared
to sequential or pulse-coded binary systems. The system's performance was
evaluated using the MNIST dataset for handwritten digit recognition, showcasing
the potential to outperform existing polaritonic neuromorphic systems, as
demonstrated by its impressive predicted classification accuracy of up to
97.5%.</div><div><a href='http://arxiv.org/abs/2401.07232v1'>2401.07232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17750v1")'>Scaling on-chip photonic neural processors using arbitrarily
  programmable wave propagation</div>
<div id='2402.17750v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:37:22Z</div><div>Authors: Tatsuhiro Onodera, Martin M. Stein, Benjamin A. Ash, Mandar M. Sohoni, Melissa Bosch, Ryotatsu Yanagimoto, Marc Jankowski, Timothy P. McKenna, Tianyu Wang, Gennady Shvets, Maxim R. Shcherbakov, Logan G. Wright, Peter L. McMahon</div><div style='padding-top: 10px; width: 80ex'>On-chip photonic processors for neural networks have potential benefits in
both speed and energy efficiency but have not yet reached the scale at which
they can outperform electronic processors. The dominant paradigm for designing
on-chip photonics is to make networks of relatively bulky discrete components
connected by one-dimensional waveguides. A far more compact alternative is to
avoid explicitly defining any components and instead sculpt the continuous
substrate of the photonic processor to directly perform the computation using
waves freely propagating in two dimensions. We propose and demonstrate a device
whose refractive index as a function of space, $n(x,z)$, can be rapidly
reprogrammed, allowing arbitrary control over the wave propagation in the
device. Our device, a 2D-programmable waveguide, combines photoconductive gain
with the electro-optic effect to achieve massively parallel modulation of the
refractive index of a slab waveguide, with an index modulation depth of
$10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a
prototype device with a functional area of $12\,\text{mm}^2$ to perform
neural-network inference with up to 49-dimensional input vectors in a single
pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7
\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyond
that of previous photonic chips relying on discrete components, illustrating
the benefit of the continuous-waves paradigm. In principle, with large enough
chip area, the reprogrammability of the device's refractive index distribution
enables the reconfigurable realization of any passive, linear photonic circuit
or device. This promises the development of more compact and versatile photonic
systems for a wide range of applications, including optical processing, smart
sensing, spectroscopy, and optical communications.</div><div><a href='http://arxiv.org/abs/2402.17750v1'>2402.17750v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09135v1")'>Unconventional Computing based on Four Wave Mixing in Highly Nonlinear
  Waveguides</div>
<div id='2402.09135v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T12:34:38Z</div><div>Authors: Kostas Sozos, Stavros Deligiannidis, Charis Mesaritakis, Adonis Bogris</div><div style='padding-top: 10px; width: 80ex'>In this work we numerically analyze a photonic unconventional accelerator
based on the four-wave mixing effect in highly nonlinear waveguides. The
proposed scheme can act as a fully analogue system for nonlinear signal
processing directly in the optical domain. By exploiting the rich Kerr-induced
nonlinearities, multiple nonlinear transformations of an input signal can be
generated and used for solving complex nonlinear tasks. We first evaluate the
performance of our scheme in the Santa-Fe chaotic time-series prediction. The
true power of this processor is revealed in the all-optical nonlinearity
compensation in an optical communication scenario where we provide results
superior to those offered by strong machine learning algorithms with reduced
power consumption and computational complexity. Finally, we showcase how the
FWM module can be used as a reconfigurable nonlinear activation module being
capable of reproducing characteristic functions such as sigmoid or rectified
linear unit.</div><div><a href='http://arxiv.org/abs/2402.09135v1'>2402.09135v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11420v1")'>Neural network representation of quantum systems</div>
<div id='2403.11420v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T02:20:22Z</div><div>Authors: Koji Hashimoto, Yuji Hirono, Jun Maeda, Jojiro Totsuka-Yoshinaka</div><div style='padding-top: 10px; width: 80ex'>It has been proposed that random wide neural networks near Gaussian process
are quantum field theories around Gaussian fixed points. In this paper, we
provide a novel map with which a wide class of quantum mechanical systems can
be cast into the form of a neural network with a statistical summation over
network parameters. Our simple idea is to use the universal approximation
theorem of neural networks to generate arbitrary paths in the Feynman's path
integral. The map can be applied to interacting quantum systems / field
theories, even away from the Gaussian limit. Our findings bring machine
learning closer to the quantum world.</div><div><a href='http://arxiv.org/abs/2403.11420v1'>2403.11420v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03245v1")'>Neural Network Learning and Quantum Gravity</div>
<div id='2403.03245v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T19:00:01Z</div><div>Authors: Stefano Lanza</div><div style='padding-top: 10px; width: 80ex'>The landscape of low-energy effective field theories stemming from string
theory is too vast for a systematic exploration. However, the meadows of the
string landscape may be fertile ground for the application of machine learning
techniques. Employing neural network learning may allow for inferring novel,
undiscovered properties that consistent theories in the landscape should
possess, or checking conjectural statements about alleged characteristics
thereof. The aim of this work is to describe to what extent the string
landscape can be explored with neural network-based learning. Our analysis is
motivated by recent studies that show that the string landscape is
characterized by finiteness properties, emerging from its underlying tame,
o-minimal structures. Indeed, employing these results, we illustrate that any
low-energy effective theory of string theory is endowed with certain
statistical learnability properties. Consequently, several learning problems
therein formulated, including interpolations and multi-class classification
problems, can be concretely addressed with machine learning, delivering results
with sufficiently high accuracy.</div><div><a href='http://arxiv.org/abs/2403.03245v1'>2403.03245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11701v1")'>Explaining the Machine Learning Solution of the Ising Model</div>
<div id='2402.11701v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T20:47:33Z</div><div>Authors: Roberto C. Alamino</div><div style='padding-top: 10px; width: 80ex'>As powerful as machine learning (ML) techniques are in solving problems
involving data with large dimensionality, explaining the results from the
fitted parameters remains a challenging task of utmost importance, especially
in physics applications. Here it is shown how this can be accomplished for the
ferromagnetic Ising model, the target of many ML studies in the last years. By
using a neural network (NN) without any hidden layers and the symmetry of the
Hamiltonian to find the critical temperature for the continuous phase
transition of the model, an explanation of its strategy is found. This allows
the prediction of the minimal extension of the NN to solve the problem when the
symmetry is not known, which is also explainable.</div><div><a href='http://arxiv.org/abs/2402.11701v1'>2402.11701v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14515v2")'>Spectral invariance and maximality properties of the frequency spectrum
  of quantum neural networks</div>
<div id='2402.14515v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T13:04:50Z</div><div>Authors: Patrick Holzer, Ivica Turkalj</div><div style='padding-top: 10px; width: 80ex'>Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine
Learning due to their close connection to Variational Quantum Circuits, making
them a promising candidate for practical applications on Noisy
Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite
Fourier series, where the set of frequencies is called the frequency spectrum.
We analyse this frequency spectrum and prove, for a large class of models,
various maximality results. Furthermore, we prove that under some mild
conditions there exists a bijection between classes of models with the same
area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the
number of qubits and $L$ the number of layers, which we consequently call
spectral invariance under area-preserving transformations. With this we explain
the symmetry in $R$ and $L$ in the results often observed in the literature and
show that the maximal frequency spectrum depends only on the area $A = RL$ and
not on the individual values of $R$ and $L$. Moreover, we extend existing
results and specify the maximum possible frequency spectrum of a QNN with
arbitrarily many layers as a function of the spectrum of its generators. If the
generators of the QNN can be further decomposed into 2-dimensional
sub-generators, then this specification follows from elementary
number-theoretical considerations. In the case of arbitrary dimensional
generators, we extend existing results based on the so-called Golomb ruler and
introduce a second novel approach based on a variation of the turnpike problem,
which we call the relaxed turnpike problem.</div><div><a href='http://arxiv.org/abs/2402.14515v2'>2402.14515v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10293v1")'>Symmetry breaking in geometric quantum machine learning in the presence
  of noise</div>
<div id='2401.10293v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T19:00:00Z</div><div>Authors: Cenk Tüysüz, Su Yeon Chang, Maria Demidik, Karl Jansen, Sofia Vallecorsa, Michele Grossi</div><div style='padding-top: 10px; width: 80ex'>Geometric quantum machine learning based on equivariant quantum neural
networks (EQNN) recently appeared as a promising direction in quantum machine
learning. Despite the encouraging progress, the studies are still limited to
theory, and the role of hardware noise in EQNN training has never been
explored. This work studies the behavior of EQNN models in the presence of
noise. We show that certain EQNN models can preserve equivariance under Pauli
channels, while this is not possible under the amplitude damping channel. We
claim that the symmetry breaking grows linearly in the number of layers and
noise strength. We support our claims with numerical data from simulations as
well as hardware up to 64 qubits. Furthermore, we provide strategies to enhance
the symmetry protection of EQNN models in the presence of noise.</div><div><a href='http://arxiv.org/abs/2401.10293v1'>2401.10293v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06300v1")'>Advantage of Quantum Neural Networks as Quantum Information Decoders</div>
<div id='2401.06300v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T23:56:29Z</div><div>Authors: Weishun Zhong, Oles Shtanko, Ramis Movassagh</div><div style='padding-top: 10px; width: 80ex'>A promising strategy to protect quantum information from noise-induced errors
is to encode it into the low-energy states of a topological quantum memory
device. However, readout errors from such memory under realistic settings is
less understood. We study the problem of decoding quantum information encoded
in the groundspaces of topological stabilizer Hamiltonians in the presence of
generic perturbations, such as quenched disorder. We first prove that the
standard stabilizer-based error correction and decoding schemes work adequately
well in such perturbed quantum codes by showing that the decoding error
diminishes exponentially in the distance of the underlying unperturbed code. We
then prove that Quantum Neural Network (QNN) decoders provide an almost
quadratic improvement on the readout error. Thus, we demonstrate provable
advantage of using QNNs for decoding realistic quantum error-correcting codes,
and our result enables the exploration of a wider range of non-stabilizer codes
in the near-term laboratory settings.</div><div><a href='http://arxiv.org/abs/2401.06300v1'>2401.06300v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17116v1")'>Quantum error mitigation and correction mediated by Yang-Baxter equation
  and artificial neural network</div>
<div id='2401.17116v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T15:50:06Z</div><div>Authors: Sahil Gulania, Yuri Alexeev, Stephen K. Gray, Bo Peng, Niranjan Govind</div><div style='padding-top: 10px; width: 80ex'>Quantum computing shows great potential, but errors pose a significant
challenge. This study explores new strategies for mitigating quantum errors
using artificial neural networks (ANN) and the Yang-Baxter equation (YBE).
Unlike traditional error correction methods, which are computationally
intensive, we investigate artificial error mitigation. The manuscript
introduces the basics of quantum error sources and explores the potential of
using classical computation for error mitigation. The Yang-Baxter equation
plays a crucial role, allowing us to compress time dynamics simulations into
constant-depth circuits. By introducing controlled noise through the YBE, we
enhance the dataset for error mitigation. We train an ANN model on partial data
from quantum simulations, demonstrating its effectiveness in correcting errors
in time-evolving quantum states.</div><div><a href='http://arxiv.org/abs/2401.17116v1'>2401.17116v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07025v1")'>Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation
  via Neural Networks</div>
<div id='2403.07025v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T15:35:41Z</div><div>Authors: Subhasree Bhattacharjee, Soumyadip Sarkar, Kunal Das, Bikramjit Sarkar</div><div style='padding-top: 10px; width: 80ex'>In the emergent realm of quantum computing, the Variational Quantum
Eigensolver (VQE) stands out as a promising algorithm for solving complex
quantum problems, especially in the noisy intermediate-scale quantum (NISQ)
era. However, the ubiquitous presence of noise in quantum devices often limits
the accuracy and reliability of VQE outcomes. This research introduces a novel
approach to ameliorate this challenge by utilizing neural networks for zero
noise extrapolation (ZNE) in VQE computations. By employing the Qiskit
framework, we crafted parameterized quantum circuits using the RY-RZ ansatz and
examined their behavior under varying levels of depolarizing noise. Our
investigations spanned from determining the expectation values of a
Hamiltonian, defined as a tensor product of Z operators, under different noise
intensities to extracting the ground state energy. To bridge the observed
outcomes under noise with the ideal noise-free scenario, we trained a Feed
Forward Neural Network on the error probabilities and their associated
expectation values. Remarkably, our model proficiently predicted the VQE
outcome under hypothetical noise-free conditions. By juxtaposing the simulation
results with real quantum device executions, we unveiled the discrepancies
induced by noise and showcased the efficacy of our neural network-based ZNE
technique in rectifying them. This integrative approach not only paves the way
for enhanced accuracy in VQE computations on NISQ devices but also underlines
the immense potential of hybrid quantum-classical paradigms in circumventing
the challenges posed by quantum noise. Through this research, we envision a
future where quantum algorithms can be reliably executed on noisy devices,
bringing us one step closer to realizing the full potential of quantum
computing.</div><div><a href='http://arxiv.org/abs/2403.07025v1'>2403.07025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07264v1")'>Near-Interpolators: Rapid Norm Growth and the Trade-Off between
  Interpolation and Generalization</div>
<div id='2403.07264v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T02:47:00Z</div><div>Authors: Yutong Wang, Rishi Sonthalia, Wei Hu</div><div style='padding-top: 10px; width: 80ex'>We study the generalization capability of nearly-interpolating linear
regressors: $\boldsymbol{\beta}$'s whose training error $\tau$ is positive but
small, i.e., below the noise floor. Under a random matrix theoretic assumption
on the data distribution and an eigendecay assumption on the data covariance
matrix $\boldsymbol{\Sigma}$, we demonstrate that any near-interpolator
exhibits rapid norm growth: for $\tau$ fixed, $\boldsymbol{\beta}$ has squared
$\ell_2$-norm $\mathbb{E}[\|{\boldsymbol{\beta}}\|_{2}^{2}] =
\Omega(n^{\alpha})$ where $n$ is the number of samples and $\alpha &gt;1$ is the
exponent of the eigendecay, i.e., $\lambda_i(\boldsymbol{\Sigma}) \sim
i^{-\alpha}$. This implies that existing data-independent norm-based bounds are
necessarily loose. On the other hand, in the same regime we precisely
characterize the asymptotic trade-off between interpolation and generalization.
Our characterization reveals that larger norm scaling exponents $\alpha$
correspond to worse trade-offs between interpolation and generalization. We
verify empirically that a similar phenomenon holds for nearly-interpolating
shallow neural networks.</div><div><a href='http://arxiv.org/abs/2403.07264v1'>2403.07264v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01092v1")'>A Dynamical Model of Neural Scaling Laws</div>
<div id='2402.01092v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T01:41:38Z</div><div>Authors: Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan</div><div style='padding-top: 10px; width: 80ex'>On a variety of tasks, the performance of neural networks predictably
improves with training time, dataset size and model size across many orders of
magnitude. This phenomenon is known as a neural scaling law. Of fundamental
importance is the compute-optimal scaling law, which reports the performance as
a function of units of compute when choosing model sizes optimally. We analyze
a random feature model trained with gradient descent as a solvable model of
network training and generalization. This reproduces many observations about
neural scaling laws. First, our model makes a prediction about why the scaling
of performance with training time and with model size have different power law
exponents. Consequently, the theory predicts an asymmetric compute-optimal
scaling rule where the number of training steps are increased faster than model
parameters, consistent with recent empirical observations. Second, it has been
observed that early in training, networks converge to their infinite-width
dynamics at a rate $1/\textit{width}$ but at late time exhibit a rate
$\textit{width}^{-c}$, where $c$ depends on the structure of the architecture
and task. We show that our model exhibits this behavior. Lastly, our theory
shows how the gap between training and test loss can gradually build up over
time due to repeated reuse of data.</div><div><a href='http://arxiv.org/abs/2402.01092v1'>2402.01092v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15926v1")'>Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of
  the Loss Improves Optimization Efficiency</div>
<div id='2402.15926v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T23:10:28Z</div><div>Authors: Jingfeng Wu, Peter L. Bartlett, Matus Telgarsky, Bin Yu</div><div style='padding-top: 10px; width: 80ex'>We consider gradient descent (GD) with a constant stepsize applied to
logistic regression with linearly separable data, where the constant stepsize
$\eta$ is so large that the loss initially oscillates. We show that GD exits
this initial oscillatory phase rapidly -- in $\mathcal{O}(\eta)$ steps -- and
subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate
after $t$ additional steps. Our results imply that, given a budget of $T$
steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with
an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or
variable stepsize schedulers. Our proof technique is versatile and also handles
general classification loss functions (where exponential tails are needed for
the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the
neural tangent kernel regime, and online stochastic gradient descent (SGD) with
a large stepsize, under suitable separability conditions.</div><div><a href='http://arxiv.org/abs/2402.15926v1'>2402.15926v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12058v1")'>The Dimension Strikes Back with Gradients: Generalization of Gradient
  Methods in Stochastic Convex Optimization</div>
<div id='2401.12058v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T15:50:32Z</div><div>Authors: Matan Schliserman, Uri Sherman, Tomer Koren</div><div style='padding-top: 10px; width: 80ex'>We study the generalization performance of gradient methods in the
fundamental stochastic convex optimization setting, focusing on its dimension
dependence. First, for full-batch gradient descent (GD) we give a construction
of a learning problem in dimension $d=O(n^2)$, where the canonical version of
GD (tuned for optimal performance of the empirical risk) trained with $n$
training examples converges, with constant probability, to an approximate
empirical risk minimizer with $\Omega(1)$ population excess risk. Our bound
translates to a lower bound of $\Omega (\sqrt{d})$ on the number of training
examples required for standard GD to reach a non-trivial test error, answering
an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b)
and showing that a non-trivial dimension dependence is unavoidable.
Furthermore, for standard one-pass stochastic gradient descent (SGD), we show
that an application of the same construction technique provides a similar
$\Omega(\sqrt{d})$ lower bound for the sample complexity of SGD to reach a
non-trivial empirical error, despite achieving optimal test performance. This
again provides an exponential improvement in the dimension dependence compared
to previous work (Koren, Livni, Mansour, and Sherman, 2022), resolving an open
question left therein.</div><div><a href='http://arxiv.org/abs/2401.12058v1'>2401.12058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01515v1")'>Enhancing Stochastic Gradient Descent: A Unified Framework and Novel
  Acceleration Methods for Faster Convergence</div>
<div id='2402.01515v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T15:55:25Z</div><div>Authors: Yichuan Deng, Zhao Song, Chiwun Yang</div><div style='padding-top: 10px; width: 80ex'>Based on SGD, previous works have proposed many algorithms that have improved
convergence speed and generalization in stochastic optimization, such as SGDm,
AdaGrad, Adam, etc. However, their convergence analysis under non-convex
conditions is challenging. In this work, we propose a unified framework to
address this issue. For any first-order methods, we interpret the updated
direction $g_t$ as the sum of the stochastic subgradient $\nabla f_t(x_t)$ and
an additional acceleration term $\frac{2|\langle v_t, \nabla f_t(x_t)
\rangle|}{\|v_t\|_2^2} v_t$, thus we can discuss the convergence by analyzing
$\langle v_t, \nabla f_t(x_t) \rangle$. Through our framework, we have
discovered two plug-and-play acceleration methods: \textbf{Reject Accelerating}
and \textbf{Random Vector Accelerating}, we theoretically demonstrate that
these two methods can directly lead to an improvement in convergence rate.</div><div><a href='http://arxiv.org/abs/2402.01515v1'>2402.01515v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15244v1")'>A Stochastic Quasi-Newton Method for Non-convex Optimization with
  Non-uniform Smoothness</div>
<div id='2403.15244v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T14:40:29Z</div><div>Authors: Zhenyu Sun, Ermin Wei</div><div style='padding-top: 10px; width: 80ex'>Classical convergence analyses for optimization algorithms rely on the
widely-adopted uniform smoothness assumption. However, recent experimental
studies have demonstrated that many machine learning problems exhibit
non-uniform smoothness, meaning the smoothness factor is a function of the
model parameter instead of a universal constant. In particular, it has been
observed that the smoothness grows with respect to the gradient norm along the
training trajectory. Motivated by this phenomenon, the recently introduced
$(L_0, L_1)$-smoothness is a more general notion, compared to traditional
$L$-smoothness, that captures such positive relationship between smoothness and
gradient norm. Under this type of non-uniform smoothness, existing literature
has designed stochastic first-order algorithms by utilizing gradient clipping
techniques to obtain the optimal $\mathcal{O}(\epsilon^{-3})$ sample complexity
for finding an $\epsilon$-approximate first-order stationary solution.
Nevertheless, the studies of quasi-Newton methods are still lacking.
Considering higher accuracy and more robustness for quasi-Newton methods, in
this paper we propose a fast stochastic quasi-Newton method when there exists
non-uniformity in smoothness. Leveraging gradient clipping and variance
reduction, our algorithm can achieve the best-known
$\mathcal{O}(\epsilon^{-3})$ sample complexity and enjoys convergence speedup
with simple hyperparameter tuning. Our numerical experiments show that our
proposed algorithm outperforms the state-of-the-art approaches.</div><div><a href='http://arxiv.org/abs/2403.15244v1'>2403.15244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07793v2")'>Tuning-Free Stochastic Optimization</div>
<div id='2402.07793v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:59:06Z</div><div>Authors: Ahmed Khaled, Chi Jin</div><div style='padding-top: 10px; width: 80ex'>Large-scale machine learning problems make the cost of hyperparameter tuning
ever more prohibitive. This creates a need for algorithms that can tune
themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that
can match the performance of optimally-tuned optimization algorithms up to
polylogarithmic factors given only loose hints on the relevant problem
parameters. We consider in particular algorithms that can match optimally-tuned
Stochastic Gradient Descent (SGD). When the domain of optimization is bounded,
we show tuning-free matching of SGD is possible and achieved by several
existing algorithms. We prove that for the task of minimizing a convex and
smooth or Lipschitz function over an unbounded domain, tuning-free optimization
is impossible. We discuss conditions under which tuning-free optimization is
possible even over unbounded domains. In particular, we show that the recently
proposed DoG and DoWG algorithms are tuning-free when the noise distribution is
sufficiently well-behaved. For the task of finding a stationary point of a
smooth and potentially nonconvex function, we give a variant of SGD that
matches the best-known high-probability convergence rate for tuned SGD at only
an additional polylogarithmic cost. However, we also give an impossibility
result that shows no algorithm can hope to match the optimal expected
convergence rate for tuned SGD with high probability.</div><div><a href='http://arxiv.org/abs/2402.07793v2'>2402.07793v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03126v2")'>How Free is Parameter-Free Stochastic Optimization?</div>
<div id='2402.03126v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T15:51:49Z</div><div>Authors: Amit Attia, Tomer Koren</div><div style='padding-top: 10px; width: 80ex'>We study the problem of parameter-free stochastic optimization, inquiring
whether, and under what conditions, do fully parameter-free methods exist:
these are methods that achieve convergence rates competitive with optimally
tuned methods, without requiring significant knowledge of the true problem
parameters. Existing parameter-free methods can only be considered
``partially'' parameter-free, as they require some non-trivial knowledge of the
true problem parameters, such as a bound on the stochastic gradient norms, a
bound on the distance to a minimizer, etc. In the non-convex setting, we
demonstrate that a simple hyperparameter search technique results in a fully
parameter-free method that outperforms more sophisticated state-of-the-art
algorithms. We also provide a similar result in the convex setting with access
to noisy function values under mild noise assumptions. Finally, assuming only
access to stochastic gradients, we establish a lower bound that renders fully
parameter-free stochastic convex optimization infeasible, and provide a method
which is (partially) parameter-free up to the limit indicated by our lower
bound.</div><div><a href='http://arxiv.org/abs/2402.03126v2'>2402.03126v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10898v2")'>The Price of Adaptivity in Stochastic Convex Optimization</div>
<div id='2402.10898v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T18:56:41Z</div><div>Authors: Yair Carmon, Oliver Hinder</div><div style='padding-top: 10px; width: 80ex'>We prove impossibility results for adaptivity in non-smooth stochastic convex
optimization. Given a set of problem parameters we wish to adapt to, we define
a "price of adaptivity" (PoA) that, roughly speaking, measures the
multiplicative increase in suboptimality due to uncertainty in these
parameters. When the initial distance to the optimum is unknown but a gradient
norm bound is known, we show that the PoA is at least logarithmic for expected
suboptimality, and double-logarithmic for median suboptimality. When there is
uncertainty in both distance and gradient norm, we show that the PoA must be
polynomial in the level of uncertainty. Our lower bounds nearly match existing
upper bounds, and establish that there is no parameter-free lunch.</div><div><a href='http://arxiv.org/abs/2402.10898v2'>2402.10898v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13971v1")'>Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity</div>
<div id='2401.13971v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T06:06:31Z</div><div>Authors: Wenzhi Gao, Qi Deng</div><div style='padding-top: 10px; width: 80ex'>This paper considers stochastic weakly convex optimization without the
standard Lipschitz continuity assumption. Based on new adaptive regularization
(stepsize) strategies, we show that a wide class of stochastic algorithms,
including the stochastic subgradient method, preserve the $\mathcal{O} ( 1 /
\sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on
rather weak assumptions: the Lipschitz parameter can be either bounded by a
general growth function of $\|x\|$ or locally estimated through independent
random samples.</div><div><a href='http://arxiv.org/abs/2401.13971v1'>2401.13971v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08992v1")'>Variance Reduction and Low Sample Complexity in Stochastic Optimization
  via Proximal Point Method</div>
<div id='2402.08992v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T07:34:22Z</div><div>Authors: Jiaming Liang</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a stochastic proximal point method to solve a stochastic
convex composite optimization problem. High probability results in stochastic
optimization typically hinge on restrictive assumptions on the stochastic
gradient noise, for example, sub-Gaussian distributions. Assuming only weak
conditions such as bounded variance of the stochastic gradient, this paper
establishes a low sample complexity to obtain a high probability guarantee on
the convergence of the proposed method. Additionally, a notable aspect of this
work is the development of a subroutine to solve the proximal subproblem, which
also serves as a novel technique for variance reduction.</div><div><a href='http://arxiv.org/abs/2402.08992v1'>2402.08992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12828v1")'>SGD with Clipping is Secretly Estimating the Median Gradient</div>
<div id='2402.12828v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T08:54:07Z</div><div>Authors: Fabian Schaipp, Guillaume Garrigos, Umut Simsekli, Robert Gower</div><div style='padding-top: 10px; width: 80ex'>There are several applications of stochastic optimization where one can
benefit from a robust estimate of the gradient. For example, domains such as
distributed learning with corrupted nodes, the presence of large outliers in
the training data, learning under privacy constraints, or even heavy-tailed
noise due to the dynamics of the algorithm itself. Here we study SGD with
robust gradient estimators based on estimating the median. We first consider
computing the median gradient across samples, and show that the resulting
method can converge even under heavy-tailed, state-dependent noise. We then
derive iterative methods based on the stochastic proximal point method for
computing the geometric median and generalizations thereof. Finally we propose
an algorithm estimating the median gradient across iterations, and find that
several well known methods - in particular different forms of clipping - are
particular cases of this framework.</div><div><a href='http://arxiv.org/abs/2402.12828v1'>2402.12828v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15838v1")'>Distributed Markov Chain Monte Carlo Sampling based on the Alternating
  Direction Method of Multipliers</div>
<div id='2401.15838v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T02:08:40Z</div><div>Authors: Alexandros E. Tzikas, Licio Romao, Mert Pilanci, Alessandro Abate, Mykel J. Kochenderfer</div><div style='padding-top: 10px; width: 80ex'>Many machine learning applications require operating on a spatially
distributed dataset. Despite technological advances, privacy considerations and
communication constraints may prevent gathering the entire dataset in a central
unit. In this paper, we propose a distributed sampling scheme based on the
alternating direction method of multipliers, which is commonly used in the
optimization literature due to its fast convergence. In contrast to distributed
optimization, distributed sampling allows for uncertainty quantification in
Bayesian inference tasks. We provide both theoretical guarantees of our
algorithm's convergence and experimental evidence of its superiority to the
state-of-the-art. For our theoretical results, we use convex optimization tools
to establish a fundamental inequality on the generated local sample iterates.
This inequality enables us to show convergence of the distribution associated
with these iterates to the underlying target distribution in Wasserstein
distance. In simulation, we deploy our algorithm on linear and logistic
regression tasks and illustrate its fast convergence compared to existing
gradient-based methods.</div><div><a href='http://arxiv.org/abs/2401.15838v1'>2401.15838v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14585v1")'>Diffusion Stochastic Optimization for Min-Max Problems</div>
<div id='2401.14585v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T01:16:59Z</div><div>Authors: Haoyuan Cai, Sulaiman A. Alghunaim, Ali H. Sayed</div><div style='padding-top: 10px; width: 80ex'>The optimistic gradient method is useful in addressing minimax optimization
problems. Motivated by the observation that the conventional stochastic version
suffers from the need for a large batch size on the order of
$\mathcal{O}(\varepsilon^{-2})$ to achieve an $\varepsilon$-stationary
solution, we introduce and analyze a new formulation termed Diffusion
Stochastic Same-Sample Optimistic Gradient (DSS-OG). We prove its convergence
and resolve the large batch issue by establishing a tighter upper bound, under
the more general setting of nonconvex Polyak-Lojasiewicz (PL) risk functions.
We also extend the applicability of the proposed method to the distributed
scenario, where agents communicate with their neighbors via a left-stochastic
protocol. To implement DSS-OG, we can query the stochastic gradient oracles in
parallel with some extra memory overhead, resulting in a complexity comparable
to its conventional counterpart. To demonstrate the efficacy of the proposed
algorithm, we conduct tests by training generative adversarial networks.</div><div><a href='http://arxiv.org/abs/2401.14585v1'>2401.14585v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01204v1")'>Stochastic gradient descent for streaming linear and rectified linear
  systems with Massart noise</div>
<div id='2403.01204v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T12:45:01Z</div><div>Authors: Halyun Jeong, Deanna Needell, Elizaveta Rebrova</div><div style='padding-top: 10px; width: 80ex'>We propose SGD-exp, a stochastic gradient descent approach for linear and
ReLU regressions under Massart noise (adversarial semi-random corruption model)
for the fully streaming setting. We show novel nearly linear convergence
guarantees of SGD-exp to the true parameter with up to $50\%$ Massart
corruption rate, and with any corruption rate in the case of symmetric
oblivious corruptions. This is the first convergence guarantee result for
robust ReLU regression in the streaming setting, and it shows the improved
convergence rate over previous robust methods for $L_1$ linear regression due
to a choice of an exponentially decaying step size, known for its efficiency in
practice. Our analysis is based on the drift analysis of a discrete stochastic
process, which could also be interesting on its own.</div><div><a href='http://arxiv.org/abs/2403.01204v1'>2403.01204v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14145v1")'>Multiply Robust Estimation for Local Distribution Shifts with Multiple
  Domains</div>
<div id='2402.14145v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T22:01:10Z</div><div>Authors: Steven Wilkins-Reeves, Xu Chen, Qi Ma, Christine Agarwal, Aude Hofleitner</div><div style='padding-top: 10px; width: 80ex'>Distribution shifts are ubiquitous in real-world machine learning
applications, posing a challenge to the generalization of models trained on one
data distribution to another. We focus on scenarios where data distributions
vary across multiple segments of the entire population and only make local
assumptions about the differences between training and test (deployment)
distributions within each segment. We propose a two-stage multiply robust
estimation method to improve model performance on each individual segment for
tabular data analysis. The method involves fitting a linear combination of the
based models, learned using clusters of training data from multiple segments,
followed by a refinement step for each segment. Our method is designed to be
implemented with commonly used off-the-shelf machine learning models. We
establish theoretical guarantees on the generalization bound of the method on
the test risk. With extensive experiments on synthetic and real datasets, we
demonstrate that the proposed method substantially improves over existing
alternatives in prediction accuracy and robustness on both regression and
classification tasks. We also assess its effectiveness on a user city
prediction dataset from a large technology company.</div><div><a href='http://arxiv.org/abs/2402.14145v1'>2402.14145v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08348v1")'>We don't need no labels: Estimating post-deployment model performance
  under covariate shift without ground truth</div>
<div id='2401.08348v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T13:29:30Z</div><div>Authors: Jakub Białek, Wojtek Kuberski, Nikolaos Perrakis</div><div style='padding-top: 10px; width: 80ex'>The performance of machine learning models often degrades after deployment
due to data distribution shifts. In many use cases, it is impossible to
calculate the post-deployment performance because labels are unavailable or
significantly delayed. Proxy methods for evaluating model performance
stability, like drift detection techniques, do not properly quantify data
distribution shift impact. As a solution, we propose a robust and accurate
performance estimation method for evaluating ML classification models on
unlabeled data that accurately quantifies the impact of covariate shift on
model performance. We call it multi-calibrated confidence-based performance
estimation (M-CBPE). It is model and data-type agnostic and works for any
performance metric. It does not require access to the monitored model - it uses
the model predictions and probability estimates. M-CBPE does not need user
input on the nature of the covariate shift as it fully learns from the data. We
evaluate it with over 600 dataset-model pairs from US census data and compare
it with multiple benchmarks using several evaluation metrics. Results show that
M-CBPE is the best method to estimate the performance of classification models
in any evaluation context.</div><div><a href='http://arxiv.org/abs/2401.08348v1'>2401.08348v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09288v3")'>EcoVal: An Efficient Data Valuation Framework for Machine Learning</div>
<div id='2402.09288v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T16:21:47Z</div><div>Authors: Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Hong Ming Tan, Bowei Chen, Mohan Kankanhalli</div><div style='padding-top: 10px; width: 80ex'>Quantifying the value of data within a machine learning workflow can play a
pivotal role in making more strategic decisions in machine learning
initiatives. The existing Shapley value based frameworks for data valuation in
machine learning are computationally expensive as they require considerable
amount of repeated training of the model to obtain the Shapley value. In this
paper, we introduce an efficient data valuation framework EcoVal, to estimate
the value of data for machine learning models in a fast and practical manner.
Instead of directly working with individual data sample, we determine the value
of a cluster of similar data points. This value is further propagated amongst
all the member cluster points. We show that the overall data value can be
determined by estimating the intrinsic and extrinsic value of each data. This
is enabled by formulating the performance of a model as a \textit{production
function}, a concept which is popularly used to estimate the amount of output
based on factors like labor and capital in a traditional free economic market.
We provide a formal proof of our valuation technique and elucidate the
principles and mechanisms that enable its accelerated performance. We
demonstrate the real-world applicability of our method by showcasing its
effectiveness for both in-distribution and out-of-sample data. This work
addresses one of the core challenges of efficient data valuation at scale in
machine learning models.</div><div><a href='http://arxiv.org/abs/2402.09288v3'>2402.09288v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13893v1")'>Data Acquisition via Experimental Design for Decentralized Data Markets</div>
<div id='2403.13893v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T18:05:52Z</div><div>Authors: Charles Lu, Baihe Huang, Sai Praneeth Karimireddy, Praneeth Vepakomma, Michael Jordan, Ramesh Raskar</div><div style='padding-top: 10px; width: 80ex'>Acquiring high-quality training data is essential for current machine
learning models. Data markets provide a way to increase the supply of data,
particularly in data-scarce domains such as healthcare, by incentivizing
potential data sellers to join the market. A major challenge for a data buyer
in such a market is selecting the most valuable data points from a data seller.
Unlike prior work in data valuation, which assumes centralized data access, we
propose a federated approach to the data selection problem that is inspired by
linear experimental design. Our proposed data selection method achieves lower
prediction error without requiring labeled validation data and can be optimized
in a fast and federated procedure. The key insight of our work is that a method
that directly estimates the benefit of acquiring data for test set prediction
is particularly compatible with a decentralized market setting.</div><div><a href='http://arxiv.org/abs/2403.13893v1'>2403.13893v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.14067v1")'>Automatic Outlier Rectification via Optimal Transport</div>
<div id='2403.14067v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T01:30:24Z</div><div>Authors: Jose Blanchet, Jiajin Li, Markus Pelger, Greg Zanotti</div><div style='padding-top: 10px; width: 80ex'>In this paper, we propose a novel conceptual framework to detect outliers
using optimal transport with a concave cost function. Conventional outlier
detection approaches typically use a two-stage procedure: first, outliers are
detected and removed, and then estimation is performed on the cleaned data.
However, this approach does not inform outlier removal with the estimation
task, leaving room for improvement. To address this limitation, we propose an
automatic outlier rectification mechanism that integrates rectification and
estimation within a joint optimization framework. We take the first step to
utilize an optimal transport distance with a concave cost function to construct
a rectification set in the space of probability distributions. Then, we select
the best distribution within the rectification set to perform the estimation
task. Notably, the concave cost function we introduced in this paper is the key
to making our estimator effectively identify the outlier during the
optimization process. We discuss the fundamental differences between our
estimator and optimal transport-based distributionally robust optimization
estimator. finally, we demonstrate the effectiveness and superiority of our
approach over conventional approaches in extensive simulation and empirical
analyses for mean estimation, least absolute regression, and the fitting of
option implied volatility surfaces.</div><div><a href='http://arxiv.org/abs/2403.14067v1'>2403.14067v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.07723v1")'>On the Last-Iterate Convergence of Shuffling Gradient Methods</div>
<div id='2403.07723v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T15:01:17Z</div><div>Authors: Zijian Liu, Zhengyuan Zhou</div><div style='padding-top: 10px; width: 80ex'>Shuffling gradient methods, which are also known as stochastic gradient
descent (SGD) without replacement, are widely implemented in practice,
particularly including three popular algorithms: Random Reshuffle (RR), Shuffle
Once (SO), and Incremental Gradient (IG). Compared to the empirical success,
the theoretical guarantee of shuffling gradient methods was not
well-understanding for a long time. Until recently, the convergence rates had
just been established for the average iterate for convex functions and the last
iterate for strongly convex problems (using squared distance as the metric).
However, when using the function value gap as the convergence criterion,
existing theories cannot interpret the good performance of the last iterate in
different settings (e.g., constrained optimization). To bridge this gap between
practice and theory, we prove last-iterate convergence rates for shuffling
gradient methods with respect to the objective value even without strong
convexity. Our new results either (nearly) match the existing last-iterate
lower bounds or are as fast as the previous best upper bounds for the average
iterate.</div><div><a href='http://arxiv.org/abs/2403.07723v1'>2403.07723v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04081v1")'>Directional Smoothness and Gradient Methods: Convergence and Adaptivity</div>
<div id='2403.04081v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T22:24:05Z</div><div>Authors: Aaron Mishkin, Ahmed Khaled, Yuanhao Wang, Aaron Defazio, Robert M. Gower</div><div style='padding-top: 10px; width: 80ex'>We develop new sub-optimality bounds for gradient descent (GD) that depend on
the conditioning of the objective along the path of optimization, rather than
on global, worst-case constants. Key to our proofs is directional smoothness, a
measure of gradient variation that we use to develop upper-bounds on the
objective. Minimizing these upper-bounds requires solving implicit equations to
obtain a sequence of strongly adapted step-sizes; we show that these equations
are straightforward to solve for convex quadratics and lead to new guarantees
for two classical step-sizes. For general functions, we prove that the Polyak
step-size and normalized GD obtain fast, path-dependent rates despite using no
knowledge of the directional smoothness. Experiments on logistic regression
show our convergence guarantees are tighter than the classical theory based on
L-smoothness.</div><div><a href='http://arxiv.org/abs/2403.04081v1'>2403.04081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03467v1")'>Stochastic Modified Flows for Riemannian Stochastic Gradient Descent</div>
<div id='2402.03467v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T14:29:38Z</div><div>Authors: Benjamin Gess, Sebastian Kassing, Nimit Rana</div><div style='padding-top: 10px; width: 80ex'>We give quantitative estimates for the rate of convergence of Riemannian
stochastic gradient descent (RSGD) to Riemannian gradient flow and to a
diffusion process, the so-called Riemannian stochastic modified flow (RSMF).
Using tools from stochastic differential geometry we show that, in the small
learning rate regime, RSGD can be approximated by the solution to the RSMF
driven by an infinite-dimensional Wiener process. The RSMF accounts for the
random fluctuations of RSGD and, thereby, increases the order of approximation
compared to the deterministic Riemannian gradient flow. The RSGD is build using
the concept of a retraction map, that is, a cost efficient approximation of the
exponential map, and we prove quantitative bounds for the weak error of the
diffusion approximation under assumptions on the retraction map, the geometry
of the manifold, and the random estimators of the gradient.</div><div><a href='http://arxiv.org/abs/2402.03467v1'>2402.03467v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01382v1")'>Emergence of heavy tails in homogenized stochastic gradient descent</div>
<div id='2402.01382v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:06:33Z</div><div>Authors: Zhe Jiao, Martin Keller-Ressel</div><div style='padding-top: 10px; width: 80ex'>It has repeatedly been observed that loss minimization by stochastic gradient
descent (SGD) leads to heavy-tailed distributions of neural network parameters.
Here, we analyze a continuous diffusion approximation of SGD, called
homogenized stochastic gradient descent, show that it behaves asymptotically
heavy-tailed, and give explicit upper and lower bounds on its tail-index. We
validate these bounds in numerical experiments and show that they are typically
close approximations to the empirical tail-index of SGD iterates. In addition,
their explicit form enables us to quantify the interplay between optimization
parameters and the tail-index. Doing so, we contribute to the ongoing
discussion on links between heavy tails and the generalization performance of
neural networks as well as the ability of SGD to avoid suboptimal local minima.</div><div><a href='http://arxiv.org/abs/2402.01382v1'>2402.01382v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07187v1")'>A Survey on Statistical Theory of Deep Learning: Approximation, Training
  Dynamics, and Generative Models</div>
<div id='2401.07187v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T02:30:19Z</div><div>Authors: Namjoon Suh, Guang Cheng</div><div style='padding-top: 10px; width: 80ex'>In this article, we review the literature on statistical theories of neural
networks from three perspectives. In the first part, results on excess risks
for neural networks are reviewed in the nonparametric framework of regression
or classification. These results rely on explicit constructions of neural
networks, leading to fast convergence rates of excess risks, in that tools from
the approximation theory are adopted. Through these constructions, the width
and depth of the networks can be expressed in terms of sample size, data
dimension, and function smoothness. Nonetheless, their underlying analysis only
applies to the global minimizer in the highly non-convex landscape of deep
neural networks. This motivates us to review the training dynamics of neural
networks in the second part. Specifically, we review papers that attempt to
answer ``how the neural network trained via gradient-based methods finds the
solution that can generalize well on unseen data.'' In particular, two
well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm,
and Mean-Field (MF) paradigm. In the last part, we review the most recent
theoretical advancements in generative models including Generative Adversarial
Networks (GANs), diffusion models, and in-context learning (ICL) in the Large
Language Models (LLMs). The former two models are known to be the main pillars
of the modern generative AI era, while ICL is a strong capability of LLMs in
learning from a few examples in the context. Finally, we conclude the paper by
suggesting several promising directions for deep learning theory.</div><div><a href='http://arxiv.org/abs/2401.07187v1'>2401.07187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04778v1")'>Generative neural networks for characteristic functions</div>
<div id='2401.04778v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T19:07:59Z</div><div>Authors: Florian Brück</div><div style='padding-top: 10px; width: 80ex'>In this work, we provide a simulation algorithm to simulate from a
(multivariate) characteristic function, which is only accessible in a black-box
format. We construct a generative neural network, whose loss function exploits
a specific representation of the Maximum-Mean-Discrepancy metric to directly
incorporate the targeted characteristic function. The construction is universal
in the sense that it is independent of the dimension and that it does not
require any assumptions on the given characteristic function. Furthermore,
finite sample guarantees on the approximation quality in terms of the
Maximum-Mean Discrepancy metric are derived. The method is illustrated in a
short simulation study.</div><div><a href='http://arxiv.org/abs/2401.04778v1'>2401.04778v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15604v3")'>Neural Network-Based Score Estimation in Diffusion Models: Optimization
  and Generalization</div>
<div id='2401.15604v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T08:13:56Z</div><div>Authors: Yinbin Han, Meisam Razaviyayn, Renyuan Xu</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have emerged as a powerful tool rivaling GANs in generating
high-quality samples with improved fidelity, flexibility, and robustness. A key
component of these models is to learn the score function through score
matching. Despite empirical success on various tasks, it remains unclear
whether gradient-based algorithms can learn the score function with a provable
accuracy. As a first step toward answering this question, this paper
establishes a mathematical framework for analyzing score estimation using
neural networks trained by gradient descent. Our analysis covers both the
optimization and the generalization aspects of the learning procedure. In
particular, we propose a parametric form to formulate the denoising
score-matching problem as a regression with noisy labels. Compared to the
standard supervised learning setup, the score-matching problem introduces
distinct challenges, including unbounded input, vector-valued output, and an
additional time variable, preventing existing techniques from being applied
directly. In this paper, we show that with proper designs, the evolution of
neural networks during training can be accurately modeled by a series of kernel
regression tasks. Furthermore, by applying an early-stopping rule for gradient
descent and leveraging recent developments in neural tangent kernels, we
establish the first generalization error (sample complexity) bounds for
learning the score function with neural networks, despite the presence of noise
in the observations. Our analysis is grounded in a novel parametric form of the
neural network and an innovative connection between score matching and
regression analysis, facilitating the application of advanced statistical and
optimization techniques.</div><div><a href='http://arxiv.org/abs/2401.15604v3'>2401.15604v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17641v1")'>Variational Learning is Effective for Large Deep Networks</div>
<div id='2402.17641v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T16:11:05Z</div><div>Authors: Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, Thomas Möllenhoff</div><div style='padding-top: 10px; width: 80ex'>We give extensive empirical evidence against the common belief that
variational learning is ineffective for large neural networks. We show that an
optimizer called Improved Variational Online Newton (IVON) consistently matches
or outperforms Adam for training large networks such as GPT-2 and ResNets from
scratch. IVON's computational costs are nearly identical to Adam but its
predictive uncertainty is better. We show several new use cases of IVON where
we improve fine-tuning and model merging in Large Language Models, accurately
predict generalization error, and faithfully estimate sensitivity to data. We
find overwhelming evidence in support of effectiveness of variational learning.</div><div><a href='http://arxiv.org/abs/2402.17641v1'>2402.17641v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15312v1")'>A Wasserstein perspective of Vanilla GANs</div>
<div id='2403.15312v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T16:04:26Z</div><div>Authors: Lea Kunkel, Mathias Trabs</div><div style='padding-top: 10px; width: 80ex'>The empirical success of Generative Adversarial Networks (GANs) caused an
increasing interest in theoretical research. The statistical literature is
mainly focused on Wasserstein GANs and generalizations thereof, which
especially allow for good dimension reduction properties. Statistical results
for Vanilla GANs, the original optimization problem, are still rather limited
and require assumptions such as smooth activation functions and equal
dimensions of the latent space and the ambient space. To bridge this gap, we
draw a connection from Vanilla GANs to the Wasserstein distance. By doing so,
existing results for Wasserstein GANs can be extended to Vanilla GANs. In
particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein
distance. The assumptions of this oracle inequality are designed to be
satisfied by network architectures commonly used in practice, such as
feedforward ReLU networks. By providing a quantitative result for the
approximation of a Lipschitz function by a feedforward ReLU network with
bounded H\"older norm, we conclude a rate of convergence for Vanilla GANs as
well as Wasserstein GANs as estimators of the unknown probability distribution.</div><div><a href='http://arxiv.org/abs/2403.15312v1'>2403.15312v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15801v1")'>On the Statistical Properties of Generative Adversarial Models for Low
  Intrinsic Data Dimension</div>
<div id='2401.15801v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T23:18:10Z</div><div>Authors: Saptarshi Chakraborty, Peter L. Bartlett</div><div style='padding-top: 10px; width: 80ex'>Despite the remarkable empirical successes of Generative Adversarial Networks
(GANs), the theoretical guarantees for their statistical accuracy remain rather
pessimistic. In particular, the data distributions on which GANs are applied,
such as natural images, are often hypothesized to have an intrinsic
low-dimensional structure in a typically high-dimensional feature space, but
this is often not reflected in the derived rates in the state-of-the-art
analyses. In this paper, we attempt to bridge the gap between the theory and
practice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs),
by deriving statistical guarantees on the estimated densities in terms of the
intrinsic dimension of the data and the latent space. We analytically show that
if one has access to $n$ samples from the unknown target distribution and the
network architectures are properly chosen, the expected Wasserstein-1 distance
of the estimates from the target scales as $O\left( n^{-1/d_\mu } \right)$ for
GANs and $O\left( n^{-1/(d_\mu+\ell)} \right)$ for BiGANs, where $d_\mu$ and
$\ell$ are the upper Wasserstein-1 dimension of the data-distribution and
latent-space dimension, respectively. The theoretical analyses not only suggest
that these methods successfully avoid the curse of dimensionality, in the sense
that the exponent of $n$ in the error rates does not depend on the data
dimension but also serve to bridge the gap between the theoretical analyses of
GANs and the known sharp rates from optimal transport literature. Additionally,
we demonstrate that GANs can effectively achieve the minimax optimal rate even
for non-smooth underlying distributions, with the use of larger generator
networks.</div><div><a href='http://arxiv.org/abs/2401.15801v1'>2401.15801v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15948v1")'>AdvNF: Reducing Mode Collapse in Conditional Normalising Flows using
  Adversarial Learning</div>
<div id='2401.15948v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T08:13:51Z</div><div>Authors: Vikas Kanaujia, Mathias S. Scheurer, Vipul Arora</div><div style='padding-top: 10px; width: 80ex'>Deep generative models complement Markov-chain-Monte-Carlo methods for
efficiently sampling from high-dimensional distributions. Among these methods,
explicit generators, such as Normalising Flows (NFs), in combination with the
Metropolis Hastings algorithm have been extensively applied to get unbiased
samples from target distributions. We systematically study central problems in
conditional NFs, such as high variance, mode collapse and data efficiency. We
propose adversarial training for NFs to ameliorate these problems. Experiments
are conducted with low-dimensional synthetic datasets and XY spin models in two
spatial dimensions.</div><div><a href='http://arxiv.org/abs/2401.15948v1'>2401.15948v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07039v1")'>Quantum Generative Diffusion Model</div>
<div id='2401.07039v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T10:56:34Z</div><div>Authors: Chuangtao Chen, Qinglin Zhao</div><div style='padding-top: 10px; width: 80ex'>This paper introduces the Quantum Generative Diffusion Model (QGDM), a fully
quantum-mechanical model for generating quantum state ensembles, inspired by
Denoising Diffusion Probabilistic Models. QGDM features a diffusion process
that introduces timestep-dependent noise into quantum states, paired with a
denoising mechanism trained to reverse this contamination. This model
efficiently evolves a completely mixed state into a target quantum state
post-training. Our comparative analysis with Quantum Generative Adversarial
Networks demonstrates QGDM's superiority, with fidelity metrics exceeding 0.99
in numerical simulations involving up to 4 qubits. Additionally, we present a
Resource-Efficient version of QGDM (RE-QGDM), which minimizes the need for
auxiliary qubits while maintaining impressive generative capabilities for tasks
involving up to 8 qubits. These results showcase the proposed models' potential
for tackling challenging quantum generation problems.</div><div><a href='http://arxiv.org/abs/2401.07039v1'>2401.07039v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01791v1")'>Variational Quantum Circuits Enhanced Generative Adversarial Network</div>
<div id='2402.01791v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T03:59:35Z</div><div>Authors: Runqiu Shu, Xusheng Xu, Man-Hong Yung, Wei Cui</div><div style='padding-top: 10px; width: 80ex'>Generative adversarial network (GAN) is one of the widely-adopted
machine-learning frameworks for a wide range of applications such as generating
high-quality images, video, and audio contents. However, training a GAN could
become computationally expensive for large neural networks. In this work, we
propose a hybrid quantum-classical architecture for improving GAN (denoted as
QC-GAN). The performance was examed numerically by benchmarking with a
classical GAN using MindSpore Quantum on the task of hand-written image
generation. The generator of the QC-GAN consists of a quantum variational
circuit together with a one-layer neural network, and the discriminator
consists of a traditional neural network. Leveraging the entangling and
expressive power of quantum circuits, our hybrid architecture achieved better
performance (Frechet Inception Distance) than the classical GAN, with much
fewer training parameters and number of iterations for convergence. We have
also demonstrated the superiority of QC-GAN over an alternative quantum GAN,
namely pathGAN, which could hardly generate 16$\times$16 or larger images. This
work demonstrates the value of combining ideas from quantum computing with
machine learning for both areas of Quantum-for-AI and AI-for-Quantum.</div><div><a href='http://arxiv.org/abs/2402.01791v1'>2402.01791v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05295v1")'>Synthesis of pulses from particle detectors with a Generative
  Adversarial Network (GAN)</div>
<div id='2401.05295v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T17:54:16Z</div><div>Authors: Alberto Regadío, Luis Esteban, Sebastián Sánchez-Prieto</div><div style='padding-top: 10px; width: 80ex'>To address the possible lack or total absence of pulses from particle
detectors during the development of its associate electronics, we propose a
model that can generate them without losing the features of the real ones. This
model is based on artificial neural networks, namely Generative Adversarial
Networks (GAN). We describe the proposed network architecture, its training
methodology and the approach to train the GAN with real pulses from a
scintillator receiving radiation from sources of ${}^{137}$Cs and ${}^{22}$Na.
The Generator was installed in a Xilinx's System-On-Chip (SoC). We show how the
network is capable of generating pulses with the same shape as the real ones
that even match the data distributions in the original pulse-height histogram
data.</div><div><a href='http://arxiv.org/abs/2401.05295v1'>2401.05295v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.13891v1")'>Overcoming Saturation in Density Ratio Estimation by Iterated
  Regularization</div>
<div id='2402.13891v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:02:14Z</div><div>Authors: Lukas Gruber, Markus Holzleitner, Johannes Lehner, Sepp Hochreiter, Werner Zellinger</div><div style='padding-top: 10px; width: 80ex'>Estimating the ratio of two probability densities from finitely many samples,
is a central task in machine learning and statistics. In this work, we show
that a large class of kernel methods for density ratio estimation suffers from
error saturation, which prevents algorithms from achieving fast error
convergence rates on highly regular learning problems. To resolve saturation,
we introduce iterated regularization in density ratio estimation to achieve
fast error rates. Our methods outperform its non-iteratively regularized
versions on benchmarks for density ratio estimation as well as on large-scale
evaluations for importance-weighted ensembling of deep unsupervised domain
adaptation models.</div><div><a href='http://arxiv.org/abs/2402.13891v1'>2402.13891v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02579v1")'>Geometric Dynamics of Signal Propagation Predict Trainability of
  Transformers</div>
<div id='2403.02579v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T01:30:34Z</div><div>Authors: Aditya Cowsik, Tamra Nebabu, Xiao-Liang Qi, Surya Ganguli</div><div style='padding-top: 10px; width: 80ex'>We investigate forward signal propagation and gradient back propagation in
deep, randomly initialized transformers, yielding simple necessary and
sufficient conditions on initialization hyperparameters that ensure
trainability of deep transformers. Our approach treats the evolution of the
representations of $n$ tokens as they propagate through the transformer layers
in terms of a discrete time dynamical system of $n$ interacting particles. We
derive simple update equations for the evolving geometry of this particle
system, starting from a permutation symmetric simplex. Our update equations
show that without MLP layers, this system will collapse to a line, consistent
with prior work on rank collapse in transformers. However, unlike prior work,
our evolution equations can quantitatively track particle geometry in the
additional presence of nonlinear MLP layers, and it reveals an order-chaos
phase transition as a function of initialization hyperparameters, like the
strength of attentional and MLP residual connections and weight variances. In
the ordered phase the particles are attractive and collapse to a line, while in
the chaotic phase the particles are repulsive and converge to a regular
$n$-simplex. We analytically derive two Lyapunov exponents: an angle exponent
that governs departures from the edge of chaos in this particle system, and a
gradient exponent that governs the rate of exponential growth or decay of
backpropagated gradients. We show through experiments that, remarkably, the
final test loss at the end of training is well predicted just by these two
exponents at the beginning of training, and that the simultaneous vanishing of
these two exponents yields a simple necessary and sufficient condition to
achieve minimal test loss.</div><div><a href='http://arxiv.org/abs/2403.02579v1'>2403.02579v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.04885v1")'>A Unified Gaussian Process for Branching and Nested Hyperparameter
  Optimization</div>
<div id='2402.04885v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T21:11:32Z</div><div>Authors: Jiazhao Zhang, Ying Hung, Chung-Ching Lin, Zicheng Liu</div><div style='padding-top: 10px; width: 80ex'>Choosing appropriate hyperparameters plays a crucial role in the success of
neural networks as hyper-parameters directly control the behavior and
performance of the training algorithms. To obtain efficient tuning, Bayesian
optimization methods based on Gaussian process (GP) models are widely used.
Despite numerous applications of Bayesian optimization in deep learning, the
existing methodologies are developed based on a convenient but restrictive
assumption that the tuning parameters are independent of each other. However,
tuning parameters with conditional dependence are common in practice. In this
paper, we focus on two types of them: branching and nested parameters. Nested
parameters refer to those tuning parameters that exist only within a particular
setting of another tuning parameter, and a parameter within which other
parameters are nested is called a branching parameter. To capture the
conditional dependence between branching and nested parameters, a unified
Bayesian optimization framework is proposed. The sufficient conditions are
rigorously derived to guarantee the validity of the kernel function, and the
asymptotic convergence of the proposed optimization framework is proven under
the continuum-armed-bandit setting. Based on the new GP model, which accounts
for the dependent structure among input variables through a new kernel
function, higher prediction accuracy and better optimization efficiency are
observed in a series of synthetic simulations and real data applications of
neural networks. Sensitivity analysis is also performed to provide insights
into how changes in hyperparameter values affect prediction accuracy.</div><div><a href='http://arxiv.org/abs/2402.04885v1'>2402.04885v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12241v1")'>Convergence of Gradient Descent for Recurrent Neural Networks: A
  Nonasymptotic Analysis</div>
<div id='2402.12241v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T15:56:43Z</div><div>Authors: Semih Cayci, Atilla Eryilmaz</div><div style='padding-top: 10px; width: 80ex'>We analyze recurrent neural networks trained with gradient descent in the
supervised learning setting for dynamical systems, and prove that gradient
descent can achieve optimality \emph{without} massive overparameterization. Our
in-depth nonasymptotic analysis (i) provides sharp bounds on the network size
$m$ and iteration complexity $\tau$ in terms of the sequence length $T$, sample
size $n$ and ambient dimension $d$, and (ii) identifies the significant impact
of long-term dependencies in the dynamical system on the convergence and
network width bounds characterized by a cutoff point that depends on the
Lipschitz continuity of the activation function. Remarkably, this analysis
reveals that an appropriately-initialized recurrent neural network trained with
$n$ samples can achieve optimality with a network size $m$ that scales only
logarithmically with $n$. This sharply contrasts with the prior works that
require high-order polynomial dependency of $m$ on $n$ to establish strong
regularity conditions. Our results are based on an explicit characterization of
the class of dynamical systems that can be approximated and learned by
recurrent neural networks via norm-constrained transportation mappings, and
establishing local smoothness properties of the hidden state with respect to
the learnable parameters.</div><div><a href='http://arxiv.org/abs/2402.12241v1'>2402.12241v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09889v1")'>Generalization of Scaled Deep ResNets in the Mean-Field Regime</div>
<div id='2403.09889v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T21:48:00Z</div><div>Authors: Yihang Chen, Fanghui Liu, Yiping Lu, Grigorios G. Chrysos, Volkan Cevher</div><div style='padding-top: 10px; width: 80ex'>Despite the widespread empirical success of ResNet, the generalization
properties of deep ResNet are rarely explored beyond the lazy training regime.
In this work, we investigate \emph{scaled} ResNet in the limit of infinitely
deep and wide neural networks, of which the gradient flow is described by a
partial differential equation in the large-neural network limit, i.e., the
\emph{mean-field} regime. To derive the generalization bounds under this
setting, our analysis necessitates a shift from the conventional time-invariant
Gram matrix employed in the lazy training regime to a time-variant,
distribution-dependent version. To this end, we provide a global lower bound on
the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides,
for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we
establish the linear convergence of the empirical error and estimate the upper
bound of the KL divergence over parameters distribution. Finally, we build the
uniform convergence for generalization bound via Rademacher complexity. Our
results offer new insights into the generalization ability of deep ResNet
beyond the lazy training regime and contribute to advancing the understanding
of the fundamental properties of deep neural networks.</div><div><a href='http://arxiv.org/abs/2403.09889v1'>2403.09889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12887v1")'>Understanding the training of infinitely deep and wide ResNets with
  Conditional Optimal Transport</div>
<div id='2403.12887v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T16:34:31Z</div><div>Authors: Raphaël Barboni, Gabriel Peyré, François-Xavier Vialard</div><div style='padding-top: 10px; width: 80ex'>We study the convergence of gradient flow for the training of deep neural
networks. If Residual Neural Networks are a popular example of very deep
architectures, their training constitutes a challenging optimization problem
due notably to the non-convexity and the non-coercivity of the objective. Yet,
in applications, those tasks are successfully solved by simple optimization
algorithms such as gradient descent. To better understand this phenomenon, we
focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide
ResNet, parameterized by probability measures over the product set of layers
and parameters and with constant marginal on the set of layers. Indeed, in the
case of shallow neural networks, mean field models have proven to benefit from
simplified loss-landscapes and good theoretical guarantees when trained with
gradient flow for the Wasserstein metric on the set of probability measures.
Motivated by this approach, we propose to train our model with gradient flow
w.r.t. the conditional Optimal Transport distance: a restriction of the
classical Wasserstein distance which enforces our marginal condition. Relying
on the theory of gradient flows in metric spaces we first show the
well-posedness of the gradient flow equation and its consistency with the
training of ResNets at finite width. Performing a local Polyak-\L{}ojasiewicz
analysis, we then show convergence of the gradient flow for well-chosen
initializations: if the number of features is finite but sufficiently large and
the risk is sufficiently small at initialization, the gradient flow converges
towards a global minimizer. This is the first result of this type for
infinitely deep and arbitrarily wide ResNets.</div><div><a href='http://arxiv.org/abs/2403.12887v1'>2403.12887v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13794v1")'>Revisiting Convergence of AdaGrad with Relaxed Assumptions</div>
<div id='2402.13794v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T13:24:14Z</div><div>Authors: Yusu Hong, Junhong Lin</div><div style='padding-top: 10px; width: 80ex'>In this study, we revisit the convergence of AdaGrad with momentum (covering
AdaGrad as a special case) on non-convex smooth optimization problems. We
consider a general noise model where the noise magnitude is controlled by the
function value gap together with the gradient magnitude. This model encompasses
a broad range of noises including bounded noise, sub-Gaussian noise, affine
variance noise and the expected smoothness, and it has been shown to be more
realistic in many practical applications. Our analysis yields a probabilistic
convergence rate which, under the general noise, could reach at
(\tilde{\mathcal{O}}(1/\sqrt{T})). This rate does not rely on prior knowledge
of problem-parameters and could accelerate to (\tilde{\mathcal{O}}(1/T)) where
(T) denotes the total number iterations, when the noise parameters related to
the function value gap and noise level are sufficiently small. The convergence
rate thus matches the lower rate for stochastic first-order methods over
non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. We
further derive a convergence bound for AdaGrad with mometum, considering the
generalized smoothness where the local smoothness is controlled by a
first-order function of the gradient norm.</div><div><a href='http://arxiv.org/abs/2402.13794v1'>2402.13794v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13868v1")'>Analysing heavy-tail properties of Stochastic Gradient Descent by means
  of Stochastic Recurrence Equations</div>
<div id='2403.13868v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T13:39:19Z</div><div>Authors: Ewa Damek, Sebastian Mentemeier</div><div style='padding-top: 10px; width: 80ex'>In recent works on the theory of machine learning, it has been observed that
heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in
the probabilistic framework of stochastic recursions. In particular,
G\"{u}rb\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup
corresponding to linear regression for which iterations of SGD can be modelled
by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for
independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a
random symmetric matrix and $B_k$ is a random vector. In this work, we will
answer several open questions of the quoted paper and extend their results by
applying the theory of irreducible-proximal (i-p) matrices.</div><div><a href='http://arxiv.org/abs/2403.13868v1'>2403.13868v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15890v1")'>Probabilistic Guarantees of Stochastic Recursive Gradient in Non-Convex
  Finite Sum Problems</div>
<div id='2401.15890v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T05:05:03Z</div><div>Authors: Yanjie Zhong, Jiaqi Li, Soumendra Lahiri</div><div style='padding-top: 10px; width: 80ex'>This paper develops a new dimension-free Azuma-Hoeffding type bound on
summation norm of a martingale difference sequence with random individual
bounds. With this novel result, we provide high-probability bounds for the
gradient norm estimator in the proposed algorithm Prob-SARAH, which is a
modified version of the StochAstic Recursive grAdient algoritHm (SARAH), a
state-of-art variance reduced algorithm that achieves optimal computational
complexity in expectation for the finite sum problem. The in-probability
complexity by Prob-SARAH matches the best in-expectation result up to
logarithmic factors. Empirical experiments demonstrate the superior
probabilistic performance of Prob-SARAH on real datasets compared to other
popular algorithms.</div><div><a href='http://arxiv.org/abs/2401.15890v1'>2401.15890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11752v2")'>Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models
  via Reparameterisation and Smoothing</div>
<div id='2402.11752v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T00:43:22Z</div><div>Authors: Dominik Wagner, Basim Khajwal, C. -H. Luke Ong</div><div style='padding-top: 10px; width: 80ex'>It is well-known that the reparameterisation gradient estimator, which
exhibits low variance in practice, is biased for non-differentiable models.
This may compromise correctness of gradient-based optimisation methods such as
stochastic gradient descent (SGD). We introduce a simple syntactic framework to
define non-differentiable functions piecewisely and present a systematic
approach to obtain smoothings for which the reparameterisation gradient
estimator is unbiased. Our main contribution is a novel variant of SGD,
Diagonalisation Stochastic Gradient Descent, which progressively enhances the
accuracy of the smoothed approximation during optimisation, and we prove
convergence to stationary points of the unsmoothed (original) objective. Our
empirical evaluation reveals benefits over the state of the art: our approach
is simple, fast, stable and attains orders of magnitude reduction in
work-normalised variance.</div><div><a href='http://arxiv.org/abs/2402.11752v2'>2402.11752v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07156v1")'>A hybrid iterative method based on MIONet for PDEs: Theory and numerical
  examples</div>
<div id='2402.07156v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T11:02:25Z</div><div>Authors: Jun Hu, Pengzhan Jin</div><div style='padding-top: 10px; width: 80ex'>We propose a hybrid iterative method based on MIONet for PDEs, which combines
the traditional numerical iterative solver and the recent powerful machine
learning method of neural operator, and further systematically analyze its
theoretical properties, including the convergence condition, the spectral
behavior, as well as the convergence rate, in terms of the errors of the
discretization and the model inference. We show the theoretical results for the
frequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel. We
give an upper bound of the convergence rate of the hybrid method w.r.t. the
model correction period, which indicates a minimum point to make the hybrid
iteration converge fastest. Several numerical examples including the hybrid
Richardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are
presented to verify our theoretical results, and also reflect an excellent
acceleration effect. As a meshless acceleration method, it is provided with
enormous potentials for practice applications.</div><div><a href='http://arxiv.org/abs/2402.07156v1'>2402.07156v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07465v1")'>Score-Based Physics-Informed Neural Networks for High-Dimensional
  Fokker-Planck Equations</div>
<div id='2402.07465v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T07:59:25Z</div><div>Authors: Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, Kenji Kawaguchi</div><div style='padding-top: 10px; width: 80ex'>The Fokker-Planck (FP) equation is a foundational PDE in stochastic
processes. However, curse of dimensionality (CoD) poses challenge when dealing
with high-dimensional FP PDEs. Although Monte Carlo and vanilla
Physics-Informed Neural Networks (PINNs) have shown the potential to tackle
CoD, both methods exhibit numerical errors in high dimensions when dealing with
the probability density function (PDF) associated with Brownian motion. The
point-wise PDF values tend to decrease exponentially as dimension increases,
surpassing the precision of numerical simulations and resulting in substantial
errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast
sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms
the FP equation into a difficult HJB equation, whose error grows rapidly with
dimension. To this end, we propose a novel approach utilizing a score-based
solver to fit the score function in SDEs. The score function, defined as the
gradient of the LL, plays a fundamental role in inferring LL and PDF and
enables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced
SM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver
operates in two stages: first, employing SM, SSM, or Score-PINN to acquire the
score; and second, solving the LL via an ODE using the obtained score.
Comparative evaluations across these methods showcase varying trade-offs. The
proposed method is evaluated across diverse SDEs, including anisotropic OU
processes, geometric Brownian, and Brownian with varying eigenspace. We also
test various distributions, including Gaussian, Log-normal, Laplace, and
Cauchy. The numerical results demonstrate the score-based SDE solver's
stability, speed, and performance across different settings, solidifying its
potential as a solution to CoD for high-dimensional FP equations.</div><div><a href='http://arxiv.org/abs/2402.07465v1'>2402.07465v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13810v1")'>The Expected Loss of Preconditioned Langevin Dynamics Reveals the
  Hessian Rank</div>
<div id='2402.13810v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T13:47:51Z</div><div>Authors: Amitay Bar, Rotem Mulayoff, Tomer Michaeli, Ronen Talmon</div><div style='padding-top: 10px; width: 80ex'>Langevin dynamics (LD) is widely used for sampling from distributions and for
optimization. In this work, we derive a closed-form expression for the expected
loss of preconditioned LD near stationary points of the objective function. We
use the fact that at the vicinity of such points, LD reduces to an
Ornstein-Uhlenbeck process, which is amenable to convenient mathematical
treatment. Our analysis reveals that when the preconditioning matrix satisfies
a particular relation with respect to the noise covariance, LD's expected loss
becomes proportional to the rank of the objective's Hessian. We illustrate the
applicability of this result in the context of neural networks, where the
Hessian rank has been shown to capture the complexity of the predictor function
but is usually computationally hard to probe. Finally, we use our analysis to
compare SGD-like and Adam-like preconditioners and identify the regimes under
which each of them leads to a lower expected loss.</div><div><a href='http://arxiv.org/abs/2402.13810v1'>2402.13810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09598v1")'>MCMC-driven learning</div>
<div id='2402.09598v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T22:10:42Z</div><div>Authors: Alexandre Bouchard-Côté, Trevor Campbell, Geoff Pleiss, Nikola Surjanovic</div><div style='padding-top: 10px; width: 80ex'>This paper is intended to appear as a chapter for the Handbook of Markov
Chain Monte Carlo. The goal of this chapter is to unify various problems at the
intersection of Markov chain Monte Carlo (MCMC) and machine
learning$\unicode{x2014}$which includes black-box variational inference,
adaptive MCMC, normalizing flow construction and transport-assisted MCMC,
surrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov
chain gradient descent, Markovian score climbing, and
more$\unicode{x2014}$within one common framework. By doing so, the theory and
methods developed for each may be translated and generalized.</div><div><a href='http://arxiv.org/abs/2402.09598v1'>2402.09598v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17890v1")'>From Inverse Optimization to Feasibility to ERM</div>
<div id='2402.17890v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:06:42Z</div><div>Authors: Saurabh Mishra, Anant Raj, Sharan Vaswani</div><div style='padding-top: 10px; width: 80ex'>Inverse optimization involves inferring unknown parameters of an optimization
problem from known solutions, and is widely used in fields such as
transportation, power systems and healthcare. We study the contextual inverse
optimization setting that utilizes additional contextual information to better
predict the unknown problem parameters. We focus on contextual inverse linear
programming (CILP), addressing the challenges posed by the non-differentiable
nature of LPs. For a linear prediction model, we reduce CILP to a convex
feasibility problem allowing the use of standard algorithms such as alternating
projections. The resulting algorithm for CILP is equipped with a linear
convergence guarantee without additional assumptions such as degeneracy or
interpolation. Next, we reduce CILP to empirical risk minimization (ERM) on a
smooth, convex loss that satisfies the Polyak-Lojasiewicz condition. This
reduction enables the use of scalable first-order optimization methods to solve
large non-convex problems, while maintaining theoretical guarantees in the
convex setting. Finally, we experimentally validate our approach on both
synthetic and real-world problems, and demonstrate improved performance
compared to existing methods.</div><div><a href='http://arxiv.org/abs/2402.17890v1'>2402.17890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01799v1")'>Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward</div>
<div id='2402.01799v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T06:29:34Z</div><div>Authors: Arnav Chavan, Raghav Magazine, Shubham Kushwaha, Mérouane Debbah, Deepak Gupta</div><div style='padding-top: 10px; width: 80ex'>Despite the impressive performance of LLMs, their widespread adoption faces
challenges due to substantial computational and memory requirements during
inference. Recent advancements in model compression and system-level
optimization methods aim to enhance LLM inference. This survey offers an
overview of these methods, emphasizing recent developments. Through experiments
on LLaMA(/2)-7B, we evaluate various compression techniques, providing
practical insights for efficient LLM deployment in a unified setting. The
empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these
methods. Drawing from survey insights, we identify current limitations and
discuss potential future directions to improve LLM inference efficiency. We
release the codebase to reproduce the results presented in this paper at
https://github.com/nyunAI/Faster-LLM-Survey</div><div><a href='http://arxiv.org/abs/2402.01799v1'>2402.01799v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01238v1")'>Flexible Variational Information Bottleneck: Achieving Diverse
  Compression with a Single Training</div>
<div id='2402.01238v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:03:38Z</div><div>Authors: Sota Kudo, Naoaki Ono, Shigehiko Kanaya, Ming Huang</div><div style='padding-top: 10px; width: 80ex'>Information Bottleneck (IB) is a widely used framework that enables the
extraction of information related to a target random variable from a source
random variable. In the objective function, IB controls the trade-off between
data compression and predictiveness through the Lagrange multiplier $\beta$.
Traditionally, to find the trade-off to be learned, IB requires a search for
$\beta$ through multiple training cycles, which is computationally expensive.
In this study, we introduce Flexible Variational Information Bottleneck (FVIB),
an innovative framework for classification task that can obtain optimal models
for all values of $\beta$ with single, computationally efficient training. We
theoretically demonstrate that across all values of reasonable $\beta$, FVIB
can simultaneously maximize an approximation of the objective function for
Variational Information Bottleneck (VIB), the conventional IB method. Then we
empirically show that FVIB can learn the VIB objective as effectively as VIB.
Furthermore, in terms of calibration performance, FVIB outperforms other IB and
calibration methods by enabling continuous optimization of $\beta$. Our codes
are available at https://github.com/sotakudo/fvib.</div><div><a href='http://arxiv.org/abs/2402.01238v1'>2402.01238v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07148v1")'>Stochastic Extragradient with Random Reshuffling: Improved Convergence
  for Variational Inequalities</div>
<div id='2403.07148v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T20:35:52Z</div><div>Authors: Konstantinos Emmanouilidis, René Vidal, Nicolas Loizou</div><div style='padding-top: 10px; width: 80ex'>The Stochastic Extragradient (SEG) method is one of the most popular
algorithms for solving finite-sum min-max optimization and variational
inequality problems (VIPs) appearing in various machine learning tasks.
However, existing convergence analyses of SEG focus on its with-replacement
variants, while practical implementations of the method randomly reshuffle
components and sequentially use them. Unlike the well-studied with-replacement
variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical
guarantees. In this work, we provide a convergence analysis of SEG-RR for three
classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We
derive conditions under which SEG-RR achieves a faster convergence rate than
the uniform with-replacement sampling SEG. In the monotone setting, our
analysis of SEG-RR guarantees convergence to an arbitrary accuracy without
large batch sizes, a strong requirement needed in the classical
with-replacement SEG. As a byproduct of our results, we provide convergence
guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the
algorithm) and the Incremental Extragradient (does not shuffle the data). We
supplement our analysis with experiments validating empirically the superior
performance of SEG-RR over the classical with-replacement sampling SEG.</div><div><a href='http://arxiv.org/abs/2403.07148v1'>2403.07148v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10763v1")'>A Primal-Dual Algorithm for Faster Distributionally Robust Optimization</div>
<div id='2403.10763v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T02:06:14Z</div><div>Authors: Ronak Mehta, Jelena Diakonikolas, Zaid Harchaoui</div><div style='padding-top: 10px; width: 80ex'>We consider the penalized distributionally robust optimization (DRO) problem
with a closed, convex uncertainty set, a setting that encompasses the $f$-DRO,
Wasserstein-DRO, and spectral/$L$-risk formulations used in practice. We
present Drago, a stochastic primal-dual algorithm that achieves a
state-of-the-art linear convergence rate on strongly convex-strongly concave
DRO problems. The method combines both randomized and cyclic components with
mini-batching, which effectively handles the unique asymmetric nature of the
primal and dual problems in DRO. We support our theoretical results with
numerical benchmarks in classification and regression.</div><div><a href='http://arxiv.org/abs/2403.10763v1'>2403.10763v1</a></div>
</div></div>
    <div><a href="arxiv_0.html">Prev (0)</a></div>
    <div><a href="arxiv_2.html">Next (2)</a></div>
    