
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02019v1")'>From Function to Distribution Modeling: A PAC-Generative Approach to
  Offline Optimization</div>
<div id='2401.02019v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T01:32:50Z</div><div>Authors: Qiang Zhang, Ruida Zhou, Yang Shen, Tie Liu</div><div style='padding-top: 10px; width: 80ex'>This paper considers the problem of offline optimization, where the objective
function is unknown except for a collection of ``offline" data examples. While
recent years have seen a flurry of work on applying various machine learning
techniques to the offline optimization problem, the majority of these work
focused on learning a surrogate of the unknown objective function and then
applying existing optimization algorithms. While the idea of modeling the
unknown objective function is intuitive and appealing, from the learning point
of view it also makes it very difficult to tune the objective of the learner
according to the objective of optimization. Instead of learning and then
optimizing the unknown objective function, in this paper we take on a less
intuitive but more direct view that optimization can be thought of as a process
of sampling from a generative model. To learn an effective generative model
from the offline data examples, we consider the standard technique of
``re-weighting", and our main technical contribution is a probably
approximately correct (PAC) lower bound on the natural optimization objective,
which allows us to jointly learn a weight function and a score-based generative
model. The robustly competitive performance of the proposed approach is
demonstrated via empirical studies using the standard offline optimization
benchmarks.</div><div><a href='http://arxiv.org/abs/2401.02019v1'>2401.02019v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14987v1")'>On the Performance of Empirical Risk Minimization with Smoothed Data</div>
<div id='2402.14987v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T21:55:41Z</div><div>Authors: Adam Block, Alexander Rakhlin, Abhishek Shetty</div><div style='padding-top: 10px; width: 80ex'>In order to circumvent statistical and computational hardness results in
sequential decision-making, recent work has considered smoothed online
learning, where the distribution of data at each time is assumed to have
bounded likeliehood ratio with respect to a base measure when conditioned on
the history. While previous works have demonstrated the benefits of smoothness,
they have either assumed that the base measure is known to the learner or have
presented computationally inefficient algorithms applying only in special
cases. This work investigates the more general setting where the base measure
is \emph{unknown} to the learner, focusing in particular on the performance of
Empirical Risk Minimization (ERM) with square loss when the data are
well-specified and smooth. We show that in this setting, ERM is able to achieve
sublinear error whenever a class is learnable with iid data; in particular, ERM
achieves error scaling as $\tilde O( \sqrt{\mathrm{comp}(\mathcal F)\cdot T}
)$, where $\mathrm{comp}(\mathcal F)$ is the statistical complexity of learning
$\mathcal F$ with iid data. In so doing, we prove a novel norm comparison bound
for smoothed data that comprises the first sharp norm comparison for dependent
data applying to arbitrary, nonlinear function classes. We complement these
results with a lower bound indicating that our analysis of ERM is essentially
tight, establishing a separation in the performance of ERM between smoothed and
iid data.</div><div><a href='http://arxiv.org/abs/2402.14987v1'>2402.14987v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01146v1")'>Limited Memory Online Gradient Descent for Kernelized Pairwise Learning
  with Dynamic Averaging</div>
<div id='2402.01146v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T05:21:50Z</div><div>Authors: Hilal AlQuabeh, William de Vazelhes, Bin Gu</div><div style='padding-top: 10px; width: 80ex'>Pairwise learning, an important domain within machine learning, addresses
loss functions defined on pairs of training examples, including those in metric
learning and AUC maximization. Acknowledging the quadratic growth in
computation complexity accompanying pairwise loss as the sample size grows,
researchers have turned to online gradient descent (OGD) methods for enhanced
scalability. Recently, an OGD algorithm emerged, employing gradient computation
involving prior and most recent examples, a step that effectively reduces
algorithmic complexity to $O(T)$, with $T$ being the number of received
examples. This approach, however, confines itself to linear models while
assuming the independence of example arrivals. We introduce a lightweight OGD
algorithm that does not require the independence of examples and generalizes to
kernel pairwise learning. Our algorithm builds the gradient based on a random
example and a moving average representing the past data, which results in a
sub-linear regret bound with a complexity of $O(T)$. Furthermore, through the
integration of $O(\sqrt{T}{\log{T}})$ random Fourier features, the complexity
of kernel calculations is effectively minimized. Several experiments with
real-world datasets show that the proposed technique outperforms kernel and
linear algorithms in offline and online scenarios.</div><div><a href='http://arxiv.org/abs/2402.01146v1'>2402.01146v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09327v1")'>Information Complexity of Stochastic Convex Optimization: Applications
  to Generalization and Memorization</div>
<div id='2402.09327v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T17:17:30Z</div><div>Authors: Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy</div><div style='padding-top: 10px; width: 80ex'>In this work, we investigate the interplay between memorization and learning
in the context of \emph{stochastic convex optimization} (SCO). We define
memorization via the information a learning algorithm reveals about its
training data points. We then quantify this information using the framework of
conditional mutual information (CMI) proposed by Steinke and Zakynthinou
(2020). Our main result is a precise characterization of the tradeoff between
the accuracy of a learning algorithm and its CMI, answering an open question
posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting
and under strong convexity, every learner with an excess error $\varepsilon$
has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,
respectively. We further demonstrate the essential role of memorization in
learning problems in SCO by designing an adversary capable of accurately
identifying a significant fraction of the training samples in specific SCO
problems. Finally, we enumerate several implications of our results, such as a
limitation of generalization bounds based on CMI and the incompressibility of
samples in SCO problems.</div><div><a href='http://arxiv.org/abs/2402.09327v1'>2402.09327v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13285v1")'>Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization
  Bounds with Complexity Measures</div>
<div id='2402.13285v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T10:15:11Z</div><div>Authors: Paul Viallard, Rémi Emonet, Amaury Habrard, Emilie Morvant, Valentina Zantedeschi</div><div style='padding-top: 10px; width: 80ex'>In statistical learning theory, a generalization bound usually involves a
complexity measure imposed by the considered theoretical framework. This limits
the scope of such bounds, as other forms of capacity measures or
regularizations are used in algorithms. In this paper, we leverage the
framework of disintegrated PAC-Bayes bounds to derive a general generalization
bound instantiable with arbitrary complexity measures. One trick to prove such
a result involves considering a commonly used family of distributions: the
Gibbs distributions. Our bound stands in probability jointly over the
hypothesis and the learning sample, which allows the complexity to be adapted
to the generalization gap as it can be customized to fit both the hypothesis
class and the task.</div><div><a href='http://arxiv.org/abs/2402.13285v1'>2402.13285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00957v2")'>Credal Learning Theory</div>
<div id='2402.00957v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T19:25:58Z</div><div>Authors: Michele Caprio, Maryam Sultana, Eleni Elia, Fabio Cuzzolin</div><div style='padding-top: 10px; width: 80ex'>Statistical learning theory is the foundation of machine learning, providing
theoretical bounds for the risk of models learnt from a (single) training set,
assumed to issue from an unknown probability distribution. In actual
deployment, however, the data distribution may (and often does) vary, causing
domain adaptation/generalization issues. In this paper we lay the foundations
for a `credal' theory of learning, using convex sets of probabilities (credal
sets) to model the variability in the data-generating distribution. Such credal
sets, we argue, may be inferred from a finite sample of training sets. Bounds
are derived for the case of finite hypotheses spaces (both assuming
realizability or not) as well as infinite model spaces, which directly
generalize classical results.</div><div><a href='http://arxiv.org/abs/2402.00957v2'>2402.00957v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14759v1")'>Generalising realisability in statistical learning theory under
  epistemic uncertainty</div>
<div id='2402.14759v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:20:25Z</div><div>Authors: Fabio Cuzzolin</div><div style='padding-top: 10px; width: 80ex'>The purpose of this paper is to look into how central notions in statistical
learning theory, such as realisability, generalise under the assumption that
train and test distribution are issued from the same credal set, i.e., a convex
set of probability distributions. This can be considered as a first step
towards a more general treatment of statistical learning under epistemic
uncertainty.</div><div><a href='http://arxiv.org/abs/2402.14759v1'>2402.14759v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14822v1")'>Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets</div>
<div id='2403.14822v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T20:29:43Z</div><div>Authors: Jie Wang, Rui Gao, Yao Xie</div><div style='padding-top: 10px; width: 80ex'>We present a new framework to address the non-convex robust hypothesis
testing problem, wherein the goal is to seek the optimal detector that
minimizes the maximum of worst-case type-I and type-II risk functions. The
distributional uncertainty sets are constructed to center around the empirical
distribution derived from samples based on Sinkhorn discrepancy. Given that the
objective involves non-convex, non-smooth probabilistic functions that are
often intractable to optimize, existing methods resort to approximations rather
than exact solutions. To tackle the challenge, we introduce an exact
mixed-integer exponential conic reformulation of the problem, which can be
solved into a global optimum with a moderate amount of input data.
Subsequently, we propose a convex approximation, demonstrating its superiority
over current state-of-the-art methodologies in literature. Furthermore, we
establish connections between robust hypothesis testing and regularized
formulations of non-robust risk functions, offering insightful interpretations.
Our numerical study highlights the satisfactory testing performance and
computational efficiency of the proposed framework.</div><div><a href='http://arxiv.org/abs/2403.14822v1'>2403.14822v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13366v1")'>Statistical curriculum learning: An elimination algorithm achieving an
  oracle risk</div>
<div id='2402.13366v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T20:44:40Z</div><div>Authors: Omer Cohen, Ron Meir, Nir Weinberger</div><div style='padding-top: 10px; width: 80ex'>We consider a statistical version of curriculum learning (CL) in a parametric
prediction setting. The learner is required to estimate a target parameter
vector, and can adaptively collect samples from either the target model, or
other source models that are similar to the target model, but less noisy. We
consider three types of learners, depending on the level of side-information
they receive. The first two, referred to as strong/weak-oracle learners,
receive high/low degrees of information about the models, and use these to
learn. The third, a fully adaptive learner, estimates the target parameter
vector without any prior information. In the single source case, we propose an
elimination learning method, whose risk matches that of a strong-oracle
learner. In the multiple source case, we advocate that the risk of the
weak-oracle learner is a realistic benchmark for the risk of adaptive learners.
We develop an adaptive multiple elimination-rounds CL algorithm, and
characterize instance-dependent conditions for its risk to match that of the
weak-oracle learner. We consider instance-dependent minimax lower bounds, and
discuss the challenges associated with defining the class of instances for the
bound. We derive two minimax lower bounds, and determine the conditions under
which the performance weak-oracle learner is minimax optimal.</div><div><a href='http://arxiv.org/abs/2402.13366v1'>2402.13366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01148v3")'>PAC-Bayes-Chernoff bounds for unbounded losses</div>
<div id='2401.01148v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T10:58:54Z</div><div>Authors: Ioar Casado, Luis A. Ortega, Andrés R. Masegosa, Aritz Pérez</div><div style='padding-top: 10px; width: 80ex'>We introduce a new PAC-Bayes oracle bound for unbounded losses. This result
can be understood as a PAC-Bayesian version of the Cram\'er-Chernoff bound. The
proof technique relies on controlling the tails of certain random variables
involving the Cram\'er transform of the loss. We highlight several applications
of the main theorem. First, we show that our result naturally allows exact
optimization of the free parameter on many PAC-Bayes bounds. Second, we recover
and generalize previous results. Finally, we show that our approach allows
working with richer assumptions that result in more informative and potentially
tighter bounds. In this direction, we provide a general bound under a new
``model-dependent bounded CGF" assumption from which we obtain bounds based on
parameter norms and log-Sobolev inequalities. All these bounds can be minimized
to obtain novel posteriors.</div><div><a href='http://arxiv.org/abs/2401.01148v3'>2401.01148v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06873v1")'>Last Iterate Convergence of Incremental Methods and Applications in
  Continual Learning</div>
<div id='2403.06873v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:24:26Z</div><div>Authors: Xufeng Cai, Jelena Diakonikolas</div><div style='padding-top: 10px; width: 80ex'>Incremental gradient methods and incremental proximal methods are a
fundamental class of optimization algorithms used for solving finite sum
problems, broadly studied in the literature. Yet, when it comes to their
convergence guarantees, nonasymptotic (first-order or proximal) oracle
complexity bounds have been obtained fairly recently, almost exclusively
applying to the average iterate. Motivated by applications in continual
learning, we obtain the first convergence guarantees for the last iterate of
both incremental gradient and incremental proximal methods, in general convex
smooth (for both) and convex Lipschitz (for the proximal variants) settings.
Our oracle complexity bounds for the last iterate nearly match (i.e., match up
to a square-root-log or a log factor) the best known oracle complexity bounds
for the average iterate, for both classes of methods. We further obtain
generalizations of our results to weighted averaging of the iterates with
increasing weights, which can be seen as interpolating between the last iterate
and the average iterate guarantees. Additionally, we discuss how our results
can be generalized to variants of studied incremental methods with permuted
ordering of updates. Our results generalize last iterate guarantees for
incremental methods compared to state of the art, as such results were
previously known only for overparameterized linear models, which correspond to
convex quadratic problems with infinitely many solutions.</div><div><a href='http://arxiv.org/abs/2403.06873v1'>2403.06873v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03362v1")'>Level Set Teleportation: An Optimization Perspective</div>
<div id='2403.03362v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T23:16:13Z</div><div>Authors: Aaron Mishkin, Alberto Bietti, Robert M. Gower</div><div style='padding-top: 10px; width: 80ex'>We study level set teleportation, an optimization sub-routine which seeks to
accelerate gradient methods by maximizing the gradient norm on a level-set of
the objective function. Since the descent lemma implies that gradient descent
(GD) decreases the objective proportional to the squared norm of the gradient,
level-set teleportation maximizes this one-step progress guarantee. For convex
functions satisfying Hessian stability, we prove that GD with level-set
teleportation obtains a combined sub-linear/linear convergence rate which is
strictly faster than standard GD when the optimality gap is small. This is in
sharp contrast to the standard (strongly) convex setting, where we show
level-set teleportation neither improves nor worsens convergence rates. To
evaluate teleportation in practice, we develop a projected-gradient-type method
requiring only Hessian-vector products. We use this method to show that
gradient methods with access to a teleportation oracle uniformly out-perform
their standard versions on a variety of learning problems.</div><div><a href='http://arxiv.org/abs/2403.03362v1'>2403.03362v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07471v1")'>On the nonconvexity of some push-forward constraints and its
  consequences in machine learning</div>
<div id='2403.07471v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:06:48Z</div><div>Authors: Lucas de Lara, Mathis Deronzier, Alberto González-Sanz, Virgile Foy</div><div style='padding-top: 10px; width: 80ex'>The push-forward operation enables one to redistribute a probability measure
through a deterministic map. It plays a key role in statistics and
optimization: many learning problems (notably from optimal transport,
generative modeling, and algorithmic fairness) include constraints or penalties
framed as push-forward conditions on the model. However, the literature lacks
general theoretical insights on the (non)convexity of such constraints and its
consequences on the associated learning problems. This paper aims at filling
this gap. In a first part, we provide a range of sufficient and necessary
conditions for the (non)convexity of two sets of functions: the maps
transporting one probability measure to another; the maps inducing equal output
distributions across distinct probability measures. This highlights that for
most probability measures, these push-forward constraints are not convex. In a
second time, we show how this result implies critical limitations on the design
of convex optimization problems for learning generative models or group-fair
predictors. This work will hopefully help researchers and practitioners have a
better understanding of the critical impact of push-forward conditions onto
convexity.</div><div><a href='http://arxiv.org/abs/2403.07471v1'>2403.07471v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11963v1")'>Transfer Learning Beyond Bounded Density Ratios</div>
<div id='2403.11963v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T17:02:41Z</div><div>Authors: Alkis Kalavasis, Ilias Zadik, Manolis Zampetakis</div><div style='padding-top: 10px; width: 80ex'>We study the fundamental problem of transfer learning where a learning
algorithm collects data from some source distribution $P$ but needs to perform
well with respect to a different target distribution $Q$. A standard change of
measure argument implies that transfer learning happens when the density ratio
$dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet
(COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where
the ratio $dQ/dP$ is unbounded, but transfer learning is possible.
  In this work, we focus on transfer learning over the class of low-degree
polynomial estimators. Our main result is a general transfer inequality over
the domain $\mathbb{R}^n$, proving that non-trivial transfer learning for
low-degree polynomials is possible under very mild assumptions, going well
beyond the classical assumption that $dQ/dP$ is bounded. For instance, it
always applies if $Q$ is a log-concave measure and the inverse ratio $dP/dQ$ is
bounded. To demonstrate the applicability of our inequality, we obtain new
results in the settings of: (1) the classical truncated regression setting,
where $dQ/dP$ equals infinity, and (2) the more recent out-of-distribution
generalization setting for in-context learning linear functions with
transformers. We also provide a discrete analogue of our transfer inequality on
the Boolean Hypercube $\{-1,1\}^n$, and study its connections with the recent
problem of Generalization on the Unseen of Abbe, Bengio, Lotfi and Rizk (ICML,
2023). Our main conceptual contribution is that the maximum influence of the
error of the estimator $\widehat{f}-f^*$ under $Q$,
$\mathrm{I}_{\max}(\widehat{f}-f^*)$, acts as a sufficient condition for
transferability; when $\mathrm{I}_{\max}(\widehat{f}-f^*)$ is appropriately
bounded, transfer is possible over the Boolean domain.</div><div><a href='http://arxiv.org/abs/2403.11963v1'>2403.11963v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08676v1")'>A Convergence Analysis of Approximate Message Passing with Non-Separable
  Functions and Applications to Multi-Class Classification</div>
<div id='2402.08676v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T18:56:55Z</div><div>Authors: Burak Çakmak, Yue M. Lu, Manfred Opper</div><div style='padding-top: 10px; width: 80ex'>Motivated by the recent application of approximate message passing (AMP) to
the analysis of convex optimizations in multi-class classifications [Loureiro,
et. al., 2021], we present a convergence analysis of AMP dynamics with
non-separable multivariate nonlinearities. As an application, we present a
complete (and independent) analysis of the motivated convex optimization
problem.</div><div><a href='http://arxiv.org/abs/2402.08676v1'>2402.08676v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07004v1")'>Convergence of Some Convex Message Passing Algorithms to a Fixed Point</div>
<div id='2403.07004v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T13:14:21Z</div><div>Authors: Vaclav Voracek, Tomas Werner</div><div style='padding-top: 10px; width: 80ex'>A popular approach to the MAP inference problem in graphical models is to
minimize an upper bound obtained from a dual linear programming or Lagrangian
relaxation by (block-)coordinate descent. Examples of such algorithms are
max-sum diffusion and sequential tree-reweighted message passing. Convergence
properties of these methods are currently not fully understood. They have been
proved to converge to the set characterized by local consistency of active
constraints, with unknown convergence rate; however, it was not clear if the
iterates converge at all (to any single point). We prove a stronger result
(which was conjectured before but never proved): the iterates converge to a
fixed point of the algorithm. Moreover, we show that they achieve precision
$\varepsilon&gt;0$ in $\mathcal{O}(1/\varepsilon)$ iterations.
  We first prove this for a version of coordinate descent applied to a general
piecewise-affine convex objective, using a novel proof technique. Then we
demonstrate the generality of this approach by reducing some popular
coordinate-descent algorithms to this problem. Finally we show that, in
contrast to our main result, a similar version of coordinate descent applied to
a constrained optimization problem need not converge.</div><div><a href='http://arxiv.org/abs/2403.07004v1'>2403.07004v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12508v1")'>SDEs for Minimax Optimization</div>
<div id='2402.12508v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:18:29Z</div><div>Authors: Enea Monzio Compagnoni, Antonio Orvieto, Hans Kersting, Frank Norbert Proske, Aurelien Lucchi</div><div style='padding-top: 10px; width: 80ex'>Minimax optimization problems have attracted a lot of attention over the past
few years, with applications ranging from economics to machine learning. While
advanced optimization methods exist for such problems, characterizing their
dynamics in stochastic scenarios remains notably challenging. In this paper, we
pioneer the use of stochastic differential equations (SDEs) to analyze and
compare Minimax optimizers. Our SDE models for Stochastic Gradient
Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient
Descent are provable approximations of their algorithmic counterparts, clearly
showcasing the interplay between hyperparameters, implicit regularization, and
implicit curvature-induced noise. This perspective also allows for a unified
and simplified analysis strategy based on the principles of It\^o calculus.
Finally, our approach facilitates the derivation of convergence conditions and
closed-form solutions for the dynamics in simplified settings, unveiling
further insights into the behavior of different optimizers.</div><div><a href='http://arxiv.org/abs/2402.12508v1'>2402.12508v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09090v1")'>Dissipative Gradient Descent Ascent Method: A Control Theory Inspired
  Algorithm for Min-max Optimization</div>
<div id='2403.09090v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T04:26:00Z</div><div>Authors: Tianqi Zheng, Nicolas Loizou, Pengcheng You, Enrique Mallada</div><div style='padding-top: 10px; width: 80ex'>Gradient Descent Ascent (GDA) methods for min-max optimization problems
typically produce oscillatory behavior that can lead to instability, e.g., in
bilinear settings. To address this problem, we introduce a dissipation term
into the GDA updates to dampen these oscillations. The proposed Dissipative GDA
(DGDA) method can be seen as performing standard GDA on a state-augmented and
regularized saddle function that does not strictly introduce additional
convexity/concavity. We theoretically show the linear convergence of DGDA in
the bilinear and strongly convex-strongly concave settings and assess its
performance by comparing DGDA with other methods such as GDA, Extra-Gradient
(EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these
methods, achieving superior convergence rates. We support our claims with two
numerical examples that showcase DGDA's effectiveness in solving saddle point
problems.</div><div><a href='http://arxiv.org/abs/2403.09090v1'>2403.09090v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08106v1")'>Mirror Descent-Ascent for mean-field min-max problems</div>
<div id='2402.08106v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T22:52:32Z</div><div>Authors: Razvan-Andrei Lascu, Mateusz B. Majka, Łukasz Szpruch</div><div style='padding-top: 10px; width: 80ex'>We study two variants of the mirror descent-ascent algorithm for solving
min-max problems on the space of measures: simultaneous and sequential. We work
under assumptions of convexity-concavity and relative smoothness of the payoff
function with respect to a suitable Bregman divergence, defined on the space of
measures via flat derivatives. We show that the convergence rates to mixed Nash
equilibria, measured in the Nikaid\`o-Isoda error, are of order
$\mathcal{O}\left(N^{-1/2}\right)$ and $\mathcal{O}\left(N^{-2/3}\right)$ for
the simultaneous and sequential schemes, respectively, which is in line with
the state-of-the-art results for related finite-dimensional algorithms.</div><div><a href='http://arxiv.org/abs/2402.08106v1'>2402.08106v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00147v1")'>Analysis of Kernel Mirror Prox for Measure Optimization</div>
<div id='2403.00147v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T21:55:17Z</div><div>Authors: Pavel Dvurechensky, Jia-Jie Zhu</div><div style='padding-top: 10px; width: 80ex'>By choosing a suitable function space as the dual to the non-negative measure
cone, we study in a unified framework a class of functional saddle-point
optimization problems, which we term the Mixed Functional Nash Equilibrium
(MFNE), that underlies several existing machine learning algorithms, such as
implicit generative models, distributionally robust optimization (DRO), and
Wasserstein barycenters. We model the saddle-point optimization dynamics as an
interacting Fisher-Rao-RKHS gradient flow when the function space is chosen as
a reproducing kernel Hilbert space (RKHS). As a discrete time counterpart, we
propose a primal-dual kernel mirror prox (KMP) algorithm, which uses a dual
step in the RKHS, and a primal entropic mirror prox step. We then provide a
unified convergence analysis of KMP in an infinite-dimensional setting for this
class of MFNE problems, which establishes a convergence rate of $O(1/N)$ in the
deterministic case and $O(1/\sqrt{N})$ in the stochastic case, where $N$ is the
iteration counter. As a case study, we apply our analysis to DRO, providing
algorithmic guarantees for DRO robustness and convergence.</div><div><a href='http://arxiv.org/abs/2403.00147v1'>2403.00147v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14012v1")'>Chasing Convex Functions with Long-term Constraints</div>
<div id='2402.14012v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:51:42Z</div><div>Authors: Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</div><div style='padding-top: 10px; width: 80ex'>We introduce and study a family of online metric problems with long-term
constraints. In these problems, an online player makes decisions $\mathbf{x}_t$
in a metric space $(X,d)$ to simultaneously minimize their hitting cost
$f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over the
time horizon $T$, the player must satisfy a long-term demand constraint
$\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fraction
of demand satisfied at time $t$. Such problems can find a wide array of
applications to online resource allocation in sustainable energy and computing
systems. We devise optimal competitive and learning-augmented algorithms for
specific instantiations of these problems, and further show that our proposed
algorithms perform well in numerical experiments.</div><div><a href='http://arxiv.org/abs/2402.14012v1'>2402.14012v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05318v1")'>Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling
  Salesman Problem</div>
<div id='2403.05318v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T13:49:21Z</div><div>Authors: Jingxiao Chen, Ziqin Gong, Minghuan Liu, Jun Wang, Yong Yu, Weinan Zhang</div><div style='padding-top: 10px; width: 80ex'>Many real-world problems can be formulated as a constrained Traveling
Salesman Problem (TSP). However, the constraints are always complex and
numerous, making the TSPs challenging to solve. When the number of complicated
constraints grows, it is time-consuming for traditional heuristic algorithms to
avoid illegitimate outcomes. Learning-based methods provide an alternative to
solve TSPs in a soft manner, which also supports GPU acceleration to generate
solutions quickly. Nevertheless, the soft manner inevitably results in
difficulty solving hard-constrained problems with learning algorithms, and the
conflicts between legality and optimality may substantially affect the
optimality of the solution. To overcome this problem and to have an effective
solution against hard constraints, we proposed a novel learning-based method
that uses looking-ahead information as the feature to improve the legality of
TSP with Time Windows (TSPTW) solutions. Besides, we constructed TSPTW datasets
with hard constraints in order to accurately evaluate and benchmark the
statistical performance of various approaches, which can serve the community
for future research. With comprehensive experiments on diverse datasets, MUSLA
outperforms existing baselines and shows generalizability potential.</div><div><a href='http://arxiv.org/abs/2403.05318v1'>2403.05318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.07961v3")'>Solution of the Probabilistic Lambert Problem: Connections with Optimal
  Mass Transport, Schrödinger Bridge and Reaction-Diffusion PDEs</div>
<div id='2401.07961v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T20:57:50Z</div><div>Authors: Alexis M. H. Teter, Iman Nodozi, Abhishek Halder</div><div style='padding-top: 10px; width: 80ex'>Lambert's problem concerns with transferring a spacecraft from a given
initial to a given terminal position within prescribed flight time via velocity
control subject to a gravitational force field. We consider a probabilistic
variant of the Lambert problem where the knowledge of the endpoint constraints
in position vectors are replaced by the knowledge of their respective joint
probability density functions. We show that the Lambert problem with endpoint
joint probability density constraints is a generalized optimal mass transport
(OMT) problem, thereby connecting this classical astrodynamics problem with a
burgeoning area of research in modern stochastic control and stochastic machine
learning. This newfound connection allows us to rigorously establish the
existence and uniqueness of solution for the probabilistic Lambert problem. The
same connection also helps to numerically solve the probabilistic Lambert
problem via diffusion regularization, i.e., by leveraging further connection of
the OMT with the Schr\"odinger bridge problem (SBP). This also shows that the
probabilistic Lambert problem with additive dynamic process noise is in fact a
generalized SBP, and can be solved numerically using the so-called
Schr\"odinger factors, as we do in this work. We explain how the resulting
analysis leads to solving a boundary-coupled system of reaction-diffusion PDEs
where the nonlinear gravitational potential appears as the reaction rate. We
propose novel algorithms for the same, and present illustrative numerical
results. Our analysis and the algorithmic framework are nonparametric, i.e., we
make neither statistical (e.g., Gaussian, first few moments, mixture or
exponential family, finite dimensionality of the sufficient statistic) nor
dynamical (e.g., Taylor series) approximations.</div><div><a href='http://arxiv.org/abs/2401.07961v3'>2401.07961v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13903v1")'>Dealing with unbounded gradients in stochastic saddle-point optimization</div>
<div id='2402.13903v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T16:13:49Z</div><div>Authors: Gergely Neu, Nneka Okolo</div><div style='padding-top: 10px; width: 80ex'>We study the performance of stochastic first-order methods for finding saddle
points of convex-concave functions. A notorious challenge faced by such methods
is that the gradients can grow arbitrarily large during optimization, which may
result in instability and divergence. In this paper, we propose a simple and
effective regularization technique that stabilizes the iterates and yields
meaningful performance guarantees even if the domain and the gradient noise
scales linearly with the size of the iterates (and is thus potentially
unbounded). Besides providing a set of general results, we also apply our
algorithm to a specific problem in reinforcement learning, where it leads to
performance guarantees for finding near-optimal policies in an average-reward
MDP without prior knowledge of the bias span.</div><div><a href='http://arxiv.org/abs/2402.13903v1'>2402.13903v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02418v1")'>From Zero to Hero: How local curvature at artless initial conditions
  leads away from bad minima</div>
<div id='2403.02418v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T19:12:13Z</div><div>Authors: Tony Bonnaire, Giulio Biroli, Chiara Cammarota</div><div style='padding-top: 10px; width: 80ex'>We investigate the optimization dynamics of gradient descent in a non-convex
and high-dimensional setting, with a focus on the phase retrieval problem as a
case study for complex loss landscapes. We first study the high-dimensional
limit where both the number $M$ and the dimension $N$ of the data are going to
infinity at fixed signal-to-noise ratio $\alpha = M/N$. By analyzing how the
local curvature changes during optimization, we uncover that for intermediate
$\alpha$, the Hessian displays a downward direction pointing towards good
minima in the first regime of the descent, before being trapped in bad minima
at the end. Hence, the local landscape is benign and informative at first,
before gradient descent brings the system into a uninformative maze. The
transition between the two regimes is associated to a BBP-type threshold in the
time-dependent Hessian. Through both theoretical analysis and numerical
experiments, we show that in practical cases, i.e. for finite but even very
large $N$, successful optimization via gradient descent in phase retrieval is
achieved by falling towards the good minima before reaching the bad ones. This
mechanism explains why successful recovery is obtained well before the
algorithmic transition corresponding to the high-dimensional limit.
Technically, this is associated to strong logarithmic corrections of the
algorithmic transition at large $N$ with respect to the one expected in the
$N\to\infty$ limit. Our analysis sheds light on such a new mechanism that
facilitate gradient descent dynamics in finite large dimensions, also
highlighting the importance of good initialization of spectral properties for
optimization in complex high-dimensional landscapes.</div><div><a href='http://arxiv.org/abs/2403.02418v1'>2403.02418v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07723v1")'>Generalization Bounds for Heavy-Tailed SDEs through the Fractional
  Fokker-Planck Equation</div>
<div id='2402.07723v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:35:32Z</div><div>Authors: Benjamin Dupuis, Umut Şimşekli</div><div style='padding-top: 10px; width: 80ex'>Understanding the generalization properties of heavy-tailed stochastic
optimization algorithms has attracted increasing attention over the past years.
While illuminating interesting aspects of stochastic optimizers by using
heavy-tailed stochastic differential equations as proxies, prior works either
provided expected generalization bounds, or introduced non-computable
information theoretic terms. Addressing these drawbacks, in this work, we prove
high-probability generalization bounds for heavy-tailed SDEs which do not
contain any nontrivial information theoretic terms. To achieve this goal, we
develop new proof techniques based on estimating the entropy flows associated
with the so-called fractional Fokker-Planck equation (a partial differential
equation that governs the evolution of the distribution of the corresponding
heavy-tailed SDE). In addition to obtaining high-probability bounds, we show
that our bounds have a better dependence on the dimension of parameters as
compared to prior art. Our results further identify a phase transition
phenomenon, which suggests that heavy tails can be either beneficial or harmful
depending on the problem structure. We support our theory with experiments
conducted in a variety of settings.</div><div><a href='http://arxiv.org/abs/2402.07723v1'>2402.07723v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11687v1")'>Nonsmooth Implicit Differentiation: Deterministic and Stochastic
  Convergence Rates</div>
<div id='2403.11687v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T11:37:53Z</div><div>Authors: Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo</div><div style='padding-top: 10px; width: 80ex'>We study the problem of efficiently computing the derivative of the
fixed-point of a parametric non-differentiable contraction map. This problem
has wide applications in machine learning, including hyperparameter
optimization, meta-learning and data poisoning attacks. We analyze two popular
approaches: iterative differentiation (ITD) and approximate implicit
differentiation (AID). A key challenge behind the nonsmooth setting is that the
chain rule does not hold anymore. Building upon the recent work by Bolte et al.
(2022), who proved the linear convergence of non-differentiable ITD, we provide
refined linear convergence rates for both ITD and AID in the deterministic
case. We further introduce NSID, a new method to compute the implicit
derivative when the fixed point is defined as the composition of an outer map
and an inner map which is accessible only through a stochastic unbiased
estimator. We establish rates for the convergence of NSID to the true
derivative, encompassing the best available rates in the smooth setting. We
present illustrative experiments confirming our analysis.</div><div><a href='http://arxiv.org/abs/2403.11687v1'>2403.11687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07613v1")'>Global optimality under amenable symmetry constraints</div>
<div id='2402.07613v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T12:38:20Z</div><div>Authors: Peter Orbanz</div><div style='padding-top: 10px; width: 80ex'>We ask whether there exists a function or measure that (1) minimizes a given
convex functional or risk and (2) satisfies a symmetry property specified by an
amenable group of transformations. Examples of such symmetry properties are
invariance, equivariance, or quasi-invariance. Our results draw on old ideas of
Stein and Le Cam and on approximate group averages that appear in ergodic
theorems for amenable groups. A class of convex sets known as orbitopes in
convex analysis emerges as crucial, and we establish properties of such
orbitopes in nonparametric settings. We also show how a simple device called a
cocycle can be used to reduce different forms of symmetry to a single problem.
As applications, we obtain results on invariant kernel mean embeddings and a
Monge-Kantorovich theorem on optimality of transport plans under symmetry
constraints. We also explain connections to the Hunt-Stein theorem on invariant
tests.</div><div><a href='http://arxiv.org/abs/2402.07613v1'>2402.07613v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09389v1")'>Learning to optimize with convergence guarantees using nonlinear system
  theory</div>
<div id='2403.09389v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T13:40:26Z</div><div>Authors: Andrea Martin, Luca Furieri</div><div style='padding-top: 10px; width: 80ex'>The increasing reliance on numerical methods for controlling dynamical
systems and training machine learning models underscores the need to devise
algorithms that dependably and efficiently navigate complex optimization
landscapes. Classical gradient descent methods offer strong theoretical
guarantees for convex problems; however, they demand meticulous hyperparameter
tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O)
automates the discovery of algorithms with optimized performance leveraging
learning models and data - yet, it lacks a theoretical framework to analyze
convergence and robustness of the learned algorithms. In this paper, we fill
this gap by harnessing nonlinear system theory. Specifically, we propose an
unconstrained parametrization of all convergent algorithms for smooth
non-convex objective functions. Notably, our framework is directly compatible
with automatic differentiation tools, ensuring convergence by design while
learning to optimize.</div><div><a href='http://arxiv.org/abs/2403.09389v1'>2403.09389v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02342v2")'>MetaOptimize: A Framework for Optimizing Step Sizes and Other
  Meta-parameters</div>
<div id='2402.02342v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T04:55:54Z</div><div>Authors: Arsalan Sharifnassab, Saber Salehkaleybar, Richard Sutton</div><div style='padding-top: 10px; width: 80ex'>This paper addresses the challenge of optimizing meta-parameters (i.e.,
hyperparameters) in machine learning algorithms, a critical factor influencing
training efficiency and model performance. Moving away from the computationally
expensive traditional meta-parameter search methods, we introduce MetaOptimize
framework that dynamically adjusts meta-parameters, particularly step sizes
(also known as learning rates), during training. More specifically,
MetaOptimize can wrap around any first-order optimization algorithm, tuning
step sizes on the fly to minimize a specific form of regret that accounts for
long-term effect of step sizes on training, through a discounted sum of future
losses. We also introduce low complexity variants of MetaOptimize that, in
conjunction with its adaptability to multiple optimization algorithms,
demonstrate performance competitive to those of best hand-crafted learning rate
schedules across various machine learning applications.</div><div><a href='http://arxiv.org/abs/2402.02342v2'>2402.02342v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13641v1")'>FlexHB: a More Efficient and Flexible Framework for Hyperparameter
  Optimization</div>
<div id='2402.13641v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T09:18:59Z</div><div>Authors: Yang Zhang, Haiyang Wu, Yuekui Yang</div><div style='padding-top: 10px; width: 80ex'>Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm
to find optimal configurations efficiently? Bayesian Optimization(BO) and the
multi-fidelity BO methods employ surrogate models to sample configurations
based on history evaluations. More recent studies obtain better performance by
integrating BO with HyperBand(HB), which accelerates evaluation by early
stopping mechanism. However, these methods ignore the advantage of a suitable
evaluation scheme over the default HyperBand, and the capability of BO is still
constrained by skewed evaluation results. In this paper, we propose FlexHB, a
new method pushing multi-fidelity BO to the limit as well as re-designing a
framework for early stopping with Successive Halving(SH). Comprehensive study
on FlexHB shows that (1) our fine-grained fidelity method considerably enhances
the efficiency of searching optimal configurations, (2) our FlexBand framework
(self-adaptive allocation of SH brackets, and global ranking of configurations
in both current and past SH procedures) grants the algorithm with more
flexibility and improves the anytime performance. Our method achieves superior
efficiency and outperforms other methods on various HPO tasks. Empirical
results demonstrate that FlexHB can achieve up to 6.9X and 11.1X speedups over
the state-of-the-art MFES-HB and BOHB respectively.</div><div><a href='http://arxiv.org/abs/2402.13641v1'>2402.13641v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15262v1")'>Dynamic Memory Based Adaptive Optimization</div>
<div id='2402.15262v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T11:19:02Z</div><div>Authors: Balázs Szegedy, Domonkos Czifra, Péter Kőrösi-Szabó</div><div style='padding-top: 10px; width: 80ex'>Define an optimizer as having memory $k$ if it stores $k$ dynamically
changing vectors in the parameter space. Classical SGD has memory $0$, momentum
SGD optimizer has $1$ and Adam optimizer has $2$. We address the following
questions: How can optimizers make use of more memory units? What information
should be stored in them? How to use them for the learning steps? As an
approach to the last question, we introduce a general method called
"Retrospective Learning Law Correction" or shortly RLLC. This method is
designed to calculate a dynamically varying linear combination (called learning
law) of memory units, which themselves may evolve arbitrarily. We demonstrate
RLLC on optimizers whose memory units have linear update rules and small memory
($\leq 4$ memory units). Our experiments show that in a variety of standard
problems, these optimizers outperform the above mentioned three classical
optimizers. We conclude that RLLC is a promising framework for boosting the
performance of known optimizers by adding more memory units and by making them
more adaptive.</div><div><a href='http://arxiv.org/abs/2402.15262v1'>2402.15262v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07413v1")'>Learning-Augmented Algorithms with Explicit Predictors</div>
<div id='2403.07413v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T08:40:21Z</div><div>Authors: Marek Elias, Haim Kaplan, Yishay Mansour, Shay Moran</div><div style='padding-top: 10px; width: 80ex'>Recent advances in algorithmic design show how to utilize predictions
obtained by machine learning models from past and present data. These
approaches have demonstrated an enhancement in performance when the predictions
are accurate, while also ensuring robustness by providing worst-case guarantees
when predictions fail. In this paper we focus on online problems; prior
research in this context was focused on a paradigm where the predictor is
pre-trained on past data and then used as a black box (to get the predictions
it was trained for). In contrast, in this work, we unpack the predictor and
integrate the learning problem it gives rise for within the algorithmic
challenge. In particular we allow the predictor to learn as it receives larger
parts of the input, with the ultimate goal of designing online learning
algorithms specifically tailored for the algorithmic task at hand. Adopting
this perspective, we focus on a number of fundamental problems, including
caching and scheduling, which have been well-studied in the black-box setting.
For each of the problems we consider, we introduce new algorithms that take
advantage of explicit learning algorithms which we carefully design towards
optimizing the overall performance. We demonstrate the potential of our
approach by deriving performance bounds which improve over those established in
previous work.</div><div><a href='http://arxiv.org/abs/2403.07413v1'>2403.07413v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17143v1")'>Energy-Efficient Scheduling with Predictions</div>
<div id='2402.17143v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T02:13:32Z</div><div>Authors: Eric Balkanski, Noemie Perivier, Clifford Stein, Hao-Ting Wei</div><div style='padding-top: 10px; width: 80ex'>An important goal of modern scheduling systems is to efficiently manage power
usage. In energy-efficient scheduling, the operating system controls the speed
at which a machine is processing jobs with the dual objective of minimizing
energy consumption and optimizing the quality of service cost of the resulting
schedule. Since machine-learned predictions about future requests can often be
learned from historical data, a recent line of work on learning-augmented
algorithms aims to achieve improved performance guarantees by leveraging
predictions. In particular, for energy-efficient scheduling, Bamas et. al.
[BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms
with predictions for the energy minimization with deadlines problem and
achieved an improved competitive ratio when the prediction error is small while
also maintaining worst-case bounds even when the prediction error is
arbitrarily large.
  In this paper, we consider a general setting for energy-efficient scheduling
and provide a flexible learning-augmented algorithmic framework that takes as
input an offline and an online algorithm for the desired energy-efficient
scheduling problem. We show that, when the prediction error is small, this
framework gives improved competitive ratios for many different energy-efficient
scheduling problems, including energy minimization with deadlines, while also
maintaining a bounded competitive ratio regardless of the prediction error.
Finally, we empirically demonstrate that this framework achieves an improved
performance on real and synthetic datasets.</div><div><a href='http://arxiv.org/abs/2402.17143v1'>2402.17143v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.12332v1")'>A Precise Characterization of SGD Stability Using Loss Surface Geometry</div>
<div id='2401.12332v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T19:46:30Z</div><div>Authors: Gregory Dexter, Borja Ocejo, Sathiya Keerthi, Aman Gupta, Ayan Acharya, Rajiv Khanna</div><div style='padding-top: 10px; width: 80ex'>Stochastic Gradient Descent (SGD) stands as a cornerstone optimization
algorithm with proven real-world empirical successes but relatively limited
theoretical understanding. Recent research has illuminated a key factor
contributing to its practical efficacy: the implicit regularization it
instigates. Several studies have investigated the linear stability property of
SGD in the vicinity of a stationary point as a predictive proxy for sharpness
and generalization error in overparameterized neural networks (Wu et al., 2022;
Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper
into the relationship between linear stability and sharpness. More
specifically, we meticulously delineate the necessary and sufficient conditions
for linear stability, contingent on hyperparameters of SGD and the sharpness at
the optimum. Towards this end, we introduce a novel coherence measure of the
loss Hessian that encapsulates pertinent geometric properties of the loss
function that are relevant to the linear stability of SGD. It enables us to
provide a simplified sufficient condition for identifying linear instability at
an optimum. Notably, compared to previous works, our analysis relies on
significantly milder assumptions and is applicable for a broader class of loss
functions than known before, encompassing not only mean-squared error but also
cross-entropy loss.</div><div><a href='http://arxiv.org/abs/2401.12332v1'>2401.12332v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08426v2")'>GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training</div>
<div id='2401.08426v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T15:11:29Z</div><div>Authors: Siddharth Krishna Kumar</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the distinctions between gradient methods applied to
non-differentiable functions (NGDMs) and classical gradient descents (GDs)
designed for differentiable functions. First, we demonstrate significant
differences in the convergence properties of NGDMs compared to GDs, challenging
the applicability of the extensive neural network convergence literature based
on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the
paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing
that increasing the regularization penalty leads to an increase in the $L_{1}$
norm of optimal solutions in NGDMs. Consequently, we show that widely adopted
$L_{1}$ penalization-based techniques for network pruning do not yield expected
results. Finally, we explore the Edge of Stability phenomenon, indicating its
inapplicability even to Lipschitz continuous convex differentiable functions,
leaving its relevance to non-convex non-differentiable neural networks
inconclusive. Our analysis exposes misguided interpretations of NGDMs in widely
referenced papers and texts due to an overreliance on strong smoothness
assumptions, emphasizing the necessity for a nuanced understanding of
foundational assumptions in the analysis of these systems.</div><div><a href='http://arxiv.org/abs/2401.08426v2'>2401.08426v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07936v1")'>A Globally Convergent Algorithm for Neural Network Parameter
  Optimization Based on Difference-of-Convex Functions</div>
<div id='2401.07936v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T19:53:35Z</div><div>Authors: Daniel Tschernutter, Mathias Kraus, Stefan Feuerriegel</div><div style='padding-top: 10px; width: 80ex'>We propose an algorithm for optimizing the parameters of single hidden layer
neural networks. Specifically, we derive a blockwise difference-of-convex (DC)
functions representation of the objective function. Based on the latter, we
propose a block coordinate descent (BCD) approach that we combine with a
tailored difference-of-convex functions algorithm (DCA). We prove global
convergence of the proposed algorithm. Furthermore, we mathematically analyze
the convergence rate of parameters and the convergence rate in value (i.e., the
training loss). We give conditions under which our algorithm converges linearly
or even faster depending on the local shape of the loss function. We confirm
our theoretical derivations numerically and compare our algorithm against
state-of-the-art gradient-based solvers in terms of both training loss and test
loss.</div><div><a href='http://arxiv.org/abs/2401.07936v1'>2401.07936v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03240v1")'>Interpreting Adaptive Gradient Methods by Parameter Scaling for
  Learning-Rate-Free Optimization</div>
<div id='2401.03240v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T15:45:29Z</div><div>Authors: Min-Kook Suh, Seung-Woo Seo</div><div style='padding-top: 10px; width: 80ex'>We address the challenge of estimating the learning rate for adaptive
gradient methods used in training deep neural networks. While several
learning-rate-free approaches have been proposed, they are typically tailored
for steepest descent. However, although steepest descent methods offer an
intuitive approach to finding minima, many deep learning applications require
adaptive gradient methods to achieve faster convergence. In this paper, we
interpret adaptive gradient methods as steepest descent applied on
parameter-scaled networks, proposing learning-rate-free adaptive gradient
methods. Experimental results verify the effectiveness of this approach,
demonstrating comparable performance to hand-tuned learning rates across
various scenarios. This work extends the applicability of learning-rate-free
methods, enhancing training with adaptive gradient methods.</div><div><a href='http://arxiv.org/abs/2401.03240v1'>2401.03240v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12511v1")'>Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient
  Deep Neural Network Training</div>
<div id='2403.12511v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T07:25:36Z</div><div>Authors: M. Rostami, S. S. Kia</div><div style='padding-top: 10px; width: 80ex'>Training a deep neural network using gradient-based methods necessitates the
calculation of gradients at each level. However, using backpropagation or
reverse mode differentiation, to calculate the gradients necessities
significant memory consumption, rendering backpropagation an inefficient method
for computing gradients. This paper focuses on analyzing the performance of the
well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by
having access to the forward mode of automatic differentiation to compute
gradients. We provide in-depth technical details that show the proposed
Algorithm does converge to the optimal solution with a sub-linear rate of
convergence by having access to the noisy estimate of the true gradient
obtained in the forward mode of automated differentiation, referred to as the
Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm,
when provided with access to the Projected Forward Gradient, fails to converge
to the optimal solution. We demonstrate the convergence attributes of our
proposed algorithms using a numerical example.</div><div><a href='http://arxiv.org/abs/2403.12511v1'>2403.12511v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06699v1")'>A Closed-form Solution for Weight Optimization in Fully-connected
  Feed-forward Neural Networks</div>
<div id='2401.06699v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T17:03:55Z</div><div>Authors: Slavisa Tomic, João Pedro Matos-Carvalho, Marko Beko</div><div style='padding-top: 10px; width: 80ex'>This work addresses weight optimization problem for fully-connected
feed-forward neural networks. Unlike existing approaches that are based on
back-propagation (BP) and chain rule gradient-based optimization (which implies
iterative execution, potentially burdensome and time-consuming in some cases),
the proposed approach offers the solution for weight optimization in
closed-form by means of least squares (LS) methodology. In the case where the
input-to-output mapping is injective, the new approach optimizes the weights in
a back-propagating fashion in a single iteration by jointly optimizing a set of
weights in each layer for each neuron. In the case where the input-to-output
mapping is not injective (e.g., in classification problems), the proposed
solution is easily adapted to obtain its final solution in a few iterations. An
important advantage over the existing solutions is that these computations (for
all neurons in a layer) are independent from each other; thus, they can be
carried out in parallel to optimize all weights in a given layer
simultaneously. Furthermore, its running time is deterministic in the sense
that one can obtain the exact number of computations necessary to optimize the
weights in all network layers (per iteration, in the case of non-injective
mapping). Our simulation and empirical results show that the proposed scheme,
BPLS, works well and is competitive with existing ones in terms of accuracy,
but significantly surpasses them in terms of running time. To summarize, the
new method is straightforward to implement, is competitive and computationally
more efficient than the existing ones, and is well-tailored for parallel
implementation.</div><div><a href='http://arxiv.org/abs/2401.06699v1'>2401.06699v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00152v1")'>Deeper or Wider: A Perspective from Optimal Generalization Error with
  Sobolev Loss</div>
<div id='2402.00152v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T20:10:10Z</div><div>Authors: Yahong Yang, Juncai He</div><div style='padding-top: 10px; width: 80ex'>Constructing the architecture of a neural network is a challenging pursuit
for the machine learning community, and the dilemma of whether to go deeper or
wider remains a persistent question. This paper explores a comparison between
deeper neural networks (DeNNs) with a flexible number of layers and wider
neural networks (WeNNs) with limited hidden layers, focusing on their optimal
generalization error in Sobolev losses. Analytical investigations reveal that
the architecture of a neural network can be significantly influenced by various
factors, including the number of sample points, parameters within the neural
networks, and the regularity of the loss function. Specifically, a higher
number of parameters tends to favor WeNNs, while an increased number of sample
points and greater regularity in the loss function lean towards the adoption of
DeNNs. We ultimately apply this theory to address partial differential
equations using deep Ritz and physics-informed neural network (PINN) methods,
guiding the design of neural networks.</div><div><a href='http://arxiv.org/abs/2402.00152v1'>2402.00152v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00326v3")'>PirateNets: Physics-informed Deep Learning with Residual Adaptive
  Networks</div>
<div id='2402.00326v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T04:17:56Z</div><div>Authors: Sifan Wang, Bowen Li, Yuhan Chen, Paris Perdikaris</div><div style='padding-top: 10px; width: 80ex'>While physics-informed neural networks (PINNs) have become a popular deep
learning framework for tackling forward and inverse problems governed by
partial differential equations (PDEs), their performance is known to degrade
when larger and deeper neural network architectures are employed. Our study
identifies that the root of this counter-intuitive behavior lies in the use of
multi-layer perceptron (MLP) architectures with non-suitable initialization
schemes, which result in poor trainablity for the network derivatives, and
ultimately lead to an unstable minimization of the PDE residual loss. To
address this, we introduce Physics-informed Residual Adaptive Networks
(PirateNets), a novel architecture that is designed to facilitate stable and
efficient training of deep PINN models. PirateNets leverage a novel adaptive
residual connection, which allows the networks to be initialized as shallow
networks that progressively deepen during training. We also show that the
proposed initialization scheme allows us to encode appropriate inductive biases
corresponding to a given PDE system into the network architecture. We provide
comprehensive empirical evidence showing that PirateNets are easier to optimize
and can gain accuracy from considerably increased depth, ultimately achieving
state-of-the-art results across various benchmarks. All code and data
accompanying this manuscript will be made publicly available at
\url{https://github.com/PredictiveIntelligenceLab/jaxpi}.</div><div><a href='http://arxiv.org/abs/2402.00326v3'>2402.00326v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01868v1")'>Challenges in Training PINNs: A Loss Landscape Perspective</div>
<div id='2402.01868v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:46:43Z</div><div>Authors: Pratik Rathore, Weimu Lei, Zachary Frangella, Lu Lu, Madeleine Udell</div><div style='padding-top: 10px; width: 80ex'>This paper explores challenges in training Physics-Informed Neural Networks
(PINNs), emphasizing the role of the loss landscape in the training process. We
examine difficulties in minimizing the PINN loss function, particularly due to
ill-conditioning caused by differential operators in the residual term. We
compare gradient-based optimizers Adam, L-BFGS, and their combination
Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel
second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN
performance. Theoretically, our work elucidates the connection between
ill-conditioned differential operators and ill-conditioning in the PINN loss
and shows the benefits of combining first- and second-order optimization
methods. Our work presents valuable insights and more powerful optimization
strategies for training PINNs, which could improve the utility of PINNs for
solving difficult partial differential equations.</div><div><a href='http://arxiv.org/abs/2402.01868v1'>2402.01868v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02300v2")'>Robust Physics Informed Neural Networks</div>
<div id='2401.02300v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T14:42:29Z</div><div>Authors: Marcin Łoś, Maciej Paszyński</div><div style='padding-top: 10px; width: 80ex'>We introduce a Robust version of the Physics-Informed Neural Networks
(RPINNs) to approximate the Partial Differential Equations (PDEs) solution.
Standard Physics Informed Neural Networks (PINN) takes into account the
governing physical laws described by PDE during the learning process. The
network is trained on a data set that consists of randomly selected points in
the physical domain and its boundary. PINNs have been successfully applied to
solve various problems described by PDEs with boundary conditions. The loss
function in traditional PINNs is based on the strong residuals of the PDEs.
This loss function in PINNs is generally not robust with respect to the true
error. The loss function in PINNs can be far from the true error, which makes
the training process more difficult. In particular, we do not know if the
training process has already converged to the solution with the required
accuracy. This is especially true if we do not know the exact solution, so we
cannot estimate the true error during the training. This paper introduces a
different way of defining the loss function. It incorporates the residual and
the inverse of the Gram matrix, computed using the energy norm. We test our
RPINN algorithm on two Laplace problems and one advection-diffusion problem in
two spatial dimensions. We conclude that RPINN is a robust method. The proposed
loss coincides well with the true error of the solution, as measured in the
energy norm. Thus, we know if our training process goes well, and we know when
to stop the training to obtain the neural network approximation of the solution
of the PDE with the true error of required accuracy.</div><div><a href='http://arxiv.org/abs/2401.02300v2'>2401.02300v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10926v1")'>Numerical analysis of physics-informed neural networks and related
  models in physics-informed machine learning</div>
<div id='2402.10926v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T10:43:27Z</div><div>Authors: Tim De Ryck, Siddhartha Mishra</div><div style='padding-top: 10px; width: 80ex'>Physics-informed neural networks (PINNs) and their variants have been very
popular in recent years as algorithms for the numerical simulation of both
forward and inverse problems for partial differential equations. This article
aims to provide a comprehensive review of currently available results on the
numerical analysis of PINNs and related models that constitute the backbone of
physics-informed machine learning. We provide a unified framework in which
analysis of the various components of the error incurred by PINNs in
approximating PDEs can be effectively carried out. A detailed review of
available results on approximation, generalization and training errors and
their behavior with respect to the type of the PDE and the dimension of the
underlying domain is presented. In particular, the role of the regularity of
the solutions and their stability to perturbations in the error analysis is
elucidated. Numerical results are also presented to illustrate the theory. We
identify training errors as a key bottleneck which can adversely affect the
overall performance of various models in physics-informed machine learning.</div><div><a href='http://arxiv.org/abs/2402.10926v1'>2402.10926v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03243v1")'>PINN-BO: A Black-box Optimization Algorithm using Physics-Informed
  Neural Networks</div>
<div id='2402.03243v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T17:58:17Z</div><div>Authors: Dat Phan-Trong, Hung The Tran, Alistair Shilton, Sunil Gupta</div><div style='padding-top: 10px; width: 80ex'>Black-box optimization is a powerful approach for discovering global optima
in noisy and expensive black-box functions, a problem widely encountered in
real-world scenarios. Recently, there has been a growing interest in leveraging
domain knowledge to enhance the efficacy of machine learning methods. Partial
Differential Equations (PDEs) often provide an effective means for elucidating
the fundamental principles governing the black-box functions. In this paper, we
propose PINN-BO, a black-box optimization algorithm employing Physics-Informed
Neural Networks that integrates the knowledge from Partial Differential
Equations (PDEs) to improve the sample efficiency of the optimization. We
analyze the theoretical behavior of our algorithm in terms of regret bound
using advances in NTK theory and prove that the use of the PDE alongside the
black-box function evaluations, PINN-BO leads to a tighter regret bound. We
perform several experiments on a variety of optimization tasks and show that
our algorithm is more sample-efficient compared to existing methods.</div><div><a href='http://arxiv.org/abs/2402.03243v1'>2402.03243v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02730v1")'>A Two-Stage Training Method for Modeling Constrained Systems With Neural
  Networks</div>
<div id='2403.02730v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T07:37:47Z</div><div>Authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</div><div style='padding-top: 10px; width: 80ex'>Real-world systems are often formulated as constrained optimization problems.
Techniques to incorporate constraints into Neural Networks (NN), such as Neural
Ordinary Differential Equations (Neural ODEs), have been used. However, these
introduce hyperparameters that require manual tuning through trial and error,
raising doubts about the successful incorporation of constraints into the
generated model. This paper describes in detail the two-stage training method
for Neural ODEs, a simple, effective, and penalty parameter-free approach to
model constrained systems. In this approach the constrained optimization
problem is rewritten as two unconstrained sub-problems that are solved in two
stages. The first stage aims at finding feasible NN parameters by minimizing a
measure of constraints violation. The second stage aims to find the optimal NN
parameters by minimizing the loss function while keeping inside the feasible
region. We experimentally demonstrate that our method produces models that
satisfy the constraints and also improves their predictive performance. Thus,
ensuring compliance with critical system properties and also contributing to
reducing data quantity requirements. Furthermore, we show that the proposed
method improves the convergence to an optimal solution and improves the
explainability of Neural ODE models. Our proposed two-stage training method can
be used with any NN architectures.</div><div><a href='http://arxiv.org/abs/2403.02730v1'>2403.02730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07251v1")'>Physics-Informed Neural Networks with Hard Linear Equality Constraints</div>
<div id='2402.07251v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-11T17:40:26Z</div><div>Authors: Hao Chen, Gonzalo E. Constante Flores, Can Li</div><div style='padding-top: 10px; width: 80ex'>Surrogate modeling is used to replace computationally expensive simulations.
Neural networks have been widely applied as surrogate models that enable
efficient evaluations over complex physical systems. Despite this, neural
networks are data-driven models and devoid of any physics. The incorporation of
physics into neural networks can improve generalization and data efficiency.
The physics-informed neural network (PINN) is an approach to leverage known
physical constraints present in the data, but it cannot strictly satisfy them
in the predictions. This work proposes a novel physics-informed neural network,
KKT-hPINN, which rigorously guarantees hard linear equality constraints through
projection layers derived from KKT conditions. Numerical experiments on Aspen
models of a continuous stirred-tank reactor (CSTR) unit, an extractive
distillation subsystem, and a chemical plant demonstrate that this model can
further enhance the prediction accuracy.</div><div><a href='http://arxiv.org/abs/2402.07251v1'>2402.07251v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09454v1")'>Machine learning for structural design models of continuous beam systems
  via influence zones</div>
<div id='2403.09454v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T14:53:18Z</div><div>Authors: Adrien Gallet, Andrew Liew, Iman Hajirasouliha, Danny Smyl</div><div style='padding-top: 10px; width: 80ex'>This work develops a machine learned structural design model for continuous
beam systems from the inverse problem perspective. After demarcating between
forward, optimisation and inverse machine learned operators, the investigation
proposes a novel methodology based on the recently developed influence zone
concept which represents a fundamental shift in approach compared to
traditional structural design methods. The aim of this approach is to
conceptualise a non-iterative structural design model that predicts
cross-section requirements for continuous beam systems of arbitrary system
size. After generating a dataset of known solutions, an appropriate neural
network architecture is identified, trained, and tested against unseen data.
The results show a mean absolute percentage testing error of 1.6% for
cross-section property predictions, along with a good ability of the neural
network to generalise well to structural systems of variable size. The CBeamXP
dataset generated in this work and an associated python-based neural network
training script are available at an open-source data repository to allow for
the reproducibility of results and to encourage further investigations.</div><div><a href='http://arxiv.org/abs/2403.09454v1'>2403.09454v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17992v2")'>Physics-Informed Machine Learning for Seismic Response Prediction OF
  Nonlinear Steel Moment Resisting Frame Structures</div>
<div id='2402.17992v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T02:16:03Z</div><div>Authors: R. Bailey Bond, Pu Ren, Jerome F. Hajjar, Hao Sun</div><div style='padding-top: 10px; width: 80ex'>There is a growing interest in utilizing machine learning (ML) methods for
structural metamodeling due to the substantial computational cost of
traditional numerical simulations. The existing data-driven strategies show
potential limitations to the model robustness and interpretability as well as
the dependency of rich data. To address these challenges, this paper presents a
novel physics-informed machine learning (PiML) method, which incorporates
scientific principles and physical laws into deep neural networks for modeling
seismic responses of nonlinear structures. The basic concept is to constrain
the solution space of the ML model within known physical bounds. This is made
possible with three main features, namely, model order reduction, a long
short-term memory (LSTM) networks, and Newton's second law (e.g., the equation
of motion). Model order reduction is essential for handling structural systems
with inherent redundancy and enhancing model efficiency. The LSTM network
captures temporal dependencies, enabling accurate prediction of time series
responses. The equation of motion is manipulated to learn system nonlinearities
and confines the solution space within physically interpretable results. These
features enable model training with relatively sparse data and offer benefits
in terms of accuracy, interpretability, and robustness. Furthermore, a dataset
of seismically designed archetype ductile planar steel moment resistant frames
under horizontal seismic loading, available in the DesignSafe-CI Database, is
considered for evaluation of the proposed method. The resulting metamodel is
capable of handling more complex data compared to existing physics-guided LSTM
models and outperforms other non-physics data-driven neural networks.</div><div><a href='http://arxiv.org/abs/2402.17992v2'>2402.17992v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01098v1")'>Bayesian Deep Learning for Remaining Useful Life Estimation via Stein
  Variational Gradient Descent</div>
<div id='2402.01098v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T02:21:06Z</div><div>Authors: Luca Della Libera, Jacopo Andreoli, Davide Dalle Pezze, Mirco Ravanelli, Gian Antonio Susto</div><div style='padding-top: 10px; width: 80ex'>A crucial task in predictive maintenance is estimating the remaining useful
life of physical systems. In the last decade, deep learning has improved
considerably upon traditional model-based and statistical approaches in terms
of predictive performance. However, in order to optimally plan maintenance
operations, it is also important to quantify the uncertainty inherent to the
predictions. This issue can be addressed by turning standard frequentist neural
networks into Bayesian neural networks, which are naturally capable of
providing confidence intervals around the estimates. Several methods exist for
training those models. Researchers have focused mostly on parametric
variational inference and sampling-based techniques, which notoriously suffer
from limited approximation power and large computational burden, respectively.
In this work, we use Stein variational gradient descent, a recently proposed
algorithm for approximating intractable distributions that overcomes the
drawbacks of the aforementioned techniques. In particular, we show through
experimental studies on simulated run-to-failure turbofan engine degradation
data that Bayesian deep learning models trained via Stein variational gradient
descent consistently outperform with respect to convergence speed and
predictive performance both the same models trained via parametric variational
inference and their frequentist counterparts trained via backpropagation.
Furthermore, we propose a method to enhance performance based on the
uncertainty information provided by the Bayesian models. We release the source
code at https://github.com/lucadellalib/bdl-rul-svgd.</div><div><a href='http://arxiv.org/abs/2402.01098v1'>2402.01098v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01789v1")'>Deep learning the Hurst parameter of linear fractional processes and
  assessing its reliability</div>
<div id='2401.01789v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T15:42:45Z</div><div>Authors: Dániel Boros, Bálint Csanády, Iván Ivkovic, Lóránt Nagy, András Lukács, László Márkus</div><div style='padding-top: 10px; width: 80ex'>This research explores the reliability of deep learning, specifically Long
Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in
fractional stochastic processes. The study focuses on three types of processes:
fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process,
and linear fractional stable motions (lfsm). The work involves a fast
generation of extensive datasets for fBm and fOU to train the LSTM network on a
large volume of data in a feasible time. The study analyses the accuracy of the
LSTM network's Hurst parameter estimation regarding various performance
measures like RMSE, MAE, MRE, and quantiles of the absolute and relative
errors. It finds that LSTM outperforms the traditional statistical methods in
the case of fBm and fOU processes; however, it has limited accuracy on lfsm
processes. The research also delves into the implications of training length
and valuation sequence length on the LSTM's performance. The methodology is
applied by estimating the Hurst parameter in Li-ion battery degradation data
and obtaining confidence bounds for the estimation. The study concludes that
while deep learning methods show promise in parameter estimation of fractional
processes, their effectiveness is contingent on the process type and the
quality of training data.</div><div><a href='http://arxiv.org/abs/2401.01789v1'>2401.01789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05244v1")'>Reliability Analysis of Complex Systems using Subset Simulations with
  Hamiltonian Neural Networks</div>
<div id='2401.05244v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T16:15:42Z</div><div>Authors: Denny Thaler, Somayajulu L. N. Dhulipala, Franz Bamer, Bernd Markert, Michael D. Shields</div><div style='padding-top: 10px; width: 80ex'>We present a new Subset Simulation approach using Hamiltonian neural
network-based Monte Carlo sampling for reliability analysis. The proposed
strategy combines the superior sampling of the Hamiltonian Monte Carlo method
with computationally efficient gradient evaluations using Hamiltonian neural
networks. This combination is especially advantageous because the neural
network architecture conserves the Hamiltonian, which defines the acceptance
criteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieves
high acceptance rates at low computational cost. Our approach estimates small
failure probabilities using Subset Simulations. However, in low-probability
sample regions, the gradient evaluation is particularly challenging. The
remarkable accuracy of the proposed strategy is demonstrated on different
reliability problems, and its efficiency is compared to the traditional
Hamiltonian Monte Carlo method. We note that this approach can reach its
limitations for gradient estimations in low-probability regions of complex and
high-dimensional distributions. Thus, we propose techniques to improve gradient
prediction in these particular situations and enable accurate estimations of
the probability of failure. The highlight of this study is the reliability
analysis of a system whose parameter distributions must be inferred with
Bayesian inference problems. In such a case, the Hamiltonian Monte Carlo method
requires a full model evaluation for each gradient evaluation and, therefore,
comes at a very high cost. However, using Hamiltonian neural networks in this
framework replaces the expensive model evaluation, resulting in tremendous
improvements in computational efficiency.</div><div><a href='http://arxiv.org/abs/2401.05244v1'>2401.05244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06821v1")'>Surrogate Neural Networks Local Stability for Aircraft Predictive
  Maintenance</div>
<div id='2401.06821v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T21:04:28Z</div><div>Authors: Mélanie Ducoffe, Guillaume Povéda, Audrey Galametz, Ryma Boumazouza, Marion-Cécile Martin, Julien Baris, Derk Daverschot, Eugene O'Higgins</div><div style='padding-top: 10px; width: 80ex'>Surrogate Neural Networks (NN) now routinely serve as substitutes for
computationally demanding simulations (e.g., finite element). They enable
faster analyses in industrial applications e.g., manufacturing processes,
performance assessment. The verification of surrogate models is a critical step
to assess their robustness under different scenarios. We explore the
combination of empirical and formal methods in one NN verification pipeline. We
showcase its efficiency on an industrial use case of aircraft predictive
maintenance. We assess the local stability of surrogate NN designed to predict
the stress sustained by an aircraft part from external loads. Our contribution
lies in the complete verification of the surrogate models that possess a
high-dimensional input and output space, thus accommodating multi-objective
constraints. We also demonstrate the pipeline effectiveness in substantially
decreasing the runtime needed to assess the targeted property.</div><div><a href='http://arxiv.org/abs/2401.06821v1'>2401.06821v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.08700v1")'>Computationally Efficient Optimisation of Elbow-Type Draft Tube Using
  Neural Network Surrogates</div>
<div id='2401.08700v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T14:05:26Z</div><div>Authors: Ante Sikirica, Ivana Lučin, Marta Alvir, Lado Kranjčević, Zoran Čarija</div><div style='padding-top: 10px; width: 80ex'>This study aims to provide a comprehensive assessment of single-objective and
multi-objective optimisation algorithms for the design of an elbow-type draft
tube, as well as to introduce a computationally efficient optimisation
workflow. The proposed workflow leverages deep neural network surrogates
trained on data obtained from numerical simulations. The use of surrogates
allows for a more flexible and faster evaluation of novel designs. The success
history-based adaptive differential evolution with linear reduction and the
multi-objective evolutionary algorithm based on decomposition were identified
as the best-performing algorithms and used to determine the influence of
different objectives in the single-objective optimisation and their combined
impact on the draft tube design in the multi-objective optimisation. The
results for the single-objective algorithm are consistent with those of the
multi-objective algorithm when the objectives are considered separately.
Multi-objective approach, however, should typically be chosen, especially for
computationally inexpensive surrogates. A multi-criteria decision analysis
method was used to obtain optimal multi-objective results, showing an
improvement of 1.5% and 17% for the pressure recovery factor and drag
coefficient, respectively. The difference between the predictions and the
numerical results is less than 0.5% for the pressure recovery factor and 3% for
the drag coefficient. As the demand for renewable energy continues to increase,
the relevance of data-driven optimisation workflows, as discussed in this
study, will become increasingly important, especially in the context of global
sustainability efforts.</div><div><a href='http://arxiv.org/abs/2401.08700v1'>2401.08700v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07692v1")'>Boundary Exploration for Bayesian Optimization With Unknown Physical
  Constraints</div>
<div id='2402.07692v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T14:59:40Z</div><div>Authors: Yunsheng Tian, Ane Zuniga, Xinwei Zhang, Johannes P. Dürholt, Payel Das, Jie Chen, Wojciech Matusik, Mina Konaković Luković</div><div style='padding-top: 10px; width: 80ex'>Bayesian optimization has been successfully applied to optimize black-box
functions where the number of evaluations is severely limited. However, in many
real-world applications, it is hard or impossible to know in advance which
designs are feasible due to some physical or system limitations. These issues
lead to an even more challenging problem of optimizing an unknown function with
unknown constraints. In this paper, we observe that in such scenarios optimal
solution typically lies on the boundary between feasible and infeasible regions
of the design space, making it considerably more difficult than that with
interior optima. Inspired by this observation, we propose BE-CBO, a new
Bayesian optimization method that efficiently explores the boundary between
feasible and infeasible designs. To identify the boundary, we learn the
constraints with an ensemble of neural networks that outperform the standard
Gaussian Processes for capturing complex boundaries. Our method demonstrates
superior performance against state-of-the-art methods through comprehensive
experiments on synthetic and real-world benchmarks.</div><div><a href='http://arxiv.org/abs/2402.07692v1'>2402.07692v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00531v1")'>Preconditioning for Physics-Informed Neural Networks</div>
<div id='2402.00531v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T11:58:28Z</div><div>Authors: Songming Liu, Chang Su, Jiachen Yao, Zhongkai Hao, Hang Su, Youjia Wu, Jun Zhu</div><div style='padding-top: 10px; width: 80ex'>Physics-informed neural networks (PINNs) have shown promise in solving
various partial differential equations (PDEs). However, training pathologies
have negatively affected the convergence and prediction accuracy of PINNs,
which further limits their practical applications. In this paper, we propose to
use condition number as a metric to diagnose and mitigate the pathologies in
PINNs. Inspired by classical numerical analysis, where the condition number
measures sensitivity and stability, we highlight its pivotal role in the
training dynamics of PINNs. We prove theorems to reveal how condition number is
related to both the error control and convergence of PINNs. Subsequently, we
present an algorithm that leverages preconditioning to improve the condition
number. Evaluations of 18 PDE problems showcase the superior performance of our
method. Significantly, in 7 of these problems, our method reduces errors by an
order of magnitude. These empirical findings verify the critical role of the
condition number in PINNs' training.</div><div><a href='http://arxiv.org/abs/2402.00531v1'>2402.00531v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01768v1")'>Enriched Physics-informed Neural Networks for Dynamic
  Poisson-Nernst-Planck Systems</div>
<div id='2402.01768v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T02:57:07Z</div><div>Authors: Xujia Huang, Fajie Wang, Benrong Zhang, Hanqing Liu</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a meshless deep learning algorithm, enriched
physics-informed neural networks (EPINNs), to solve dynamic
Poisson-Nernst-Planck (PNP) equations with strong coupling and nonlinear
characteristics. The EPINNs takes the traditional physics-informed neural
networks as the foundation framework, and adds the adaptive loss weight to
balance the loss functions, which automatically assigns the weights of losses
by updating the parameters in each iteration based on the maximum likelihood
estimate. The resampling strategy is employed in the EPINNs to accelerate the
convergence of loss function. Meanwhile, the GPU parallel computing technique
is adopted to accelerate the solving process. Four examples are provided to
demonstrate the validity and effectiveness of the proposed method. Numerical
results indicate that the new method has better applicability than traditional
numerical methods in solving such coupled nonlinear systems. More importantly,
the EPINNs is more accurate, stable, and fast than the traditional
physics-informed neural networks. This work provides a simple and
high-performance numerical tool for addressing PNPs with arbitrary boundary
shapes and boundary conditions.</div><div><a href='http://arxiv.org/abs/2402.01768v1'>2402.01768v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12650v1")'>Adaptive Multilevel Neural Networks for Parametric PDEs with Error
  Estimation</div>
<div id='2403.12650v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T11:34:40Z</div><div>Authors: Janina E. Schütte, Martin Eigel</div><div style='padding-top: 10px; width: 80ex'>To solve high-dimensional parameter-dependent partial differential equations
(pPDEs), a neural network architecture is presented. It is constructed to map
parameters of the model data to corresponding finite element solutions. To
improve training efficiency and to enable control of the approximation error,
the network mimics an adaptive finite element method (AFEM). It outputs a
coarse grid solution and a series of corrections as produced in an AFEM,
allowing a tracking of the error decay over successive layers of the network.
The observed errors are measured by a reliable residual based a posteriori
error estimator, enabling the reduction to only few parameters for the
approximation in the output of the network. This leads to a problem adapted
representation of the solution on locally refined grids. Furthermore, each
solution of the AFEM is discretized in a hierarchical basis. For the
architecture, convolutional neural networks (CNNs) are chosen. The hierarchical
basis then allows to handle sparse images for finely discretized meshes.
Additionally, as corrections on finer levels decrease in amplitude, i.e.,
importance for the overall approximation, the accuracy of the network
approximation is allowed to decrease successively. This can either be
incorporated in the number of generated high fidelity samples used for training
or the size of the network components responsible for the fine grid outputs.
The architecture is described and preliminary numerical examples are presented.</div><div><a href='http://arxiv.org/abs/2403.12650v1'>2403.12650v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08544v1")'>N-Adaptive Ritz Method: A Neural Network Enriched Partition of Unity for
  Boundary Value Problems</div>
<div id='2401.08544v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T18:11:14Z</div><div>Authors: Jonghyuk Baek, Yanran Wang, J. S. Chen</div><div style='padding-top: 10px; width: 80ex'>Conventional finite element methods are known to be tedious in adaptive
refinements due to their conformal regularity requirements. Further, the
enrichment functions for adaptive refinements are often not readily available
in general applications. This work introduces a novel neural network-enriched
Partition of Unity (NN-PU) approach for solving boundary value problems via
artificial neural networks with a potential energy-based loss function
minimization. The flexibility and adaptivity of the NN function space are
utilized to capture complex solution patterns that the conventional Galerkin
methods fail to capture. The NN enrichment is constructed by combining
pre-trained feature-encoded NN blocks with an additional untrained NN block.
The pre-trained NN blocks learn specific local features during the offline
stage, enabling efficient enrichment of the approximation space during the
online stage through the Ritz-type energy minimization. The NN enrichment is
introduced under the Partition of Unity (PU) framework, ensuring convergence of
the proposed method. The proposed NN-PU approximation and feature-encoded
transfer learning forms an adaptive approximation framework, termed the
neural-refinement (n-refinement), for solving boundary value problems.
Demonstrated by solving various elasticity problems, the proposed method offers
accurate solutions while notably reducing the computational cost compared to
the conventional adaptive refinement in the mesh-based methods.</div><div><a href='http://arxiv.org/abs/2401.08544v1'>2401.08544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17172v1")'>Learning Domain-Independent Green's Function For Elliptic Partial
  Differential Equations</div>
<div id='2401.17172v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:00:22Z</div><div>Authors: Pawan Negi, Maggie Cheng, Mahesh Krishnamurthy, Wenjun Ying, Shuwang Li</div><div style='padding-top: 10px; width: 80ex'>Green's function characterizes a partial differential equation (PDE) and maps
its solution in the entire domain as integrals. Finding the analytical form of
Green's function is a non-trivial exercise, especially for a PDE defined on a
complex domain or a PDE with variable coefficients. In this paper, we propose a
novel boundary integral network to learn the domain-independent Green's
function, referred to as BIN-G. We evaluate the Green's function in the BIN-G
using a radial basis function (RBF) kernel-based neural network. We train the
BIN-G by minimizing the residual of the PDE and the mean squared errors of the
solutions to the boundary integral equations for prescribed test functions. By
leveraging the symmetry of the Green's function and controlling refinements of
the RBF kernel near the singularity of the Green function, we demonstrate that
our numerical scheme enables fast training and accurate evaluation of the
Green's function for PDEs with variable coefficients. The learned Green's
function is independent of the domain geometries, forcing terms, and boundary
conditions in the boundary integral formulation. Numerical experiments verify
the desired properties of the method and the expected accuracy for the
two-dimensional Poisson and Helmholtz equations with variable coefficients.</div><div><a href='http://arxiv.org/abs/2401.17172v1'>2401.17172v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07003v1")'>Deep Neural Network Solutions for Oscillatory Fredholm Integral
  Equations</div>
<div id='2401.07003v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-13T07:26:47Z</div><div>Authors: Jie Jiang, Yuesheng Xu</div><div style='padding-top: 10px; width: 80ex'>We studied the use of deep neural networks (DNNs) in the numerical solution
of the oscillatory Fredholm integral equation of the second kind. It is known
that the solution of the equation exhibits certain oscillatory behaviors due to
the oscillation of the kernel. It was pointed out recently that standard DNNs
favour low frequency functions, and as a result, they often produce poor
approximation for functions containing high frequency components. We addressed
this issue in this study. We first developed a numerical method for solving the
equation with DNNs as an approximate solution by designing a numerical
quadrature that tailors to computing oscillatory integrals involving DNNs. We
proved that the error of the DNN approximate solution of the equation is
bounded by the training loss and the quadrature error. We then proposed a
multi-grade deep learning (MGDL) model to overcome the spectral bias issue of
neural networks. Numerical experiments demonstrate that the MGDL model is
effective in extracting multiscale information of the oscillatory solution and
overcoming the spectral bias issue from which a standard DNN model suffers.</div><div><a href='http://arxiv.org/abs/2401.07003v1'>2401.07003v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06656v1")'>Neural Networks for Singular Perturbations</div>
<div id='2401.06656v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T16:02:18Z</div><div>Authors: Joost A. A. Opschoor, Christoph Schwab, Christos Xenophontos</div><div style='padding-top: 10px; width: 80ex'>We prove deep neural network (DNN for short) expressivity rate bounds for
solution sets of a model class of singularly perturbed, elliptic two-point
boundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. We
assume that the given source term and reaction coefficient are analytic in
$[-1,1]$.
  We establish expression rate bounds in Sobolev norms in terms of the NN size
which are uniform with respect to the singular perturbation parameter for
several classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and
$\tanh$- and sigmoid-activated NNs. The latter activations can represent
``exponential boundary layer solution features'' explicitly, in the last hidden
layer of the DNN, i.e. in a shallow subnetwork, and afford improved robust
expression rate bounds in terms of the NN size.
  We prove that all DNN architectures allow robust exponential solution
expression in so-called `energy' as well as in `balanced' Sobolev norms, for
analytic input data.</div><div><a href='http://arxiv.org/abs/2401.06656v1'>2401.06656v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13412v1")'>Scaling physics-informed hard constraints with mixture-of-experts</div>
<div id='2402.13412v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T22:45:00Z</div><div>Authors: Nithin Chalapathi, Yiheng Du, Aditi Krishnapriyan</div><div style='padding-top: 10px; width: 80ex'>Imposing known physical constraints, such as conservation laws, during neural
network training introduces an inductive bias that can improve accuracy,
reliability, convergence, and data efficiency for modeling physical dynamics.
While such constraints can be softly imposed via loss function penalties,
recent advancements in differentiable physics and optimization improve
performance by incorporating PDE-constrained optimization as individual layers
in neural networks. This enables a stricter adherence to physical constraints.
However, imposing hard constraints significantly increases computational and
memory costs, especially for complex dynamical systems. This is because it
requires solving an optimization problem over a large number of points in a
mesh, representing spatial and temporal discretizations, which greatly
increases the complexity of the constraint. To address this challenge, we
develop a scalable approach to enforce hard physical constraints using
Mixture-of-Experts (MoE), which can be used with any neural network
architecture. Our approach imposes the constraint over smaller decomposed
domains, each of which is solved by an "expert" through differentiable
optimization. During training, each expert independently performs a localized
backpropagation step by leveraging the implicit function theorem; the
independence of each expert allows for parallelization across multiple GPUs.
Compared to standard differentiable optimization, our scalable approach
achieves greater accuracy in the neural PDE solver setting for predicting the
dynamics of challenging non-linear systems. We also improve training stability
and require significantly less computation time during both training and
inference stages.</div><div><a href='http://arxiv.org/abs/2402.13412v1'>2402.13412v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11126v1")'>Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning
  (PIML) Methods: Towards Robust Metrics</div>
<div id='2402.11126v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T23:21:40Z</div><div>Authors: Michael Penwarden, Houman Owhadi, Robert M. Kirby</div><div style='padding-top: 10px; width: 80ex'>Physics-informed machine learning (PIML) as a means of solving partial
differential equations (PDE) has garnered much attention in the Computational
Science and Engineering (CS&amp;E) world. This topic encompasses a broad array of
methods and models aimed at solving a single or a collection of PDE problems,
called multitask learning. PIML is characterized by the incorporation of
physical laws into the training process of machine learning models in lieu of
large data when solving PDE problems. Despite the overall success of this
collection of methods, it remains incredibly difficult to analyze, benchmark,
and generally compare one approach to another. Using Kolmogorov n-widths as a
measure of effectiveness of approximating functions, we judiciously apply this
metric in the comparison of various multitask PIML architectures. We compute
lower accuracy bounds and analyze the model's learned basis functions on
various PDE problems. This is the first objective metric for comparing
multitask PIML architectures and helps remove uncertainty in model validation
from selective sampling and overfitting. We also identify avenues of
improvement for model architectures, such as the choice of activation function,
which can drastically affect model generalization to "worst-case" scenarios,
which is not observed when reporting task-specific errors. We also incorporate
this metric into the optimization process through regularization, which
improves the models' generalizability over the multitask PDE problem.</div><div><a href='http://arxiv.org/abs/2402.11126v1'>2402.11126v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03492v1")'>Neural Networks with Kernel-Weighted Corrective Residuals for Solving
  Partial Differential Equations</div>
<div id='2401.03492v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T14:09:42Z</div><div>Authors: Carlos Mora, Amin Yousefpour, Shirin Hosseinmardi, Ramin Bostanabad</div><div style='padding-top: 10px; width: 80ex'>Physics-informed machine learning (PIML) has emerged as a promising
alternative to conventional numerical methods for solving partial differential
equations (PDEs). PIML models are increasingly built via deep neural networks
(NNs) whose architecture and training process are designed such that the
network satisfies the PDE system. While such PIML models have substantially
advanced over the past few years, their performance is still very sensitive to
the NN's architecture and loss function. Motivated by this limitation, we
introduce kernel-weighted Corrective Residuals (CoRes) to integrate the
strengths of kernel methods and deep NNs for solving nonlinear PDE systems. To
achieve this integration, we design a modular and robust framework which
consistently outperforms competing methods in solving a broad range of
benchmark problems. This performance improvement has a theoretical
justification and is particularly attractive since we simplify the training
process while negligibly increasing the inference costs. Additionally, our
studies on solving multiple PDEs indicate that kernel-weighted CoRes
considerably decrease the sensitivity of NNs to factors such as random
initialization, architecture type, and choice of optimizer. We believe our
findings have the potential to spark a renewed interest in leveraging kernel
methods for solving PDEs.</div><div><a href='http://arxiv.org/abs/2401.03492v1'>2401.03492v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12764v1")'>Neural Parameter Regression for Explicit Representations of PDE Solution
  Operators</div>
<div id='2403.12764v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T14:30:56Z</div><div>Authors: Konrad Mundinger, Max Zimmer, Sebastian Pokutta</div><div style='padding-top: 10px; width: 80ex'>We introduce Neural Parameter Regression (NPR), a novel framework
specifically developed for learning solution operators in Partial Differential
Equations (PDEs). Tailored for operator learning, this approach surpasses
traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural
Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN)
parameters. By parametrizing each solution based on specific initial
conditions, it effectively approximates a mapping between function spaces. Our
method enhances parameter efficiency by incorporating low-rank matrices,
thereby boosting computational efficiency and scalability. The framework shows
remarkable adaptability to new initial and boundary conditions, allowing for
rapid fine-tuning and inference, even in cases of out-of-distribution examples.</div><div><a href='http://arxiv.org/abs/2403.12764v1'>2403.12764v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17232v1")'>Two-scale Neural Networks for Partial Differential Equations with Small
  Parameters</div>
<div id='2402.17232v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T05:57:45Z</div><div>Authors: Qiao Zhuang, Chris Ziyi Yao, Zhongqiang Zhang, George Em Karniadakis</div><div style='padding-top: 10px; width: 80ex'>We propose a two-scale neural network method for solving partial differential
equations (PDEs) with small parameters using physics-informed neural networks
(PINNs). We directly incorporate the small parameters into the architecture of
neural networks. The proposed method enables solving PDEs with small parameters
in a simple fashion, without adding Fourier features or other computationally
taxing searches of truncation parameters. Various numerical examples
demonstrate reasonable accuracy in capturing features of large derivatives in
the solutions caused by small parameters.</div><div><a href='http://arxiv.org/abs/2402.17232v1'>2402.17232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04883v1")'>Learning Traveling Solitary Waves Using Separable Gaussian Neural
  Networks</div>
<div id='2403.04883v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T20:16:18Z</div><div>Authors: Siyuan Xing, Efstathios G. Charalampidis</div><div style='padding-top: 10px; width: 80ex'>In this paper, we apply a machine-learning approach to learn traveling
solitary waves across various families of partial differential equations
(PDEs). Our approach integrates a novel interpretable neural network (NN)
architecture, called Separable Gaussian Neural Networks (SGNN) into the
framework of Physics-Informed Neural Networks (PINNs). Unlike the traditional
PINNs that treat spatial and temporal data as independent inputs, the present
method leverages wave characteristics to transform data into the so-called
co-traveling wave frame. This adaptation effectively addresses the issue of
propagation failure in PINNs when applied to large computational domains. Here,
the SGNN architecture demonstrates robust approximation capabilities for
single-peakon, multi-peakon, and stationary solutions within the
(1+1)-dimensional, $b$-family of PDEs. In addition, we expand our
investigations, and explore not only peakon solutions in the $ab$-family but
also compacton solutions in (2+1)-dimensional, Rosenau-Hyman family of PDEs. A
comparative analysis with MLP reveals that SGNN achieves comparable accuracy
with fewer than a tenth of the neurons, underscoring its efficiency and
potential for broader application in solving complex nonlinear PDEs.</div><div><a href='http://arxiv.org/abs/2403.04883v1'>2403.04883v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15656v2")'>Learning Semilinear Neural Operators : A Unified Recursive Framework For
  Prediction And Data Assimilation</div>
<div id='2402.15656v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T00:10:51Z</div><div>Authors: Ashutosh Singh, Ricardo Augusto Borsoi, Deniz Erdogmus, Tales Imbiriba</div><div style='padding-top: 10px; width: 80ex'>Recent advances in the theory of Neural Operators (NOs) have enabled fast and
accurate computation of the solutions to complex systems described by partial
differential equations (PDEs). Despite their great success, current NO-based
solutions face important challenges when dealing with spatio-temporal PDEs over
long time scales. Specifically, the current theory of NOs does not present a
systematic framework to perform data assimilation and efficiently correct the
evolution of PDE solutions over time based on sparsely sampled noisy
measurements. In this paper, we propose a learning-based state-space approach
to compute the solution operators to infinite-dimensional semilinear PDEs.
Exploiting the structure of semilinear PDEs and the theory of nonlinear
observers in function spaces, we develop a flexible recursive method that
allows for both prediction and data assimilation by combining prediction and
correction operations. The proposed framework is capable of producing fast and
accurate predictions over long time horizons, dealing with irregularly sampled
noisy measurements to correct the solution, and benefits from the decoupling
between the spatial and temporal dynamics of this class of PDEs. We show
through experiments on the Kuramoto-Sivashinsky, Navier-Stokes and Korteweg-de
Vries equations that the proposed model is robust to noise and can leverage
arbitrary amounts of measurements to correct its prediction over a long time
horizon with little computational overhead.</div><div><a href='http://arxiv.org/abs/2402.15656v2'>2402.15656v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05211v1")'>Error estimation for physics-informed neural networks with implicit
  Runge-Kutta methods</div>
<div id='2401.05211v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T15:18:56Z</div><div>Authors: Jochen Stiasny, Spyros Chatzivasileiadis</div><div style='padding-top: 10px; width: 80ex'>The ability to accurately approximate trajectories of dynamical systems
enables their analysis, prediction, and control. Neural network (NN)-based
approximations have attracted significant interest due to fast evaluation with
good accuracy over long integration time steps. In contrast to established
numerical approximation schemes such as Runge-Kutta methods, the estimation of
the error of the NN-based approximations proves to be difficult. In this work,
we propose to use the NN's predictions in a high-order implicit Runge-Kutta
(IRK) method. The residuals in the implicit system of equations can be related
to the NN's prediction error, hence, we can provide an error estimate at
several points along a trajectory. We find that this error estimate highly
correlates with the NN's prediction error and that increasing the order of the
IRK method improves this estimate. We demonstrate this estimation methodology
for Physics-Informed Neural Network (PINNs) on the logistic equation as an
illustrative example and then apply it to a four-state electric generator model
that is regularly used in power system modelling.</div><div><a href='http://arxiv.org/abs/2401.05211v1'>2401.05211v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03028v1")'>Functional SDE approximation inspired by a deep operator network
  architecture</div>
<div id='2402.03028v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:12:35Z</div><div>Authors: Martin Eigel, Charles Miranda</div><div style='padding-top: 10px; width: 80ex'>A novel approach to approximate solutions of Stochastic Differential
Equations (SDEs) by Deep Neural Networks is derived and analysed. The
architecture is inspired by the notion of Deep Operator Networks (DeepONets),
which is based on operator learning in function spaces in terms of a reduced
basis also represented in the network. In our setting, we make use of a
polynomial chaos expansion (PCE) of stochastic processes and call the
corresponding architecture SDEONet. The PCE has been used extensively in the
area of uncertainty quantification (UQ) with parametric partial differential
equations. This however is not the case with SDE, where classical sampling
methods dominate and functional approaches are seen rarely. A main challenge
with truncated PCEs occurs due to the drastic growth of the number of
components with respect to the maximum polynomial degree and the number of
basis elements. The proposed SDEONet architecture aims to alleviate the issue
of exponential complexity by learning an optimal sparse truncation of the
Wiener chaos expansion. A complete convergence and complexity analysis is
presented, making use of recent Neural Network approximation results. Numerical
experiments illustrate the promising performance of the suggested approach in
1D and higher dimensions.</div><div><a href='http://arxiv.org/abs/2402.03028v1'>2402.03028v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00435v1")'>A practical existence theorem for reduced order models based on
  convolutional autoencoders</div>
<div id='2402.00435v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T09:01:58Z</div><div>Authors: Nicola Rares Franco, Simone Brugiapaglia</div><div style='padding-top: 10px; width: 80ex'>In recent years, deep learning has gained increasing popularity in the fields
of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM),
providing domain practitioners with new powerful data-driven techniques such as
Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator
Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context,
deep autoencoders based on Convolutional Neural Networks (CNNs) have proven
extremely effective, outperforming established techniques, such as the reduced
basis method, when dealing with complex nonlinear problems. However, despite
the empirical success of CNN-based autoencoders, there are only a few
theoretical results supporting these architectures, usually stated in the form
of universal approximation theorems. In particular, although the existing
literature provides users with guidelines for designing convolutional
autoencoders, the subsequent challenge of learning the latent features has been
barely investigated. Furthermore, many practical questions remain unanswered,
e.g., the number of snapshots needed for convergence or the neural network
training strategy. In this work, using recent techniques from sparse
high-dimensional function approximation, we fill some of these gaps by
providing a new practical existence theorem for CNN-based autoencoders when the
parameter-to-solution map is holomorphic. This regularity assumption arises in
many relevant classes of parametric PDEs, such as the parametric diffusion
equation, for which we discuss an explicit application of our general theory.</div><div><a href='http://arxiv.org/abs/2402.00435v1'>2402.00435v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12806v2")'>Binary structured physics-informed neural networks for solving equations
  with rapidly changing solutions</div>
<div id='2401.12806v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T14:37:51Z</div><div>Authors: Yanzhi Liu, Ruifan Wu, Ying Jiang</div><div style='padding-top: 10px; width: 80ex'>Physics-informed neural networks (PINNs), rooted in deep learning, have
emerged as a promising approach for solving partial differential equations
(PDEs). By embedding the physical information described by PDEs into
feedforward neural networks, PINNs are trained as surrogate models to
approximate solutions without the need for label data. Nevertheless, even
though PINNs have shown remarkable performance, they can face difficulties,
especially when dealing with equations featuring rapidly changing solutions.
These difficulties encompass slow convergence, susceptibility to becoming
trapped in local minima, and reduced solution accuracy. To address these
issues, we propose a binary structured physics-informed neural network (BsPINN)
framework, which employs binary structured neural network (BsNN) as the neural
network component. By leveraging a binary structure that reduces inter-neuron
connections compared to fully connected neural networks, BsPINNs excel in
capturing the local features of solutions more effectively and efficiently.
These features are particularly crucial for learning the rapidly changing in
the nature of solutions. In a series of numerical experiments solving Burgers
equation, Euler equation, Helmholtz equation, and high-dimension Poisson
equation, BsPINNs exhibit superior convergence speed and heightened accuracy
compared to PINNs. From these experiments, we discover that BsPINNs resolve the
issues caused by increased hidden layers in PINNs resulting in over-smoothing,
and prevent the decline in accuracy due to non-smoothness of PDEs solutions.</div><div><a href='http://arxiv.org/abs/2401.12806v2'>2401.12806v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07888v1")'>Multifidelity domain decomposition-based physics-informed neural
  networks for time-dependent problems</div>
<div id='2401.07888v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T18:32:53Z</div><div>Authors: Alexander Heinlein, Amanda A. Howard, Damien Beecroft, Panos Stinis</div><div style='padding-top: 10px; width: 80ex'>Multiscale problems are challenging for neural network-based discretizations
of differential equations, such as physics-informed neural networks (PINNs).
This can be (partly) attributed to the so-called spectral bias of neural
networks. To improve the performance of PINNs for time-dependent problems, a
combination of multifidelity stacking PINNs and domain decomposition-based
finite basis PINNs are employed. In particular, to learn the high-fidelity part
of the multifidelity model, a domain decomposition in time is employed. The
performance is investigated for a pendulum and a two-frequency problem as well
as the Allen-Cahn equation. It can be observed that the domain decomposition
approach clearly improves the PINN and stacking PINN approaches.</div><div><a href='http://arxiv.org/abs/2401.07888v1'>2401.07888v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01783v3")'>Approximating Numerical Fluxes Using Fourier Neural Operators for
  Hyperbolic Conservation Laws</div>
<div id='2401.01783v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T15:16:25Z</div><div>Authors: Taeyoung Kim, Myungjoo Kang</div><div style='padding-top: 10px; width: 80ex'>Traditionally, classical numerical schemes have been employed to solve
partial differential equations (PDEs) using computational methods. Recently,
neural network-based methods have emerged. Despite these advancements, neural
network-based methods, such as physics-informed neural networks (PINNs) and
neural operators, exhibit deficiencies in robustness and generalization. To
address these issues, numerous studies have integrated classical numerical
frameworks with machine learning techniques, incorporating neural networks into
parts of traditional numerical methods. In this study, we focus on hyperbolic
conservation laws by replacing traditional numerical fluxes with neural
operators. To this end, we developed loss functions inspired by established
numerical schemes related to conservation laws and approximated numerical
fluxes using Fourier neural operators (FNOs). Our experiments demonstrated that
our approach combines the strengths of both traditional numerical schemes and
FNOs, outperforming standard FNO methods in several respects. For instance, we
demonstrate that our method is robust, has resolution invariance, and is
feasible as a data-driven method. In particular, our method can make continuous
predictions over time and exhibits superior generalization capabilities with
out-of-distribution (OOD) samples, which are challenges that existing neural
operator methods encounter.</div><div><a href='http://arxiv.org/abs/2401.01783v3'>2401.01783v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02363v1")'>Integration of physics-informed operator learning and finite element
  method for parametric learning of partial differential equations</div>
<div id='2401.02363v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T17:01:54Z</div><div>Authors: Shahed Rezaei, Ahmad Moeineddin, Michael Kaliske, Markus Apel</div><div style='padding-top: 10px; width: 80ex'>We present a method that employs physics-informed deep learning techniques
for parametrically solving partial differential equations. The focus is on the
steady-state heat equations within heterogeneous solids exhibiting significant
phase contrast. Similar equations manifest in diverse applications like
chemical diffusion, electrostatics, and Darcy flow. The neural network aims to
establish the link between the complex thermal conductivity profiles and
temperature distributions, as well as heat flux components within the
microstructure, under fixed boundary conditions. A distinctive aspect is our
independence from classical solvers like finite element methods for data. A
noteworthy contribution lies in our novel approach to defining the loss
function, based on the discretized weak form of the governing equation. This
not only reduces the required order of derivatives but also eliminates the need
for automatic differentiation in the construction of loss terms, accepting
potential numerical errors from the chosen discretization method. As a result,
the loss function in this work is an algebraic equation that significantly
enhances training efficiency. We benchmark our methodology against the standard
finite element method, demonstrating accurate yet faster predictions using the
trained neural network for temperature and flux profiles. We also show higher
accuracy by using the proposed method compared to purely data-driven approaches
for unforeseen scenarios.</div><div><a href='http://arxiv.org/abs/2401.02363v1'>2401.02363v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12503v2")'>PARCv2: Physics-aware Recurrent Convolutional Neural Networks for
  Spatiotemporal Dynamics Modeling</div>
<div id='2402.12503v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T20:11:46Z</div><div>Authors: Phong C. H. Nguyen, Xinlun Cheng, Shahab Azarfar, Pradeep Seshadri, Yen T. Nguyen, Munho Kim, Sanghun Choi, H. S. Udaykumar, Stephen Baek</div><div style='padding-top: 10px; width: 80ex'>Modeling unsteady, fast transient, and advection-dominated physics problems
is a pressing challenge for physics-aware deep learning (PADL). The physics of
complex systems is governed by large systems of partial differential equations
(PDEs) and ancillary constitutive models with nonlinear structures, as well as
evolving state fields exhibiting sharp gradients and rapidly deforming material
interfaces. Here, we investigate an inductive bias approach that is versatile
and generalizable to model generic nonlinear field evolution problems. Our
study focuses on the recent physics-aware recurrent convolutions (PARC), which
incorporates a differentiator-integrator architecture that inductively models
the spatiotemporal dynamics of generic physical systems. We extend the
capabilities of PARC to simulate unsteady, transient, and advection-dominant
systems. The extended model, referred to as PARCv2, is equipped with
differential operators to model advection-reaction-diffusion equations, as well
as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested
on both standard benchmark problems in fluid dynamics, namely Burgers and
Navier-Stokes equations, and then applied to more complex shock-induced
reaction problems in energetic materials. We evaluate the behavior of PARCv2 in
comparison to other physics-informed and learning bias models and demonstrate
its potential to model unsteady and advection-dominant dynamics regimes.</div><div><a href='http://arxiv.org/abs/2402.12503v2'>2402.12503v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12365v1")'>Universal Physics Transformers</div>
<div id='2402.12365v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:52:13Z</div><div>Authors: Benedikt Alkin, Andreas Fürst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter</div><div style='padding-top: 10px; width: 80ex'>Deep neural network based surrogates for partial differential equations have
recently gained increased interest. However, akin to their numerical
counterparts, different techniques are used across applications, even if the
underlying dynamics of the systems are similar. A prominent example is the
Lagrangian and Eulerian specification in computational fluid dynamics, posing a
challenge for neural networks to effectively model particle- as opposed to
grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a
novel learning paradigm which models a wide range of spatio-temporal problems -
both for Lagrangian and Eulerian discretization schemes. UPTs operate without
grid- or particle-based latent structures, enabling flexibility across meshes
and particles. UPTs efficiently propagate dynamics in the latent space,
emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for
queries of the latent space representation at any point in space-time. We
demonstrate the efficacy of UPTs in mesh-based fluid simulations, steady-state
Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.
Project page: https://ml-jku.github.io/UPT</div><div><a href='http://arxiv.org/abs/2402.12365v1'>2402.12365v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17853v1")'>Latent Neural PDE Solver: a reduced-order modelling framework for
  partial differential equations</div>
<div id='2402.17853v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T19:36:27Z</div><div>Authors: Zijie Li, Saurabh Patil, Francis Ogoke, Dule Shu, Wilson Zhen, Michael Schneier, John R. Buchanan, Jr., Amir Barati Farimani</div><div style='padding-top: 10px; width: 80ex'>Neural networks have shown promising potential in accelerating the numerical
simulation of systems governed by partial differential equations (PDEs).
Different from many existing neural network surrogates operating on
high-dimensional discretized fields, we propose to learn the dynamics of the
system in the latent space with much coarser discretizations. In our proposed
framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first
trained to project the full-order representation of the system onto the
mesh-reduced space, then a temporal model is trained to predict the future
state in this mesh-reduced space. This reduction process simplifies the
training of the temporal model by greatly reducing the computational cost
accompanying a fine discretization. We study the capability of the proposed
framework and several other popular neural PDE solvers on various types of
systems including single-phase and multi-phase flows along with varying system
parameters. We showcase that it has competitive accuracy and efficiency
compared to the neural PDE solver that operates on full-order space.</div><div><a href='http://arxiv.org/abs/2402.17853v1'>2402.17853v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15636v1")'>Smooth and Sparse Latent Dynamics in Operator Learning with Jerk
  Regularization</div>
<div id='2402.15636v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T22:38:45Z</div><div>Authors: Xiaoyu Xie, Saviz Mowlavi, Mouhacine Benosman</div><div style='padding-top: 10px; width: 80ex'>Spatiotemporal modeling is critical for understanding complex systems across
various scientific and engineering disciplines, but governing equations are
often not fully known or computationally intractable due to inherent system
complexity. Data-driven reduced-order models (ROMs) offer a promising approach
for fast and accurate spatiotemporal forecasting by computing solutions in a
compressed latent space. However, these models often neglect temporal
correlations between consecutive snapshots when constructing the latent space,
leading to suboptimal compression, jagged latent trajectories, and limited
extrapolation ability over time. To address these issues, this paper introduces
a continuous operator learning framework that incorporates jerk regularization
into the learning of the compressed latent space. This jerk regularization
promotes smoothness and sparsity of latent space dynamics, which not only
yields enhanced accuracy and convergence speed but also helps identify
intrinsic latent space coordinates. Consisting of an implicit neural
representation (INR)-based autoencoder and a neural ODE latent dynamics model,
the framework allows for inference at any desired spatial or temporal
resolution. The effectiveness of this framework is demonstrated through a
two-dimensional unsteady flow problem governed by the Navier-Stokes equations,
highlighting its potential to expedite high-fidelity simulations in various
scientific and engineering applications.</div><div><a href='http://arxiv.org/abs/2402.15636v1'>2402.15636v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14591v5")'>Ricci flow-guided autoencoders in learning time-dependent dynamics</div>
<div id='2401.14591v5' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T01:36:48Z</div><div>Authors: Andrew Gracyk</div><div style='padding-top: 10px; width: 80ex'>We present a manifold-based autoencoder method for learning nonlinear
dynamics in time, notably partial differential equations (PDEs), in which the
manifold latent space evolves according to Ricci flow. This can be accomplished
by simulating Ricci flow in a physics-informed setting, and manifold quantities
can be matched so that Ricci flow is empirically achieved. With our
methodology, the manifold is learned as part of the training procedure, so
ideal geometries may be discerned, while the evolution simultaneously induces a
more accommodating latent representation over static methods. We present our
method on a range of numerical experiments consisting of PDEs that encompass
desirable characteristics such as periodicity and randomness, remarking error
on in-distribution and extrapolation scenarios.</div><div><a href='http://arxiv.org/abs/2401.14591v5'>2401.14591v5</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12335v1")'>Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical
  Systems</div>
<div id='2403.12335v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T00:48:25Z</div><div>Authors: Indranil Nayak, Debdipta Goswami, Mrinal Kumar, Fernando Teixeira</div><div style='padding-top: 10px; width: 80ex'>Absence of sufficiently high-quality data often poses a key challenge in
data-driven modeling of high-dimensional spatio-temporal dynamical systems.
Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks
(DNNs), the dimension reduction capabilities of autoencoders, and the spectral
properties of the Koopman operator to learn a reduced-order feature space with
simpler, linear dynamics. However, the effectiveness of KAEs is hindered by
limited and noisy training datasets, leading to poor generalizability. To
address this, we introduce the Temporally-Consistent Koopman Autoencoder
(tcKAE), designed to generate accurate long-term predictions even with
constrained and noisy training data. This is achieved through a consistency
regularization term that enforces prediction coherence across different time
steps, thus enhancing the robustness and generalizability of tcKAE over
existing models. We provide analytical justification for this approach based on
Koopman spectral theory and empirically demonstrate tcKAE's superior
performance over state-of-the-art KAE models across a variety of test cases,
including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea
surface temperature data.</div><div><a href='http://arxiv.org/abs/2403.12335v1'>2403.12335v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07594v1")'>Foundational Inference Models for Dynamical Systems</div>
<div id='2402.07594v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:48:54Z</div><div>Authors: Patrick Seifner, Kostadin Cvejoski, Ramses J. Sanchez</div><div style='padding-top: 10px; width: 80ex'>Ordinary differential equations (ODEs) underlie dynamical systems which serve
as models for a vast number of natural and social phenomena. Yet inferring the
ODE that best describes a set of noisy observations on one such phenomenon can
be remarkably challenging, and the models available to achieve it tend to be
highly specialized and complex too. In this work we propose a novel supervised
learning framework for zero-shot inference of ODEs from noisy data. We first
generate large datasets of one-dimensional ODEs, by sampling distributions over
the space of initial conditions, and the space of vector fields defining them.
We then learn neural maps between noisy observations on the solutions of these
equations, and their corresponding initial condition and vector fields. The
resulting models, which we call foundational inference models (FIM), can be (i)
copied and matched along the time dimension to increase their resolution; and
(ii) copied and composed to build inference models of any dimensionality,
without the need of any finetuning. We use FIM to model both ground-truth
dynamical systems of different dimensionalities and empirical time series data
in a zero-shot fashion, and outperform state-of-the-art models which are
finetuned to these systems. Our (pretrained) FIMs are available online</div><div><a href='http://arxiv.org/abs/2402.07594v1'>2402.07594v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18377v1")'>Out-of-Domain Generalization in Dynamical Systems Reconstruction</div>
<div id='2402.18377v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T14:52:58Z</div><div>Authors: Niclas Göring, Florian Hess, Manuel Brenner, Zahra Monfared, Daniel Durstewitz</div><div style='padding-top: 10px; width: 80ex'>In science we are interested in finding the governing equations, the
dynamical rules, underlying empirical phenomena. While traditionally scientific
models are derived through cycles of human insight and experimentation,
recently deep learning (DL) techniques have been advanced to reconstruct
dynamical systems (DS) directly from time series data. State-of-the-art
dynamical systems reconstruction (DSR) methods show promise in capturing
invariant and long-term properties of observed DS, but their ability to
generalize to unobserved domains remains an open challenge. Yet, this is a
crucial property we would expect from any viable scientific theory. In this
work, we provide a formal framework that addresses generalization in DSR. We
explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly
differs from OODG considered elsewhere in machine learning. We introduce
mathematical notions based on topological concepts and ergodic theory to
formalize the idea of learnability of a DSR model. We formally prove that
black-box DL techniques, without adequate structural priors, generally will not
be able to learn a generalizing DSR model. We also show this empirically,
considering major classes of DSR algorithms proposed so far, and illustrate
where and why they fail to generalize across the whole phase space. Our study
provides the first comprehensive mathematical treatment of OODG in DSR, and
gives a deeper conceptual understanding of where the fundamental problems in
OODG lie and how they could possibly be addressed in practice.</div><div><a href='http://arxiv.org/abs/2402.18377v1'>2402.18377v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14475v1")'>DynGMA: a robust approach for learning stochastic differential equations
  from data</div>
<div id='2402.14475v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T12:09:52Z</div><div>Authors: Aiqing Zhu, Qianxiao Li</div><div style='padding-top: 10px; width: 80ex'>Learning unknown stochastic differential equations (SDEs) from observed data
is a significant and challenging task with applications in various fields.
Current approaches often use neural networks to represent drift and diffusion
functions, and construct likelihood-based loss by approximating the transition
density to train these networks. However, these methods often rely on one-step
stochastic numerical schemes, necessitating data with sufficiently high time
resolution. In this paper, we introduce novel approximations to the transition
density of the parameterized SDE: a Gaussian density approximation inspired by
the random perturbation theory of dynamical systems, and its extension, the
dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust
density approximation, our method exhibits superior accuracy compared to
baseline methods in learning the fully unknown drift and diffusion functions
and computing the invariant distribution from trajectory data. And it is
capable of handling trajectory data with low time resolution and variable, even
uncontrollable, time step sizes, such as data generated from Gillespie's
stochastic simulations. We then conduct several experiments across various
scenarios to verify the advantages and robustness of the proposed method.</div><div><a href='http://arxiv.org/abs/2402.14475v1'>2402.14475v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11418v1")'>Variational Sampling of Temporal Trajectories</div>
<div id='2403.11418v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T02:12:12Z</div><div>Authors: Jurijs Nazarovs, Zhichun Huang, Xingjian Zhen, Sourav Pal, Rudrasis Chakraborty, Vikas Singh</div><div style='padding-top: 10px; width: 80ex'>A deterministic temporal process can be determined by its trajectory, an
element in the product space of (a) initial condition $z_0 \in \mathcal{Z}$ and
(b) transition function $f: (\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$ often
influenced by the control of the underlying dynamical system. Existing methods
often model the transition function as a differential equation or as a
recurrent neural network. Despite their effectiveness in predicting future
measurements, few results have successfully established a method for sampling
and statistical inference of trajectories using neural networks, partially due
to constraints in the parameterization. In this work, we introduce a mechanism
to learn the distribution of trajectories by parameterizing the transition
function $f$ explicitly as an element in a function space. Our framework allows
efficient synthesis of novel trajectories, while also directly providing a
convenient tool for inference, i.e., uncertainty estimation, likelihood
evaluations and out of distribution detection for abnormal trajectories. These
capabilities can have implications for various downstream tasks, e.g.,
simulation and evaluation for reinforcement learning.</div><div><a href='http://arxiv.org/abs/2403.11418v1'>2403.11418v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18512v1")'>Log Neural Controlled Differential Equations: The Lie Brackets Make a
  Difference</div>
<div id='2402.18512v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T17:40:05Z</div><div>Authors: Benjamin Walker, Andrew D. McLeod, Tiexin Qin, Yichuan Cheng, Haoliang Li, Terry Lyons</div><div style='padding-top: 10px; width: 80ex'>The vector field of a controlled differential equation (CDE) describes the
relationship between a control path and the evolution of a solution path.
Neural CDEs (NCDEs) treat time series data as observations from a control path,
parameterise a CDE's vector field using a neural network, and use the solution
path as a continuously evolving hidden state. As their formulation makes them
robust to irregular sampling rates, NCDEs are a powerful approach for modelling
real-world data. Building on neural rough differential equations (NRDEs), we
introduce Log-NCDEs, a novel and effective method for training NCDEs. The core
component of Log-NCDEs is the Log-ODE method, a tool from the study of rough
paths for approximating a CDE's solution. On a range of multivariate time
series classification benchmarks, Log-NCDEs are shown to achieve a higher
average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models,
S5 and the linear recurrent unit.</div><div><a href='http://arxiv.org/abs/2402.18512v1'>2402.18512v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04979v1")'>Invertible Solution of Neural Differential Equations for Analysis of
  Irregularly-Sampled Time Series</div>
<div id='2401.04979v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T07:51:02Z</div><div>Authors: YongKyung Oh, Dongyoung Lim, Sungil Kim</div><div style='padding-top: 10px; width: 80ex'>To handle the complexities of irregular and incomplete time series data, we
propose an invertible solution of Neural Differential Equations (NDE)-based
method. While NDE-based methods are a powerful method for analyzing
irregularly-sampled time series, they typically do not guarantee reversible
transformations in their standard form. Our method suggests the variation of
Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which
ensures invertibility while maintaining a lower computational burden.
Additionally, it enables the training of a dual latent space, enhancing the
modeling of dynamic temporal dynamics. Our research presents an advanced
framework that excels in both classification and interpolation tasks. At the
core of our approach is an enhanced dual latent states architecture, carefully
designed for high precision across various time series tasks. Empirical
analysis demonstrates that our method significantly outperforms existing
models. This work significantly advances irregular time series analysis,
introducing innovative techniques and offering a versatile tool for diverse
practical applications.</div><div><a href='http://arxiv.org/abs/2401.04979v1'>2401.04979v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14989v2")'>Stable Neural Stochastic Differential Equations in Analyzing Irregular
  Time Series Data</div>
<div id='2402.14989v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T22:00:03Z</div><div>Authors: YongKyung Oh, Dongyoung Lim, Sungil Kim</div><div style='padding-top: 10px; width: 80ex'>Irregular sampling intervals and missing values in real-world time series
data present challenges for conventional methods that assume consistent
intervals and complete data. Neural Ordinary Differential Equations (Neural
ODEs) offer an alternative approach, utilizing neural networks combined with
ODE solvers to learn continuous latent representations through parameterized
vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend
Neural ODEs by incorporating a diffusion term, although this addition is not
trivial, particularly when addressing irregular intervals and missing values.
Consequently, careful design of drift and diffusion functions is crucial for
maintaining stability and enhancing performance, while incautious choices can
result in adverse properties such as the absence of strong solutions,
stochastic destabilization, or unstable Euler discretizations, significantly
affecting Neural SDEs' performance. In this study, we propose three stable
classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE.
Then, we rigorously demonstrate their robustness in maintaining excellent
performance under distribution shift, while effectively preventing overfitting.
To assess the effectiveness of our approach, we conduct extensive experiments
on four benchmark datasets for interpolation, forecasting, and classification
tasks, and analyze the robustness of our methods with 30 public datasets under
different missing rates. Our results demonstrate the efficacy of the proposed
method in handling real-world irregular time series data.</div><div><a href='http://arxiv.org/abs/2402.14989v2'>2402.14989v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04246v1")'>Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic
  Differential Equations</div>
<div id='2403.04246v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T06:07:31Z</div><div>Authors: Shuaiyu Li, Yang Ruan, Changzhou Long, Yuzhong Cheng</div><div style='padding-top: 10px; width: 80ex'>This study addresses the challenges in parameter estimation of stochastic
differential equations driven by non-Gaussian noises, which are critical in
understanding dynamic phenomena such as price fluctuations and the spread of
infectious diseases. Previous research highlighted the potential of LSTM
networks in estimating parameters of alpha stable Levy driven SDEs but faced
limitations including high time complexity and constraints of the LSTM chaining
property. To mitigate these issues, we introduce the PEnet, a novel
CNN-LSTM-based three-stage model that offers an end to end approach with
superior accuracy and adaptability to varying data structures, enhanced
inference speed for long sequence observations through initial data feature
condensation by CNN, and high generalization capability, allowing its
application to various complex SDE scenarios. Experiments on synthetic datasets
confirm PEnet significant advantage in estimating SDE parameters associated
with noise characteristics, establishing it as a competitive method for SDE
parameter estimation in the presence of Levy noise.</div><div><a href='http://arxiv.org/abs/2403.04246v1'>2403.04246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13867v1")'>Capsule Neural Networks as Noise Stabilizer for Time Series Data</div>
<div id='2403.13867v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T12:17:49Z</div><div>Authors: Soyeon Kim, Jihyeon Seong, Hyunkyung Han, Jaesik Choi</div><div style='padding-top: 10px; width: 80ex'>Capsule Neural Networks utilize capsules, which bind neurons into a single
vector and learn position equivariant features, which makes them more robust
than original Convolutional Neural Networks. CapsNets employ an affine
transformation matrix and dynamic routing with coupling coefficients to learn
robustly. In this paper, we investigate the effectiveness of CapsNets in
analyzing highly sensitive and noisy time series sensor data. To demonstrate
CapsNets robustness, we compare their performance with original CNNs on
electrocardiogram data, a medical time series sensor data with complex patterns
and noise. Our study provides empirical evidence that CapsNets function as
noise stabilizers, as investigated by manual and adversarial attack experiments
using the fast gradient sign method and three manual attacks, including offset
shifting, gradual drift, and temporal lagging. In summary, CapsNets outperform
CNNs in both manual and adversarial attacked data. Our findings suggest that
CapsNets can be effectively applied to various sensor systems to improve their
resilience to noise attacks. These results have significant implications for
designing and implementing robust machine learning models in real world
applications. Additionally, this study contributes to the effectiveness of
CapsNet models in handling noisy data and highlights their potential for
addressing the challenges of noise data in time series analysis.</div><div><a href='http://arxiv.org/abs/2403.13867v1'>2403.13867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01338v1")'>Inferring the Langevin Equation with Uncertainty via Bayesian Neural
  Networks</div>
<div id='2402.01338v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T11:47:56Z</div><div>Authors: Youngkyoung Bae, Seungwoong Ha, Hawoong Jeong</div><div style='padding-top: 10px; width: 80ex'>Pervasive across diverse domains, stochastic systems exhibit fluctuations in
processes ranging from molecular dynamics to climate phenomena. The Langevin
equation has served as a common mathematical model for studying such systems,
enabling predictions of their temporal evolution and analyses of thermodynamic
quantities, including absorbed heat, work done on the system, and entropy
production. However, inferring the Langevin equation from observed trajectories
remains challenging, particularly for nonlinear and high-dimensional systems.
In this study, we present a comprehensive framework that employs Bayesian
neural networks for inferring Langevin equations in both overdamped and
underdamped regimes. Our framework first provides the drift force and diffusion
matrix separately and then combines them to construct the Langevin equation. By
providing a distribution of predictions instead of a single value, our approach
allows us to assess prediction uncertainties, which can prevent potential
misunderstandings and erroneous decisions about the system. We demonstrate the
effectiveness of our framework in inferring Langevin equations for various
scenarios including a neuron model and microscopic engine, highlighting its
versatility and potential impact.</div><div><a href='http://arxiv.org/abs/2402.01338v1'>2402.01338v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07379v1")'>Inference of dynamical gene regulatory networks from single-cell data
  with physics informed neural networks</div>
<div id='2401.07379v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-14T21:43:10Z</div><div>Authors: Maria Mircea, Diego Garlaschelli, Stefan Semrau</div><div style='padding-top: 10px; width: 80ex'>One of the main goals of developmental biology is to reveal the gene
regulatory networks (GRNs) underlying the robust differentiation of multipotent
progenitors into precisely specified cell types. Most existing methods to infer
GRNs from experimental data have limited predictive power as the inferred GRNs
merely reflect gene expression similarity or correlation. Here, we demonstrate,
how physics-informed neural networks (PINNs) can be used to infer the
parameters of predictive, dynamical GRNs that provide mechanistic understanding
of biological processes. Specifically we study GRNs that exhibit bifurcation
behavior and can therefore model cell differentiation. We show that PINNs
outperform regular feed-forward neural networks on the parameter inference task
and analyze two relevant experimental scenarios: 1. a system with cell
communication for which gene expression trajectories are available and 2.
snapshot measurements of a cell population in which cell communication is
absent. Our analysis will inform the design of future experiments to be
analyzed with PINNs and provides a starting point to explore this powerful
class of neural network models further.</div><div><a href='http://arxiv.org/abs/2401.07379v1'>2401.07379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.01371v1")'>Large-scale variational Gaussian state-space models</div>
<div id='2403.01371v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T02:19:49Z</div><div>Authors: Matthew Dowling, Yuan Zhao, Il Memming Park</div><div style='padding-top: 10px; width: 80ex'>We introduce an amortized variational inference algorithm and structured
variational approximation for state-space models with nonlinear dynamics driven
by Gaussian noise. Importantly, the proposed framework allows for efficient
evaluation of the ELBO and low-variance stochastic gradient estimates without
resorting to diagonal Gaussian approximations by exploiting (i) the low-rank
structure of Monte-Carlo approximations to marginalize the latent state through
the dynamics (ii) an inference network that approximates the update step with
low-rank precision matrix updates (iii) encoding current and future
observations into pseudo observations -- transforming the approximate smoothing
problem into an (easier) approximate filtering problem. Overall, the necessary
statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$
is the series length, $L$ is the state-space dimensionality, $S$ are the number
of samples used to approximate the predict step statistics, and $r$ is the rank
of the approximate precision matrix update in the update step (which can be
made of much lower dimension than $L$).</div><div><a href='http://arxiv.org/abs/2403.01371v1'>2403.01371v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13724v1")'>Probabilistic Forecasting with Stochastic Interpolants and Föllmer
  Processes</div>
<div id='2403.13724v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T16:33:06Z</div><div>Authors: Yifan Chen, Mark Goldstein, Mengjian Hua, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden</div><div style='padding-top: 10px; width: 80ex'>We propose a framework for probabilistic forecasting of dynamical systems
based on generative modeling. Given observations of the system state over time,
we formulate the forecasting problem as sampling from the conditional
distribution of the future system state given its current state. To this end,
we leverage the framework of stochastic interpolants, which facilitates the
construction of a generative model between an arbitrary base distribution and
the target. We design a fictitious, non-physical stochastic dynamics that takes
as initial condition the current system state and produces as output a sample
from the target conditional distribution in finite time and without bias. This
process therefore maps a point mass centered at the current state onto a
probabilistic ensemble of forecasts. We prove that the drift coefficient
entering the stochastic differential equation (SDE) achieving this task is
non-singular, and that it can be learned efficiently by square loss regression
over the time-series data. We show that the drift and the diffusion
coefficients of this SDE can be adjusted after training, and that a specific
choice that minimizes the impact of the estimation error gives a F\"ollmer
process. We highlight the utility of our approach on several complex,
high-dimensional forecasting problems, including stochastically forced
Navier-Stokes and video prediction on the KTH and CLEVRER datasets.</div><div><a href='http://arxiv.org/abs/2403.13724v1'>2403.13724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09470v1")'>Rolling Diffusion Models</div>
<div id='2402.09470v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T08:16:10Z</div><div>Authors: David Ruhe, Jonathan Heek, Tim Salimans, Emiel Hoogeboom</div><div style='padding-top: 10px; width: 80ex'>Diffusion models have recently been increasingly applied to temporal data
such as video, fluid mechanics simulations, or climate data. These methods
generally treat subsequent frames equally regarding the amount of noise in the
diffusion process. This paper explores Rolling Diffusion: a new approach that
uses a sliding window denoising process. It ensures that the diffusion process
progressively corrupts through time by assigning more noise to frames that
appear later in a sequence, reflecting greater uncertainty about the future as
the generation process unfolds. Empirically, we show that when the temporal
dynamics are complex, Rolling Diffusion is superior to standard diffusion. In
particular, this result is demonstrated in a video prediction task using the
Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting
experiment.</div><div><a href='http://arxiv.org/abs/2402.09470v1'>2402.09470v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02297v1")'>Denoising Diffusion-Based Control of Nonlinear Systems</div>
<div id='2402.02297v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T23:19:26Z</div><div>Authors: Karthik Elamvazhuthi, Darshan Gadginmath, Fabio Pasqualetti</div><div style='padding-top: 10px; width: 80ex'>We propose a novel approach based on Denoising Diffusion Probabilistic Models
(DDPMs) to control nonlinear dynamical systems. DDPMs are the state-of-art of
generative models that have achieved success in a wide variety of sampling
tasks. In our framework, we pose the feedback control problem as a generative
task of drawing samples from a target set under control system constraints. The
forward process of DDPMs constructs trajectories originating from a target set
by adding noise. We learn to control a dynamical system in reverse such that
the terminal state belongs to the target set. For control-affine systems
without drift, we prove that the control system can exactly track the
trajectory of the forward process in reverse, whenever the the Lie bracket
based condition for controllability holds. We numerically study our approach on
various nonlinear systems and verify our theoretical results. We also conduct
numerical experiments for cases beyond our theoretical results on a
physics-engine.</div><div><a href='http://arxiv.org/abs/2402.02297v1'>2402.02297v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04082v1")'>Inference via Interpolation: Contrastive Representations Provably Enable
  Planning and Inference</div>
<div id='2403.04082v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T22:27:30Z</div><div>Authors: Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, Sergey Levine</div><div style='padding-top: 10px; width: 80ex'>Given time series data, how can we answer questions like "what will happen in
the future?" and "how did we get here?" These sorts of probabilistic inference
questions are challenging when observations are high-dimensional. In this
paper, we show how these questions can have compact, closed form solutions in
terms of learned representations. The key idea is to apply a variant of
contrastive learning to time series data. Prior work already shows that the
representations learned by contrastive learning encode a probability ratio. By
extending prior work to show that the marginal distribution over
representations is Gaussian, we can then prove that joint distribution of
representations is also Gaussian. Taken together, these results show that
representations learned via temporal contrastive learning follow a Gauss-Markov
chain, a graphical model where inference (e.g., prediction, planning) over
representations corresponds to inverting a low-dimensional matrix. In one
special case, inferring intermediate representations will be equivalent to
interpolating between the learned representations. We validate our theory using
numerical simulations on tasks up to 46-dimensions.</div><div><a href='http://arxiv.org/abs/2403.04082v1'>2403.04082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09623v1")'>Conformalized Adaptive Forecasting of Heterogeneous Trajectories</div>
<div id='2402.09623v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T23:57:19Z</div><div>Authors: Yanfei Zhou, Lars Lindemann, Matteo Sesia</div><div style='padding-top: 10px; width: 80ex'>This paper presents a new conformal method for generating simultaneous
forecasting bands guaranteed to cover the entire path of a new random
trajectory with sufficiently high probability. Prompted by the need for
dependable uncertainty estimates in motion planning applications where the
behavior of diverse objects may be more or less unpredictable, we blend
different techniques from online conformal prediction of single and multiple
time series, as well as ideas for addressing heteroscedasticity in regression.
This solution is both principled, providing precise finite-sample guarantees,
and effective, often leading to more informative predictions than prior
methods.</div><div><a href='http://arxiv.org/abs/2402.09623v1'>2402.09623v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04648v1")'>A novel framework for generalization of deep hidden physics models</div>
<div id='2401.04648v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T16:16:32Z</div><div>Authors: Vijay Kag, Birupaksha Pal</div><div style='padding-top: 10px; width: 80ex'>Modelling of systems where the full system information is unknown is an oft
encountered problem for various engineering and industrial applications, as
it's either impossible to consider all the complex physics involved or simpler
models are considered to keep within the limits of the available resources.
Recent advances in greybox modelling like the deep hidden physics models
address this space by combining data and physics. However, for most real-life
applications, model generalizability is a key issue, as retraining a model for
every small change in system inputs and parameters or modification in domain
configuration can render the model economically unviable. In this work we
present a novel enhancement to the idea of hidden physics models which can
generalize for changes in system inputs, parameters and domains. We also show
that this approach holds promise in system discovery as well and helps learn
the hidden physics for the changed system inputs, parameters and domain
configuration.</div><div><a href='http://arxiv.org/abs/2401.04648v1'>2401.04648v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.11196v1")'>Machine learning based state observer for discrete time systems evolving
  on Lie groups</div>
<div id='2401.11196v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T10:21:51Z</div><div>Authors: Soham Shanbhag, Dong Eui Chang</div><div style='padding-top: 10px; width: 80ex'>In this paper, a machine learning based observer for systems evolving on
manifolds is designed such that the state of the observer is restricted to the
Lie group on which the system evolves. Conventional techniques involving
machine learning based observers on systems evolving on Lie groups involve
designing charts for the Lie group, training a machine learning based observer
for each chart, and switching between the trained models based on the state of
the system. We propose a novel deep learning based technique whose predictions
are restricted to a measure 0 subset of Euclidean space without using charts.
Using this network, we design an observer ensuring that the state of the
observer is restricted to the Lie group, and predicting the state using only
one trained algorithm. The deep learning network predicts an ``error term'' on
the Lie algebra of the Lie group, uses the map from the Lie algebra to the
group, and uses the group action and the present state to estimate the state at
the next epoch. This model being purely data driven does not require the model
of the system. The proposed algorithm provides a novel framework for
constraining the output of machine learning networks to a measure 0 subset of a
Euclidean space without chart specific training and without requiring
switching. We show the validity of this method using Monte Carlo simulations
performed of the rigid body rotation and translation system.</div><div><a href='http://arxiv.org/abs/2401.11196v1'>2401.11196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.19287v1")'>StiefelGen: A Simple, Model Agnostic Approach for Time Series Data
  Augmentation over Riemannian Manifolds</div>
<div id='2402.19287v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:52:21Z</div><div>Authors: Prasad Cheema, Mahito Sugiyama</div><div style='padding-top: 10px; width: 80ex'>Data augmentation is an area of research which has seen active development in
many machine learning fields, such as in image-based learning models,
reinforcement learning for self driving vehicles, and general noise injection
for point cloud data. However, convincing methods for general time series data
augmentation still leaves much to be desired, especially since the methods
developed for these models do not readily cross-over. Three common approaches
for time series data augmentation include: (i) Constructing a physics-based
model and then imbuing uncertainty over the coefficient space (for example),
(ii) Adding noise to the observed data set(s), and, (iii) Having access to
ample amounts of time series data sets from which a robust generative neural
network model can be trained. However, for many practical problems that work
with time series data in the industry: (i) One usually does not have access to
a robust physical model, (ii) The addition of noise can in of itself require
large or difficult assumptions (for example, what probability distribution
should be used? Or, how large should the noise variance be?), and, (iii) In
practice, it can be difficult to source a large representative time series data
base with which to train the neural network model for the underlying problem.
In this paper, we propose a methodology which attempts to simultaneously tackle
all three of these previous limitations to a large extent. The method relies
upon the well-studied matrix differential geometry of the Stiefel manifold, as
it proposes a simple way in which time series signals can placed on, and then
smoothly perturbed over the manifold. We attempt to clarify how this method
works by showcasing several potential use cases which in particular work to
take advantage of the unique properties of this underlying manifold.</div><div><a href='http://arxiv.org/abs/2402.19287v1'>2402.19287v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14514v1")'>Machine-learning invariant foliations in forced systems for reduced
  order modelling</div>
<div id='2403.14514v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T16:10:42Z</div><div>Authors: Robert Szalai</div><div style='padding-top: 10px; width: 80ex'>We identify reduced order models (ROM) of forced systems from data using
invariant foliations. The forcing can be external, parametric, periodic or
quasi-periodic. The process has four steps: 1. identify an approximate
invariant torus and the linear dynamics about the torus; 2. identify a globally
defined invariant foliation about the torus; 3. identify a local foliation
about an invariant manifold that complements the global foliation 4. extract
the invariant manifold as the leaf going through the torus and interpret the
result. We combine steps 2 and 3, so that we can track the location of the
invariant torus and scale the invariance equations appropriately. We highlight
some fundamental limitations of invariant manifolds and foliations when fitting
them to data, that require further mathematics to resolve.</div><div><a href='http://arxiv.org/abs/2403.14514v1'>2403.14514v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16131v1")'>A VAE-based Framework for Learning Multi-Level Neural Granger-Causal
  Connectivity</div>
<div id='2402.16131v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T16:11:32Z</div><div>Authors: Jiahe Lin, Huitian Lei, George Michailidis</div><div style='padding-top: 10px; width: 80ex'>Granger causality has been widely used in various application domains to
capture lead-lag relationships amongst the components of complex dynamical
systems, and the focus in extant literature has been on a single dynamical
system. In certain applications in macroeconomics and neuroscience, one has
access to data from a collection of related such systems, wherein the modeling
task of interest is to extract the shared common structure that is embedded
across them, as well as to identify the idiosyncrasies within individual ones.
This paper introduces a Variational Autoencoder (VAE) based framework that
jointly learns Granger-causal relationships amongst components in a collection
of related-yet-heterogeneous dynamical systems, and handles the aforementioned
task in a principled way. The performance of the proposed framework is
evaluated on several synthetic data settings and benchmarked against existing
approaches designed for individual system learning. The method is further
illustrated on a real dataset involving time series data from a
neurophysiological experiment and produces interpretable results.</div><div><a href='http://arxiv.org/abs/2402.16131v1'>2402.16131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17506v1")'>Thermodynamics-informed super-resolution of scarce temporal dynamics
  data</div>
<div id='2402.17506v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T13:46:45Z</div><div>Authors: Carlos Bermejo-Barbanoj, Beatriz Moya, Alberto Badías, Francisco Chinesta, Elías Cueto</div><div style='padding-top: 10px; width: 80ex'>We present a method to increase the resolution of measurements of a physical
system and subsequently predict its time evolution using thermodynamics-aware
neural networks. Our method uses adversarial autoencoders, which reduce the
dimensionality of the full order model to a set of latent variables that are
enforced to match a prior, for example a normal distribution. Adversarial
autoencoders are seen as generative models, and they can be trained to generate
high-resolution samples from low-resoution inputs, meaning they can address the
so-called super-resolution problem. Then, a second neural network is trained to
learn the physical structure of the latent variables and predict their temporal
evolution. This neural network is known as an structure-preserving neural
network. It learns the metriplectic-structure of the system and applies a
physical bias to ensure that the first and second principles of thermodynamics
are fulfilled. The integrated trajectories are decoded to their original
dimensionality, as well as to the higher dimensionality space produced by the
adversarial autoencoder and they are compared to the ground truth solution. The
method is tested with two examples of flow over a cylinder, where the fluid
properties are varied between both examples.</div><div><a href='http://arxiv.org/abs/2402.17506v1'>2402.17506v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17157v1")'>Generative Learning for Forecasting the Dynamics of Complex Systems</div>
<div id='2402.17157v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T02:44:40Z</div><div>Authors: Han Gao, Sebastian Kaltenbach, Petros Koumoutsakos</div><div style='padding-top: 10px; width: 80ex'>We introduce generative models for accelerating simulations of complex
systems through learning and evolving their effective dynamics. In the proposed
Generative Learning of Effective Dynamics (G-LED), instances of high
dimensional data are down sampled to a lower dimensional manifold that is
evolved through an auto-regressive attention mechanism. In turn, Bayesian
diffusion models, that map this low-dimensional manifold onto its corresponding
high-dimensional space, capture the statistics of the system dynamics. We
demonstrate the capabilities and drawbacks of G-LED in simulations of several
benchmark systems, including the Kuramoto-Sivashinsky (KS) equation,
two-dimensional high Reynolds number flow over a backward-facing step, and
simulations of three-dimensional turbulent channel flow. The results
demonstrate that generative learning offers new frontiers for the accurate
forecasting of the statistical properties of complex systems at a reduced
computational cost.</div><div><a href='http://arxiv.org/abs/2402.17157v1'>2402.17157v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13282v1")'>RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing</div>
<div id='2401.13282v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T07:47:01Z</div><div>Authors: Junaid Farooq, Danish Rafiq, Pantelis R. Vlachas, Mohammad Abid Bazaz</div><div style='padding-top: 10px; width: 80ex'>Forecasting complex system dynamics, particularly for long-term predictions,
is persistently hindered by error accumulation and computational burdens. This
study presents RefreshNet, a multiscale framework developed to overcome these
challenges, delivering an unprecedented balance between computational
efficiency and predictive accuracy. RefreshNet incorporates convolutional
autoencoders to identify a reduced order latent space capturing essential
features of the dynamics, and strategically employs multiple recurrent neural
network (RNN) blocks operating at varying temporal resolutions within the
latent space, thus allowing the capture of latent dynamics at multiple temporal
scales. The unique "refreshing" mechanism in RefreshNet allows coarser blocks
to reset inputs of finer blocks, effectively controlling and alleviating error
accumulation. This design demonstrates superiority over existing techniques
regarding computational efficiency and predictive accuracy, especially in
long-term forecasting. The framework is validated using three benchmark
applications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and
Kuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms
state-of-the-art methods in long-term forecasting accuracy and speed, marking a
significant advancement in modeling complex systems and opening new avenues in
understanding and predicting their behavior.</div><div><a href='http://arxiv.org/abs/2401.13282v1'>2401.13282v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14148v3")'>Neural Networks and Friction: Slide, Hold, Learn</div>
<div id='2402.14148v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T22:11:01Z</div><div>Authors: Joaquin Garcia-Suarez</div><div style='padding-top: 10px; width: 80ex'>In this study, it is demonstrated that Recurrent Neural Networks (RNNs),
specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess
the capability to learn the complex dynamics of rate-and-state friction laws
from synthetic data. The data employed for training the network is generated
through the application of traditional rate-and-state friction equations
coupled with the aging law for state evolution. A novel aspect of our approach
is the formulation of a loss function that explicitly accounts for initial
conditions, the direct effect, and the evolution of state variables during
training. It is found that the RNN, with its GRU architecture, effectively
learns to predict changes in the friction coefficient resulting from velocity
jumps, thereby showcasing the potential of machine learning models in
understanding and simulating the physics of frictional processes.</div><div><a href='http://arxiv.org/abs/2402.14148v3'>2402.14148v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10724v1")'>Machine Learning based Prediction of Ditching Loads</div>
<div id='2402.10724v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T14:30:46Z</div><div>Authors: Henning Schwarz, Micha Überrück, Jens-Peter M. Zemke, Thomas Rung</div><div style='padding-top: 10px; width: 80ex'>We present approaches to predict dynamic ditching loads on aircraft fuselages
using machine learning. The employed learning procedure is structured into two
parts, the reconstruction of the spatial loads using a convolutional
autoencoder (CAE) and the transient evolution of these loads in a subsequent
part. Different CAE strategies are assessed and combined with either long
short-term memory (LSTM) networks or Koopman-operator based methods to predict
the transient behaviour. The training data is compiled by an extension of the
momentum method of von-Karman and Wagner and the rationale of the training
approach is briefly summarised. The application included refers to a full-scale
fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach
velocities at 6{\deg} incidence. Results indicate a satisfactory level of
predictive agreement for all four investigated surrogate models examined, with
the combination of an LSTM and a deep decoder CAE showing the best performance.</div><div><a href='http://arxiv.org/abs/2402.10724v1'>2402.10724v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.05848v2")'>tLaSDI: Thermodynamics-informed latent space dynamics identification</div>
<div id='2403.05848v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T09:17:23Z</div><div>Authors: Jun Sur Richard Park, Siu Wun Cheung, Youngsoo Choi, Yeonjong Shin</div><div style='padding-top: 10px; width: 80ex'>We propose a latent space dynamics identification method, namely tLaSDI, that
embeds the first and second principles of thermodynamics. The latent variables
are learned through an autoencoder as a nonlinear dimension reduction model.
The latent dynamics are constructed by a neural network-based model that
precisely preserves certain structures for the thermodynamic laws through the
GENERIC formalism. An abstract error estimate is established, which provides a
new loss formulation involving the Jacobian computation of autoencoder. The
autoencoder and the latent dynamics are simultaneously trained to minimize the
new loss. Computational examples demonstrate the effectiveness of tLaSDI, which
exhibits robust generalization ability, even in extrapolation. In addition, an
intriguing correlation is empirically observed between a quantity from tLaSDI
in the latent space and the behaviors of the full-state solution.</div><div><a href='http://arxiv.org/abs/2403.05848v2'>2403.05848v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10490v1")'>Generalization Error Guaranteed Auto-Encoder-Based Nonlinear Model
  Reduction for Operator Learning</div>
<div id='2401.10490v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T05:01:43Z</div><div>Authors: Hao Liu, Biraj Dahal, Rongjie Lai, Wenjing Liao</div><div style='padding-top: 10px; width: 80ex'>Many physical processes in science and engineering are naturally represented
by operators between infinite-dimensional function spaces. The problem of
operator learning, in this context, seeks to extract these physical processes
from empirical data, which is challenging due to the infinite or high
dimensionality of data. An integral component in addressing this challenge is
model reduction, which reduces both the data dimensionality and problem size.
In this paper, we utilize low-dimensional nonlinear structures in model
reduction by investigating Auto-Encoder-based Neural Network (AENet). AENet
first learns the latent variables of the input data and then learns the
transformation from these latent variables to corresponding output data. Our
numerical experiments validate the ability of AENet to accurately learn the
solution operator of nonlinear partial differential equations. Furthermore, we
establish a mathematical and statistical estimation theory that analyzes the
generalization error of AENet. Our theoretical framework shows that the sample
complexity of training AENet is intricately tied to the intrinsic dimension of
the modeled process, while also demonstrating the remarkable resilience of
AENet to noise.</div><div><a href='http://arxiv.org/abs/2401.10490v1'>2401.10490v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09499v1")'>Functional Autoencoder for Smoothing and Representation Learning</div>
<div id='2401.09499v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T08:33:25Z</div><div>Authors: Sidi Wu, Cédric Beaulac, Jiguo Cao</div><div style='padding-top: 10px; width: 80ex'>A common pipeline in functional data analysis is to first convert the
discretely observed data to smooth functions, and then represent the functions
by a finite-dimensional vector of coefficients summarizing the information.
Existing methods for data smoothing and dimensional reduction mainly focus on
learning the linear mappings from the data space to the representation space,
however, learning only the linear representations may not be sufficient. In
this study, we propose to learn the nonlinear representations of functional
data using neural network autoencoders designed to process data in the form it
is usually collected without the need of preprocessing. We design the encoder
to employ a projection layer computing the weighted inner product of the
functional data and functional weights over the observed timestamp, and the
decoder to apply a recovery layer that maps the finite-dimensional vector
extracted from the functional data back to functional space using a set of
predetermined basis functions. The developed architecture can accommodate both
regularly and irregularly spaced data. Our experiments demonstrate that the
proposed method outperforms functional principal component analysis in terms of
prediction and classification, and maintains superior smoothing ability and
better computational efficiency in comparison to the conventional autoencoders
under both linear and nonlinear settings.</div><div><a href='http://arxiv.org/abs/2401.09499v1'>2401.09499v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14682v1")'>Deep Generative Domain Adaptation with Temporal Relation Knowledge for
  Cross-User Activity Recognition</div>
<div id='2403.14682v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T22:48:23Z</div><div>Authors: Xiaozhou Ye, Kevin I-Kai Wang</div><div style='padding-top: 10px; width: 80ex'>In human activity recognition (HAR), the assumption that training and testing
data are independent and identically distributed (i.i.d.) often fails,
particularly in cross-user scenarios where data distributions vary
significantly. This discrepancy highlights the limitations of conventional
domain adaptation methods in HAR, which typically overlook the inherent
temporal relations in time-series data. To bridge this gap, our study
introduces a Conditional Variational Autoencoder with Universal Sequence
Mapping (CVAE-USM) approach, which addresses the unique challenges of
time-series domain adaptation in HAR by relaxing the i.i.d. assumption and
leveraging temporal relations to align data distributions effectively across
different users. This method combines the strengths of Variational Autoencoder
(VAE) and Universal Sequence Mapping (USM) to capture and utilize common
temporal patterns between users for improved activity recognition. Our results,
evaluated on two public HAR datasets (OPPT and PAMAP2), demonstrate that
CVAE-USM outperforms existing state-of-the-art methods, offering a more
accurate and generalizable solution for cross-user activity recognition.</div><div><a href='http://arxiv.org/abs/2403.14682v1'>2403.14682v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.06992v1")'>Phase autoencoder for limit-cycle oscillators</div>
<div id='2403.06992v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T06:03:55Z</div><div>Authors: Koichiro Yawata, Kai Fukami, Kunihiko Taira, Hiroya Nakao</div><div style='padding-top: 10px; width: 80ex'>We present a phase autoencoder that encodes the asymptotic phase of a
limit-cycle oscillator, a fundamental quantity characterizing its
synchronization dynamics. This autoencoder is trained in such a way that its
latent variables directly represent the asymptotic phase of the oscillator. The
trained autoencoder can perform two functions without relying on the
mathematical model of the oscillator: first, it can evaluate the asymptotic
phase and phase sensitivity function of the oscillator; second, it can
reconstruct the oscillator state on the limit cycle in the original space from
the phase value as an input. Using several examples of limit-cycle oscillators,
we demonstrate that the asymptotic phase and phase sensitivity function can be
estimated only from time-series data by the trained autoencoder. We also
present a simple method for globally synchronizing two oscillators as an
application of the trained autoencoder.</div><div><a href='http://arxiv.org/abs/2403.06992v1'>2403.06992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.02031v1")'>Multi-fidelity physics constrained neural networks for dynamical systems</div>
<div id='2402.02031v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:05:26Z</div><div>Authors: Hao Zhou, Sibo Cheng, Rossella Arcucci</div><div style='padding-top: 10px; width: 80ex'>Physics-constrained neural networks are commonly employed to enhance
prediction robustness compared to purely data-driven models, achieved through
the inclusion of physical constraint losses during the model training process.
However, one of the major challenges of physics-constrained neural networks
consists of the training complexity especially for high-dimensional systems. In
fact, conventional physics-constrained models rely on singular-fidelity data
necessitating the assessment of physical constraints within high-dimensional
fields, which introduces computational difficulties. Furthermore, due to the
fixed input size of the neural networks, employing multi-fidelity training data
can also be cumbersome. In this paper, we propose the Multi-Scale
Physics-Constrained Neural Network (MSPCNN), which offers a novel methodology
for incorporating data with different levels of fidelity into a unified latent
space through a customised multi-fidelity autoencoder. Additionally, multiple
decoders are concurrently trained to map latent representations of inputs into
various fidelity physical spaces. As a result, during the training of
predictive models, physical constraints can be evaluated within low-fidelity
spaces, yielding a trade-off between training efficiency and accuracy. In
addition, unlike conventional methods, MSPCNN also manages to employ
multi-fidelity data to train the predictive model. We assess the performance of
MSPCNN in two fluid dynamics problems, namely a two-dimensional Burgers' system
and a shallow water system. Numerical results clearly demonstrate the
enhancement of prediction accuracy and noise robustness when introducing
physical constraints in low-fidelity fields. On the other hand, as expected,
the training complexity can be significantly reduced by computing physical
constraint loss in the low-fidelity field rather than the high-fidelity one.</div><div><a href='http://arxiv.org/abs/2402.02031v1'>2402.02031v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03153v1")'>Learning solutions of parametric Navier-Stokes with physics-informed
  neural networks</div>
<div id='2402.03153v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:19:53Z</div><div>Authors: M. Naderibeni, M. J. T. Reinders, L. Wu, D. M. J. Tax</div><div style='padding-top: 10px; width: 80ex'>We leverage Physics-Informed Neural Networks (PINNs) to learn solution
functions of parametric Navier-Stokes Equations (NSE). Our proposed approach
results in a feasible optimization problem setup that bypasses PINNs'
limitations in converging to solutions of highly nonlinear parametric-PDEs like
NSE. We consider the parameter(s) of interest as inputs of PINNs along with
spatio-temporal coordinates, and train PINNs on generated numerical solutions
of parametric-PDES for instances of the parameters. We perform experiments on
the classical 2D flow past cylinder problem aiming to learn velocities and
pressure functions over a range of Reynolds numbers as parameter of interest.
Provision of training data from generated numerical simulations allows for
interpolation of the solution functions for a range of parameters. Therefore,
we compare PINNs with unconstrained conventional Neural Networks (NN) on this
problem setup to investigate the effectiveness of considering the PDEs
regularization in the loss function. We show that our proposed approach results
in optimizing PINN models that learn the solution functions while making sure
that flow predictions are in line with conservational laws of mass and
momentum. Our results show that PINN results in accurate prediction of
gradients compared to NN model, this is clearly visible in predicted vorticity
fields given that none of these models were trained on vorticity labels.</div><div><a href='http://arxiv.org/abs/2402.03153v1'>2402.03153v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17346v1")'>Understanding the training of PINNs for unsteady flow past a plunging
  foil through the lens of input subdomain level loss function gradients</div>
<div id='2402.17346v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T09:27:54Z</div><div>Authors: Rahul Sundar, Didier Lucor, Sunetra Sarkar</div><div style='padding-top: 10px; width: 80ex'>Recently immersed boundary method-inspired physics-informed neural networks
(PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the
ability to accurately reconstruct velocity and recover pressure as a hidden
variable for unsteady flow past moving bodies. Considering flow past a plunging
foil, MB-PINNs were trained with global physics loss relaxation and also in
conjunction with a physics-based undersampling method, obtaining good accuracy.
The purpose of this study was to investigate which input spatial subdomain
contributes to the training under the effect of physics loss relaxation and
physics-based undersampling. In the context of MB-PINNs training, three spatial
zones: the moving body, wake, and outer zones were defined. To quantify which
spatial zone drives the training, two novel metrics are computed from the zonal
loss component gradient statistics and the proportion of sample points in each
zone. Results confirm that the learning indeed depends on the combined effect
of the zonal loss component gradients and the proportion of points in each
zone. Moreover, the dominant input zones are also the ones that have the
strongest solution gradients in some sense.</div><div><a href='http://arxiv.org/abs/2402.17346v1'>2402.17346v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08886v1")'>RiemannONets: Interpretable Neural Operators for Riemann Problems</div>
<div id='2401.08886v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T23:45:14Z</div><div>Authors: Ahmad Peyvan, Vivek Oommen, Ameya D. Jagtap, George Em Karniadakis</div><div style='padding-top: 10px; width: 80ex'>Developing the proper representations for simulating high-speed flows with
strong shock waves, rarefactions, and contact discontinuities has been a
long-standing question in numerical analysis. Herein, we employ neural
operators to solve Riemann problems encountered in compressible flows for
extreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we
first consider the DeepONet that we train in a two-stage process, following the
recent work of Lee and Shin, wherein the first stage, a basis is extracted from
the trunk net, which is orthonormalized and subsequently is used in the second
stage in training the branch net. This simple modification of DeepONet has a
profound effect on its accuracy, efficiency, and robustness and leads to very
accurate solutions to Riemann problems compared to the vanilla version. It also
enables us to interpret the results physically as the hierarchical data-driven
produced basis reflects all the flow features that would otherwise be
introduced using ad hoc feature expansion layers. We also compare the results
with another neural operator based on the U-Net for low, intermediate, and very
high-pressure ratios that are very accurate for Riemann problems, especially
for large pressure ratios, due to their multiscale nature but computationally
more expensive. Overall, our study demonstrates that simple neural network
architectures, if properly pre-trained, can achieve very accurate solutions of
Riemann problems for real-time forecasting.</div><div><a href='http://arxiv.org/abs/2401.08886v1'>2401.08886v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16327v2")'>PICL: Physics Informed Contrastive Learning for Partial Differential
  Equations</div>
<div id='2401.16327v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:32:22Z</div><div>Authors: Cooper Lorsung, Amir Barati Farimani</div><div style='padding-top: 10px; width: 80ex'>Neural operators have recently grown in popularity as Partial Differential
Equation (PDEs) surrogate models. Learning solution functionals, rather than
functions, has proven to be a powerful approach to calculate fast, accurate
solutions to complex PDEs. While much work has been done evaluating neural
operator performance on a wide variety of surrogate modeling tasks, these works
normally evaluate performance on a single equation at a time. In this work, we
develop a novel contrastive pretraining framework utilizing Generalized
Contrastive Loss that improves neural operator generalization across multiple
governing equations simultaneously. Governing equation coefficients are used to
measure ground-truth similarity between systems. A combination of
physics-informed system evolution and latent-space model output are anchored to
input data and used in our distance function. We find that physics-informed
contrastive pretraining improves both accuracy and generalization for the
Fourier Neural Operator in fixed-future task, with comparable performance on
the autoregressive rollout, and superresolution tasks for the 1D Heat,
Burgers', and linear advection equations.</div><div><a href='http://arxiv.org/abs/2401.16327v2'>2401.16327v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16059v1")'>Gradient-enhanced deep Gaussian processes for multifidelity modelling</div>
<div id='2402.16059v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T11:08:19Z</div><div>Authors: Viv Bone, Chris van der Heide, Kieran Mackle, Ingo H. J. Jahn, Peter M. Dower, Chris Manzie</div><div style='padding-top: 10px; width: 80ex'>Multifidelity models integrate data from multiple sources to produce a single
approximator for the underlying process. Dense low-fidelity samples are used to
reduce interpolation error, while sparse high-fidelity samples are used to
compensate for bias or noise in the low-fidelity samples. Deep Gaussian
processes (GPs) are attractive for multifidelity modelling as they are
non-parametric, robust to overfitting, perform well for small datasets, and,
critically, can capture nonlinear and input-dependent relationships between
data of different fidelities. Many datasets naturally contain gradient data,
especially when they are generated by computational models that are compatible
with automatic differentiation or have adjoint solutions. Principally, this
work extends deep GPs to incorporate gradient data. We demonstrate this method
on an analytical test problem and a realistic partial differential equation
problem, where we predict the aerodynamic coefficients of a hypersonic flight
vehicle over a range of flight conditions and geometries. In both examples, the
gradient-enhanced deep GP outperforms a gradient-enhanced linear GP model and
their non-gradient-enhanced counterparts.</div><div><a href='http://arxiv.org/abs/2402.16059v1'>2402.16059v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18729v1")'>A Priori Uncertainty Quantification of Reacting Turbulence Closure
  Models using Bayesian Neural Networks</div>
<div id='2402.18729v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T22:19:55Z</div><div>Authors: Graham Pash, Malik Hassanaly, Shashank Yellapantula</div><div style='padding-top: 10px; width: 80ex'>While many physics-based closure model forms have been posited for the
sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data
available from direct numerical simulation (DNS) create opportunities to
leverage data-driven modeling techniques. Albeit flexible, data-driven models
still depend on the dataset and the functional form of the model chosen.
Increased adoption of such models requires reliable uncertainty estimates both
in the data-informed and out-of-distribution regimes. In this work, we employ
Bayesian neural networks (BNNs) to capture both epistemic and aleatoric
uncertainties in a reacting flow model. In particular, we model the filtered
progress variable scalar dissipation rate which plays a key role in the
dynamics of turbulent premixed flames. We demonstrate that BNN models can
provide unique insights about the structure of uncertainty of the data-driven
closure models. We also propose a method for the incorporation of
out-of-distribution information in a BNN. The efficacy of the model is
demonstrated by a priori evaluation on a dataset consisting of a variety of
flame conditions and fuels.</div><div><a href='http://arxiv.org/abs/2402.18729v1'>2402.18729v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11385v1")'>Stochastic approach for elliptic problems in perforated domains</div>
<div id='2403.11385v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T00:22:33Z</div><div>Authors: Jihun Han, Yoonsang Lee</div><div style='padding-top: 10px; width: 80ex'>A wide range of applications in science and engineering involve a PDE model
in a domain with perforations, such as perforated metals or air filters.
Solving such perforated domain problems suffers from computational challenges
related to resolving the scale imposed by the geometries of perforations. We
propose a neural network-based mesh-free approach for perforated domain
problems. The method is robust and efficient in capturing various configuration
scales, including the averaged macroscopic behavior of the solution that
involves a multiscale nature induced by small perforations. The new approach
incorporates the derivative-free loss method that uses a stochastic
representation or the Feynman-Kac formulation. In particular, we implement the
Neumann boundary condition for the derivative-free loss method to handle the
interface between the domain and perforations. A suite of stringent numerical
tests is provided to support the proposed method's efficacy in handling various
perforation scales.</div><div><a href='http://arxiv.org/abs/2403.11385v1'>2403.11385v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10238v1")'>Parametric Learning of Time-Advancement Operators for Unstable Flame
  Evolution</div>
<div id='2402.10238v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:12:42Z</div><div>Authors: Rixin Yu, Erdzan Hodzic</div><div style='padding-top: 10px; width: 80ex'>This study investigates the application of machine learning, specifically
Fourier Neural Operator (FNO) and Convolutional Neural Network (CNN), to learn
time-advancement operators for parametric partial differential equations
(PDEs). Our focus is on extending existing operator learning methods to handle
additional inputs representing PDE parameters. The goal is to create a unified
learning approach that accurately predicts short-term solutions and provides
robust long-term statistics under diverse parameter conditions, facilitating
computational cost savings and accelerating development in engineering
simulations. We develop and compare parametric learning methods based on FNO
and CNN, evaluating their effectiveness in learning parametric-dependent
solution time-advancement operators for one-dimensional PDEs and realistic
flame front evolution data obtained from direct numerical simulations of the
Navier-Stokes equations.</div><div><a href='http://arxiv.org/abs/2402.10238v1'>2402.10238v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01855v1")'>SPDE priors for uncertainty quantification of end-to-end neural data
  assimilation schemes</div>
<div id='2402.01855v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:18:12Z</div><div>Authors: Maxime Beauchamp, Nicolas Desassis, J. Emmanuel Johnson, Simon Benaichouche, Pierre Tandeo, Ronan Fablet</div><div style='padding-top: 10px; width: 80ex'>The spatio-temporal interpolation of large geophysical datasets has
historically been adressed by Optimal Interpolation (OI) and more sophisticated
model-based or data-driven DA techniques. In the last ten years, the link
established between Stochastic Partial Differential Equations (SPDE) and
Gaussian Markov Random Fields (GMRF) opened a new way of handling both large
datasets and physically-induced covariance matrix in Optimal Interpolation.
Recent advances in the deep learning community also enables to adress this
problem as neural architecture embedding data assimilation variational
framework. The reconstruction task is seen as a joint learning problem of the
prior involved in the variational inner cost and the gradient-based
minimization of the latter: both prior models and solvers are stated as neural
networks with automatic differentiation which can be trained by minimizing a
loss function, typically stated as the mean squared error between some ground
truth and the reconstruction. In this work, we draw from the SPDE-based
Gaussian Processes to estimate complex prior models able to handle
non-stationary covariances in both space and time and provide a stochastic
framework for interpretability and uncertainty quantification. Our neural
variational scheme is modified to embed an augmented state formulation with
both state and SPDE parametrization to estimate. Instead of a neural prior, we
use a stochastic PDE as surrogate model along the data assimilation window. The
training involves a loss function for both reconstruction task and SPDE prior
model, where the likelihood of the SPDE parameters given the true states is
involved in the training. Because the prior is stochastic, we can easily draw
samples in the prior distribution before conditioning to provide a flexible way
to estimate the posterior distribution based on thousands of members.</div><div><a href='http://arxiv.org/abs/2402.01855v1'>2402.01855v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10620v1")'>Polytopic Autoencoders with Smooth Clustering for Reduced-order
  Modelling of Flows</div>
<div id='2401.10620v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T10:52:57Z</div><div>Authors: Jan Heiland, Yongho Kim</div><div style='padding-top: 10px; width: 80ex'>With the advancement of neural networks, there has been a notable increase,
both in terms of quantity and variety, in research publications concerning the
application of autoencoders to reduced-order models. We propose a polytopic
autoencoder architecture that includes a lightweight nonlinear encoder, a
convex combination decoder, and a smooth clustering network. Supported by
several proofs, the model architecture ensures that all reconstructed states
lie within a polytope, accompanied by a metric indicating the quality of the
constructed polytopes, referred to as polytope error. Additionally, it offers a
minimal number of convex coordinates for polytopic linear-parameter varying
systems while achieving acceptable reconstruction errors compared to proper
orthogonal decomposition (POD). To validate our proposed model, we conduct
simulations involving two flow scenarios with the incompressible Navier-Stokes
equation. Numerical results demonstrate the guaranteed properties of the model,
low reconstruction errors compared to POD, and the improvement in error using a
clustering network.</div><div><a href='http://arxiv.org/abs/2401.10620v1'>2401.10620v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03534v1")'>Physics-informed Neural Networks for Encoding Dynamics in Real Physical
  Systems</div>
<div id='2401.03534v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T16:19:28Z</div><div>Authors: Hamza Alsharif</div><div style='padding-top: 10px; width: 80ex'>This dissertation investigates physics-informed neural networks (PINNs) as
candidate models for encoding governing equations, and assesses their
performance on experimental data from two different systems. The first system
is a simple nonlinear pendulum, and the second is 2D heat diffusion across the
surface of a metal block. We show that for the pendulum system the PINNs
outperformed equivalent uninformed neural networks (NNs) in the ideal data
case, with accuracy improvements of 18x and 6x for 10 linearly-spaced and 10
uniformly-distributed random training points respectively. In similar test
cases with real data collected from an experiment, PINNs outperformed NNs with
9.3x and 9.1x accuracy improvements for 67 linearly-spaced and
uniformly-distributed random points respectively. For the 2D heat diffusion, we
show that both PINNs and NNs do not fare very well in reconstructing the
heating regime due to difficulties in optimizing the network parameters over a
large domain in both time and space. We highlight that data denoising and
smoothing, reducing the size of the optimization problem, and using LBFGS as
the optimizer are all ways to improve the accuracy of the predicted solution
for both PINNs and NNs. Additionally, we address the viability of deploying
physics-informed models within physical systems, and we choose FPGAs as the
compute substrate for deployment. In light of this, we perform our experiments
using a PYNQ-Z1 FPGA and identify issues related to time-coherent sensing and
spatial data alignment. We discuss the insights gained from this work and list
future work items based on the proposed architecture for the system that our
methods work to develop.</div><div><a href='http://arxiv.org/abs/2401.03534v1'>2401.03534v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08667v1")'>Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective</div>
<div id='2401.08667v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T16:31:16Z</div><div>Authors: Sunwoong Yang, Hojin Kim, Yoonpyo Hong, Kwanjung Yee, Romit Maulik, Namwoo Kang</div><div style='padding-top: 10px; width: 80ex'>This study explores the potential of physics-informed neural networks (PINNs)
for the realization of digital twins (DT) from various perspectives. First,
various adaptive sampling approaches for collocation points are investigated to
verify their effectiveness in the mesh-free framework of PINNs, which allows
automated construction of virtual representation without manual mesh
generation. Then, the overall performance of the data-driven PINNs (DD-PINNs)
framework is examined, which can utilize the acquired datasets in DT scenarios.
Its scalability to more general physics is validated within parametric
Navier-Stokes equations, where PINNs do not need to be retrained as the
Reynolds number varies. In addition, since datasets can be often collected from
different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also
proposed and evaluated. They show remarkable prediction performance even in the
extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity
approach. Finally, the uncertainty quantification performance of multi-fidelity
DD-PINNs is investigated by the ensemble method to verify their potential in
DT, where an accurate measure of predictive uncertainty is critical. The
DD-PINN frameworks explored in this study are found to be more suitable for DT
scenarios than traditional PINNs from the above perspectives, bringing
engineers one step closer to seamless DT realization.</div><div><a href='http://arxiv.org/abs/2401.08667v1'>2401.08667v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16903v1")'>A novel data generation scheme for surrogate modelling with deep
  operator networks</div>
<div id='2402.16903v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T14:42:42Z</div><div>Authors: Shivam Choubey, Birupaksha Pal, Manish Agrawal</div><div style='padding-top: 10px; width: 80ex'>Operator-based neural network architectures such as DeepONets have emerged as
a promising tool for the surrogate modeling of physical systems. In general,
towards operator surrogate modeling, the training data is generated by solving
the PDEs using techniques such as Finite Element Method (FEM). The
computationally intensive nature of data generation is one of the biggest
bottleneck in deploying these surrogate models for practical applications. In
this study, we propose a novel methodology to alleviate the computational
burden associated with training data generation for DeepONets. Unlike existing
literature, the proposed framework for data generation does not use any partial
differential equation integration strategy, thereby significantly reducing the
computational cost associated with generating training dataset for DeepONet. In
the proposed strategy, first, the output field is generated randomly,
satisfying the boundary conditions using Gaussian Process Regression (GPR).
From the output field, the input source field can be calculated easily using
finite difference techniques. The proposed methodology can be extended to other
operator learning methods, making the approach widely applicable. To validate
the proposed approach, we employ the heat equations as the model problem and
develop the surrogate model for numerous boundary value problems.</div><div><a href='http://arxiv.org/abs/2402.16903v1'>2402.16903v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02398v1")'>Generating synthetic data for neural operators</div>
<div id='2401.02398v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T18:31:21Z</div><div>Authors: Erisa Hasani, Rachel A. Ward</div><div style='padding-top: 10px; width: 80ex'>Numerous developments in the recent literature show the promising potential
of deep learning in obtaining numerical solutions to partial differential
equations (PDEs) beyond the reach of current numerical solvers. However,
data-driven neural operators all suffer from the same problem: the data needed
to train a network depends on classical numerical solvers such as finite
difference or finite element, among others. In this paper, we propose a new
approach to generating synthetic functional training data that does not require
solving a PDE numerically. The way we do this is simple: we draw a large number
$N$ of independent and identically distributed `random functions' $u_j$ from
the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the
solution lies according to classical theory. We then plug each such random
candidate solution into the equation and get a corresponding right-hand side
function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as
supervised training data for learning the underlying inverse problem $f
\rightarrow u$. This `backwards' approach to generating training data only
requires derivative computations, in contrast to standard `forward' approaches,
which require a numerical PDE solver, enabling us to generate a large number of
such data points quickly and efficiently. While the idea is simple, we hope
that this method will expand the potential for developing neural PDE solvers
that do not depend on classical numerical solvers.</div><div><a href='http://arxiv.org/abs/2401.02398v1'>2401.02398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.17185v1")'>Inpainting Computational Fluid Dynamics with Deep Learning</div>
<div id='2402.17185v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T03:44:55Z</div><div>Authors: Dule Shu, Wilson Zhen, Zijie Li, Amir Barati Farimani</div><div style='padding-top: 10px; width: 80ex'>Fluid data completion is a research problem with high potential benefit for
both experimental and computational fluid dynamics. An effective fluid data
completion method reduces the required number of sensors in a fluid dynamics
experiment, and allows a coarser and more adaptive mesh for a Computational
Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid
data completion problem makes it prohibitively difficult to obtain a
theoretical solution and presents high numerical uncertainty and instability
for a data-driven approach (e.g., a neural network model). To address these
challenges, we leverage recent advancements in computer vision, employing the
vector quantization technique to map both complete and incomplete fluid data
spaces onto discrete-valued lower-dimensional representations via a two-stage
learning procedure. We demonstrated the effectiveness of our approach on
Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different
size and arrangement. Experimental results show that our proposed model
consistently outperforms benchmark models under different occlusion settings in
terms of point-wise reconstruction accuracy as well as turbulent energy
spectrum and vorticity distribution.</div><div><a href='http://arxiv.org/abs/2402.17185v1'>2402.17185v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15318v1")'>Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting</div>
<div id='2401.15318v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T06:45:22Z</div><div>Authors: Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang</div><div style='padding-top: 10px; width: 80ex'>We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.</div><div><a href='http://arxiv.org/abs/2401.15318v1'>2401.15318v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14244v1")'>Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering</div>
<div id='2403.14244v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T09:02:31Z</div><div>Authors: Yuanhao Gong, Lantao Yu, Guanghui Yue</div><div style='padding-top: 10px; width: 80ex'>The 3D Gaussian splatting method has drawn a lot of attention, thanks to its
high performance in training and high quality of the rendered image. However,
it uses anisotropic Gaussian kernels to represent the scene. Although such
anisotropic kernels have advantages in representing the geometry, they lead to
difficulties in terms of computation, such as splitting or merging two kernels.
In this paper, we propose to use isotropic Gaussian kernels to avoid such
difficulties in the computation, leading to a higher performance method. The
experiments confirm that the proposed method is about {\bf 100X} faster without
losing the geometry representation accuracy. The proposed method can be applied
in a large range applications where the radiance field is needed, such as 3D
reconstruction, view synthesis, and dynamic object modeling.</div><div><a href='http://arxiv.org/abs/2403.14244v1'>2403.14244v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15245v1")'>GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface
  Scattering Representation</div>
<div id='2401.15245v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T23:31:53Z</div><div>Authors: Barış Yıldırım, Murat Kurt</div><div style='padding-top: 10px; width: 80ex'>This paper presents a plugin that adds a representation of homogeneous and
heterogeneous, optically thick, translucent materials on the Blender 3D
modeling tool. The working principle of this plugin is based on a combination
of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based
subsurface scattering method (GenSSS). The proposed plugin has been implemented
using Mitsuba renderer, which is an open source rendering software. The
proposed plugin has been validated on measured subsurface scattering data. It's
shown that the proposed plugin visualizes homogeneous and heterogeneous
subsurface scattering effects, accurately, compactly and computationally
efficiently.</div><div><a href='http://arxiv.org/abs/2401.15245v1'>2401.15245v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15913v1")'>Vision-Informed Flow Image Super-Resolution with Quaternion Spatial
  Modeling and Dynamic Flow Convolution</div>
<div id='2401.15913v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T06:48:16Z</div><div>Authors: Qinglong Cao, Zhengqin Xu, Chao Ma, Xiaokang Yang, Yuntian Chen</div><div style='padding-top: 10px; width: 80ex'>Flow image super-resolution (FISR) aims at recovering high-resolution
turbulent velocity fields from low-resolution flow images. Existing FISR
methods mainly process the flow images in natural image patterns, while the
critical and distinct flow visual properties are rarely considered. This
negligence would cause the significant domain gap between flow and natural
images to severely hamper the accurate perception of flow turbulence, thereby
undermining super-resolution performance. To tackle this dilemma, we
comprehensively consider the flow visual properties, including the unique flow
imaging principle and morphological information, and propose the first flow
visual property-informed FISR algorithm. Particularly, different from natural
images that are constructed by independent RGB channels in the light field,
flow images build on the orthogonal UVW velocities in the flow field. To
empower the FISR network with an awareness of the flow imaging principle, we
propose quaternion spatial modeling to model this orthogonal spatial
relationship for improved FISR. Moreover, due to viscosity and surface tension
characteristics, fluids often exhibit a droplet-like morphology in flow images.
Inspired by this morphological property, we design the dynamic flow convolution
to effectively mine the morphological information to enhance FISR. Extensive
experiments on the newly acquired flow image datasets demonstrate the
state-of-the-art performance of our method. Code and data will be made
available.</div><div><a href='http://arxiv.org/abs/2401.15913v1'>2401.15913v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.18846v1")'>Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling</div>
<div id='2402.18846v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T04:40:25Z</div><div>Authors: Ruijia Niu, Dongxia Wu, Kai Kim, Yi-An Ma, Duncan Watson-Parris, Rose Yu</div><div style='padding-top: 10px; width: 80ex'>Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the
highest fidelity level by combining data from multiple sources. Traditional
methods relying on Gaussian processes can hardly scale to high-dimensional
data. Deep learning approaches utilize neural network based encoders and
decoders to improve scalability. These approaches share encoded representations
across fidelities without including corresponding decoder parameters. At the
highest fidelity, the representations are decoded with different parameters,
making the shared information inherently inaccurate. This hinders inference
performance, especially in out-of-distribution scenarios when the highest
fidelity data has limited domain coverage. To address these limitations, we
propose Multi-fidelity Residual Neural Processes (MFRNP), a novel
multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity
decoders for accurate information sharing by aggregating lower fidelity
surrogate outputs and models residual between the aggregation and ground truth
on the highest fidelity. We show that MFRNP significantly outperforms current
state-of-the-art in learning partial differential equations and a real-world
climate modeling task.</div><div><a href='http://arxiv.org/abs/2402.18846v1'>2402.18846v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03459v1")'>TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs</div>
<div id='2403.03459v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T04:49:18Z</div><div>Authors: Yanlai Chen, Yajie Ji, Akil Narayan, Zhenli Xu</div><div style='padding-top: 10px; width: 80ex'>We introduce the Transformed Generative Pre-Trained Physics-Informed Neural
Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of
transport-dominated partial differential equations in an MOR-integrating PINNs
framework. Building on the recent development of the GPT-PINN that is a
network-of-networks design achieving snapshot-based model reduction, we design
and test a novel paradigm for nonlinear model reduction that can effectively
tackle problems with parameter-dependent discontinuities. Through incorporation
of a shock-capturing loss function component as well as a parameter-dependent
transform layer, the TGPT-PINN overcomes the limitations of linear model
reduction in the transport-dominated regime. We demonstrate this new capability
for nonlinear model reduction in the PINNs framework by several nontrivial
parametric partial differential equations.</div><div><a href='http://arxiv.org/abs/2403.03459v1'>2403.03459v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04750v1")'>JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework</div>
<div id='2403.04750v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T18:53:53Z</div><div>Authors: Artur P. Toshev, Harish Ramachandran, Jonas A. Erbesdobler, Gianluca Galletti, Johannes Brandstetter, Nikolaus A. Adams</div><div style='padding-top: 10px; width: 80ex'>Particle-based fluid simulations have emerged as a powerful tool for solving
the Navier-Stokes equations, especially in cases that include intricate physics
and free surfaces. The recent addition of machine learning methods to the
toolbox for solving such problems is pushing the boundary of the quality vs.
speed tradeoff of such numerical simulations. In this work, we lead the way to
Lagrangian fluid simulators compatible with deep learning frameworks, and
propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented
in JAX. JAX-SPH builds on the code for dataset generation from the
LagrangeBench project (Toshev et al., 2023) and extends this code in multiple
ways: (a) integration of further key SPH algorithms, (b) restructuring the code
toward a Python library, (c) verification of the gradients through the solver,
and (d) demonstration of the utility of the gradients for solving inverse
problems as well as a Solver-in-the-Loop application. Our code is available at
https://github.com/tumaer/jax-sph.</div><div><a href='http://arxiv.org/abs/2403.04750v1'>2403.04750v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02425v1")'>EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics</div>
<div id='2402.02425v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T09:45:35Z</div><div>Authors: Qilong Ma, Haixu Wu, Lanxiang Xing, Jianmin Wang, Mingsheng Long</div><div style='padding-top: 10px; width: 80ex'>Accurately predicting the future fluid is important to extensive areas, such
as meteorology, oceanology and aerodynamics. However, since the fluid is
usually observed from an Eulerian perspective, its active and intricate
dynamics are seriously obscured and confounded in static grids, bringing horny
challenges to the prediction. This paper introduces a new Lagrangian-guided
paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting
the future based on Eulerian observations, we propose the Eulerian-Lagrangian
Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by
tracking movements of adaptively sampled key particles on multiple scales and
integrating dynamics information over time. Concretely, a EuLag Block is
presented to communicate the learned Eulerian and Lagrangian features at each
moment and scale, where the motion of tracked particles is inferred from
Eulerian observations and their accumulated dynamics information is
incorporated into Eulerian fields to guide future prediction. Tracking key
particles not only provides a clear and interpretable clue for fluid dynamics
but also makes our model free from modeling complex correlations among massive
grids for better efficiency. Experimentally, EuLagNet excels in three
challenging fluid prediction tasks, covering both 2D and 3D, simulated and
real-world fluids.</div><div><a href='http://arxiv.org/abs/2402.02425v1'>2402.02425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09198v3")'>Space and Time Continuous Physics Simulation From Partial Observations</div>
<div id='2401.09198v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T13:24:04Z</div><div>Authors: Janny Steeven, Nadri Madiha, Digne Julie, Wolf Christian</div><div style='padding-top: 10px; width: 80ex'>Modern techniques for physical simulations rely on numerical schemes and
mesh-refinement methods to address trade-offs between precision and complexity,
but these handcrafted solutions are tedious and require high computational
power. Data-driven methods based on large-scale machine learning promise high
adaptivity by integrating long-range dependencies more directly and
efficiently. In this work, we focus on fluid dynamics and address the
shortcomings of a large part of the literature, which are based on fixed
support for computations and predictions in the form of regular or irregular
grids. We propose a novel setup to perform predictions in a continuous spatial
and temporal domain while being trained on sparse observations. We formulate
the task as a double observation problem and propose a solution with two
interlinked dynamical systems defined on, respectively, the sparse positions
and the continuous domain, which allows to forecast and interpolate a solution
from the initial condition. Our practical implementation involves recurrent
GNNs and a spatio-temporal attention observer capable of interpolating the
solution at arbitrary locations. Our model not only generalizes to new initial
conditions (as standard auto-regressive models do) but also performs evaluation
at arbitrary space and time locations. We evaluate on three standard datasets
in fluid dynamics and compare to strong baselines, which are outperformed both
in classical settings and in the extended new task requiring continuous
predictions.</div><div><a href='http://arxiv.org/abs/2401.09198v3'>2401.09198v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12971v1")'>How Temporal Unrolling Supports Neural Physics Simulators</div>
<div id='2402.12971v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T12:40:31Z</div><div>Authors: Bjoern List, Li-Wei Chen, Kartik Bali, Nils Thuerey</div><div style='padding-top: 10px; width: 80ex'>Unrolling training trajectories over time strongly influences the inference
accuracy of neural network-augmented physics simulators. We analyze these
effects by studying three variants of training neural networks on discrete
ground truth trajectories. In addition to commonly used one-step setups and
fully differentiable unrolling, we include a third, less widely used variant:
unrolling without temporal gradients. Comparing networks trained with these
three modalities makes it possible to disentangle the two dominant effects of
unrolling, training distribution shift and long-term gradients. We present a
detailed study across physical systems, network sizes, network architectures,
training setups, and test scenarios. It provides an empirical basis for our
main findings: A non-differentiable but unrolled training setup supported by a
numerical solver can yield 4.5-fold improvements over a fully differentiable
prediction setup that does not utilize this solver. We also quantify a
difference in the accuracy of models trained in a fully differentiable setup
compared to their non-differentiable counterparts. While differentiable setups
perform best, the accuracy of unrolling without temporal gradients comes
comparatively close. Furthermore, we empirically show that these behaviors are
invariant to changes in the underlying physical system, the network
architecture and size, and the numerical scheme. These results motivate
integrating non-differentiable numerical simulators into training setups even
if full differentiability is unavailable. We also observe that the convergence
rate of common neural architectures is low compared to numerical algorithms.
This encourages the use of hybrid approaches combining neural and numerical
algorithms to utilize the benefits of both.</div><div><a href='http://arxiv.org/abs/2402.12971v1'>2402.12971v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.05970v1")'>Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning
  and Levels-of-Experts</div>
<div id='2402.05970v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:27:07Z</div><div>Authors: Kun Wang, Hao Wu, Guibin Zhang, Junfeng Fang, Yuxuan Liang, Yuankai Wu, Roger Zimmermann, Yang Wang</div><div style='padding-top: 10px; width: 80ex'>In this paper, we address the issue of modeling and estimating changes in the
state of the spatio-temporal dynamical systems based on a sequence of
observations like video frames. Traditional numerical simulation systems depend
largely on the initial settings and correctness of the constructed partial
differential equations (PDEs). Despite recent efforts yielding significant
success in discovering data-driven PDEs with neural networks, the limitations
posed by singular scenarios and the absence of local insights prevent them from
performing effectively in a broader real-world context. To this end, this paper
propose the universal expert module -- that is, optical flow estimation
component, to capture the evolution laws of general physical processes in a
data-driven fashion. To enhance local insight, we painstakingly design a
finer-grained physical pipeline, since local characteristics may be influenced
by various internal contextual information, which may contradict the
macroscopic properties of the whole system. Further, we harness currently
popular neural discrete learning to unveil the underlying important features in
its latent space, this process better injects interpretability, which can help
us obtain a powerful prior over these discrete random variables. We conduct
extensive experiments and ablations to demonstrate that the proposed framework
achieves large performance margins, compared with the existing SOTA baselines.</div><div><a href='http://arxiv.org/abs/2402.05970v1'>2402.05970v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02366v1")'>Transolver: A Fast Transformer Solver for PDEs on General Geometries</div>
<div id='2402.02366v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T06:37:38Z</div><div>Authors: Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, Mingsheng Long</div><div style='padding-top: 10px; width: 80ex'>Transformers have empowered many milestones across various fields and have
recently been applied to solve partial differential equations (PDEs). However,
since PDEs are typically discretized into large-scale meshes with complex
geometries, it is challenging for Transformers to capture intricate physical
correlations directly from massive individual points. Going beyond superficial
and unwieldy meshes, we present Transolver based on a more foundational idea,
which is learning intrinsic physical states hidden behind discretized
geometries. Specifically, we propose a new Physics-Attention to adaptively
split the discretized domain into a series of learnable slices of flexible
shapes, where mesh points under similar physical states will be ascribed to the
same slice. By calculating attention to physics-aware tokens encoded from
slices, Transovler can effectively capture intricate physical correlations
under complex geometrics, which also empowers the solver with endogenetic
geometry-general modeling capacity and can be efficiently computed in linear
complexity. Transolver achieves consistent state-of-the-art with 22\% relative
gain across six standard benchmarks and also excels in large-scale industrial
simulations, including car and airfoil designs.</div><div><a href='http://arxiv.org/abs/2402.02366v1'>2402.02366v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12553v1")'>Pretraining Codomain Attention Neural Operators for Solving Multiphysics
  PDEs</div>
<div id='2403.12553v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T08:56:20Z</div><div>Authors: Md Ashiqur Rahman, Robert Joseph George, Mogab Elleithy, Daniel Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A. Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar</div><div style='padding-top: 10px; width: 80ex'>Existing neural operator architectures face challenges when solving
multiphysics problems with coupled partial differential equations (PDEs), due
to complex geometries, interactions between physical variables, and the lack of
large amounts of high-resolution training data. To address these issues, we
propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions
along the codomain or channel space, enabling self-supervised learning or
pretraining of multiple PDE systems. Specifically, we extend positional
encoding, self-attention, and normalization layers to the function space.
CoDA-NO can learn representations of different PDE systems with a single model.
We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs
over multiple systems by considering few-shot learning settings. On complex
downstream tasks with limited data, such as fluid flow simulations and
fluid-structure interactions, we found CoDA-NO to outperform existing methods
on the few-shot learning task by over $36\%$. The code is available at
https://github.com/ashiq24/CoDA-NO.</div><div><a href='http://arxiv.org/abs/2403.12553v1'>2403.12553v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01132v1")'>MPIPN: A Multi Physics-Informed PointNet for solving parametric
  acoustic-structure systems</div>
<div id='2403.01132v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T08:27:05Z</div><div>Authors: Chu Wang, Jinhong Wu, Yanzhi Wang, Zhijian Zha, Qi Zhou</div><div style='padding-top: 10px; width: 80ex'>Machine learning is employed for solving physical systems governed by general
nonlinear partial differential equations (PDEs). However, complex multi-physics
systems such as acoustic-structure coupling are often described by a series of
PDEs that incorporate variable physical quantities, which are referred to as
parametric systems. There are lack of strategies for solving parametric systems
governed by PDEs that involve explicit and implicit quantities. In this paper,
a deep learning-based Multi Physics-Informed PointNet (MPIPN) is proposed for
solving parametric acoustic-structure systems. First, the MPIPN induces an
enhanced point-cloud architecture that encompasses explicit physical quantities
and geometric features of computational domains. Then, the MPIPN extracts local
and global features of the reconstructed point-cloud as parts of solving
criteria of parametric systems, respectively. Besides, implicit physical
quantities are embedded by encoding techniques as another part of solving
criteria. Finally, all solving criteria that characterize parametric systems
are amalgamated to form distinctive sequences as the input of the MPIPN, whose
outputs are solutions of systems. The proposed framework is trained by adaptive
physics-informed loss functions for corresponding computational domains. The
framework is generalized to deal with new parametric conditions of systems. The
effectiveness of the MPIPN is validated by applying it to solve steady
parametric acoustic-structure coupling systems governed by the Helmholtz
equations. An ablation experiment has been implemented to demonstrate the
efficacy of physics-informed impact with a minority of supervised data. The
proposed method yields reasonable precision across all computational domains
under constant parametric conditions and changeable combinations of parametric
conditions for acoustic-structure systems.</div><div><a href='http://arxiv.org/abs/2403.01132v1'>2403.01132v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.08055v1")'>DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design
  and Graph-Based Drag Prediction</div>
<div id='2403.08055v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T20:02:39Z</div><div>Authors: Mohamed Elrefaie, Angela Dai, Faez Ahmed</div><div style='padding-top: 10px; width: 80ex'>This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of
3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional
neural network model, both aimed at aerodynamic car design through machine
learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million
surface mesh faces and comprehensive aerodynamic performance data comprising of
full 3D pressure, velocity fields, and wall-shear stresses, addresses the
critical need for extensive datasets to train deep learning models in
engineering applications. It is 60\% larger than the previously available
largest public dataset of cars, and is the only open-source dataset that also
models wheels and underbody. RegDGCNN leverages this large-scale dataset to
provide high-precision drag estimates directly from 3D meshes, bypassing
traditional limitations such as the need for 2D image rendering or Signed
Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN
facilitates rapid aerodynamic assessments, offering a substantial leap towards
integrating data-driven methods in automotive design. Together, DrivAerNet and
RegDGCNN promise to accelerate the car design process and contribute to the
development of more efficient vehicles. To lay the groundwork for future
innovations in the field, the dataset and code used in our study are publicly
accessible at \url{https://github.com/Mohamedelrefaie/DrivAerNet}</div><div><a href='http://arxiv.org/abs/2403.08055v1'>2403.08055v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09234v2")'>Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash
  Simulations Using Graph Convolutional Neural Networks</div>
<div id='2402.09234v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T15:22:59Z</div><div>Authors: Jonas Kneifl, Jörg Fehr, Steven L. Brunton, J. Nathan Kutz</div><div style='padding-top: 10px; width: 80ex'>Crash simulations play an essential role in improving vehicle safety, design
optimization, and injury risk estimation. Unfortunately, numerical solutions of
such problems using state-of-the-art high-fidelity models require significant
computational effort. Conventional data-driven surrogate modeling approaches
create low-dimensional embeddings for evolving the dynamics in order to
circumvent this computational effort. Most approaches directly operate on
high-resolution data obtained from numerical discretization, which is both
costly and complicated for mapping the flow of information over large spatial
distances. Furthermore, working with a fixed resolution prevents the adaptation
of surrogate models to environments with variable computing capacities,
different visualization resolutions, and different accuracy requirements. We
thus propose a multi-hierarchical framework for structurally creating a series
of surrogate models for a kart frame, which is a good proxy for
industrial-relevant crash simulations, at different levels of resolution. For
multiscale phenomena, macroscale features are captured on a coarse surrogate,
whereas microscale effects are resolved by finer ones. The learned behavior of
the individual surrogates is passed from coarse to finer levels through
transfer learning. In detail, we perform a mesh simplification on the kart
model to obtain multi-resolution representations of it. We then train a
graph-convolutional neural network-based surrogate that learns
parameter-dependent low-dimensional latent dynamics on the coarsest
representation. Subsequently, another, similarly structured surrogate is
trained on the residual of the first surrogate using a finer resolution. This
step can be repeated multiple times. By doing so, we construct multiple
surrogates for the same system with varying hardware requirements and
increasing accuracy.</div><div><a href='http://arxiv.org/abs/2402.09234v2'>2402.09234v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00972v1")'>Closure Discovery for Coarse-Grained Partial Differential Equations
  using Multi-Agent Reinforcement Learning</div>
<div id='2402.00972v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T19:41:04Z</div><div>Authors: Jan-Philipp von Bassewitz, Sebastian Kaltenbach, Petros Koumoutsakos</div><div style='padding-top: 10px; width: 80ex'>Reliable predictions of critical phenomena, such as weather, wildfires and
epidemics are often founded on models described by Partial Differential
Equations (PDEs). However, simulations that capture the full range of
spatio-temporal scales in such PDEs are often prohibitively expensive.
Consequently, coarse-grained simulations that employ heuristics and empirical
closure terms are frequently utilized as an alternative. We propose a novel and
systematic approach for identifying closures in under-resolved PDEs using
Multi-Agent Reinforcement Learning (MARL). The MARL formulation incorporates
inductive bias and exploits locality by deploying a central policy represented
efficiently by Convolutional Neural Networks (CNN). We demonstrate the
capabilities and limitations of MARL through numerical solutions of the
advection equation and the Burgers' equation. Our results show accurate
predictions for in- and out-of-distribution test cases as well as a significant
speedup compared to resolving all scales.</div><div><a href='http://arxiv.org/abs/2402.00972v1'>2402.00972v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04631v1")'>Deep Reinforcement Multi-agent Learning framework for Information
  Gathering with Local Gaussian Processes for Water Monitoring</div>
<div id='2401.04631v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T15:58:15Z</div><div>Authors: Samuel Yanes Luis, Dmitriy Shutin, Juan Marchal Gómez, Daniel Gutiérrez Reina, Sergio Toral Marín</div><div style='padding-top: 10px; width: 80ex'>The conservation of hydrological resources involves continuously monitoring
their contamination. A multi-agent system composed of autonomous surface
vehicles is proposed in this paper to efficiently monitor the water quality. To
achieve a safe control of the fleet, the fleet policy should be able to act
based on measurements and to the the fleet state. It is proposed to use Local
Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective
monitoring policies. Local Gaussian processes, unlike classical global Gaussian
processes, can accurately model the information in a dissimilar spatial
correlation which captures more accurately the water quality information. A
Deep convolutional policy is proposed, that bases the decisions on the
observation on the mean and variance of this model, by means of an information
gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to
minimize the estimation error in a safe manner thanks to a Consensus-based
heuristic. Simulation results indicate an improvement of up to 24% in terms of
the mean absolute error with the proposed models. Also, training results with
1-3 agents indicate that our proposed approach returns 20% and 24% smaller
average estimation errors for, respectively, monitoring water quality variables
and monitoring algae blooms, as compared to state-of-the-art approaches</div><div><a href='http://arxiv.org/abs/2401.04631v1'>2401.04631v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13850v1")'>Spatio-Temporal Fluid Dynamics Modeling via Physical-Awareness and
  Parameter Diffusion Guidance</div>
<div id='2403.13850v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:57:47Z</div><div>Authors: Hao Wu, Fan Xu, Yifan Duan, Ziwei Niu, Weiyan Wang, Gaofeng Lu, Kun Wang, Yuxuan Liang, Yang Wang</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a two-stage framework named ST-PAD for spatio-temporal
fluid dynamics modeling in the field of earth sciences, aiming to achieve
high-precision simulation and prediction of fluid dynamics through
spatio-temporal physics awareness and parameter diffusion guidance. In the
upstream stage, we design a vector quantization reconstruction module with
temporal evolution characteristics, ensuring balanced and resilient parameter
distribution by introducing general physical constraints. In the downstream
stage, a diffusion probability network involving parameters is utilized to
generate high-quality future states of fluids, while enhancing the model's
generalization ability by perceiving parameters in various physical setups.
Extensive experiments on multiple benchmark datasets have verified the
effectiveness and robustness of the ST-PAD framework, which showcase that
ST-PAD outperforms current mainstream models in fluid dynamics modeling and
prediction, especially in effectively capturing local representations and
maintaining significant advantages in OOD generations.</div><div><a href='http://arxiv.org/abs/2403.13850v1'>2403.13850v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16196v1")'>Combining Machine Learning with Computational Fluid Dynamics using
  OpenFOAM and SmartSim</div>
<div id='2402.16196v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T20:39:44Z</div><div>Authors: Tomislav Maric, Mohammed Elwardi Fadeli, Alessandro Rigazzi, Andrew Shao, Andre Weiner</div><div style='padding-top: 10px; width: 80ex'>Combining machine learning (ML) with computational fluid dynamics (CFD) opens
many possibilities for improving simulations of technical and natural systems.
However, CFD+ML algorithms require exchange of data, synchronization, and
calculation on heterogeneous hardware, making their implementation for
large-scale problems exceptionally challenging.
  We provide an effective and scalable solution to developing CFD+ML algorithms
using open source software OpenFOAM and SmartSim. SmartSim provides an
Orchestrator that significantly simplifies the programming of CFD+ML algorithms
and a Redis database that ensures highly scalable data exchange between ML and
CFD clients. We show how to leverage SmartSim to effectively couple different
segments of OpenFOAM with ML, including pre/post-processing applications,
solvers, function objects, and mesh motion solvers. We additionally provide an
OpenFOAM sub-module with examples that can be used as starting points for
real-world applications in CFD+ML.</div><div><a href='http://arxiv.org/abs/2402.16196v1'>2402.16196v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13672v1")'>Machine Learning Optimized Approach for Parameter Selection in MESHFREE
  Simulations</div>
<div id='2403.13672v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T15:29:59Z</div><div>Authors: Paulami Banerjee, Mohan Padmanabha, Chaitanya Sanghavi, Isabel Michel, Simone Gramsch</div><div style='padding-top: 10px; width: 80ex'>Meshfree simulation methods are emerging as compelling alternatives to
conventional mesh-based approaches, particularly in the fields of Computational
Fluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a
comprehensive overview of our research combining Machine Learning (ML) and
Fraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a
numerical point cloud in a Generalized Finite Difference Method (GFDM). This
tool enables the effective handling of complex flow domains, moving geometries,
and free surfaces, while allowing users to finely tune local refinement and
quality parameters for an optimal balance between computation time and results
accuracy. However, manually determining the optimal parameter combination poses
challenges, especially for less experienced users. We introduce a novel
ML-optimized approach, using active learning, regression trees, and
visualization on MESHFREE simulation data, demonstrating the impact of input
combinations on results quality and computation time. This research contributes
valuable insights into parameter optimization in meshfree simulations,
enhancing accessibility and usability for a broader user base in scientific and
engineering applications.</div><div><a href='http://arxiv.org/abs/2403.13672v1'>2403.13672v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00774v1")'>Mesh motion in fluid-structure interaction with deep operator networks</div>
<div id='2402.00774v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:04:04Z</div><div>Authors: Ottar Hellan</div><div style='padding-top: 10px; width: 80ex'>A mesh motion model based on deep operator networks is presented. The model
is trained on and evaluated against a biharmonic mesh motion model on a
fluid-structure interaction benchmark problem and further evaluated in a
setting where biharmonic mesh motion fails. The performance of the proposed
mesh motion model is comparable to the biharmonic mesh motion on the test
problems.</div><div><a href='http://arxiv.org/abs/2402.00774v1'>2402.00774v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.11179v1")'>Uncertainty Quantification of Graph Convolution Neural Network Models of
  Evolving Processes</div>
<div id='2402.11179v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T03:19:23Z</div><div>Authors: Jeremiah Hauth, Cosmin Safta, Xun Huan, Ravi G. Patel, Reese E. Jones</div><div style='padding-top: 10px; width: 80ex'>The application of neural network models to scientific machine learning tasks
has proliferated in recent years. In particular, neural network models have
proved to be adept at modeling processes with spatial-temporal complexity.
Nevertheless, these highly parameterized models have garnered skepticism in
their ability to produce outputs with quantified error bounds over the regimes
of interest. Hence there is a need to find uncertainty quantification methods
that are suitable for neural networks. In this work we present comparisons of
the parametric uncertainty quantification of neural networks modeling complex
spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational
gradient descent and its projected variant. Specifically we apply these methods
to graph convolutional neural network models of evolving systems modeled with
recurrent neural network and neural ordinary differential equations
architectures. We show that Stein variational inference is a viable alternative
to Monte Carlo methods with some clear advantages for complex neural network
models. For our exemplars, Stein variational interference gave similar
uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit
with generally more generous variance.Projected Stein variational gradient
descent also produced similar uncertainty profiles to the non-projected
counterpart, but large reductions in the active weight space were confounded by
the stability of the neural network predictions and the convoluted likelihood
landscape.</div><div><a href='http://arxiv.org/abs/2402.11179v1'>2402.11179v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13299v1")'>Bridging scales in multiscale bubble growth dynamics with correlated
  fluctuations using neural operator learning</div>
<div id='2403.13299v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T04:56:02Z</div><div>Authors: Minglei Lu, Chensen Lin, Martian Maxey, George Karniadakis, Zhen Li</div><div style='padding-top: 10px; width: 80ex'>The intricate process of bubble growth dynamics involves a broad spectrum of
physical phenomena from microscale mechanics of bubble formation to macroscale
interplay between bubbles and surrounding thermo-hydrodynamics. Traditional
bubble dynamics models including atomistic approaches and continuum-based
methods segment the bubble dynamics into distinct scale-specific models. In
order to bridge the gap between microscale stochastic fluid models and
continuum-based fluid models for bubble dynamics, we develop a composite neural
operator model to unify the analysis of nonlinear bubble dynamics across
microscale and macroscale regimes by integrating a many-body dissipative
particle dynamics (mDPD) model with a continuum-based Rayleigh-Plesset (RP)
model through a novel neural network architecture, which consists of a deep
operator network for learning the mean behavior of bubble growth subject to
pressure variations and a long short-term memory network for learning the
statistical features of correlated fluctuations in microscale bubble dynamics.
Training and testing data are generated by conducting mDPD and RP simulations
for nonlinear bubble dynamics with initial bubble radii ranging from 0.1 to 1.5
micrometers. Results show that the trained composite neural operator model can
accurately predict bubble dynamics across scales, with a 99% accuracy for the
time evaluation of the bubble radius under varying external pressure while
containing correct size-dependent stochastic fluctuations in microscale bubble
growth dynamics. The composite neural operator is the first deep learning
surrogate for multiscale bubble growth dynamics that can capture correct
stochastic fluctuations in microscopic fluid phenomena, which sets a new
direction for future research in multiscale fluid dynamics modeling.</div><div><a href='http://arxiv.org/abs/2403.13299v1'>2403.13299v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06755v2")'>Solving the Discretised Multiphase Flow Equations with Interface
  Capturing on Structured Grids Using Machine Learning Libraries</div>
<div id='2401.06755v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T18:42:42Z</div><div>Authors: Boyang Chen, Claire E. Heaney, Jefferson L. M. A. Gomes, Omar K. Matar, Christopher C. Pain</div><div style='padding-top: 10px; width: 80ex'>This paper solves the discretised multiphase flow equations using tools and
methods from machine-learning libraries. The idea comes from the observation
that convolutional layers can be used to express a discretisation as a neural
network whose weights are determined by the numerical method, rather than by
training, and hence, we refer to this approach as Neural Networks for PDEs
(NN4PDEs). To solve the discretised multiphase flow equations, a multigrid
solver is implemented through a convolutional neural network with a U-Net
architecture. Immiscible two-phase flow is modelled by the 3D incompressible
Navier-Stokes equations with surface tension and advection of a volume fraction
field, which describes the interface between the fluids. A new compressive
algebraic volume-of-fluids method is introduced, based on a residual
formulation using Petrov-Galerkin for accuracy and designed with NN4PDEs in
mind. High-order finite-element based schemes are chosen to model a collapsing
water column and a rising bubble. Results compare well with experimental data
and other numerical results from the literature, demonstrating that, for the
first time, finite element discretisations of multiphase flows can be solved
using an approach based on (untrained) convolutional neural networks. A benefit
of expressing numerical discretisations as neural networks is that the code can
run, without modification, on CPUs, GPUs or the latest accelerators designed
especially to run AI codes.</div><div><a href='http://arxiv.org/abs/2401.06755v2'>2401.06755v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17913v1")'>Using AI libraries for Incompressible Computational Fluid Dynamics</div>
<div id='2402.17913v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T22:00:50Z</div><div>Authors: Boyang Chen, Claire E. Heaney, Christopher C. Pain</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been a huge effort focused on developing highly efficient
open source libraries to perform Artificial Intelligence (AI) related
computations on different computer architectures (for example, CPUs, GPUs and
new AI processors). This has not only made the algorithms based on these
libraries highly efficient and portable between different architectures, but
also has substantially simplified the entry barrier to develop methods using
AI. Here, we present a novel methodology to bring the power of both AI software
and hardware into the field of numerical modelling by repurposing AI methods,
such as Convolutional Neural Networks (CNNs), for the standard operations
required in the field of the numerical solution of Partial Differential
Equations (PDEs). The aim of this work is to bring the high performance,
architecture agnosticism and ease of use into the field of the numerical
solution of PDEs. We use the proposed methodology to solve the
advection-diffusion equation, the non-linear Burgers equation and
incompressible flow past a bluff body. For the latter, a convolutional neural
network is used as a multigrid solver in order to enforce the incompressibility
constraint. We show that the presented methodology can solve all these problems
using repurposed AI libraries in an efficient way, and presents a new avenue to
explore in the development of methods to solve PDEs and Computational Fluid
Dynamics problems with implicit methods.</div><div><a href='http://arxiv.org/abs/2402.17913v1'>2402.17913v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14848v1")'>Learning WENO for entropy stable schemes to solve conservation laws</div>
<div id='2403.14848v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T21:39:05Z</div><div>Authors: Philip Charles, Deep Ray</div><div style='padding-top: 10px; width: 80ex'>Entropy conditions play a crucial role in the extraction of a physically
relevant solution for a system of conservation laws, thus motivating the
construction of entropy stable schemes that satisfy a discrete analogue of such
conditions. TeCNO schemes (Fjordholm et al. 2012) form a class of arbitrary
high-order entropy stable finite difference solvers, which require specialized
reconstruction algorithms satisfying the sign property at each cell interface.
Recently, third-order WENO schemes called SP-WENO (Fjordholm and Ray, 2016) and
SP-WENOc (Ray, 2018) have been designed to satisfy the sign property. However,
these WENO algorithms can perform poorly near shocks, with the numerical
solutions exhibiting large spurious oscillations. In the present work, we
propose a variant of the SP-WENO, termed as Deep Sign-Preserving WENO
(DSP-WENO), where a neural network is trained to learn the WENO weighting
strategy. The sign property and third-order accuracy are strongly imposed in
the algorithm, which constrains the WENO weight selection region to a convex
polygon. Thereafter, a neural network is trained to select the WENO weights
from this convex region with the goal of improving the shock-capturing
capabilities without sacrificing the rate of convergence in smooth regions. The
proposed synergistic approach retains the mathematical framework of the TeCNO
scheme while integrating deep learning to remedy the computational issues of
the WENO-based reconstruction. We present several numerical experiments to
demonstrate the significant improvement with DSP-WENO over the existing
variants of WENO satisfying the sign property.</div><div><a href='http://arxiv.org/abs/2403.14848v1'>2403.14848v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16845v1")'>Neural Operators with Localized Integral and Differential Kernels</div>
<div id='2402.16845v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:59:31Z</div><div>Authors: Miguel Liu-Schiaffini, Julius Berner, Boris Bonev, Thorsten Kurth, Kamyar Azizzadenesheli, Anima Anandkumar</div><div style='padding-top: 10px; width: 80ex'>Neural operators learn mappings between function spaces, which is practical
for learning solution operators of PDEs and other scientific modeling
applications. Among them, the Fourier neural operator (FNO) is a popular
architecture that performs global convolutions in the Fourier space. However,
such global operations are often prone to over-smoothing and may fail to
capture local details. In contrast, convolutional neural networks (CNN) can
capture local features but are limited to training and inference at a single
resolution. In this work, we present a principled approach to operator learning
that can capture local features under two frameworks by learning differential
operators and integral operators with locally supported kernels. Specifically,
inspired by stencil methods, we prove that we obtain differential operators
under an appropriate scaling of the kernel values of CNNs. To obtain local
integral operators, we utilize suitable basis representations for the kernels
based on discrete-continuous convolutions. Both these approaches preserve the
properties of operator learning and, hence, the ability to predict at any
resolution. Adding our layers to FNOs significantly improves their performance,
reducing the relative L2-error by 34-72% in our experiments on turbulent 2D
Navier-Stokes fluid flow and the spherical shallow water equations.</div><div><a href='http://arxiv.org/abs/2402.16845v1'>2402.16845v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12489v1")'>Unsupervised Learning Method for the Wave Equation Based on Finite
  Difference Residual Constraints Loss</div>
<div id='2401.12489v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T05:06:29Z</div><div>Authors: Xin Feng, Yi Jiang, Jia-Xian Qin, Lai-Ping Zhang, Xiao-Gang Deng</div><div style='padding-top: 10px; width: 80ex'>The wave equation is an important physical partial differential equation, and
in recent years, deep learning has shown promise in accelerating or replacing
traditional numerical methods for solving it. However, existing deep learning
methods suffer from high data acquisition costs, low training efficiency, and
insufficient generalization capability for boundary conditions. To address
these issues, this paper proposes an unsupervised learning method for the wave
equation based on finite difference residual constraints. We construct a novel
finite difference residual constraint based on structured grids and finite
difference methods, as well as an unsupervised training strategy, enabling
convolutional neural networks to train without data and predict the forward
propagation process of waves. Experimental results show that finite difference
residual constraints have advantages over physics-informed neural networks
(PINNs) type physical information constraints, such as easier fitting, lower
computational costs, and stronger source term generalization capability, making
our method more efficient in training and potent in application.</div><div><a href='http://arxiv.org/abs/2401.12489v1'>2401.12489v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02304v4")'>Efficient Numerical Wave Propagation Enhanced By An End-to-End Deep
  Learning Model</div>
<div id='2402.02304v4' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T00:07:05Z</div><div>Authors: Luis Kaiser, Richard Tsai, Christian Klingenberg</div><div style='padding-top: 10px; width: 80ex'>Recent advances in wave modeling use sufficiently accurate fine solver
outputs to train a neural network that enhances the accuracy of a fast but
inaccurate coarse solver. In this paper we build upon the work of Nguyen and
Tsai (2023) and present a novel unified system that integrates a numerical
solver with a deep learning component into an end-to-end framework. In the
proposed setting, we investigate refinements to the network architecture and
data generation algorithm. A stable and fast solver further allows the use of
Parareal, a parallel-in-time algorithm to correct high-frequency wave
components. Our results show that the cohesive structure improves performance
without sacrificing speed, and demonstrate the importance of temporal dynamics,
as well as Parareal, for accurate wave propagation.</div><div><a href='http://arxiv.org/abs/2402.02304v4'>2402.02304v4</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14730v1")'>Clifford-Steerable Convolutional Neural Networks</div>
<div id='2402.14730v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T17:42:15Z</div><div>Authors: Maksim Zhdanov, David Ruhe, Maurice Weiler, Ana Lucic, Johannes Brandstetter, Patrick Forré</div><div style='padding-top: 10px; width: 80ex'>We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a
novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector
fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance,
$\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on
Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit
parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group
equivariant neural networks. We significantly and consistently outperform
baseline methods on fluid dynamics as well as relativistic electrodynamics
forecasting tasks.</div><div><a href='http://arxiv.org/abs/2402.14730v1'>2402.14730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.14084v1")'>Learning-based Multi-continuum Model for Multiscale Flow Problems</div>
<div id='2403.14084v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T02:30:56Z</div><div>Authors: Fan Wang, Yating Wang, Wing Tat Leung, Zongben Xu</div><div style='padding-top: 10px; width: 80ex'>Multiscale problems can usually be approximated through numerical
homogenization by an equation with some effective parameters that can capture
the macroscopic behavior of the original system on the coarse grid to speed up
the simulation. However, this approach usually assumes scale separation and
that the heterogeneity of the solution can be approximated by the solution
average in each coarse block. For complex multiscale problems, the computed
single effective properties/continuum might be inadequate. In this paper, we
propose a novel learning-based multi-continuum model to enrich the homogenized
equation and improve the accuracy of the single continuum model for multiscale
problems with some given data. Without loss of generalization, we consider a
two-continuum case. The first flow equation keeps the information of the
original homogenized equation with an additional interaction term. The second
continuum is newly introduced, and the effective permeability in the second
flow equation is determined by a neural network. The interaction term between
the two continua aligns with that used in the Dual-porosity model but with a
learnable coefficient determined by another neural network. The new model with
neural network terms is then optimized using trusted data. We discuss both
direct back-propagation and the adjoint method for the PDE-constraint
optimization problem. Our proposed learning-based multi-continuum model can
resolve multiple interacted media within each coarse grid block and describe
the mass transfer among them, and it has been demonstrated to significantly
improve the simulation results through numerical experiments involving both
linear and nonlinear flow equations.</div><div><a href='http://arxiv.org/abs/2403.14084v1'>2403.14084v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14646v1")'>CoLoRA: Continuous low-rank adaptation for reduced implicit neural
  modeling of parameterized partial differential equations</div>
<div id='2402.14646v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T15:45:31Z</div><div>Authors: Jules Berman, Benjamin Peherstorfer</div><div style='padding-top: 10px; width: 80ex'>This work introduces reduced models based on Continuous Low Rank Adaptation
(CoLoRA) that pre-train neural networks for a given partial differential
equation and then continuously adapt low-rank weights in time to rapidly
predict the evolution of solution fields at new physics parameters and new
initial conditions. The adaptation can be either purely data-driven or via an
equation-driven variational approach that provides Galerkin-optimal
approximations. Because CoLoRA approximates solution fields locally in time,
the rank of the weights can be kept small, which means that only few training
trajectories are required offline so that CoLoRA is well suited for data-scarce
regimes. Predictions with CoLoRA are orders of magnitude faster than with
classical methods and their accuracy and parameter efficiency is higher
compared to other neural network approaches.</div><div><a href='http://arxiv.org/abs/2402.14646v1'>2402.14646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19242v1")'>Derivative-enhanced Deep Operator Network</div>
<div id='2402.19242v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T15:18:37Z</div><div>Authors: Yuan Qiu, Nolan Bridges, Peng Chen</div><div style='padding-top: 10px; width: 80ex'>Deep operator networks (DeepONets), a class of neural operators that learn
mappings between function spaces, have recently been developed as surrogate
models for parametric partial differential equations (PDEs). In this work we
propose a derivative-enhanced deep operator network (DE-DeepONet), which
leverages the derivative information to enhance the prediction accuracy, and
provide a more accurate approximation of the derivatives, especially when the
training data are limited. DE-DeepONet incorporates dimension reduction of
input into DeepONet and includes two types of derivative labels in the loss
function for training, that is, the directional derivatives of the output
function with respect to the input function and the gradient of the output
function with respect to the physical domain variables. We test DE-DeepONet on
three different equations with increasing complexity to demonstrate its
effectiveness compared to the vanilla DeepONet.</div><div><a href='http://arxiv.org/abs/2402.19242v1'>2402.19242v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09018v1")'>Neural Operators Meet Energy-based Theory: Operator Learning for
  Hamiltonian and Dissipative PDEs</div>
<div id='2402.09018v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T08:50:14Z</div><div>Authors: Yusuke Tanaka, Takaharu Yaguchi, Tomoharu Iwata, Naonori Ueda</div><div style='padding-top: 10px; width: 80ex'>The operator learning has received significant attention in recent years,
with the aim of learning a mapping between function spaces. Prior works have
proposed deep neural networks (DNNs) for learning such a mapping, enabling the
learning of solution operators of partial differential equations (PDEs).
However, these works still struggle to learn dynamics that obeys the laws of
physics. This paper proposes Energy-consistent Neural Operators (ENOs), a
general framework for learning solution operators of PDEs that follows the
energy conservation or dissipation law from observed solution trajectories. We
introduce a novel penalty function inspired by the energy-based theory of
physics for training, in which the energy functional is modeled by another DNN,
allowing one to bias the outputs of the DNN-based solution operators to ensure
energetic consistency without explicit PDEs. Experiments on multiple physical
systems show that ENO outperforms existing DNN models in predicting solutions
from data, especially in super-resolution settings.</div><div><a href='http://arxiv.org/abs/2402.09018v1'>2402.09018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09730v1")'>DOF: Accelerating High-order Differential Operators with Forward
  Propagation</div>
<div id='2402.09730v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T05:59:21Z</div><div>Authors: Ruichen Li, Chuwei Wang, Haotian Ye, Di He, Liwei Wang</div><div style='padding-top: 10px; width: 80ex'>Solving partial differential equations (PDEs) efficiently is essential for
analyzing complex physical systems. Recent advancements in leveraging deep
learning for solving PDE have shown significant promise. However, machine
learning methods, such as Physics-Informed Neural Networks (PINN), face
challenges in handling high-order derivatives of neural network-parameterized
functions. Inspired by Forward Laplacian, a recent method of accelerating
Laplacian computation, we propose an efficient computational framework,
Differential Operator with Forward-propagation (DOF), for calculating general
second-order differential operators without losing any precision. We provide
rigorous proof of the advantages of our method over existing methods,
demonstrating two times improvement in efficiency and reduced memory
consumption on any architectures. Empirical results illustrate that our method
surpasses traditional automatic differentiation (AutoDiff) techniques,
achieving 2x improvement on the MLP structure and nearly 20x improvement on the
MLP with Jacobian sparsity.</div><div><a href='http://arxiv.org/abs/2402.09730v1'>2402.09730v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14081v1")'>Accelerating Fractional PINNs using Operational Matrices of Derivative</div>
<div id='2401.14081v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:00:19Z</div><div>Authors: Tayebeh Taheri, Alireza Afzal Aghaei, Kourosh Parand</div><div style='padding-top: 10px; width: 80ex'>This paper presents a novel operational matrix method to accelerate the
training of fractional Physics-Informed Neural Networks (fPINNs). Our approach
involves a non-uniform discretization of the fractional Caputo operator,
facilitating swift computation of fractional derivatives within Caputo-type
fractional differential problems with $0&lt;\alpha&lt;1$. In this methodology, the
operational matrix is precomputed, and during the training phase, automatic
differentiation is replaced with a matrix-vector product. While our methodology
is compatible with any network, we particularly highlight its successful
implementation in PINNs, emphasizing the enhanced accuracy achieved when
utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates
Legendre polynomials into the PINN structure, providing a significant boost in
accuracy. The effectiveness of our proposed method is validated across diverse
differential equations, including Delay Differential Equations (DDEs) and
Systems of Differential Algebraic Equations (DAEs). To demonstrate its
versatility, we extend the application of the method to systems of differential
equations, specifically addressing nonlinear Pantograph fractional-order
DDEs/DAEs. The results are supported by a comprehensive analysis of numerical
outcomes.</div><div><a href='http://arxiv.org/abs/2401.14081v1'>2401.14081v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02737v1")'>Neural Fractional Differential Equations</div>
<div id='2403.02737v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T07:45:29Z</div><div>Authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</div><div style='padding-top: 10px; width: 80ex'>Fractional Differential Equations (FDEs) are essential tools for modelling
complex systems in science and engineering. They extend the traditional
concepts of differentiation and integration to non-integer orders, enabling a
more precise representation of processes characterised by non-local and
memory-dependent behaviours.
  This property is useful in systems where variables do not respond to changes
instantaneously, but instead exhibit a strong memory of past interactions.
  Having this in mind, and drawing inspiration from Neural Ordinary
Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep
neural network architecture that adjusts a FDE to the dynamics of data.
  This work provides a comprehensive overview of the numerical method employed
in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest
that, despite being more computationally demanding, the Neural FDE may
outperform the Neural ODE in modelling systems with memory or dependencies on
past states, and it can effectively be applied to learn more intricate
dynamical systems.</div><div><a href='http://arxiv.org/abs/2403.02737v1'>2403.02737v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12938v1")'>Neural Differential Algebraic Equations</div>
<div id='2403.12938v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T17:43:57Z</div><div>Authors: James Koch, Madelyn Shapiro, Himanshu Sharma, Draguna Vrabie, Jan Drgona</div><div style='padding-top: 10px; width: 80ex'>Differential-Algebraic Equations (DAEs) describe the temporal evolution of
systems that obey both differential and algebraic constraints. Of particular
interest are systems that contain implicit relationships between their
components, such as conservation relationships. Here, we present Neural
Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of
DAEs. This methodology is built upon the concept of the Universal Differential
Equation; that is, a model constructed as a system of Neural Ordinary
Differential Equations informed by theory from particular science domains. In
this work, we show that the proposed NDAEs abstraction is suitable for relevant
system-theoretic data-driven modeling tasks. Presented examples include (i) the
inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a
network of pumps, tanks, and pipes. Our experiments demonstrate the proposed
method's robustness to noise and extrapolation ability to (i) learn the
behaviors of the system components and their interaction physics and (ii)
disambiguate between data trends and mechanistic relationships contained in the
system.</div><div><a href='http://arxiv.org/abs/2403.12938v1'>2403.12938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14131v1")'>Equivariant Manifold Neural ODEs and Differential Invariants</div>
<div id='2401.14131v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T12:23:22Z</div><div>Authors: Emma Andersdotter, Fredrik Ohlsson</div><div style='padding-top: 10px; width: 80ex'>In this paper we develop a manifestly geometric framework for equivariant
manifold neural ordinary differential equations (NODEs), and use it to analyse
their modelling capabilities for symmetric data. First, we consider the action
of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence
between equivariance of vector fields, symmetries of the corresponding Cauchy
problems, and equivariance of the associated NODEs. We also propose a novel
formulation of the equivariant NODEs in terms of the differential invariants of
the action of $G$ on $M$, based on Lie theory for symmetries of differential
equations, which provides an efficient parameterisation of the space of
equivariant vector fields in a way that is agnostic to both the manifold $M$
and the symmetry group $G$. Second, we construct augmented manifold NODEs,
through embeddings into equivariant flows, and show that they are universal
approximators of equivariant diffeomorphisms on any path-connected $M$.
Furthermore, we show that the augmented NODEs can be incorporated in the
geometric framework and parameterised using higher order differential
invariants. Finally, we consider the induced action of $G$ on different fields
on $M$ and show how it can be used to generalise previous work, on, e.g.,
continuous normalizing flows, to equivariant models in any geometry.</div><div><a href='http://arxiv.org/abs/2401.14131v1'>2401.14131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02362v1")'>Unification of Symmetries Inside Neural Networks: Transformer,
  Feedforward and Neural ODE</div>
<div id='2402.02362v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T06:11:54Z</div><div>Authors: Koji Hashimoto, Yuji Hirono, Akiyoshi Sannai</div><div style='padding-top: 10px; width: 80ex'>Understanding the inner workings of neural networks, including transformers,
remains one of the most challenging puzzles in machine learning. This study
introduces a novel approach by applying the principles of gauge symmetries, a
key concept in physics, to neural network architectures. By regarding model
functions as physical observables, we find that parametric redundancies of
various machine learning models can be interpreted as gauge symmetries. We
mathematically formulate the parametric redundancies in neural ODEs, and find
that their gauge symmetries are given by spacetime diffeomorphisms, which play
a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a
continuum version of feedforward neural networks, we show that the parametric
redundancies in feedforward neural networks are indeed lifted to
diffeomorphisms in neural ODEs. We further extend our analysis to transformer
models, finding natural correspondences with neural ODEs and their gauge
symmetries. The concept of gauge symmetries sheds light on the complex behavior
of deep learning models through physics and provides us with a unifying
perspective for analyzing various machine learning architectures.</div><div><a href='http://arxiv.org/abs/2402.02362v1'>2402.02362v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06481v1")'>Machine learning a fixed point action for SU(3) gauge theory with a
  gauge equivariant convolutional neural network</div>
<div id='2401.06481v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T10:03:00Z</div><div>Authors: Kieran Holland, Andreas Ipp, David I. Müller, Urs Wenger</div><div style='padding-top: 10px; width: 80ex'>Fixed point lattice actions are designed to have continuum classical
properties unaffected by discretization effects and reduced lattice artifacts
at the quantum level. They provide a possible way to extract continuum physics
with coarser lattices, thereby allowing to circumvent problems with critical
slowing down and topological freezing toward the continuum limit. A crucial
ingredient for practical applications is to find an accurate and compact
parametrization of a fixed point action, since many of its properties are only
implicitly defined. Here we use machine learning methods to revisit the
question of how to parametrize fixed point actions. In particular, we obtain a
fixed point action for four-dimensional SU(3) gauge theory using convolutional
neural networks with exact gauge invariance. The large operator space allows us
to find superior parametrizations compared to previous studies, a necessary
first step for future Monte Carlo simulations.</div><div><a href='http://arxiv.org/abs/2401.06481v1'>2401.06481v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02810v1")'>Dynamic Gaussian Graph Operator: Learning parametric partial
  differential equations in arbitrary discrete mechanics problems</div>
<div id='2403.02810v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T09:25:31Z</div><div>Authors: Chu Wang, Jinhong Wu, Yanzhi Wang, Zhijian Zha, Qi Zhou</div><div style='padding-top: 10px; width: 80ex'>Deep learning methods have access to be employed for solving physical systems
governed by parametric partial differential equations (PDEs) due to massive
scientific data. It has been refined to operator learning that focuses on
learning non-linear mapping between infinite-dimensional function spaces,
offering interface from observations to solutions. However, state-of-the-art
neural operators are limited to constant and uniform discretization, thereby
leading to deficiency in generalization on arbitrary discretization schemes for
computational domain. In this work, we propose a novel operator learning
algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands
neural operators to learning parametric PDEs in arbitrary discrete mechanics
problems. The Dynamic Gaussian Graph (DGG) kernel learns to map the observation
vectors defined in general Euclidean space to metric vectors defined in
high-dimensional uniform metric space. The DGG integral kernel is parameterized
by Gaussian kernel weighted Riemann sum approximating and using dynamic message
passing graph to depict the interrelation within the integral term. Fourier
Neural Operator is selected to localize the metric vectors on spatial and
frequency domains. Metric vectors are regarded as located on latent uniform
domain, wherein spatial and spectral transformation offer highly regular
constraints on solution space. The efficiency and robustness of DGGO are
validated by applying it to solve numerical arbitrary discrete mechanics
problems in comparison with mainstream neural operators. Ablation experiments
are implemented to demonstrate the effectiveness of spatial transformation in
the DGG kernel. The proposed method is utilized to forecast stress field of
hyper-elastic material with geometrically variable void as engineering
application.</div><div><a href='http://arxiv.org/abs/2403.02810v1'>2403.02810v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08187v1")'>Learning time-dependent PDE via graph neural networks and deep operator
  network for robust accuracy on irregular grids</div>
<div id='2402.08187v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T03:14:32Z</div><div>Authors: Sung Woong Cho, Jae Yong Lee, Hyung Ju Hwang</div><div style='padding-top: 10px; width: 80ex'>Scientific computing using deep learning has seen significant advancements in
recent years. There has been growing interest in models that learn the operator
from the parameters of a partial differential equation (PDE) to the
corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural
operator, among other models, have been designed with structures suitable for
handling functions as inputs and outputs, enabling real-time predictions as
surrogate models for solution operators. There has also been significant
progress in the research on surrogate models based on graph neural networks
(GNNs), specifically targeting the dynamics in time-dependent PDEs. In this
paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to
effectively adapt DeepONet, which is well-known for successful operator
learning. GraphDeepONet exhibits robust accuracy in predicting solutions
compared to existing GNN-based PDE solver models. It maintains consistent
performance even on irregular grids, leveraging the advantages inherited from
DeepONet and enabling predictions on arbitrary grids. Additionally, unlike
traditional DeepONet and its variants, GraphDeepONet enables time extrapolation
for time-dependent PDE solutions. We also provide theoretical analysis of the
universal approximation capability of GraphDeepONet in approximating continuous
operators across arbitrary time intervals.</div><div><a href='http://arxiv.org/abs/2402.08187v1'>2402.08187v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09323v1")'>BENO: Boundary-embedded Neural Operators for Elliptic PDEs</div>
<div id='2401.09323v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T16:47:39Z</div><div>Authors: Haixin Wang, Jiaxin Li, Anubhav Dwivedi, Kentaro Hara, Tailin Wu</div><div style='padding-top: 10px; width: 80ex'>Elliptic partial differential equations (PDEs) are a major class of
time-independent PDEs that play a key role in many scientific and engineering
domains such as fluid dynamics, plasma physics, and solid mechanics. Recently,
neural operators have emerged as a promising technique to solve elliptic PDEs
more efficiently by directly mapping the input to solutions. However, existing
networks typically cannot handle complex geometries and inhomogeneous boundary
values present in the real world. Here we introduce Boundary-Embedded Neural
Operators (BENO), a novel neural operator architecture that embeds the complex
geometries and inhomogeneous boundary values into the solving of elliptic PDEs.
Inspired by classical Green's function, BENO consists of two branches of Graph
Neural Networks (GNNs) for interior source term and boundary values,
respectively. Furthermore, a Transformer encoder maps the global boundary
geometry into a latent vector which influences each message passing layer of
the GNNs. We test our model extensively in elliptic PDEs with various boundary
conditions. We show that all existing baseline methods fail to learn the
solution operator. In contrast, our model, endowed with boundary-embedded
architecture, outperforms state-of-the-art neural operators and strong
baselines by an average of 60.96\%. Our source code can be found
https://github.com/AI4Science-WestlakeU/beno.git.</div><div><a href='http://arxiv.org/abs/2401.09323v1'>2401.09323v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00825v1")'>Resolution invariant deep operator network for PDEs with complex
  geometries</div>
<div id='2402.00825v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T18:11:22Z</div><div>Authors: Jianguo Huang, Yue Qiu</div><div style='padding-top: 10px; width: 80ex'>Neural operators (NO) are discretization invariant deep learning methods with
functional output and can approximate any continuous operator. NO have
demonstrated the superiority of solving partial differential equations (PDEs)
over other deep learning methods. However, the spatial domain of its input
function needs to be identical to its output, which limits its applicability.
For instance, the widely used Fourier neural operator (FNO) fails to
approximate the operator that maps the boundary condition to the PDE solution.
To address this issue, we propose a novel framework called resolution-invariant
deep operator (RDO) that decouples the spatial domain of the input and output.
RDO is motivated by the Deep operator network (DeepONet) and it does not
require retraining the network when the input/output is changed compared with
DeepONet. RDO takes functional input and its output is also functional so that
it keeps the resolution invariant property of NO. It can also resolve PDEs with
complex geometries whereas NO fail. Various numerical experiments demonstrate
the advantage of our method over DeepONet and FNO.</div><div><a href='http://arxiv.org/abs/2402.00825v1'>2402.00825v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13101v1")'>A Microstructure-based Graph Neural Network for Accelerating Multiscale
  Simulations</div>
<div id='2402.13101v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:54:24Z</div><div>Authors: J. Storm, I. B. C. M. Rocha, F. P. van der Meer</div><div style='padding-top: 10px; width: 80ex'>Simulating the mechanical response of advanced materials can be done more
accurately using concurrent multiscale models than with single-scale
simulations. However, the computational costs stand in the way of the practical
application of this approach. The costs originate from microscale Finite
Element (FE) models that must be solved at every macroscopic integration point.
A plethora of surrogate modeling strategies attempt to alleviate this cost by
learning to predict macroscopic stresses from macroscopic strains, completely
replacing the microscale models. In this work, we introduce an alternative
surrogate modeling strategy that allows for keeping the multiscale nature of
the problem, allowing it to be used interchangeably with an FE solver for any
time step. Our surrogate provides all microscopic quantities, which are then
homogenized to obtain macroscopic quantities of interest. We achieve this for
an elasto-plastic material by predicting full-field microscopic strains using a
graph neural network (GNN) while retaining the microscopic constitutive
material model to obtain the stresses. This hybrid data-physics graph-based
approach avoids the high dimensionality originating from predicting full-field
responses while allowing non-locality to arise. By training the GNN on a
variety of meshes, it learns to generalize to unseen meshes, allowing a single
model to be used for a range of microstructures. The embedded microscopic
constitutive model in the GNN implicitly tracks history-dependent variables and
leads to improved accuracy. We demonstrate for several challenging scenarios
that the surrogate can predict complex macroscopic stress-strain paths. As the
computation time of our method scales favorably with the number of elements in
the microstructure compared to the FE method, our method can significantly
accelerate FE2 simulations.</div><div><a href='http://arxiv.org/abs/2402.13101v1'>2402.13101v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01045v1")'>LatticeGraphNet: A two-scale graph neural operator for simulating
  lattice structures</div>
<div id='2402.01045v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T22:24:58Z</div><div>Authors: Ayush Jain, Ehsan Haghighat, Sai Nelaturi</div><div style='padding-top: 10px; width: 80ex'>This study introduces a two-scale Graph Neural Operator (GNO), namely,
LatticeGraphNet (LGN), designed as a surrogate model for costly nonlinear
finite-element simulations of three-dimensional latticed parts and structures.
LGN has two networks: LGN-i, learning the reduced dynamics of lattices, and
LGN-ii, learning the mapping from the reduced representation onto the
tetrahedral mesh. LGN can predict deformation for arbitrary lattices, therefore
the name operator. Our approach significantly reduces inference time while
maintaining high accuracy for unseen simulations, establishing the use of GNOs
as efficient surrogate models for evaluating mechanical responses of lattices
and structures.</div><div><a href='http://arxiv.org/abs/2402.01045v1'>2402.01045v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16914v2")'>Energy-conserving equivariant GNN for elasticity of lattice architected
  metamaterials</div>
<div id='2401.16914v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T11:25:49Z</div><div>Authors: Ivan Grega, Ilyes Batatia, Gábor Csányi, Sri Karlapati, Vikram S. Deshpande</div><div style='padding-top: 10px; width: 80ex'>Lattices are architected metamaterials whose properties strongly depend on
their geometrical design. The analogy between lattices and graphs enables the
use of graph neural networks (GNNs) as a faster surrogate model compared to
traditional methods such as finite element modelling. In this work, we generate
a big dataset of structure-property relationships for strut-based lattices. The
dataset is made available to the community which can fuel the development of
methods anchored in physical principles for the fitting of fourth-order
tensors. In addition, we present a higher-order GNN model trained on this
dataset. The key features of the model are (i) SE(3) equivariance, and (ii)
consistency with the thermodynamic law of conservation of energy. We compare
the model to non-equivariant models based on a number of error metrics and
demonstrate its benefits in terms of predictive performance and reduced
training requirements. Finally, we demonstrate an example application of the
model to an architected material design task. The methods which we developed
are applicable to fourth-order tensors beyond elasticity such as piezo-optical
tensor etc.</div><div><a href='http://arxiv.org/abs/2401.16914v2'>2401.16914v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13570v1")'>Guided Diffusion for Fast Inverse Design of Density-based Mechanical
  Metamaterials</div>
<div id='2401.13570v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T16:31:50Z</div><div>Authors: Yanyan Yang, Lili Wang, Xiaoya Zhai, Kai Chen, Wenming Wu, Yunkai Zhao, Ligang Liu, Xiao-Ming Fu</div><div style='padding-top: 10px; width: 80ex'>Mechanical metamaterial is a synthetic material that can possess
extraordinary physical characteristics, such as abnormal elasticity, stiffness,
and stability, by carefully designing its internal structure. To make
metamaterials contain delicate local structures with unique mechanical
properties, it is a potential method to represent them through high-resolution
voxels. However, it brings a substantial computational burden. To this end,
this paper proposes a fast inverse design method, whose core is an advanced
deep generative AI algorithm, to generate voxel-based mechanical metamaterials.
Specifically, we use the self-conditioned diffusion model, capable of
generating a microstructure with a resolution of $128^3$ to approach the
specified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid
reverse design tool facilitates the exploration of extreme metamaterials, the
sequence interpolation in metamaterials, and the generation of diverse
microstructures for multi-scale design. This flexible and adaptive generative
tool is of great value in structural engineering or other mechanical systems
and can stimulate more subsequent research.</div><div><a href='http://arxiv.org/abs/2401.13570v1'>2401.13570v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11103v1")'>Toward Learning Latent-Variable Representations of Microstructures by
  Optimizing in Spatial Statistics Space</div>
<div id='2402.11103v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T22:16:14Z</div><div>Authors: Sayed Sajad Hashemi, Michael Guerzhoy, Noah H. Paulson</div><div style='padding-top: 10px; width: 80ex'>In Materials Science, material development involves evaluating and optimizing
the internal structures of the material, generically referred to as
microstructures. Microstructures structure is stochastic, analogously to image
textures. A particular microstructure can be well characterized by its spatial
statistics, analogously to image texture being characterized by the response to
a Fourier-like filter bank. Material design would benefit from low-dimensional
representation of microstructures Paulson et al. (2017).
  In this work, we train a Variational Autoencoders (VAE) to produce
reconstructions of textures that preserve the spatial statistics of the
original texture, while not necessarily reconstructing the same image in data
space. We accomplish this by adding a differentiable term to the cost function
in order to minimize the distance between the original and the reconstruction
in spatial statistics space.
  Our experiments indicate that it is possible to train a VAE that minimizes
the distance in spatial statistics space between the original and the
reconstruction of synthetic images. In future work, we will apply the same
techniques to microstructures, with the goal of obtaining low-dimensional
representations of material microstructures.</div><div><a href='http://arxiv.org/abs/2402.11103v1'>2402.11103v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17806v1")'>Material Microstructure Design Using VAE-Regression with Multimodal
  Prior</div>
<div id='2402.17806v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T11:27:32Z</div><div>Authors: Avadhut Sardeshmukh, Sreedhar Reddy, BP Gautham, Pushpak Bhattacharyya</div><div style='padding-top: 10px; width: 80ex'>We propose a variational autoencoder (VAE)-based model for building forward
and inverse structure-property linkages, a problem of paramount importance in
computational materials science. Our model systematically combines VAE with
regression, linking the two models through a two-level prior conditioned on the
regression variables. The regression loss is optimized jointly with the
reconstruction loss of the variational autoencoder, learning microstructure
features relevant for property prediction and reconstruction. The resultant
model can be used for both forward and inverse prediction i.e., for predicting
the properties of a given microstructure as well as for predicting the
microstructure required to obtain given properties. Since the inverse problem
is ill-posed (one-to-many), we derive the objective function using a
multi-modal Gaussian mixture prior enabling the model to infer multiple
microstructures for a target set of properties. We show that for forward
prediction, our model is as accurate as state-of-the-art forward-only models.
Additionally, our method enables direct inverse inference. We show that the
microstructures inferred using our model achieve desired properties reasonably
accurately, avoiding the need for expensive optimization loops.</div><div><a href='http://arxiv.org/abs/2402.17806v1'>2402.17806v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05206v1")'>Tailoring Frictional Properties of Surfaces Using Diffusion Models</div>
<div id='2401.05206v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T09:15:07Z</div><div>Authors: Even Marius Nordhagen, Henrik Andersen Sveinsson, Anders Malthe-Sørenssen</div><div style='padding-top: 10px; width: 80ex'>This Letter introduces an approach for precisely designing surface friction
properties using a conditional generative machine learning model, specifically
a diffusion denoising probabilistic model (DDPM). We created a dataset of
synthetic surfaces with frictional properties determined by molecular dynamics
simulations, which trained the DDPM to predict surface structures from desired
frictional outcomes. Unlike traditional trial-and-error and numerical
optimization methods, our approach directly yields surface designs meeting
specified frictional criteria with high accuracy and efficiency. This
advancement in material surface engineering demonstrates the potential of
machine learning in reducing the iterative nature of surface design processes.
Our findings not only provide a new pathway for precise surface property
tailoring but also suggest broader applications in material science where
surface characteristics are critical.</div><div><a href='http://arxiv.org/abs/2401.05206v1'>2401.05206v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08414v1")'>Enhancing Dynamical System Modeling through Interpretable Machine
  Learning Augmentations: A Case Study in Cathodic Electrophoretic Deposition</div>
<div id='2401.08414v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:58:21Z</div><div>Authors: Christian Jacobsen, Jiayuan Dong, Mehdi Khalloufi, Xun Huan, Karthik Duraisamy, Maryam Akram, Wanjiao Liu</div><div style='padding-top: 10px; width: 80ex'>We introduce a comprehensive data-driven framework aimed at enhancing the
modeling of physical systems, employing inference techniques and machine
learning enhancements. As a demonstrative application, we pursue the modeling
of cathodic electrophoretic deposition (EPD), commonly known as e-coating. Our
approach illustrates a systematic procedure for enhancing physical models by
identifying their limitations through inference on experimental data and
introducing adaptable model enhancements to address these shortcomings. We
begin by tackling the issue of model parameter identifiability, which reveals
aspects of the model that require improvement. To address generalizability , we
introduce modifications which also enhance identifiability. However, these
modifications do not fully capture essential experimental behaviors. To
overcome this limitation, we incorporate interpretable yet flexible
augmentations into the baseline model. These augmentations are parameterized by
simple fully-connected neural networks (FNNs), and we leverage machine learning
tools, particularly Neural Ordinary Differential Equations (Neural ODEs), to
learn these augmentations. Our simulations demonstrate that the machine
learning-augmented model more accurately captures observed behaviors and
improves predictive accuracy. Nevertheless, we contend that while the model
updates offer superior performance and capture the relevant physics, we can
reduce off-line computational costs by eliminating certain dynamics without
compromising accuracy or interpretability in downstream predictions of
quantities of interest, particularly film thickness predictions. The entire
process outlined here provides a structured approach to leverage data-driven
methods. Firstly, it helps us comprehend the root causes of model inaccuracies,
and secondly, it offers a principled method for enhancing model performance.</div><div><a href='http://arxiv.org/abs/2401.08414v1'>2401.08414v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00071v1")'>Unraveling the Impact of Initial Choices and In-Loop Interventions on
  Learning Dynamics in Autonomous Scanning Probe Microscopy</div>
<div id='2402.00071v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T20:08:15Z</div><div>Authors: Boris N. Slautin, Yongtao Liu, Hiroshi Funakubo, Sergei V. Kalinin</div><div style='padding-top: 10px; width: 80ex'>The current focus in Autonomous Experimentation (AE) is on developing robust
workflows to conduct the AE effectively. This entails the need for well-defined
approaches to guide the AE process, including strategies for hyperparameter
tuning and high-level human interventions within the workflow loop. This paper
presents a comprehensive analysis of the influence of initial experimental
conditions and in-loop interventions on the learning dynamics of Deep Kernel
Learning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore
the concept of 'seed effect', where the initial experiment setup has a
substantial impact on the subsequent learning trajectory. Additionally, we
introduce an approach of the seed point interventions in AE allowing the
operator to influence the exploration process. Using a dataset from
Piezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the
impact of the 'seed effect' and in-loop seed interventions on the effectiveness
of DKL in predicting material properties. The study highlights the importance
of initial choices and adaptive interventions in optimizing learning rates and
enhancing the efficiency of automated material characterization. This work
offers valuable insights into designing more robust and effective AE workflows
in microscopy with potential applications across various characterization
techniques. The analysis code that supports the funding is publicly available
at https://github.com/Slautin/2024_Seed_effect_DKL_BO.</div><div><a href='http://arxiv.org/abs/2402.00071v1'>2402.00071v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.15815v1")'>A Generative Machine Learning Model for Material Microstructure 3D
  Reconstruction and Performance Evaluation</div>
<div id='2402.15815v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T13:42:34Z</div><div>Authors: Yilin Zheng, Zhigong Song</div><div style='padding-top: 10px; width: 80ex'>The reconstruction of 3D microstructures from 2D slices is considered to hold
significant value in predicting the spatial structure and physical properties
of materials.The dimensional extension from 2D to 3D is viewed as a highly
challenging inverse problem from the current technological
perspective.Recently,methods based on generative adversarial networks have
garnered widespread attention.However,they are still hampered by numerous
limitations,including oversimplified models,a requirement for a substantial
number of training samples,and difficulties in achieving model convergence
during training.In light of this,a novel generative model that integrates the
multiscale properties of U-net with and the generative capabilities of GAN has
been proposed.Based on this,the innovative construction of a multi-scale
channel aggregation module,a multi-scale hierarchical feature aggregation
module and a convolutional block attention mechanism can better capture the
properties of the material microstructure and extract the image information.The
model's accuracy is further improved by combining the image regularization loss
with the Wasserstein distance loss.In addition,this study utilizes the
anisotropy index to accurately distinguish the nature of the image,which can
clearly determine the isotropy and anisotropy of the image.It is also the first
time that the generation quality of material samples from different domains is
evaluated and the performance of the model itself is compared.The experimental
results demonstrate that the present model not only shows a very high
similarity between the generated 3D structures and real samples but is also
highly consistent with real data in terms of statistical data analysis.</div><div><a href='http://arxiv.org/abs/2402.15815v1'>2402.15815v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03152v1")'>Controllable Image Synthesis of Industrial Data Using Stable Diffusion</div>
<div id='2401.03152v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T08:09:24Z</div><div>Authors: Gabriele Valvano, Antonino Agostino, Giovanni De Magistris, Antonino Graziano, Giacomo Veneri</div><div style='padding-top: 10px; width: 80ex'>Training supervised deep neural networks that perform defect detection and
segmentation requires large-scale fully-annotated datasets, which can be hard
or even impossible to obtain in industrial environments. Generative AI offers
opportunities to enlarge small industrial datasets artificially, thus enabling
the usage of state-of-the-art supervised approaches in the industry.
Unfortunately, also good generative models need a lot of data to train, while
industrial datasets are often tiny. Here, we propose a new approach for reusing
general-purpose pre-trained generative models on industrial data, ultimately
allowing the generation of self-labelled defective images. First, we let the
model learn the new concept, entailing the novel data distribution. Then, we
force it to learn to condition the generative process, producing industrial
images that satisfy well-defined topological characteristics and show defects
with a given geometry and location. To highlight the advantage of our approach,
we use the synthetic dataset to optimise a crack segmentor for a real
industrial use case. When the available data is small, we observe considerable
performance increase under several metrics, showing the method's potential in
production environments.</div><div><a href='http://arxiv.org/abs/2401.03152v1'>2401.03152v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12098v1")'>Deep Generative Design for Mass Production</div>
<div id='2403.12098v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T01:32:00Z</div><div>Authors: Jihoon Kim, Yongmin Kwon, Namwoo Kang</div><div style='padding-top: 10px; width: 80ex'>Generative Design (GD) has evolved as a transformative design approach,
employing advanced algorithms and AI to create diverse and innovative solutions
beyond traditional constraints. Despite its success, GD faces significant
challenges regarding the manufacturability of complex designs, often
necessitating extensive manual modifications due to limitations in standard
manufacturing processes and the reliance on additive manufacturing, which is
not ideal for mass production. Our research introduces an innovative framework
addressing these manufacturability concerns by integrating constraints
pertinent to die casting and injection molding into GD, through the utilization
of 2D depth images. This method simplifies intricate 3D geometries into
manufacturable profiles, removing unfeasible features such as
non-manufacturable overhangs and allowing for the direct consideration of
essential manufacturing aspects like thickness and rib design. Consequently,
designs previously unsuitable for mass production are transformed into viable
solutions. We further enhance this approach by adopting an advanced 2D
generative model, which offer a more efficient alternative to traditional 3D
shape generation methods. Our results substantiate the efficacy of this
framework, demonstrating the production of innovative, and, importantly,
manufacturable designs. This shift towards integrating practical manufacturing
considerations into GD represents a pivotal advancement, transitioning from
purely inspirational concepts to actionable, production-ready solutions. Our
findings underscore usefulness and potential of GD for broader industry
adoption, marking a significant step forward in aligning GD with the demands of
manufacturing challenges.</div><div><a href='http://arxiv.org/abs/2403.12098v1'>2403.12098v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08540v1")'>Generative VS non-Generative Models in Engineering Shape Optimization</div>
<div id='2402.08540v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T15:45:20Z</div><div>Authors: Muhammad Usama, Zahid Masood, Shahroz Khan, Konstantinos Kostas, Panagiotis Kaklis</div><div style='padding-top: 10px; width: 80ex'>In this work, we perform a systematic comparison of the effectiveness and
efficiency of generative and non-generative models in constructing design
spaces for novel and efficient design exploration and shape optimization. We
apply these models in the case of airfoil/hydrofoil design and conduct the
comparison on the resulting design spaces. A conventional Generative
Adversarial Network (GAN) and a state-of-the-art generative model, the
Performance-Augmented Diverse Generative Adversarial Network (PaDGAN), are
juxtaposed with a linear non-generative model based on the coupling of the
Karhunen-Lo\`eve Expansion and a physics-informed Shape Signature Vector
(SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding
and a physics-augmented design space, non-generative models have the potential
to cost-effectively generate high-performing valid designs with enhanced
coverage of the design space. In this work, both approaches are applied to two
large foil profile datasets comprising real-world and artificial designs
generated through either a profile-generating parametric model or deep-learning
approach. These datasets are further enriched with integral properties of their
members' shapes as well as physics-informed parameters. Our results illustrate
that the design spaces constructed by the non-generative model outperform the
generative model in terms of design validity, generating robust latent spaces
with none or significantly fewer invalid designs when compared to generative
models. We aspire that these findings will aid the engineering design community
in making informed decisions when constructing designs spaces for shape
optimization, as we have show that under certain conditions computationally
inexpensive approaches can closely match or even outperform state-of-the art
generative models.</div><div><a href='http://arxiv.org/abs/2402.08540v1'>2402.08540v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13171v2")'>Compositional Generative Inverse Design</div>
<div id='2401.13171v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T01:33:39Z</div><div>Authors: Tailin Wu, Takashi Maruyama, Long Wei, Tao Zhang, Yilun Du, Gianluca Iaccarino, Jure Leskovec</div><div style='padding-top: 10px; width: 80ex'>Inverse design, where we seek to design input variables in order to optimize
an underlying objective function, is an important problem that arises across
fields such as mechanical engineering to aerospace engineering. Inverse design
is typically formulated as an optimization problem, with recent works
leveraging optimization across learned dynamics models. However, as models are
optimized they tend to fall into adversarial modes, preventing effective
sampling. We illustrate that by instead optimizing over the learned energy
function captured by the diffusion model, we can avoid such adversarial
examples and significantly improve design performance. We further illustrate
how such a design system is compositional, enabling us to combine multiple
different diffusion models representing subcomponents of our desired system to
design systems with every specified component. In an N-body interaction task
and a challenging 2D multi-airfoil design task, we demonstrate that by
composing the learned diffusion model at test time, our method allows us to
design initial states and boundary shapes that are more complex than those in
the training data. Our method generalizes to more objects for N-body dataset
and discovers formation flying to minimize drag in the multi-airfoil design
task. Project website and code can be found at
https://github.com/AI4Science-WestlakeU/cindm.</div><div><a href='http://arxiv.org/abs/2401.13171v2'>2401.13171v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14882v1")'>Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms
  with Target Conditions</div>
<div id='2402.14882v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T03:31:00Z</div><div>Authors: Sumin Lee, Jihoon Kim, Namwoo Kang</div><div style='padding-top: 10px; width: 80ex'>Mechanisms are essential components designed to perform specific tasks in
various mechanical systems. However, designing a mechanism that satisfies
certain kinematic or quasi-static requirements is a challenging task. The
kinematic requirements may include the workspace of a mechanism, while the
quasi-static requirements of a mechanism may include its torque transmission,
which refers to the ability of the mechanism to transfer power and torque
effectively. In this paper, we propose a deep learning-based generative model
for generating multiple crank-rocker four-bar linkage mechanisms that satisfy
both the kinematic and quasi-static requirements aforementioned. The proposed
model is based on a conditional generative adversarial network (cGAN) with
modifications for mechanism synthesis, which is trained to learn the
relationship between the requirements of a mechanism with respect to linkage
lengths. The results demonstrate that the proposed model successfully generates
multiple distinct mechanisms that satisfy specific kinematic and quasi-static
requirements. To evaluate the novelty of our approach, we provide a comparison
of the samples synthesized by the proposed cGAN, traditional cVAE and NSGA-II.
Our approach has several advantages over traditional design methods. It enables
designers to efficiently generate multiple diverse and feasible design
candidates while exploring a large design space. Also, the proposed model
considers both the kinematic and quasi-static requirements, which can lead to
more efficient and effective mechanisms for real-world use, making it a
promising tool for linkage mechanism design.</div><div><a href='http://arxiv.org/abs/2402.14882v1'>2402.14882v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.15563v1")'>BrepGen: A B-rep Generative Diffusion Model with Structured Latent
  Geometry</div>
<div id='2401.15563v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T04:07:59Z</div><div>Authors: Xiang Xu, Joseph G. Lambourne, Pradeep Kumar Jayaraman, Zhengqing Wang, Karl D. D. Willis, Yasutaka Furukawa</div><div style='padding-top: 10px; width: 80ex'>This paper presents BrepGen, a diffusion-based generative approach that
directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD)
model. BrepGen represents a B-rep model as a novel structured latent geometry
in a hierarchical tree. With the root node representing a whole CAD solid, each
element of a B-rep model (i.e., a face, an edge, or a vertex) progressively
turns into a child-node from top to bottom. B-rep geometry information goes
into the nodes as the global bounding box of each primitive along with a latent
code describing the local geometric shape. The B-rep topology information is
implicitly represented by node duplication. When two faces share an edge, the
edge curve will appear twice in the tree, and a T-junction vertex with three
incident edges appears six times in the tree with identical node features.
Starting from the root and progressing to the leaf, BrepGen employs
Transformer-based diffusion models to sequentially denoise node features while
duplicated nodes are detected and merged, recovering the B-Rep topology
information. Extensive experiments show that BrepGen sets a new milestone in
CAD B-rep generation, surpassing existing methods on various benchmarks.
Results on our newly collected furniture dataset further showcase its
exceptional capability in generating complicated geometry. While previous
methods were limited to generating simple prismatic shapes, BrepGen
incorporates free-form and doubly-curved surfaces for the first time.
Additional applications of BrepGen include CAD autocomplete and design
interpolation. The code, pretrained models, and dataset will be released.</div><div><a href='http://arxiv.org/abs/2401.15563v1'>2401.15563v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05579v1")'>An Augmented Surprise-guided Sequential Learning Framework for
  Predicting the Melt Pool Geometry</div>
<div id='2401.05579v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T23:05:23Z</div><div>Authors: Ahmed Shoyeb Raihan, Hamed Khosravi, Tanveer Hossain Bhuiyan, Imtiaz Ahmed</div><div style='padding-top: 10px; width: 80ex'>Metal Additive Manufacturing (MAM) has reshaped the manufacturing industry,
offering benefits like intricate design, minimal waste, rapid prototyping,
material versatility, and customized solutions. However, its full industry
adoption faces hurdles, particularly in achieving consistent product quality. A
crucial aspect for MAM's success is understanding the relationship between
process parameters and melt pool characteristics. Integrating Artificial
Intelligence (AI) into MAM is essential. Traditional machine learning (ML)
methods, while effective, depend on large datasets to capture complex
relationships, a significant challenge in MAM due to the extensive time and
resources required for dataset creation. Our study introduces a novel
surprise-guided sequential learning framework, SurpriseAF-BO, signaling a
significant shift in MAM. This framework uses an iterative, adaptive learning
process, modeling the dynamics between process parameters and melt pool
characteristics with limited data, a key benefit in MAM's cyber manufacturing
context. Compared to traditional ML models, our sequential learning method
shows enhanced predictive accuracy for melt pool dimensions. Further improving
our approach, we integrated a Conditional Tabular Generative Adversarial
Network (CTGAN) into our framework, forming the CT-SurpriseAF-BO. This produces
synthetic data resembling real experimental data, improving learning
effectiveness. This enhancement boosts predictive precision without requiring
additional physical experiments. Our study demonstrates the power of advanced
data-driven techniques in cyber manufacturing and the substantial impact of
sequential AI and ML, particularly in overcoming MAM's traditional challenges.</div><div><a href='http://arxiv.org/abs/2401.05579v1'>2401.05579v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00669v1")'>Advancing Additive Manufacturing through Deep Learning: A Comprehensive
  Review of Current Progress and Future Challenges</div>
<div id='2403.00669v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T17:01:47Z</div><div>Authors: Amirul Islam Saimon, Emmanuel Yangue, Xiaowei Yue, Zhenyu James Kong, Chenang Liu</div><div style='padding-top: 10px; width: 80ex'>Additive manufacturing (AM) has already proved itself to be the potential
alternative to widely-used subtractive manufacturing due to its extraordinary
capacity of manufacturing highly customized products with minimum material
wastage. Nevertheless, it is still not being considered as the primary choice
for the industry due to some of its major inherent challenges, including
complex and dynamic process interactions, which are sometimes difficult to
fully understand even with traditional machine learning because of the
involvement of high-dimensional data such as images, point clouds, and voxels.
However, the recent emergence of deep learning (DL) is showing great promise in
overcoming many of these challenges as DL can automatically capture complex
relationships from high-dimensional data without hand-crafted feature
extraction. Therefore, the volume of research in the intersection of AM and DL
is exponentially growing each year which makes it difficult for the researchers
to keep track of the trend and future potential directions. Furthermore, to the
best of our knowledge, there is no comprehensive review paper in this research
track summarizing the recent studies. Therefore, this paper reviews the recent
studies that apply DL for making the AM process better with a high-level
summary of their contributions and limitations. Finally, it summarizes the
current challenges and recommends some of the promising opportunities in this
domain for further investigation with a special focus on generalizing DL models
for wide-range of geometry types, managing uncertainties both in AM data and DL
models, overcoming limited and noisy AM data issues by incorporating generative
models, and unveiling the potential of interpretable DL for AM.</div><div><a href='http://arxiv.org/abs/2403.00669v1'>2403.00669v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02403v1")'>Real-Time 2D Temperature Field Prediction in Metal Additive
  Manufacturing Using Physics-Informed Neural Networks</div>
<div id='2401.02403v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T18:42:28Z</div><div>Authors: Pouyan Sajadi, Mostafa Rahmani Dehaghani, Yifan Tang, G. Gary Wang</div><div style='padding-top: 10px; width: 80ex'>Accurately predicting the temperature field in metal additive manufacturing
(AM) processes is critical to preventing overheating, adjusting process
parameters, and ensuring process stability. While physics-based computational
models offer precision, they are often time-consuming and unsuitable for
real-time predictions and online control in iterative design scenarios.
Conversely, machine learning models rely heavily on high-quality datasets,
which can be costly and challenging to obtain within the metal AM domain. Our
work addresses this by introducing a physics-informed neural network framework
specifically designed for temperature field prediction in metal AM. This
framework incorporates a physics-informed input, physics-informed loss
function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.
Utilizing real-time temperature data from the process, our model predicts 2D
temperature fields for future timestamps across diverse geometries, deposition
patterns, and process parameters. We validate the proposed framework in two
scenarios: full-field temperature prediction for a thin wall and 2D temperature
field prediction for cylinder and cubic parts, demonstrating errors below 3%
and 1%, respectively. Our proposed framework exhibits the flexibility to be
applied across diverse scenarios with varying process parameters, geometries,
and deposition patterns.</div><div><a href='http://arxiv.org/abs/2401.02403v1'>2401.02403v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16119v1")'>DeepForge: Leveraging AI for Microstructural Control in Metal Forming
  via Model Predictive Control</div>
<div id='2402.16119v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T15:37:14Z</div><div>Authors: Jan Petrik, Markus Bambach</div><div style='padding-top: 10px; width: 80ex'>This study presents a novel method for microstructure control in closed die
hot forging that combines Model Predictive Control (MPC) with a developed
machine learning model called DeepForge. DeepForge uses an architecture that
combines 1D convolutional neural networks and gated recurrent units. It uses
surface temperature measurements of a workpiece as input to predict
microstructure changes during forging. The paper also details DeepForge's
architecture and the finite element simulation model used to generate the data
set, using a three-stroke forging process. The results demonstrate DeepForge's
ability to predict microstructure with a mean absolute error of 0.4$\pm$0.3%.
In addition, the study explores the use of MPC to adjust inter-stroke wait
times, effectively counteracting temperature disturbances to achieve a target
grain size of less than 35 microns within a specific 2D region of the
workpiece. These results are then verified experimentally, demonstrating a
significant step towards improved control and quality in forging processes
where temperature can be used as an additional degree of freedom in the
process.</div><div><a href='http://arxiv.org/abs/2402.16119v1'>2402.16119v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17718v1")'>Towards a Digital Twin Framework in Additive Manufacturing: Machine
  Learning and Bayesian Optimization for Time Series Process Optimization</div>
<div id='2402.17718v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:53:13Z</div><div>Authors: Vispi Karkaria, Anthony Goeckner, Rujing Zha, Jie Chen, Jianjing Zhang, Qi Zhu, Jian Cao, Robert X. Gao, Wei Chen</div><div style='padding-top: 10px; width: 80ex'>Laser-directed-energy deposition (DED) offers advantages in additive
manufacturing (AM) for creating intricate geometries and material grading. Yet,
challenges like material inconsistency and part variability remain, mainly due
to its layer-wise fabrication. A key issue is heat accumulation during DED,
which affects the material microstructure and properties. While closed-loop
control methods for heat management are common in DED research, few integrate
real-time monitoring, physics-based modeling, and control in a unified
framework. Our work presents a digital twin (DT) framework for real-time
predictive control of DED process parameters to meet specific design
objectives. We develop a surrogate model using Long Short-Term Memory
(LSTM)-based machine learning with Bayesian Inference to predict temperatures
in DED parts. This model predicts future temperature states in real time. We
also introduce Bayesian Optimization (BO) for Time Series Process Optimization
(BOTSPO), based on traditional BO but featuring a unique time series process
profile generator with reduced dimensions. BOTSPO dynamically optimizes
processes, identifying optimal laser power profiles to attain desired
mechanical properties. The established process trajectory guides online
optimizations, aiming to enhance performance. This paper outlines the digital
twin framework's components, promoting its integration into a comprehensive
system for AM.</div><div><a href='http://arxiv.org/abs/2402.17718v1'>2402.17718v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06888v2")'>Process signature-driven high spatio-temporal resolution alignment of
  multimodal data</div>
<div id='2403.06888v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:45:19Z</div><div>Authors: Abhishek Hanchate, Himanshu Balhara, Vishal S. Chindepalli, Satish T. S. Bukkapatnam</div><div style='padding-top: 10px; width: 80ex'>We present HiRA-Pro, a novel procedure to align, at high spatio-temporal
resolutions, multimodal signals from real-world processes and systems that
exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing
machines. It is based on discerning and synchronizing the process signatures of
salient kinematic and dynamic events in these disparate signals. HiRA-Pro
addresses the challenge of aligning data with sub-millisecond phenomena, where
traditional timestamp, external trigger, or clock-based alignment methods fall
short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing
context, where it aligns data from 13+ channels acquired during 3D-printing and
milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data
is then voxelized to generate 0.25 second aligned data chunks that correspond
to physical voxels on the produced part. The superiority of HiRA-Pro is further
showcased through case studies in additive manufacturing, demonstrating
improved machine learning-based predictive performance due to precise
multimodal data alignment. Specifically, testing classification accuracies
improved by almost 35% with the application of HiRA-Pro, even with limited
data, allowing for precise localization of artifacts. The paper also provides a
comprehensive discussion on the proposed method, its applications, and
comparative qualitative analysis with a few other alignment methods. HiRA-Pro
achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to
generate datasets that register with physical voxels on the 3D-printed and
milled part. These resolutions are at least an order of magnitude finer than
the existing alignment methods that employ individual timestamps, statistical
correlations, or common clocks, which achieve precision of hundreds of
milliseconds.</div><div><a href='http://arxiv.org/abs/2403.06888v2'>2403.06888v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15544v1")'>Analog and Multi-modal Manufacturing Datasets Acquired on the Future
  Factories Platform</div>
<div id='2401.15544v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T02:26:58Z</div><div>Authors: Ramy Harik, Fadi El Kalach, Jad Samaha, Devon Clark, Drew Sander, Philip Samaha, Liam Burns, Ibrahim Yousif, Victor Gadow, Theodros Tarekegne, Nitol Saha</div><div style='padding-top: 10px; width: 80ex'>Two industry-grade datasets are presented in this paper that were collected
at the Future Factories Lab at the University of South Carolina on December
11th and 12th of 2023. These datasets are generated by a manufacturing assembly
line that utilizes industrial standards with respect to actuators, control
mechanisms, and transducers. The two datasets were both generated
simultaneously by operating the assembly line for 30 consecutive hours (with
minor filtering) and collecting data from sensors equipped throughout the
system. During operation, defects were also introduced into the assembly
operation by manually removing parts needed for the final assembly. The
datasets generated include a time series analog dataset and the other is a time
series multi-modal dataset which includes images of the system alongside the
analog data. These datasets were generated with the objective of providing
tools to further the research towards enhancing intelligence in manufacturing.
Real manufacturing datasets can be scarce let alone datasets with anomalies or
defects. As such these datasets hope to address this gap and provide
researchers with a foundation to build and train Artificial Intelligence models
applicable for the manufacturing industry. Finally, these datasets are the
first iteration of published data from the future Factories lab and can be
further adjusted to fit more researchers needs moving forward.</div><div><a href='http://arxiv.org/abs/2401.15544v1'>2401.15544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02198v2")'>Co-orchestration of Multiple Instruments to Uncover Structure-Property
  Relationships in Combinatorial Libraries</div>
<div id='2402.02198v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T16:03:17Z</div><div>Authors: Boris N. Slautin, Utkarsh Pratiush, Ilia N. Ivanov, Yongtao Liu, Rohit Pant, Xiaohang Zhang, Ichiro Takeuchi, Maxim A. Ziatdinov, Sergei V. Kalinin</div><div style='padding-top: 10px; width: 80ex'>The rapid growth of automated and autonomous instrumentations brings forth an
opportunity for the co-orchestration of multimodal tools, equipped with
multiple sequential detection methods, or several characterization tools to
explore identical samples. This can be exemplified by the combinatorial
libraries that can be explored in multiple locations by multiple tools
simultaneously, or downstream characterization in automated synthesis systems.
In the co-orchestration approaches, information gained in one modality should
accelerate the discovery of other modalities. Correspondingly, the
orchestrating agent should select the measurement modality based on the
anticipated knowledge gain and measurement cost. Here, we propose and implement
a co-orchestration approach for conducting measurements with complex
observables such as spectra or images. The method relies on combining
dimensionality reduction by variational autoencoders with representation
learning for control over the latent space structure, and integrated into
iterative workflow via multi-task Gaussian Processes (GP). This approach
further allows for the native incorporation of the system's physics via a
probabilistic model as a mean function of the GP. We illustrated this method
for different modalities of piezoresponse force microscopy and micro-Raman on
combinatorial $Sm-BiFeO_3$ library. However, the proposed framework is general
and can be extended to multiple measurement modalities and arbitrary
dimensionality of measured signals. The analysis code that supports the funding
is publicly available at https://github.com/Slautin/2024_Co-orchestration.</div><div><a href='http://arxiv.org/abs/2402.02198v2'>2402.02198v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16501v1")'>AFSD-Physics: Exploring the governing equations of temperature evolution
  during additive friction stir deposition by a human-AI teaming approach</div>
<div id='2401.16501v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T19:17:42Z</div><div>Authors: Tony Shi, Mason Ma, Jiajie Wu, Chase Post, Elijah Charles, Tony Schmitz</div><div style='padding-top: 10px; width: 80ex'>This paper presents a modeling effort to explore the underlying physics of
temperature evolution during additive friction stir deposition (AFSD) by a
human-AI teaming approach. AFSD is an emerging solid-state additive
manufacturing technology that deposits materials without melting. However, both
process modeling and modeling of the AFSD tool are at an early stage. In this
paper, a human-AI teaming approach is proposed to combine models based on first
principles with AI. The resulting human-informed machine learning method,
denoted as AFSD-Physics, can effectively learn the governing equations of
temperature evolution at the tool and the build from in-process measurements.
Experiments are designed and conducted to collect in-process measurements for
the deposition of aluminum 7075 with a total of 30 layers. The acquired
governing equations are physically interpretable models with low computational
cost and high accuracy. Model predictions show good agreement with the
measurements. Experimental validation with new process parameters demonstrates
the model's generalizability and potential for use in tool temperature control
and process optimization.</div><div><a href='http://arxiv.org/abs/2401.16501v1'>2401.16501v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17573v1")'>Tensor-based process control and monitoring for semiconductor
  manufacturing with unstable disturbances</div>
<div id='2401.17573v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T03:35:08Z</div><div>Authors: Yanrong Li, Juan Du, Fugee Tsung, Wei Jiang</div><div style='padding-top: 10px; width: 80ex'>With the development and popularity of sensors installed in manufacturing
systems, complex data are collected during manufacturing processes, which
brings challenges for traditional process control methods. This paper proposes
a novel process control and monitoring method for the complex structure of
high-dimensional image-based overlay errors (modeled in tensor form), which are
collected in semiconductor manufacturing processes. The proposed method aims to
reduce overlay errors using limited control recipes. We first build a
high-dimensional process model and propose different tensor-on-vector
regression algorithms to estimate parameters in the model to alleviate the
curse of dimensionality. Then, based on the estimate of tensor parameters, the
exponentially weighted moving average (EWMA) controller for tensor data is
designed whose stability is theoretically guaranteed. Considering the fact that
low-dimensional control recipes cannot compensate for all high-dimensional
disturbances on the image, control residuals are monitored to prevent
significant drifts of uncontrollable high-dimensional disturbances. Through
extensive simulations and real case studies, the performances of parameter
estimation algorithms and the EWMA controller in tensor space are evaluated.
Compared with existing image-based feedback controllers, the superiority of our
method is verified especially when disturbances are not stable.</div><div><a href='http://arxiv.org/abs/2401.17573v1'>2401.17573v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.02536v1")'>Novel End-to-End Production-Ready Machine Learning Flow for
  Nanolithography Modeling and Correction</div>
<div id='2401.02536v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T20:53:43Z</div><div>Authors: Mohamed S. E. Habib, Hossam A. H. Fahmy, Mohamed F. Abu-ElYazeed</div><div style='padding-top: 10px; width: 80ex'>Optical lithography is the main enabler to semiconductor manufacturing. It
requires extensive processing to perform the Resolution Enhancement Techniques
(RETs) required to transfer the design data to a working Integrated Circuits
(ICs). The processing power and computational runtime for RETs tasks is ever
increasing due to the continuous reduction of the feature size and the
expansion of the chip area. State-of-the-art research sought Machine Learning
(ML) technologies to reduce runtime and computational power, however they are
still not used in production yet. In this study, we analyze the reasons holding
back ML computational lithography from being production ready and present a
novel highly scalable end-to-end flow that enables production ready ML-RET
correction.</div><div><a href='http://arxiv.org/abs/2401.02536v1'>2401.02536v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.13192v2")'>Generative Design of Crystal Structures by Point Cloud Representations
  and Diffusion Model</div>
<div id='2401.13192v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T02:36:52Z</div><div>Authors: Zhelin Li, Rami Mrad, Runxian Jiao, Guan Huang, Jun Shan, Shibing Chu, Yuanping Chen</div><div style='padding-top: 10px; width: 80ex'>Efficiently generating energetically stable crystal structures has long been
a challenge in material design, primarily due to the immense arrangement of
atoms in a crystal lattice. To facilitate the discovery of stable material, we
present a framework for the generation of synthesizable materials, leveraging a
point cloud representation to encode intricate structural information. At the
heart of this framework lies the introduction of a diffusion model as its
foundational pillar. To gauge the efficacy of our approach, we employ it to
reconstruct input structures from our training datasets, rigorously validating
its high reconstruction performance. Furthermore, we demonstrate the profound
potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely
new materials, emphasizing their synthesizability. Our research stands as a
noteworthy contribution to the advancement of materials design and synthesis
through the cutting-edge avenue of generative design instead of the
conventional substitution or experience-based discovery.</div><div><a href='http://arxiv.org/abs/2401.13192v2'>2401.13192v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17695v1")'>Geometric Deep Learning for Computer-Aided Design: A Survey</div>
<div id='2402.17695v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T17:11:35Z</div><div>Authors: Negar Heidari, Alexandros Iosifidis</div><div style='padding-top: 10px; width: 80ex'>Geometric Deep Learning techniques have become a transformative force in the
field of Computer-Aided Design (CAD), and have the potential to revolutionize
how designers and engineers approach and enhance the design process. By
harnessing the power of machine learning-based methods, CAD designers can
optimize their workflows, save time and effort while making better informed
decisions, and create designs that are both innovative and practical. The
ability to process the CAD designs represented by geometric data and to analyze
their encoded features enables the identification of similarities among diverse
CAD models, the proposition of alternative designs and enhancements, and even
the generation of novel design alternatives. This survey offers a comprehensive
overview of learning-based methods in computer-aided design across various
categories, including similarity analysis and retrieval, 2D and 3D CAD model
synthesis, and CAD generation from point clouds. Additionally, it provides a
complete list of benchmark datasets and their characteristics, along with
open-source codes that have propelled research in this domain. The final
discussion delves into the challenges prevalent in this field, followed by
potential future research directions in this rapidly evolving field.</div><div><a href='http://arxiv.org/abs/2402.17695v1'>2402.17695v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11291v1")'>Advanced Knowledge Extraction of Physical Design Drawings, Translation
  and conversion to CAD formats using Deep Learning</div>
<div id='2403.11291v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T18:06:06Z</div><div>Authors: Jesher Joshua M, Ragav V, Syed Ibrahim S P</div><div style='padding-top: 10px; width: 80ex'>The maintenance, archiving and usage of the design drawings is cumbersome in
physical form in different industries for longer period. It is hard to extract
information by simple scanning of drawing sheets. Converting them to their
digital formats such as Computer-Aided Design (CAD), with needed knowledge
extraction can solve this problem. The conversion of these machine drawings to
its digital form is a crucial challenge which requires advanced techniques.
This research proposes an innovative methodology utilizing Deep Learning
methods. The approach employs object detection model, such as Yolov7, Faster
R-CNN, to detect physical drawing objects present in the images followed by,
edge detection algorithms such as canny filter to extract and refine the
identified lines from the drawing region and curve detection techniques to
detect circle. Also ornaments (complex shapes) within the drawings are
extracted. To ensure comprehensive conversion, an Optical Character Recognition
(OCR) tool is integrated to identify and extract the text elements from the
drawings. The extracted data which includes the lines, shapes and text is
consolidated and stored in a structured comma separated values(.csv) file
format. The accuracy and the efficiency of conversion is evaluated. Through
this, conversion can be automated to help organizations enhance their
productivity, facilitate seamless collaborations and preserve valuable design
information in a digital format easily accessible. Overall, this study
contributes to the advancement of CAD conversions, providing accurate results
from the translating process. Future research can focus on handling diverse
drawing types, enhanced accuracy in shape and line detection and extraction.</div><div><a href='http://arxiv.org/abs/2403.11291v1'>2403.11291v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11093v1")'>Modular Graph Extraction for Handwritten Circuit Diagram Images</div>
<div id='2402.11093v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T21:39:28Z</div><div>Authors: Johannes Bayer, Leo van Waveren, Andreas Dengel</div><div style='padding-top: 10px; width: 80ex'>As digitization in engineering progressed, circuit diagrams (also referred to
as schematics) are typically developed and maintained in computer-aided
engineering (CAE) systems, thus allowing for automated verification, simulation
and further processing in downstream engineering steps. However, apart from
printed legacy schematics, hand-drawn circuit diagrams are still used today in
the educational domain, where they serve as an easily accessible mean for
trainees and students to learn drawing this type of diagrams. Furthermore,
hand-drawn schematics are typically used in examinations due to legal
constraints. In order to harness the capabilities of digital circuit
representations, automated means for extracting the electrical graph from
raster graphics are required.
  While respective approaches have been proposed in literature, they are
typically conducted on small or non-disclosed datasets. This paper describes a
modular end-to-end solution on a larger, public dataset, in which approaches
for the individual sub-tasks are evaluated to form a new baseline. These
sub-tasks include object detection (for electrical symbols and texts), binary
segmentation (drafter's stroke vs. background), handwritten character
recognition and orientation regression for electrical symbols and texts.
Furthermore, computer-vision graph assembly and rectification algorithms are
presented. All methods are integrated in a publicly available prototype.</div><div><a href='http://arxiv.org/abs/2402.11093v1'>2402.11093v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11401v2")'>GraphKD: Exploring Knowledge Distillation Towards Document Object
  Detection with Structured Graph Creation</div>
<div id='2402.11401v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T23:08:32Z</div><div>Authors: Ayan Banerjee, Sanket Biswas, Josep Lladós, Umapada Pal</div><div style='padding-top: 10px; width: 80ex'>Object detection in documents is a key step to automate the structural
elements identification process in a digital or scanned document through
understanding the hierarchical structure and relationships between different
elements. Large and complex models, while achieving high accuracy, can be
computationally expensive and memory-intensive, making them impractical for
deployment on resource constrained devices. Knowledge distillation allows us to
create small and more efficient models that retain much of the performance of
their larger counterparts. Here we present a graph-based knowledge distillation
framework to correctly identify and localize the document objects in a document
image. Here, we design a structured graph with nodes containing proposal-level
features and edges representing the relationship between the different proposal
regions. Also, to reduce text bias an adaptive node sampling strategy is
designed to prune the weight distribution and put more weightage on non-text
nodes. We encode the complete graph as a knowledge representation and transfer
it from the teacher to the student through the proposed distillation loss by
effectively capturing both local and global information concurrently. Extensive
experimentation on competitive benchmarks demonstrates that the proposed
framework outperforms the current state-of-the-art approaches. The code will be
available at: https://github.com/ayanban011/GraphKD.</div><div><a href='http://arxiv.org/abs/2402.11401v2'>2402.11401v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08425v1")'>U-DIADS-Bib: a full and few-shot pixel-precise dataset for document
  layout analysis of ancient manuscripts</div>
<div id='2401.08425v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T15:11:18Z</div><div>Authors: Silvia Zottin, Axel De Nardin, Emanuela Colombi, Claudio Piciarelli, Filippo Pavan, Gian Luca Foresti</div><div style='padding-top: 10px; width: 80ex'>Document Layout Analysis, which is the task of identifying different semantic
regions inside of a document page, is a subject of great interest for both
computer scientists and humanities scholars as it represents a fundamental step
towards further analysis tasks for the former and a powerful tool to improve
and facilitate the study of the documents for the latter. However, many of the
works currently present in the literature, especially when it comes to the
available datasets, fail to meet the needs of both worlds and, in particular,
tend to lean towards the needs and common practices of the computer science
side, leading to resources that are not representative of the humanities real
needs. For this reason, the present paper introduces U-DIADS-Bib, a novel,
pixel-precise, non-overlapping and noiseless document layout analysis dataset
developed in close collaboration between specialists in the fields of computer
vision and humanities. Furthermore, we propose a novel, computer-aided,
segmentation pipeline in order to alleviate the burden represented by the
time-consuming process of manual annotation, necessary for the generation of
the ground truth segmentation maps. Finally, we present a standardized few-shot
version of the dataset (U-DIADS-BibFS), with the aim of encouraging the
development of models and solutions able to address this task with as few
samples as possible, which would allow for more effective use in a real-world
scenario, where collecting a large number of segmentations is not always
feasible.</div><div><a href='http://arxiv.org/abs/2401.08425v1'>2401.08425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.03747v1")'>An invariance constrained deep learning network for PDE discovery</div>
<div id='2402.03747v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T06:28:17Z</div><div>Authors: Chao Chen, Hui Li, Xiaowei Jin</div><div style='padding-top: 10px; width: 80ex'>The discovery of partial differential equations (PDEs) from datasets has
attracted increased attention. However, the discovery of governing equations
from sparse data with high noise is still very challenging due to the
difficulty of derivatives computation and the disturbance of noise. Moreover,
the selection principles for the candidate library to meet physical laws need
to be further studied. The invariance is one of the fundamental laws for
governing equations. In this study, we propose an invariance constrained deep
learning network (ICNet) for the discovery of PDEs. Considering that temporal
and spatial translation invariance (Galilean invariance) is a fundamental
property of physical laws, we filter the candidates that cannot meet the
requirement of the Galilean transformations. Subsequently, we embedded the
fixed and possible terms into the loss function of neural network,
significantly countering the effect of sparse data with high noise. Then, by
filtering out redundant terms without fixing learnable parameters during the
training process, the governing equations discovered by the ICNet method can
effectively approximate the real governing equations. We select the 2D Burgers
equation, the equation of 2D channel flow over an obstacle, and the equation of
3D intracranial aneurysm as examples to verify the superiority of the ICNet for
fluid mechanics. Furthermore, we extend similar invariance methods to the
discovery of wave equation (Lorentz Invariance) and verify it through Single
and Coupled Klein-Gordon equation. The results show that the ICNet method with
physical constraints exhibits excellent performance in governing equations
discovery from sparse and noisy data.</div><div><a href='http://arxiv.org/abs/2402.03747v1'>2402.03747v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03541v1")'>HAMLET: Graph Transformer Neural Operator for Partial Differential
  Equations</div>
<div id='2402.03541v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T21:55:24Z</div><div>Authors: Andrey Bryutkin, Jiahao Huang, Zhongying Deng, Guang Yang, Carola-Bibiane Schönlieb, Angelica Aviles-Rivero</div><div style='padding-top: 10px; width: 80ex'>We present a novel graph transformer framework, HAMLET, designed to address
the challenges in solving partial differential equations (PDEs) using neural
networks. The framework uses graph transformers with modular input encoders to
directly incorporate differential equation information into the solution
process. This modularity enhances parameter correspondence control, making
HAMLET adaptable to PDEs of arbitrary geometries and varied input formats.
Notably, HAMLET scales effectively with increasing data complexity and noise,
showcasing its robustness. HAMLET is not just tailored to a single type of
physical simulation, but can be applied across various domains. Moreover, it
boosts model resilience and performance, especially in scenarios with limited
data. We demonstrate, through extensive experiments, that our framework is
capable of outperforming current techniques for PDEs.</div><div><a href='http://arxiv.org/abs/2402.03541v1'>2402.03541v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.16517v1")'>Discovering Artificial Viscosity Models for Discontinuous Galerkin
  Approximation of Conservation Laws using Physics-Informed Machine Learning</div>
<div id='2402.16517v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T11:58:02Z</div><div>Authors: Matteo Caldana, Paola F. Antonietti, Luca Dede'</div><div style='padding-top: 10px; width: 80ex'>Finite element-based high-order solvers of conservation laws offer large
accuracy but face challenges near discontinuities due to the Gibbs phenomenon.
Artificial viscosity is a popular and effective solution to this problem based
on physical insight. In this work, we present a physics-informed machine
learning algorithm to automate the discovery of artificial viscosity models in
a non-supervised paradigm. The algorithm is inspired by reinforcement learning
and trains a neural network acting cell-by-cell (the viscosity model) by
minimizing a loss defined as the difference with respect to a reference
solution thanks to automatic differentiation. This enables a dataset-free
training procedure. We prove that the algorithm is effective by integrating it
into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase
several numerical tests on scalar and vectorial problems, such as Burgers' and
Euler's equations in one and two dimensions. Results demonstrate that the
proposed approach trains a model that is able to outperform classical viscosity
models. Moreover, we show that the learnt artificial viscosity model is able to
generalize across different problems and parameters.</div><div><a href='http://arxiv.org/abs/2402.16517v1'>2402.16517v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04986v1")'>Structure-Preserving Physics-Informed Neural Networks With Energy or
  Lyapunov Structure</div>
<div id='2401.04986v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T08:02:38Z</div><div>Authors: Haoyu Chu, Yuto Miyatake, Wenjun Cui, Shikui Wei, Daisuke Furihata</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been growing interest in using physics-informed neural
networks (PINNs) to solve differential equations. However, the preservation of
structure, such as energy and stability, in a suitable manner has yet to be
established. This limitation could be a potential reason why the learning
process for PINNs is not always efficient and the numerical results may suggest
nonphysical behavior. Besides, there is little research on their applications
on downstream tasks. To address these issues, we propose structure-preserving
PINNs to improve their performance and broaden their applications for
downstream tasks. Firstly, by leveraging prior knowledge about the physical
system, a structure-preserving loss function is designed to assist the PINN in
learning the underlying structure. Secondly, a framework that utilizes
structure-preserving PINN for robust image recognition is proposed. Here,
preserving the Lyapunov structure of the underlying system ensures the
stability of the system. Experimental results demonstrate that the proposed
method improves the numerical accuracy of PINNs for partial differential
equations. Furthermore, the robustness of the model against adversarial
perturbations in image data is enhanced.</div><div><a href='http://arxiv.org/abs/2401.04986v1'>2401.04986v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04847v1")'>Solving Inverse Problems with Model Mismatch using Untrained Neural
  Networks within Model-based Architectures</div>
<div id='2403.04847v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T19:02:13Z</div><div>Authors: Peimeng Guan, Naveed Iqbal, Mark A. Davenport, Mudassir Masood</div><div style='padding-top: 10px; width: 80ex'>Model-based deep learning methods such as \emph{loop unrolling} (LU) and
\emph{deep equilibrium model} (DEQ) extensions offer outstanding performance in
solving inverse problems (IP). These methods unroll the optimization iterations
into a sequence of neural networks that in effect learn a regularization
function from data. While these architectures are currently state-of-the-art in
numerous applications, their success heavily relies on the accuracy of the
forward model. This assumption can be limiting in many physical applications
due to model simplifications or uncertainties in the apparatus. To address
forward model mismatch, we introduce an untrained forward model residual block
within the model-based architecture to match the data consistency in the
measurement domain for each instance. We propose two variants in well-known
model-based architectures (LU and DEQ) and prove convergence under mild
conditions. The experiments show significant quality improvement in removing
artifacts and preserving details across three distinct applications,
encompassing both linear and nonlinear inverse problems. Moreover, we highlight
reconstruction effectiveness in intermediate steps and showcase robustness to
random initialization of the residual block and a higher number of iterations
during evaluation.</div><div><a href='http://arxiv.org/abs/2403.04847v1'>2403.04847v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08313v1")'>Approximating Families of Sharp Solutions to Fisher's Equation with
  Physics-Informed Neural Networks</div>
<div id='2402.08313v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T09:17:20Z</div><div>Authors: Franz M. Rohrhofer, Stefan Posch, Clemens Gößnitzer, Bernhard C. Geiger</div><div style='padding-top: 10px; width: 80ex'>This paper employs physics-informed neural networks (PINNs) to solve Fisher's
equation, a fundamental representation of a reaction-diffusion system with both
simplicity and significance. The focus lies specifically in investigating
Fisher's equation under conditions of large reaction rate coefficients, wherein
solutions manifest as traveling waves, posing a challenge for numerical methods
due to the occurring steepness of the wave front. To address optimization
challenges associated with the standard PINN approach, a residual weighting
scheme is introduced. This scheme is designed to enhance the tracking of
propagating wave fronts by considering the reaction term in the
reaction-diffusion equation. Furthermore, a specific network architecture is
studied which is tailored for solutions in the form of traveling waves. Lastly,
the capacity of PINNs to approximate an entire family of solutions is assessed
by incorporating the reaction rate coefficient as an additional input to the
network architecture. This modification enables the approximation of the
solution across a broad and continuous range of reaction rate coefficients,
thus solving a class of reaction-diffusion systems using a single PINN
instance.</div><div><a href='http://arxiv.org/abs/2402.08313v1'>2402.08313v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00720v1")'>Subhomogeneous Deep Equilibrium Models</div>
<div id='2403.00720v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T18:12:46Z</div><div>Authors: Pietro Sittoni, Francesco Tudisco</div><div style='padding-top: 10px; width: 80ex'>Implicit-depth neural networks have grown as powerful alternatives to
traditional networks in various applications in recent years. However, these
models often lack guarantees of existence and uniqueness, raising stability,
performance, and reproducibility issues. In this paper, we present a new
analysis of the existence and uniqueness of fixed points for implicit-depth
neural networks based on the concept of subhomogeneous operators and the
nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our
theory allows for weaker assumptions on the parameter matrices, thus yielding a
more flexible framework for well-defined implicit networks. We illustrate the
performance of the resulting subhomogeneous networks on feed-forward,
convolutional, and graph neural network examples.</div><div><a href='http://arxiv.org/abs/2403.00720v1'>2403.00720v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.07862v1")'>Adaptive Neural-Operator Backstepping Control of a Benchmark Hyperbolic
  PDE</div>
<div id='2401.07862v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T17:52:15Z</div><div>Authors: Maxence Lamarque, Luke Bhan, Yuanyuan Shi, Miroslav Krstic</div><div style='padding-top: 10px; width: 80ex'>To stabilize PDEs, feedback controllers require gain kernel functions, which
are themselves governed by PDEs. Furthermore, these gain-kernel PDEs depend on
the PDE plants' functional coefficients. The functional coefficients in PDE
plants are often unknown. This requires an adaptive approach to PDE control,
i.e., an estimation of the plant coefficients conducted concurrently with
control, where a separate PDE for the gain kernel must be solved at each
timestep upon the update in the plant coefficient function estimate. Solving a
PDE at each timestep is computationally expensive and a barrier to the
implementation of real-time adaptive control of PDEs. Recently, results in
neural operator (NO) approximations of functional mappings have been introduced
into PDE control, for replacing the computation of the gain kernel with a
neural network that is trained, once offline, and reused in real-time for rapid
solution of the PDEs. In this paper, we present the first result on applying
NOs in adaptive PDE control, presented for a benchmark 1-D hyperbolic PDE with
recirculation. We establish global stabilization via Lyapunov analysis, in the
plant and parameter error states, and also present an alternative approach, via
passive identifiers, which avoids the strong assumptions on kernel
differentiability. We then present numerical simulations demonstrating
stability and observe speedups up to three orders of magnitude, highlighting
the real-time efficacy of neural operators in adaptive control. Our code
(Github) is made publicly available for future researchers.</div><div><a href='http://arxiv.org/abs/2401.07862v1'>2401.07862v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02511v1")'>Gain Scheduling with a Neural Operator for a Transport PDE with
  Nonlinear Recirculation</div>
<div id='2401.02511v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T19:45:27Z</div><div>Authors: Maxence Lamarque, Luke Bhan, Rafael Vazquez, Miroslav Krstic</div><div style='padding-top: 10px; width: 80ex'>To stabilize PDE models, control laws require space-dependent functional
gains mapped by nonlinear operators from the PDE functional coefficients. When
a PDE is nonlinear and its "pseudo-coefficient" functions are state-dependent,
a gain-scheduling (GS) nonlinear design is the simplest approach to the design
of nonlinear feedback. The GS version of PDE backstepping employs gains
obtained by solving a PDE at each value of the state. Performing such PDE
computations in real time may be prohibitive. The recently introduced neural
operators (NO) can be trained to produce the gain functions, rapidly in real
time, for each state value, without requiring a PDE solution. In this paper we
introduce NOs for GS-PDE backstepping. GS controllers act on the premise that
the state change is slow and, as a result, guarantee only local stability, even
for ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear
recirculation using both a "full-kernel" approach and the "gain-only" approach
to gain operator approximation. Numerical simulations illustrate stabilization
and demonstrate speedup by three orders of magnitude over traditional PDE
gain-scheduling. Code (Github) for the numerical implementation is published to
enable exploration.</div><div><a href='http://arxiv.org/abs/2401.02511v1'>2401.02511v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07763v1")'>Multi-level Optimal Control with Neural Surrogate Models</div>
<div id='2402.07763v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T16:28:57Z</div><div>Authors: Dante Kalise, Estefanía Loayza-Romero, Kirsten A. Morris, Zhengang Zhong</div><div style='padding-top: 10px; width: 80ex'>Optimal actuator and control design is studied as a multi-level optimisation
problem, where the actuator design is evaluated based on the performance of the
associated optimal closed loop. The evaluation of the optimal closed loop for a
given actuator realisation is a computationally demanding task, for which the
use of a neural network surrogate is proposed. The use of neural network
surrogates to replace the lower level of the optimisation hierarchy enables the
use of fast gradient-based and gradient-free consensus-based optimisation
methods to determine the optimal actuator design. The effectiveness of the
proposed surrogate models and optimisation methods is assessed in a test
related to optimal actuator location for heat control.</div><div><a href='http://arxiv.org/abs/2402.07763v1'>2402.07763v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.06955v1")'>Training dynamics in Physics-Informed Neural Networks with feature
  mapping</div>
<div id='2402.06955v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T13:51:09Z</div><div>Authors: Chengxi Zeng, Tilo Burghardt, Alberto M Gambaruto</div><div style='padding-top: 10px; width: 80ex'>Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine
learning approach for solving Partial Differential Equations (PDEs). Although
its variants have achieved significant progress, the empirical success of
utilising feature mapping from the wider Implicit Neural Representations
studies has been substantially neglected. We investigate the training dynamics
of PINNs with a feature mapping layer via the limiting Conjugate Kernel and
Neural Tangent Kernel, which sheds light on the convergence and generalisation
of the model. We also show the inadequacy of commonly used Fourier-based
feature mapping in some scenarios and propose the conditional positive definite
Radial Basis Function as a better alternative. The empirical results reveal the
efficacy of our method in diverse forward and inverse problem sets. This simple
technique can be easily implemented in coordinate input networks and benefits
the broad PINNs research.</div><div><a href='http://arxiv.org/abs/2402.06955v1'>2402.06955v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08367v1")'>RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural
  Networks</div>
<div id='2402.08367v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T10:54:43Z</div><div>Authors: Chengxi Zeng, Tilo Burghardt, Alberto M Gambaruto</div><div style='padding-top: 10px; width: 80ex'>While many recent Physics-Informed Neural Networks (PINNs) variants have had
considerable success in solving Partial Differential Equations, the empirical
benefits of feature mapping drawn from the broader Neural Representations
research have been largely overlooked. We highlight the limitations of widely
used Fourier-based feature mapping in certain situations and suggest the use of
the conditionally positive definite Radial Basis Function. The empirical
findings demonstrate the effectiveness of our approach across a variety of
forward and inverse problem cases. Our method can be seamlessly integrated into
coordinate-based input neural networks and contribute to the wider field of
PINNs research.</div><div><a href='http://arxiv.org/abs/2402.08367v1'>2402.08367v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07595v2")'>E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy</div>
<div id='2401.07595v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T11:04:47Z</div><div>Authors: Oliver T. Unke, Hartmut Maennel</div><div style='padding-top: 10px; width: 80ex'>This work introduces E3x, a software package for building neural networks
that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$,
consisting of translations, rotations, and reflections of three-dimensional
space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models
promise benefits whenever input and/or output data are quantities associated
with three-dimensional objects. This is because the numeric values of such
quantities (e.g. positions) typically depend on the chosen coordinate system.
Under transformations of the reference frame, the values change predictably,
but the underlying rules can be difficult to learn for ordinary machine
learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks
are guaranteed to satisfy the relevant transformation rules exactly, resulting
in superior data efficiency and accuracy. The code for E3x is available from
https://github.com/google-research/e3x, detailed documentation and usage
examples can be found on https://e3x.readthedocs.io.</div><div><a href='http://arxiv.org/abs/2401.07595v2'>2401.07595v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15182v1")'>PDE-CNNs: Axiomatic Derivations and Applications</div>
<div id='2403.15182v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T13:11:26Z</div><div>Authors: Gijs Bellaard, Sei Sakata, Bart M. N. Smets, Remco Duits</div><div style='padding-top: 10px; width: 80ex'>PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of
geometrically meaningful evolution PDEs as substitutes for the conventional
components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer
parameters, inherent equivariance, better performance, data efficiency, and
geometric interpretability. In this article we focus on Euclidean equivariant
PDE-G-CNNs where the feature maps are two dimensional throughout. We call this
variant of the framework a PDE-CNN. We list several practically desirable
axioms and derive from these which PDEs should be used in a PDE-CNN. Here our
approach to geometric learning via PDEs is inspired by the axioms of classical
linear and morphological scale-space theory, which we generalize by introducing
semifield-valued signals. Furthermore, we experimentally confirm for small
networks that PDE-CNNs offer fewer parameters, better performance, and data
efficiency in comparison to CNNs. We also investigate what effect the use of
different semifields has on the performance of the models.</div><div><a href='http://arxiv.org/abs/2403.15182v1'>2403.15182v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09235v1")'>A Characterization Theorem for Equivariant Networks with Point-wise
  Activations</div>
<div id='2401.09235v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T14:30:46Z</div><div>Authors: Marco Pacini, Xiaowen Dong, Bruno Lepri, Gabriele Santin</div><div style='padding-top: 10px; width: 80ex'>Equivariant neural networks have shown improved performance, expressiveness
and sample complexity on symmetrical domains. But for some specific symmetries,
representations, and choice of coordinates, the most common point-wise
activations, such as ReLU, are not equivariant, hence they cannot be employed
in the design of equivariant neural networks. The theorem we present in this
paper describes all possible combinations of finite-dimensional
representations, choice of coordinates and point-wise activations to obtain an
exactly equivariant layer, generalizing and strengthening existing
characterizations. Notable cases of practical relevance are discussed as
corollaries. Indeed, we prove that rotation-equivariant networks can only be
invariant, as it happens for any network which is equivariant with respect to
connected compact groups. Then, we discuss implications of our findings when
applied to important instances of exactly equivariant networks. First, we
completely characterize permutation equivariant networks such as Invariant
Graph Networks with point-wise nonlinearities and their geometric counterparts,
highlighting a plethora of models whose expressive power and performance are
still unknown. Second, we show that feature spaces of disentangled steerable
convolutional neural networks are trivial representations.</div><div><a href='http://arxiv.org/abs/2401.09235v1'>2401.09235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01869v1")'>On the hardness of learning under symmetries</div>
<div id='2401.01869v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T18:24:18Z</div><div>Authors: Bobak T. Kiani, Thien Le, Hannah Lawrence, Stefanie Jegelka, Melanie Weber</div><div style='padding-top: 10px; width: 80ex'>We study the problem of learning equivariant neural networks via gradient
descent. The incorporation of known symmetries ("equivariance") into neural
nets has empirically improved the performance of learning pipelines, in domains
ranging from biology to computer vision. However, a rich yet separate line of
learning theoretic research has demonstrated that actually learning shallow,
fully-connected (i.e. non-symmetric) networks has exponential complexity in the
correlational statistical query (CSQ) model, a framework encompassing gradient
descent. In this work, we ask: are known problem symmetries sufficient to
alleviate the fundamental hardness of learning neural nets with gradient
descent? We answer this question in the negative. In particular, we give lower
bounds for shallow graph neural networks, convolutional networks, invariant
polynomials, and frame-averaged networks for permutation subgroups, which all
scale either superpolynomially or exponentially in the relevant input
dimension. Therefore, in spite of the significant inductive bias imparted via
symmetry, actually learning the complete classes of functions represented by
equivariant neural networks via gradient descent remains hard.</div><div><a href='http://arxiv.org/abs/2401.01869v1'>2401.01869v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14009v1")'>Geometry-Informed Neural Networks</div>
<div id='2402.14009v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T18:50:12Z</div><div>Authors: Arturs Berzins, Andreas Radler, Sebastian Sanokowski, Sepp Hochreiter, Johannes Brandstetter</div><div style='padding-top: 10px; width: 80ex'>We introduce the concept of geometry-informed neural networks (GINNs), which
encompass (i) learning under geometric constraints, (ii) neural fields as a
suitable representation, and (iii) generating diverse solutions to
under-determined systems often encountered in geometric tasks. Notably, the
GINN formulation does not require training data, and as such can be considered
generative modeling driven purely by constraints. We add an explicit diversity
loss to mitigate mode collapse. We consider several constraints, in particular,
the connectedness of components which we convert to a differentiable loss
through Morse theory. Experimentally, we demonstrate the efficacy of the GINN
learning paradigm across a range of two and three-dimensional scenarios with
increasing levels of complexity.</div><div><a href='http://arxiv.org/abs/2402.14009v1'>2402.14009v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06612v1")'>Pulling back symmetric Riemannian geometry for data analysis</div>
<div id='2403.06612v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T10:59:55Z</div><div>Authors: Willem Diepeveen</div><div style='padding-top: 10px; width: 80ex'>Data sets tend to live in low-dimensional non-linear subspaces. Ideal data
analysis tools for such data sets should therefore account for such non-linear
geometry. The symmetric Riemannian geometry setting can be suitable for a
variety of reasons. First, it comes with a rich mathematical structure to
account for a wide range of non-linear geometries that has been shown to be
able to capture the data geometry through empirical evidence from classical
non-linear embedding. Second, many standard data analysis tools initially
developed for data in Euclidean space can also be generalised efficiently to
data on a symmetric Riemannian manifold. A conceptual challenge comes from the
lack of guidelines for constructing a symmetric Riemannian structure on the
data space itself and the lack of guidelines for modifying successful
algorithms on symmetric Riemannian manifolds for data analysis to this setting.
This work considers these challenges in the setting of pullback Riemannian
geometry through a diffeomorphism. The first part of the paper characterises
diffeomorphisms that result in proper, stable and efficient data analysis. The
second part then uses these best practices to guide construction of such
diffeomorphisms through deep learning. As a proof of concept, different types
of pullback geometries -- among which the proposed construction -- are tested
on several data analysis tasks and on several toy data sets. The numerical
experiments confirm the predictions from theory, i.e., that the diffeomorphisms
generating the pullback geometry need to map the data manifold into a geodesic
subspace of the pulled back Riemannian manifold while preserving local isometry
around the data manifold for proper, stable and efficient data analysis, and
that pulling back positive curvature can be problematic in terms of stability.</div><div><a href='http://arxiv.org/abs/2403.06612v1'>2403.06612v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15413v1")'>G-RepsNet: A Fast and General Construction of Equivariant Networks for
  Arbitrary Matrix Groups</div>
<div id='2402.15413v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T16:19:49Z</div><div>Authors: Sourya Basu, Suhas Lohit, Matthew Brand</div><div style='padding-top: 10px; width: 80ex'>Group equivariance is a strong inductive bias useful in a wide range of deep
learning tasks. However, constructing efficient equivariant networks for
general groups and domains is difficult. Recent work by Finzi et al. (2021)
directly solves the equivariance constraint for arbitrary matrix groups to
obtain equivariant MLPs (EMLPs). But this method does not scale well and
scaling is crucial in deep learning. Here, we introduce Group Representation
Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix
groups with features represented using tensor polynomials. The key intuition
for our design is that using tensor representations in the hidden layers of a
neural network along with simple inexpensive tensor operations can lead to
expressive universal equivariant networks. We find G-RepsNet to be competitive
to EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3)
with scalars, vectors, and second-order tensors as data types. On image
classification tasks, we find that G-RepsNet using second-order representations
is competitive and often even outperforms sophisticated state-of-the-art
equivariant models such as GCNNs (Cohen &amp; Welling, 2016a) and E(2)-CNNs (Weiler
&amp; Cesa, 2019). To further illustrate the generality of our approach, we show
that G-RepsNet is competitive to G-FNO (Helwig et al., 2023) and EGNN (Satorras
et al., 2021) on N-body predictions and solving PDEs, respectively, while being
efficient.</div><div><a href='http://arxiv.org/abs/2402.15413v1'>2402.15413v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17002v3")'>Discovering Symmetry Group Structures via Implicit Orthogonality Bias</div>
<div id='2402.17002v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T20:18:43Z</div><div>Authors: Dongsung Huh</div><div style='padding-top: 10px; width: 80ex'>We introduce the HyperCube network, a novel approach for autonomously
discovering symmetry group structures within data. The key innovation is a
unique factorization architecture coupled with a novel regularizer that
instills a powerful inductive bias towards learning orthogonal representations.
This leverages a fundamental theorem of representation theory that all
compact/finite groups can be represented by orthogonal matrices. HyperCube
efficiently learns general group operations from partially observed data,
successfully recovering complete operation tables. Remarkably, the learned
factors correspond directly to exact matrix representations of the underlying
group. Moreover, these factors capture the group's complete set of irreducible
representations, forming the generalized Fourier basis for performing group
convolutions. In extensive experiments with both group and non-group symbolic
operations, HyperCube demonstrates a dramatic 100-1000x improvement in training
speed and 2-10x greater sample efficiency compared to the Transformer baseline.
These results suggest that our approach unlocks a new class of deep learning
models capable of harnessing inherent symmetries within data, leading to
significant improvements in performance and broader applicability.</div><div><a href='http://arxiv.org/abs/2402.17002v3'>2402.17002v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.05965v1")'>Hybrid Neural Representations for Spherical Data</div>
<div id='2402.05965v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T13:03:00Z</div><div>Authors: Hyomin Kim, Yunhui Jang, Jaeho Lee, Sungsoo Ahn</div><div style='padding-top: 10px; width: 80ex'>In this paper, we study hybrid neural representations for spherical data, a
domain of increasing relevance in scientific research. In particular, our work
focuses on weather and climate data as well as comic microwave background (CMB)
data. Although previous studies have delved into coordinate-based neural
representations for spherical signals, they often fail to capture the intricate
details of highly nonlinear signals. To address this limitation, we introduce a
novel approach named Hybrid Neural Representations for Spherical data (HNeR-S).
Our main idea is to use spherical feature-grids to obtain positional features
which are combined with a multilayer perception to predict the target signal.
We consider feature-grids with equirectangular and hierarchical equal area
isolatitude pixelization structures that align with weather data and CMB data,
respectively. We extensively verify the effectiveness of our HNeR-S for
regression, super-resolution, temporal interpolation, and compression tasks.</div><div><a href='http://arxiv.org/abs/2402.05965v1'>2402.05965v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.08784v1")'>Preconditioners for the Stochastic Training of Implicit Neural
  Representations</div>
<div id='2402.08784v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T20:46:37Z</div><div>Authors: Shin-Fang Chng, Hemanth Saratchandran, Simon Lucey</div><div style='padding-top: 10px; width: 80ex'>Implicit neural representations have emerged as a powerful technique for
encoding complex continuous multidimensional signals as neural networks,
enabling a wide range of applications in computer vision, robotics, and
geometry. While Adam is commonly used for training due to its stochastic
proficiency, it entails lengthy training durations. To address this, we explore
alternative optimization techniques for accelerated training without
sacrificing accuracy. Traditional second-order optimizers like L-BFGS are
suboptimal in stochastic settings, making them unsuitable for large-scale data
sets. Instead, we propose stochastic training using curvature-aware diagonal
preconditioners, showcasing their effectiveness across various signal
modalities such as images, shape reconstruction, and Neural Radiance Fields
(NeRF).</div><div><a href='http://arxiv.org/abs/2402.08784v1'>2402.08784v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01391v1")'>On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional
  Encoding</div>
<div id='2401.01391v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T10:51:52Z</div><div>Authors: Guying Lin, Lei Yang, Yuan Liu, Congyi Zhang, Junhui Hou, Xiaogang Jin, Taku Komura, John Keyser, Wenping Wang</div><div style='padding-top: 10px; width: 80ex'>Neural implicit fields, such as the neural signed distance field (SDF) of a
shape, have emerged as a powerful representation for many applications, e.g.,
encoding a 3D shape and performing collision detection. Typically, implicit
fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding
(PE) to capture high-frequency geometric details. However, a notable side
effect of such PE-equipped MLPs is the noisy artifacts present in the learned
implicit fields. While increasing the sampling rate could in general mitigate
these artifacts, in this paper we aim to explain this adverse phenomenon
through the lens of Fourier analysis. We devise a tool to determine the
appropriate sampling rate for learning an accurate neural implicit field
without undesirable side effects. Specifically, we propose a simple yet
effective method to estimate the intrinsic frequency of a given network with
randomized weights based on the Fourier analysis of the network's responses. It
is observed that a PE-equipped MLP has an intrinsic frequency much higher than
the highest frequency component in the PE layer. Sampling against this
intrinsic frequency following the Nyquist-Sannon sampling theorem allows us to
determine an appropriate training sampling rate. We empirically show in the
setting of SDF fitting that this recommended sampling rate is sufficient to
secure accurate fitting results, while further increasing the sampling rate
would not further noticeably reduce the fitting error. Training PE-equipped
MLPs simply with our sampling strategy leads to performances superior to the
existing methods.</div><div><a href='http://arxiv.org/abs/2401.01391v1'>2401.01391v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13861v1")'>Improving Efficiency of Iso-Surface Extraction on Implicit Neural
  Representations Using Uncertainty Propagation</div>
<div id='2402.13861v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T15:10:20Z</div><div>Authors: Haoyu Li, Han-Wei Shen</div><div style='padding-top: 10px; width: 80ex'>Implicit Neural representations (INRs) are widely used for scientific data
reduction and visualization by modeling the function that maps a spatial
location to a data value. Without any prior knowledge about the spatial
distribution of values, we are forced to sample densely from INRs to perform
visualization tasks like iso-surface extraction which can be very
computationally expensive. Recently, range analysis has shown promising results
in improving the efficiency of geometric queries, such as ray casting and
hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic
rules to bound the output range of the network within a spatial region.
However, the analysis bounds are often too conservative for complex scientific
data. In this paper, we present an improved technique for range analysis by
revisiting the arithmetic rules and analyzing the probability distribution of
the network output within a spatial region. We model this distribution
efficiently as a Gaussian distribution by applying the central limit theorem.
Excluding low probability values, we are able to tighten the output bounds,
resulting in a more accurate estimation of the value range, and hence more
accurate identification of iso-surface cells and more efficient iso-surface
extraction on INRs. Our approach demonstrates superior performance in terms of
the iso-surface extraction time on four datasets compared to the original range
analysis method and can also be generalized to other geometric query tasks.</div><div><a href='http://arxiv.org/abs/2402.13861v1'>2402.13861v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04937v1")'>Gradient-free neural topology optimization</div>
<div id='2403.04937v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T23:00:49Z</div><div>Authors: Gawel Kus, Miguel A. Bessa</div><div style='padding-top: 10px; width: 80ex'>Gradient-free optimizers allow for tackling problems regardless of the
smoothness or differentiability of their objective function, but they require
many more iterations to converge when compared to gradient-based algorithms.
This has made them unviable for topology optimization due to the high
computational cost per iteration and high dimensionality of these problems. We
propose a pre-trained neural reparameterization strategy that leads to at least
one order of magnitude decrease in iteration count when optimizing the designs
in latent space, as opposed to the conventional approach without latent
reparameterization. We demonstrate this via extensive computational experiments
in- and out-of-distribution with the training data. Although gradient-based
topology optimization is still more efficient for differentiable problems, such
as compliance optimization of structures, we believe this work will open up a
new path for problems where gradient information is not readily available (e.g.
fracture).</div><div><a href='http://arxiv.org/abs/2403.04937v1'>2403.04937v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06070v1")'>Peridynamic Neural Operators: A Data-Driven Nonlocal Constitutive Model
  for Complex Material Responses</div>
<div id='2401.06070v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T17:37:20Z</div><div>Authors: Siavash Jafarzadeh, Stewart Silling, Ning Liu, Zhongqiang Zhang, Yue Yu</div><div style='padding-top: 10px; width: 80ex'>Neural operators, which can act as implicit solution operators of hidden
governing equations, have recently become popular tools for learning the
responses of complex real-world physical systems. Nevertheless, most neural
operator applications have thus far been data-driven and neglect the intrinsic
preservation of fundamental physical laws in data. In this work, we introduce a
novel integral neural operator architecture called the Peridynamic Neural
Operator (PNO) that learns a nonlocal constitutive law from data. This neural
operator provides a forward model in the form of state-based peridynamics, with
objectivity and momentum balance laws automatically guaranteed. As
applications, we demonstrate the expressivity and efficacy of our model in
learning complex material behaviors from both synthetic and experimental data
sets. We show that, owing to its ability to capture complex responses, our
learned neural operator achieves improved accuracy and efficiency compared to
baseline models that use predefined constitutive laws. Moreover, by preserving
the essential physical laws within the neural network architecture, the PNO is
robust in treating noisy data. The method shows generalizability to different
domain configurations, external loadings, and discretizations.</div><div><a href='http://arxiv.org/abs/2401.06070v1'>2401.06070v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13486v1")'>Separable Physics-Informed Neural Networks for the solution of
  elasticity problems</div>
<div id='2401.13486v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T14:34:59Z</div><div>Authors: Vasiliy A. Es'kin, Danil V. Davydov, Julia V. Gur'eva, Alexey O. Malkhanov, Mikhail E. Smorkalov</div><div style='padding-top: 10px; width: 80ex'>A method for solving elasticity problems based on separable physics-informed
neural networks (SPINN) in conjunction with the deep energy method (DEM) is
presented. Numerical experiments have been carried out for a number of problems
showing that this method has a significantly higher convergence rate and
accuracy than the vanilla physics-informed neural networks (PINN) and even
SPINN based on a system of partial differential equations (PDEs). In addition,
using the SPINN in the framework of DEM approach it is possible to solve
problems of the linear theory of elasticity on complex geometries, which is
unachievable with the help of PINNs in frames of partial differential
equations. Considered problems are very close to the industrial problems in
terms of geometry, loading, and material parameters.</div><div><a href='http://arxiv.org/abs/2401.13486v1'>2401.13486v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.14382v1")'>An Orthogonal Polynomial Kernel-Based Machine Learning Model for
  Differential-Algebraic Equations</div>
<div id='2401.14382v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T18:37:17Z</div><div>Authors: Tayebeh Taheri, Alireza Afzal Aghaei, Kourosh Parand</div><div style='padding-top: 10px; width: 80ex'>The recent introduction of the Least-Squares Support Vector Regression
(LS-SVR) algorithm for solving differential and integral equations has sparked
interest. In this study, we expand the application of this algorithm to address
systems of differential-algebraic equations (DAEs). Our work presents a novel
approach to solving general DAEs in an operator format by establishing
connections between the LS-SVR machine learning model, weighted residual
methods, and Legendre orthogonal polynomials. To assess the effectiveness of
our proposed method, we conduct simulations involving various DAE scenarios,
such as nonlinear systems, fractional-order derivatives, integro-differential,
and partial DAEs. Finally, we carry out comparisons between our proposed method
and currently established state-of-the-art approaches, demonstrating its
reliability and effectiveness.</div><div><a href='http://arxiv.org/abs/2401.14382v1'>2401.14382v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.04783v1")'>Hyperbolic Machine Learning Moment Closures for the BGK Equations</div>
<div id='2401.04783v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T19:14:57Z</div><div>Authors: Andrew J. Christlieb, Mingchang Ding, Juntao Huang, Nicholas A. Krupansky</div><div style='padding-top: 10px; width: 80ex'>We introduce a hyperbolic closure for the Grad moment expansion of the
Bhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trained
on BGK's moment data. This closure is motivated by the exact closure for the
free streaming limit that we derived in our paper on closures in transport
\cite{Huang2022-RTE1}. The exact closure relates the gradient of the highest
moment to the gradient of four lower moments. As with our past work, the model
presented here learns the gradient of the highest moment in terms of the
coefficients of gradients for all lower ones. By necessity, this means that the
resulting hyperbolic system is not conservative in the highest moment. For
stability, the output layers of the NN are designed to enforce hyperbolicity
and Galilean invariance. This ensures the model can be run outside of the
training window of the NN. Unlike our previous work on radiation transport that
dealt with linear models, the BGK model's nonlinearity demanded advanced
training tools. These comprised an optimal learning rate discovery, one cycle
training, batch normalization in each neural layer, and the use of the
\texttt{AdamW} optimizer. To address the non-conservative structure of the
hyperbolic model, we adopt the FORCE numerical method to achieve robust
solutions. This results in a comprehensive computing model combining learned
closures with methods for solving hyperbolic models. The proposed model can
capture accurate moment solutions across a broad spectrum of Knudsen numbers.
Our paper details the multi-scale model construction and is run on a range of
test problems.</div><div><a href='http://arxiv.org/abs/2401.04783v1'>2401.04783v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12188v1")'>PETScML: Second-order solvers for training regression problems in
  Scientific Machine Learning</div>
<div id='2403.12188v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T18:59:42Z</div><div>Authors: Stefano Zampini, Umberto Zerbinati, George Turkiyyah, David Keyes</div><div style='padding-top: 10px; width: 80ex'>In recent years, we have witnessed the emergence of scientific machine
learning as a data-driven tool for the analysis, by means of deep-learning
techniques, of data produced by computational science and engineering
applications. At the core of these methods is the supervised training algorithm
to learn the neural network realization, a highly non-convex optimization
problem that is usually solved using stochastic gradient methods. However,
distinct from deep-learning practice, scientific machine-learning training
problems feature a much larger volume of smooth data and better
characterizations of the empirical risk functions, which make them suited for
conventional solvers for unconstrained optimization. We introduce a lightweight
software framework built on top of the Portable and Extensible Toolkit for
Scientific computation to bridge the gap between deep-learning software and
conventional solvers for unconstrained minimization. We empirically demonstrate
the superior efficacy of a trust region method based on the Gauss-Newton
approximation of the Hessian in improving the generalization errors arising
from regression tasks when learning surrogate models for a wide range of
scientific machine-learning techniques and test cases. All the conventional
second-order solvers tested, including L-BFGS and inexact Newton with
line-search, compare favorably, either in terms of cost or accuracy, with the
adaptive first-order methods used to validate the surrogate models.</div><div><a href='http://arxiv.org/abs/2403.12188v1'>2403.12188v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13077v1")'>Mechanistic Neural Networks for Scientific Machine Learning</div>
<div id='2402.13077v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T15:23:24Z</div><div>Authors: Adeel Pervez, Francesco Locatello, Efstratios Gavves</div><div style='padding-top: 10px; width: 80ex'>This paper presents Mechanistic Neural Networks, a neural network design for
machine learning applications in the sciences. It incorporates a new
Mechanistic Block in standard architectures to explicitly learn governing
differential equations as representations, revealing the underlying dynamics of
data and enhancing interpretability and efficiency in data modeling. Central to
our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by
a technique that reduces solving linear ODEs to solving linear programs. This
integrates well with neural networks and surpasses the limitations of
traditional ODE solvers enabling scalable GPU parallel processing. Overall,
Mechanistic Neural Networks demonstrate their versatility for scientific
machine learning applications, adeptly managing tasks from equation discovery
to dynamic systems modeling. We prove their comprehensive capabilities in
analyzing and interpreting complex scientific data across various applications,
showing significant performance against specialized state-of-the-art methods.</div><div><a href='http://arxiv.org/abs/2402.13077v1'>2402.13077v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13945v1")'>Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty
  in Scientific Machine Learning</div>
<div id='2402.13945v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T17:15:47Z</div><div>Authors: Farhad Pourkamali-Anaraki, Jamal F. Husseini, Scott E. Stapleton</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the use of probabilistic neural networks (PNNs) to
model aleatoric uncertainty, which refers to the inherent variability in the
input-output relationships of a system, often characterized by unequal variance
or heteroscedasticity. Unlike traditional neural networks that produce
deterministic outputs, PNNs generate probability distributions for the target
variable, allowing the determination of both predicted means and intervals in
regression scenarios. Contributions of this paper include the development of a
probabilistic distance metric to optimize PNN architecture, and the deployment
of PNNs in controlled data sets as well as a practical material science case
involving fiber-reinforced composites. The findings confirm that PNNs
effectively model aleatoric uncertainty, proving to be more appropriate than
the commonly employed Gaussian process regression for this purpose.
Specifically, in a real-world scientific machine learning context, PNNs yield
remarkably accurate output mean estimates with R-squared scores approaching
0.97, and their predicted intervals exhibit a high correlation coefficient of
nearly 0.80, closely matching observed data intervals. Hence, this research
contributes to the ongoing exploration of leveraging the sophisticated
representational capacity of neural networks to delineate complex input-output
relationships in scientific problems.</div><div><a href='http://arxiv.org/abs/2402.13945v1'>2402.13945v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00760v1")'>EuroPED-NN: Uncertainty aware surrogate model</div>
<div id='2402.00760v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:50:41Z</div><div>Authors: A. Panera Alvarez, A. Ho, A. Jarvinen, S. Saarelma, S. Wiesen, JET Contributors</div><div style='padding-top: 10px; width: 80ex'>This work successfully generates uncertainty aware surrogate models, via the
Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of
the EuroPED plasma pedestal model using data from the JET-ILW pedestal database
and subsequent model evaluations. All this conform EuroPED-NN. The BNN-NCP
technique is proven to be a good fit for uncertainty aware surrogate models,
matching the output results as a regular neural network, providing prediction's
confidence as uncertainties, and highlighting the out of distribution (OOD)
regions using surrogate model uncertainties. This provides critical insights
into model robustness and reliability. EuroPED-NN has been physically
validated, first, analyzing electron density
$n_e\!\left(\psi_{\text{pol}}=0.94\right)$ with respect to increasing plasma
current, $I_p$, and second, validating the $\Delta-\beta_{p,ped}$ relation
associated with the EuroPED model. Affirming the robustness of the underlying
physics learned by the surrogate model.</div><div><a href='http://arxiv.org/abs/2402.00760v1'>2402.00760v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17424v1")'>Application of Neural Networks for the Reconstruction of Supernova
  Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions</div>
<div id='2401.17424v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T20:27:28Z</div><div>Authors: Sajad Abbar, Meng-Ru Wu, Zewei Xiong</div><div style='padding-top: 10px; width: 80ex'>Neutrinos can undergo fast flavor conversions (FFCs) within extremely dense
astrophysical environments such as core-collapse supernovae (CCSNe) and neutron
star mergers (NSMs). In this study, we explore FFCs in a \emph{multi-energy}
neutrino gas, revealing that when the FFC growth rate significantly exceeds
that of the vacuum Hamiltonian, all neutrinos (regardless of energy) share a
common survival probability dictated by the energy-integrated neutrino
spectrum. We then employ physics-informed neural networks (PINNs) to predict
the asymptotic outcomes of FFCs within such a multi-energy neutrino gas. These
predictions are based on the first two moments of neutrino angular
distributions for each energy bin, typically available in state-of-the-art CCSN
and NSM simulations. Our PINNs achieve errors as low as $\lesssim6\%$ and
$\lesssim 18\%$ for predicting the number of neutrinos in the electron channel
and the relative absolute error in the neutrino moments, respectively.</div><div><a href='http://arxiv.org/abs/2401.17424v1'>2401.17424v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12435v1")'>Emulating the interstellar medium chemistry with neural operators</div>
<div id='2402.12435v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:00:01Z</div><div>Authors: Lorenzo Branca, Andrea Pallottini</div><div style='padding-top: 10px; width: 80ex'>Galaxy formation and evolution critically depend on understanding the complex
photo-chemical processes that govern the evolution and thermodynamics of the
InterStellar Medium (ISM). Computationally, solving chemistry is among the most
heavy tasks in cosmological and astrophysical simulations. The evolution of
such non-equilibrium photo-chemical network relies on implicit, precise,
computationally costly, ordinary differential equations (ODE) solvers. Here, we
aim at substituting such procedural solvers with fast, pre-trained, emulators
based on neural operators. We emulate a non-equilibrium chemical network up to
H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism,
i.e. by splitting the ODE solver operator that maps the initial conditions and
time evolution into a tensor product of two neural networks. We use
$\texttt{KROME}$ to generate a training set spanning $-2\leq
\log(n/\mathrm{cm}^{-3}) \leq 3.5$, $\log(20) \leq\log(T/\mathrm{K}) \leq 5.5$,
$-6 \leq \log(n_i/n) &lt; 0$, and by adopting an incident radiation field
$\textbf{F}$ sampled in 10 energy bins with a continuity prior. We separately
train the solver for $T$ and each $n_i$ for $\simeq 4.34\,\rm GPUhrs$. Compared
with the reference solutions obtained by $\texttt{KROME}$ for single zone
models, the typical precision obtained is of order $10^{-2}$, i.e. the $10
\times$ better with a training that is $40 \times$ less costly with respect to
previous emulators which however considered only a fixed $\mathbf{F}$. The
present model achieves a speed-up of a factor of $128 \times$ with respect to
stiff ODE solvers. Our neural emulator represents a significant leap forward in
the modeling of ISM chemistry, offering a good balance of precision,
versatility, and computational efficiency.</div><div><a href='http://arxiv.org/abs/2402.12435v1'>2402.12435v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.16645v1")'>Speeding up and reducing memory usage for scientific machine learning
  via mixed precision</div>
<div id='2401.16645v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T00:37:57Z</div><div>Authors: Joel Hayford, Jacob Goldman-Wetzler, Eric Wang, Lu Lu</div><div style='padding-top: 10px; width: 80ex'>Scientific machine learning (SciML) has emerged as a versatile approach to
address complex computational science and engineering problems. Within this
field, physics-informed neural networks (PINNs) and deep operator networks
(DeepONets) stand out as the leading techniques for solving partial
differential equations by incorporating both physical equations and
experimental data. However, training PINNs and DeepONets requires significant
computational resources, including long computational times and large amounts
of memory. In search of computational efficiency, training neural networks
using half precision (float16) rather than the conventional single (float32) or
double (float64) precision has gained substantial interest, given the inherent
benefits of reduced computational time and memory consumed. However, we find
that float16 cannot be applied to SciML methods, because of gradient divergence
at the start of training, weight updates going to zero, and the inability to
converge to a local minima. To overcome these limitations, we explore mixed
precision, which is an approach that combines the float16 and float32 numerical
formats to reduce memory usage and increase computational speed. Our
experiments showcase that mixed precision training not only substantially
decreases training times and memory demands but also maintains model accuracy.
We also reinforce our empirical observations with a theoretical analysis. The
research has broad implications for SciML in various computational
applications.</div><div><a href='http://arxiv.org/abs/2401.16645v1'>2401.16645v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10642v1")'>Using Uncertainty Quantification to Characterize and Improve
  Out-of-Domain Learning for PDEs</div>
<div id='2403.10642v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T19:21:27Z</div><div>Authors: S. Chandra Mouli, Danielle C. Maddix, Shima Alizadeh, Gaurav Gupta, Andrew Stuart, Michael W. Mahoney, Yuyang Wang</div><div style='padding-top: 10px; width: 80ex'>Existing work in scientific machine learning (SciML) has shown that
data-driven learning of solution operators can provide a fast approximate
alternative to classical numerical partial differential equation (PDE) solvers.
Of these, Neural Operators (NOs) have emerged as particularly promising. We
observe that several uncertainty quantification (UQ) methods for NOs fail for
test inputs that are even moderately out-of-domain (OOD), even when the model
approximates the solution well for in-domain tasks. To address this limitation,
we show that ensembling several NOs can identify high-error regions and provide
good uncertainty estimates that are well-correlated with prediction errors.
Based on this, we propose a cost-effective alternative, DiverseNO, that mimics
the properties of the ensemble by encouraging diverse predictions from its
multiple heads in the last feed-forward layer. We then introduce
Operator-ProbConserv, a method that uses these well-calibrated UQ estimates
within the ProbConserv framework to update the model. Our empirical results
show that Operator-ProbConserv enhances OOD model performance for a variety of
challenging PDE problems and satisfies physical constraints such as
conservation laws.</div><div><a href='http://arxiv.org/abs/2403.10642v1'>2403.10642v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08383v1")'>Uncertainty Quantification for Forward and Inverse Problems of PDEs via
  Latent Global Evolution</div>
<div id='2402.08383v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T11:22:59Z</div><div>Authors: Tailin Wu, Willie Neiswanger, Hongtao Zheng, Stefano Ermon, Jure Leskovec</div><div style='padding-top: 10px; width: 80ex'>Deep learning-based surrogate models have demonstrated remarkable advantages
over classical solvers in terms of speed, often achieving speedups of 10 to
1000 times over traditional partial differential equation (PDE) solvers.
However, a significant challenge hindering their widespread adoption in both
scientific and industrial domains is the lack of understanding about their
prediction uncertainties, particularly in scenarios that involve critical
decision making. To address this limitation, we propose a method that
integrates efficient and precise uncertainty quantification into a deep
learning-based surrogate model. Our method, termed Latent Evolution of PDEs
with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based
surrogate models with robust and efficient uncertainty quantification
capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent
vectors within a latent space to evolve both the system's state and its
corresponding uncertainty estimation. The latent vectors are decoded to provide
predictions for the system's state as well as estimates of its uncertainty. In
extensive experiments, we demonstrate the accurate uncertainty quantification
performance of our approach, surpassing that of strong baselines including deep
ensembles, Bayesian neural network layers, and dropout. Our method excels at
propagating uncertainty over extended auto-regressive rollouts, making it
suitable for scenarios involving long-term predictions. Our code is available
at: https://github.com/AI4Science-WestlakeU/le-pde-uq.</div><div><a href='http://arxiv.org/abs/2402.08383v1'>2402.08383v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13858v1")'>A conditional latent autoregressive recurrent model for generation and
  forecasting of beam dynamics in particle accelerators</div>
<div id='2403.13858v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T22:05:17Z</div><div>Authors: Mahindra Rautela, Alan Williams, Alexander Scheinker</div><div style='padding-top: 10px; width: 80ex'>Particle accelerators are complex systems that focus, guide, and accelerate
intense charged particle beams to high energy. Beam diagnostics present a
challenging problem due to limited non-destructive measurements,
computationally demanding simulations, and inherent uncertainties in the
system. We propose a two-step unsupervised deep learning framework named as
Conditional Latent Autoregressive Recurrent Model (CLARM) for learning the
spatiotemporal dynamics of charged particles in accelerators. CLARM consists of
a Conditional Variational Autoencoder (CVAE) transforming six-dimensional phase
space into a lower-dimensional latent distribution and a Long Short-Term Memory
(LSTM) network capturing temporal dynamics in an autoregressive manner. The
CLARM can generate projections at various accelerator modules by sampling and
decoding the latent space representation. The model also forecasts future
states (downstream locations) of charged particles from past states (upstream
locations). The results demonstrate that the generative and forecasting ability
of the proposed approach is promising when tested against a variety of
evaluation metrics.</div><div><a href='http://arxiv.org/abs/2403.13858v1'>2403.13858v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15115v1")'>Physics-constrained polynomial chaos expansion for scientific machine
  learning and uncertainty quantification</div>
<div id='2402.15115v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T06:04:15Z</div><div>Authors: Himanshu Sharma, Lukáš Novák, Michael D. Shields</div><div style='padding-top: 10px; width: 80ex'>We present a novel physics-constrained polynomial chaos expansion as a
surrogate modeling method capable of performing both scientific machine
learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method
possesses a unique capability: it seamlessly integrates SciML into UQ and vice
versa, which allows it to quantify the uncertainties in SciML tasks effectively
and leverage SciML for improved uncertainty assessment during UQ-related tasks.
The proposed surrogate model can effectively incorporate a variety of physical
constraints, such as governing partial differential equations (PDEs) with
associated initial and boundary conditions constraints, inequality-type
constraints (e.g., monotonicity, convexity, non-negativity, among others), and
additional a priori information in the training process to supplement limited
data. This ensures physically realistic predictions and significantly reduces
the need for expensive computational model evaluations to train the surrogate
model. Furthermore, the proposed method has a built-in uncertainty
quantification (UQ) feature to efficiently estimate output uncertainties. To
demonstrate the effectiveness of the proposed method, we apply it to a diverse
set of problems, including linear/non-linear PDEs with deterministic and
stochastic parameters, data-driven surrogate modeling of a complex physical
system, and UQ of a stochastic system with parameters modeled as random fields.</div><div><a href='http://arxiv.org/abs/2402.15115v1'>2402.15115v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10748v1")'>A Comprehensive Review of Latent Space Dynamics Identification
  Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling</div>
<div id='2403.10748v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T00:45:06Z</div><div>Authors: Christophe Bonneville, Xiaolong He, April Tran, Jun Sur Park, William Fries, Daniel A. Messenger, Siu Wun Cheung, Yeonjong Shin, David M. Bortz, Debojyoti Ghosh, Jiun-Shyan Chen, Jonathan Belof, Youngsoo Choi</div><div style='padding-top: 10px; width: 80ex'>Numerical solvers of partial differential equations (PDEs) have been widely
employed for simulating physical systems. However, the computational cost
remains a major bottleneck in various scientific and engineering applications,
which has motivated the development of reduced-order models (ROMs). Recently,
machine-learning-based ROMs have gained significant popularity and are
promising for addressing some limitations of traditional ROM methods,
especially for advection dominated systems. In this chapter, we focus on a
particular framework known as Latent Space Dynamics Identification (LaSDI),
which transforms the high-fidelity data, governed by a PDE, to simpler and
low-dimensional latent-space data, governed by ordinary differential equations
(ODEs). These ODEs can be learned and subsequently interpolated to make ROM
predictions. Each building block of LaSDI can be easily modulated depending on
the application, which makes the LaSDI framework highly flexible. In
particular, we present strategies to enforce the laws of thermodynamics into
LaSDI models (tLaSDI), enhance robustness in the presence of noise through the
weak form (WLaSDI), select high-fidelity training data efficiently through
active learning (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty
through Gaussian processes (GPLaSDI). We demonstrate the performance of
different LaSDI approaches on Burgers equation, a non-linear heat conduction
problem, and a plasma physics problem, showing that LaSDI algorithms can
achieve relative errors of less than a few percent and up to thousands of times
speed-ups.</div><div><a href='http://arxiv.org/abs/2403.10748v1'>2403.10748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02889v2")'>Energy-Preserving Reduced Operator Inference for Efficient Design and
  Control</div>
<div id='2401.02889v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T16:39:48Z</div><div>Authors: Tomoki Koike, Elizabeth Qian</div><div style='padding-top: 10px; width: 80ex'>Many-query computations, in which a computational model for an engineering
system must be evaluated many times, are crucial in design and control. For
systems governed by partial differential equations (PDEs), typical
high-fidelity numerical models are high-dimensional and too computationally
expensive for the many-query setting. Thus, efficient surrogate models are
required to enable low-cost computations in design and control. This work
presents a physics-preserving reduced model learning approach that targets PDEs
whose quadratic operators preserve energy, such as those arising in governing
equations in many fluids problems. The approach is based on the Operator
Inference method, which fits reduced model operators to state snapshot and time
derivative data in a least-squares sense. However, Operator Inference does not
generally learn a reduced quadratic operator with the energy-preserving
property of the original PDE. Thus, we propose a new energy-preserving Operator
Inference (EP-OpInf) approach, which imposes this structure on the learned
reduced model via constrained optimization. Numerical results using the viscous
Burgers' and Kuramoto-Sivashinksy equation (KSE) demonstrate that EP-OpInf
learns efficient and accurate reduced models that retain this energy-preserving
structure.</div><div><a href='http://arxiv.org/abs/2401.02889v2'>2401.02889v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17061v1")'>A Multi-Fidelity Methodology for Reduced Order Models with
  High-Dimensional Inputs</div>
<div id='2402.17061v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T22:47:03Z</div><div>Authors: Bilal Mufti, Christian Perron, Dimitri N. Mavris</div><div style='padding-top: 10px; width: 80ex'>In the early stages of aerospace design, reduced order models (ROMs) are
crucial for minimizing computational costs associated with using physics-rich
field information in many-query scenarios requiring multiple evaluations. The
intricacy of aerospace design demands the use of high-dimensional design spaces
to capture detailed features and design variability accurately. However, these
spaces introduce significant challenges, including the curse of dimensionality,
which stems from both high-dimensional inputs and outputs necessitating
substantial training data and computational effort. To address these
complexities, this study introduces a novel multi-fidelity, parametric, and
non-intrusive ROM framework designed for high-dimensional contexts. It
integrates machine learning techniques for manifold alignment and dimension
reduction employing Proper Orthogonal Decomposition (POD) and Model-based
Active Subspace with multi-fidelity regression for ROM construction. Our
approach is validated through two test cases: the 2D RAE~2822 airfoil and the
3D NASA CRM wing, assessing combinations of various fidelity levels, training
data ratios, and sample sizes. Compared to the single-fidelity PCAS method, our
multi-fidelity solution offers improved cost-accuracy benefits and achieves
better predictive accuracy with reduced computational demands. Moreover, our
methodology outperforms the manifold-aligned ROM (MA-ROM) method by 50% in
handling scenarios with large input dimensions, underscoring its efficacy in
addressing the complex challenges of aerospace design.</div><div><a href='http://arxiv.org/abs/2402.17061v1'>2402.17061v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17177v1")'>Data-Driven Discovery of PDEs via the Adjoint Method</div>
<div id='2401.17177v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:10:42Z</div><div>Authors: Mohsen Sadr, Tony Tohme, Kamal Youcef-Toumi</div><div style='padding-top: 10px; width: 80ex'>In this work, we present an adjoint-based method for discovering the
underlying governing partial differential equations (PDEs) given data. The idea
is to consider a parameterized PDE in a general form, and formulate the
optimization problem that minimizes the error of PDE solution from data. Using
variational calculus, we obtain an evolution equation for the Lagrange
multipliers (adjoint equations) allowing us to compute the gradient of the
objective function with respect to the parameters of PDEs given data in a
straightforward manner. In particular, for a family of parameterized and
nonlinear PDEs, we show how the corresponding adjoint equations can be derived.
Here, we show that given smooth data set, the proposed adjoint method can
recover the true PDE up to machine accuracy. However, in the presence of noise,
the accuracy of the adjoint method becomes comparable to the famous PDE
Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy
et al., 2017). Even though the presented adjoint method relies on
forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to
the analytic expressions for gradients of the cost function with respect to
each PDE parameter.</div><div><a href='http://arxiv.org/abs/2401.17177v1'>2401.17177v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17739v1")'>Operator learning without the adjoint</div>
<div id='2401.17739v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T10:59:57Z</div><div>Authors: Nicolas Boullé, Diana Halikias, Samuel E. Otto, Alex Townsend</div><div style='padding-top: 10px; width: 80ex'>There is a mystery at the heart of operator learning: how can one recover a
non-self-adjoint operator from data without probing the adjoint? Current
practical approaches suggest that one can accurately recover an operator while
only using data generated by the forward action of the operator without access
to the adjoint. However, naively, it seems essential to sample the action of
the adjoint. In this paper, we partially explain this mystery by proving that
without querying the adjoint, one can approximate a family of non-self-adjoint
infinite-dimensional compact operators via projection onto a Fourier basis. We
then apply the result to recovering Green's functions of elliptic partial
differential operators and derive an adjoint-free sample complexity bound.
While existing theory justifies low sample complexity in operator learning,
ours is the first adjoint-free analysis that attempts to close the gap between
theory and practice.</div><div><a href='http://arxiv.org/abs/2401.17739v1'>2401.17739v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.03233v1")'>From Displacements to Distributions: A Machine-Learning Enabled
  Framework for Quantifying Uncertainties in Parameters of Computational Models</div>
<div id='2403.03233v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T20:40:50Z</div><div>Authors: Taylor Roper, Harri Hakula, Troy Butler</div><div style='padding-top: 10px; width: 80ex'>This work presents novel extensions for combining two frameworks for
quantifying both aleatoric (i.e., irreducible) and epistemic (i.e., reducible)
sources of uncertainties in the modeling of engineered systems. The
data-consistent (DC) framework poses an inverse problem and solution for
quantifying aleatoric uncertainties in terms of pullback and push-forward
measures for a given Quantity of Interest (QoI) map. Unfortunately, a
pre-specified QoI map is not always available a priori to the collection of
data associated with system outputs. The data themselves are often polluted
with measurement errors (i.e., epistemic uncertainties), which complicates the
process of specifying a useful QoI. The Learning Uncertain Quantities (LUQ)
framework defines a formal three-step machine-learning enabled process for
transforming noisy datasets into samples of a learned QoI map to enable
DC-based inversion. We develop a robust filtering step in LUQ that can learn
the most useful quantitative information present in spatio-temporal datasets.
The learned QoI map transforms simulated and observed datasets into
distributions to perform DC-based inversion. We also develop a DC-based
inversion scheme that iterates over time as new spatial datasets are obtained
and utilizes quantitative diagnostics to identify both the quality and impact
of inversion at each iteration. Reproducing Kernel Hilbert Space theory is
leveraged to mathematically analyze the learned QoI map and develop a
quantitative sufficiency test for evaluating the filtered data. An illustrative
example is utilized throughout while the final two examples involve the
manufacturing of shells of revolution to demonstrate various aspects of the
presented frameworks.</div><div><a href='http://arxiv.org/abs/2403.03233v1'>2403.03233v1</a></div>
</div></div>
    <div><a href="arxiv_1.html">Prev (1)</a></div>
    <div><a href="arxiv_3.html">Next (3)</a></div>
    