
<!doctype html>
<meta charset="utf-8">
<style>
body { margin: 20px; }
</style>
<script>
function toggle(arxiv) {
  let elt = document.getElementById(arxiv);
  console.log(elt, elt.style.display);
  if(elt.style.display == "block") {
    elt.style.display = "none";
  } else {
    elt.style.display = "block";
  }
}
</script>
<div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04900v1")'>SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation</div>
<div id='2401.04900v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-10T03:03:12Z</div><div>Authors: Mengmeng Zhang, Fan Wu, Yude Bu, Shanshan Li, Zhenping Yi, Meng Liu, Xiaoming Kong</div><div style='padding-top: 10px; width: 80ex'>The age and mass of red giants are essential for understanding the structure
and evolution of the Milky Way. Traditional isochrone methods for these
estimations are inherently limited due to overlapping isochrones in the
Hertzsprung-Russell diagram, while asteroseismology, though more precise,
requires high-precision, long-term observations. In response to these
challenges, we developed a novel framework, Spectral Transformer (SPT), to
predict the age and mass of red giants aligned with asteroseismology from their
spectra. A key component of SPT, the Multi-head Hadamard Self-Attention
mechanism, designed specifically for spectra, can capture complex relationships
across different wavelength. Further, we introduced a Mahalanobis
distance-based loss function to address scale imbalance and interaction mode
loss, and incorporated Monte Carlo dropout for quantitative analysis of
prediction uncertainty.Trained and tested on 3,880 red giant spectra from
LAMOST, the SPT achieved remarkable age and mass estimations with average
percentage errors of 17.64% and 6.61%, respectively, and provided uncertainties
for each corresponding prediction. The results significantly outperform those
of traditional machine learning algorithms and demonstrate a high level of
consistency with asteroseismology methods and isochrone fitting techniques. In
the future, our work will leverage datasets from the Chinese Space Station
Telescope and the Large Synoptic Survey Telescope to enhance the precision of
the model and broaden its applicability in the field of astronomy and
astrophysics.</div><div><a href='http://arxiv.org/abs/2401.04900v1'>2401.04900v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17898v1")'>Exoplanets Prediction in Multi-Planetary Systems and Determining the
  Correlation Between the Parameters of Planets and Host Stars Using Artificial
  Intelligence</div>
<div id='2402.17898v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T21:28:08Z</div><div>Authors: Mahdiyar Mousavi-Sadr</div><div style='padding-top: 10px; width: 80ex'>The number of extrasolar planets discovered is increasing, so that more than
five thousand exoplanets have been confirmed to date. Now we have an
opportunity to test the validity of the laws governing planetary systems and
take steps to discover the relationships between the physical parameters of
planets and stars. Firstly, we present the results of a search for additional
exoplanets in 229 multi-planetary systems that house at least three or more
confirmed planets, employing a logarithmic spacing between planets in our Solar
System known as the Titius-Bode (TB) relation. We find that the planets in
$\sim53\%$ of these systems adhere to a logarithmic spacing relation remarkably
better than the Solar System planets. We predict the presence of 426 additional
exoplanets, 47 of which are located within the habitable zone (HZ), and five of
the 47 planets have a maximum mass limit of 0.1-2$M_{\oplus}$ and a maximum
radius lower than 1.25$R_{\oplus}$. Secondly, we employ efficient machine
learning approaches to analyze a dataset comprising 762 confirmed exoplanets
and eight Solar System planets, aiming to characterize their fundamental
quantities. We classify the data into two main classes: 'small' and 'giant'
planets, with cut-off values at $R_{p}=8.13R_{\oplus}$ and
$M_{p}=52.48M_{\oplus}$. Giant planets have lower densities, suggesting higher
H-He mass fractions, while small planets are denser, composed mainly of heavier
elements. We highlight that planetary mass, orbital period, and stellar mass
play crucial roles in predicting exoplanet radius. Notably, our study reveals a
noteworthy result: for giant planets, we observe a strong correlation between
planetary radius and the mass of their host stars, which might provide
intriguing insights into the relationship between giant planet formation and
stellar characteristics.</div><div><a href='http://arxiv.org/abs/2402.17898v1'>2402.17898v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06839v1")'>Inferring Stellar Parameters from Iodine-Imprinted Keck/HIRES Spectra
  with Machine Learning</div>
<div id='2401.06839v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T19:00:00Z</div><div>Authors: Jude Gussman, Malena Rice</div><div style='padding-top: 10px; width: 80ex'>The properties of exoplanet host stars are traditionally characterized
through a detailed forward-modeling analysis of high-resolution spectra.
However, many exoplanet radial velocity surveys employ iodine-cell-calibrated
spectrographs, such that the vast majority of spectra obtained include an
imprinted forest of iodine absorption lines. For surveys that use iodine cells,
iodine-free "template" spectra must be separately obtained for precise stellar
characterization. These template spectra often require extensive additional
observing time to obtain, and they are not always feasible to obtain for faint
stars. In this paper, we demonstrate that machine learning methods can be
applied to infer stellar parameters and chemical abundances from
iodine-imprinted spectra with high accuracy and precision. The methods
presented in this work are broadly applicable to any iodine-cell-calibrated
spectrograph. We make publicly available our spectroscopic pipeline, the Cannon
HIRES Iodine Pipeline (CHIP), which derives stellar parameters and 15 chemical
abundances from iodine-imprinted spectra of FGK stars and which has been set up
for ease of use with Keck/HIRES spectra. Our proof-of-concept offers an
efficient new avenue to rapidly estimate a large number of stellar parameters
even in the absence of an iodine-free template spectrum.</div><div><a href='http://arxiv.org/abs/2401.06839v1'>2401.06839v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14235v1")'>RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU
  Pilot Survey</div>
<div id='2403.14235v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T08:52:39Z</div><div>Authors: Nikhel Gupta, Ray P. Norris, Zeeshan Hayder, Minh Huynh, Lars Petersson, X. Rosalind Wang, Andrew M. Hopkins, Heinz Andernach, Yjan Gordon, Simone Riggi, Miranda Yew, Evan J. Crawford, Bärbel Koribalski, Miroslav D. Filipović, Anna D. Kapinśka, Stanislav Shabala, Tessa Vernstrom, Joshua R. Marvil</div><div style='padding-top: 10px; width: 80ex'>We present source detection and catalogue construction pipelines to build the
first catalogue of radio galaxies from the 270 $\rm deg^2$ pilot survey of the
Evolutionary Map of the Universe (EMU-PS) conducted with the Australian Square
Kilometre Array Pathfinder (ASKAP) telescope. The detection pipeline uses
Gal-DINO computer-vision networks (Gupta et al., 2024) to predict the
categories of radio morphology and bounding boxes for radio sources, as well as
their potential infrared host positions. The Gal-DINO network is trained and
evaluated on approximately 5,000 visually inspected radio galaxies and their
infrared hosts, encompassing both compact and extended radio morphologies. We
find that the Intersection over Union (IoU) for the predicted and ground truth
bounding boxes is larger than 0.5 for 99% of the radio sources, and 98% of
predicted host positions are within $3^{\prime \prime}$ of the ground truth
infrared host in the evaluation set. The catalogue construction pipeline uses
the predictions of the trained network on the radio and infrared image cutouts
based on the catalogue of radio components identified using the Selavy source
finder algorithm. Confidence scores of the predictions are then used to
prioritize Selavy components with higher scores and incorporate them first into
the catalogue. This results in identifications for a total of 211,625 radio
sources, with 201,211 classified as compact and unresolved. The remaining
10,414 are categorized as extended radio morphologies, including 582 FR-I,
5,602 FR-II, 1,494 FR-x (uncertain whether FR-I or FR-II), 2,375 R (single-peak
resolved) radio galaxies, and 361 with peculiar and other rare morphologies. We
cross-match the radio sources in the catalogue with the infrared and optical
catalogues, finding infrared cross-matches for 73% and photometric redshifts
for 36% of the radio galaxies.</div><div><a href='http://arxiv.org/abs/2403.14235v1'>2403.14235v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15232v1")'>Classification of compact radio sources in the Galactic plane with
  supervised machine learning</div>
<div id='2402.15232v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T09:47:42Z</div><div>Authors: S. Riggi, G. Umana, C. Trigilio, C. Bordiu, F. Bufano, A. Ingallinera, F. Cavallaro, Y. Gordon, R. P. Norris, G. Gürkan, P. Leto, C. Buemi, S. Loru, A. M. Hopkins, M. D. Filipović, T. Cecconello</div><div style='padding-top: 10px; width: 80ex'>Generation of science-ready data from processed data products is one of the
major challenges in next-generation radio continuum surveys with the Square
Kilometre Array (SKA) and its precursors, due to the expected data volume and
the need to achieve a high degree of automated processing. Source extraction,
characterization, and classification are the major stages involved in this
process. In this work we focus on the classification of compact radio sources
in the Galactic plane using both radio and infrared images as inputs. To this
aim, we produced a curated dataset of ~20,000 images of compact sources of
different astronomical classes, obtained from past radio and infrared surveys,
and novel radio data from pilot surveys carried out with the Australian SKA
Pathfinder (ASKAP). Radio spectral index information was also obtained for a
subset of the data. We then trained two different classifiers on the produced
dataset. The first model uses gradient-boosted decision trees and is trained on
a set of pre-computed features derived from the data, which include
radio-infrared colour indices and the radio spectral index. The second model is
trained directly on multi-channel images, employing convolutional neural
networks. Using a completely supervised procedure, we obtained a high
classification accuracy (F1-score&gt;90%) for separating Galactic objects from the
extragalactic background. Individual class discrimination performances, ranging
from 60% to 75%, increased by 10% when adding far-infrared and spectral index
information, with extragalactic objects, PNe and HII regions identified with
higher accuracies. The implemented tools and trained models were publicly
released, and made available to the radioastronomical community for future
application on new radio data.</div><div><a href='http://arxiv.org/abs/2402.15232v1'>2402.15232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.12120v1")'>Light Curve Classification with DistClassiPy: a new distance-based
  classifier</div>
<div id='2403.12120v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T18:00:00Z</div><div>Authors: Siddharth Chaini, Ashish Mahabal, Ajit Kembhavi, Federica B. Bianco</div><div style='padding-top: 10px; width: 80ex'>The rise of synoptic sky surveys has ushered in an era of big data in
time-domain astronomy, making data science and machine learning essential tools
for studying celestial objects. Tree-based (e.g. Random Forests) and deep
learning models represent the current standard in the field. We explore the use
of different distance metrics to aid in the classification of objects. For
this, we developed a new distance metric based classifier called DistClassiPy.
The direct use of distance metrics is an approach that has not been explored in
time-domain astronomy, but distance-based methods can aid in increasing the
interpretability of the classification result and decrease the computational
costs. In particular, we classify light curves of variable stars by comparing
the distances between objects of different classes. Using 18 distance metrics
applied to a catalog of 6,000 variable stars in 10 classes, we demonstrate
classification and dimensionality reduction. We show that this classifier meets
state-of-the-art performance but has lower computational requirements and
improved interpretability. We have made DistClassiPy open-source and accessible
at https://pypi.org/project/distclassipy/ with the goal of broadening its
applications to other classification scenarios within and beyond astronomy.</div><div><a href='http://arxiv.org/abs/2403.12120v1'>2403.12120v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12448v1")'>DBNets: A publicly available deep learning tool to measure the masses of
  young planets in dusty protoplanetary discs</div>
<div id='2402.12448v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T19:00:09Z</div><div>Authors: Alessandro Ruzza, Giuseppe Lodato, Giovanni Pietro Rosotti</div><div style='padding-top: 10px; width: 80ex'>Current methods to characterize embedded planets in protoplanetary disc
observations are severely limited either in their ability to fully account for
the observed complex physics or in their computational and time costs. To
address this shortcoming, we developed DBNets: a deep learning tool, based on
convolutional neural networks, that analyses substructures observed in the dust
continuum emission of protoplanetary discs to quickly infer the mass of
allegedly embedded planets. We focussed on developing a method to reliably
quantify not only the planet mass, but also the associated uncertainty
introduced by our modelling and adopted techniques. Our tests gave promising
results achieving an 87% reduction of the log Mp mean squared error with
respect to an analytical formula fitted on the same data (DBNets metrics: lmse
0.016, r2-score 97%). With the goal of providing the final user of DBNets with
all the tools needed to interpret their measurements and decide on their
significance, we extensively tested our tool on out-of-distribution data. We
found that DBNets can identify inputs strongly outside its training scope
returning an uncertainty above a specific threshold and we thus provided a
rejection criterion that helps determine the significance of the results
obtained. Additionally, we outlined some limitations of our tool: it can be
reliably applied only on discs observed with inclinations below approximately
60{\deg}, in the optically thin regime, with a resolution 8 times better than
the gap radial location and with a signal-to-noise ratio higher than
approximately ten. Finally, we applied DBNets to 33 actual observations of
protoplanetary discs measuring the mass of 48 proposed planets and comparing
our results with the available literature. We confirmed that most of the
observed gaps imply planets in the sub-Jupiter regime. DBNets is publicly
available at dbnets.fisica.unimi.it.</div><div><a href='http://arxiv.org/abs/2402.12448v1'>2402.12448v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12369v1")'>Short-Period Variables in TESS Full-Frame Image Light Curves Identified
  via Convolutional Neural Networks</div>
<div id='2402.12369v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:56:35Z</div><div>Authors: Greg Olmschenk, Richard K. Barry, Stela Ishitani Silva, Brian P. Powell, Ethan Kruse, Jeremy D. Schnittman, Agnieszka M. Cieplak, Thomas Barclay, Siddhant Solanki, Bianca Ortega, John Baker, Yesenia Helem Salinas Mamani</div><div style='padding-top: 10px; width: 80ex'>The Transiting Exoplanet Survey Satellite (TESS) mission measured light from
stars in ~85% of the sky throughout its two-year primary mission, resulting in
millions of TESS 30-minute cadence light curves to analyze in the search for
transiting exoplanets. To search this vast dataset, we aim to provide an
approach that is both computationally efficient, produces highly performant
predictions, and minimizes the required human search effort. We present a
convolutional neural network that we train to identify short period variables.
To make a prediction for a given light curve, our network requires no prior
target parameters identified using other methods. Our network performs
inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU,
enabling large scale archival searches. We present a collection of 14156
short-period variables identified by our network. The majority of our
identified variables fall into two prominent populations, one of short-period
main sequence binaries and another of Delta Scuti stars. Our neural network
model and related code is additionally provided as open-source code for public
use and extension.</div><div><a href='http://arxiv.org/abs/2402.12369v1'>2402.12369v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13673v1")'>Computing Transiting Exoplanet Parameters with 1D Convolutional Neural
  Networks</div>
<div id='2402.13673v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T10:17:23Z</div><div>Authors: Santiago Iglesias Álvarez, Enrique Díez Alonso, María Luisa Sánchez Rodríguez, Javier Rodríguez Rodríguez, Saúl Pérez Fernández, Francisco Javier de Cos Juez</div><div style='padding-top: 10px; width: 80ex'>The transit method allows the detection and characterization of planetary
systems by analyzing stellar light curves. Convolutional neural networks appear
to offer a viable solution for automating these analyses. In this research, two
1D convolutional neural network models, which work with simulated light curves
in which transit-like signals were injected, are presented. One model operates
on complete light curves and estimates the orbital period, and the other one
operates on phase-folded light curves and estimates the semimajor axis of the
orbit and the square of the planet-to-star radius ratio. Both models were
tested on real data from TESS light curves with confirmed planets to ensure
that they are able to work with real data. The results obtained show that 1D
CNNs are able to characterize transiting exoplanets from their host star's
detrended light curve and, furthermore, reducing both the required time and
computational costs compared with the current detection and characterization
algorithms.</div><div><a href='http://arxiv.org/abs/2402.13673v1'>2402.13673v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03427v1")'>Single Transit Detection In Kepler With Machine Learning And Onboard
  Spacecraft Diagnostics</div>
<div id='2403.03427v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T03:16:47Z</div><div>Authors: Matthew T. Hansen, Jason A. Dittmann</div><div style='padding-top: 10px; width: 80ex'>Exoplanet discovery at long orbital periods requires reliably detecting
individual transits without additional information about the system. Techniques
like phase-folding of light curves and periodogram analysis of radial velocity
data are more sensitive to planets with shorter orbital periods, leaving a
dearth of planet discoveries at long periods. We present a novel technique
using an ensemble of Convolutional Neural Networks incorporating the onboard
spacecraft diagnostics of \emph{Kepler} to classify transits within a light
curve. We create a pipeline to recover the location of individual transits, and
the period of the orbiting planet, which maintains $&gt;80\%$ transit recovery
sensitivity out to an 800-day orbital period. Our neural network pipeline has
the potential to discover additional planets in the \emph{Kepler} dataset, and
crucially, within the $\eta$-Earth regime. We report our first candidate from
this pipeline, KOI 1271.02. KOI 1271.01 is known to exhibit strong Transit
Timing Variations (TTVs), and so we jointly model the TTVs and transits of both
transiting planets to constrain the orbital configuration and planetary
parameters and conclude with a series of potential parameters for KOI 1271.02,
as there is not enough data currently to uniquely constrain the system. We
conclude that KOI 1271.02 has a radius of 5.32 $\pm$ 0.20 $R_{\oplus}$ and a
mass of $28.94^{0.23}_{-0.47}$ $M_{\oplus}$. Future constraints on the nature
of KOI 1271.02 require measuring additional TTVs of KOI 1271.01 or observing a
second transit of KOI 1271.02.</div><div><a href='http://arxiv.org/abs/2403.03427v1'>2403.03427v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07507v1")'>Reconstructions of Jupiter's magnetic field using physics informed
  neural networks</div>
<div id='2403.07507v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T10:43:52Z</div><div>Authors: Philip W. Livermore, Leyuan Wu, Longwei Chen, Sjoerd A. L. de Ridder</div><div style='padding-top: 10px; width: 80ex'>Magnetic sounding using data collected from the Juno mission can be used to
provide constraints on Jupiter's interior. However, inwards continuation of
reconstructions assuming zero electrical conductivity and a representation in
spherical harmonics are limited by the enhancement of noise at small scales. In
this paper we describe new reconstructions of Jupiter's internal magnetic field
based on physics-informed neural networks and either the first 33 (PINN33) or
the first 50 (PINN50) of Juno's orbits. The method can resolve local
structures, and allows for weak ambient electrical currents. Compared with
other methods, our reconstructions of Jupiter's magnetic field both on and
above the surface are similar, and we achieve a similar fit to the Juno data.
However, our models are not hampered by noise at depth, and so offer a much
clearer picture of the interior structure. We estimate that the dynamo boundary
is at a fractional radius of 0.8. At this depth, the magnetic field is arranged
into longitudinal bands, and the great blue spot appears to be rooted in
neighbouring structures of oppositely signed flux.</div><div><a href='http://arxiv.org/abs/2403.07507v1'>2403.07507v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07492v1")'>Convolutional Neural Networks for signal detection in real LIGO data</div>
<div id='2402.07492v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T09:00:27Z</div><div>Authors: Ondřej Zelenka, Bernd Brügmann, Frank Ohme</div><div style='padding-top: 10px; width: 80ex'>Searching the data of gravitational-wave detectors for signals from compact
binary mergers is a computationally demanding task. Recently, machine learning
algorithms have been proposed to address current and future challenges.
However, the results of these publications often differ greatly due to
differing choices in the evaluation procedure. The Machine Learning
Gravitational-Wave Search Challenge was organized to resolve these issues and
produce a unified framework for machine-learning search evaluation. Six teams
submitted contributions, four of which are based on machine learning methods
and two are state-of-the-art production analyses. This paper describes the
submission from the team TPI FSU Jena and its updated variant. We also apply
our algorithm to real O3b data and recover the relevant events of the GWTC-3
catalog.</div><div><a href='http://arxiv.org/abs/2402.07492v1'>2402.07492v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15632v1")'>Deep Learning for Gamma-Ray Bursts: A data driven event framework for
  X/Gamma-Ray analysis in space telescopes</div>
<div id='2401.15632v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-28T11:49:57Z</div><div>Authors: Riccardo Crupi</div><div style='padding-top: 10px; width: 80ex'>This thesis comprises the first three chapters dedicated to providing an
overview of Gamma Ray-Bursts (GRBs), their properties, the instrumentation used
to detect them, and Artificial Intelligence (AI) applications in the context of
GRBs, including a literature review and future prospects. Considering both the
current and the next generation of high X-ray monitors, such as Fermi-GBM and
HERMES Pathfinder (an in-orbit demonstration of six 3U nano-satellites), the
research question revolves around the detection of long and faint high-energy
transients, potentially GRBs, that might have been missed by previous detection
algorithms. To address this, two chapters introduce a new data-driven
framework, DeepGRB.
  In Chapter 4, a Neural Network (NN) is described for background count rate
estimation for X/gamma-ray detectors, providing a performance evaluation in
different periods, including both solar maxima, solar minima periods, and one
containing an ultra-long GRB. The application of eXplainable Artificial
Intelligence (XAI) is performed for global and local feature importance
analysis to better understand the behavior of the NN.
  Chapter 5 employs FOCuS-Poisson for anomaly detection in count rate
observations and estimation from the NN. DeepGRB demonstrates its capability to
process Fermi-GBM data, confirming cataloged events and identifying new ones,
providing further analysis with estimates for localization, duration, and
classification. The chapter concludes with an automated classification method
using Machine Learning techniques that incorporates XAI for eventual bias
identification.</div><div><a href='http://arxiv.org/abs/2401.15632v1'>2401.15632v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03336v1")'>A deep learning framework for jointly extracting spectra and
  source-count distributions in astronomy</div>
<div id='2401.03336v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T23:45:16Z</div><div>Authors: Florian Wolf, Florian List, Nicholas L. Rodd, Oliver Hahn</div><div style='padding-top: 10px; width: 80ex'>Astronomical observations typically provide three-dimensional maps, encoding
the distribution of the observed flux in (1) the two angles of the celestial
sphere and (2) energy/frequency. An important task regarding such maps is to
statistically characterize populations of point sources too dim to be
individually detected. As the properties of a single dim source will be poorly
constrained, instead one commonly studies the population as a whole, inferring
a source-count distribution (SCD) that describes the number density of sources
as a function of their brightness. Statistical and machine learning methods for
recovering SCDs exist; however, they typically entirely neglect spectral
information associated with the energy distribution of the flux. We present a
deep learning framework able to jointly reconstruct the spectra of different
emission components and the SCD of point-source populations. In a
proof-of-concept example, we show that our method accurately extracts even
complex-shaped spectra and SCDs from simulated maps.</div><div><a href='http://arxiv.org/abs/2401.03336v1'>2401.03336v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08763v1")'>The weird and the wonderful in our Solar System: Searching for
  serendipity in the Legacy Survey of Space and Time</div>
<div id='2401.08763v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T19:00:03Z</div><div>Authors: Brian Rogers, Chris J. Lintott, Steve Croft, Megan E. Schwamb, James R. A. Davenport</div><div style='padding-top: 10px; width: 80ex'>We present a novel method for anomaly detection in Solar System object data,
in preparation for the Legacy Survey of Space and Time. We train a deep
autoencoder for anomaly detection and use the learned latent space to search
for other interesting objects. We demonstrate the efficacy of the autoencoder
approach by finding interesting examples, such as interstellar objects, and
show that using the autoencoder, further examples of interesting classes can be
found. We also investigate the limits of classic unsupervised approaches to
anomaly detection through the generation of synthetic anomalies and evaluate
the feasibility of using a supervised learning approach. Future work should
consider expanding the feature space to increase the variety of anomalies that
can be uncovered during the survey using an autoencoder.</div><div><a href='http://arxiv.org/abs/2401.08763v1'>2401.08763v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17029v1")'>LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning
  Approaches and Exploring its Applications</div>
<div id='2401.17029v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T14:06:09Z</div><div>Authors: Rahul Shah, Soumadeep Saha, Purba Mukherjee, Utpal Garain, Supratik Pal</div><div style='padding-top: 10px; width: 80ex'>We investigate the prospect of reconstructing the ``cosmic distance ladder''
of the Universe using a novel deep learning framework called LADDER - Learning
Algorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on
the apparent magnitude data from the Pantheon Type Ia supernovae compilation,
incorporating the full covariance information among data points, to produce
predictions along with corresponding errors. After employing several validation
tests with a number of deep learning models, we pick LADDER as the best
performing one. We then demonstrate applications of our method in the
cosmological context, that include serving as a model-independent tool for
consistency checks for other datasets like baryon acoustic oscillations,
calibration of high-redshift datasets such as gamma ray bursts, use as a
model-independent mock catalog generator for future probes, etc. Our analysis
advocates for interesting yet cautious consideration of machine learning
applications in these contexts.</div><div><a href='http://arxiv.org/abs/2401.17029v1'>2401.17029v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16356v3")'>cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and
  Glitch Generation</div>
<div id='2401.16356v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T17:59:26Z</div><div>Authors: Tom Dooney, Lyana Curier, Daniel Tan, Melissa Lopez, Chris Van Den Broeck, Stefano Bromuri</div><div style='padding-top: 10px; width: 80ex'>Simulating realistic time-domain observations of gravitational waves (GWs)
and GW detector glitches can help in advancing GW data analysis. Simulated data
can be used in downstream tasks by augmenting datasets for signal searches,
balancing data sets for machine learning, and validating detection schemes. In
this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional
model in the Generative Adversarial Network framework for simulating multiple
classes of time-domain observations that represent gravitational waves (GWs)
and detector glitches. cDVGAN can also generate generalized hybrid samples that
span the variation between classes through interpolation in the conditioned
class vector. cDVGAN introduces an additional player into the typical 2-player
adversarial game of GANs, where an auxiliary discriminator analyzes the
first-order derivative time-series. Our results show that this provides
synthetic data that better captures the features of the original data. cDVGAN
conditions on three classes, two denoised from LIGO blip and tomte glitch
events from its 3rd observing run (O3), and the third representing binary black
hole (BBH) mergers. Our proposed cDVGAN outperforms 4 different baseline GAN
models in replicating the features of the three classes. Specifically, our
experiments show that training convolutional neural networks (CNNs) with our
cDVGAN-generated data improves the detection of samples embedded in detector
noise beyond the synthetic data from other state-of-the-art GAN models. Our
best synthetic dataset yields as much as a 4.2% increase in
area-under-the-curve (AUC) performance compared to synthetic datasets from
baseline GANs. Moreover, training the CNN with hybrid samples from our cDVGAN
outperforms CNNs trained only on the standard classes, when identifying real
samples embedded in LIGO detector background (4% AUC improvement for cDVGAN).</div><div><a href='http://arxiv.org/abs/2401.16356v3'>2401.16356v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02536v1")'>Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable
  Machine Learning</div>
<div id='2403.02536v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-04T23:12:17Z</div><div>Authors: Spiridon Kasapis, Irina N. Kitiashvili, Paul Kosovich, Alexander G. Kosovichev, Viacheslav M. Sadykov, Patrick O'Keefe, Vincent Wang</div><div style='padding-top: 10px; width: 80ex'>Prediction of the Solar Energetic Particle (SEP) events garner increasing
interest as space missions extend beyond Earth's protective magnetosphere.
These events, which are, in most cases, products of magnetic
reconnection-driven processes during solar flares or fast
coronal-mass-ejection-driven shock waves, pose significant radiation hazards to
aviation, space-based electronics, and particularly, space exploration. In this
work, we utilize the recently developed dataset that combines the Solar
Dynamics Observatory/Helioseismic and Magnetic Imager's (SDO/HMI) Space weather
HMI Active Region Patches (SHARP) and the Solar and Heliospheric
Observatory/Michelson Doppler Imager's (SoHO/MDI) Space Weather MDI Active
Region Patches (SMARP). We employ a suite of machine learning strategies,
including Support Vector Machines (SVM) and regression models, to evaluate the
predictive potential of this new data product for a forecast of post-solar
flare SEP events. Our study indicates that despite the augmented volume of
data, the prediction accuracy reaches 0.7 +- 0.1, which aligns with but does
not exceed these published benchmarks. A linear SVM model with training and
testing configurations that mimic an operational setting (positive-negative
imbalance) reveals a slight increase (+ 0.04 +- 0.05) in the accuracy of a
14-hour SEP forecast compared to previous studies. This outcome emphasizes the
imperative for more sophisticated, physics-informed models to better understand
the underlying processes leading to SEP events.</div><div><a href='http://arxiv.org/abs/2403.02536v1'>2403.02536v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10290v1")'>Early Prediction of Geomagnetic Storms by Machine Learning Algorithms</div>
<div id='2401.10290v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T05:17:40Z</div><div>Authors: Iris Yan</div><div style='padding-top: 10px; width: 80ex'>Geomagnetic storms (GS) occur when solar winds disrupt Earth's magnetosphere.
GS can cause severe damages to satellites, power grids, and communication
infrastructures. Estimate of direct economic impacts of a large scale GS
exceeds $40 billion a day in the US. Early prediction is critical in preventing
and minimizing the hazards. However, current methods either predict several
hours ahead but fail to identify all types of GS, or make predictions within
short time, e.g., one hour ahead of the occurrence. This work aims to predict
all types of geomagnetic storms reliably and as early as possible using big
data and machine learning algorithms. By fusing big data collected from
multiple ground stations in the world on different aspects of solar
measurements and using Random Forests regression with feature selection and
downsampling on minor geomagnetic storm instances (which carry majority of the
data), we are able to achieve an accuracy of 82.55% on data collected in 2021
when making early predictions three hours in advance. Given that important
predictive features such as historic Kp indices are measured every 3 hours and
their importance decay quickly with the amount of time in advance, an early
prediction of 3 hours ahead of time is believed to be close to the practical
limit.</div><div><a href='http://arxiv.org/abs/2401.10290v1'>2401.10290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03474v1")'>Active Region-based Flare Forecasting with Sliding Window Multivariate
  Time Series Forest Classifiers</div>
<div id='2402.03474v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T19:34:12Z</div><div>Authors: Anli Ji, Berkay Aydin</div><div style='padding-top: 10px; width: 80ex'>Over the past few decades, many applications of physics-based simulations and
data-driven techniques (including machine learning and deep learning) have
emerged to analyze and predict solar flares. These approaches are pivotal in
understanding the dynamics of solar flares, primarily aiming to forecast these
events and minimize potential risks they may pose to Earth. Although current
methods have made significant progress, there are still limitations to these
data-driven approaches. One prominent drawback is the lack of consideration for
the temporal evolution characteristics in the active regions from which these
flares originate. This oversight hinders the ability of these methods to grasp
the relationships between high-dimensional active region features, thereby
limiting their usability in operations. This study centers on the development
of interpretable classifiers for multivariate time series and the demonstration
of a novel feature ranking method with sliding window-based sub-interval
ranking. The primary contribution of our work is to bridge the gap between
complex, less understandable black-box models used for high-dimensional data
and the exploration of relevant sub-intervals from multivariate time series,
specifically in the context of solar flare forecasting. Our findings
demonstrate that our sliding-window time series forest classifier performs
effectively in solar flare prediction (with a True Skill Statistic of over
85\%) while also pinpointing the most crucial features and sub-intervals for a
given learning task.</div><div><a href='http://arxiv.org/abs/2402.03474v1'>2402.03474v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08373v1")'>Time-Series Classification for Dynamic Strategies in Multi-Step
  Forecasting</div>
<div id='2402.08373v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T11:10:14Z</div><div>Authors: Riku Green, Grant Stevens, Telmo de Menezes e Silva Filho, Zahraa Abdallah</div><div style='padding-top: 10px; width: 80ex'>Multi-step forecasting (MSF) in time-series, the ability to make predictions
multiple time steps into the future, is fundamental to almost all temporal
domains. To make such forecasts, one must assume the recursive complexity of
the temporal dynamics. Such assumptions are referred to as the forecasting
strategy used to train a predictive model. Previous work shows that it is not
clear which forecasting strategy is optimal a priori to evaluating on unseen
data. Furthermore, current approaches to MSF use a single (fixed) forecasting
strategy.
  In this paper, we characterise the instance-level variance of optimal
forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We
experiment using 10 datasets from different scales, domains, and lengths of
multi-step horizons. When using a random-forest-based classifier, DyStrat
outperforms the best fixed strategy, which is not knowable a priori, 94% of the
time, with an average reduction in mean-squared error of 11%. Our approach
typically triples the top-1 accuracy compared to current approaches. Notably,
we show DyStrat generalises well for any MSF task.</div><div><a href='http://arxiv.org/abs/2402.08373v1'>2402.08373v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14735v1")'>Foundation Models for Time Series Analysis: A Tutorial and Survey</div>
<div id='2403.14735v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T10:08:37Z</div><div>Authors: Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen</div><div style='padding-top: 10px; width: 80ex'>Time series analysis stands as a focal point within the data mining
community, serving as a cornerstone for extracting valuable insights crucial to
a myriad of real-world applications. Recent advancements in Foundation Models
(FMs) have fundamentally reshaped the paradigm of model design for time series
analysis, boosting various downstream tasks in practice. These innovative
approaches often leverage pre-trained or fine-tuned FMs to harness generalized
knowledge tailored specifically for time series analysis. In this survey, we
aim to furnish a comprehensive and up-to-date overview of FMs for time series
analysis. While prior surveys have predominantly focused on either the
application or the pipeline aspects of FMs in time series analysis, they have
often lacked an in-depth understanding of the underlying mechanisms that
elucidate why and how FMs benefit time series analysis. To address this gap,
our survey adopts a model-centric classification, delineating various pivotal
elements of time-series FMs, including model architectures, pre-training
techniques, adaptation methods, and data modalities. Overall, this survey
serves to consolidate the latest advancements in FMs pertinent to time series
analysis, accentuating their theoretical underpinnings, recent strides in
development, and avenues for future research exploration.</div><div><a href='http://arxiv.org/abs/2403.14735v1'>2403.14735v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03182v1")'>Empowering Time Series Analysis with Large Language Models: A Survey</div>
<div id='2402.03182v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:46:35Z</div><div>Authors: Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, Dongjin Song</div><div style='padding-top: 10px; width: 80ex'>Recently, remarkable progress has been made over large language models
(LLMs), demonstrating their unprecedented capability in varieties of natural
language tasks. However, completely training a large general-purpose model from
the scratch is challenging for time series analysis, due to the large volumes
and varieties of time series data, as well as the non-stationarity that leads
to concept drift impeding continuous model adaptation and re-training. Recent
advances have shown that pre-trained LLMs can be exploited to capture complex
dependencies in time series data and facilitate various applications. In this
survey, we provide a systematic overview of existing methods that leverage LLMs
for time series analysis. Specifically, we first state the challenges and
motivations of applying language models in the context of time series as well
as brief preliminaries of LLMs. Next, we summarize the general pipeline for
LLM-based time series analysis, categorize existing methods into different
groups (i.e., direct query, tokenization, prompt design, fine-tune, and model
integration), and highlight the key ideas within each group. We also discuss
the applications of LLMs for both general and spatial-temporal time series
data, tailored to specific domains. Finally, we thoroughly discuss future
research opportunities to empower time series analysis with LLMs.</div><div><a href='http://arxiv.org/abs/2402.03182v1'>2402.03182v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01801v2")'>Large Language Models for Time Series: A Survey</div>
<div id='2402.01801v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T07:24:35Z</div><div>Authors: Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, Jingbo Shang</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have seen significant use in domains such as
natural language processing and computer vision. Going beyond text, image and
graphics, LLMs present a significant potential for analysis of time series
data, benefiting domains such as climate, IoT, healthcare, traffic, audio and
finance. This survey paper provides an in-depth exploration and a detailed
taxonomy of the various methodologies employed to harness the power of LLMs for
time series analysis. We address the inherent challenge of bridging the gap
between LLMs' original text data training and the numerical nature of time
series data, and explore strategies for transferring and distilling knowledge
from LLMs to numerical time series analysis. We detail various methodologies,
including (1) direct prompting of LLMs, (2) time series quantization, (3)
alignment techniques, (4) utilization of the vision modality as a bridging
mechanism, and (5) the combination of LLMs with tools. Additionally, this
survey offers a comprehensive overview of the existing multimodal time series
and text datasets and delves into the challenges and future opportunities of
this emerging field. We maintain an up-to-date Github repository which includes
all the papers and datasets discussed in the survey.</div><div><a href='http://arxiv.org/abs/2402.01801v2'>2402.01801v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00795v1")'>LLMs learn governing principles of dynamical systems, revealing an
  in-context neural scaling law</div>
<div id='2402.00795v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:28:10Z</div><div>Authors: Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls</div><div style='padding-top: 10px; width: 80ex'>Pretrained large language models (LLMs) are surprisingly effective at
performing zero-shot tasks, including time-series forecasting. However,
understanding the mechanisms behind such capabilities remains highly
challenging due to the complexity of the models. In this paper, we study LLMs'
ability to extrapolate the behavior of dynamical systems whose evolution is
governed by principles of physical interest. Our results show that LLaMA 2, a
language model trained primarily on texts, achieves accurate predictions of
dynamical system time series without fine-tuning or prompt engineering.
Moreover, the accuracy of the learned physical rules increases with the length
of the input context window, revealing an in-context version of neural scaling
law. Along the way, we present a flexible and efficient algorithm for
extracting probability density functions of multi-digit numbers directly from
LLMs.</div><div><a href='http://arxiv.org/abs/2402.00795v1'>2402.00795v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03175v1")'>The Matrix: A Bayesian learning model for LLMs</div>
<div id='2402.03175v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:42:10Z</div><div>Authors: Siddhartha Dalal, Vishal Misra</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce a Bayesian learning model to understand the
behavior of Large Language Models (LLMs). We explore the optimization metric of
LLMs, which is based on predicting the next token, and develop a novel model
grounded in this principle. Our approach involves constructing an ideal
generative text model represented by a multinomial transition probability
matrix with a prior, and we examine how LLMs approximate this matrix. We
discuss the continuity of the mapping between embeddings and multinomial
distributions, and present the Dirichlet approximation theorem to approximate
any prior. Additionally, we demonstrate how text generation by LLMs aligns with
Bayesian learning principles and delve into the implications for in-context
learning, specifically explaining why in-context learning emerges in larger
models where prompts are considered as samples to be updated. Our findings
indicate that the behavior of LLMs is consistent with Bayesian Learning,
offering new insights into their functioning and potential applications.</div><div><a href='http://arxiv.org/abs/2402.03175v1'>2402.03175v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.07043v1")'>A Tale of Tails: Model Collapse as a Change of Scaling Laws</div>
<div id='2402.07043v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T21:06:34Z</div><div>Authors: Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, Julia Kempe</div><div style='padding-top: 10px; width: 80ex'>As AI model size grows, neural scaling laws have become a crucial tool to
predict the improvements of large models when increasing capacity and the size
of original (human or natural) training data. Yet, the widespread use of
popular models means that the ecosystem of online data and text will co-evolve
to progressively contain increased amounts of synthesized data. In this paper
we ask: How will the scaling laws change in the inevitable regime where
synthetic data makes its way into the training corpus? Will future models,
still improve, or be doomed to degenerate up to total (model) collapse? We
develop a theoretical framework of model collapse through the lens of scaling
laws. We discover a wide range of decay phenomena, analyzing loss of scaling,
shifted scaling with number of generations, the ''un-learning" of skills, and
grokking when mixing human and synthesized data. Our theory is validated by
large-scale experiments with a transformer on an arithmetic task and text
generation using the large language model Llama2.</div><div><a href='http://arxiv.org/abs/2402.07043v1'>2402.07043v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03867v1")'>On the Origins of Linear Representations in Large Language Models</div>
<div id='2403.03867v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T17:17:36Z</div><div>Authors: Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, Victor Veitch</div><div style='padding-top: 10px; width: 80ex'>Recent works have argued that high-level semantic concepts are encoded
"linearly" in the representation space of large language models. In this work,
we study the origins of such linear representations. To that end, we introduce
a simple latent variable model to abstract and formalize the concept dynamics
of the next token prediction. We use this formalism to show that the next token
prediction objective (softmax with cross-entropy) and the implicit bias of
gradient descent together promote the linear representation of concepts.
Experiments show that linear representations emerge when learning from data
matching the latent variable model, confirming that this simple structure
already suffices to yield linear representations. We additionally confirm some
predictions of the theory using the LLaMA-2 large language model, giving
evidence that the simplified model yields generalizable insights.</div><div><a href='http://arxiv.org/abs/2403.03867v1'>2403.03867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03471v1")'>The Information of Large Language Model Geometry</div>
<div id='2402.03471v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T12:50:43Z</div><div>Authors: Zhiquan Tan, Chenghai Li, Weiran Huang</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the information encoded in the embeddings of large
language models (LLMs). We conduct simulations to analyze the representation
entropy and discover a power law relationship with model sizes. Building upon
this observation, we propose a theory based on (conditional) entropy to
elucidate the scaling law phenomenon. Furthermore, we delve into the
auto-regressive structure of LLMs and examine the relationship between the last
token and previous context tokens using information theory and regression
techniques. Specifically, we establish a theoretical connection between the
information gain of new tokens and ridge regression. Additionally, we explore
the effectiveness of Lasso regression in selecting meaningful tokens, which
sometimes outperforms the closely related attention weights. Finally, we
conduct controlled experiments, and find that information is distributed across
tokens, rather than being concentrated in specific "meaningful" tokens alone.</div><div><a href='http://arxiv.org/abs/2402.03471v1'>2402.03471v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02969v1")'>Towards Understanding the Word Sensitivity of Attention Layers: A Study
  via Random Features</div>
<div id='2402.02969v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T12:47:19Z</div><div>Authors: Simone Bombari, Marco Mondelli</div><div style='padding-top: 10px; width: 80ex'>Unveiling the reasons behind the exceptional success of transformers requires
a better understanding of why attention layers are suitable for NLP tasks. In
particular, such tasks require predictive models to capture contextual meaning
which often depends on one or few words, even if the sentence is long. Our work
studies this key property, dubbed word sensitivity (WS), in the prototypical
setting of random features. We show that attention layers enjoy high WS,
namely, there exists a vector in the space of embeddings that largely perturbs
the random attention features map. The argument critically exploits the role of
the softmax in the attention layer, highlighting its benefit compared to other
activations (e.g., ReLU). In contrast, the WS of standard random features is of
order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and
thus it decays with the length of the context. We then translate these results
on the word sensitivity into generalization bounds: due to their low WS, random
features provably cannot learn to distinguish between two sentences that differ
only in a single word; in contrast, due to their high WS, random attention
features have higher generalization capabilities. We validate our theoretical
results with experimental evidence over the BERT-Base word embeddings of the
imdb review dataset.</div><div><a href='http://arxiv.org/abs/2402.02969v1'>2402.02969v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15449v1")'>Repetition Improves Language Model Embeddings</div>
<div id='2402.15449v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T17:25:10Z</div><div>Authors: Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, Aditi Raghunathan</div><div style='padding-top: 10px; width: 80ex'>Recent approaches to improving the extraction of text embeddings from
autoregressive large language models (LLMs) have largely focused on
improvements to data, backbone pretrained language models, or improving
task-differentiation via instructions. In this work, we address an
architectural limitation of autoregressive models: token embeddings cannot
contain information from tokens that appear later in the input. To address this
limitation, we propose a simple approach, "echo embeddings," in which we repeat
the input twice in context and extract embeddings from the second occurrence.
We show that echo embeddings of early tokens can encode information about later
tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On
the MTEB leaderboard, echo embeddings improve over classical embeddings by over
9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a
Mistral-7B model achieve state-of-the-art compared to prior open source models
that do not leverage synthetic fine-tuning data.</div><div><a href='http://arxiv.org/abs/2402.15449v1'>2402.15449v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.13015v1")'>HyperVQ: MLR-based Vector Quantization in Hyperbolic Space</div>
<div id='2403.13015v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T03:17:08Z</div><div>Authors: Nabarun Goswami, Yusuke Mukuta, Tatsuya Harada</div><div style='padding-top: 10px; width: 80ex'>The success of models operating on tokenized data has led to an increased
demand for effective tokenization methods, particularly when applied to vision
or auditory tasks, which inherently involve non-discrete data. One of the most
popular tokenization methods is Vector Quantization (VQ), a key component of
several recent state-of-the-art methods across various domains. Typically, a VQ
Variational Autoencoder (VQVAE) is trained to transform data to and from its
tokenized representation. However, since the VQVAE is trained with a
reconstruction objective, there is no constraint for the embeddings to be well
disentangled, a crucial aspect for using them in discriminative tasks.
Recently, several works have demonstrated the benefits of utilizing hyperbolic
spaces for representation learning. Hyperbolic spaces induce compact latent
representations due to their exponential volume growth and inherent ability to
model hierarchical and structured data. In this work, we explore the use of
hyperbolic spaces for vector quantization (HyperVQ), formulating the VQ
operation as a hyperbolic Multinomial Logistic Regression (MLR) problem, in
contrast to the Euclidean K-Means clustering used in VQVAE. Through extensive
experiments, we demonstrate that hyperVQ performs comparably in reconstruction
and generative tasks while outperforming VQ in discriminative tasks and
learning a highly disentangled latent space.</div><div><a href='http://arxiv.org/abs/2403.13015v1'>2403.13015v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03737v1")'>Probabilistic Topic Modelling with Transformer Representations</div>
<div id='2403.03737v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T14:27:29Z</div><div>Authors: Arik Reuter, Anton Thielmann, Christoph Weisser, Benjamin Säfken, Thomas Kneib</div><div style='padding-top: 10px; width: 80ex'>Topic modelling was mostly dominated by Bayesian graphical models during the
last decade. With the rise of transformers in Natural Language Processing,
however, several successful models that rely on straightforward clustering
approaches in transformer-based embedding spaces have emerged and consolidated
the notion of topics as clusters of embedding vectors. We propose the
Transformer-Representation Neural Topic Model (TNTM), which combines the
benefits of topic representations in transformer-based embedding spaces and
probabilistic modelling. Therefore, this approach unifies the powerful and
versatile notion of topics based on transformer embeddings with fully
probabilistic modelling, as in models such as Latent Dirichlet Allocation
(LDA). We utilize the variational autoencoder (VAE) framework for improved
inference speed and modelling flexibility. Experimental results show that our
proposed model achieves results on par with various state-of-the-art approaches
in terms of embedding coherence while maintaining almost perfect topic
diversity. The corresponding source code is available at
https://github.com/ArikReuter/TNTM.</div><div><a href='http://arxiv.org/abs/2403.03737v1'>2403.03737v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.12865v1")'>Backward Lens: Projecting Language Model Gradients into the Vocabulary
  Space</div>
<div id='2402.12865v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T09:57:08Z</div><div>Authors: Shahar Katz, Yonatan Belinkov, Mor Geva, Lior Wolf</div><div style='padding-top: 10px; width: 80ex'>Understanding how Transformer-based Language Models (LMs) learn and recall
information is a key goal of the deep learning community. Recent
interpretability methods project weights and hidden states obtained from the
forward pass to the models' vocabularies, helping to uncover how information
flows within LMs. In this work, we extend this methodology to LMs' backward
pass and gradients. We first prove that a gradient matrix can be cast as a
low-rank linear combination of its forward and backward passes' inputs. We then
develop methods to project these gradients into vocabulary items and explore
the mechanics of how new information is stored in the LMs' neurons.</div><div><a href='http://arxiv.org/abs/2402.12865v1'>2402.12865v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09635v1")'>Transformers Get Stable: An End-to-End Signal Propagation Theory for
  Language Models</div>
<div id='2403.09635v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T17:59:14Z</div><div>Authors: Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, Haejun Lee</div><div style='padding-top: 10px; width: 80ex'>In spite of their huge success, transformer models remain difficult to scale
in depth. In this work, we develop a unified signal propagation theory and
provide formulae that govern the moments of the forward and backward signal
through the transformer model. Our framework can be used to understand and
mitigate vanishing/exploding gradients, rank collapse, and instability
associated with high attention scores. We also propose DeepScaleLM, an
initialization and scaling scheme that conserves unit output/gradient moments
throughout the model, enabling the training of very deep models with 100s of
layers. We find that transformer models could be much deeper - our deep models
with fewer parameters outperform shallow models in Language Modeling, Speech
Translation, and Image Classification, across Encoder-only, Decoder-only and
Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for
multiple datasets and model sizes. These improvements also translate into
improved performance on downstream Question Answering tasks and improved
robustness for image classification.</div><div><a href='http://arxiv.org/abs/2403.09635v1'>2403.09635v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00986v2")'>Merging Text Transformer Models from Different Initializations</div>
<div id='2403.00986v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T21:16:29Z</div><div>Authors: Neha Verma, Maha Elbayad</div><div style='padding-top: 10px; width: 80ex'>Recent work on one-shot permutation-based model merging has shown impressive
low- or zero-barrier mode connectivity between models from completely different
initializations. However, this line of work has not yet extended to the
Transformer architecture, despite its dominant popularity in the language
domain. Therefore, in this work, we investigate the extent to which separate
Transformer minima learn similar features, and propose a model merging
technique to investigate the relationship between these minima in the loss
landscape. The specifics of the architecture, like its residual connections,
multi-headed attention, and discrete, sequential input, require specific
interventions in order to compute model permutations that remain within the
same functional equivalence class. In merging these models with our method, we
consistently find lower loss barriers between minima compared to model
averaging for several models trained on a masked-language modeling task or
fine-tuned on a language understanding benchmark. Our results show that the
minima of these models are less sharp and isolated than previously understood,
and provide a basis for future work on merging separately trained Transformer
models.</div><div><a href='http://arxiv.org/abs/2403.00986v2'>2403.00986v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.07993v2")'>Carrying over algorithm in transformers</div>
<div id='2401.07993v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T22:36:11Z</div><div>Authors: Jorrit Kruthoff</div><div style='padding-top: 10px; width: 80ex'>Addition is perhaps one of the simplest arithmetic tasks one can think of and
is usually performed using the carrying over algorithm. This algorithm consists
of two tasks: adding digits in the same position and carrying over a one
whenever necessary. We study how transformer models implement this algorithm
and how the two aforementioned tasks are allocated to different parts of the
network. We first focus on two-layer encoder-only models and show that the
carrying over algorithm is implemented in a modular fashion. The first layer is
mostly responsible for adding digits in the same position. The second layer
first decides, in the attention, which positions need a carried one or not, and
then performs the carrying of the one in the final MLP. We provide a simple way
of precisely identifying which neurons are responsible for that task. This
implementation of the carrying over algorithm occurs across a range of
hyperparameters for two as well as three-layer models. For small decoder-only
models, we observe the same implementation and provide suggestive evidence for
its existence in three 7B large language models.</div><div><a href='http://arxiv.org/abs/2401.07993v2'>2401.07993v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01267v1")'>Dissecting Language Models: Machine Unlearning via Selective Pruning</div>
<div id='2403.01267v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T17:10:44Z</div><div>Authors: Nicholas Pochinkov, Nandi Schoots</div><div style='padding-top: 10px; width: 80ex'>Understanding and shaping the behaviour of Large Language Models (LLMs) is
increasingly important as applications become more powerful and more frequently
adopted. This paper introduces a machine unlearning method specifically
designed for LLMs. We introduce a selective pruning method for LLMs that
removes neurons based on their relative importance on a targeted capability
compared to overall network performance. This approach is a compute- and
data-efficient method for identifying and removing neurons that enable specific
behaviours. Our findings reveal that both feed-forward and attention neurons in
LLMs are specialized; that is, for specific tasks, certain neurons are more
crucial than others.</div><div><a href='http://arxiv.org/abs/2403.01267v1'>2403.01267v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16736v3")'>Engineering A Large Language Model From Scratch</div>
<div id='2401.16736v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T04:29:48Z</div><div>Authors: Abiodun Finbarrs Oketunji</div><div style='padding-top: 10px; width: 80ex'>The proliferation of deep learning in natural language processing (NLP) has
led to the development and release of innovative technologies capable of
understanding and generating human language with remarkable proficiency.
Atinuke, a Transformer-based neural network, optimises performance across
various language tasks by utilising a unique configuration. The architecture
interweaves layers for processing sequential data with attention mechanisms to
draw meaningful affinities between inputs and outputs. Due to the configuration
of its topology and hyperparameter tuning, it can emulate human-like language
by extracting features and learning complex mappings. Atinuke is modular,
extensible, and integrates seamlessly with existing machine learning pipelines.
Advanced matrix operations like softmax, embeddings, and multi-head attention
enable nuanced handling of textual, acoustic, and visual signals. By unifying
modern deep learning techniques with software design principles and
mathematical theory, the system achieves state-of-the-art results on natural
language tasks whilst remaining interpretable and robust.</div><div><a href='http://arxiv.org/abs/2401.16736v3'>2401.16736v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16819v2")'>Nemotron-4 15B Technical Report</div>
<div id='2402.16819v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:43:45Z</div><div>Authors: Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, Bryan Catanzaro</div><div style='padding-top: 10px; width: 80ex'>We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual
language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates
strong performance when assessed on English, multilingual, and coding tasks: it
outperforms all existing similarly-sized open models on 4 out of 7 downstream
evaluation areas and achieves competitive performance to the leading open
models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best
multilingual capabilities of all similarly-sized models, even outperforming
models over four times larger and those explicitly specialized for multilingual
tasks.</div><div><a href='http://arxiv.org/abs/2402.16819v2'>2402.16819v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12246v1")'>Orion-14B: Open-source Multilingual Large Language Models</div>
<div id='2401.12246v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T12:29:27Z</div><div>Authors: Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, Kun Han</div><div style='padding-top: 10px; width: 80ex'>In this study, we introduce Orion-14B, a collection of multilingual large
language models with 14 billion parameters. We utilize a data scheduling
approach to train a foundational model on a diverse corpus of 2.5 trillion
tokens, sourced from texts in English, Chinese, Japanese, Korean, and other
languages. Additionally, we fine-tuned a series of models tailored for
conversational applications and other specific use cases. Our evaluation
results demonstrate that Orion-14B achieves state-of-the-art performance across
a broad spectrum of tasks. We make the Orion-14B model family and its
associated code publicly accessible https://github.com/OrionStarAI/Orion,
aiming to inspire future research and practical applications in the field.</div><div><a href='http://arxiv.org/abs/2401.12246v1'>2401.12246v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16818v1")'>H2O-Danube-1.8B Technical Report</div>
<div id='2401.16818v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T08:45:08Z</div><div>Authors: Philipp Singer, Pascal Pfeiffer, Yauhen Babakhin, Maximilian Jeblick, Nischay Dhankhar, Gabor Fodor, Sri Satish Ambati</div><div style='padding-top: 10px; width: 80ex'>We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens
following the core principles of LLama 2 and Mistral. We leverage and refine
various techniques for pre-training large language models. Although our model
is trained on significantly fewer total tokens compared to reference models of
similar size, it exhibits highly competitive metrics across a multitude of
benchmarks. We additionally release a chat model trained with supervised
fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B
openly available under Apache 2.0 license further democratizing LLMs to a wider
audience economically.</div><div><a href='http://arxiv.org/abs/2401.16818v1'>2401.16818v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00786v3")'>CroissantLLM: A Truly Bilingual French-English Language Model</div>
<div id='2402.00786v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T17:17:55Z</div><div>Authors: Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, António Loison, Duarte M. Alves, Caio Corro, Nicolas Boizard, João Alves, Ricardo Rei, Pedro H. Martins, Antoni Bigata Casademunt, François Yvon, André F. T. Martins, Gautier Viaud, Céline Hudelot, Pierre Colombo</div><div style='padding-top: 10px; width: 80ex'>We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T
English and French tokens, to bring to the research and industrial community a
high-performance, fully open-sourced bilingual model that runs swiftly on
consumer-grade local hardware. To that end, we pioneer the approach of training
an intrinsically bilingual model with a 1:1 English-to-French pretraining data
ratio, a custom tokenizer, and bilingual finetuning datasets. We release the
training dataset, notably containing a French split with manually curated,
high-quality, and varied data sources. To assess performance outside of
English, we craft a novel benchmark, FrenchBench, consisting of an array of
classification and generation tasks, covering various orthogonal aspects of
model performance in the French Language. Additionally, rooted in transparency
and to foster further Large Language Model research, we release codebases, and
dozens of checkpoints across various model sizes, training data distributions,
and training steps, as well as fine-tuned Chat models, and strong translation
models. We evaluate our model through the FMTI framework, and validate 81 % of
the transparency criteria, far beyond the scores of even most open initiatives.
This work enriches the NLP landscape, breaking away from previous
English-centric work in order to strengthen our understanding of
multilinguality in language models.</div><div><a href='http://arxiv.org/abs/2402.00786v3'>2402.00786v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14228v1")'>Assessing the Portability of Parameter Matrices Trained by
  Parameter-Efficient Finetuning Methods</div>
<div id='2401.14228v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T15:11:07Z</div><div>Authors: Mohammed Sabry, Anya Belz</div><div style='padding-top: 10px; width: 80ex'>As the cost of training ever larger language models has grown, so has the
interest in reusing previously learnt knowledge. Transfer learning methods have
shown how reusing non-task-specific knowledge can help in subsequent
task-specific learning. In this paper, we investigate the inverse: porting
whole functional modules that encode task-specific knowledge from one model to
another. We designed a study comprising 1,440 training/testing runs to test the
portability of modules trained by parameter-efficient finetuning (PEFT)
techniques, using sentiment analysis as an example task. We test portability in
a wide range of scenarios, involving different PEFT techniques and different
pretrained host models, among other dimensions. We compare the performance of
ported modules with that of equivalent modules trained (i) from scratch, and
(ii) from parameters sampled from the same distribution as the ported module.
We find that the ported modules far outperform the two alternatives tested, but
that there are interesting performance differences between the four PEFT
techniques. We conclude that task-specific knowledge in the form of
structurally modular sets of parameters as produced by PEFT techniques is
highly portable, but that degree of success depends on type of PEFT and on
differences between originating and receiving pretrained models.</div><div><a href='http://arxiv.org/abs/2401.14228v1'>2401.14228v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.13851v2")'>Scaling NVIDIA's Multi-speaker Multi-lingual TTS Systems with Zero-Shot
  TTS to Indic Languages</div>
<div id='2401.13851v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T23:18:33Z</div><div>Authors: Akshit Arora, Rohan Badlani, Sungwon Kim, Rafael Valle, Bryan Catanzaro</div><div style='padding-top: 10px; width: 80ex'>In this paper, we describe the TTS models developed by NVIDIA for the
MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024
Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by
training additionally on 5 minutes of target speaker data. In Track 3, we
utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as
well as external datasets. We use HiFi-GAN vocoders for all submissions.
RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on
Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS)
of 3.62.</div><div><a href='http://arxiv.org/abs/2401.13851v2'>2401.13851v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.00212v1")'>Transcription and translation of videos using fine-tuned XLSR Wav2Vec2
  on custom dataset and mBART</div>
<div id='2403.00212v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T01:15:45Z</div><div>Authors: Aniket Tathe, Anand Kamble, Suyash Kumbharkar, Atharva Bhandare, Anirban C. Mitra</div><div style='padding-top: 10px; width: 80ex'>This research addresses the challenge of training an ASR model for
personalized voices with minimal data. Utilizing just 14 minutes of custom
audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to
create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual
Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this
dataset. The developed web-based GUI efficiently transcribes and translates
input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns
the translated text with the video timeline, delivering an accessible solution
for multilingual video content transcription and translation for personalized
voice.</div><div><a href='http://arxiv.org/abs/2403.00212v1'>2403.00212v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.03515v1")'>RoBERTurk: Adjusting RoBERTa for Turkish</div>
<div id='2401.03515v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T15:13:24Z</div><div>Authors: Nuri Tas</div><div style='padding-top: 10px; width: 80ex'>We pretrain RoBERTa on a Turkish corpora using BPE tokenizer. Our model
outperforms BERTurk family models on the BOUN dataset for the POS task while
resulting in underperformance on the IMST dataset for the same task and
achieving competitive scores on the Turkish split of the XTREME dataset for the
NER task - all while being pretrained on smaller data than its competitors. We
release our pretrained model and tokenizer.</div><div><a href='http://arxiv.org/abs/2401.03515v1'>2401.03515v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.06495v1")'>An investigation of structures responsible for gender bias in BERT and
  DistilBERT</div>
<div id='2401.06495v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T10:42:20Z</div><div>Authors: Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Christophe Gravier</div><div style='padding-top: 10px; width: 80ex'>In recent years, large Transformer-based Pre-trained Language Models (PLM)
have changed the Natural Language Processing (NLP) landscape, by pushing the
performance boundaries of the state-of-the-art on a wide variety of tasks.
However, this performance gain goes along with an increase in complexity, and
as a result, the size of such models (up to billions of parameters) represents
a constraint for their deployment on embedded devices or short-inference time
tasks. To cope with this situation, compressed models emerged (e.g.
DistilBERT), democratizing their usage in a growing number of applications that
impact our daily lives. A crucial issue is the fairness of the predictions made
by both PLMs and their distilled counterparts. In this paper, we propose an
empirical exploration of this problem by formalizing two questions: (1) Can we
identify the neural mechanism(s) responsible for gender bias in BERT (and by
extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate
gender bias (e.g. is DistilBERT more prone to gender bias than its uncompressed
version, BERT)? Our findings are the following: (I) one cannot identify a
specific layer that produces bias; (II) every attention head uniformly encodes
bias; except in the context of underrepresented classes with a high imbalance
of the sensitive attribute; (III) this subset of heads is different as we
re-fine tune the network; (IV) bias is more homogeneously produced by the heads
in the distilled model.</div><div><a href='http://arxiv.org/abs/2401.06495v1'>2401.06495v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.16065v1")'>Training a Bilingual Language Model by Mapping Tokens onto a Shared
  Character Space</div>
<div id='2402.16065v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T11:26:39Z</div><div>Authors: Aviad Rom, Kfir Bar</div><div style='padding-top: 10px; width: 80ex'>We train a bilingual Arabic-Hebrew language model using a transliterated
version of Arabic texts in Hebrew, to ensure both languages are represented in
the same script. Given the morphological, structural similarities, and the
extensive number of cognates shared among Arabic and Hebrew, we assess the
performance of a language model that employs a unified script for both
languages, on machine translation which requires cross-lingual knowledge. The
results are promising: our model outperforms a contrasting model which keeps
the Arabic texts in the Arabic script, demonstrating the efficacy of the
transliteration step. Despite being trained on a dataset approximately 60%
smaller than that of other existing language models, our model appears to
deliver comparable performance in machine translation across both translation
directions.</div><div><a href='http://arxiv.org/abs/2402.16065v1'>2402.16065v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.00518v1")'>EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit
  Large Language Models</div>
<div id='2402.00518v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T11:39:04Z</div><div>Authors: Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou</div><div style='padding-top: 10px; width: 80ex'>This work introduces EE-Tuning, a lightweight and economical solution to
training/tuning early-exit large language models (LLMs). In contrast to the
common approach of full-parameter pre-training, EE-Tuning augments any
pre-trained (and possibly fine-tuned) standard LLM with additional early-exit
layers that are tuned in a parameter-efficient manner, which requires
significantly less computational resources and training data. Our
implementation of EE-Tuning achieves outstanding training efficiency via
extensive performance optimizations, as well as scalability due to its full
compatibility with 3D parallelism. Results of systematic experiments validate
the efficacy of EE-Tuning, confirming that effective early-exit LLM inference
can be achieved with a limited training budget. In hope of making early-exit
LLMs accessible to the community, we release the source code of our
implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.</div><div><a href='http://arxiv.org/abs/2402.00518v1'>2402.00518v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12200v1")'>APT: Adaptive Pruning and Tuning Pretrained Language Models for
  Efficient Training and Inference</div>
<div id='2401.12200v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T18:39:40Z</div><div>Authors: Bowen Zhao, Hannaneh Hajishirzi, Qingqing Cao</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning and inference with large Language Models (LM) are generally known
to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces
training memory by updating a small number of LM parameters but does not
improve inference efficiency. Structured pruning improves LM inference
efficiency by removing consistent parameter blocks, yet often increases
training memory and time. To improve both training and inference efficiency, we
introduce APT that adaptively prunes and tunes parameters for the LMs. At the
early stage of fine-tuning, APT dynamically adds salient tuning parameters for
fast and accurate convergence while discarding unimportant parameters for
efficiency. Compared to baselines, our experiments show that APT maintains up
to 98% task performance when pruning RoBERTa and T5 models with 40% parameters
left while keeping 86.4% LLaMA models' performance with 70% parameters
remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces
large LMs memory training footprint by up to 70%.</div><div><a href='http://arxiv.org/abs/2401.12200v1'>2401.12200v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02938v1")'>Fast and Optimal Weight Update for Pruned Large Language Models</div>
<div id='2401.02938v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T23:10:23Z</div><div>Authors: Vladimír Boža</div><div style='padding-top: 10px; width: 80ex'>Pruning large language models (LLMs) is a challenging task due to their
enormous size. The primary difficulty is fine-tuning the model after pruning,
which is needed to recover the lost performance caused by dropping weights.
Recent approaches have either ignored fine-tuning entirely, focusing on
efficient pruning criteria, or attempted layer-wise weight updates, preserving
the behavior of each layer. However, even layer-wise weight updates can be
costly for LLMs, and previous works have resorted to various approximations.
  In our paper, we propose a fast and optimal weight update algorithm for
pruned layers based on the Alternating Direction Method of Multipliers (ADMM).
Coupled with a simple iterative pruning mask selection, our algorithm achieves
state-of-the-art pruning performance across a wide range of LLMs. Code is
available at https://github.com/fmfi-compbio/admm-pruning.</div><div><a href='http://arxiv.org/abs/2401.02938v1'>2401.02938v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16880v1")'>BESA: Pruning Large Language Models with Blockwise Parameter-Efficient
  Sparsity Allocation</div>
<div id='2402.16880v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T12:44:15Z</div><div>Authors: Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping Luo</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have demonstrated outstanding performance in
various tasks, such as text summarization, text question-answering, and etc.
While their performance is impressive, the computational footprint due to their
vast number of parameters can be prohibitive. Existing solutions such as
SparseGPT and Wanda attempt to alleviate this issue through weight pruning.
However, their layer-wise approach results in significant perturbation to the
model's output and requires meticulous hyperparameter tuning, such as the
pruning rate, which can adversely affect overall model performance. To address
this, this paper introduces a novel LLM pruning technique dubbed blockwise
parameter-efficient sparsity allocation (BESA) by applying a blockwise
reconstruction loss. In contrast to the typical layer-wise pruning techniques,
BESA is characterized by two distinctive attributes: i) it targets the overall
pruning error with respect to individual transformer blocks, and ii) it
allocates layer-specific sparsity in a differentiable manner, both of which
ensure reduced performance degradation after pruning. Our experiments show that
BESA achieves state-of-the-art performance, efficiently pruning LLMs like
LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five
hours. Code is available at
\href{https://github.com/OpenGVLab/LLMPrune-BESA}{here}.</div><div><a href='http://arxiv.org/abs/2402.16880v1'>2402.16880v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09025v1")'>SLEB: Streamlining LLMs through Redundancy Verification and Elimination
  of Transformer Blocks</div>
<div id='2402.09025v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T09:01:13Z</div><div>Authors: Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have proven to be highly effective across
various natural language processing tasks. However, their large number of
parameters poses significant challenges for practical deployment. Pruning, a
technique aimed at reducing the size and complexity of LLMs, offers a potential
solution by removing redundant components from the network. Despite the promise
of pruning, existing methods often struggle to achieve substantial end-to-end
LLM inference speedup. In this paper, we introduce SLEB, a novel approach
designed to streamline LLMs by eliminating redundant transformer blocks. We
choose the transformer block as the fundamental unit for pruning, because LLMs
exhibit block-level redundancy with high similarity between the outputs of
neighboring blocks. This choice allows us to effectively enhance the processing
speed of LLMs. Our experimental results demonstrate that SLEB successfully
accelerates LLM inference without compromising the linguistic capabilities of
these models, making it a promising technique for optimizing the efficiency of
LLMs. The code is available at: https://github.com/leapingjagg-dev/SLEB</div><div><a href='http://arxiv.org/abs/2402.09025v1'>2402.09025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10774v1")'>Medusa: Simple LLM Inference Acceleration Framework with Multiple
  Decoding Heads</div>
<div id='2401.10774v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T15:48:40Z</div><div>Authors: Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao</div><div style='padding-top: 10px; width: 80ex'>The inference process in Large Language Models (LLMs) is often limited due to
the absence of parallelism in the auto-regressive decoding process, resulting
in most operations being restricted by the memory bandwidth of accelerators.
While methods such as speculative decoding have been suggested to address this
issue, their implementation is impeded by the challenges associated with
acquiring and maintaining a separate draft model. In this paper, we present
Medusa, an efficient method that augments LLM inference by adding extra
decoding heads to predict multiple subsequent tokens in parallel. Using a
tree-based attention mechanism, Medusa constructs multiple candidate
continuations and verifies them simultaneously in each decoding step. By
leveraging parallel processing, Medusa introduces only minimal overhead in
terms of single-step latency while substantially reducing the number of
decoding steps required.
  We present two levels of fine-tuning procedures for Medusa to meet the needs
of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a
frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa
is fine-tuned together with the backbone LLM, enabling better prediction
accuracy of Medusa heads and higher speedup but needing a special training
recipe that preserves the backbone model's capabilities.
  Moreover, we propose several extensions that improve or expand the utility of
Medusa, including a self-distillation to handle situations where no training
data is available and a typical acceptance scheme to boost the acceptance rate
while maintaining generation quality. We evaluate Medusa on models of various
sizes and training procedures. Our experiments demonstrate that Medusa-1 can
achieve over 2.2x speedup without compromising generation quality, while
Medusa-2 further improves the speedup to 2.3-3.6x.</div><div><a href='http://arxiv.org/abs/2401.10774v1'>2401.10774v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12522v2")'>BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language
  Models</div>
<div id='2401.12522v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T06:36:49Z</div><div>Authors: Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong Xiao</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) commonly employ autoregressive generation during
inference, leading to high memory bandwidth demand and consequently extended
latency. To mitigate this inefficiency, we present Bi-directional Tuning for
lossless Acceleration (BiTA), an innovative method expediting LLMs via
streamlined semi-autoregressive generation and draft verification. Inspired by
the concept of prompt tuning, we enhance LLMs with a parameter-efficient design
called bi-directional tuning for the capability in semi-autoregressive
generation. Employing efficient tree-based decoding, the models perform draft
candidate generation and verification in parallel, ensuring outputs identical
to their autoregressive counterparts under greedy sampling. BiTA serves as a
lightweight plug-in module, seamlessly boosting the inference efficiency of
existing LLMs without requiring additional assistance models or incurring
significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat
achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments
confirm our method surpasses state-of-the-art acceleration techniques.</div><div><a href='http://arxiv.org/abs/2401.12522v2'>2401.12522v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11809v1")'>Generation Meets Verification: Accelerating Large Language Model
  Inference with Smart Parallel Auto-Correct Decoding</div>
<div id='2402.11809v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T03:39:10Z</div><div>Authors: Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao</div><div style='padding-top: 10px; width: 80ex'>This research aims to accelerate the inference speed of large language models
(LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel
\textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative
approach designed for achieving lossless acceleration of LLMs. By integrating
semi-autoregressive inference and speculative decoding capabilities, SPACE
uniquely enables autoregressive LLMs to parallelize token generation and
verification. This is realized through a specialized semi-autoregressive
supervised fine-tuning process that equips existing LLMs with the ability to
simultaneously predict multiple tokens. Additionally, an auto-correct decoding
algorithm facilitates the simultaneous generation and verification of token
sequences within a single model invocation. Through extensive experiments on a
range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x
on HumanEval-X while maintaining output quality.</div><div><a href='http://arxiv.org/abs/2402.11809v1'>2402.11809v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13485v1")'>ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel
  Decoding</div>
<div id='2402.13485v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T02:51:07Z</div><div>Authors: Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru Huang</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in generative large language models (LLMs) have
significantly boosted the performance in natural language processing tasks.
However, their efficiency is hampered by the inherent limitations in
autoregressive token generation. While parallel decoding with token tree
verification, e.g., Medusa, has been proposed to improve decoding parallelism
and efficiency, it often struggles with maintaining contextual relationships
due to its independent token prediction approach and incurs significant
verification overhead, especially with large tree sizes and batch processing.
In this paper, we propose ProPD, an efficient LLM parallel decoding framework
based on dynamic token tree pruning and generation. ProPD features an advanced
early pruning mechanism to efficiently eliminate unpromising token sequences to
improve verification efficiency. Additionally, it introduces a dynamic token
tree generation algorithm to balance the computation and parallelism of the
verification phase in real-time and maximize the overall efficiency across
different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across
a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD
consistently outperforms existing decoding algorithms by 1.1-3.2x.</div><div><a href='http://arxiv.org/abs/2402.13485v1'>2402.13485v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14160v2")'>Recursive Speculative Decoding: Accelerating LLM Inference via Sampling
  Without Replacement</div>
<div id='2402.14160v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T22:57:49Z</div><div>Authors: Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, Christopher Lott</div><div style='padding-top: 10px; width: 80ex'>Speculative decoding is an inference-acceleration method for large language
models (LLMs) where a small language model generates a draft-token sequence
which is further verified by the target LLM in parallel. Recent works have
advanced this method by establishing a draft-token tree, achieving superior
performance over a single-sequence speculative decoding. However, those works
independently generate tokens at each level of the tree, not leveraging the
tree's entire diversifiability. Besides, their empirical superiority has been
shown for fixed length of sequences, implicitly granting more computational
resource to LLM for the tree-based methods. None of the existing works has
conducted empirical studies with fixed target computational budgets despite its
importance to resource-bounded devices. We present Recursive Speculative
Decoding (RSD), a novel tree-based method that samples draft tokens without
replacement and maximizes the diversity of the tree. During RSD's drafting, the
tree is built by either Gumbel-Top-$k$ trick that draws tokens without
replacement in parallel or Stochastic Beam Search that samples sequences
without replacement while early-truncating unlikely draft sequences and
reducing the computational cost of LLM. We empirically evaluate RSD with Llama
2 and OPT models, showing that RSD outperforms the baseline methods,
consistently for fixed draft sequence length and in most cases for fixed
computational budgets at LLM.</div><div><a href='http://arxiv.org/abs/2402.14160v2'>2402.14160v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11131v1")'>Speculative Streaming: Fast LLM Inference without Auxiliary Models</div>
<div id='2402.11131v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T23:36:43Z</div><div>Authors: Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Mohammad Rastegari, Mahyar Najibi</div><div style='padding-top: 10px; width: 80ex'>Speculative decoding is a prominent technique to speed up the inference of a
large target language model based on predictions of an auxiliary draft model.
While effective, in application-specific settings, it often involves
fine-tuning both draft and target models to achieve high acceptance rates. As
the number of downstream tasks grows, these draft models add significant
complexity to inference systems. We propose Speculative Streaming, a
single-model speculative decoding method that fuses drafting into the target
model by changing the fine-tuning objective from next token prediction to
future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -
3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and
Meaning Representation, without sacrificing generation quality. Additionally,
Speculative Streaming is parameter-efficient. It achieves on-par/higher
speed-ups than Medusa-style architectures while using ~10000X fewer extra
parameters, making it well-suited for resource-constrained devices.</div><div><a href='http://arxiv.org/abs/2402.11131v1'>2402.11131v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09398v1")'>Get More with LESS: Synthesizing Recurrence with KV Cache Compression
  for Efficient LLM Inference</div>
<div id='2402.09398v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:54:56Z</div><div>Authors: Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen</div><div style='padding-top: 10px; width: 80ex'>Many computational factors limit broader deployment of large language models.
In this paper, we focus on a memory bottleneck imposed by the key-value (KV)
cache, a computational shortcut that requires storing previous KV pairs during
decoding. While existing KV cache methods approach this problem by pruning or
evicting large swaths of relatively less important KV pairs to dramatically
reduce the memory footprint of the cache, they can have limited success in
tasks that require recollecting a majority of previous tokens. To alleviate
this issue, we propose LESS, a simple integration of a (nearly free) constant
sized cache with eviction-based cache methods, such that all tokens can be
queried at later decoding steps. Its ability to retain information throughout
time shows merit on a variety of tasks where we demonstrate LESS can help
reduce the performance gap from caching everything, sometimes even matching it,
all while being efficient.</div><div><a href='http://arxiv.org/abs/2402.09398v1'>2402.09398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01869v1")'>APIServe: Efficient API Support for Large-Language Model Inferencing</div>
<div id='2402.01869v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:47:57Z</div><div>Authors: Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao Zhang, Yiying Zhang</div><div style='padding-top: 10px; width: 80ex'>Large language models are increasingly integrated with external tools and
APIs like ChatGPT plugins to extend their capability beyond language-centric
tasks. However, today's LLM inference systems are designed for standalone LLMs.
They treat API calls as new requests, causing unnecessary recomputation of
already computed contexts, which accounts for 37-40% of total model forwarding
time. This paper presents APIServe, the first LLM inference framework targeting
API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API
calls and dedicates saved memory for serving more requests. APISERVE improves
the overall serving throughput by 1.6x and completes 2x more requests per
second compared to the state-of-the-art LLM inference systems.</div><div><a href='http://arxiv.org/abs/2402.01869v1'>2402.01869v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18789v1")'>FlexLLM: A System for Co-Serving Large Language Model Inference and
  Parameter-Efficient Finetuning</div>
<div id='2402.18789v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T01:33:08Z</div><div>Authors: Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin Unger, Zhihao Jia</div><div style='padding-top: 10px; width: 80ex'>Parameter-efficient finetuning (PEFT) is a widely used technique to adapt
large language models for different tasks. Service providers typically create
separate systems for users to perform PEFT model finetuning and inference
tasks. This is because existing systems cannot handle workloads that include a
mix of inference and PEFT finetuning requests. As a result, shared GPU
resources are underutilized, leading to inefficiencies. To address this
problem, we present FlexLLM, the first system that can serve inference and
parameter-efficient finetuning requests in the same iteration. Our system
leverages the complementary nature of these two tasks and utilizes shared GPU
resources to run them jointly, using a method called co-serving. To achieve
this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks
down the finetuning computation of a sequence into smaller token-level
computations and uses dependent parallelization and graph pruning, two static
compilation optimizations, to minimize the memory overhead and latency for
co-serving. Compared to existing systems, FlexLLM's co-serving approach reduces
the activation GPU memory overhead by up to 8x, and the end-to-end GPU memory
requirement of finetuning by up to 36% while maintaining a low inference
latency and improving finetuning throughput. For example, under a heavy
inference workload, FlexLLM can still preserve more than 80% of the peak
finetuning throughput, whereas existing systems cannot make any progress with
finetuning. The source code of FlexLLM is publicly available at
https://github.com/flexflow/FlexFlow.</div><div><a href='http://arxiv.org/abs/2402.18789v1'>2402.18789v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08383v2")'>Exploiting Inter-Layer Expert Affinity for Accelerating
  Mixture-of-Experts Model Inference</div>
<div id='2401.08383v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T14:16:47Z</div><div>Authors: Jinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar K., Panda</div><div style='padding-top: 10px; width: 80ex'>In large language models like the Generative Pre-trained Transformer, the
Mixture of Experts paradigm has emerged as a powerful technique for enhancing
model expressiveness and accuracy. However, deploying GPT MoE models for
parallel inference on distributed systems presents significant challenges,
primarily due to the extensive Alltoall communication required for expert
routing and aggregation. This communication bottleneck exacerbates the already
complex computational landscape, hindering the efficient utilization of
high-performance computing resources. In this paper, we propose a lightweight
optimization technique called ExFlow, to largely accelerate the inference of
these MoE models. We take a new perspective on alleviating the communication
overhead by exploiting the inter-layer expert affinity. Unlike previous
methods, our solution can be directly applied to pre-trained MoE models without
any fine-tuning or accuracy degradation. By proposing a context-coherent expert
parallelism on distributed systems, our design only uses one Alltoall
communication to deliver the same functionality while previous methods all
require two Alltoalls. By carefully examining the conditional probability in
tokens' routing across multiple layers, we proved that pre-trained GPT MoE
models implicitly exhibit a strong inter-layer expert affinity. We then design
an efficient integer programming model to capture such features and show that
by properly placing the experts on corresponding GPUs, we can reduce up to 67%
cross-GPU routing latency. Our solution beats the cutting-edge MoE
implementations with experts from 8 to 64, with up to 2.2x improvement in
inference throughput. We further provide a detailed study of how the model
implicitly acquires this expert affinity at the very early training stage and
how this affinity evolves and stabilizes during training.</div><div><a href='http://arxiv.org/abs/2401.08383v2'>2401.08383v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14351v1")'>ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language
  Models</div>
<div id='2401.14351v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T17:55:07Z</div><div>Authors: Yao Fu, Leyang Xue, Yeqi Huang, Andrei-Octavian Brabete, Dmitrii Ustiugov, Yuvraj Patel, Luo Mai</div><div style='padding-top: 10px; width: 80ex'>This paper presents ServerlessLLM, a locality-enhanced serverless inference
system for Large Language Models (LLMs). ServerlessLLM exploits the substantial
capacity and bandwidth of storage and memory devices available on GPU servers,
thereby reducing costly remote checkpoint downloads and achieving efficient
checkpoint loading. ServerlessLLM achieves this through three main
contributions: (i) fast LLM checkpoint loading via a novel loading-optimized
checkpoint format design, coupled with an efficient multi-tier checkpoint
loading system; (ii) locality-driven LLM inference with live migration, which
allows ServerlessLLM to effectively achieve locality-driven server allocation
while preserving the low latency of ongoing LLM inference; and (iii)
locality-aware server allocation, enabling ServerlessLLM to evaluate the status
of each server in a cluster and effectively schedule model startup time to
capitalize on local checkpoint placement. Our comprehensive experiments, which
include microbenchmarks and real-world traces, show that ServerlessLLM
surpasses state-of-the-art systems by 10 - 200X in latency performance when
running various LLM inference workloads.</div><div><a href='http://arxiv.org/abs/2401.14351v1'>2401.14351v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.10444v1")'>Optimal Block-Level Draft Verification for Accelerating Speculative
  Decoding</div>
<div id='2403.10444v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T16:28:22Z</div><div>Authors: Ziteng Sun, Jae Hun Ro, Ahmad Beirami, Ananda Theertha Suresh</div><div style='padding-top: 10px; width: 80ex'>Speculative decoding has shown to be an effective method for lossless
acceleration of large language models (LLMs) during inference. In each
iteration, the algorithm first uses a smaller model to draft a block of tokens.
The tokens are then verified by the large model in parallel and only a subset
of tokens will be kept to guarantee that the final output follows the
distribution of the large model. In all of the prior speculative decoding
works, the draft verification is performed token-by-token independently. In
this work, we propose a better draft verification algorithm that provides
additional wall-clock speedup without incurring additional computation cost and
draft tokens. We first formulate the draft verification step as a block-level
optimal transport problem. The block-level formulation allows us to consider a
wider range of draft verification algorithms and obtain a higher number of
accepted tokens in expectation in one draft block. We propose a verification
algorithm that achieves the optimal accepted length for the block-level
transport problem. We empirically evaluate our proposed block-level
verification algorithm in a wide range of tasks and datasets, and observe
consistent improvements in wall-clock speedup when compared to token-level
verification algorithm. To the best of our knowledge, our work is the first to
establish improvement over speculative decoding through a better draft
verification algorithm.</div><div><a href='http://arxiv.org/abs/2403.10444v1'>2403.10444v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01528v1")'>Decoding Speculative Decoding</div>
<div id='2402.01528v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T16:15:24Z</div><div>Authors: Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman</div><div style='padding-top: 10px; width: 80ex'>Speculative Decoding is a widely used technique to speed up inference for
Large Language Models (LLMs) without modifying its outcome. When performing
inference on an LLM, speculative decoding uses a smaller draft model which
generates speculative tokens and then uses the target LLM to verify those draft
tokens. The speedup provided by speculative decoding heavily depends on the
choice of the draft model. It has been widely suggested to select a draft model
that provides a high probability of the generated token being accepted by the
LLM to achieve the highest throughput. However, our experiments indicate the
contrary with throughput diminishing as the probability of generated tokens to
be accepted by the target model increases. To understand this phenomenon, we
perform extensive experiments to characterize the different factors that affect
speculative decoding and how those factors interact and affect the speedups.
Based on our experiments we describe an analytical model which can be used to
decide the right draft model for a given workload. Further, using our insights
we design a new draft model for LLaMA-65B which can provide 30% higher
throughput than existing draft models.</div><div><a href='http://arxiv.org/abs/2402.01528v1'>2402.01528v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00143v1")'>Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree
  Averaging</div>
<div id='2403.00143v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T21:49:31Z</div><div>Authors: Behzad Shayegh, Yuqiao Wen, Lili Mou</div><div style='padding-top: 10px; width: 80ex'>We address unsupervised discontinuous constituency parsing, where we observe
a high variance in the performance of the only previous model. We propose to
build an ensemble of different runs of the existing discontinuous parser by
averaging the predicted trees, to stabilize and boost performance. To begin
with, we provide comprehensive computational complexity analysis (in terms of P
and NP-complete) for tree averaging under different setups of binarity and
continuity. We then develop an efficient exact algorithm to tackle the task,
which runs in a reasonable time for all samples in our experiments. Results on
three datasets show our method outperforms all baselines in all metrics; we
also provide in-depth analyses of our approach.</div><div><a href='http://arxiv.org/abs/2403.00143v1'>2403.00143v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00858v3")'>Direct Alignment of Draft Model for Speculative Decoding with
  Chat-Fine-Tuned LLMs</div>
<div id='2403.00858v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T19:55:06Z</div><div>Authors: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott</div><div style='padding-top: 10px; width: 80ex'>Text generation with Large Language Models (LLMs) is known to be memory bound
due to the combination of their auto-regressive nature, huge parameter counts,
and limited memory bandwidths, often resulting in low token rates. Speculative
decoding has been proposed as a solution for LLM inference acceleration.
However, since draft models are often unavailable in the modern open-source LLM
families, e.g., for Llama 2 7B, training a high-quality draft model is required
to enable inference acceleration via speculative decoding. In this paper, we
propose a simple draft model training framework for direct alignment to
chat-capable target models. With the proposed framework, we train Llama 2 Chat
Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of
the original size. Our training framework only consists of pretraining,
distillation dataset generation, and finetuning with knowledge distillation,
with no additional alignment procedure. For the finetuning step, we use
instruction-response pairs generated by target model for distillation in
plausible data distribution, and propose a new Total Variation Distance++
(TVD++) loss that incorporates variance reduction techniques inspired from the
policy gradient method in reinforcement learning. Our empirical results show
that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3
block efficiency and 2.4$\times$ speed-up relative to autoregressive decoding
on various tasks with no further task-specific fine-tuning.</div><div><a href='http://arxiv.org/abs/2403.00858v3'>2403.00858v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15077v2")'>EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</div>
<div id='2401.15077v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T18:59:01Z</div><div>Authors: Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang</div><div style='padding-top: 10px; width: 80ex'>Autoregressive decoding makes the inference of Large Language Models (LLMs)
time-consuming. In this paper, we reconsider speculative sampling and derive
two key observations. Firstly, autoregression at the feature
(second-to-top-layer) level is more straightforward than at the token level.
Secondly, the inherent uncertainty in feature (second-to-top-layer) level
autoregression constrains its performance. Based on these insights, we
introduce EAGLE (Extrapolation Algorithm for Greater Language-model
Efficiency), a simple yet highly efficient speculative sampling framework. By
incorporating a token sequence advanced by one time step, EAGLE effectively
resolves the uncertainty, enabling precise second-to-top-layer feature
prediction with minimal overhead. We conducted comprehensive evaluations of
EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE
model Mixtral 8x7B Instruct, and tasks in dialogue, code generation,
mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE
achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while
maintaining the distribution of the generated text.</div><div><a href='http://arxiv.org/abs/2401.15077v2'>2401.15077v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16844v1")'>Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding</div>
<div id='2402.16844v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:59:28Z</div><div>Authors: Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami Bejnordi</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have become ubiquitous in practice and are
widely used for generation tasks such as translation, summarization and
instruction following. However, their enormous size and reliance on
autoregressive decoding increase deployment costs and complicate their use in
latency-critical applications. In this work, we propose a hybrid approach that
combines language models of different sizes to increase the efficiency of
autoregressive decoding while maintaining high performance. Our method utilizes
a pretrained frozen LLM that encodes all prompt tokens once in parallel, and
uses the resulting representations to condition and guide a small language
model (SLM), which then generates the response more efficiently. We investigate
the combination of encoder-decoder LLMs with both encoder-decoder and
decoder-only SLMs from different model families and only require fine-tuning of
the SLM. Experiments with various benchmarks show substantial speedups of up to
$4\times$, with minor performance penalties of $1-2\%$ for translation and
summarization tasks compared to the LLM.</div><div><a href='http://arxiv.org/abs/2402.16844v1'>2402.16844v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02057v1")'>Break the Sequential Dependency of LLM Inference Using Lookahead
  Decoding</div>
<div id='2402.02057v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T06:37:50Z</div><div>Authors: Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang</div><div style='padding-top: 10px; width: 80ex'>Autoregressive decoding of large language models (LLMs) is memory bandwidth
bounded, resulting in high latency and significant wastes of the parallel
processing power of modern accelerators. Existing methods for accelerating LLM
decoding often require a draft model (e.g., speculative decoding), which is
nontrivial to obtain and unable to generalize. In this paper, we introduce
Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM
decoding without needing auxiliary models or data stores. It allows trading
per-step log(FLOPs) to reduce the number of total decoding steps, is more
parallelizable on single or multiple modern accelerators, and is compatible
with concurrent memory-efficient attention (e.g., FlashAttention). Our
implementation of Lookahead decoding can speed up autoregressive decoding by up
to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code
completion tasks. Our code is avialable at
https://github.com/hao-ai-lab/LookaheadDecoding</div><div><a href='http://arxiv.org/abs/2402.02057v1'>2402.02057v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09360v1")'>HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM
  Inference</div>
<div id='2402.09360v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T18:04:36Z</div><div>Authors: Yashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli</div><div style='padding-top: 10px; width: 80ex'>Autoregressive decoding with generative Large Language Models (LLMs) on
accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent
on transferring model parameters from high bandwidth memory (HBM) to cache. On
the other hand, recent works show that LLMs can maintain quality with
significant sparsity/redundancy in the feedforward (FFN) layers by
appropriately training the model to operate on a top-$k$ fraction of
rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the
transfer of model parameters, and hence latency. However, exploiting this
sparsity for improving latency is hindered by the fact that identifying top
rows/columns is data-dependent and is usually performed using full matrix
operations, severely limiting potential gains. To address these issues, we
introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of
two novel components: (i) a compression scheme to cheaply predict top-$k$
rows/columns with high recall, followed by full computation restricted to the
predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate
top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE
applied to both the softmax as well as feedforward layers, achieves almost
matching pretraining and downstream accuracy, and speeds up inference latency
by $1.47\times$ on a single TPUv5e device.</div><div><a href='http://arxiv.org/abs/2402.09360v1'>2402.09360v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.07921v1")'>Merino: Entropy-driven Design for Generative Language Models on IoT
  Devices</div>
<div id='2403.07921v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T03:20:27Z</div><div>Authors: Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, Jun Wang</div><div style='padding-top: 10px; width: 80ex'>Generative Large Language Models (LLMs) stand as a revolutionary advancement
in the modern era of artificial intelligence (AI). However, directly deploying
LLMs in resource-constrained hardware, such as Internet-of-Things (IoT)
devices, is difficult due to their high computational cost. In this paper, we
propose a novel information-entropy framework for designing mobile-friendly
generative language models. Our key design paradigm is to maximize the entropy
of transformer decoders within the given computational budgets. The whole
design procedure involves solving a mathematical programming (MP) problem,
which can be done on the CPU within minutes, making it nearly zero-cost. We
evaluate our designed models, termed MeRino, across nine NLP downstream tasks,
showing their competitive performance against the state-of-the-art
autoregressive transformer models under the mobile setting. Notably, MeRino
achieves similar or better zero performance compared to the 350M parameter OPT
while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model
size. Code will be made available soon.</div><div><a href='http://arxiv.org/abs/2403.07921v1'>2403.07921v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09919v2")'>Recurrent Drafter for Fast Speculative Decoding in Large Language Models</div>
<div id='2403.09919v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T23:40:56Z</div><div>Authors: Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce an improved approach of speculative decoding
aimed at enhancing the efficiency of serving large language models. Our method
capitalizes on the strengths of two established techniques: the classic
two-model speculative decoding approach, and the more recent single-model
approach, Medusa. Drawing inspiration from Medusa, our approach adopts a
single-model strategy for speculative decoding. However, our method
distinguishes itself by employing a single, lightweight draft head with a
recurrent dependency design, akin in essence to the small, draft model uses in
classic speculative decoding, but without the complexities of the full
transformer architecture. And because of the recurrent dependency, we can use
beam search to swiftly filter out undesired candidates with the draft head. The
outcome is a method that combines the simplicity of single-model design and
avoids the need to create a data-dependent tree attention structure only for
inference in Medusa. We empirically demonstrate the effectiveness of the
proposed method on several popular open source language models, along with a
comprehensive analysis of the trade-offs involved in adopting this approach.</div><div><a href='http://arxiv.org/abs/2403.09919v2'>2403.09919v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10799v1")'>Efficient Pruning of Large Language Model with Adaptive Estimation
  Fusion</div>
<div id='2403.10799v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-16T04:12:50Z</div><div>Authors: Jun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong, Geng Yuan, Wei Niu, Dong Huang, Yanzhi Wang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have become crucial for many generative
downstream tasks, leading to an inevitable trend and significant challenge to
deploy them efficiently on resource-constrained devices. Structured pruning is
a widely used method to address this challenge. However, when dealing with the
complex structure of the multiple decoder layers, general methods often employ
common estimation approaches for pruning. These approaches lead to a decline in
accuracy for specific downstream tasks. In this paper, we introduce a simple
yet efficient method that adaptively models the importance of each
substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained
estimations based on the results from complex and multilayer structures. All
aspects of our design seamlessly integrate into the endto-end pruning
framework. Our experimental results, compared with state-of-the-art methods on
mainstream datasets, demonstrate average accuracy improvements of 1.1%, 1.02%,
2.0%, and 1.2% for LLaMa-7B,Vicuna-7B, Baichuan-7B, and Bloom-7b1,
respectively.</div><div><a href='http://arxiv.org/abs/2403.10799v1'>2403.10799v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05605v1")'>Scaling Laws for Forgetting When Fine-Tuning Large Language Models</div>
<div id='2401.05605v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T00:44:25Z</div><div>Authors: Damjan Kalajdzievski</div><div style='padding-top: 10px; width: 80ex'>We study and quantify the problem of forgetting when fine-tuning pre-trained
large language models (LLMs) on a downstream task. We find that
parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters
(LoRA), still suffer from catastrophic forgetting. In particular, we identify a
strong inverse linear relationship between the fine-tuning performance and the
amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise
scaling laws that show forgetting increases as a shifted power law in the
number of parameters fine-tuned and the number of update steps. We also examine
the impact of forgetting on knowledge, reasoning, and the safety guardrails
trained into Llama 2 7B chat. Our study suggests that forgetting cannot be
avoided through early stopping or by varying the number of parameters
fine-tuned. We believe this opens up an important safety-critical direction for
future research to evaluate and develop fine-tuning schemes which mitigate
forgetting</div><div><a href='http://arxiv.org/abs/2401.05605v1'>2401.05605v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12264v1")'>Uncertainty quantification in fine-tuned LLMs using LoRA ensembles</div>
<div id='2402.12264v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T16:26:00Z</div><div>Authors: Oleksandr Balabanov, Hampus Linander</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning large language models can improve task specific performance,
although a general understanding of what the fine-tuned model has learned,
forgotten and how to trust its predictions is still missing. We derive
principled uncertainty quantification for fine-tuned LLMs with posterior
approximations using computationally efficient low-rank adaptation ensembles.
We analyze three common multiple-choice datasets using low-rank adaptation
ensembles based on Mistral-7b, and draw quantitative and qualitative
conclusions on their perceived complexity and model efficacy on the different
target domains during and after fine-tuning. In particular, backed by the
numerical experiments, we hypothesise about signals from entropic uncertainty
measures for data domains that are inherently difficult for a given
architecture to learn.</div><div><a href='http://arxiv.org/abs/2402.12264v1'>2402.12264v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11867v1")'>LoRA Training in the NTK Regime has No Spurious Local Minima</div>
<div id='2402.11867v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T06:22:09Z</div><div>Authors: Uijeong Jang, Jason D. Lee, Ernest K. Ryu</div><div style='padding-top: 10px; width: 80ex'>Low-rank adaptation (LoRA) has become the standard approach for
parameter-efficient fine-tuning of large language models (LLM), but our
theoretical understanding of LoRA has been limited. In this work, we
theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)
regime with $N$ data points, showing: (i) full fine-tuning (without LoRA)
admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with
rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient
descent to find the low-rank solutions; (iii) the low-rank solution found using
LoRA generalizes well.</div><div><a href='http://arxiv.org/abs/2402.11867v1'>2402.11867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12354v1")'>LoRA+: Efficient Low Rank Adaptation of Large Models</div>
<div id='2402.12354v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T18:33:49Z</div><div>Authors: Soufiane Hayou, Nikhil Ghosh, Bin Yu</div><div style='padding-top: 10px; width: 80ex'>In this paper, we show that Low Rank Adaptation (LoRA) as originally
introduced in Hu et al. (2021) leads to suboptimal finetuning of models with
large width (embedding dimension). This is due to the fact that adapter
matrices A and B in LoRA are updated with the same learning rate. Using scaling
arguments for large width networks, we demonstrate that using the same learning
rate for A and B does not allow efficient feature learning. We then show that
this suboptimality of LoRA can be corrected simply by setting different
learning rates for the LoRA adapter matrices A and B with a well-chosen ratio.
We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$
improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$
2X SpeedUp), at the same computational cost as LoRA.</div><div><a href='http://arxiv.org/abs/2402.12354v1'>2402.12354v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16842v2")'>Asymmetry in Low-Rank Adapters of Foundation Models</div>
<div id='2402.16842v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T18:59:12Z</div><div>Authors: Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Sáez de Ocáriz Borde, Rickard Brüel Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, Justin Solomon</div><div style='padding-top: 10px; width: 80ex'>Parameter-efficient fine-tuning optimizes large, pre-trained foundation
models by updating a subset of parameters; in this class, Low-Rank Adaptation
(LoRA) is particularly effective. Inspired by an effort to investigate the
different roles of LoRA matrices during fine-tuning, this paper characterizes
and leverages unexpected asymmetry in the importance of low-rank adapter
matrices. Specifically, when updating the parameter matrices of a neural
network by adding a product $BA$, we observe that the $B$ and $A$ matrices have
distinct functions: $A$ extracts features from the input, while $B$ uses these
features to create the desired output. Based on this observation, we
demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning
$A$, and that a random untrained $A$ should perform nearly as well as a
fine-tuned one. Using an information-theoretic lens, we also bound the
generalization of low-rank adapters, showing that the parameter savings of
exclusively training $B$ improves the bound. We support our conclusions with
experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.</div><div><a href='http://arxiv.org/abs/2402.16842v2'>2402.16842v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14946v1")'>A Single Linear Layer Yields Task-Adapted Low-Rank Matrices</div>
<div id='2403.14946v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T04:38:42Z</div><div>Authors: Hwichan Kim, Shota Sasaki, Sho Hoshino, Ukyo Honda</div><div style='padding-top: 10px; width: 80ex'>Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning
(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix
$\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study
suggested that there is correlation between $W_0$ and $\Delta W$. In this
study, we aim to delve deeper into relationships between $W_0$ and low-rank
matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,
we analyze a conversion matrix that transform $W_0$ into low-rank matrices,
which encapsulates information about the relationships. Our analysis reveals
that the conversion matrices are similar across each layer. Inspired by these
findings, we hypothesize that a single linear layer, which takes each layer's
$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this
hypothesis, we devise a method named Conditionally Parameterized LoRA
(CondLoRA) that updates initial weight matrices with low-rank matrices derived
from a single linear layer. Our empirical results show that CondLoRA maintains
a performance on par with LoRA, despite the fact that the trainable parameters
of CondLoRA are fewer than those of LoRA. Therefore, we conclude that "a single
linear layer yields task-adapted low-rank matrices."</div><div><a href='http://arxiv.org/abs/2403.14946v1'>2403.14946v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08505v1")'>Harnessing Orthogonality to Train Low-Rank Neural Networks</div>
<div id='2401.08505v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T17:07:22Z</div><div>Authors: Daniel Coquelin, Katharina Flügel, Marie Weiel, Nicholas Kiefer, Charlotte Debus, Achim Streit, Markus Götz</div><div style='padding-top: 10px; width: 80ex'>This study explores the learning dynamics of neural networks by analyzing the
singular value decomposition (SVD) of their weights throughout training. Our
investigation reveals that an orthogonal basis within each multidimensional
weight's SVD representation stabilizes during training. Building upon this, we
introduce Orthogonality-Informed Adaptive Low-Rank (OIALR) training, a novel
training method exploiting the intrinsic orthogonality of neural networks.
OIALR seamlessly integrates into existing training workflows with minimal
accuracy loss, as demonstrated by benchmarking on various datasets and
well-established network architectures. With appropriate hyperparameter tuning,
OIALR can surpass conventional training setups, including those of
state-of-the-art models.</div><div><a href='http://arxiv.org/abs/2401.08505v1'>2401.08505v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10686v1")'>Manipulating Sparse Double Descent</div>
<div id='2401.10686v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T13:33:23Z</div><div>Authors: Ya Shi Zhang</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the double descent phenomenon in two-layer neural
networks, focusing on the role of L1 regularization and representation
dimensions. It explores an alternative double descent phenomenon, named sparse
double descent. The study emphasizes the complex relationship between model
complexity, sparsity, and generalization, and suggests further research into
more diverse models and datasets. The findings contribute to a deeper
understanding of neural network training and optimization.</div><div><a href='http://arxiv.org/abs/2401.10686v1'>2401.10686v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01046v2")'>A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex
  Lasso Models with Reflection Features</div>
<div id='2403.01046v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T00:33:45Z</div><div>Authors: Emi Zeger, Yifei Wang, Aaron Mishkin, Tolga Ergen, Emmanuel Candès, Mert Pilanci</div><div style='padding-top: 10px; width: 80ex'>We prove that training neural networks on 1-D data is equivalent to solving a
convex Lasso problem with a fixed, explicitly defined dictionary matrix of
features. The specific dictionary depends on the activation and depth. We
consider 2-layer networks with piecewise linear activations, deep narrow ReLU
networks with up to 4 layers, and rectangular and tree networks with sign
activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer
creates features that represent reflections of training data about themselves.
The Lasso representation sheds insight to globally optimal networks and the
solution landscape.</div><div><a href='http://arxiv.org/abs/2403.01046v2'>2403.01046v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09018v1")'>Residual Alignment: Uncovering the Mechanisms of Residual Networks</div>
<div id='2401.09018v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T07:30:14Z</div><div>Authors: Jianing Li, Vardan Papyan</div><div style='padding-top: 10px; width: 80ex'>The ResNet architecture has been widely adopted in deep learning due to its
significant boost to performance through the use of simple skip connections,
yet the underlying mechanisms leading to its success remain largely unknown. In
this paper, we conduct a thorough empirical study of the ResNet architecture in
classification tasks by linearizing its constituent residual blocks using
Residual Jacobians and measuring their singular value decompositions. Our
measurements reveal a process called Residual Alignment (RA) characterized by
four properties:
  (RA1) intermediate representations of a given input are equispaced on a line,
embedded in high dimensional space, as observed by Gai and Zhang [2021];
  (RA2) top left and right singular vectors of Residual Jacobians align with
each other and across different depths;
  (RA3) Residual Jacobians are at most rank C for fully-connected ResNets,
where C is the number of classes; and
  (RA4) top singular values of Residual Jacobians scale inversely with depth.
  RA consistently occurs in models that generalize well, in both
fully-connected and convolutional architectures, across various depths and
widths, for varying numbers of classes, on all tested benchmark datasets, but
ceases to occur once the skip connections are removed. It also provably occurs
in a novel mathematical model we propose. This phenomenon reveals a strong
alignment between residual branches of a ResNet (RA2+4), imparting a highly
rigid geometric structure to the intermediate representations as they progress
linearly through the network (RA1) up to the final layer, where they undergo
Neural Collapse.</div><div><a href='http://arxiv.org/abs/2401.09018v1'>2401.09018v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13728v1")'>Average gradient outer product as a mechanism for deep neural collapse</div>
<div id='2402.13728v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T11:40:27Z</div><div>Authors: Daniel Beaglehole, Peter Súkeník, Marco Mondelli, Mikhail Belkin</div><div style='padding-top: 10px; width: 80ex'>Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the
data representations in the final layers of Deep Neural Networks (DNNs). Though
the phenomenon has been measured in a wide variety of settings, its emergence
is only partially understood. In this work, we provide substantial evidence
that DNC formation occurs primarily through deep feature learning with the
average gradient outer product (AGOP). This takes a step further compared to
efforts that explain neural collapse via feature-agnostic approaches, such as
the unconstrained features model. We proceed by providing evidence that the
right singular vectors and values of the weights are responsible for the
majority of within-class variability collapse in DNNs. As shown in recent work,
this singular structure is highly correlated with that of the AGOP. We then
establish experimentally and theoretically that AGOP induces neural collapse in
a randomly initialized neural network. In particular, we demonstrate that Deep
Recursive Feature Machines, a method originally introduced as an abstraction
for AGOP feature learning in convolutional neural networks, exhibits DNC.</div><div><a href='http://arxiv.org/abs/2402.13728v1'>2402.13728v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14379v1")'>Tensor network compressibility of convolutional models</div>
<div id='2403.14379v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T13:12:33Z</div><div>Authors: Sukhbinder Singh, Saeed S. Jahromi, Roman Orus</div><div style='padding-top: 10px; width: 80ex'>Convolutional neural networks (CNNs) represent one of the most widely used
neural network architectures, showcasing state-of-the-art performance in
computer vision tasks. Although larger CNNs generally exhibit higher accuracy,
their size can be effectively reduced by "tensorization" while maintaining
accuracy. Tensorization consists of replacing the convolution kernels with
compact decompositions such as Tucker, Canonical Polyadic decompositions, or
quantum-inspired decompositions such as matrix product states, and directly
training the factors in the decompositions to bias the learning towards
low-rank decompositions. But why doesn't tensorization seem to impact the
accuracy adversely? We explore this by assessing how truncating the convolution
kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we
truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50
pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We
found that kernels (especially those inside deeper layers) could often be
truncated along several cuts resulting in significant loss in kernel norm but
not in classification accuracy. This suggests that such ``correlation
compression'' (underlying tensorization) is an intrinsic feature of how
information is encoded in dense CNNs. We also found that aggressively truncated
models could often recover the pre-truncation accuracy after only a few epochs
of re-training, suggesting that compressing the internal correlations of
convolution layers does not often transport the model to a worse minimum. Our
results can be applied to tensorize and compress CNN models more effectively.</div><div><a href='http://arxiv.org/abs/2403.14379v1'>2403.14379v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.03384v1")'>conv_einsum: A Framework for Representation and Fast Evaluation of
  Multilinear Operations in Convolutional Tensorial Neural Networks</div>
<div id='2401.03384v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-07T04:30:12Z</div><div>Authors: Tahseen Rabbani, Jiahao Su, Xiaoyu Liu, David Chan, Geoffrey Sangston, Furong Huang</div><div style='padding-top: 10px; width: 80ex'>Modern ConvNets continue to achieve state-of-the-art results over a vast
array of vision and image classification tasks, but at the cost of increasing
parameters. One strategy for compactifying a network without sacrificing much
expressive power is to reshape it into a tensorial neural network (TNN), which
is a higher-order tensorization of its layers, followed by a factorization,
such as a CP-decomposition, which strips a weight down to its critical basis
components. Passes through TNNs can be represented as sequences of multilinear
operations (MLOs), where the evaluation path can greatly affect the number of
floating point operations (FLOPs) incurred. While functions such as the popular
einsum can evaluate simple MLOs such as contractions, existing implementations
cannot process multi-way convolutions, resulting in scant assessments of how
optimal evaluation paths through tensorized convolutional layers can improve
training speed. In this paper, we develop a unifying framework for representing
tensorial convolution layers as einsum-like strings and a meta-algorithm
conv_einsum which is able to evaluate these strings in a FLOPs-minimizing
manner. Comprehensive experiments, using our open-source implementation, over a
wide range of models, tensor decompositions, and diverse tasks, demonstrate
that conv_einsum significantly increases both computational and
memory-efficiency of convolutional TNNs.</div><div><a href='http://arxiv.org/abs/2401.03384v1'>2401.03384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.15073v1")'>On the Inclusion of Charge and Spin States in Cartesian Tensor Neural
  Network Potentials</div>
<div id='2403.15073v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-22T09:54:04Z</div><div>Authors: Guillem Simeon, Antonio Mirarchi, Raul P. Pelaez, Raimondas Galvelis, Gianni De Fabritiis</div><div style='padding-top: 10px; width: 80ex'>In this letter, we present an extension to TensorNet, a state-of-the-art
equivariant Cartesian tensor neural network potential, allowing it to handle
charged molecules and spin states without architectural changes or increased
costs. By incorporating these attributes, we address input degeneracy issues,
enhancing the model's predictive accuracy across diverse chemical systems. This
advancement significantly broadens TensorNet's applicability, maintaining its
efficiency and accuracy.</div><div><a href='http://arxiv.org/abs/2403.15073v1'>2403.15073v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06890v1")'>Application of Quantum Tensor Networks for Protein Classification</div>
<div id='2403.06890v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T16:47:09Z</div><div>Authors: Debarshi Kundu, Archisman Ghosh, Srinivasan Ekambaram, Jian Wang, Nikolay Dokholyan, Swaroop Ghosh</div><div style='padding-top: 10px; width: 80ex'>We show that protein sequences can be thought of as sentences in natural
language processing and can be parsed using the existing Quantum Natural
Language framework into parameterized quantum circuits of reasonable qubits,
which can be trained to solve various protein-related machine-learning
problems. We classify proteins based on their subcellular locations, a pivotal
task in bioinformatics that is key to understanding biological processes and
disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we
demonstrate that Quantum Tensor Networks (QTN) can effectively handle the
complexity and diversity of protein sequences. We present a detailed
methodology that adapts QTN architectures to the nuanced requirements of
protein data, supported by comprehensive experimental results. We demonstrate
two distinct QTNs, inspired by classical recurrent neural networks (RNN) and
convolutional neural networks (CNN), to solve the binary classification task
mentioned above. Our top-performing quantum model has achieved a 94% accuracy
rate, which is comparable to the performance of a classical model that uses the
ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is
extremely large, containing 8 million parameters in its smallest configuration,
whereas our best quantum model requires only around 800 parameters. We
demonstrate that these hybrid models exhibit promising performance, showcasing
their potential to compete with classical models of similar complexity.</div><div><a href='http://arxiv.org/abs/2403.06890v1'>2403.06890v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.19095v2")'>A Protein Structure Prediction Approach Leveraging Transformer and CNN
  Integration</div>
<div id='2402.19095v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T12:24:20Z</div><div>Authors: Yanlin Zhou, Kai Tan, Xinyu Shen, Zheng He, Haotian Zheng</div><div style='padding-top: 10px; width: 80ex'>Proteins are essential for life, and their structure determines their
function. The protein secondary structure is formed by the folding of the
protein primary structure, and the protein tertiary structure is formed by the
bending and folding of the secondary structure. Therefore, the study of protein
secondary structure is very helpful to the overall understanding of protein
structure. Although the accuracy of protein secondary structure prediction has
continuously improved with the development of machine learning and deep
learning, progress in the field of protein structure prediction, unfortunately,
remains insufficient to meet the large demand for protein information.
Therefore, based on the advantages of deep learning-based methods in feature
extraction and learning ability, this paper adopts a two-dimensional fusion
deep neural network model, DstruCCN, which uses Convolutional Neural Networks
(CCN) and a supervised Transformer protein language model for single-sequence
protein structure prediction. The training features of the two are combined to
predict the protein Transformer binding site matrix, and then the
three-dimensional structure is reconstructed using energy minimization.</div><div><a href='http://arxiv.org/abs/2402.19095v2'>2402.19095v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.12132v1")'>Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis
  Using Sequential Multisequence MRI</div>
<div id='2401.12132v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-22T17:14:47Z</div><div>Authors: John D. Mayfield, Issam El Naqa</div><div style='padding-top: 10px; width: 80ex'>Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term
Memory (LSTM) models were studied to provide sequential relationships for each
timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot
study, we compared three QCNN-LSTM models for binary classification of MS
disability benchmarked against classical neural network architectures. Our
hypothesis is that quantum models will provide competitive performance. Methods
Matrix Product State (MPS), reverse Multistate Entanglement Renormalization
Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM
layer to process near-annual MRI data of patients diagnosed with MS. These were
benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision
Transformer (ViViT). Predicted logits were measured against ground truth labels
of each patient's Extended Disability Severity Score (EDSS) using binary
cross-entropy loss. Training/validation/holdout testing was partitioned using
5-fold cross validation with a total split of 60:20:20. Levene's test of
variance was used to measure statistical difference and Student's t-test for
paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and
TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively
(p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73
and 0.77, respectively (p-value 0.631). Overall variance and mean were not
statistically significant (p-value 0.713), however, time to train was
significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218,
respectively, p-value &lt;0.001). Conclusion QCNN-LSTM models perform
competitively to their classical counterparts with greater efficiency in train
time. Clinically, these can add value in terms of efficiency to time-dependent
deep learning prediction of disease progression based upon medical imaging.</div><div><a href='http://arxiv.org/abs/2401.12132v1'>2401.12132v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.17992v1")'>Multilinear Operator Networks</div>
<div id='2401.17992v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T16:52:19Z</div><div>Authors: Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, Volkan Cevher</div><div style='padding-top: 10px; width: 80ex'>Despite the remarkable capabilities of deep neural networks in image
recognition, the dependence on activation functions remains a largely
unexplored area and has yet to be eliminated. On the other hand, Polynomial
Networks is a class of models that does not require activation functions, but
have yet to perform on par with modern architectures. In this work, we aim
close this gap and propose MONet, which relies solely on multilinear operators.
The core layer of MONet, called Mu-Layer, captures multiplicative interactions
of the elements of the input token. MONet captures high-degree interactions of
the input elements and we demonstrate the efficacy of our approach on a series
of image recognition and scientific computing benchmarks. The proposed model
outperforms prior polynomial networks and performs on par with modern
architectures. We believe that MONet can inspire further research on models
that use entirely multilinear operations.</div><div><a href='http://arxiv.org/abs/2401.17992v1'>2401.17992v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02736v1")'>On the numerical reliability of nonsmooth autodiff: a MaxPool case study</div>
<div id='2401.02736v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T10:14:39Z</div><div>Authors: Ryan Boustany</div><div style='padding-top: 10px; width: 80ex'>This paper considers the reliability of automatic differentiation (AD) for
neural networks involving the nonsmooth MaxPool operation. We investigate the
behavior of AD across different precision levels (16, 32, 64 bits) and
convolutional architectures (LeNet, VGG, and ResNet) on various datasets
(MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent
research has shown that it coincides with the derivative almost everywhere,
even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the
other hand, in practice, AD operates with floating-point numbers (not real
numbers), and there is, therefore, a need to explore subsets on which AD can be
numerically incorrect. These subsets include a bifurcation zone (where AD is
incorrect over reals) and a compensation zone (where AD is incorrect over
floating-point numbers but correct over reals). Using SGD for the training
process, we study the impact of different choices of the nonsmooth Jacobian for
the MaxPool function on the precision of 16 and 32 bits. These findings suggest
that nonsmooth MaxPool Jacobians with lower norms help maintain stable and
efficient test accuracy, whereas those with higher norms can result in
instability and decreased performance. We also observe that the influence of
MaxPool's nonsmooth Jacobians on learning can be reduced by using batch
normalization, Adam-like optimizers, or increasing the precision level.</div><div><a href='http://arxiv.org/abs/2401.02736v1'>2401.02736v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.11826v1")'>CapsLorentzNet: Integrating Physics Inspired Features with Graph
  Convolution</div>
<div id='2403.11826v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T14:31:09Z</div><div>Authors: Rameswar Sahu</div><div style='padding-top: 10px; width: 80ex'>With the advent of advanced machine learning techniques, boosted object
tagging has witnessed significant progress. In this article, we take this field
further by introducing novel architectural modifications compatible with a wide
array of Graph Neural Network (GNN) architectures. Our approach advocates for
integrating capsule layers, replacing the conventional decoding blocks in
standard GNNs. These capsules are a group of neurons with vector activations.
The orientation of these vectors represents important properties of the objects
under study, with their magnitude characterizing whether the object under study
belongs to the class represented by the capsule. Moreover, capsule networks
incorporate a regularization by reconstruction mechanism, facilitating the
seamless integration of expert-designed high-level features into the analysis.
We have studied the usefulness of our architecture with the LorentzNet
architecture for quark-gluon tagging. Here, we have replaced the decoding block
of LorentzNet with a capsulated decoding block and have called the resulting
architecture CapsLorentzNet. Our new architecture can enhance the performance
of LorentzNet by 20 \% for the quark-gluon tagging task.</div><div><a href='http://arxiv.org/abs/2403.11826v1'>2403.11826v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11872v1")'>NuGraph2: A Graph Neural Network for Neutrino Physics Event
  Reconstruction</div>
<div id='2403.11872v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:26:05Z</div><div>Authors: V Hewes, Adam Aurisano, Giuseppe Cerati, Jim Kowalkowski, Claire Lee, Wei-keng Liao, Daniel Grzenda, Kaushal Gumpula, Xiaohe Zhang</div><div style='padding-top: 10px; width: 80ex'>Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a
wealth of high-resolution information on particle interactions, and leveraging
that information to its full potential requires sophisticated automated
reconstruction techniques. This article describes NuGraph2, a Graph Neural
Network (GNN) for low-level reconstruction of simulated neutrino interactions
in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE
detector geometry are described as heterogeneous graphs, with energy
depositions on each detector plane forming nodes on planar subgraphs. The
network utilizes a multi-head attention message-passing mechanism to perform
background filtering and semantic labelling on these graph nodes, identifying
those associated with the primary physics interaction with 98.0\% efficiency
and labelling them according to particle type with 94.9\% efficiency. The
network operates directly on detector observables across multiple 2D
representations, but utilizes a 3D-context-aware mechanism to encourage
consistency between these representations. Model inference takes 0.12 s/event
on a CPU, and 0.005 s/event batched on a GPU. This architecture is designed to
be a general-purpose solution for particle reconstruction in neutrino physics,
with the potential for deployment across a broad range of detector
technologies, and offers a core convolution engine that can be leveraged for a
variety of tasks beyond the two described in this article.</div><div><a href='http://arxiv.org/abs/2403.11872v1'>2403.11872v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11538v1")'>PASCL: Supervised Contrastive Learning with Perturbative Augmentation
  for Particle Decay Reconstruction</div>
<div id='2402.11538v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T10:38:34Z</div><div>Authors: Junjian Lu, Siwei Liu, Dmitrii Kobylianski, Etienne Dreyer, Eilam Gross, Shangsong Liang</div><div style='padding-top: 10px; width: 80ex'>In high-energy physics, particles produced in collision events decay in a
format of a hierarchical tree structure, where only the final decay products
can be observed using detectors. However, the large combinatorial space of
possible tree structures makes it challenging to recover the actual decay
process given a set of final particles. To better analyse the hierarchical tree
structure, we propose a graph-based deep learning model to infer the tree
structure to reconstruct collision events. In particular, we use a compact
matrix representation termed as lowest common ancestor generations (LCAG)
matrix, to encode the particle decay tree structure. Then, we introduce a
perturbative augmentation technique applied to node features, aiming to mimic
experimental uncertainties and increase data diversity. We further propose a
supervised graph contrastive learning algorithm to utilize the information of
inter-particle relations from multiple decay processes. Extensive experiments
show that our proposed supervised graph contrastive learning with perturbative
augmentation (PASCL) method outperforms state-of-the-art baseline models on an
existing physics-based dataset, significantly improving the reconstruction
accuracy. This method provides a more effective training strategy for models
with the same parameters and makes way for more accurate and efficient
high-energy particle physics data analysis.</div><div><a href='http://arxiv.org/abs/2402.11538v1'>2402.11538v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16016v1")'>Combined track finding with GNN &amp; CKF</div>
<div id='2401.16016v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T10:05:37Z</div><div>Authors: Lukas Heinrich, Benjamin Huth, Andreas Salzburger, Tilo Wettig</div><div style='padding-top: 10px; width: 80ex'>The application of Graph Neural Networks (GNN) in track reconstruction is a
promising approach to cope with the challenges arising at the High-Luminosity
upgrade of the Large Hadron Collider (HL-LHC). GNNs show good track-finding
performance in high-multiplicity scenarios and are naturally parallelizable on
heterogeneous compute architectures.
  Typical high-energy-physics detectors have high resolution in the innermost
layers to support vertex reconstruction but lower resolution in the outer
parts. GNNs mainly rely on 3D space-point information, which can cause reduced
track-finding performance in the outer regions.
  In this contribution, we present a novel combination of GNN-based track
finding with the classical Combinatorial Kalman Filter (CKF) algorithm to
circumvent this issue: The GNN resolves the track candidates in the inner pixel
region, where 3D space points can represent measurements very well. These
candidates are then picked up by the CKF in the outer regions, where the CKF
performs well even for 1D measurements.
  Using the ACTS infrastructure, we present a proof of concept based on truth
tracking in the pixels as well as a dedicated GNN pipeline trained on
$t\bar{t}$ events with pile-up 200 in the OpenDataDetector.</div><div><a href='http://arxiv.org/abs/2401.16016v1'>2401.16016v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.04990v2")'>Jet Discrimination with Quantum Complete Graph Neural Network</div>
<div id='2403.04990v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T02:02:23Z</div><div>Authors: Yi-An Chen, Kai-Feng Chen</div><div style='padding-top: 10px; width: 80ex'>Machine learning, particularly deep neural networks, has been widely utilized
in high energy physics and has shown remarkable results in various
applications. Moreover, the concept of machine learning has been extended to
quantum computers, giving rise to a new research area known as quantum machine
learning. In this paper, we propose a novel variational quantum circuit model,
Quantum Complete Graph Neural Network (QCGNN), designed for learning complete
graphs. We argue that QCGNN has a polynomial speedup against its classical
counterpart, due to the property of quantum parallelism. In this paper, we
study the application of QCGNN through the challenging jet discrimination,
where the jets are represented with complete graphs. Subsequently, we conduct a
comparative analysis with classical graph neural networks to establish a
benchmark.</div><div><a href='http://arxiv.org/abs/2403.04990v2'>2403.04990v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.15984v1")'>A unified Fourier slice method to derive ridgelet transform for a
  variety of depth-2 neural networks</div>
<div id='2402.15984v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T04:30:04Z</div><div>Authors: Sho Sonoda, Isao Ishikawa, Masahiro Ikeda</div><div style='padding-top: 10px; width: 80ex'>To investigate neural network parameters, it is easier to study the
distribution of parameters than to study the parameters in each neuron. The
ridgelet transform is a pseudo-inverse operator that maps a given function $f$
to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$
reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected
networks on a Euclidean space, the ridgelet transform has been discovered up to
the closed-form expression, thus we could describe how the parameters are
distributed. However, for a variety of modern neural network architectures, the
closed-form expression has not been known. In this paper, we explain a
systematic method using Fourier expressions to derive ridgelet transforms for a
variety of modern networks such as networks on finite fields $\mathbb{F}_p$,
group convolutional networks on abstract Hilbert space $\mathcal{H}$,
fully-connected networks on noncompact symmetric spaces $G/K$, and pooling
layers, or the $d$-plane ridgelet transform.</div><div><a href='http://arxiv.org/abs/2402.15984v1'>2402.15984v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01268v1")'>$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy</div>
<div id='2401.01268v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T16:14:02Z</div><div>Authors: Nicola Novello, Andrea M. Tonello</div><div style='padding-top: 10px; width: 80ex'>In deep learning, classification tasks are formalized as optimization
problems solved via the minimization of the cross-entropy. However, recent
advancements in the design of objective functions allow the $f$-divergence
measure to generalize the formulation of the optimization problem for
classification. With this goal in mind, we adopt a Bayesian perspective and
formulate the classification task as a maximum a posteriori probability
problem. We propose a class of objective functions based on the variational
representation of the $f$-divergence, from which we extract a list of five
posterior probability estimators leveraging well-known $f$-divergences. In
addition, driven by the challenge of improving the state-of-the-art approach,
we propose a bottom-up method that leads us to the formulation of a new
objective function (and posterior probability estimator) corresponding to a
novel $f$-divergence referred to as shifted log (SL). First, we theoretically
prove the convergence property of the posterior probability estimators. Then,
we numerically test the set of proposed objective functions in three
application scenarios: toy examples, image data sets, and signal
detection/decoding problems. The analyzed tasks demonstrate the effectiveness
of the proposed estimators and that the SL divergence achieves the highest
classification accuracy in almost all the scenarios.</div><div><a href='http://arxiv.org/abs/2401.01268v1'>2401.01268v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03071v1")'>On a Neural Implementation of Brenier's Polar Factorization</div>
<div id='2403.03071v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T15:59:54Z</div><div>Authors: Nina Vesseron, Marco Cuturi</div><div style='padding-top: 10px; width: 80ex'>In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for
square matrices -- factored as PSD $\times$ unitary -- to any vector field
$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar
factorization theorem, states that any field $F$ can be recovered as the
composition of the gradient of a convex function $u$ with a measure-preserving
map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of
this far-reaching theoretical result, and explore possible uses within machine
learning. The theorem is closely related to optimal transport (OT) theory, and
we borrow from recent advances in the field of neural optimal transport to
parameterize the potential $u$ as an input convex neural network. The map $M$
can be either evaluated pointwise using $u^*$, the convex conjugate of $u$,
through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary
network. Because $M$ is, in general, not injective, we consider the additional
task of estimating the ill-posed inverse map that can approximate the pre-image
measure $M^{-1}$ using a stochastic generator. We illustrate possible
applications of \citeauthor{Brenier1991PolarFA}'s polar factorization to
non-convex optimization problems, as well as sampling of densities that are not
log-concave.</div><div><a href='http://arxiv.org/abs/2403.03071v1'>2403.03071v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10809v2")'>Neglected Hessian component explains mysteries in Sharpness
  regularization</div>
<div id='2401.10809v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:52:53Z</div><div>Authors: Yann N. Dauphin, Atish Agarwala, Hossein Mobahi</div><div style='padding-top: 10px; width: 80ex'>Recent work has shown that methods like SAM which either explicitly or
implicitly penalize second order information can improve generalization in deep
learning. Seemingly similar methods like weight noise and gradient penalties
often fail to provide such benefits. We show that these differences can be
explained by the structure of the Hessian of the loss. First, we show that a
common decomposition of the Hessian can be quantitatively interpreted as
separating the feature exploitation from feature exploration. The feature
exploration, which can be described by the Nonlinear Modeling Error matrix
(NME), is commonly neglected in the literature since it vanishes at
interpolation. Our work shows that the NME is in fact important as it can
explain why gradient penalties are sensitive to the choice of activation
function. Using this insight we design interventions to improve performance. We
also provide evidence that challenges the long held equivalence of weight noise
and gradient penalties. This equivalence relies on the assumption that the NME
can be ignored, which we find does not hold for modern networks since they
involve significant feature learning. We find that regularizing feature
exploitation but not feature exploration yields performance similar to gradient
penalties.</div><div><a href='http://arxiv.org/abs/2401.10809v2'>2401.10809v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03021v2")'>Data-induced multiscale losses and efficient multirate gradient descent
  schemes</div>
<div id='2402.03021v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T14:00:53Z</div><div>Authors: Juncai He, Liangchen Liu, Yen-Hsi Richard Tsai</div><div style='padding-top: 10px; width: 80ex'>This paper investigates the impact of multiscale data on machine learning
algorithms, particularly in the context of deep learning. A dataset is
multiscale if its distribution shows large variations in scale across different
directions. This paper reveals multiscale structures in the loss landscape,
including its gradients and Hessians inherited from the data. Correspondingly,
it introduces a novel gradient descent approach, drawing inspiration from
multiscale algorithms used in scientific computing. This approach seeks to
transcend empirical learning rate selection, offering a more systematic,
data-informed strategy to enhance training efficiency, especially in the later
stages.</div><div><a href='http://arxiv.org/abs/2402.03021v2'>2402.03021v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12646v1")'>Training Artificial Neural Networks by Coordinate Search Algorithm</div>
<div id='2402.12646v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T01:47:25Z</div><div>Authors: Ehsan Rokhsatyazdi, Shahryar Rahnamayan, Sevil Zanjani Miyandoab, Azam Asilian Bidgoli, H. R. Tizhoosh</div><div style='padding-top: 10px; width: 80ex'>Training Artificial Neural Networks poses a challenging and critical problem
in machine learning. Despite the effectiveness of gradient-based learning
methods, such as Stochastic Gradient Descent (SGD), in training neural
networks, they do have several limitations. For instance, they require
differentiable activation functions, and cannot optimize a model based on
several independent non-differentiable loss functions simultaneously; for
example, the F1-score, which is used during testing, can be used during
training when a gradient-free optimization algorithm is utilized. Furthermore,
the training in any DNN can be possible with a small size of the training
dataset. To address these concerns, we propose an efficient version of the
gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern
Search methods, for training neural networks. The proposed algorithm can be
used with non-differentiable activation functions and tailored to
multi-objective/multi-loss problems. Finding the optimal values for weights of
ANNs is a large-scale optimization problem. Therefore instead of finding the
optimal value for each variable, which is the common technique in classical CS,
we accelerate optimization and convergence by bundling the weights. In fact,
this strategy is a form of dimension reduction for optimization problems. Based
on the experimental results, the proposed method, in some cases, outperforms
the gradient-based approach, particularly, in situations with insufficient
labeled training data. The performance plots demonstrate a high convergence
rate, highlighting the capability of our suggested method to find a reasonable
solution with fewer function calls. As of now, the only practical and efficient
way of training ANNs with hundreds of thousands of weights is gradient-based
algorithms such as SGD or Adam. In this paper we introduce an alternative
method for training ANN.</div><div><a href='http://arxiv.org/abs/2402.12646v1'>2402.12646v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10964v2")'>Optimal feature rescaling in machine learning based on neural networks</div>
<div id='2402.10964v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T21:57:31Z</div><div>Authors: Federico Maria Vitrò, Marco Leonesio, Lorenzo Fagiano</div><div style='padding-top: 10px; width: 80ex'>This paper proposes a novel approach to improve the training efficiency and
the generalization performance of Feed Forward Neural Networks (FFNNs)
resorting to an optimal rescaling of input features (OFR) carried out by a
Genetic Algorithm (GA). The OFR reshapes the input space improving the
conditioning of the gradient-based algorithm used for the training. Moreover,
the scale factors exploration entailed by GA trials and selection corresponds
to different initialization of the first layer weights at each training
attempt, thus realizing a multi-start global search algorithm (even though
restrained to few weights only) which fosters the achievement of a global
minimum. The approach has been tested on a FFNN modeling the outcome of a real
industrial process (centerless grinding).</div><div><a href='http://arxiv.org/abs/2402.10964v2'>2402.10964v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02454v1")'>On the Role of Initialization on the Implicit Bias in Deep Linear
  Networks</div>
<div id='2402.02454v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T11:54:07Z</div><div>Authors: Oria Gruber, Haim Avron</div><div style='padding-top: 10px; width: 80ex'>Despite Deep Learning's (DL) empirical success, our theoretical understanding
of its efficacy remains limited. One notable paradox is that while conventional
wisdom discourages perfect data fitting, deep neural networks are designed to
do just that, yet they generalize effectively. This study focuses on exploring
this phenomenon attributed to the implicit bias at play. Various sources of
implicit bias have been identified, such as step size, weight initialization,
optimization algorithm, and number of parameters. In this work, we focus on
investigating the implicit bias originating from weight initialization. To this
end, we examine the problem of solving underdetermined linear systems in
various contexts, scrutinizing the impact of initialization on the implicit
regularization when using deep networks to solve such systems. Our findings
elucidate the role of initialization in the optimization and generalization
paradoxes, contributing to a more comprehensive understanding of DL's
performance characteristics.</div><div><a href='http://arxiv.org/abs/2402.02454v1'>2402.02454v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16184v1")'>Deep Neural Network Initialization with Sparsity Inducing Activations</div>
<div id='2402.16184v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-25T20:11:40Z</div><div>Authors: Ilan Price, Nicholas Daultry Ball, Samuel C. H. Lam, Adam C. Jones, Jared Tanner</div><div style='padding-top: 10px; width: 80ex'>Inducing and leveraging sparse activations during training and inference is a
promising avenue for improving the computational efficiency of deep networks,
which is increasingly important as network sizes continue to grow and their
application becomes more widespread. Here we use the large width Gaussian
process limit to analyze the behaviour, at random initialization, of nonlinear
activations that induce sparsity in the hidden outputs. A previously unreported
form of training instability is proven for arguably two of the most natural
candidates for hidden layer sparsification; those being a shifted ReLU
($\phi(x)=\max(0, x-\tau)$ for $\tau\ge 0$) and soft thresholding ($\phi(x)=0$
for $|x|\le\tau$ and $x-\text{sign}(x)\tau$ for $|x|&gt;\tau$). We show that this
instability is overcome by clipping the nonlinear activation magnitude, at a
level prescribed by the shape of the associated Gaussian process variance map.
Numerical experiments verify the theory and show that the proposed magnitude
clipped sparsifying activations can be trained with training and test
fractional sparsity as high as 85\% while retaining close to full accuracy.</div><div><a href='http://arxiv.org/abs/2402.16184v1'>2402.16184v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.08269v1")'>Geometry-induced Implicit Regularization in Deep ReLU Neural Networks</div>
<div id='2402.08269v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-13T07:49:57Z</div><div>Authors: Joachim Bona-Pellissier, Fran çois Malgouyres, Fran çois Bachoc</div><div style='padding-top: 10px; width: 80ex'>It is well known that neural networks with many more parameters than training
examples do not overfit. Implicit regularization phenomena, which are still not
well understood, occur during optimization and 'good' networks are favored.
Thus the number of parameters is not an adequate measure of complexity if we do
not consider all possible networks but only the 'good' ones. To better
understand which networks are favored during optimization, we study the
geometry of the output set as parameters vary. When the inputs are fixed, we
prove that the dimension of this set changes and that the local dimension,
called batch functional dimension, is almost surely determined by the
activation patterns in the hidden layers. We prove that the batch functional
dimension is invariant to the symmetries of the network parameterization:
neuron permutations and positive rescalings. Empirically, we establish that the
batch functional dimension decreases during optimization. As a consequence,
optimization leads to parameters with low batch functional dimensions. We call
this phenomenon geometry-induced implicit regularization.The batch functional
dimension depends on both the network parameters and inputs. To understand the
impact of the inputs, we study, for fixed parameters, the largest attainable
batch functional dimension when the inputs vary. We prove that this quantity,
called computable full functional dimension, is also invariant to the
symmetries of the network's parameterization, and is determined by the
achievable activation patterns. We also provide a sampling theorem, showing a
fast convergence of the estimation of the computable full functional dimension
for a random input of increasing size. Empirically we find that the computable
full functional dimension remains close to the number of parameters, which is
related to the notion of local identifiability. This differs from the observed
values for the batch functional dimension computed on training inputs and test
inputs. The latter are influenced by geometry-induced implicit regularization.</div><div><a href='http://arxiv.org/abs/2402.08269v1'>2402.08269v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11397v1")'>Random Projection Neural Networks of Best Approximation: Convergence
  theory and practical applications</div>
<div id='2402.11397v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T22:40:22Z</div><div>Authors: Gianluca Fabiani</div><div style='padding-top: 10px; width: 80ex'>We investigate the concept of Best Approximation for Feedforward Neural
Networks (FNN) and explore their convergence properties through the lens of
Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for
all, internal weights and biases, offering computational efficiency. We
demonstrate that there exists a choice of external weights, for any family of
such RPNNs, with non-polynomial infinitely differentiable activation functions,
that exhibit an exponential convergence rate when approximating any infinitely
differentiable function. For illustration purposes, we test the proposed
RPNN-based function approximation, with parsimoniously chosen basis functions,
across five benchmark function approximation problems. Results show that RPNNs
achieve comparable performance to established methods such as Legendre
Polynomials, highlighting their potential for efficient and accurate function
approximation.</div><div><a href='http://arxiv.org/abs/2402.11397v1'>2402.11397v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15715v1")'>Operator Learning: Algorithms and Analysis</div>
<div id='2402.15715v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T04:40:27Z</div><div>Authors: Nikola B. Kovachki, Samuel Lanthaler, Andrew M. Stuart</div><div style='padding-top: 10px; width: 80ex'>Operator learning refers to the application of ideas from machine learning to
approximate (typically nonlinear) operators mapping between Banach spaces of
functions. Such operators often arise from physical models expressed in terms
of partial differential equations (PDEs). In this context, such approximate
operators hold great potential as efficient surrogate models to complement
traditional numerical methods in many-query tasks. Being data-driven, they also
enable model discovery when a mathematical description in terms of a PDE is not
available. This review focuses primarily on neural operators, built on the
success of deep neural networks in the approximation of functions defined on
finite dimensional Euclidean spaces. Empirically, neural operators have shown
success in a variety of applications, but our theoretical understanding remains
incomplete. This review article summarizes recent progress and the current
state of our theoretical understanding of neural operators, focusing on an
approximation theoretic point of view.</div><div><a href='http://arxiv.org/abs/2402.15715v1'>2402.15715v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09516v2")'>Accelerating Data Generation for Neural Operators via Krylov Subspace
  Recycling</div>
<div id='2401.09516v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T16:20:12Z</div><div>Authors: Hong Wang, Zhongkai Hao, Jie Wang, Zijie Geng, Zhen Wang, Bin Li, Feng Wu</div><div style='padding-top: 10px; width: 80ex'>Learning neural operators for solving partial differential equations (PDEs)
has attracted great attention due to its high inference efficiency. However,
training such operators requires generating a substantial amount of labeled
data, i.e., PDE problems together with their solutions. The data generation
process is exceptionally time-consuming, as it involves solving numerous
systems of linear equations to obtain numerical solutions to the PDEs. Many
existing methods solve these systems independently without considering their
inherent similarities, resulting in extremely redundant computations. To tackle
this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR),
to boost the efficiency of solving these systems, thus significantly
accelerating data generation for neural operators training. To the best of our
knowledge, SKR is the first attempt to address the time-consuming nature of
data generation for learning neural operators. The working horse of SKR is
Krylov subspace recycling, a powerful technique for solving a series of
interrelated systems by leveraging their inherent similarities. Specifically,
SKR employs a sorting algorithm to arrange these systems in a sequence, where
adjacent systems exhibit high similarities. Then it equips a solver with Krylov
subspace recycling to solve the systems sequentially instead of independently,
thus effectively enhancing the solving efficiency. Both theoretical analysis
and extensive experiments demonstrate that SKR can significantly accelerate
neural operator data generation, achieving a remarkable speedup of up to 13.9
times.</div><div><a href='http://arxiv.org/abs/2401.09516v2'>2401.09516v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12278v1")'>Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices</div>
<div id='2403.12278v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T21:53:56Z</div><div>Authors: Gregory Dexter, Christos Boutsikas, Linkai Ma, Ilse C. F. Ipsen, Petros Drineas</div><div style='padding-top: 10px; width: 80ex'>Motivated by the popularity of stochastic rounding in the context of machine
learning and the training of large-scale deep neural network models, we
consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many
more rows than columns. We provide novel theoretical evidence, supported by
extensive experimental evaluation that, with high probability, the smallest
singular value of a stochastically rounded matrix is well bounded away from
zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and
even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding
\textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that
the rounded version has full column rank. Our proofs leverage powerful results
in random matrix theory, and the idea that stochastic rounding errors do not
concentrate in low-dimensional column spaces.</div><div><a href='http://arxiv.org/abs/2403.12278v1'>2403.12278v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.02681v1")'>SGD with Partial Hessian for Deep Neural Networks Optimization</div>
<div id='2403.02681v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T06:10:21Z</div><div>Authors: Ying Sun, Hongwei Yong, Lei Zhang</div><div style='padding-top: 10px; width: 80ex'>Due to the effectiveness of second-order algorithms in solving classical
optimization problems, designing second-order optimizers to train deep neural
networks (DNNs) has attracted much research interest in recent years. However,
because of the very high dimension of intermediate features in DNNs, it is
difficult to directly compute and store the Hessian matrix for network
optimization. Most of the previous second-order methods approximate the Hessian
information imprecisely, resulting in unstable performance. In this work, we
propose a compound optimizer, which is a combination of a second-order
optimizer with a precise partial Hessian matrix for updating channel-wise
parameters and the first-order stochastic gradient descent (SGD) optimizer for
updating the other parameters. We show that the associated Hessian matrices of
channel-wise parameters are diagonal and can be extracted directly and
precisely from Hessian-free methods. The proposed method, namely SGD with
Partial Hessian (SGD-PH), inherits the advantages of both first-order and
second-order optimizers. Compared with first-order optimizers, it adopts a
certain amount of information from the Hessian matrix to assist optimization,
while compared with the existing second-order optimizers, it keeps the good
generalization performance of first-order optimizers. Experiments on image
classification tasks demonstrate the effectiveness of our proposed optimizer
SGD-PH. The code is publicly available at
\url{https://github.com/myingysun/SGDPH}.</div><div><a href='http://arxiv.org/abs/2403.02681v1'>2403.02681v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11025v1")'>Training Bayesian Neural Networks with Sparse Subspace Variational
  Inference</div>
<div id='2402.11025v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-16T19:15:49Z</div><div>Authors: Junbo Li, Zichen Miao, Qiang Qiu, Ruqi Zhang</div><div style='padding-top: 10px; width: 80ex'>Bayesian neural networks (BNNs) offer uncertainty quantification but come
with the downside of substantially increased training and inference costs.
Sparse BNNs have been investigated for efficient inference, typically by either
slowly introducing sparsity throughout the training or by post-training
compression of dense BNNs. The dilemma of how to cut down massive training
costs remains, particularly given the requirement to learn about the
uncertainty. To solve this challenge, we introduce Sparse Subspace Variational
Inference (SSVI), the first fully sparse BNN framework that maintains a
consistently highly sparse Bayesian model throughout the training and inference
phases. Starting from a randomly initialized low-dimensional sparse subspace,
our approach alternately optimizes the sparse subspace basis selection and its
associated parameters. While basis selection is characterized as a
non-differentiable problem, we approximate the optimal solution with a
removal-and-addition strategy, guided by novel criteria based on weight
distribution statistics. Our extensive experiments show that SSVI sets new
benchmarks in crafting sparse BNNs, achieving, for instance, a 10-20x
compression in model size with under 3\% performance drop, and up to 20x FLOPs
reduction during training compared with dense VI training. Remarkably, SSVI
also demonstrates enhanced robustness to hyperparameters, reducing the need for
intricate tuning in VI and occasionally even surpassing VI-trained dense BNNs
on both accuracy and uncertainty metrics.</div><div><a href='http://arxiv.org/abs/2402.11025v1'>2402.11025v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.12950v1")'>Bayesian Semi-structured Subspace Inference</div>
<div id='2401.12950v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T18:15:58Z</div><div>Authors: Daniel Dold, David Rügamer, Beate Sick, Oliver Dürr</div><div style='padding-top: 10px; width: 80ex'>Semi-structured regression models enable the joint modeling of interpretable
structured and complex unstructured feature effects. The structured model part
is inspired by statistical models and can be used to infer the input-output
relationship for features of particular importance. The complex unstructured
part defines an arbitrary deep neural network and thereby provides enough
flexibility to achieve competitive prediction performance. While these models
can also account for aleatoric uncertainty, there is still a lack of work on
accounting for epistemic uncertainty. In this paper, we address this problem by
presenting a Bayesian approximation for semi-structured regression models using
subspace inference. To this end, we extend subspace inference for joint
posterior sampling from a full parameter space for structured effects and a
subspace for unstructured effects. Apart from this hybrid sampling scheme, our
method allows for tunable complexity of the subspace and can capture multiple
minima in the loss landscape. Numerical experiments validate our approach's
efficacy in recovering structured effect parameter posteriors in
semi-structured models and approaching the full-space posterior distribution of
MCMC for increasing subspace dimension. Further, our approach exhibits
competitive predictive performance across simulated and real-world datasets.</div><div><a href='http://arxiv.org/abs/2401.12950v1'>2401.12950v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.04545v1")'>Improve Generalization Ability of Deep Wide Residual Network with A
  Suitable Scaling Factor</div>
<div id='2403.04545v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T14:40:53Z</div><div>Authors: Songtao Tian, Zixiong Yu</div><div style='padding-top: 10px; width: 80ex'>Deep Residual Neural Networks (ResNets) have demonstrated remarkable success
across a wide range of real-world applications. In this paper, we identify a
suitable scaling factor (denoted by $\alpha$) on the residual branch of deep
wide ResNets to achieve good generalization ability. We show that if $\alpha$
is a constant, the class of functions induced by Residual Neural Tangent Kernel
(RNTK) is asymptotically not learnable, as the depth goes to infinity. We also
highlight a surprising phenomenon: even if we allow $\alpha$ to decrease with
increasing depth $L$, the degeneration phenomenon may still occur. However,
when $\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK
with early stopping can achieve the minimax rate provided that the target
regression function falls in the reproducing kernel Hilbert space associated
with the infinite-depth RNTK. Our simulation studies on synthetic data and real
classification tasks such as MNIST, CIFAR10 and CIFAR100 support our
theoretical criteria for choosing $\alpha$.</div><div><a href='http://arxiv.org/abs/2403.04545v1'>2403.04545v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02478v1")'>Why are hyperbolic neural networks effective? A study on hierarchical
  representation capability</div>
<div id='2402.02478v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T13:15:59Z</div><div>Authors: Shicheng Tan, Huanjing Zhao, Shu Zhao, Yanping Zhang</div><div style='padding-top: 10px; width: 80ex'>Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been
widely applied in recent years, motivated by the existence of an optimal
embedding in hyperbolic space that can preserve data hierarchical relationships
(termed Hierarchical Representation Capability, HRC) more accurately than
Euclidean space. However, there is no evidence to suggest that HNNs can achieve
this theoretical optimal embedding, leading to much research being built on
flawed motivations. In this paper, we propose a benchmark for evaluating HRC
and conduct a comprehensive analysis of why HNNs are effective through
large-scale experiments. Inspired by the analysis results, we propose several
pre-training strategies to enhance HRC and improve the performance of
downstream tasks, further validating the reliability of the analysis.
Experiments show that HNNs cannot achieve the theoretical optimal embedding.
The HRC is significantly affected by the optimization objectives and
hierarchical structures, and enhancing HRC through pre-training strategies can
significantly improve the performance of HNNs.</div><div><a href='http://arxiv.org/abs/2402.02478v1'>2402.02478v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.04553v1")'>Linear Recursive Feature Machines provably recover low-rank matrices</div>
<div id='2401.04553v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T13:44:12Z</div><div>Authors: Adityanarayanan Radhakrishnan, Mikhail Belkin, Dmitriy Drusvyatskiy</div><div style='padding-top: 10px; width: 80ex'>A fundamental problem in machine learning is to understand how neural
networks make accurate predictions, while seemingly bypassing the curse of
dimensionality. A possible explanation is that common training algorithms for
neural networks implicitly perform dimensionality reduction - a process called
feature learning. Recent work posited that the effects of feature learning can
be elicited from a classical statistical estimator called the average gradient
outer product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as
an algorithm that explicitly performs feature learning by alternating between
(1) reweighting the feature vectors by the AGOP and (2) learning the prediction
function in the transformed space. In this work, we develop the first
theoretical guarantees for how RFM performs dimensionality reduction by
focusing on the class of overparametrized problems arising in sparse linear
regression and low-rank matrix recovery. Specifically, we show that RFM
restricted to linear models (lin-RFM) generalizes the well-studied Iteratively
Reweighted Least Squares (IRLS) algorithm. Our results shed light on the
connection between feature learning in neural networks and classical sparse
recovery algorithms. In addition, we provide an implementation of lin-RFM that
scales to matrices with millions of missing entries. Our implementation is
faster than the standard IRLS algorithm as it is SVD-free. It also outperforms
deep linear networks for sparse linear regression and low-rank matrix
completion.</div><div><a href='http://arxiv.org/abs/2401.04553v1'>2401.04553v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03655v1")'>Operator SVD with Neural Networks via Nested Low-Rank Approximation</div>
<div id='2402.03655v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-06T03:06:06Z</div><div>Authors: J. Jon Ryu, Xiangxiang Xu, H. S. Melihcan Erol, Yuheng Bu, Lizhong Zheng, Gregory W. Wornell</div><div style='padding-top: 10px; width: 80ex'>Computing eigenvalue decomposition (EVD) of a given linear operator, or
finding its leading eigenvalues and eigenfunctions, is a fundamental task in
many machine learning and scientific computing problems. For high-dimensional
eigenvalue problems, training neural networks to parameterize the
eigenfunctions is considered as a promising alternative to the classical
numerical linear algebra techniques. This paper proposes a new optimization
framework based on the low-rank approximation characterization of a truncated
singular value decomposition, accompanied by new techniques called nesting for
learning the top-$L$ singular values and singular functions in the correct
order. The proposed method promotes the desired orthogonality in the learned
functions implicitly and efficiently via an unconstrained optimization
formulation, which is easy to solve with off-the-shelf gradient-based
optimization algorithms. We demonstrate the effectiveness of the proposed
optimization framework for use cases in computational physics and machine
learning.</div><div><a href='http://arxiv.org/abs/2402.03655v1'>2402.03655v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17595v1")'>Implicit Regularization via Spectral Neural Networks and Non-linear
  Matrix Sensing</div>
<div id='2402.17595v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T15:28:01Z</div><div>Authors: Hong T. M. Chu, Subhro Ghosh, Chi Thanh Lam, Soumendu Sundar Mukherjee</div><div style='padding-top: 10px; width: 80ex'>The phenomenon of implicit regularization has attracted interest in recent
years as a fundamental aspect of the remarkable generalizing ability of neural
networks. In a nutshell, it entails that gradient descent dynamics in many
neural nets, even without any explicit regularizer in the loss function,
converges to the solution of a regularized learning problem. However, known
results attempting to theoretically explain this phenomenon focus
overwhelmingly on the setting of linear neural nets, and the simplicity of the
linear structure is particularly crucial to existing arguments. In this paper,
we explore this problem in the context of more realistic neural networks with a
general class of non-linear activation functions, and rigorously demonstrate
the implicit regularization phenomenon for such networks in the setting of
matrix sensing problems, together with rigorous rate guarantees that ensure
exponentially fast convergence of gradient descent.In this vein, we contribute
a network architecture called Spectral Neural Networks (abbrv. SNN) that is
particularly suitable for matrix learning problems. Conceptually, this entails
coordinatizing the space of matrices by their singular values and singular
vectors, as opposed to by their entries, a potentially fruitful perspective for
matrix learning. We demonstrate that the SNN architecture is inherently much
more amenable to theoretical analysis than vanilla neural nets and confirm its
effectiveness in the context of matrix sensing, via both mathematical
guarantees and empirical investigations. We believe that the SNN architecture
has the potential to be of wide applicability in a broad class of matrix
learning scenarios.</div><div><a href='http://arxiv.org/abs/2402.17595v1'>2402.17595v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10232v1")'>Matrix Completion via Nonsmooth Regularization of Fully Connected Neural
  Networks</div>
<div id='2403.10232v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T12:00:37Z</div><div>Authors: Sajad Faramarzi, Farzan Haddadi, Sajjad Amini, Masoud Ahookhosh</div><div style='padding-top: 10px; width: 80ex'>Conventional matrix completion methods approximate the missing values by
assuming the matrix to be low-rank, which leads to a linear approximation of
missing values. It has been shown that enhanced performance could be attained
by using nonlinear estimators such as deep neural networks. Deep fully
connected neural networks (FCNNs), one of the most suitable architectures for
matrix completion, suffer from over-fitting due to their high capacity, which
leads to low generalizability. In this paper, we control over-fitting by
regularizing the FCNN model in terms of the $\ell_{1}$ norm of intermediate
representations and nuclear norm of weight matrices. As such, the resulting
regularized objective function becomes nonsmooth and nonconvex, i.e., existing
gradient-based methods cannot be applied to our model. We propose a variant of
the proximal gradient method and investigate its convergence to a critical
point. In the initial epochs of FCNN training, the regularization terms are
ignored, and through epochs, the effect of that increases. The gradual addition
of nonsmooth regularization terms is the main reason for the better performance
of the deep neural network with nonsmooth regularization terms (DNN-NSR)
algorithm. Our simulations indicate the superiority of the proposed algorithm
in comparison with existing linear and nonlinear algorithms.</div><div><a href='http://arxiv.org/abs/2403.10232v1'>2403.10232v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01344v2")'>Monotone, Bi-Lipschitz, and Polyak-Lojasiewicz Networks</div>
<div id='2402.01344v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T12:02:42Z</div><div>Authors: Ruigang Wang, Krishnamurthy Dvijotham, Ian R. Manchester</div><div style='padding-top: 10px; width: 80ex'>This paper presents a new \emph{bi-Lipschitz} invertible neural network, the
BiLipNet, which has the ability to control both its \emph{Lipschitzness}
(output sensitivity to input perturbations) and \emph{inverse Lipschitzness}
(input distinguishability from different outputs). The main contribution is a
novel invertible residual layer with certified strong monotonicity and
Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz
networks. The certification is based on incremental quadratic constraints,
which achieves much tighter bounds compared to spectral normalization.
Moreover, we formulate the model inverse calculation as a three-operator
splitting problem, for which fast algorithms are known. Based on the proposed
bi-Lipschitz network, we introduce a new scalar-output network, the PLNet,
which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn
non-convex surrogate losses with favourable properties, e.g., a unique and
efficiently-computable global minimum.</div><div><a href='http://arxiv.org/abs/2402.01344v2'>2402.01344v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01339v1")'>Uniform $\mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric
  Functions, Embedding Dimensions, and Polynomial Representations</div>
<div id='2403.01339v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T23:19:10Z</div><div>Authors: Soumya Ganguly, Khoa Tran, Rahul Sarkar</div><div style='padding-top: 10px; width: 80ex'>For any subgroup $G$ of the symmetric group $\mathcal{S}_n$ on $n$ symbols,
we present results for the uniform $\mathcal{C}^k$ approximation of
$G$-invariant functions by $G$-invariant polynomials. For the case of totally
symmetric functions ($G = \mathcal{S}_n$), we show that this gives rise to the
sum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both the
inner and outer functions can be chosen to be smooth, and moreover, the inner
function can be chosen to be independent of the target function being
approximated. In particular, we show that the embedding dimension required is
independent of the regularity of the target function, the accuracy of the
desired approximation, as well as $k$. Next, we show that a similar procedure
allows us to obtain a uniform $\mathcal{C}^k$ approximation of antisymmetric
functions as a sum of $K$ terms, where each term is a product of a smooth
totally symmetric function and a smooth antisymmetric homogeneous polynomial of
degree at most $\binom{n}{2}$. We also provide upper and lower bounds on $K$
and show that $K$ is independent of the regularity of the target function, the
desired approximation accuracy, and $k$.</div><div><a href='http://arxiv.org/abs/2403.01339v1'>2403.01339v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.07027v1")'>Quantum Speedup for Spectral Approximation of Kronecker Products</div>
<div id='2402.07027v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-10T19:21:29Z</div><div>Authors: Yeqi Gao, Zhao Song, Ruizhe Zhang</div><div style='padding-top: 10px; width: 80ex'>Given its widespread application in machine learning and optimization, the
Kronecker product emerges as a pivotal linear algebra operator. However, its
computational demands render it an expensive operation, leading to heightened
costs in spectral approximation of it through traditional computation
algorithms. Existing classical methods for spectral approximation exhibit a
linear dependency on the matrix dimension denoted by $n$, considering matrices
of size $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times
d}$. Our work introduces an innovative approach to efficiently address the
spectral approximation of the Kronecker product $A_1 \otimes A_2$ using quantum
methods. By treating matrices as quantum states, our proposed method
significantly reduces the time complexity of spectral approximation to
$O_{d,\epsilon}(\sqrt{n})$.</div><div><a href='http://arxiv.org/abs/2402.07027v1'>2402.07027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.02442v1")'>A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix
  Decomposition</div>
<div id='2402.02442v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:43:35Z</div><div>Authors: Qingsong Wang, Chunfeng Cui, Deren Han</div><div style='padding-top: 10px; width: 80ex'>Recently, there has been a growing interest in the exploration of Nonlinear
Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims
to find a low-rank matrix from a sparse nonnegative matrix with a per-element
nonlinear function. A typical choice is the Rectified Linear Unit (ReLU)
activation function. To address over-fitting in the existing ReLU-based NMD
model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to
as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for
handling the ReLU-NMD-T model. A distinctive feature, setting our work apart
from most existing studies, is the incorporation of both positive and negative
momentum parameters in our algorithm. Our numerical experiments on real-world
datasets show the effectiveness of the proposed model and algorithm. Moreover,
the code is available at https://github.com/nothing2wang/NMD-TM.</div><div><a href='http://arxiv.org/abs/2402.02442v1'>2402.02442v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.14398v1")'>Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact
  Subproblem Solver for Training Structured Neural Network</div>
<div id='2403.14398v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-21T13:43:49Z</div><div>Authors: Zih-Syuan Huang, Ching-pei Lee</div><div style='padding-top: 10px; width: 80ex'>We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm
for training structured neural networks. Similar to existing regularized
adaptive methods, the subproblem for computing the update direction of RAMDA
involves a nonsmooth regularizer and a diagonal preconditioner, and therefore
does not possess a closed-form solution in general. We thus also carefully
devise an implementable inexactness condition that retains convergence
guarantees similar to the exact versions, and propose a companion efficient
solver for the subproblems of both RAMDA and existing methods to make them
practically feasible. We leverage the theory of manifold identification in
variational analysis to show that, even in the presence of such inexactness,
the iterates of RAMDA attain the ideal structure induced by the regularizer at
the stationary point of asymptotic convergence. This structure is locally
optimal near the point of convergence, so RAMDA is guaranteed to obtain the
best structure possible among all methods converging to the same point, making
it the first regularized adaptive method outputting models that possess
outstanding predictive performance while being (locally) optimally structured.
Extensive numerical experiments in large-scale modern computer vision, language
modeling, and speech tasks show that the proposed RAMDA is efficient and
consistently outperforms state of the art for training structured neural
network. Implementation of our algorithm is available at
http://www.github.com/ismoptgroup/RAMDA/.</div><div><a href='http://arxiv.org/abs/2403.14398v1'>2403.14398v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05809v1")'>Shallow ReLU neural networks and finite elements</div>
<div id='2403.05809v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T06:12:06Z</div><div>Authors: Pengzhan Jin</div><div style='padding-top: 10px; width: 80ex'>We point out that (continuous or discontinuous) piecewise linear functions on
a convex polytope mesh can be represented by two-hidden-layer ReLU neural
networks in a weak sense. In addition, the numbers of neurons of the two hidden
layers required to weakly represent are accurately given based on the numbers
of polytopes and hyperplanes involved in this mesh. The results naturally hold
for constant and linear finite element functions. Such weak representation
establishes a bridge between shallow ReLU neural networks and finite element
functions, and leads to a perspective for analyzing approximation capability of
ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we
discuss the strict representation for tensor finite element functions via the
recent tensor neural networks.</div><div><a href='http://arxiv.org/abs/2403.05809v1'>2403.05809v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13652v3")'>Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity
  Detectors</div>
<div id='2401.13652v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-24T18:44:14Z</div><div>Authors: Francesco Della Santa, Sandra Pieraccini</div><div style='padding-top: 10px; width: 80ex'>In this paper, we present a novel approach for detecting the discontinuity
interfaces of a discontinuous function. This approach leverages Graph-Informed
Neural Networks (GINNs) and sparse grids to address discontinuity detection
also in domains of dimension larger than 3. GINNs, trained to identify troubled
points on sparse grids, exploit graph structures built on the grids to achieve
efficient and accurate discontinuity detection performances. We also introduce
a recursive algorithm for general sparse grid-based detectors, characterized by
convergence properties and easy applicability. Numerical experiments on
functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust
generalization of GINNs in detecting discontinuity interfaces. Notably, the
trained GINNs offer portability and versatility, allowing integration into
various algorithms and sharing among users.</div><div><a href='http://arxiv.org/abs/2401.13652v3'>2401.13652v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.03158v1")'>Optimal and Near-Optimal Adaptive Vector Quantization</div>
<div id='2402.03158v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T16:27:59Z</div><div>Authors: Ran Ben-Basat, Yaniv Ben-Itzhak, Michael Mitzenmacher, Shay Vargaftik</div><div style='padding-top: 10px; width: 80ex'>Quantization is a fundamental optimization for many machine-learning use
cases, including compressing gradients, model weights and activations, and
datasets. The most accurate form of quantization is \emph{adaptive}, where the
error is minimized with respect to a given input, rather than optimizing for
the worst case. However, optimal adaptive quantization methods are considered
infeasible in terms of both their runtime and memory requirements.
  We revisit the Adaptive Vector Quantization (AVQ) problem and present
algorithms that find optimal solutions with asymptotically improved time and
space complexity. We also present an even faster near-optimal algorithm for
large inputs. Our experiments show our algorithms may open the door to using
AVQ more extensively in a variety of machine learning applications.</div><div><a href='http://arxiv.org/abs/2402.03158v1'>2402.03158v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08281v1")'>The Faiss library</div>
<div id='2401.08281v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T11:12:36Z</div><div>Authors: Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou</div><div style='padding-top: 10px; width: 80ex'>Vector databases manage large collections of embedding vectors. As AI
applications are growing rapidly, so are the number of embeddings that need to
be stored and indexed. The Faiss library is dedicated to vector similarity
search, a core functionality of vector databases. Faiss is a toolkit of
indexing methods and related primitives used to search, cluster, compress and
transform vectors. This paper first describes the tradeoff space of vector
search, then the design principles of Faiss in terms of structure, approach to
optimization and interfacing. We benchmark key features of the library and
discuss a few selected applications to highlight its broad applicability.</div><div><a href='http://arxiv.org/abs/2401.08281v1'>2401.08281v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02044v1")'>Locally-Adaptive Quantization for Streaming Vector Search</div>
<div id='2402.02044v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T05:43:39Z</div><div>Authors: Cecilia Aguerrebere, Mark Hildebrand, Ishwar Singh Bhati, Theodore Willke, Mariano Tepper</div><div style='padding-top: 10px; width: 80ex'>Retrieving the most similar vector embeddings to a given query among a
massive collection of vectors has long been a key component of countless
real-world applications. The recently introduced Retrieval-Augmented Generation
is one of the most prominent examples. For many of these applications, the
database evolves over time by inserting new data and removing outdated data. In
these cases, the retrieval problem is known as streaming similarity search.
While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector
compression method, yields state-of-the-art search performance for non-evolving
databases, its usefulness in the streaming setting has not been yet
established. In this work, we study LVQ in streaming similarity search. In
support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and
multi-means LVQ that boost its search performance by up to 28% and 27%,
respectively. Our studies show that LVQ and its new variants enable blazing
fast vector search, outperforming its closest competitor by up to 9.4x for
identically distributed data and by up to 8.8x under the challenging scenario
of data distribution shifts (i.e., where the statistical distribution of the
data changes over time). We release our contributions as part of Scalable
Vector Search, an open-source library for high-performance similarity search.</div><div><a href='http://arxiv.org/abs/2402.02044v1'>2402.02044v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.11215v1")'>Selecting Walk Schemes for Database Embedding</div>
<div id='2401.11215v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-20T11:39:32Z</div><div>Authors: Yuval Lev Lubarsky, Jan Tönshoff, Martin Grohe, Benny Kimelfeld</div><div style='padding-top: 10px; width: 80ex'>Machinery for data analysis often requires a numeric representation of the
input. Towards that, a common practice is to embed components of structured
data into a high-dimensional vector space. We study the embedding of the tuples
of a relational database, where existing techniques are often based on
optimization tasks over a collection of random walks from the database. The
focus of this paper is on the recent FoRWaRD algorithm that is designed for
dynamic databases, where walks are sampled by following foreign keys between
tuples. Importantly, different walks have different schemas, or "walk schemes",
that are derived by listing the relations and attributes along the walk. Also
importantly, different walk schemes describe relationships of different natures
in the database. We show that by focusing on a few informative walk schemes, we
can obtain tuple embedding significantly faster, while retaining the quality.
We define the problem of scheme selection for tuple embedding, devise several
approaches and strategies for scheme selection, and conduct a thorough
empirical study of the performance over a collection of downstream tasks. Our
results confirm that with effective strategies for scheme selection, we can
obtain high-quality embeddings considerably (e.g., three times) faster,
preserve the extensibility to newly inserted tuples, and even achieve an
increase in the precision of some tasks.</div><div><a href='http://arxiv.org/abs/2401.11215v1'>2401.11215v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.05440v1")'>Is Cosine-Similarity of Embeddings Really About Similarity?</div>
<div id='2403.05440v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T16:48:20Z</div><div>Authors: Harald Steck, Chaitanya Ekanadham, Nathan Kallus</div><div style='padding-top: 10px; width: 80ex'>Cosine-similarity is the cosine of the angle between two vectors, or
equivalently the dot product between their normalizations. A popular
application is to quantify semantic similarity between high-dimensional objects
by applying cosine-similarity to a learned low-dimensional feature embedding.
This can work better but sometimes also worse than the unnormalized dot-product
between embedded vectors in practice. To gain insight into this empirical
observation, we study embeddings derived from regularized linear models, where
closed-form solutions facilitate analytical insights. We derive analytically
how cosine-similarity can yield arbitrary and therefore meaningless
`similarities.' For some linear models the similarities are not even unique,
while for others they are implicitly controlled by the regularization. We
discuss implications beyond linear models: a combination of different
regularizations are employed when learning deep models; these have implicit and
unintended effects when taking cosine-similarities of the resulting embeddings,
rendering results opaque and possibly arbitrary. Based on these insights, we
caution against blindly using cosine-similarity and outline alternatives.</div><div><a href='http://arxiv.org/abs/2403.05440v1'>2403.05440v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13708v1")'>Accelerating hyperbolic t-SNE</div>
<div id='2401.13708v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T12:59:40Z</div><div>Authors: Martin Skrodzki, Hunter van Geffen, Nicolas F. Chaves-de-Plaza, Thomas Höllt, Elmar Eisemann, Klaus Hildebrandt</div><div style='padding-top: 10px; width: 80ex'>The need to understand the structure of hierarchical or high-dimensional data
is present in a variety of fields. Hyperbolic spaces have proven to be an
important tool for embedding computations and analysis tasks as their
non-linear nature lends itself well to tree or graph data. Subsequently, they
have also been used in the visualization of high-dimensional data, where they
exhibit increased embedding performance. However, none of the existing
dimensionality reduction methods for embedding into hyperbolic spaces scale
well with the size of the input data. That is because the embeddings are
computed via iterative optimization schemes and the computation cost of every
iteration is quadratic in the size of the input. Furthermore, due to the
non-linear nature of hyperbolic spaces, Euclidean acceleration structures
cannot directly be translated to the hyperbolic setting. This paper introduces
the first acceleration structure for hyperbolic embeddings, building upon a
polar quadtree. We compare our approach with existing methods and demonstrate
that it computes embeddings of similar quality in significantly less time.
Implementation and scripts for the experiments can be found at
https://graphics.tudelft.nl/accelerating-hyperbolic-tsne.</div><div><a href='http://arxiv.org/abs/2401.13708v1'>2401.13708v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09524v1")'>Guided Quantum Compression for Higgs Identification</div>
<div id='2402.09524v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-14T19:01:51Z</div><div>Authors: Vasilis Belis, Patrick Odagiu, Michele Grossi, Florentin Reiter, Günther Dissertori, Sofia Vallecorsa</div><div style='padding-top: 10px; width: 80ex'>Quantum machine learning provides a fundamentally novel and promising
approach to analyzing data. However, many data sets are too complex for
currently available quantum computers. Consequently, quantum machine learning
applications conventionally resort to dimensionality reduction algorithms,
e.g., auto-encoders, before passing data through the quantum models. We show
that using a classical auto-encoder as an independent preprocessing step can
significantly decrease the classification performance of a quantum machine
learning algorithm. To ameliorate this issue, we design an architecture that
unifies the preprocessing and quantum classification algorithms into a single
trainable model: the guided quantum compression model. The utility of this
model is demonstrated by using it to identify the Higgs boson in proton-proton
collisions at the LHC, where the conventional approach proves ineffective.
Conversely, the guided quantum compression model excels at solving this
classification problem, achieving a good accuracy. Additionally, the model
developed herein shows better performance compared to the classical benchmark
when using only low-level kinematic features.</div><div><a href='http://arxiv.org/abs/2402.09524v1'>2402.09524v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.11261v1")'>A Lie Group Approach to Riemannian Batch Normalization</div>
<div id='2403.11261v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T16:24:07Z</div><div>Authors: Ziheng Chen, Yue Song, Yunmei Liu, Nicu Sebe</div><div style='padding-top: 10px; width: 80ex'>Manifold-valued measurements exist in numerous applications within computer
vision and machine learning. Recent studies have extended Deep Neural Networks
(DNNs) to manifolds, and concomitantly, normalization techniques have also been
adapted to several manifolds, referred to as Riemannian normalization.
Nonetheless, most of the existing Riemannian normalization methods have been
derived in an ad hoc manner and only apply to specific manifolds. This paper
establishes a unified framework for Riemannian Batch Normalization (RBN)
techniques on Lie groups. Our framework offers the theoretical guarantee of
controlling both the Riemannian mean and variance. Empirically, we focus on
Symmetric Positive Definite (SPD) manifolds, which possess three distinct types
of Lie group structures. Using the deformation concept, we generalize the
existing Lie groups on SPD manifolds into three families of parameterized Lie
groups. Specific normalization layers induced by these Lie groups are then
proposed for SPD neural networks. We demonstrate the effectiveness of our
approach through three sets of experiments: radar recognition, human action
recognition, and electroencephalography (EEG) classification. The code is
available at https://github.com/GitZH-Chen/LieBN.git.</div><div><a href='http://arxiv.org/abs/2403.11261v1'>2403.11261v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05645v1")'>Geometric Neural Network based on Phase Space for BCI decoding</div>
<div id='2403.05645v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T19:36:20Z</div><div>Authors: Igor Carrara, Bruno Aristimunha, Marie-Constance Corsi, Raphael Y. de Camargo, Sylvain Chevallier, Théodore Papadopoulo</div><div style='padding-top: 10px; width: 80ex'>The integration of Deep Learning (DL) algorithms on brain signal analysis is
still in its nascent stages compared to their success in fields like Computer
Vision, especially in Brain-Computer Interface (BCI), where the brain activity
is decoded to control external devices without requiring muscle control.
Electroencephalography (EEG) is a widely adopted choice for designing BCI
systems due to its non-invasive and cost-effective nature and excellent
temporal resolution. Still, it comes at the expense of limited training data,
poor signal-to-noise, and a large variability across and within-subject
recordings. Finally, setting up a BCI system with many electrodes takes a long
time, hindering the widespread adoption of reliable DL architectures in BCIs
outside research laboratories. To improve adoption, we need to improve user
comfort using, for instance, reliable algorithms that operate with few
electrodes. \textbf{Approach:} Our research aims to develop a DL algorithm that
delivers effective results with a limited number of electrodes. Taking
advantage of the Augmented Covariance Method with SPDNet, we propose the
SPDNet$_{\psi}$ architecture and analyze its performance and computational
impact, as well as the interpretability of the results. The evaluation is
conducted on 5-fold cross-validation, using only three electrodes positioned
above the Motor Cortex. The methodology was tested on nearly 100 subjects from
several open-source datasets using the Mother Of All BCI Benchmark (MOABB)
framework. \textbf{Main results:} The results of our SPDNet$_{\psi}$
demonstrate that the augmented approach combined with the SPDNet significantly
outperforms all the current state-of-the-art DL architecture in MI decoding.
\textbf{Significance:} This new architecture is explainable, with a low number
of trainable parameters and a reduced carbon footprint.</div><div><a href='http://arxiv.org/abs/2403.05645v1'>2403.05645v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10746v2")'>A Systematic Evaluation of Euclidean Alignment with Deep Learning for
  EEG Decoding</div>
<div id='2401.10746v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T15:13:30Z</div><div>Authors: Bruna Junqueira, Bruno Aristimunha, Sylvain Chevallier, Raphael Y. de Camargo</div><div style='padding-top: 10px; width: 80ex'>Electroencephalography (EEG) signals are frequently used for various
Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have
shown promising results, they are hindered by the substantial data
requirements. By leveraging data from multiple subjects, transfer learning
enables more effective training of DL models. A technique that is gaining
popularity is Euclidean Alignment (EA) due to its ease of use, low
computational complexity, and compatibility with Deep Learning models. However,
few studies evaluate its impact on the training performance of shared and
individual DL models. In this work, we systematically evaluate the effect of EA
combined with DL for decoding BCI signals. We used EA to train shared models
with data from multiple subjects and evaluated its transferability to new
subjects. Our experimental results show that it improves decoding in the target
subject by 4.33% and decreases convergence time by more than 70%. We also
trained individual models for each subject to use as a majority-voting ensemble
classifier. In this scenario, using EA improved the 3-model ensemble accuracy
by 3.7%. However, when compared to the shared model with EA, the ensemble
accuracy was 3.62% lower.</div><div><a href='http://arxiv.org/abs/2401.10746v2'>2401.10746v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05446v1")'>Self-supervised Learning for Electroencephalogram: A Systematic Survey</div>
<div id='2401.05446v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T08:59:30Z</div><div>Authors: Weining Weng, Yang Gu, Shuai Guo, Yuan Ma, Zhaohua Yang, Yuchen Liu, Yiqiang Chen</div><div style='padding-top: 10px; width: 80ex'>Electroencephalogram (EEG) is a non-invasive technique to record
bioelectrical signals. Integrating supervised deep learning techniques with EEG
signals has recently facilitated automatic analysis across diverse EEG-based
tasks. However, the label issues of EEG signals have constrained the
development of EEG-based deep models. Obtaining EEG annotations is difficult
that requires domain experts to guide collection and labeling, and the
variability of EEG signals among different subjects causes significant label
shifts. To solve the above challenges, self-supervised learning (SSL) has been
proposed to extract representations from unlabeled samples through
well-designed pretext tasks. This paper concentrates on integrating SSL
frameworks with temporal EEG signals to achieve efficient representation and
proposes a systematic review of the SSL for EEG signals. In this paper, 1) we
introduce the concept and theory of self-supervised learning and typical SSL
frameworks. 2) We provide a comprehensive review of SSL for EEG analysis,
including taxonomy, methodology, and technique details of the existing
EEG-based SSL frameworks, and discuss the difference between these methods. 3)
We investigate the adaptation of the SSL approach to various downstream tasks,
including the task description and related benchmark datasets. 4) Finally, we
discuss the potential directions for future SSL-EEG research.</div><div><a href='http://arxiv.org/abs/2401.05446v1'>2401.05446v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17772v1")'>EEG2Rep: Enhancing Self-supervised EEG Representation Through
  Informative Masked Inputs</div>
<div id='2402.17772v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T05:22:41Z</div><div>Authors: Navid Mohammadi Foumani, Geoffrey Mackellar, Soheila Ghane, Saad Irtza, Nam Nguyen, Mahsa Salehi</div><div style='padding-top: 10px; width: 80ex'>Self-supervised approaches for electroencephalography (EEG) representation
learning face three specific challenges inherent to EEG data: (1) The low
signal-to-noise ratio which challenges the quality of the representation
learned, (2) The wide range of amplitudes from very small to relatively large
due to factors such as the inter-subject variability, risks the models to be
dominated by higher amplitude ranges, and (3) The absence of explicit
segmentation in the continuous-valued sequences which can result in less
informative representations. To address these challenges, we introduce EEG2Rep,
a self-prediction approach for self-supervised representation learning from
EEG. Two core novel components of EEG2Rep are as follows: 1) Instead of
learning to predict the masked input from raw EEG, EEG2Rep learns to predict
masked input in latent representation space, and 2) Instead of conventional
masking methods, EEG2Rep uses a new semantic subsequence preserving (SSP)
method which provides informative masked inputs to guide EEG2Rep to generate
rich semantic representations. In experiments on 6 diverse EEG tasks with
subject variability, EEG2Rep significantly outperforms state-of-the-art
methods. We show that our semantic subsequence preserving improves the existing
masking methods in self-prediction literature and find that preserving 50\% of
EEG recordings will result in the most accurate results on all 6 tasks on
average. Finally, we show that EEG2Rep is robust to noise addressing a
significant challenge that exists in EEG data. Models and code are available
at: https://github.com/Navidfoumani/EEG2Rep</div><div><a href='http://arxiv.org/abs/2402.17772v1'>2402.17772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10278v1")'>EEGFormer: Towards Transferable and Interpretable Large-Scale EEG
  Foundation Model</div>
<div id='2401.10278v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T17:36:24Z</div><div>Authors: Yuqi Chen, Kan Ren, Kaitao Song, Yansen Wang, Yifan Wang, Dongsheng Li, Lili Qiu</div><div style='padding-top: 10px; width: 80ex'>Self-supervised learning has emerged as a highly effective approach in the
fields of natural language processing and computer vision. It is also
applicable to brain signals such as electroencephalography (EEG) data, given
the abundance of available unlabeled data that exist in a wide spectrum of
real-world medical applications ranging from seizure detection to wave
analysis. The existing works leveraging self-supervised learning on EEG
modeling mainly focus on pretraining upon each individual dataset corresponding
to a single downstream task, which cannot leverage the power of abundant data,
and they may derive sub-optimal solutions with a lack of generalization.
Moreover, these methods rely on end-to-end model learning which is not easy for
humans to understand. In this paper, we present a novel EEG foundation model,
namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained
model cannot only learn universal representations on EEG signals with adaptable
performance on various downstream tasks but also provide interpretable outcomes
of the useful patterns within the data. To validate the effectiveness of our
model, we extensively evaluate it on various downstream tasks and assess the
performance under different transfer settings. Furthermore, we demonstrate how
the learned model exhibits transferable anomaly detection performance and
provides valuable interpretability of the acquired patterns via self-supervised
learning.</div><div><a href='http://arxiv.org/abs/2401.10278v1'>2401.10278v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.18006v2")'>EEG-GPT: Exploring Capabilities of Large Language Models for EEG
  Classification and Interpretation</div>
<div id='2401.18006v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T17:08:34Z</div><div>Authors: Jonathan W. Kim, Ahmed Alaa, Danilo Bernardo</div><div style='padding-top: 10px; width: 80ex'>In conventional machine learning (ML) approaches applied to
electroencephalography (EEG), this is often a limited focus, isolating specific
brain activities occurring across disparate temporal scales (from transient
spikes in milliseconds to seizures lasting minutes) and spatial scales (from
localized high-frequency oscillations to global sleep activity). This siloed
approach limits the development EEG ML models that exhibit multi-scale
electrophysiological understanding and classification capabilities. Moreover,
typical ML EEG approaches utilize black-box approaches, limiting their
interpretability and trustworthiness in clinical contexts. Thus, we propose
EEG-GPT, a unifying approach to EEG classification that leverages advances in
large language models (LLM). EEG-GPT achieves excellent performance comparable
to current state-of-the-art deep learning methods in classifying normal from
abnormal EEG in a few-shot learning paradigm utilizing only 2% of training
data. Furthermore, it offers the distinct advantages of providing intermediate
reasoning steps and coordinating specialist EEG tools across multiple scales in
its operation, offering transparent and interpretable step-by-step
verification, thereby promoting trustworthiness in clinical contexts.</div><div><a href='http://arxiv.org/abs/2401.18006v2'>2401.18006v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03276v1")'>ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals
  to Identify Epileptic Seizures</div>
<div id='2403.03276v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T19:15:17Z</div><div>Authors: Salim Rukhsar, Anil Kumar Tiwari</div><div style='padding-top: 10px; width: 80ex'>We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently
applies attention layers along a sequence and has linear complexity with
respect to the sequence length. The proposed model operates on multi-channel
EEG signals rather than single channel signals and leverages parallel
computation. In this cell, the attention layer is a computational unit that
efficiently applies self-attention and cross-attention mechanisms to compute a
recurrent function over a wide number of state vectors and input signals. Our
architecture is inspired in part by the attention layer and long short-term
memory (LSTM) cells, and it uses long-short style gates, but it scales this
typical cell up by several orders to parallelize for multi-channel EEG signals.
It inherits the advantages of attention layers and LSTM gate while avoiding
their respective drawbacks. We evaluated the model effectiveness through
extensive experiments with heterogeneous datasets, including the CHB-MIT and
UPenn and Mayos Clinic, CHB-MIT datasets. The empirical findings suggest that
the ARNN model outperforms baseline methods such as LSTM, Vision Transformer
(ViT), Compact Convolution Transformer (CCT), and R-Transformer (RT),
showcasing superior performance and faster processing capabilities across a
wide range of tasks. The code has been made publicly accessible at
\url{https://github.com/Salim-Lysiun/ARNN}.</div><div><a href='http://arxiv.org/abs/2403.03276v1'>2403.03276v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06027v1")'>Multimodal deep learning approach to predicting neurological recovery
  from coma after cardiac arrest</div>
<div id='2403.06027v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-09T22:29:24Z</div><div>Authors: Felix H. Krones, Ben Walker, Guy Parsons, Terry Lyons, Adam Mahdi</div><div style='padding-top: 10px; width: 80ex'>This work showcases our team's (The BEEGees) contributions to the 2023 George
B. Moody PhysioNet Challenge. The aim was to predict neurological recovery from
coma following cardiac arrest using clinical data and time-series such as
multi-channel EEG and ECG signals. Our modelling approach is multimodal, based
on two-dimensional spectrogram representations derived from numerous EEG
channels, alongside the integration of clinical data and features extracted
directly from EEG recordings. Our submitted model achieved a Challenge score of
$0.53$ on the hidden test set for predictions made $72$ hours after return of
spontaneous circulation. Our study shows the efficacy and limitations of
employing transfer learning in medical classification. With regard to
prospective implementation, our analysis reveals that the performance of the
model is strongly linked to the selection of a decision threshold and exhibits
strong variability across data splits.</div><div><a href='http://arxiv.org/abs/2403.06027v1'>2403.06027v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.10251v3")'>Brant-2: Foundation Model for Brain Signals</div>
<div id='2402.10251v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T16:04:11Z</div><div>Authors: Zhizhang Yuan, Daoze Zhang, Junru Chen, Gefei Gu, Yang Yang</div><div style='padding-top: 10px; width: 80ex'>Foundational models benefit from pre-training on large amounts of unlabeled
data and enable strong performance in a wide variety of applications with a
small amount of labeled data. Such models can be particularly effective in
analyzing brain signals, as this field encompasses numerous application
scenarios, and it is costly to perform large-scale annotation. In this work, we
present the largest foundation model in brain signals, Brant-2. Compared to
Brant, a foundation model designed for intracranial neural signals, Brant-2 not
only exhibits robustness towards data variations and modeling scales but also
can be applied to a broader range of brain neural data. By experimenting on an
extensive range of tasks, we demonstrate that Brant-2 is adaptive to various
application scenarios in brain signals. Further analyses reveal the scalability
of the Brant-2, validate each component's effectiveness, and showcase our
model's ability to maintain performance in scenarios with scarce labels. The
source code and pre-trained weights are available at:
https://github.com/yzz673/Brant-2.</div><div><a href='http://arxiv.org/abs/2402.10251v3'>2402.10251v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09438v1")'>Subject-Independent Deep Architecture for EEG-based Motor Imagery
  Classification</div>
<div id='2402.09438v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-27T23:05:51Z</div><div>Authors: Shadi Sartipi, Mujdat Cetin</div><div style='padding-top: 10px; width: 80ex'>Motor imagery (MI) classification based on electroencephalogram (EEG) is a
widely-used technique in non-invasive brain-computer interface (BCI) systems.
Since EEG recordings suffer from heterogeneity across subjects and labeled data
insufficiency, designing a classifier that performs the MI independently from
the subject with limited labeled samples would be desirable. To overcome these
limitations, we propose a novel subject-independent semi-supervised deep
architecture (SSDA). The proposed SSDA consists of two parts: an unsupervised
and a supervised element. The training set contains both labeled and unlabeled
data samples from multiple subjects. First, the unsupervised part, known as the
columnar spatiotemporal auto-encoder (CST-AE), extracts latent features from
all the training samples by maximizing the similarity between the original and
reconstructed data. A dimensional scaling approach is employed to reduce the
dimensionality of the representations while preserving their discriminability.
Second, a supervised part learns a classifier based on the labeled training
samples using the latent features acquired in the unsupervised part. Moreover,
we employ center loss in the supervised part to minimize the embedding space
distance of each point in a class to its center. The model optimizes both parts
of the network in an end-to-end fashion. The performance of the proposed SSDA
is evaluated on test subjects who were not seen by the model during the
training phase. To assess the performance, we use two benchmark EEG-based MI
task datasets. The results demonstrate that SSDA outperforms state-of-the-art
methods and that a small number of labeled training samples can be sufficient
for strong classification performance.</div><div><a href='http://arxiv.org/abs/2402.09438v1'>2402.09438v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11772v1")'>S-JEPA: towards seamless cross-dataset transfer through dynamic spatial
  attention</div>
<div id='2403.11772v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T13:30:12Z</div><div>Authors: Pierre Guetschel, Thomas Moreau, Michael Tangermann</div><div style='padding-top: 10px; width: 80ex'>Motivated by the challenge of seamless cross-dataset transfer in EEG signal
processing, this article presents an exploratory study on the use of Joint
Embedding Predictive Architectures (JEPAs). In recent years, self-supervised
learning has emerged as a promising approach for transfer learning in various
domains. However, its application to EEG signals remains largely unexplored. In
this article, we introduce Signal-JEPA for representing EEG recordings which
includes a novel domain-specific spatial block masking strategy and three novel
architectures for downstream classification. The study is conducted on a
54~subjects dataset and the downstream performance of the models is evaluated
on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study
provides preliminary evidence for the potential of JEPAs in EEG signal
encoding. Notably, our results highlight the importance of spatial filtering
for accurate downstream classification and reveal an influence of the length of
the pre-training examples but not of the mask size on the downstream
performance.</div><div><a href='http://arxiv.org/abs/2403.11772v1'>2403.11772v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14213v1")'>Contrastive Learning of Shared Spatiotemporal EEG Representations Across
  Individuals for Naturalistic Neuroscience</div>
<div id='2402.14213v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T01:42:12Z</div><div>Authors: Xinke Shen, Lingyi Tao, Xuyang Chen, Sen Song, Quanying Liu, Dan Zhang</div><div style='padding-top: 10px; width: 80ex'>Neural representations induced by naturalistic stimuli offer insights into
how humans respond to peripheral stimuli in daily life. The key to
understanding the general neural mechanisms underlying naturalistic stimuli
processing involves aligning neural activities across individuals and
extracting inter-subject shared neural representations. Targeting the
Electroencephalogram (EEG) technique, known for its rich spatial and temporal
information, this study presents a general framework for Contrastive Learning
of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER).
Harnessing the representational capabilities of contrastive learning, CL-SSTER
utilizes a neural network to maximize the similarity of EEG representations
across individuals for identical stimuli, contrasting with those for varied
stimuli. The network employed spatial and temporal convolutions to
simultaneously learn the spatial and temporal patterns inherent in EEG. The
versatility of CL-SSTER was demonstrated on three EEG datasets, including a
synthetic dataset, a speech audio EEG dataset, and an emotional video EEG
dataset. CL-SSTER attained the highest inter-subject correlation (ISC) values
compared to the state-of-the-art ISC methods. The latent representations
generated by CL-SSTER exhibited reliable spatiotemporal EEG patterns, which can
be explained by specific aspects of the stimuli. CL-SSTER serves as an
interpretable and scalable foundational framework for the identification of
inter-subject shared neural representations in the realm of naturalistic
neuroscience.</div><div><a href='http://arxiv.org/abs/2402.14213v1'>2402.14213v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.02344v1")'>Multi-Source Domain Adaptation with Transformer-based Feature Generation
  for Subject-Independent EEG-based Emotion Recognition</div>
<div id='2401.02344v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-04T16:38:47Z</div><div>Authors: Shadi Sartipi, Mujdat Cetin</div><div style='padding-top: 10px; width: 80ex'>Although deep learning-based algorithms have demonstrated excellent
performance in automated emotion recognition via electroencephalogram (EEG)
signals, variations across brain signal patterns of individuals can diminish
the model's effectiveness when applied across different subjects. While
transfer learning techniques have exhibited promising outcomes, they still
encounter challenges related to inadequate feature representations and may
overlook the fact that source subjects themselves can possess distinct
characteristics. In this work, we propose a multi-source domain adaptation
approach with a transformer-based feature generator (MSDA-TF) designed to
leverage information from multiple sources. The proposed feature generator
retains convolutional layers to capture shallow spatial, temporal, and spectral
EEG data representations, while self-attention mechanisms extract global
dependencies within these features. During the adaptation process, we group the
source subjects based on correlation values and aim to align the moments of the
target subject with each source as well as within the sources. MSDA-TF is
validated on the SEED dataset and is shown to yield promising results.</div><div><a href='http://arxiv.org/abs/2401.02344v1'>2401.02344v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.08851v1")'>Using i-vectors for subject-independent cross-session EEG transfer
  learning</div>
<div id='2401.08851v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T21:56:27Z</div><div>Authors: Jonathan Lasko, Jeff Ma, Mike Nicoletti, Jonathan Sussman-Fort, Sooyoung Jeong, William Hartmann</div><div style='padding-top: 10px; width: 80ex'>Cognitive load classification is the task of automatically determining an
individual's utilization of working memory resources during performance of a
task based on physiologic measures such as electroencephalography (EEG). In
this paper, we follow a cross-disciplinary approach, where tools and
methodologies from speech processing are used to tackle this problem. The
corpus we use was released publicly in 2021 as part of the first passive
brain-computer interface competition on cross-session workload estimation. We
present our approach which used i-vector-based neural network classifiers to
accomplish inter-subject cross-session EEG transfer learning, achieving 18%
relative improvement over equivalent subject-dependent models. We also report
experiments showing how our subject-independent models perform competitively on
held-out subjects and improve with additional subject data, suggesting that
subject-dependent training is not required for effective cognitive load
determination.</div><div><a href='http://arxiv.org/abs/2401.08851v1'>2401.08851v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.18112v2")'>Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS
  to Exclude Abnormal Input</div>
<div id='2402.18112v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T07:02:08Z</div><div>Authors: Zhihao Cao</div><div style='padding-top: 10px; width: 80ex'>Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for
monitoring brain activity. To better understand the brain, researchers often
use deep learning to address the classification challenges of fNIRS data. Our
study shows that while current networks in fNIRS are highly accurate for
predictions within their training distribution, they falter at identifying and
excluding abnormal data which is out-of-distribution, affecting their
reliability. We propose integrating metric learning and supervised methods into
fNIRS research to improve networks capability in identifying and excluding
out-of-distribution outliers. This method is simple yet effective. In our
experiments, it significantly enhances the performance of various networks in
fNIRS, particularly transformer-based one, which shows the great improvement in
reliability. We will make our experiment data available on GitHub.</div><div><a href='http://arxiv.org/abs/2402.18112v2'>2402.18112v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15266v2")'>Calibration of Deep Learning Classification Models in fNIRS</div>
<div id='2402.15266v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T11:27:10Z</div><div>Authors: Zhihao Cao, Zizhou Luo</div><div style='padding-top: 10px; width: 80ex'>Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool
for monitoring brain activity. The classification of fNIRS data in relation to
conscious activity holds significance for advancing our understanding of the
brain and facilitating the development of brain-computer interfaces (BCI). Many
researchers have turned to deep learning to tackle the classification
challenges inherent in fNIRS data due to its strong generalization and
robustness. In the application of fNIRS, reliability is really important, and
one mathematical formulation of the reliability of confidence is calibration.
However, many researchers overlook the important issue of calibration. To
address this gap, we propose integrating calibration into fNIRS field and
assess the reliability of existing models. Surprisingly, our results indicate
poor calibration performance in many proposed models. To advance calibration
development in the fNIRS field, we summarize three practical tips. Through this
letter, we hope to emphasize the critical role of calibration in fNIRS research
and argue for enhancing the reliability of deep learning-based predictions in
fNIRS classification tasks. All data from our experimental process are openly
available on GitHub.</div><div><a href='http://arxiv.org/abs/2402.15266v2'>2402.15266v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06644v1")'>SeizNet: An AI-enabled Implantable Sensor Network System for Seizure
  Prediction</div>
<div id='2401.06644v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-12T15:51:40Z</div><div>Authors: Ali Saeizadeh, Douglas Schonholtz, Daniel Uvaydov, Raffaele Guida, Emrecan Demirors, Pedram Johari, Jorge M. Jimenez, Joseph S. Neimat, Tommaso Melodia</div><div style='padding-top: 10px; width: 80ex'>In this paper, we introduce SeizNet, a closed-loop system for predicting
epileptic seizures through the use of Deep Learning (DL) method and implantable
sensor networks. While pharmacological treatment is effective for some epilepsy
patients (with ~65M people affected worldwide), one out of three suffer from
drug-resistant epilepsy. To alleviate the impact of seizure, predictive systems
have been developed that can notify such patients of an impending seizure,
allowing them to take precautionary measures. SeizNet leverages DL techniques
and combines data from multiple recordings, specifically intracranial
electroencephalogram (iEEG) and electrocardiogram (ECG) sensors, that can
significantly improve the specificity of seizure prediction while preserving
very high levels of sensitivity. SeizNet DL algorithms are designed for
efficient real-time execution at the edge, minimizing data privacy concerns,
data transmission overhead, and power inefficiencies associated with
cloud-based solutions. Our results indicate that SeizNet outperforms
traditional single-modality and non-personalized prediction systems in all
metrics, achieving up to 99% accuracy in predicting seizure, offering a
promising new avenue in refractory epilepsy treatment.</div><div><a href='http://arxiv.org/abs/2401.06644v1'>2401.06644v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09450v3")'>Guiding Masked Representation Learning to Capture Spatio-Temporal
  Relationship of Electrocardiogram</div>
<div id='2402.09450v3' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T10:04:13Z</div><div>Authors: Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo</div><div style='padding-top: 10px; width: 80ex'>Electrocardiograms (ECG) are widely employed as a diagnostic tool for
monitoring electrical signals originating from a heart. Recent machine learning
research efforts have focused on the application of screening various diseases
using ECG signals. However, adapting to the application of screening disease is
challenging in that labeled ECG data are limited. Achieving general
representation through self-supervised learning (SSL) is a well-known approach
to overcome the scarcity of labeled data; however, a naive application of SSL
to ECG data, without considering the spatial-temporal relationships inherent in
ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM
(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn
spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM
outperforms other SSL baseline methods in various experimental settings for
arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is
adaptable to various lead combinations. Through quantitative and qualitative
analysis, we show a spatio-temporal relationship within ECG data. Our code is
available at https://github.com/bakqui/ST-MEM.</div><div><a href='http://arxiv.org/abs/2402.09450v3'>2402.09450v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.05434v1")'>ECGformer: Leveraging transformer for ECG heartbeat arrhythmia
  classification</div>
<div id='2401.05434v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-06T06:14:48Z</div><div>Authors: Taymaz Akan, Sait Alp, Mohammad Alfrad Nobel Bhuiyan</div><div style='padding-top: 10px; width: 80ex'>An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat.
There are various types of arrhythmias that can originate from different areas
of the heart, resulting in either a rapid, slow, or irregular heartbeat. An
electrocardiogram (ECG) is a vital diagnostic tool used to detect heart
irregularities and abnormalities, allowing experts to analyze the heart's
electrical signals to identify intricate patterns and deviations from the norm.
Over the past few decades, numerous studies have been conducted to develop
automated methods for classifying heartbeats based on ECG data. In recent
years, deep learning has demonstrated exceptional capabilities in tackling
various medical challenges, particularly with transformers as a model
architecture for sequence processing. By leveraging the transformers, we
developed the ECGformer model for the classification of various arrhythmias
present in electrocardiogram data. We assessed the suggested approach using the
MIT-BIH and PTB datasets. ECG heartbeat arrhythmia classification results show
that the proposed method is highly effective.</div><div><a href='http://arxiv.org/abs/2401.05434v1'>2401.05434v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09474v1")'>Deciphering Heartbeat Signatures: A Vision Transformer Approach to
  Explainable Atrial Fibrillation Detection from ECG Signals</div>
<div id='2402.09474v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T11:04:08Z</div><div>Authors: Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David Vorchheimer</div><div style='padding-top: 10px; width: 80ex'>Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.</div><div><a href='http://arxiv.org/abs/2402.09474v1'>2402.09474v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.09421v1")'>EEG Based Generative Depression Discriminator</div>
<div id='2402.09421v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-19T16:05:13Z</div><div>Authors: Ziming Mao, Hao wu, Yongxi Tan, Yuhe Jin</div><div style='padding-top: 10px; width: 80ex'>Depression is a very common but serious mood disorder.In this paper, We built
a generative detection network(GDN) in accordance with three physiological
laws. Our aim is that we expect the neural network to learn the relevant brain
activity based on the EEG signal and, at the same time, to regenerate the
target electrode signal based on the brain activity. We trained two generators,
the first one learns the characteristics of depressed brain activity, and the
second one learns the characteristics of control group's brain activity. In the
test, a segment of EEG signal was put into the two generators separately, if
the relationship between the EEG signal and brain activity conforms to the
characteristics of a certain category, then the signal generated by the
generator of the corresponding category is more consistent with the original
signal. Thus it is possible to determine the category corresponding to a
certain segment of EEG signal. We obtained an accuracy of 92.30\% on the MODMA
dataset and 86.73\% on the HUSM dataset. Moreover, this model is able to output
explainable information, which can be used to help the user to discover
possible misjudgments of the network.Our code will be released.</div><div><a href='http://arxiv.org/abs/2402.09421v1'>2402.09421v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.03526v1")'>FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task
  Based on A Deep Neural Network</div>
<div id='2403.03526v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-06T08:05:53Z</div><div>Authors: Young-Min Go, Seong-Hyun Yu, Hyeong-Yeong Park, Minji Lee, Ji-Hoon Jeong</div><div style='padding-top: 10px; width: 80ex'>Brain-computer interface (BCI) technology facilitates communication between
the human brain and computers, primarily utilizing electroencephalography (EEG)
signals to discern human intentions. Although EEG-based BCI systems have been
developed for paralysis individuals, ongoing studies explore systems for speech
imagery and motor imagery (MI). This study introduces FingerNet, a specialized
network for fine MI classification, departing from conventional gross MI
studies. The proposed FingerNet could extract spatial and temporal features
from EEG signals, improving classification accuracy within the same hand. The
experimental results demonstrated that performance showed significantly higher
accuracy in classifying five finger-tapping tasks, encompassing thumb, index,
middle, ring, and little finger movements. FingerNet demonstrated dominant
performance compared to the conventional baseline models, EEGNet and
DeepConvNet. The average accuracy for FingerNet was 0.3049, whereas EEGNet and
DeepConvNet exhibited lower accuracies of 0.2196 and 0.2533, respectively.
Statistical validation also demonstrates the predominance of FingerNet over
baseline networks. For biased predictions, particularly for thumb and index
classes, we led to the implementation of weighted cross-entropy and also
adapted the weighted cross-entropy, a method conventionally employed to
mitigate class imbalance. The proposed FingerNet involves optimizing network
structure, improving performance, and exploring applications beyond fine MI.
Moreover, the weighted Cross Entropy approach employed to address such biased
predictions appears to have broader applicability and relevance across various
domains involving multi-class classification tasks. We believe that effective
execution of motor imagery can be achieved not only for fine MI, but also for
local muscle MI</div><div><a href='http://arxiv.org/abs/2403.03526v1'>2403.03526v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09448v1")'>A Comparative Study of Conventional and Tripolar EEG for
  High-Performance Reach-to-Grasp BCI Systems</div>
<div id='2402.09448v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T23:35:44Z</div><div>Authors: Ali Rabiee, Sima Ghafoori, Anna Cetera, Walter Besio, Reza Abiri</div><div style='padding-top: 10px; width: 80ex'>This study aims to enhance BCI applications for individuals with motor
impairments by comparing the effectiveness of tripolar EEG (tEEG) with
conventional EEG. The focus is on interpreting and decoding various grasping
movements, such as power grasp and precision grasp. The goal is to determine
which EEG technology is more effective in processing and translating grasp
related neural signals. The approach involved experimenting on ten healthy
participants who performed two distinct grasp movements: power grasp and
precision grasp, with a no movement condition serving as the baseline. Our
research presents a thorough comparison between EEG and tEEG in decoding
grasping movements. This comparison spans several key parameters, including
signal to noise ratio (SNR), spatial resolution via functional connectivity,
ERPs, and wavelet time frequency analysis. Additionally, our study involved
extracting and analyzing statistical features from the wavelet coefficients,
and both binary and multiclass classification methods were employed. Four
machine learning algorithms were used to evaluate the decoding accuracies. Our
results indicated that tEEG demonstrated superior performance over conventional
EEG in various aspects. This included a higher signal to noise ratio, enhanced
spatial resolution, and more informative data in ERPs and wavelet time
frequency analysis. The use of tEEG led to notable improvements in decoding
accuracy for differentiating movement types. Specifically, tEEG achieved around
90% accuracy in binary and 75.97% for multiclass classification. These results
are markedly better than those from standard EEG, which recorded a maximum of
77.85% and 61.27% in similar tasks, respectively. These findings highlight the
superior effectiveness of tEEG over EEG in decoding grasp types and its
competitive or superior performance in complex classifications compared with
existing research.</div><div><a href='http://arxiv.org/abs/2402.09448v1'>2402.09448v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09447v1")'>Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and
  Natural Grasp Types</div>
<div id='2402.09447v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T23:13:38Z</div><div>Authors: Ali Rabiee, Sima Ghafoori, Anna Cetera, Reza Abiri</div><div style='padding-top: 10px; width: 80ex'>This research aims to decode hand grasps from Electroencephalograms (EEGs)
for dexterous neuroprosthetic development and Brain-Computer Interface (BCI)
applications, especially for patients with motor disorders. Particularly, it
focuses on distinguishing two complex natural power and precision grasps in
addition to a neutral condition as a no-movement condition using a new
EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved
generating time-frequency and topographic maps from wavelet power coefficients.
Then, by using machine learning techniques with novel wavelet features, we
achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement
vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs
Precision, demonstrating the effectiveness of these features in EEG-based grasp
differentiation. In contrast to previous studies, a critical part of our study
was permutation feature importance analysis, which highlighted key features for
grasp classification. It revealed that the most crucial brain activities during
grasping occur in the motor cortex, within the alpha and beta frequency bands.
These insights demonstrate the potential of wavelet features in real-time
neuroprosthetic technology and BCI applications.</div><div><a href='http://arxiv.org/abs/2402.09447v1'>2402.09447v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17790v1")'>EEG classifier cross-task transfer to avoid training sessions in
  robot-assisted rehabilitation</div>
<div id='2402.17790v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T11:26:38Z</div><div>Authors: Niklas Kueper, Su Kyoung Kim, Elsa Andrea Kirchner</div><div style='padding-top: 10px; width: 80ex'>Background: For an individualized support of patients during rehabilitation,
learning of individual machine learning models from the human
electroencephalogram (EEG) is required. Our approach allows labeled training
data to be recorded without the need for a specific training session. For this,
the planned exoskeleton-assisted rehabilitation enables bilateral mirror
therapy, in which movement intentions can be inferred from the activity of the
unaffected arm. During this therapy, labeled EEG data can be collected to
enable movement predictions of only the affected arm of a patient. Methods: A
study was conducted with 8 healthy subjects and the performance of the
classifier transfer approach was evaluated. Each subject performed 3 runs of 40
self-intended unilateral and bilateral reaching movements toward a target while
EEG data was recorded from 64 channels. A support vector machine (SVM)
classifier was trained under both movement conditions to make predictions for
the same type of movement. Furthermore, the classifier was evaluated to predict
unilateral movements by only beeing trained on the data of the bilateral
movement condition. Results: The results show that the performance of the
classifier trained on selected EEG channels evoked by bilateral movement
intentions is not significantly reduced compared to a classifier trained
directly on EEG data including unilateral movement intentions. Moreover, the
results show that our approach also works with only 8 or even 4 channels.
Conclusion: It was shown that the proposed classifier transfer approach enables
motion prediction without explicit collection of training data. Since the
approach can be applied even with a small number of EEG channels, this speaks
for the feasibility of the approach in real therapy sessions with patients and
motivates further investigations with stroke patients.</div><div><a href='http://arxiv.org/abs/2402.17790v1'>2402.17790v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.10285v1")'>Analyzing Brain Activity During Learning Tasks with EEG and Machine
  Learning</div>
<div id='2401.10285v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T18:57:25Z</div><div>Authors: Ryan Cho, Mobasshira Zaman, Kyu Taek Cho, Jaejin Hwang</div><div style='padding-top: 10px; width: 80ex'>This study aimed to analyze brain activity during various STEM activities,
exploring the feasibility of classifying between different tasks. EEG brain
data from twenty subjects engaged in five cognitive tasks were collected and
segmented into 4-second clips. Power spectral densities of brain frequency
waves were then analyzed. Testing different k-intervals with XGBoost, Random
Forest, and Bagging Classifier revealed that Random Forest performed best,
achieving a testing accuracy of 91.07% at an interval size of two. When
utilizing all four EEG channels, cognitive flexibility was most recognizable.
Task-specific classification accuracy showed the right frontal lobe excelled in
mathematical processing and planning, the left frontal lobe in cognitive
flexibility and mental flexibility, and the left temporoparietal lobe in
connections. Notably, numerous connections between frontal and temporoparietal
lobes were observed during STEM activities. This study contributes to a deeper
understanding of implementing machine learning in analyzing brain activity and
sheds light on the brain's mechanisms.</div><div><a href='http://arxiv.org/abs/2401.10285v1'>2401.10285v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17792v1")'>EGNN-C+: Interpretable Evolving Granular Neural Network and Application
  in Classification of Weakly-Supervised EEG Data Streams</div>
<div id='2402.17792v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T15:11:41Z</div><div>Authors: Daniel Leite, Alisson Silva, Gabriella Casalino, Arnab Sharma, Danielle Fortunato, Axel-Cyrille Ngomo</div><div style='padding-top: 10px; width: 80ex'>We introduce a modified incremental learning algorithm for evolving Granular
Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to
represent granules, and customize the adaptation procedures to enhance the
robustness of outer boxes for data coverage and noise suppression, while
ensuring that inner boxes remain flexible to capture drifts. The classifier
evolves from scratch, incorporates new classes on the fly, and performs local
incremental feature weighting. As an application, we focus on the
classification of emotion-related patterns within electroencephalogram (EEG)
signals. Emotion recognition is crucial for enhancing the realism and
interactivity of computer systems. We extract features from the Fourier
spectrum of EEG signals obtained from 28 individuals engaged in playing
computer games -- a public dataset. Each game elicits a different predominant
emotion: boredom, calmness, horror, or joy. We analyze individual electrodes,
time window lengths, and frequency bands to assess the accuracy and
interpretability of resulting user-independent neural models. The findings
indicate that both brain hemispheres assist classification, especially
electrodes on the temporal (T8) and parietal (P7) areas, alongside
contributions from frontal and occipital electrodes. While patterns may
manifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz)
bands, in this order, exhibited higher correspondence with the emotion classes.
The eGNN-C+ demonstrates effectiveness in learning EEG data. It achieves an
accuracy of 81.7% and a 0.0029 II interpretability using 10-second time
windows, even in face of a highly-stochastic time-varying 4-class
classification problem.</div><div><a href='http://arxiv.org/abs/2402.17792v1'>2402.17792v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09443v1")'>Review of algorithms for predicting fatigue using EEG</div>
<div id='2402.09443v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T17:32:02Z</div><div>Authors: Ildar Rakhmatulin</div><div style='padding-top: 10px; width: 80ex'>Fatigue detection is of paramount importance in enhancing safety,
productivity, and well-being across diverse domains, including transportation,
healthcare, and industry. This scientific paper presents a comprehensive
investigation into the application of machine learning algorithms for the
detection of physiological fatigue using Electroencephalogram (EEG) signals.
The primary objective of this study was to assess the efficacy of various
algorithms in predicting an individual's level of fatigue based on EEG data.</div><div><a href='http://arxiv.org/abs/2402.09443v1'>2402.09443v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18241v1")'>Affective State Detection using fNIRs and Machine Learning</div>
<div id='2402.18241v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T11:12:47Z</div><div>Authors: Ritam Ghosh</div><div style='padding-top: 10px; width: 80ex'>Affective states regulate our day to day to function and has a tremendous
effect on mental and physical health. Detection of affective states is of
utmost importance for mental health monitoring, smart entertainment selection
and dynamic workload management. In this paper, we discussed relevant
literature on affective state detection using physiology data, the benefits and
limitations of different sensors and methods used for collecting physiology
data, and our rationale for selecting functional near-infrared spectroscopy. We
present the design of an experiment involving nine subjects to evoke the
affective states of meditation, amusement and cognitive load and the results of
the attempt to classify using machine learning. A mean accuracy of 83.04% was
achieved in three class classification with an individual model; 84.39%
accuracy was achieved for a group model and 60.57% accuracy was achieved for
subject independent model using leave one out cross validation. It was found
that prediction accuracy for cognitive load was higher (evoked using a pen and
paper task) than the other two classes (evoked using computer bases tasks). To
verify that this discrepancy was not due to motor skills involved in the pen
and paper task, a second experiment was conducted using four participants and
the results of that experiment has also been presented in the paper.</div><div><a href='http://arxiv.org/abs/2402.18241v1'>2402.18241v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15513v1")'>Investigating the Generalizability of Physiological Characteristics of
  Anxiety</div>
<div id='2402.15513v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-23T16:49:54Z</div><div>Authors: Emily Zhou, Mohammad Soleymani, Maja J. Matarić</div><div style='padding-top: 10px; width: 80ex'>Recent works have demonstrated the effectiveness of machine learning (ML)
techniques in detecting anxiety and stress using physiological signals, but it
is unclear whether ML models are learning physiological features specific to
stress. To address this ambiguity, we evaluated the generalizability of
physiological features that have been shown to be correlated with anxiety and
stress to high-arousal emotions. Specifically, we examine features extracted
from electrocardiogram (ECG) and electrodermal (EDA) signals from the following
three datasets: Anxiety Phases Dataset (APD), Wearable Stress and Affect
Detection (WESAD), and the Continuously Annotated Signals of Emotion (CASE)
dataset. We aim to understand whether these features are specific to anxiety or
general to other high-arousal emotions through a statistical regression
analysis, in addition to a within-corpus, cross-corpus, and
leave-one-corpus-out cross-validation across instances of stress and arousal.
We used the following classifiers: Support Vector Machines, LightGBM, Random
Forest, XGBoost, and an ensemble of the aforementioned models. We found that
models trained on an arousal dataset perform relatively well on a previously
unseen stress dataset, and vice versa. Our experimental results suggest that
the evaluated models may be identifying emotional arousal instead of stress.
This work is the first cross-corpus evaluation across stress and arousal from
ECG and EDA signals, contributing new findings about the generalizability of
stress detection.</div><div><a href='http://arxiv.org/abs/2402.15513v1'>2402.15513v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.05425v1")'>An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic
  Seizure Detection</div>
<div id='2401.05425v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T21:06:40Z</div><div>Authors: Abdul Aziz, Nhat Pham, Neel Vora, Cody Reynolds, Jaime Lehnen, Pooja Venkatesh, Zhuoran Yao, Jay Harvey, Tam Vu, Kan Ding, Phuc Nguyen</div><div style='padding-top: 10px; width: 80ex'>Epilepsy is one of the most common neurological diseases globally, affecting
around 50 million people worldwide. Fortunately, up to 70 percent of people
with epilepsy could live seizure-free if properly diagnosed and treated, and a
reliable technique to monitor the onset of seizures could improve the quality
of life of patients who are constantly facing the fear of random seizure
attacks. The scalp-based EEG test, despite being the gold standard for
diagnosing epilepsy, is costly, necessitates hospitalization, demands skilled
professionals for operation, and is discomforting for users. In this paper, we
propose EarSD, a novel lightweight, unobtrusive, and socially acceptable
ear-worn system to detect epileptic seizure onsets by measuring the
physiological signals from behind the user's ears. EarSD includes an integrated
custom-built sensing, computing, and communication PCB to collect and amplify
the signals of interest, remove the noises caused by motion artifacts and
environmental impacts, and stream the data wirelessly to the computer or mobile
phone nearby, where data are uploaded to the host computer for further
processing. We conducted both in-lab and in-hospital experiments with epileptic
seizure patients who were hospitalized for seizure studies. The preliminary
results confirm that EarSD can detect seizures with up to 95.3 percent accuracy
by just using classical machine learning algorithms.</div><div><a href='http://arxiv.org/abs/2401.05425v1'>2401.05425v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00965v1")'>Multi-Modal Machine Learning Framework for Automated Seizure Detection
  in Laboratory Rats</div>
<div id='2402.00965v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T19:31:51Z</div><div>Authors: Aaron Mullen, Samuel E. Armstrong, Jasmine Perdeh, Bjorn Bauer, Jeffrey Talbert, V. K. Cody Bumgardner</div><div style='padding-top: 10px; width: 80ex'>A multi-modal machine learning system uses multiple unique data sources and
types to improve its performance. This article proposes a system that combines
results from several types of models, all of which are trained on different
data signals. As an example to illustrate the efficacy of the system, an
experiment is described in which multiple types of data are collected from rats
suffering from seizures. This data includes electrocorticography readings,
piezoelectric motion sensor data, and video recordings. Separate models are
trained on each type of data, with the goal of classifying each time frame as
either containing a seizure or not. After each model has generated its
classification predictions, these results are combined. While each data signal
works adequately on its own for prediction purposes, the significant imbalance
in class labels leads to increased numbers of false positives, which can be
filtered and removed by utilizing all data sources. This paper will demonstrate
that, after postprocessing and combination techniques, classification accuracy
is improved with this multi-modal system when compared to the performance of
each individual data source.</div><div><a href='http://arxiv.org/abs/2402.00965v1'>2402.00965v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.00684v1")'>A Temporal Filter to Extract Doped Conducting Polymer Information
  Features from an Electronic Nose</div>
<div id='2401.00684v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-01T07:04:20Z</div><div>Authors: Wiem Haj Ammar, Aicha Boujnah, Antoine Baron, Aimen Boubaker, Adel Kalboussi, Kamal Lmimouni, Sebastien Pecqueur</div><div style='padding-top: 10px; width: 80ex'>Identifying relevant machine-learning features for multi-sensing platforms is
both an applicative limitation to recognize environments and a necessity to
interpret the physical relevance of transducers' complementarity in their
information processing. Particularly for long acquisitions, feature extraction
must be fully automatized without human intervention and resilient to
perturbations without increasing significantly the computational cost of a
classifier. In this study, we investigate on the relative resistance and
current modulation of a 24-dimensional conductimetric electronic nose, which
uses the exponential moving average as a floating reference in a low-cost
information descriptor for environment recognition. In particular, we
identified that depending on the structure of a linear classifier, the 'modema'
descriptor is optimized for different material sensing elements' contributions
to classify information patterns. The low-pass filtering optimization leads to
opposite behaviors between unsupervised and supervised learning: the latter one
favors longer integration of the reference, allowing to recognize five
different classes over 90%, while the first one prefers using the latest events
as its reference to clusterize patterns by environment nature. Its electronic
implementation shall greatly diminish the computational requirements of
conductimetric electronic noses for on-board environment recognition without
human supervision.</div><div><a href='http://arxiv.org/abs/2401.00684v1'>2401.00684v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2401.07889v1")'>Machine Learning Techniques to Identify Hand Gestures amidst Forearm
  Muscle Signals</div>
<div id='2401.07889v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-15T18:39:13Z</div><div>Authors: Ryan Cho, Sunil Patel, Kyu Taek Cho, Jaejin Hwang</div><div style='padding-top: 10px; width: 80ex'>This study investigated the use of forearm EMG data for distinguishing eight
hand gestures, employing the Neural Network and Random Forest algorithms on
data from ten participants. The Neural Network achieved 97 percent accuracy
with 1000-millisecond windows, while the Random Forest achieved 85 percent
accuracy with 200-millisecond windows. Larger window sizes improved gesture
classification due to increased temporal resolution. The Random Forest
exhibited faster processing at 92 milliseconds, compared to the Neural
Network's 124 milliseconds. In conclusion, the study identified a Neural
Network with a 1000-millisecond stream as the most accurate (97 percent), and a
Random Forest with a 200-millisecond stream as the most efficient (85 percent).
Future research should focus on increasing sample size, incorporating more hand
gestures, and exploring different feature extraction methods and modeling
algorithms to enhance system accuracy and efficiency.</div><div><a href='http://arxiv.org/abs/2401.07889v1'>2401.07889v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.09228v1")'>Uncertainty Quantification for cross-subject Motor Imagery
  classification</div>
<div id='2403.09228v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T09:48:48Z</div><div>Authors: Prithviraj Manivannan, Ivo Pascal de Jong, Matias Valdenegro-Toro, Andreea Ioana Sburlea</div><div style='padding-top: 10px; width: 80ex'>Uncertainty Quantification aims to determine when the prediction from a
Machine Learning model is likely to be wrong. Computer Vision research has
explored methods for determining epistemic uncertainty (also known as model
uncertainty), which should correspond with generalisation error. These methods
theoretically allow to predict misclassifications due to inter-subject
variability. We applied a variety of Uncertainty Quantification methods to
predict misclassifications for a Motor Imagery Brain Computer Interface. Deep
Ensembles performed best, both in terms of classification performance and
cross-subject Uncertainty Quantification performance. However, we found that
standard CNNs with Softmax output performed better than some of the more
advanced methods.</div><div><a href='http://arxiv.org/abs/2403.09228v1'>2403.09228v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.02773v1")'>Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode
  Subsets</div>
<div id='2401.02773v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-05T12:13:00Z</div><div>Authors: Joao Pereira, Dimitrios Chalatsis, Balint Hodossy, Dario Farina</div><div style='padding-top: 10px; width: 80ex'>sEMG pattern recognition algorithms have been explored extensively in
decoding movement intent, yet are known to be vulnerable to changing recording
conditions, exhibiting significant drops in performance across subjects, and
even across sessions. Multi-channel surface EMG, also referred to as
high-density sEMG (HD-sEMG) systems, have been used to improve performance with
the information collected through the use of additional electrodes. However, a
lack of robustness is ever present due to limited datasets and the difficulties
in addressing sources of variability, such as electrode placement. In this
study, we propose training on a collection of input channel subsets and
augmenting our training distribution with data from different electrode
locations, simultaneously targeting electrode shift and reducing input
dimensionality. Our method increases robustness against electrode shift and
results in significantly higher intersession performance across subjects and
classification algorithms.</div><div><a href='http://arxiv.org/abs/2401.02773v1'>2401.02773v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2402.07721v1")'>LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation</div>
<div id='2402.07721v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-12T15:34:56Z</div><div>Authors: Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao</div><div style='padding-top: 10px; width: 80ex'>Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to
fine-tune the pre-trained model under limited computing resources. But it still
faces challenges of resource consumption when scaling up to larger models.
Previous studies employ pruning techniques by evaluating the importance of LoRA
parameters for different layers to address the problem. However, these efforts
only analyzed parameter features to evaluate their importance. Indeed, the
output of LoRA related to the parameters and data is the factor that directly
impacts the frozen model. To this end, we propose LoRA-drop which evaluates the
importance of the parameters by analyzing the LoRA output. We retain LoRA for
important layers and the LoRA of the other layers share the same parameters.
Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of
LoRA-drop.</div><div><a href='http://arxiv.org/abs/2402.07721v1'>2402.07721v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13037v1")'>BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient
  Low-Rank Adaptation of Large Pre-trained Models</div>
<div id='2403.13037v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T14:11:20Z</div><div>Authors: Rushi Qiang, Ruiyi Zhang, Pengtao Xie</div><div style='padding-top: 10px; width: 80ex'>Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale
pre-trained models in downstream tasks by learning low-rank incremental
matrices. Though LoRA and its variants effectively reduce the number of
trainable parameters compared to full fine-tuning methods, they often overfit
training data, resulting in sub-optimal generalization on test data. To address
this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning
approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular
value decomposition to parameterize low-rank incremental matrices and splits
the training of pseudo singular vectors and values across two different subsets
of training data. This division, embedded within separate levels of the BLO
framework, mitigates the risk of overfitting to a single dataset. Tested on ten
datasets covering natural language understanding and generation tasks and
applied to various well-known large pre-trained models, BiLoRA significantly
outperforms LoRA methods and other fine-tuning approaches, with similar amounts
of trainable parameters.</div><div><a href='http://arxiv.org/abs/2403.13037v1'>2403.13037v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09113v2")'>AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based
  on Meta Learning</div>
<div id='2403.09113v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T05:29:35Z</div><div>Authors: Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, Pengtao Xie</div><div style='padding-top: 10px; width: 80ex'>Large-scale pretraining followed by task-specific finetuning has achieved
great success in various NLP tasks. Since finetuning all parameters of large
pretrained models poses substantial computational and memory challenges,
several efficient finetuning methods have been developed. Among them, low-rank
adaptation (LoRA), which finetunes low-rank incremental update matrices on top
of frozen pretrained weights, has proven particularly effective. Nonetheless,
LoRA's uniform rank assignment across all layers, along with its reliance on an
exhaustive search to find the best rank, leads to high computation costs and
suboptimal finetuning performance. To address these limitations, we introduce
AutoLoRA, a meta learning based framework for automatically identifying the
optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a
low-rank update matrix with a selection variable, which determines whether the
rank-1 matrix should be discarded. A meta learning based method is developed to
learn these selection variables. The optimal rank is determined by thresholding
the values of these variables. Our comprehensive experiments on natural
language understanding, generation, and sequence labeling demonstrate the
effectiveness of AutoLoRA.</div><div><a href='http://arxiv.org/abs/2403.09113v2'>2403.09113v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.13128v1")'>AdaFish: Fast low-rank parameter-efficient fine-tuning by using
  second-order information</div>
<div id='2403.13128v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T19:57:37Z</div><div>Authors: Jiang Hu, Quanzheng Li</div><div style='padding-top: 10px; width: 80ex'>Recent advancements in large-scale pretrained models have significantly
improved performance across a variety of tasks in natural language processing
and computer vision. However, the extensive number of parameters in these
models necessitates substantial memory and computational resources for full
training. To adapt these models for downstream tasks or specific
application-oriented datasets, parameter-efficient fine-tuning methods
leveraging pretrained parameters have gained considerable attention. However,
it can still be time-consuming due to lots of parameters and epochs. In this
work, we introduce AdaFish, an efficient algorithm of the second-order type
designed to expedite the training process within low-rank decomposition-based
fine-tuning frameworks. Our key observation is that the associated generalized
Fisher information matrix is either low-rank or extremely small-scaled. Such a
generalized Fisher information matrix is shown to be equivalent to the Hessian
matrix. Moreover, we prove the global convergence of AdaFish, along with its
iteration/oracle complexity. Numerical experiments show that our algorithm is
quite competitive with the state-of-the-art AdamW method.</div><div><a href='http://arxiv.org/abs/2403.13128v1'>2403.13128v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11417v1")'>LoRETTA: Low-Rank Economic Tensor-Train Adaptation for
  Ultra-Low-Parameter Fine-Tuning of Large Language Models</div>
<div id='2402.11417v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T01:20:00Z</div><div>Authors: Yifan Yang, Jiajun Zhou, Ngai Wong, Zheng Zhang</div><div style='padding-top: 10px; width: 80ex'>Various parameter-efficient fine-tuning (PEFT) techniques have been proposed
to enable computationally efficient fine-tuning while maintaining model
performance. However, existing PEFT methods are still limited by the growing
number of trainable parameters with the rapid deployment of Large Language
Models (LLMs). To address this challenge, we present LoRETTA, an
ultra-parameter-efficient framework that significantly reduces trainable
parameters through tensor-train decomposition. Specifically, we propose two
methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs
tensorized adapters, offering a high-performance yet lightweight approach for
the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight
parameterization with a set of small tensor factors. LoRETTA achieves
comparable or better performance than most widely used PEFT methods with up to
$100\times$ fewer parameters on the LLaMA-2-7B models. Furthermore, empirical
results demonstrate that the proposed method effectively improves training
efficiency, enjoys better multi-task learning performance, and enhances the
anti-overfitting capability. Plug-and-play codes built upon the Huggingface
framework and PEFT library will be released.</div><div><a href='http://arxiv.org/abs/2402.11417v1'>2402.11417v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01376v2")'>LoTR: Low Tensor Rank Weight Adaptation</div>
<div id='2402.01376v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T13:00:38Z</div><div>Authors: Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, Aleksandr Mikhalev, Ivan Oseledets</div><div style='padding-top: 10px; width: 80ex'>In this paper we generalize and extend an idea of low-rank adaptation (LoRA)
of large language models (LLMs) based on Transformer architecture. Widely used
LoRA-like methods of fine-tuning LLMs are based on matrix factorization of
gradient update. We introduce LoTR, a novel approach for parameter-efficient
fine-tuning of LLMs which represents a gradient update to parameters in a form
of tensor decomposition. Low-rank adapter for each layer is constructed as a
product of three matrices, and tensor structure arises from sharing left and
right multipliers of this product among layers. Simultaneous compression of a
sequence of layers with low-rank tensor representation allows LoTR to archive
even better parameter efficiency then LoRA especially for deep models.
Moreover, the core tensor does not depend on original weight dimension and can
be made arbitrary small, which allows for extremely cheap and fast downstream
fine-tuning.</div><div><a href='http://arxiv.org/abs/2402.01376v2'>2402.01376v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11887v1")'>SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer
  Attention Modules</div>
<div id='2403.11887v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-18T15:40:36Z</div><div>Authors: Xiangyu Chen, Jing Liu, Ye Wang, Pu Perry Wang, Matthew Brand, Guanghui Wang, Toshiaki Koike-Akino</div><div style='padding-top: 10px; width: 80ex'>Low-rank adaptation (LoRA) and its variants are widely employed in
fine-tuning large models, including large language models for natural language
processing and diffusion models for computer vision. This paper proposes a
generalized framework called SuperLoRA that unifies and extends different LoRA
variants, which can be realized under different hyper-parameter settings.
Introducing grouping, folding, shuffling, projecting, and tensor factoring,
SuperLoRA offers high flexibility compared with other LoRA variants and
demonstrates superior performance for transfer learning tasks especially in the
extremely few-parameter regimes.</div><div><a href='http://arxiv.org/abs/2403.11887v1'>2403.11887v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14180v1")'>Linear Transformers are Versatile In-Context Learners</div>
<div id='2402.14180v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T23:45:57Z</div><div>Authors: Max Vladymyrov, Johannes von Oswald, Mark Sandler, Rong Ge</div><div style='padding-top: 10px; width: 80ex'>Recent research has demonstrated that transformers, particularly linear
attention models, implicitly execute gradient-descent-like algorithms on data
provided in-context during their forward inference step. However, their
capability in handling more complex problems remains unexplored. In this paper,
we prove that any linear transformer maintains an implicit linear model and can
be interpreted as performing a variant of preconditioned gradient descent. We
also investigate the use of linear transformers in a challenging scenario where
the training data is corrupted with different levels of noise. Remarkably, we
demonstrate that for this problem linear transformers discover an intricate and
highly effective optimization algorithm, surpassing or matching in performance
many reasonable baselines. We reverse-engineer this algorithm and show that it
is a novel approach incorporating momentum and adaptive rescaling based on
noise levels. Our findings show that even linear transformers possess the
surprising ability to discover sophisticated optimization strategies.</div><div><a href='http://arxiv.org/abs/2402.14180v1'>2402.14180v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.03183v1")'>How Well Can Transformers Emulate In-context Newton's Method?</div>
<div id='2403.03183v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T18:20:10Z</div><div>Authors: Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, Jason D. Lee</div><div style='padding-top: 10px; width: 80ex'>Transformer-based models have demonstrated remarkable in-context learning
capabilities, prompting extensive research into its underlying mechanisms.
Recent studies have suggested that Transformers can implement first-order
optimization algorithms for in-context learning and even second order ones for
the case of linear regression. In this work, we study whether Transformers can
perform higher order optimization methods, beyond the case of linear
regression. We establish that linear attention Transformers with ReLU layers
can approximate second order optimization algorithms for the task of logistic
regression and achieve $\epsilon$ error with only a logarithmic to the error
more layers. As a by-product we demonstrate the ability of even linear
attention-only Transformers in implementing a single step of Newton's iteration
for matrix inversion with merely two layers. These results suggest the ability
of the Transformer architecture to implement complex algorithms, beyond
gradient descent.</div><div><a href='http://arxiv.org/abs/2403.03183v1'>2403.03183v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.11639v1")'>In-Context Learning with Transformers: Softmax Attention Adapts to
  Function Lipschitzness</div>
<div id='2402.11639v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-18T16:37:32Z</div><div>Authors: Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, Sanjay Shakkottai</div><div style='padding-top: 10px; width: 80ex'>A striking property of transformers is their ability to perform in-context
learning (ICL), a machine learning framework in which the learner is presented
with a novel context during inference implicitly through some data, and tasked
with making a prediction in that context. As such that learner must adapt to
the context without additional training. We explore the role of softmax
attention in an ICL setting where each context encodes a regression task. We
show that an attention unit learns a window that it uses to implement a
nearest-neighbors predictor adapted to the landscape of the pretraining tasks.
Specifically, we show that this window widens with decreasing Lipschitzness and
increasing label noise in the pretraining tasks. We also show that on low-rank,
linear problems, the attention unit learns to project onto the appropriate
subspace before inference. Further, we show that this adaptivity relies
crucially on the softmax activation and thus cannot be replicated by the linear
activation often studied in prior theoretical analyses.</div><div><a href='http://arxiv.org/abs/2402.11639v1'>2402.11639v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.17426v1")'>Superiority of Multi-Head Attention in In-Context Linear Regression</div>
<div id='2401.17426v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T20:29:06Z</div><div>Authors: Yingqian Cui, Jie Ren, Pengfei He, Jiliang Tang, Yue Xing</div><div style='padding-top: 10px; width: 80ex'>We present a theoretical analysis of the performance of transformer with
softmax attention in in-context learning with linear regression tasks. While
the existing literature predominantly focuses on the convergence of
transformers with single-/multi-head attention, our research centers on
comparing their performance. We conduct an exact theoretical analysis to
demonstrate that multi-head attention with a substantial embedding dimension
performs better than single-head attention. When the number of in-context
examples D increases, the prediction loss using single-/multi-head attention is
in O(1/D), and the one for multi-head attention has a smaller multiplicative
constant. In addition to the simplest data distribution setting, we consider
more scenarios, e.g., noisy labels, local examples, correlated features, and
prior knowledge. We observe that, in general, multi-head attention is preferred
over single-head attention. Our results verify the effectiveness of the design
of multi-head attention in the transformer architecture.</div><div><a href='http://arxiv.org/abs/2401.17426v1'>2401.17426v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00743v1")'>Benefits of Transformer: In-Context Learning in Linear Regression Tasks
  with Unstructured Data</div>
<div id='2402.00743v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T16:39:45Z</div><div>Authors: Yue Xing, Xiaofeng Lin, Namjoon Suh, Qifan Song, Guang Cheng</div><div style='padding-top: 10px; width: 80ex'>In practice, it is observed that transformer-based models can learn concepts
in context in the inference stage. While existing literature, e.g.,
\citet{zhang2023trained,huang2023context}, provide theoretical explanations on
this in-context learning ability, they assume the input $x_i$ and the output
$y_i$ for each sample are embedded in the same token (i.e., structured data).
However, in reality, they are presented in two tokens (i.e., unstructured data
\cite{wibisono2023role}). In this case, this paper conducts experiments in
linear regression tasks to study the benefits of the architecture of
transformers and provides some corresponding theoretical intuitions to explain
why the transformer can learn from unstructured data. We study the exact
components in a transformer that facilitate the in-context learning. In
particular, we observe that (1) a transformer with two layers of softmax
(self-)attentions with look-ahead attention mask can learn from the prompt if
$y_i$ is in the token next to $x_i$ for each example; (2) positional encoding
can further improve the performance; and (3) multi-head attention with a high
input embedding dimension has a better prediction performance than single-head
attention.</div><div><a href='http://arxiv.org/abs/2402.00743v1'>2402.00743v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14951v1")'>In-Context Learning of a Linear Transformer Block: Benefits of the MLP
  Component and One-Step GD Initialization</div>
<div id='2402.14951v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T20:26:08Z</div><div>Authors: Ruiqi Zhang, Jingfeng Wu, Peter L. Bartlett</div><div style='padding-top: 10px; width: 80ex'>We study the \emph{in-context learning} (ICL) ability of a \emph{Linear
Transformer Block} (LTB) that combines a linear attention component and a
linear multi-layer perceptron (MLP) component. For ICL of linear regression
with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve
nearly Bayes optimal ICL risk. In contrast, using only linear attention must
incur an irreducible additive approximation error. Furthermore, we establish a
correspondence between LTB and one-step gradient descent estimators with
learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense
that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by
an LTB estimator and every optimal LTB estimator that minimizes the in-class
ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator.
Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be
efficiently optimized with gradient flow, despite a non-convex training
objective. Our results reveal that LTB achieves ICL by implementing
$\mathsf{GD}\text{-}\mathbf{\beta}$, and they highlight the role of MLP layers
in reducing approximation error.</div><div><a href='http://arxiv.org/abs/2402.14951v1'>2402.14951v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18819v1")'>Dual Operating Modes of In-Context Learning</div>
<div id='2402.18819v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-29T03:06:10Z</div><div>Authors: Ziqian Lin, Kangwook Lee</div><div style='padding-top: 10px; width: 80ex'>In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,
acquiring a new skill from in-context samples, and task retrieval, i.e.,
locating and activating a relevant pretrained skill. Recent theoretical work
investigates various mathematical models to analyze ICL, but existing models
explain only one operating mode at a time. We introduce a probabilistic model,
with which one can explain the dual operating modes of ICL simultaneously.
Focusing on in-context learning of linear functions, we extend existing models
for pretraining data by introducing multiple task groups and task-dependent
input distributions. We then analyze the behavior of the optimally pretrained
model under the squared loss, i.e., the MMSE estimator of the label given
in-context examples. Regarding pretraining task distribution as prior and
in-context examples as the observation, we derive the closed-form expression of
the task posterior distribution. With the closed-form expression, we obtain a
quantitative understanding of the two operating modes of ICL. Furthermore, we
shed light on an unexplained phenomenon observed in practice: under certain
settings, the ICL risk initially increases and then decreases with more
in-context examples. Our model offers a plausible explanation for this "early
ascent" phenomenon: a limited number of in-context samples may lead to the
retrieval of an incorrect skill, thereby increasing the risk, which will
eventually diminish as task learning takes effect with more in-context samples.
We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,
where in-context examples are assigned random labels. Lastly, we validate our
findings and predictions via experiments involving Transformers and large
language models.</div><div><a href='http://arxiv.org/abs/2402.18819v1'>2402.18819v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02009v2")'>Robust Multi-Task Learning with Excess Risks</div>
<div id='2402.02009v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-03T03:46:14Z</div><div>Authors: Yifei He, Shiji Zhou, Guojun Zhang, Hyokun Yun, Yi Xu, Belinda Zeng, Trishul Chilimbi, Han Zhao</div><div style='padding-top: 10px; width: 80ex'>Multi-task learning (MTL) considers learning a joint model for multiple tasks
by optimizing a convex combination of all task losses. To solve the
optimization problem, existing methods use an adaptive weight updating scheme,
where task weights are dynamically adjusted based on their respective losses to
prioritize difficult tasks. However, these algorithms face a great challenge
whenever label noise is present, in which case excessive weights tend to be
assigned to noisy tasks that have relatively large Bayes optimal errors,
thereby overshadowing other tasks and causing performance to drop across the
board. To overcome this limitation, we propose Multi-Task Learning with Excess
Risks (ExcessMTL), an excess risk-based task balancing method that updates the
task weights by their distances to convergence instead. Intuitively, ExcessMTL
assigns higher weights to worse-trained tasks that are further from
convergence. To estimate the excess risks, we develop an efficient and accurate
method with Taylor approximation. Theoretically, we show that our proposed
algorithm achieves convergence guarantees and Pareto stationarity. Empirically,
we evaluate our algorithm on various MTL benchmarks and demonstrate its
superior performance over existing methods in the presence of label noise.</div><div><a href='http://arxiv.org/abs/2402.02009v2'>2402.02009v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.01258v1")'>Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field
  Dynamics on the Attention Landscape</div>
<div id='2402.01258v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T09:29:40Z</div><div>Authors: Juno Kim, Taiji Suzuki</div><div style='padding-top: 10px; width: 80ex'>Large language models based on the Transformer architecture have demonstrated
impressive capabilities to learn in context. However, existing theoretical
studies on how this phenomenon arises are limited to the dynamics of a single
layer of attention trained on linear regression tasks. In this paper, we study
the optimization of a Transformer consisting of a fully connected layer
followed by a linear attention layer. The MLP acts as a common nonlinear
representation or feature map, greatly enhancing the power of in-context
learning. We prove in the mean-field and two-timescale limit that the
infinite-dimensional loss landscape for the distribution of parameters, while
highly nonconvex, becomes quite benign. We also analyze the second-order
stability of mean-field dynamics and show that Wasserstein gradient flow almost
always avoids saddle points. Furthermore, we establish novel methods for
obtaining concrete improvement rates both away from and near critical points.
This represents the first saddle point analysis of mean-field dynamics in
general and the techniques are of independent interest.</div><div><a href='http://arxiv.org/abs/2402.01258v1'>2402.01258v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.03485v1")'>Attention Meets Post-hoc Interpretability: A Mathematical Perspective</div>
<div id='2402.03485v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-05T19:56:56Z</div><div>Authors: Gianluigi Lopardo, Frederic Precioso, Damien Garreau</div><div style='padding-top: 10px; width: 80ex'>Attention-based architectures, in particular transformers, are at the heart
of a technological revolution. Interestingly, in addition to helping obtain
state-of-the-art results on a wide range of applications, the attention
mechanism intrinsically provides meaningful insights on the internal behavior
of the model. Can these insights be used as explanations? Debate rages on. In
this paper, we mathematically study a simple attention-based architecture and
pinpoint the differences between post-hoc and attention-based explanations. We
show that they provide quite different results, and that, despite their
limitations, post-hoc methods are capable of capturing more useful insights
than merely examining the attention weights.</div><div><a href='http://arxiv.org/abs/2402.03485v1'>2402.03485v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14753v1")'>Prompting a Pretrained Transformer Can Be a Universal Approximator</div>
<div id='2402.14753v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T18:12:48Z</div><div>Authors: Aleksandar Petrov, Philip H. S. Torr, Adel Bibi</div><div style='padding-top: 10px; width: 80ex'>Despite the widespread adoption of prompting, prompt tuning and prefix-tuning
of transformer models, our theoretical understanding of these fine-tuning
methods remains limited. A key question is whether one can arbitrarily modify
the behavior of pretrained model by prompting or prefix-tuning it. Formally,
whether prompting and prefix-tuning a pretrained model can universally
approximate sequence-to-sequence functions. This paper answers in the
affirmative and demonstrates that much smaller pretrained models than
previously thought can be universal approximators when prefixed. In fact, the
attention mechanism is uniquely suited for universal approximation with
prefix-tuning a single attention head being sufficient to approximate any
continuous function. Moreover, any sequence-to-sequence function can be
approximated by prefixing a transformer with depth linear in the sequence
length. Beyond these density-type results, we also offer Jackson-type bounds on
the length of the prefix needed to approximate a function to a desired
precision.</div><div><a href='http://arxiv.org/abs/2402.14753v1'>2402.14753v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.00269v2")'>Large Convolutional Model Tuning via Filter Subspace</div>
<div id='2403.00269v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T04:16:08Z</div><div>Authors: Wei Chen, Zichen Miao, Qiang Qiu</div><div style='padding-top: 10px; width: 80ex'>Efficient fine-tuning methods are critical to address the high computational
and parameter complexity while adapting large pre-trained models to downstream
tasks. Our study is inspired by prior research that represents each convolution
filter as a linear combination of a small set of filter subspace elements,
referred to as filter atoms. In this paper, we propose to fine-tune pre-trained
models by adjusting only filter atoms, which are responsible for spatial-only
convolution, while preserving spatially-invariant channel combination knowledge
in atom coefficients. In this way, we bring a new filter subspace view for
model tuning. Furthermore, each filter atom can be recursively decomposed as a
combination of another set of atoms, which naturally expands the number of
tunable parameters in the filter subspace. By only adapting filter atoms
constructed by a small number of parameters, while maintaining the rest of
model parameters constant, the proposed approach is highly parameter-efficient.
It effectively preserves the capabilities of pre-trained models and prevents
overfitting to downstream tasks. Extensive experiments show that such a simple
scheme surpasses previous tuning baselines for both discriminate and generative
tasks.</div><div><a href='http://arxiv.org/abs/2403.00269v2'>2403.00269v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02347v2")'>Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models</div>
<div id='2402.02347v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T05:05:43Z</div><div>Authors: Fangzhao Zhang, Mert Pilanci</div><div style='padding-top: 10px; width: 80ex'>In this work we study the enhancement of Low Rank Adaptation (LoRA)
fine-tuning procedure by introducing a Riemannian preconditioner in its
optimization step. Specifically, we introduce an $r\times r$ preconditioner in
each gradient step where $r$ is the LoRA rank. This preconditioner requires a
small change to existing optimizer code and creates virtually minuscule storage
and runtime overhead. Our experimental results with both large language models
and text-to-image diffusion models show that with our preconditioner, the
convergence and reliability of SGD and AdamW can be significantly enhanced.
Moreover, the training process becomes much more robust to hyperparameter
choices such as learning rate. Theoretically, we show that fine-tuning a
two-layer ReLU network in the convex paramaterization with our preconditioner
has convergence rate independent of condition number of the data matrix. This
new Riemannian preconditioner, previously explored in classic low-rank matrix
recovery, is introduced to deep learning tasks for the first time in our work.
We release our code at
https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.</div><div><a href='http://arxiv.org/abs/2402.02347v2'>2402.02347v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16137v1")'>X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme
  Multi-Profile Scenarios</div>
<div id='2401.16137v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T13:13:32Z</div><div>Authors: Namju Kwak, Taesup Kim</div><div style='padding-top: 10px; width: 80ex'>Parameter-efficient fine-tuning (PEFT) techniques, such as adapter tuning,
aim to fine-tune a pre-trained language model (PLM) using a minimal number of
parameters for a specific task or profile. Although adapter tuning provides
increased parameter efficiency compared to full-model fine-tuning, it
introduces a small set of additional parameters attached to a PLM for each
profile. This can become problematic in practical applications with multiple
profiles, particularly when a significant increase in the number of profiles
linearly boosts the total number of additional parameters. To mitigate this
issue, we introduce X-PEFT, a novel PEFT method that leverages a multitude of
given adapters by fine-tuning an extremely small set of compact tensors for a
new profile, which serve as binary masks to adaptively select the given
adapters. To efficiently validate our proposed method, we implement it using a
large number of trained or untrained (random) adapters. We evaluate the
performance of X-PEFT through LaMP and GLUE tasks and demonstrate that it
either matches or surpasses the effectiveness of conventional adapter tuning,
despite reducing the memory requirements per profile by a factor of 10,000
compared to it.</div><div><a href='http://arxiv.org/abs/2401.16137v1'>2401.16137v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.11366v2")'>JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning</div>
<div id='2403.11366v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-17T23:02:04Z</div><div>Authors: Anique Tahir, Lu Cheng, Huan Liu</div><div style='padding-top: 10px; width: 80ex'>The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU.</div><div><a href='http://arxiv.org/abs/2403.11366v2'>2403.11366v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15082v1")'>PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables
  Parameter-Efficient Transfer Learning</div>
<div id='2402.15082v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T03:59:18Z</div><div>Authors: Zhisheng Lin, Han Fu, Chenghao Liu, Zhuo Li, Jianling Sun</div><div style='padding-top: 10px; width: 80ex'>Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for
adapting pre-trained language models to various tasks efficiently. Recently,
there has been a growing interest in transferring knowledge from one or
multiple tasks to the downstream target task to achieve performance
improvements. However, current approaches typically either train adapters on
individual tasks or distill shared knowledge from source tasks, failing to
fully exploit task-specific knowledge and the correlation between source and
target tasks. To overcome these limitations, we propose PEMT, a novel
parameter-efficient fine-tuning framework based on multi-task transfer
learning. PEMT extends the mixture-of-experts (MoE) framework to capture the
transferable knowledge as a weighted combination of adapters trained on source
tasks. These weights are determined by a gated unit, measuring the correlation
between the target and each source task using task description prompt vectors.
To fully exploit the task-specific knowledge, we also propose the Task Sparsity
Loss to improve the sparsity of the gated unit. We conduct experiments on a
broad range of tasks over 17 datasets. The experimental results demonstrate our
PEMT yields stable improvements over full fine-tuning, and state-of-the-art
PEFT and knowledge transferring methods on various tasks. The results highlight
the effectiveness of our method which is capable of sufficiently exploiting the
knowledge and correlation features across multiple tasks.</div><div><a href='http://arxiv.org/abs/2402.15082v1'>2402.15082v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12656v2")'>HyperMoE: Paying Attention to Unselected Experts in Mixture of Experts
  via Dynamic Transfer</div>
<div id='2402.12656v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T02:09:55Z</div><div>Authors: Hao Zhao, Zihan Qiu, Huijia Wu, Zili Wang, Zhaofeng He, Jie Fu</div><div style='padding-top: 10px; width: 80ex'>The Mixture of Experts (MoE) for language models has been proven effective in
augmenting the capacity of models by dynamically routing each input token to a
specific subset of experts for processing. Despite the success, most existing
methods face a challenge for balance between sparsity and the availability of
expert knowledge: enhancing performance through increased use of expert
knowledge often results in diminishing sparsity during expert selection. To
mitigate this contradiction, we propose HyperMoE, a novel MoE framework built
upon Hypernetworks. This framework integrates the computational processes of
MoE with the concept of knowledge transferring in multi-task learning. Specific
modules generated based on the information of unselected experts serve as
supplementary information, which allows the knowledge of experts not selected
to be used while maintaining selection sparsity. Our comprehensive empirical
evaluations across multiple datasets and backbones establish that HyperMoE
significantly outperforms existing MoE methods under identical conditions
concerning the number of experts.</div><div><a href='http://arxiv.org/abs/2402.12656v2'>2402.12656v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.13920v1")'>LocMoE: A Low-overhead MoE for Large Language Model Training</div>
<div id='2401.13920v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T03:36:39Z</div><div>Authors: Jing Li, Zhijie Sun, Xuan He, Li Zeng, Yi Lin, Entong Li, Binfan Zheng, Rongqian Zhao, Xin Chen</div><div style='padding-top: 10px; width: 80ex'>The Mixtures-of-Experts (MoE) model is a widespread distributed and
integrated learning method for large language models (LLM), which is favored
due to its ability to sparsify and expand models efficiently. However, the
performance of MoE is limited by load imbalance and high latency of All-To-All
communication, along with relatively redundant computation owing to large
expert capacity. Load imbalance may result from existing routing policies that
consistently tend to select certain experts. The frequent inter-node
communication in the All-To-All procedure also significantly prolongs the
training time. To alleviate the above performance problems, we propose a novel
routing strategy that combines load balance and locality by converting partial
inter-node communication to that of intra-node. Notably, we elucidate that
there is a minimum threshold for expert capacity, calculated through the
maximal angular deviation between the gating weights of the experts and the
assigned tokens. We port these modifications on the PanGu-Sigma model based on
the MindSpore framework with multi-level routing and conduct experiments on
Ascend clusters. The experiment results demonstrate that the proposed LocMoE
reduces training time per epoch by 12.68% to 22.24% compared to classical
routers, such as hash router and switch router, without impacting the model
accuracy.</div><div><a href='http://arxiv.org/abs/2401.13920v1'>2401.13920v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12399v2")'>Turn Waste into Worth: Rectifying Top-$k$ Router of MoE</div>
<div id='2402.12399v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-17T06:23:27Z</div><div>Authors: Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou, Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, Xipeng Qiu</div><div style='padding-top: 10px; width: 80ex'>Sparse Mixture of Experts (MoE) models are popular for training large
language models due to their computational efficiency. However, the commonly
used top-$k$ routing mechanism suffers from redundancy computation and memory
costs due to the unbalanced routing. Some experts are overflow, where the
exceeding tokens are dropped. While some experts are vacant, which are padded
with zeros, negatively impacting model performance. To address the dropped
tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU
Rectification and the Fill-in Rectification. The Intra-GPU Rectification
handles dropped tokens, efficiently routing them to experts within the GPU
where they are located to avoid inter-GPU communication. The Fill-in
Rectification addresses padding by replacing padding tokens with the tokens
that have high routing scores. Our experimental results demonstrate that the
Intra-GPU Rectification and the Fill-in Rectification effectively handle
dropped tokens and padding, respectively. Furthermore, the combination of them
achieves superior performance, surpassing the accuracy of the vanilla top-1
router by 4.7%.</div><div><a href='http://arxiv.org/abs/2402.12399v2'>2402.12399v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.09259v1")'>To Label or Not to Label: Hybrid Active Learning for Neural Machine
  Translation</div>
<div id='2403.09259v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-14T10:33:28Z</div><div>Authors: Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza</div><div style='padding-top: 10px; width: 80ex'>Active learning (AL) techniques reduce labeling costs for training neural
machine translation (NMT) models by selecting smaller representative subsets
from unlabeled data for annotation. Diversity sampling techniques select
heterogeneous instances, while uncertainty sampling methods select instances
with the highest model uncertainty. Both approaches have limitations -
diversity methods may extract varied but trivial examples, while uncertainty
sampling can yield repetitive, uninformative instances. To bridge this gap, we
propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines
uncertainty and diversity for sentence selection. HUDS computes uncertainty
scores for unlabeled sentences and subsequently stratifies them. It then
clusters sentence embeddings within each stratum using k-MEANS and computes
diversity scores by distance to the centroid. A weighted hybrid score that
combines uncertainty and diversity is then used to select the top instances for
annotation in each AL iteration. Experiments on multi-domain German-English
datasets demonstrate the better performance of HUDS over other strong AL
baselines. We analyze the sentence selection with HUDS and show that it
prioritizes diverse instances having high model uncertainty for annotation in
early AL iterations.</div><div><a href='http://arxiv.org/abs/2403.09259v1'>2403.09259v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17110v1")'>Sinkhorn Distance Minimization for Knowledge Distillation</div>
<div id='2402.17110v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T01:13:58Z</div><div>Authors: Xiao Cui, Yulei Qin, Yuting Gao, Enwei Zhang, Zihan Xu, Tong Wu, Ke Li, Xing Sun, Wengang Zhou, Houqiang Li</div><div style='padding-top: 10px; width: 80ex'>Knowledge distillation (KD) has been widely adopted to compress large
language models (LLMs). Existing KD methods investigate various divergence
measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL),
and Jensen-Shannon (JS) divergences. However, due to limitations inherent in
their assumptions and definitions, these measures fail to deliver effective
supervision when few distribution overlap exists between the teacher and the
student. In this paper, we show that the aforementioned KL, RKL, and JS
divergences respectively suffer from issues of mode-averaging, mode-collapsing,
and mode-underestimation, which deteriorates logits-based KD for diverse NLP
tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the
Sinkhorn distance to ensure a nuanced and precise assessment of the disparity
between teacher and student distributions. Besides, profit by properties of the
Sinkhorn metric, we can get rid of sample-wise KD that restricts the perception
of divergence in each teacher-student sample pair. Instead, we propose a
batch-wise reformulation to capture geometric intricacies of distributions
across samples in the high-dimensional space. Comprehensive evaluation on GLUE
and SuperGLUE, in terms of comparability, validity, and generalizability,
highlights our superiority over state-of-the-art methods on all kinds of LLMs
with encoder-only, encoder-decoder, and decoder-only architectures.</div><div><a href='http://arxiv.org/abs/2402.17110v1'>2402.17110v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01867v1")'>Leveraging Large Language Models for Structure Learning in Prompted Weak
  Supervision</div>
<div id='2402.01867v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T19:45:39Z</div><div>Authors: Jinyan Su, Peilin Yu, Jieyu Zhang, Stephen H. Bach</div><div style='padding-top: 10px; width: 80ex'>Prompted weak supervision (PromptedWS) applies pre-trained large language
models (LLMs) as the basis for labeling functions (LFs) in a weak supervision
framework to obtain large labeled datasets. We further extend the use of LLMs
in the loop to address one of the key challenges in weak supervision: learning
the statistical dependency structure among supervision sources. In this work,
we ask the LLM how similar are these prompted LFs. We propose a Structure
Refining Module, a simple yet effective first approach based on the
similarities of the prompts by taking advantage of the intrinsic structure in
the embedding space. At the core of Structure Refining Module are Labeling
Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared
to previous methods that learn the dependencies from weak labels, our method
finds the dependencies which are intrinsic to the LFs and less dependent on the
data. We show that our Structure Refining Module improves the PromptedWS
pipeline by up to 12.7 points on the benchmark tasks. We also explore the
trade-offs between efficiency and performance with comprehensive ablation
experiments and analysis. Code for this project can be found in
https://github.com/BatsResearch/su-bigdata23-code.</div><div><a href='http://arxiv.org/abs/2402.01867v1'>2402.01867v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01218v2")'>Zero-Shot Position Debiasing for Large Language Models</div>
<div id='2401.01218v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-02T14:12:41Z</div><div>Authors: Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun Ren, Pengjie Ren, Zhumin Chen</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning has been demonstrated to be an effective method to improve the
domain performance of large language models (LLMs). However, LLMs might fit the
dataset bias and shortcuts for prediction, leading to poor generation
performance. Previous works have proven that LLMs are prone to exhibit position
bias, i.e., leveraging information positioned at the beginning or end, or
specific positional cues within the input. Existing debiasing methods for LLMs
require external bias knowledge or annotated non-biased samples, which is
lacking for position debiasing and impractical in reality. In this work, we
propose a zero-shot position debiasing (ZOE) framework to mitigate position
bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for
debiasing without relying on any external knowledge. To improve the quality of
unsupervised responses, we propose a MSA module to prune these responses.
Experiments on eight datasets and five tasks show that ZOE consistently
outperforms existing methods in mitigating three types of position biases.
Besides, ZOE achieves this by sacrificing only a small performance on biased
samples, which is general and effective. To facilitate the reproducibility of
the results, we share the code of all methods and datasets on
https://anonymous.4open.science/r/ZOE-F06B.</div><div><a href='http://arxiv.org/abs/2401.01218v2'>2401.01218v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14948v2")'>Re-Examine Distantly Supervised NER: A New Benchmark and a Simple
  Approach</div>
<div id='2402.14948v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T20:07:02Z</div><div>Authors: Yuepei Li, Kang Zhou, Qiao Qiao, Qing Wang, Qi Li</div><div style='padding-top: 10px; width: 80ex'>This paper delves into Named Entity Recognition (NER) under the framework of
Distant Supervision (DS-NER), where the main challenge lies in the compromised
quality of labels due to inherent errors such as false positives, false
negatives, and positive type errors. We critically assess the efficacy of
current DS-NER methodologies using a real-world benchmark dataset named QTL,
revealing that their performance often does not meet expectations. To tackle
the prevalent issue of label noise, we introduce a simple yet effective
approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which
strategically starts on "easy" and cleaner samples during the training process
to enhance model resilience to noisy samples. Our empirical results highlight
the capability of CuPUL to significantly reduce the impact of noisy labels and
outperform existing methods. QTL dataset and our code is available on GitHub.</div><div><a href='http://arxiv.org/abs/2402.14948v2'>2402.14948v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.13269v1")'>AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient
  Fine-Tuning of Large Models</div>
<div id='2403.13269v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-20T03:07:50Z</div><div>Authors: Zeyu Liu, Souvik Kundu, Anni Li, Junrui Wan, Lianghao Jiang, Peter Anthony Beerel</div><div style='padding-top: 10px; width: 80ex'>We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as
Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each
pre-trained frozen weight tensor, we add a parallel path of trainable low-rank
matrices, namely a down-projection and an up-projection matrix, each of which
is followed by a feature transformation vector. Based on a novel freezing
score, we the incrementally freeze these projection matrices during fine-tuning
to reduce the computation and alleviate over-fitting. Our experimental results
demonstrate that we can achieve state-of-the-art performance with an average
improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up
to $9.5\times$ fewer average trainable parameters. While compared in terms of
runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar
PEFT alternatives. Besides the practical utility of our approach, we provide
insights on the trainability requirements of LoRA paths at different modules
and the freezing schedule for the different projection matrices. Code will be
released.</div><div><a href='http://arxiv.org/abs/2403.13269v1'>2403.13269v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.13516v2")'>ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity
  within Large Language Models</div>
<div id='2402.13516v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T03:58:49Z</div><div>Authors: Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun</div><div style='padding-top: 10px; width: 80ex'>Activation sparsity refers to the existence of considerable
weakly-contributed elements among activation outputs. As a prevalent property
of the models using the ReLU activation function, it has been proven a
promising paradigm to boost model inference efficiency. Nevertheless, most
large language models (LLMs) adopt activation functions without intrinsic
activation sparsity (e.g., GELU and Swish). Some recent efforts have explored
introducing ReLU or its variants as the substitutive activation function to
help LLMs achieve activation sparsity and inference acceleration, but few can
simultaneously obtain high sparsity and comparable model performance. This
paper introduces an effective sparsification method named "ProSparse" to push
LLMs for higher activation sparsity without decreasing model performance.
Specifically, after substituting the activation function of LLMs with ReLU,
ProSparse adopts progressive sparsity regularization with a factor smoothly
increasing along sine curves in multiple stages. This can enhance activation
sparsity and alleviate performance degradation by avoiding radical shifts in
activation distribution. With ProSparse, we obtain high sparsity of 89.32% and
88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable
performance to their original Swish-activated versions. Our inference
acceleration experiments further demonstrate the practical acceleration brought
by higher activation sparsity.</div><div><a href='http://arxiv.org/abs/2402.13516v2'>2402.13516v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.01830v1")'>Iterative Mask Filling: An Effective Text Augmentation Method Using
  Masked Language Modeling</div>
<div id='2401.01830v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-03T16:47:13Z</div><div>Authors: Himmet Toprak Kesgin, Mehmet Fatih Amasyali</div><div style='padding-top: 10px; width: 80ex'>Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.</div><div><a href='http://arxiv.org/abs/2401.01830v1'>2401.01830v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09192v3")'>Preparing Lessons for Progressive Training on Language Models</div>
<div id='2401.09192v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-17T13:04:14Z</div><div>Authors: Yu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, Ming Zhang, Lifeng Shang, Xin Jiang, Qun Liu</div><div style='padding-top: 10px; width: 80ex'>The rapid progress of Transformers in artificial intelligence has come at the
cost of increased resource consumption and greenhouse gas emissions due to
growing model sizes. Prior work suggests using pretrained small models to
improve training efficiency, but this approach may not be suitable for new
model structures. On the other hand, training from scratch can be slow, and
progressively stacking layers often fails to achieve significant acceleration.
To address these challenges, we propose a novel method called Apollo, which
prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by
\textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of
low layers. Our approach involves low-value-prioritized sampling (LVPS) to
train different depths and weight sharing to facilitate efficient expansion. We
also introduce an interpolation method for stable model depth extension.
Experiments demonstrate that Apollo achieves state-of-the-art acceleration
ratios, even rivaling methods using pretrained models, making it a universal
and efficient solution for training deep models while reducing time, financial,
and environmental costs.</div><div><a href='http://arxiv.org/abs/2401.09192v3'>2401.09192v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15173v1")'>Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed
  Zeroth-Order Optimizer</div>
<div id='2402.15173v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T08:11:55Z</div><div>Authors: Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian, Ivor W. Tsang</div><div style='padding-top: 10px; width: 80ex'>Fine-tuning large language models (LLMs) with classic first-order optimizers
entails prohibitive GPU memory due to the backpropagation process. Recent works
have turned to zeroth-order optimizers for fine-tuning, which save substantial
memory by using two forward passes. However, these optimizers are plagued by
the heterogeneity of parameter curvatures across different dimensions. In this
work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer
which is the first work to leverage the diagonal Hessian to enhance
zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the
expensive memory cost and only increases one forward pass per step. Extensive
experiments on various models (350M~66B parameters) indicate that HiZOO
improves model convergence, significantly reducing training steps and
effectively enhancing model accuracy. Moreover, we visualize the optimization
trajectories of HiZOO on test functions, illustrating its effectiveness in
handling heterogeneous curvatures. Lastly, we provide theoretical proofs of
convergence for HiZOO. Code is publicly available at
https://anonymous.4open.science/r/HiZOO27F8.</div><div><a href='http://arxiv.org/abs/2402.15173v1'>2402.15173v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.15207v2")'>HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy</div>
<div id='2401.15207v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-26T21:14:32Z</div><div>Authors: Yongkang Liu, Yiqun Zhang, Qian Li, Tong Liu, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Schütze</div><div style='padding-top: 10px; width: 80ex'>Full-parameter fine-tuning has become the go-to choice for adapting language
models (LMs) to downstream tasks due to its excellent performance. As LMs grow
in size, fine-tuning the full parameters of LMs requires a prohibitively large
amount of GPU memory. Existing approaches utilize zeroth-order optimizer to
conserve GPU memory, which can potentially compromise the performance of LMs as
non-zero order optimizers tend to converge more readily on most downstream
tasks. In this paper, we propose a novel optimizer-independent end-to-end
hierarchical fine-tuning strategy, HiFT, which only updates a subset of
parameters at each training step. HiFT can significantly reduce the amount of
gradients and optimizer state parameters residing in GPU memory at the same
time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT
achieves comparable performance to parameter-efficient fine-tuning and standard
full parameter fine-tuning. (2) HiFT supports various optimizers including
AdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\% GPU memory compared
with standard full-parameter fine-tuning for 7B model. (4) HiFT enables
full-parameter fine-tuning of a 7B model on single 48G A6000 with a precision
of 32 using the AdamW optimizer, without using any memory saving techniques.</div><div><a href='http://arxiv.org/abs/2401.15207v2'>2401.15207v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15751v1")'>Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM
  Fine-Tuning</div>
<div id='2402.15751v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-24T07:22:04Z</div><div>Authors: Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, Yang You</div><div style='padding-top: 10px; width: 80ex'>While fine-tuning large language models (LLMs) for specific tasks often
yields impressive results, it comes at the cost of memory inefficiency due to
back-propagation in gradient-based training. Memory-efficient Zeroth-order
(MeZO) optimizers, recently proposed to address this issue, only require
forward passes during training, making them more memory-friendly. However, the
quality of gradient estimates in zeroth order optimization often depends on the
data dimensionality, potentially explaining why MeZO still exhibits significant
performance drops compared to standard fine-tuning across various tasks.
Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper
introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization
approach that applies ZO only to a carefully chosen subset of parameters. We
propose a simple yet effective parameter selection scheme that yields
significant performance gains with Sparse-MeZO. Additionally, we develop a
memory-optimized implementation for sparse masking, ensuring the algorithm
requires only inference-level memory consumption, allowing Sparse-MeZO to
fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that
Sparse-MeZO consistently improves both performance and convergence speed over
MeZO without any overhead. For example, it achieves a 9\% absolute accuracy
improvement and 3.5x speedup over MeZO on the RTE task.</div><div><a href='http://arxiv.org/abs/2402.15751v1'>2402.15751v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16405v2")'>Scaling Sparse Fine-Tuning to Large Language Models</div>
<div id='2401.16405v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:43:49Z</div><div>Authors: Alan Ansell, Ivan Vulić, Hannah Sterz, Anna Korhonen, Edoardo M. Ponti</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with
instructions or human feedback) due to their sheer number of parameters. A
family of parameter-efficient sparse fine-tuning methods have proven promising
in terms of performance but their memory requirements increase proportionally
to the size of the LLMs. In this work, we scale sparse fine-tuning to
state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse
fine-tuning method which, for a desired density level, maintains an array of
parameter indices and the deltas of these parameters relative to their
pretrained values. It iterates over: (a) updating the active deltas, (b)
pruning indices (based on the change of magnitude of their deltas) and (c)
regrowth of indices. For regrowth, we explore two criteria based on either the
accumulated gradients of a few candidate parameters or their approximate
momenta estimated using the efficient SM3 optimizer. We experiment with
instruction-tuning of LLMs on standard dataset mixtures, finding that SpIEL is
often superior to popular parameter-efficient fine-tuning methods like LoRA
(low-rank adaptation) in terms of performance and comparable in terms of run
time. We additionally show that SpIEL is compatible with both quantization and
efficient optimizers, to facilitate scaling to ever-larger model sizes. We
release the code for SpIEL at https://github.com/AlanAnsell/peft and for the
instruction-tuning experiments at https://github.com/ducdauge/sft-llm.</div><div><a href='http://arxiv.org/abs/2401.16405v2'>2401.16405v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.04679v6")'>RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation</div>
<div id='2401.04679v6' style='display: none; margin-left: 20px'><div>Date: 2024-01-09T17:09:01Z</div><div>Authors: Mahdi Nikdan, Soroush Tabesh, Elvir Crnčević, Dan Alistarh</div><div style='padding-top: 10px; width: 80ex'>We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis that
jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on
top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms LoRA, pure sparse fine-tuning, and alternative hybrid methods at
the same parameter budget, and can even recover the performance of FFT on some
tasks. We provide system support for RoSA to complement the training algorithm,
specifically in the form of sparse GPU kernels which enable memory- and
computationally-efficient training, and show that it is also compatible with
low-precision base weights, resulting in the first joint representation
combining quantization, low-rank and sparse approximations. Our code is
accessible at https://github.com/IST-DASLab/RoSA.</div><div><a href='http://arxiv.org/abs/2401.04679v6'>2401.04679v6</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.02446v2")'>LQER: Low-Rank Quantization Error Reconstruction for LLMs</div>
<div id='2402.02446v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-04T10:59:52Z</div><div>Authors: Cheng Zhang, Jianyi Cheng, George A. Constantinides, Yiren Zhao</div><div style='padding-top: 10px; width: 80ex'>Post-training quantization of Large Language Models (LLMs) is challenging. In
this work, we introduce Low-rank Quantization Error Reduction (LQER), which
combines quantization and low-rank approximation to recover the model
capability. LQER leverages an activation-induced scale matrix to drive the
singular value distribution of quantization error towards a desirable
distribution, which enables nearly-lossless W4A8 quantization on various LLMs
and downstream tasks without the need for knowledge distillation, grid search,
or gradient-base iterative optimization. Unlike existing methods, the
computation pattern of LQER eliminates the need for specialized Scatter and
Gather processes to collect high-precision weights from irregular memory
locations. Our W4A8 LLMs achieve near-lossless performance on six popular
downstream tasks, while using 1.36$\times$ fewer hardware resources than the
leading state-of-the-art method. We will open-source our framework once the
paper is accepted.</div><div><a href='http://arxiv.org/abs/2402.02446v2'>2402.02446v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.02775v1")'>EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs</div>
<div id='2403.02775v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T08:45:30Z</div><div>Authors: Hanlin Tang, Yifu Sun, Decheng Wu, Kai Liu, Jianchen Zhu, Zhanhui Kang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have proven to be very superior to conventional
methods in various tasks. However, their expensive computations and high memory
requirements are prohibitive for deployment. Model quantization is an effective
method for reducing this overhead. The problem is that in most previous works,
the quantized model was calibrated using few samples from the training data,
which might affect the generalization of the quantized LLMs to unknown cases
and tasks. Hence in this work, we explore an important question: Can we design
a data-independent quantization method for LLMs to guarantee its generalization
performance? In this work, we propose EasyQuant, a training-free and
data-independent weight-only quantization algorithm for LLMs. Our observation
indicates that two factors: outliers in the weight and quantization ranges, are
essential for reducing the quantization error. Therefore, in EasyQuant, we
leave the outliers (less than 1%) unchanged and optimize the quantization range
to reduce the reconstruction error. With these methods, we surprisingly find
that EasyQuant achieves comparable performance to the original model. Since
EasyQuant does not depend on any training data, the generalization performance
of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented
in parallel so that the quantized model could be attained in a few minutes even
for LLMs over 100B. To our best knowledge, we are the first work that achieves
almost lossless quantization performance for LLMs under a data-independent
setting and our algorithm runs over 10 times faster than the data-dependent
methods.</div><div><a href='http://arxiv.org/abs/2403.02775v1'>2403.02775v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.06118v2")'>Extreme Compression of Large Language Models via Additive Quantization</div>
<div id='2401.06118v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-11T18:54:44Z</div><div>Authors: Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan Alistarh</div><div style='padding-top: 10px; width: 80ex'>The emergence of accurate open large language models (LLMs) has led to a race
towards quantization techniques for such models enabling execution on end-user
devices. In this paper, we revisit the problem of "extreme" LLM
compression--defined as targeting extremely low bit counts, such as 2 to 3 bits
per parameter, from the point of view of classic methods in Multi-Codebook
Quantization (MCQ). Our work builds on top of Additive Quantization, a classic
algorithm from the MCQ family, and adapts it to the quantization of language
models. The resulting algorithm advances the state-of-the-art in LLM
compression, outperforming all recently-proposed techniques in terms of
accuracy at a given compression budget. For instance, when compressing Llama 2
models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93
perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points
from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B
model to 3.94 perplexity (a .22 improvement) on WikiText2. We release our
implementation of Additive Quantization for Language Models AQLM as a baseline
to facilitate future research in LLM quantization.</div><div><a href='http://arxiv.org/abs/2401.06118v2'>2401.06118v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06408v1")'>What Makes Quantization for Large Language Models Hard? An Empirical
  Study from the Lens of Perturbation</div>
<div id='2403.06408v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-11T03:42:51Z</div><div>Authors: Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan</div><div style='padding-top: 10px; width: 80ex'>Quantization has emerged as a promising technique for improving the memory
and computational efficiency of large language models (LLMs). Though the
trade-off between performance and efficiency is well-known, there is still much
to be learned about the relationship between quantization and LLM performance.
To shed light on this relationship, we propose a new perspective on
quantization, viewing it as perturbations added to the weights and activations
of LLMs. We call this approach "the lens of perturbation". Using this lens, we
conduct experiments with various artificial perturbations to explore their
impact on LLM performance. Our findings reveal several connections between the
properties of perturbations and LLM performance, providing insights into the
failure cases of uniform quantization and suggesting potential solutions to
improve the robustness of LLM quantization. To demonstrate the significance of
our findings, we implement a simple non-uniform quantization approach based on
our insights. Our experiments show that this approach achieves minimal
performance degradation on both 4-bit weight quantization and 8-bit
quantization for weights and activations. These results validate the
correctness of our approach and highlight its potential to improve the
efficiency of LLMs without sacrificing performance.</div><div><a href='http://arxiv.org/abs/2403.06408v1'>2403.06408v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18096v1")'>No Token Left Behind: Reliable KV Cache Compression via Importance-Aware
  Mixed Precision Quantization</div>
<div id='2402.18096v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T06:34:54Z</div><div>Authors: June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee</div><div style='padding-top: 10px; width: 80ex'>Key-Value (KV) Caching has become an essential technique for accelerating the
inference speed and throughput of generative Large Language Models~(LLMs).
However, the memory footprint of the KV cache poses a critical bottleneck in
LLM deployment as the cache size grows with batch size and sequence length,
often surpassing even the size of the model itself. Although recent methods
were proposed to select and evict unimportant KV pairs from the cache to reduce
memory consumption, the potential ramifications of eviction on the generative
process are yet to be thoroughly examined. In this paper, we examine the
detrimental impact of cache eviction and observe that unforeseen risks arise as
the information contained in the KV pairs is exhaustively discarded, resulting
in safety breaches, hallucinations, and context loss. Surprisingly, we find
that preserving even a small amount of information contained in the evicted KV
pairs via reduced precision quantization substantially recovers the incurred
degradation. On the other hand, we observe that the important KV pairs must be
kept at a relatively higher precision to safeguard the generation quality.
Motivated by these observations, we propose \textit{Mixed-precision KV
cache}~(MiKV), a reliable cache compression method that simultaneously
preserves the context details by retaining the evicted KV pairs in
low-precision and ensure generation quality by keeping the important KV pairs
in high-precision. Experiments on diverse benchmarks and LLM backbones show
that our proposed method offers a state-of-the-art trade-off between
compression ratio and performance, compared to other baselines.</div><div><a href='http://arxiv.org/abs/2402.18096v1'>2402.18096v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.05527v2")'>GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
  Generative Inference of LLM</div>
<div id='2403.05527v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-08T18:48:30Z</div><div>Authors: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao</div><div style='padding-top: 10px; width: 80ex'>Key-value (KV) caching has become the de-facto to accelerate generation speed
for large language models (LLMs) inference. However, the growing cache demand
with increasing sequence length has transformed LLM inference to be a memory
bound problem, significantly constraining the system throughput. Existing
methods rely on dropping unimportant tokens or quantizing all entries
uniformly. Such methods, however, often incur high approximation errors to
represent the compressed matrices. The autoregressive decoding process further
compounds the error of each step, resulting in critical deviation in model
generation and deterioration of performance. To tackle this challenge, we
propose GEAR, an efficient KV cache compression framework that achieves
near-lossless high-ratio compression. GEAR first applies quantization to
majority of entries of similar magnitudes to ultra-low precision. It then
employs a low rank matrix to approximate the quantization error, and a sparse
matrix to remedy individual errors from outlier entries. By adeptly integrating
three techniques, GEAR is able to fully exploit their synergistic potentials.
Our experiments demonstrate that compared to alternatives, GEAR achieves
near-lossless 4-bit KV cache compression with up to 2.38x throughput
improvement, while reducing peak-memory size up to 2.29x. Our code is publicly
available at https://github.com/HaoKang-Timmy/GEAR.</div><div><a href='http://arxiv.org/abs/2403.05527v2'>2403.05527v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.14109v1")'>CompactifAI: Extreme Compression of Large Language Models using
  Quantum-Inspired Tensor Networks</div>
<div id='2401.14109v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-25T11:45:21Z</div><div>Authors: Andrei Tomut, Saeed S. Jahromi, Sukhbinder Singh, Faysal Ishtiaq, Cesar Muñoz, Prabdeep Singh Bajaj, Ali Elborady, Gianni del Bimbo, Mehrazin Alizadeh, David Montero, Pablo Martin-Ramiro, Muhammad Ibrahim, Oussama Tahiri Alaoui, John Malcolm, Samuel Mugel, Roman Orus</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly
in generative Artificial Intelligence (AI), but their immense size poses
significant challenges, such as huge training and inference costs, substantial
energy demands, and limitations for on-site deployment. Traditional compression
methods such as pruning, distillation, and low-rank approximation focus on
reducing the effective number of neurons in the network, while quantization
focuses on reducing the numerical precision of individual weights to reduce the
model size while keeping the number of neurons fixed. While these compression
methods have been relatively successful in practice, there's no compelling
reason to believe that truncating the number of neurons is an optimal strategy.
In this context, this paper introduces CompactifAI, an innovative LLM
compression approach using quantum-inspired Tensor Networks that focuses on the
model's correlation space instead, allowing for a more controlled, refined and
interpretable model compression. Our method is versatile and can be implemented
with - or on top of - other compression techniques. As a benchmark, we
demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model
to only $30\%$ of its original size while recovering over $90\%$ of the
original accuracy after a brief distributed retraining.</div><div><a href='http://arxiv.org/abs/2401.14109v1'>2401.14109v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16367v1")'>TQCompressor: improving tensor decomposition methods in neural networks
  via permutations</div>
<div id='2401.16367v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T18:07:56Z</div><div>Authors: V. Abronin, A. Naumov, D. Mazur, D. Bystrov, K. Tsarova, Ar. Melnikov, I. Oseledets, S. Dolgov, R. Brasher, M. Perelshtein</div><div style='padding-top: 10px; width: 80ex'>We introduce TQCompressor, a novel method for neural network model
compression with improved tensor decompositions. We explore the challenges
posed by the computational and storage demands of pre-trained language models
in NLP tasks and propose a permutation-based enhancement to Kronecker
decomposition. This enhancement makes it possible to reduce loss in model
expressivity which is usually associated with factorization. We demonstrate
this method applied to the GPT-2$_{small}$. The result of the compression is
TQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in
the GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further
enhance the performance of the TQCompressedGPT-2 through a training strategy
involving multi-step knowledge distillation, using only a 3.1% of the
OpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative
evaluations, marking an advancement in the efficient and effective deployment
of models in resource-constrained environments.</div><div><a href='http://arxiv.org/abs/2401.16367v1'>2401.16367v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.00290v1")'>Semantic Text Transmission via Prediction with Small Language Models:
  Cost-Similarity Trade-off</div>
<div id='2403.00290v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-01T05:20:16Z</div><div>Authors: Bhavani A Madhabhavi, Gangadhar Karevvanavar, Rajshekhar V Bhat, Nikolaos Pappas</div><div style='padding-top: 10px; width: 80ex'>We consider the communication of natural language text from a source to a
destination over noiseless and character-erasure channels. We exploit
language's inherent correlations and predictability to constrain transmission
costs by allowing the destination to predict or complete words with potential
dissimilarity with the source text. Concretely, our objective is to obtain
achievable $(\bar{c}, \bar{s})$ pairs, where $\bar{c}$ is the average
transmission cost at the source and $\bar{s}$ is the average semantic
similarity measured via cosine similarity between vector embedding of words at
the source and those predicted/completed at the destination. We obtain
$(\bar{c}, \bar{s})$ pairs for neural language and first-order Markov
chain-based small language models (SLM) for prediction, using both a threshold
policy that transmits a word if its cosine similarity with that
predicted/completed at the destination is below a threshold, and a periodic
policy, which transmits words after a specific interval and predicts/completes
the words in between, at the destination. We adopt an SLM for word completion.
We demonstrate that, when communication occurs over a noiseless channel, the
threshold policy achieves a higher $\bar{s}$ for a given $\bar{c}$ than the
periodic policy and that the $\bar{s}$ achieved with the neural SLM is greater
than or equal to that of the Markov chain-based algorithm for the same
$\bar{c}$. The improved performance comes with a higher complexity in terms of
time and computing requirements. However, when communication occurs over a
character-erasure channel, all prediction algorithms and scheduling policies
perform poorly. Furthermore, if character-level Huffman coding is used, the
required $\bar{c}$ to achieve a given $\bar{s}$ is reduced, but the above
observations still apply.</div><div><a href='http://arxiv.org/abs/2403.00290v1'>2403.00290v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.02871v1")'>Quantum Mixed-State Self-Attention Network</div>
<div id='2403.02871v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-05T11:29:05Z</div><div>Authors: Fu Chen, Qinglin Zhao, Li Feng, Chuangtao Chen, Yangbin Lin, Jianhong Lin</div><div style='padding-top: 10px; width: 80ex'>The rapid advancement of quantum computing has increasingly highlighted its
potential in the realm of machine learning, particularly in the context of
natural language processing (NLP) tasks. Quantum machine learning (QML)
leverages the unique capabilities of quantum computing to offer novel
perspectives and methodologies for complex data processing and pattern
recognition challenges. This paper introduces a novel Quantum Mixed-State
Attention Network (QMSAN), which integrates the principles of quantum computing
with classical machine learning algorithms, especially self-attention networks,
to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model
employs a quantum attention mechanism based on mixed states, enabling efficient
direct estimation of similarity between queries and keys within the quantum
domain, leading to more effective attention weight acquisition. Additionally,
we propose an innovative quantum positional encoding scheme, implemented
through fixed quantum gates within the quantum circuit, to enhance the model's
accuracy. Experimental validation on various datasets demonstrates that QMSAN
model outperforms existing quantum and classical models in text classification,
achieving significant performance improvements. QMSAN model not only
significantly reduces the number of parameters but also exceeds classical
self-attention networks in performance, showcasing its strong capability in
data representation and information extraction. Furthermore, our study
investigates the model's robustness in different quantum noise environments,
showing that QMSAN possesses commendable robustness to low noise.</div><div><a href='http://arxiv.org/abs/2403.02871v1'>2403.02871v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.01384v1")'>On the Compressibility of Quantized Large Language Models</div>
<div id='2403.01384v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-03T03:27:07Z</div><div>Authors: Yu Mao, Weilan Wang, Hongchao Du, Nan Guan, Chun Jason Xue</div><div style='padding-top: 10px; width: 80ex'>Deploying Large Language Models (LLMs) on edge or mobile devices offers
significant benefits, such as enhanced data privacy and real-time processing
capabilities. However, it also faces critical challenges due to the substantial
memory requirement of LLMs. Quantization is an effective way of reducing the
model size while maintaining good performance. However, even after
quantization, LLMs may still be too big to fit entirely into the limited memory
of edge or mobile devices and have to be partially loaded from the storage to
complete the inference. In this case, the I/O latency of model loading becomes
the bottleneck of the LLM inference latency. In this work, we take a
preliminary step of studying applying data compression techniques to reduce
data movement and thus speed up the inference of quantized LLM on
memory-constrained devices. In particular, we discussed the compressibility of
quantized LLMs, the trade-off between the compressibility and performance of
quantized LLMs, and opportunities to optimize both of them jointly.</div><div><a href='http://arxiv.org/abs/2403.01384v1'>2403.01384v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09748v1")'>Model Compression and Efficient Inference for Large Language Models: A
  Survey</div>
<div id='2402.09748v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T06:58:30Z</div><div>Authors: Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, Xiaofei He</div><div style='padding-top: 10px; width: 80ex'>Transformer based large language models have achieved tremendous success.
However, the significant memory and computational costs incurred during the
inference process make it challenging to deploy large models on
resource-constrained devices. In this paper, we investigate compression and
efficient inference methods for large language models from an algorithmic
perspective. Regarding taxonomy, similar to smaller models, compression and
acceleration algorithms for large language models can still be categorized into
quantization, pruning, distillation, compact architecture design, dynamic
networks. However, Large language models have two prominent characteristics
compared to smaller models: (1) Most of compression algorithms require
finetuning or even retraining the model after compression. The most notable
aspect of large models is the very high cost associated with model finetuning
or training. Therefore, many algorithms for large models, such as quantization
and pruning, start to explore tuning-free algorithms. (2) Large models
emphasize versatility and generalization rather than performance on a single
task. Hence, many algorithms, such as knowledge distillation, focus on how to
preserving their versatility and generalization after compression. Since these
two characteristics were not very pronounced in early large models, we further
distinguish large language models into medium models and ``real'' large models.
Additionally, we also provide an introduction to some mature frameworks for
efficient inference of large models, which can support basic compression or
acceleration algorithms, greatly facilitating model deployment for users.</div><div><a href='http://arxiv.org/abs/2402.09748v1'>2402.09748v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13934v1")'>Do Efficient Transformers Really Save Computation?</div>
<div id='2402.13934v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T17:00:56Z</div><div>Authors: Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, Liwei Wang</div><div style='padding-top: 10px; width: 80ex'>As transformer-based language models are trained on increasingly large
datasets and with vast numbers of parameters, finding more efficient
alternatives to the standard Transformer has become very valuable. While many
efficient Transformers and Transformer alternatives have been proposed, none
provide theoretical guarantees that they are a suitable replacement for the
standard Transformer. This makes it challenging to identify when to use a
specific model and what directions to prioritize for further investigation. In
this paper, we aim to understand the capabilities and limitations of efficient
Transformers, specifically the Sparse Transformer and the Linear Transformer.
We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT)
prompts and follow previous works to model them as Dynamic Programming (DP)
problems. Our results show that while these models are expressive enough to
solve general DP tasks, contrary to expectations, they require a model size
that scales with the problem size. Nonetheless, we identify a class of DP
problems for which these models can be more efficient than the standard
Transformer. We confirm our theoretical results through experiments on
representative DP tasks, adding to the understanding of efficient Transformers'
practical strengths and weaknesses.</div><div><a href='http://arxiv.org/abs/2402.13934v1'>2402.13934v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.04666v1")'>Telecom Language Models: Must They Be Large?</div>
<div id='2403.04666v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-07T17:13:12Z</div><div>Authors: Nicola Piovesan, Antonio De Domenico, Fadhel Ayed</div><div style='padding-top: 10px; width: 80ex'>The increasing interest in Large Language Models (LLMs) within the
telecommunications sector underscores their potential to revolutionize
operational efficiency. However, the deployment of these sophisticated models
is often hampered by their substantial size and computational demands, raising
concerns about their viability in resource-constrained environments. Addressing
this challenge, recent advancements have seen the emergence of small language
models that surprisingly exhibit performance comparable to their larger
counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a
compact yet powerful model, exemplifies this new wave of efficient small
language models. This paper conducts a comprehensive evaluation of Phi-2's
intrinsic understanding of the telecommunications domain. Recognizing the
scale-related limitations, we enhance Phi-2's capabilities through a
Retrieval-Augmented Generation approach, meticulously integrating an extensive
knowledge base specifically curated with telecom standard specifications. The
enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering
questions about telecom standards with a precision that closely rivals the more
resource-intensive GPT-3.5. The paper further explores the refined capabilities
of Phi-2 in addressing problem-solving scenarios within the telecom sector,
highlighting its potential and limitations.</div><div><a href='http://arxiv.org/abs/2403.04666v1'>2403.04666v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.16269v1")'>From Large Language Models and Optimization to Decision Optimization
  CoPilot: A Research Manifesto</div>
<div id='2402.16269v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-26T03:10:11Z</div><div>Authors: Segev Wasserkrug, Leonard Boussioux, Dick den Hertog, Farzaneh Mirzazadeh, Ilker Birbil, Jannis Kurtz, Donato Maragno</div><div style='padding-top: 10px; width: 80ex'>Significantly simplifying the creation of optimization models for real-world
business problems has long been a major goal in applying mathematical
optimization more widely to important business and societal decisions. The
recent capabilities of Large Language Models (LLMs) present a timely
opportunity to achieve this goal. Therefore, we propose research at the
intersection of LLMs and optimization to create a Decision Optimization CoPilot
(DOCP) - an AI tool designed to assist any decision maker, interacting in
natural language to grasp the business problem, subsequently formulating and
solving the corresponding optimization model. This paper outlines our DOCP
vision and identifies several fundamental requirements for its implementation.
We describe the state of the art through a literature survey and experiments
using ChatGPT. We show that a) LLMs already provide substantial novel
capabilities relevant to a DOCP, and b) major research challenges remain to be
addressed. We also propose possible research directions to overcome these gaps.
We also see this work as a call to action to bring together the LLM and
optimization communities to pursue our vision, thereby enabling much more
widespread improved decision-making.</div><div><a href='http://arxiv.org/abs/2402.16269v1'>2402.16269v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.00421v2")'>From PARIS to LE-PARIS: Toward Patent Response Automation with
  Recommender Systems and Collaborative Large Language Models</div>
<div id='2402.00421v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T08:37:13Z</div><div>Authors: Jung-Mei Chu, Hao-Cheng Lo, Jieh Hsiang, Chun-Chieh Cho</div><div style='padding-top: 10px; width: 80ex'>In patent prosecution, timely and effective responses to Office Actions (OAs)
are crucial for securing patents. However, past automation and artificial
intelligence research have largely overlooked this aspect. To bridge this gap,
our study introduces the Patent Office Action Response Intelligence System
(PARIS) and its advanced version, the Large Language Model (LLM) Enhanced PARIS
(LE-PARIS). These systems are designed to enhance the efficiency of patent
attorneys in handling OA responses through collaboration with AI. The systems'
key features include the construction of an OA Topics Database, development of
Response Templates, and implementation of Recommender Systems and LLM-based
Response Generation. To validate the effectiveness of the systems, we have
employed a multi-paradigm analysis using the USPTO Office Action database and
longitudinal data based on attorney interactions with our systems over six
years. Through five studies, we have examined the constructiveness of OA topics
(studies 1 and 2) using topic modeling and our proposed Delphi process, the
efficacy of our proposed hybrid LLM-based recommender system tailored for OA
responses (study 3), the quality of generated responses (study 4), and the
systems' practical value in real-world scenarios through user studies (study
5). The results indicate that both PARIS and LE-PARIS significantly achieve key
metrics and have a positive impact on attorney performance.</div><div><a href='http://arxiv.org/abs/2402.00421v2'>2402.00421v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.01136v1")'>LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition
  and Adaptive Quantization</div>
<div id='2403.01136v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T08:40:07Z</div><div>Authors: Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Chuan Wu</div><div style='padding-top: 10px; width: 80ex'>Recent breakthroughs in Large-scale language models (LLMs) have demonstrated
impressive performance on various tasks. The immense sizes of LLMs have led to
very high resource demand and cost for running the models. Though the models
are largely served using uniform high-caliber GPUs nowadays, utilizing a
heterogeneous cluster with a mix of available high- and low-capacity GPUs can
potentially substantially reduce the serving cost. There is a lack of designs
to support efficient LLM serving using a heterogeneous cluster, while the
current solutions focus on model partition and uniform compression among
homogeneous devices. This paper proposes LLM-PQ, a system that advocates
adaptive model quantization and phase-aware partition to improve LLM serving
efficiency on heterogeneous GPU clusters. We carefully decide on
mixed-precision model quantization together with phase-aware model partition
and micro-batch sizing in distributed LLM serving with an efficient algorithm,
to greatly enhance inference throughput while fulfilling user-specified model
quality targets. Extensive experiments on production inference workloads in 11
different clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on
average) throughput improvement in inference, showing great advantages over
state-of-the-art works.</div><div><a href='http://arxiv.org/abs/2403.01136v1'>2403.01136v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.09687v1")'>Robust Learning-Augmented Dictionaries</div>
<div id='2402.09687v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T03:45:44Z</div><div>Authors: Ali Zeynali, Shahin Kamali, Mohammad Hajiesmaili</div><div style='padding-top: 10px; width: 80ex'>We present the first learning-augmented data structure for implementing
dictionaries with optimal consistency and robustness. Our data structure, named
RobustSL, is a skip list augmented by predictions of access frequencies of
elements in a data sequence. With proper predictions, RobustSL has optimal
consistency (achieves static optimality). At the same time, it maintains a
logarithmic running time for each operation, ensuring optimal robustness, even
if predictions are generated adversarially. Therefore, RobustSL has all the
advantages of the recent learning-augmented data structures of Lin, Luo, and
Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness
guarantees that are absent in the previous work. Numerical experiments show
that RobustSL outperforms alternative data structures using both synthetic and
real datasets.</div><div><a href='http://arxiv.org/abs/2402.09687v1'>2402.09687v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.13429v1")'>Everything You Always Wanted to Know About Storage Compressibility of
  Pre-Trained ML Models but Were Afraid to Ask</div>
<div id='2402.13429v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-20T23:45:37Z</div><div>Authors: Zhaoyuan Su, Ammar Ahmed, Zirui Wang, Ali Anwar, Yue Cheng</div><div style='padding-top: 10px; width: 80ex'>As the number of pre-trained machine learning (ML) models is growing
exponentially, data reduction tools are not catching up. Existing data
reduction techniques are not specifically designed for pre-trained model (PTM)
dataset files. This is largely due to a lack of understanding of the patterns
and characteristics of these datasets, especially those relevant to data
reduction and compressibility.
  This paper presents the first, exhaustive analysis to date of PTM datasets on
storage compressibility. Our analysis spans different types of data reduction
and compression techniques, from hash-based data deduplication, data similarity
detection, to dictionary-coding compression. Our analysis explores these
techniques at three data granularity levels, from model layers, model chunks,
to model parameters. We draw new observations that indicate that modern data
reduction tools are not effective when handling PTM datasets. There is a
pressing need for new compression methods that take into account PTMs' data
characteristics for effective storage reduction.
  Motivated by our findings, we design ELF, a simple yet effective,
error-bounded, lossy floating-point compression method. ELF transforms
floating-point parameters in such a way that the common exponent field of the
transformed parameters can be completely eliminated to save storage space. We
develop Elves, a compression framework that integrates ELF along with several
other data reduction methods. Elves uses the most effective method to compress
PTMs that exhibit different patterns. Evaluation shows that Elves achieves an
overall compression ratio of $1.52\times$, which is $1.31\times$, $1.32\times$
and $1.29\times$ higher than a general-purpose compressor (zstd), an
error-bounded lossy compressor (SZ3), and the uniform model quantization,
respectively, with negligible model accuracy loss.</div><div><a href='http://arxiv.org/abs/2402.13429v1'>2402.13429v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2401.17139v1")'>Large Language Model Evaluation via Matrix Entropy</div>
<div id='2401.17139v1' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T16:19:55Z</div><div>Authors: Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, Weiran Huang</div><div style='padding-top: 10px; width: 80ex'>Large language models (LLMs) have revolutionized the field of natural
language processing, extending their strong capabilities into multi-modal
domains. Thus, it is vital to define proper and diversified metrics for the
evaluation of LLMs.
  In this paper, we introduce matrix entropy, a novel metric rooted in
information theory and geometry principles to quantify the data compression
proficiency in LLMs. It reflects the model's ability to extract relevant
information and eliminate unnecessary elements, thereby providing insight into
the language model's intrinsic capability. Specifically, we demonstrate its
applicability in both single-modal (language) and multi-modal settings. For
language models, our findings reveal that the matrix entropy of representations
follows a scaling law type reduction when the model scales up, serving as a
complement to the traditional loss scaling law. For the multi-modal setting, we
also propose an evaluation method based on matrix entropy for assessing
alignment quality and we find that modern large multi-modal models exhibit
great alignment performance.</div><div><a href='http://arxiv.org/abs/2401.17139v1'>2401.17139v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01763v2")'>When Large Language Models Meet Vector Databases: A Survey</div>
<div id='2402.01763v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-30T23:35:28Z</div><div>Authors: Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, Min Zhang</div><div style='padding-top: 10px; width: 80ex'>This survey explores the synergistic potential of Large Language Models
(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving
research area. With the proliferation of LLMs comes a host of challenges,
including hallucinations, outdated knowledge, prohibitive commercial
application costs, and memory issues. VecDBs emerge as a compelling solution to
these issues by offering an efficient means to store, retrieve, and manage the
high-dimensional vector representations intrinsic to LLM operations. Through
this nuanced review, we delineate the foundational principles of LLMs and
VecDBs and critically analyze their integration's impact on enhancing LLM
functionalities. This discourse extends into a discussion on the speculative
future developments in this domain, aiming to catalyze further research into
optimizing the confluence of LLMs and VecDBs for advanced data handling and
knowledge extraction capabilities.</div><div><a href='http://arxiv.org/abs/2402.01763v2'>2402.01763v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.15337v1")'>Ranking Entities along Conceptual Space Dimensions with LLMs: An
  Analysis of Fine-Tuning Strategies</div>
<div id='2402.15337v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-23T14:17:01Z</div><div>Authors: Nitesh Kumar, Usashi Chatterjee, Steven Schockaert</div><div style='padding-top: 10px; width: 80ex'>Conceptual spaces represent entities in terms of their primitive semantic
features. Such representations are highly valuable but they are notoriously
difficult to learn, especially when it comes to modelling perceptual and
subjective features. Distilling conceptual spaces from Large Language Models
(LLMs) has recently emerged as a promising strategy. However, existing work has
been limited to probing pre-trained LLMs using relatively simple zero-shot
strategies. We focus in particular on the task of ranking entities according to
a given conceptual space dimension. Unfortunately, we cannot directly fine-tune
LLMs on this task, because ground truth rankings for conceptual space
dimensions are rare. We therefore use more readily available features as
training data and analyse whether the ranking capabilities of the resulting
models transfer to perceptual and subjective features. We find that this is
indeed the case, to some extent, but having perceptual and subjective features
in the training data seems essential for achieving the best results. We
furthermore find that pointwise ranking strategies are competitive against
pairwise approaches, in defiance of common wisdom.</div><div><a href='http://arxiv.org/abs/2402.15337v1'>2402.15337v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(50.0%, 50.0%, 50.0%)' onclick='toggle("2403.07378v2")'>SVD-LLM: Truncation-aware Singular Value Decomposition for Large
  Language Model Compression</div>
<div id='2403.07378v2' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T07:31:18Z</div><div>Authors: Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang</div><div style='padding-top: 10px; width: 80ex'>The advancements in Large Language Models (LLMs) have been hindered by their
substantial sizes, which necessitate LLM compression methods for practical
deployment. Singular Value Decomposition (SVD) offers a promising solution for
LLM compression. However, state-of-the-art SVD-based LLM compression methods
have two key limitations: truncating smaller singular values may lead to higher
compression loss, and the lack of update on the remaining model parameters
after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM
compression method that addresses the limitations of existing methods. SVD-LLM
incorporates a truncation-aware data whitening strategy to ensure a direct
mapping between singular values and compression loss. Moreover, SVD-LLM adopts
a layer-wise closed-form model parameter update strategy to compensate for
accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total
of 11 datasets and seven models from three different LLM families at four
different scales. Our results demonstrate the superiority of SVD-LLM over
state-of-the-arts, especially at high model compression ratios. The source code
is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.</div><div><a href='http://arxiv.org/abs/2403.07378v2'>2403.07378v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09977v1")'>Fast Vocabulary Transfer for Language Model Compression</div>
<div id='2402.09977v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T14:37:07Z</div><div>Authors: Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni</div><div style='padding-top: 10px; width: 80ex'>Real-world business applications require a trade-off between language model
performance and size. We propose a new method for model compression that relies
on vocabulary transfer. We evaluate the method on various vertical domains and
downstream tasks. Our results indicate that vocabulary transfer can be
effectively used in combination with other compression techniques, yielding a
significant reduction in model size and inference time while marginally
compromising on performance.</div><div><a href='http://arxiv.org/abs/2402.09977v1'>2402.09977v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2402.14151v1")'>BIRCO: A Benchmark of Information Retrieval Tasks with Complex
  Objectives</div>
<div id='2402.14151v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T22:22:30Z</div><div>Authors: Xiaoyue Wang, Jianyou Wang, Weili Cao, Kaicheng Wang, Ramamohan Paturi, Leon Bergen</div><div style='padding-top: 10px; width: 80ex'>We present the Benchmark of Information Retrieval (IR) tasks with Complex
Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve
documents given multi-faceted user objectives. The benchmark's complexity and
compact size make it suitable for evaluating large language model (LLM)-based
information retrieval systems. We present a modular framework for investigating
factors that may influence LLM performance on retrieval tasks, and identify a
simple baseline model which matches or outperforms existing approaches and more
complex alternatives. No approach achieves satisfactory performance on all
benchmark tasks, suggesting that stronger models and new retrieval protocols
are necessary to address complex user needs.</div><div><a href='http://arxiv.org/abs/2402.14151v1'>2402.14151v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.16184v3")'>On the Semantics of LM Latent Space: A Vocabulary-defined Approach</div>
<div id='2401.16184v3' style='display: none; margin-left: 20px'><div>Date: 2024-01-29T14:29:48Z</div><div>Authors: Jian Gu, Chunyang Chen, Aldeida Aleti</div><div style='padding-top: 10px; width: 80ex'>Understanding the latent space of language models (LM) is crucial to refining
their performance and interpretability. Existing analyses often fall short in
providing disentangled (model-centric) insights into LM semantics, and neglect
essential aspects of LM adaption. In response, we introduce a pioneering method
called vocabulary-defined semantics, which establishes a reference frame within
the LM latent space, ensuring disentangled semantic analysis grounded in LM
vocabulary. Our approach transcends prior entangled analysis, leveraging LM
vocabulary for model-centric insights. Furthermore, we propose a novel
technique to compute logits, emphasising differentiability and local isotropy,
and introduce a neural clustering module for semantically calibrating data
representations during LM adaptation. Through extensive experiments across
diverse text understanding datasets, our approach outperforms state-of-the-art
methods of retrieval-augmented generation and parameter-efficient finetuning,
showcasing its efficacy and broad applicability. Our findings not only shed
light on LM mechanics, but also offer practical solutions to enhance LM
performance and interpretability.</div><div><a href='http://arxiv.org/abs/2401.16184v3'>2401.16184v3</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14890v2")'>Vygotsky Distance: Measure for Benchmark Task Similarity</div>
<div id='2402.14890v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-22T12:00:32Z</div><div>Authors: Maxim K. Surkov, Ivan P. Yamshchikov</div><div style='padding-top: 10px; width: 80ex'>Evaluation plays a significant role in modern natural language processing.
Most modern NLP benchmarks consist of arbitrary sets of tasks that neither
guarantee any generalization potential for the model once applied outside the
test set nor try to minimize the resource consumption needed for model
evaluation. This paper presents a theoretical instrument and a practical
algorithm to calculate similarity between benchmark tasks, we call this
similarity measure "Vygotsky distance". The core idea of this similarity
measure is that it is based on relative performance of the "students" on a
given task, rather that on the properties of the task itself. If two tasks are
close to each other in terms of Vygotsky distance the models tend to have
similar relative performance on them. Thus knowing Vygotsky distance between
tasks one can significantly reduce the number of evaluation tasks while
maintaining a high validation quality. Experiments on various benchmarks,
including GLUE, SuperGLUE, CLUE, and RussianSuperGLUE, demonstrate that a vast
majority of NLP benchmarks could be at least 40% smaller in terms of the tasks
included. Most importantly, Vygotsky distance could also be used for the
validation of new tasks thus increasing the generalization potential of the
future NLP models.</div><div><a href='http://arxiv.org/abs/2402.14890v2'>2402.14890v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.17764v1")'>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</div>
<div id='2402.17764v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-27T18:56:19Z</div><div>Authors: Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei</div><div style='padding-top: 10px; width: 80ex'>Recent research, such as BitNet, is paving the way for a new era of 1-bit
Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,
namely BitNet b1.58, in which every single parameter (or weight) of the LLM is
ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)
Transformer LLM with the same model size and training tokens in terms of both
perplexity and end-task performance, while being significantly more
cost-effective in terms of latency, memory, throughput, and energy consumption.
More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for
training new generations of LLMs that are both high-performance and
cost-effective. Furthermore, it enables a new computation paradigm and opens
the door for designing specific hardware optimized for 1-bit LLMs.</div><div><a href='http://arxiv.org/abs/2402.17764v1'>2402.17764v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.18079v2")'>KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
  Quantization</div>
<div id='2401.18079v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T18:58:14Z</div><div>Authors: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami</div><div style='padding-top: 10px; width: 80ex'>LLMs are seeing growing use for applications such as document analysis and
summarization which require large context windows, and with these large context
windows KV cache activations surface as the dominant contributor to memory
consumption during inference. Quantization is a promising approach for
compressing KV cache activations; however, existing solutions fail to represent
activations accurately in ultra-low precisions, such as sub-4-bit. In this
work, we present KVQuant, which addresses this problem by incorporating novel
methods for quantizing cached KV activations, including: (i) Per-Channel Key
Quantization, where we adjust the dimension along which we quantize the Key
activations to better match the distribution; (ii) Pre-RoPE Key Quantization,
where we quantize Key activations before the rotary positional embedding to
mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,
where we derive per-layer sensitivity-weighted non-uniform datatypes that
better represent the distributions; (iv) Per-Vector Dense-and-Sparse
Quantization, where we isolate outliers separately for each vector to minimize
skews in quantization ranges; and (v) Q-Norm, where we normalize quantization
centroids in order to mitigate distribution shift, providing additional
benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,
and Mistral models, we achieve $&lt;0.1$ perplexity degradation with 3-bit
quantization on both Wikitext-2 and C4, outperforming existing approaches. Our
method enables serving the LLaMA-7B model with a context length of up to 1
million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.</div><div><a href='http://arxiv.org/abs/2401.18079v2'>2401.18079v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.06265v1")'>Unpacking Tokenization: Evaluating Text Compression and its Correlation
  with Model Performance</div>
<div id='2403.06265v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-10T17:02:53Z</div><div>Authors: Omer Goldman, Avi Caciularu, Matan Eyal, Kris Cao, Idan Szpektor, Reut Tsarfaty</div><div style='padding-top: 10px; width: 80ex'>Despite it being the cornerstone of BPE, the most common tokenization
algorithm, the importance of compression in the tokenization process is still
unclear. In this paper, we argue for the theoretical importance of compression,
that can be viewed as 0-gram language modeling where equal probability is
assigned to all tokens. We also demonstrate the empirical importance of
compression for downstream success of pre-trained language models. We control
the compression ability of several BPE tokenizers by varying the amount of
documents available during their training: from 1 million documents to a
character-based tokenizer equivalent to no training data at all. We then
pre-train English language models based on those tokenizers and fine-tune them
over several tasks. We show that there is a correlation between tokenizers'
compression and models' downstream performance, suggesting that compression is
a reliable intrinsic indicator of tokenization quality. These correlations are
more pronounced for generation tasks (over classification) or for smaller
models (over large ones). We replicated a representative part of our
experiments on Turkish and found similar results, confirming that our results
hold for languages with typological characteristics dissimilar to English. We
conclude that building better compressing tokenizers is a fruitful avenue for
further research and for improving overall model performance.</div><div><a href='http://arxiv.org/abs/2403.06265v1'>2403.06265v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.09949v1")'>Multi-Word Tokenization for Sequence Compression</div>
<div id='2402.09949v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-15T13:52:23Z</div><div>Authors: Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini</div><div style='padding-top: 10px; width: 80ex'>Large Language Models have proven highly successful at modelling a variety of
tasks. However, this comes at a steep computational cost that hinders wider
industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer
that goes beyond word boundaries by representing frequent multi-word
expressions as single tokens. MWTs produce a more compact and efficient
tokenization that yields two benefits: (1) Increase in performance due to a
greater coverage of input data given a fixed sequence length and budget; (2)
Faster and lighter inference due to the ability to reduce the sequence length
with negligible drops in performance. Our results show that MWT is more robust
across shorter sequence lengths, thus allowing for major speedups via early
sequence truncation.</div><div><a href='http://arxiv.org/abs/2402.09949v1'>2402.09949v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.10691v1")'>MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual
  Language Modeling</div>
<div id='2403.10691v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-15T21:21:11Z</div><div>Authors: Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, Luke Zettlemoyer</div><div style='padding-top: 10px; width: 80ex'>A major consideration in multilingual language modeling is how to best
represent languages with diverse vocabularies and scripts. Although
contemporary text encoding methods cover most of the world's writing systems,
they exhibit bias towards the high-resource languages of the Global West. As a
result, texts of underrepresented languages tend to be segmented into long
sequences of linguistically meaningless units. To address the disparities, we
introduce a new paradigm that encodes the same information with segments of
consistent size across diverse languages. Our encoding convention (MYTE) is
based on morphemes, as their inventories are more balanced across languages
than characters, which are used in previous methods. We show that MYTE produces
shorter encodings for all 99 analyzed languages, with the most notable
improvements for non-European languages and non-Latin scripts. This, in turn,
improves multilingual LM performance and diminishes the perplexity gap
throughout diverse languages.</div><div><a href='http://arxiv.org/abs/2403.10691v1'>2403.10691v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(75.0%, 75.0%, 75.0%)' onclick='toggle("2403.12544v1")'>AffineQuant: Affine Transformation Quantization for Large Language
  Models</div>
<div id='2403.12544v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-19T08:40:21Z</div><div>Authors: Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong Ji</div><div style='padding-top: 10px; width: 80ex'>The significant resource requirements associated with Large-scale Language
Models (LLMs) have generated considerable interest in the development of
techniques aimed at compressing and accelerating neural networks. Among these
techniques, Post-Training Quantization (PTQ) has emerged as a subject of
considerable interest due to its noteworthy compression efficiency and
cost-effectiveness in the context of training. Existing PTQ methods for LLMs
limit the optimization scope to scaling transformations between pre- and
post-quantization weights. In this paper, we advocate for the direct
optimization using equivalent Affine transformations in PTQ (AffineQuant). This
approach extends the optimization scope and thus significantly minimizing
quantization errors. Additionally, by employing the corresponding inverse
matrix, we can ensure equivalence between the pre- and post-quantization
outputs of PTQ, thereby maintaining its efficiency and generalization
capabilities. To ensure the invertibility of the transformation during
optimization, we further introduce a gradual mask optimization method. This
method initially focuses on optimizing the diagonal elements and gradually
extends to the other elements. Such an approach aligns with the
Levy-Desplanques theorem, theoretically ensuring invertibility of the
transformation. As a result, significant performance improvements are evident
across different LLMs on diverse datasets. To illustrate, we attain a C4
perplexity of 15.76 (2.26 lower vs 18.02 in OmniQuant) on the LLaMA2-7B model
of W4A4 quantization without overhead. On zero-shot tasks, AffineQuant achieves
an average of 58.61 accuracy (1.98 lower vs 56.63 in OmniQuant) when using
4/4-bit quantization for LLaMA-30B, which setting a new state-of-the-art
benchmark for PTQ in LLMs.</div><div><a href='http://arxiv.org/abs/2403.12544v1'>2403.12544v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.14866v1")'>APTQ: Attention-aware Post-Training Mixed-Precision Quantization for
  Large Language Models</div>
<div id='2402.14866v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-21T07:45:22Z</div><div>Authors: Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, Hao Yu</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) have greatly advanced the natural language
processing paradigm. However, the high computational load and huge model sizes
pose a grand challenge for deployment on edge devices. To this end, we propose
APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,
which considers not only the second-order information of each layer's weights,
but also, for the first time, the nonlinear effect of attention outputs on the
entire model. We leverage the Hessian trace as a sensitivity metric for
mixed-precision quantization, ensuring an informed precision reduction that
retains model performance. Experiments show APTQ surpasses previous
quantization methods, achieving an average of 4 bit width a 5.22 perplexity
nearly equivalent to full precision in the C4 dataset. In addition, APTQ
attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an
average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating
its effectiveness to produce high-quality quantized LLMs.</div><div><a href='http://arxiv.org/abs/2402.14866v1'>2402.14866v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.12065v2")'>WKVQuant: Quantizing Weight and Key/Value Cache for Large Language
  Models Gains More</div>
<div id='2402.12065v2' style='display: none; margin-left: 20px'><div>Date: 2024-02-19T11:33:21Z</div><div>Authors: Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, Liqiang Nie</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) face significant deployment challenges due to
their substantial memory requirements and the computational demands of
auto-regressive text generation process. This paper addresses these challenges
by focusing on the quantization of LLMs, a technique that reduces memory
consumption by converting model parameters and activations into low-bit
integers. We critically analyze the existing quantization approaches,
identifying their limitations in balancing the accuracy and efficiency of the
quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ
framework especially designed for quantizing weights and the key/value (KV)
cache of LLMs. Specifically, we incorporates past-only quantization to improve
the computation of attention. Additionally, we introduce two-dimensional
quantization strategy to handle the distribution of KV cache, along with a
cross-block reconstruction regularization for parameter optimization.
Experiments show that WKVQuant achieves almost comparable memory savings to
weight-activation quantization, while also approaching the performance of
weight-only quantization.</div><div><a href='http://arxiv.org/abs/2402.12065v2'>2402.12065v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2401.09486v2")'>LoMA: Lossless Compressed Memory Attention</div>
<div id='2401.09486v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-16T09:18:46Z</div><div>Authors: Yumeng Wang, Zhenyang Xiao</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) face limitations due to the high demand on GPU
memory and computational resources when handling long contexts. While sparsify
the Key-Value (KV) cache of transformer model is a typical strategy to
alleviate resource usage, it unavoidably results in the loss of information. We
introduce Lossless Compressed Memory Attention (LoMA), a novel approach that
enables lossless compression of the KV cache, thereby reducing the memory and
computational demands during autoregressive generation. LoMA incorporates a
specialized training or fine-tuning precedure alongside an autoregressive
generation algorithm optimized for the compressed context. Our method
compresses the KV cache after every $tc$ generated tokens with a compression
ratio of $c$ and a target compressed length $t$, and this process occurs within
a single inference pass without dependency on auxiliary models. We engineered
an efficient training scheme involving specific inputs, attention masks, and
position identifiers to instill this compression capability. Experimental
validation has demonstrated that LoMA significantly reducing computational
consumption and memory usage through achieving lossless KV cache compression.</div><div><a href='http://arxiv.org/abs/2401.09486v2'>2401.09486v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.18668v1")'>Simple linear attention language models balance the recall-throughput
  tradeoff</div>
<div id='2402.18668v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-28T19:28:27Z</div><div>Authors: Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher Ré</div><div style='padding-top: 10px; width: 80ex'>Recent work has shown that attention-based language models excel at recall,
the ability to ground generations in tokens previously seen in context.
However, the efficiency of attention-based models is bottle-necked during
inference by the KV-cache's aggressive memory consumption. In this work, we
explore whether we can improve language model efficiency (e.g. by reducing
memory consumption) without compromising on recall. By applying experiments and
theory to a broad set of architectures, we identify a key tradeoff between a
model's state size and recall ability. We show that efficient alternatives to
attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but
struggle at recall. We propose BASED a simple architecture combining linear and
sliding window attention. By varying BASED window size and linear attention
feature dimension, we can dial the state size and traverse the pareto frontier
of the recall-memory tradeoff curve, recovering the full quality of attention
on one end and the small state size of attention-alternatives on the other. We
train language models up to 1.3b parameters and show that BASED matches the
strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them
on real-world recall-intensive tasks by 6.22 accuracy points. Implementations
of linear attention are often less efficient than optimized standard attention
implementations. To make BASED competitive, we develop IO-aware algorithms that
enable 24x higher throughput on language generation than FlashAttention-2, when
generating 1024 tokens using 1.3b parameter models. Code for this work is
provided at: https://github.com/HazyResearch/based.</div><div><a href='http://arxiv.org/abs/2402.18668v1'>2402.18668v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.08058v1")'>CHAI: Clustered Head Attention for Efficient LLM Inference</div>
<div id='2403.08058v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-12T20:10:04Z</div><div>Authors: Saurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee, Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean Wu</div><div style='padding-top: 10px; width: 80ex'>Large Language Models (LLMs) with hundreds of billions of parameters have
transformed the field of machine learning. However, serving these models at
inference time is both compute and memory intensive, where a single request can
require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is
one of the key components of LLMs, which can account for over 50% of LLMs
memory and compute requirement. We observe that there is a high amount of
redundancy across heads on which tokens they pay attention to. Based on this
insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a
high amount of correlation for self-attention at runtime, thus reducing both
memory and compute. In our experiments, we show that CHAI is able to reduce the
memory requirements for storing K,V cache by up to 21.4% and inference time
latency by up to 1.73x without any fine-tuning required. CHAI achieves this
with a maximum 3.2% deviation in accuracy across 3 different models (i.e.
OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.</div><div><a href='http://arxiv.org/abs/2403.08058v1'>2403.08058v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2403.01273v1")'>NoMAD-Attention: Efficient LLM Inference on CPUs Through
  Multiply-add-free Attention</div>
<div id='2403.01273v1' style='display: none; margin-left: 20px'><div>Date: 2024-03-02T17:29:22Z</div><div>Authors: Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, Anshumali Shrivastava</div><div style='padding-top: 10px; width: 80ex'>Large language model inference on Central Processing Units (CPU) is
challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix
operations in the attention computations. In this paper, we argue that there is
a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,
which allow for ultra-low-latency lookups in batch. We leverage this unique
capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm
that replaces MAD operations with in-register lookups. Through hardware-aware
algorithmic designs, NoMAD-Attention achieves the computation of attention
scores using repeated fast accesses to SIMD registers despite their highly
limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based
LLMs without model finetuning. Empirical evaluations demonstrate that
NoMAD-Attention maintains the quality of the original LLMs well, and speeds up
the 4-bit quantized LLaMA-7B-based model by up to 2$\times$ at 16k context
length. Our results are reproducible at
https://github.com/tonyzhang617/nomad-dist.</div><div><a href='http://arxiv.org/abs/2403.01273v1'>2403.01273v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.10930v2")'>ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters</div>
<div id='2402.10930v2' style='display: none; margin-left: 20px'><div>Date: 2024-01-31T17:52:52Z</div><div>Authors: Shiwei Liu, Guanchen Tao, Yifei Zou, Derek Chow, Zichen Fan, Kauna Lei, Bangfei Pan, Dennis Sylvester, Gregory Kielian, Mehdi Saligane</div><div style='padding-top: 10px; width: 80ex'>The self-attention mechanism sets transformer-based large language model
(LLM) apart from the convolutional and recurrent neural networks. Despite the
performance improvement, achieving real-time LLM inference on silicon is
challenging due to the extensively used Softmax in self-attention. Apart from
the non-linearity, the low arithmetic intensity greatly reduces the processing
parallelism, which becomes the bottleneck especially when dealing with a longer
context. To address this challenge, we propose Constant Softmax (ConSmax), a
software-hardware co-design as an efficient Softmax alternative. ConSmax
employs differentiable normalization parameters to remove the maximum searching
and denominator summation in Softmax. It allows for massive parallelization
while performing the critical tasks of Softmax. In addition, a scalable ConSmax
hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless
non-linear operation and support mix-precision computing. It further
facilitates efficient LLM inference. Experimental results show that ConSmax
achieves a minuscule power consumption of 0.43 mW and area of 0.001 mm2 at
1-GHz working frequency and 22-nm CMOS technology. Compared to state-of-the-art
Softmax hardware, ConSmax results in 14.5x energy and 14.0x area savings with a
comparable accuracy on a GPT-2 model and the WikiText103 dataset.</div><div><a href='http://arxiv.org/abs/2402.10930v2'>2402.10930v2</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01047v1")'>Ultra Fast Transformers on FPGAs for Particle Physics Experiments</div>
<div id='2402.01047v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-01T22:32:39Z</div><div>Authors: Zhixing Jiang, Dennis Yin, Elham E Khoda, Vladimir Loncar, Ekaterina Govorkova, Eric Moreno, Philip Harris, Scott Hauck, Shih-Chieh Hsu</div><div style='padding-top: 10px; width: 80ex'>This work introduces a highly efficient implementation of the transformer
architecture on a Field-Programmable Gate Array (FPGA) by using the
\texttt{hls4ml} tool. Given the demonstrated effectiveness of transformer
models in addressing a wide range of problems, their application in
experimental triggers within particle physics becomes a subject of significant
interest. In this work, we have implemented critical components of a
transformer model, such as multi-head attention and softmax layers. To evaluate
the effectiveness of our implementation, we have focused on a particle physics
jet flavor tagging problem, employing a public dataset. We recorded latency
under 2 $\mu$s on the Xilinx UltraScale+ FPGA, which is compatible with
hardware trigger requirements at the CERN Large Hadron Collider experiments.</div><div><a href='http://arxiv.org/abs/2402.01047v1'>2402.01047v1</a></div>
</div></div><div><div style='margin-top: 5px; border-top: 1px solid rgb(100.0%, 100.0%, 100.0%)' onclick='toggle("2402.01876v1")'>Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC</div>
<div id='2402.01876v1' style='display: none; margin-left: 20px'><div>Date: 2024-02-02T20:02:12Z</div><div>Authors: Patrick Odagiu, Zhiqiang Que, Javier Duarte, Johannes Haller, Gregor Kasieczka, Artur Lobanov, Vladimir Loncar, Wayne Luk, Jennifer Ngadiuba, Maurizio Pierini, Philipp Rincke, Arpita Seksaria, Sioni Summers, Andre Sznajder, Alexander Tapper, Thea K. Aarrestad</div><div style='padding-top: 10px; width: 80ex'>We study various machine learning based algorithms for performing accurate
jet flavor classification on field-programmable gate arrays and demonstrate how
latency and resource consumption scale with the input size and choice of
algorithm. These architectures provide an initial design for models that could
be used for tagging at the CERN LHC during its high-luminosity phase. The
high-luminosity upgrade will lead to a five-fold increase in its instantaneous
luminosity for proton-proton collisions and, in turn, higher data volume and
complexity, such as the availability of jet constituents. Through
quantization-aware training and efficient hardware implementations, we show
that O(100) ns inference of complex architectures such as deep sets and
interaction networks is feasible at a low computational resource cost.</div><div><a href='http://arxiv.org/abs/2402.01876v1'>2402.01876v1</a></div>
</div></div>
    <div><a href="arxiv_-1.html">Prev (-1)</a></div>
    <div><a href="arxiv_1.html">Next (1)</a></div>
    